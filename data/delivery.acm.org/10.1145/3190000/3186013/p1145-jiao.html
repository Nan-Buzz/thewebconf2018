<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Find the Conversation Killers: A Predictive Study of Thread-ending Posts</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Find the Conversation Killers: A Predictive Study of Thread-ending Posts</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Yunhao</span>     <span class="surName">Jiao</span>,     Zhejiang University, <a href="mailto:jiao_yunhao@zju.edu.cn">jiao_yunhao@zju.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Cheng</span>     <span class="surName">Li</span>,     University of Michigan, <a href="mailto:lichengz@umich.edu">lichengz@umich.edu</a>    </div>    <div class="author">     <span class="givenName">Fei</span>     <span class="surName">Wu</span>,     Zhejiang University, <a href="mailto:wufei@zju.edu.cn">wufei@zju.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Qiaozhu</span>     <span class="surName">Mei</span>,     University of Michigan, <a href="mailto:qmei@umich.edu">qmei@umich.edu</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186013" target="_blank">https://doi.org/10.1145/3178876.3186013</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>How to improve the quality of conversations in online communities has attracted considerable attention recently. Having engaged, civil, and reactive online conversations has a critical effect on the social life of Internet users. In this study, we are particularly interested in identifying a post in a multi-party conversation that is unlikely to be further replied to, which therefore kills that thread of the conversation. For this purpose, we propose a deep learning model called the ConverNet. ConverNet is attractive due to its capability of modeling the internal structure of a long conversation and its appropriate encoding of the contextual information of the conversation, through effective integration of attention mechanisms. Empirical experiments on real-world datasets demonstrate the effectiveness of the proposed model. For the widely concerned topic, our analysis also offers implications for how to improve the quality and user experience of online conversations, or how to engage users in a conversation with a chatbot.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Online conversations; conversation prediction; deep learning</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yunhao Jiao, Cheng Li, Fei Wu, and Qiaozhu Mei. 2018. Find the Conversation Killers: A Predictive Study of Thread-ending Posts. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186013" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186013</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p><a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>More and more people are relying on online communities to access the latest information, exchange ideas, express options, and participate in social discussions. Facilitating these natural conversations in online communities has become increasingly important. On one hand, decision makers utilize these conversations to optimize their online marketing strategies; social scientists study how opinions are shaped and diffused through discussions; politicians analyze how users respond to certain governmental policies. On the other hand, effective and healthy conversations lead to increasing satisfaction and engagement of users; low-quality and ill conversations hurt the users&#x2019; social experience, turn them away, and even convert them into trolls. How to engage people in online conversations has aroused the interest of researchers from various domains.</p>    <p>Facilitating user conversations in online communities has also become an active line of research in the field of data mining. Many studies focus on predicting the number of retweets accumulated by a particular Tweet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] on Twitter, identifying various factors that could help users get a higher response rate from the audience. Similar studies have also been conducted on online forums [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>], e.g., to parse and predict thread structures, which can potentially enhance information access and sharing.</p>    <p>While most existing work on online conversations focuses on posts that are positively received (e.g., highly retweeted Tweets), there are much fewer studies on posts that are negatively received. For example, there is no existing work identifying conversation killers, posts that result in no further replies in a multi-party conversation. Analyzing the ineffectiveness of conversations is as important as analyzing the effectiveness. Indeed, a conversation killer not only prevents new information from being introduced and new opinions from being expressed, but also projects negative experience to the author himself - the lack of response often leads to disappointment and lower self-evaluation, and further decreases their interest and engagement. Developing a system that identifies potential conversation-killing posts and providing suggestions accordingly could greatly improve the engagement of users into the conversations. For example, if a user intends to expand a discussion, the system could send out a notice before they submit their post if it is more likely to end the discussion. Such a notice, together with possible suggestions, is also plausible in a two-way conversation when one intends to have the other engaged.</p>    <p>We study the novel task of predicting thread-ending posts, which we use as a practical surrogate for &#x201C;conversation killers.&#x201D; Although not all thread-ending posts are killing a conversation, and not all are done in an unintended way, knowing whether or not there will be further replies does help one to avoid becoming a potential conversation killer. We analyze various properties that are potentially predictive to ending a conversation, including text content, conversation background, conversation structure, and sentiments. We find that a standard SVM model is able to distinguish predictive signals from others. To make the best use of these predictive signals, a specially designed recurrent neural network (RNN), named the ConverNet, is employed to model conversations as sequences of posts, as RNNs are known to be good at learning high-level representations from the text. A particular challenge of modeling conversations is the large variance of thread length, which makes standard RNNs ineffective due to their weakness to handle long term dependency and makes standard attention mechanisms ineffective due to the inability of handling sequences with various lengths. To address this challenge, we propose a simple yet powerful attention mechanism that is specifically designed for this task. The attention mechanism not only resolves the issue of lengthy threads, but also provides an effective way to model important context information (e.g., timestamps and authorship) in the conversation.</p>    <p>We conduct large-scale experiments with conversations in two representative domains &#x2013; online forums (e.g., Reddit) and movie dialogs. The results demonstrate great effectiveness and generality of ConverNet, which outperforms a portfolio of strong baselines, including feature-based SVMs and deep learning methods equipped with standard attention mechanisms. By comparing the results of ConverNet and SVM, we present interesting implications of how to engage the participants in a conversation.</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> RELATED WORK</h2>    </div>    </header>    <p>The large quantity of information on the social media platforms exhibits a great potential for scientific research in the domain of data mining and natural language processing. In this paper, we focus on the task of predicting posts that will end a conversation. There are several lines of related work in literature.</p>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Predicting Replies or Retweets of Microblogs</h3>     </div>    </header>    <p>One line of work concerning response prediction on social networks aims to predict the number of replies. This helps content generators, especially advertisers and celebrities, to increase their exposure and maintain their public image. Rowe et al. (2011) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] targeted the prediction of seed posts and their potential numbers of replies in the future. Suh et al. (2010) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] and Hong et al. (2011) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] focused on predicting the number of retweets and analyzed what kinds of Tweets attract more retweets.</p>    <p>Besides predicting the number of replies, there is also work on predicting the binary task of whether a Tweet can get a response (e.g., replies or retweets). Yoav Artzi et al.(2012) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>] tackled this task mainly based on users&#x2019; social network and historical influence. Rowe et al. (2014) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] further investigated more features that might affect user engagement across various social media platforms.</p>    <p>The above-mentioned work mainly focuses on a single post (e.g., a Tweet), which is not as a part of a longer conversation. On the contrary, we identify the thread-ending posts in the context of a multi-party conversation, which has a complex internal structure and richer context information beyond the text content.</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Prediction Tasks in Online Forums</h3>     </div>    </header>    <p>Another line of work comes from the domain of online forums. Some studies aim at predicting thread structures. The work proposed by Wang et al. 2011 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>] approached this task by detecting initiation-response pairs, which are pairs of utterances that the first part sets up an expectation for the second part. Balali et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] followed their work by reconstructing the thread structure, formulating it as a supervised learning task.</p>    <p>There are other types of tasks in online forums that are relevant to our work, such as assessing the quality of posts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>] and categorization of post types (e.g., question, solution, spam) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>].</p>    <p>Although the context and features are relevant, these tasks do not aim at the problem we are solving, which is to identify thread-ending posts in a conversation. A new investigation is needed to identify what types of information could be predictive in our task.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Modeling Conversational Interactions</h3>     </div>    </header>    <p>There has been extensive work on modeling conversational interactions on social media platforms. Honeycutt and Herring [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] analyzed how to make Twitter more usable as a tool for collaboration. Boyd et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] studied how retweeting can be used as a way to converse with others. Ritter et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] used unsupervised conversation models to cluster utterances with similar conversational roles. These tasks focus more on descriptive analyses of conversations, rather than predictions.</p>    <p>Recently, researchers began to study how to automatically generate responses given a conversation history [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. In this work, we do not aim at generating responses. Rather, we attempt to help users understand whether their posts that are about to be submitted would terminate the thread of the conversation.</p>    <p>In summary, our work uses data from different platforms and complement existing studies, but with a unique focus on the effect of a post to the entire conversation. We propose a deep learning model that takes consideration of the content, structure, and context of the entire conversation and predicts the outcome of a single post. Our model and findings could help the aforementioned tasks in literature and facilitate user engagement in online conversations.</p>    </section>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> ConverNet for Thread-ending prediction</h2>    </div>    </header>    <p>In this section, we propose a specifically designed neural network model that uses information of an entire conversation to predict thread-ending posts.</p>    <p>We start with a few definitions. A <strong>post</strong> is a message submitted by a single user, while a <strong>conversation</strong> is a set of posts concerning a focused topic posted by a group of people. A <strong>thread</strong> of a conversation is a subset of posts that are organized as a <em>tree</em> structure through the reply-to relationships. We only focus on threads with at least two posts, as more than one person has to be involved when there is a conversation<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>. A <strong>thread-ending post</strong> is a post in a given thread that will not receive any further replies. In this prediction task, we care more about these thread enders than their counterparts, thus we label a thread-ending post as <em>positive</em> and the others as <em>negative</em>. In this way, the problem is formulated as a binary classification task. Note that we use thread-ending posts as surrogates for conversation killing posts because they are widely available and have explicit labels.</p>    <p>Our model builds upon the insight that recurrent neural networks (RNNs) with a specifically designed attention mechanism have a great advantage in dealing with the internal structure of posts in a thread and that additional context information can further boost the classification performance. The reasons are as follows.</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">Posts in a given thread have strong connections between each other. For explicit tree structure and latent connections behind these posts, RNN models are more suitable compared with standard classification methods, e.g., SVMs. Similar to using RNNs to encode sentences in machine translation tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], we can also use them to model posts in a thread, which can be used for the downstream classification task.<br/></li>    <li id="list2" label="&#x2022;">Compared with traditional models, deep learning models have an advantage in dealing with large-scale data sets, by training on one batch at a time. This is desirable when we are working on a large amount of user-generated content.<br/></li>    <li id="list3" label="&#x2022;">We empirically found that context information, e.g., post time and authorship, could greatly complement text information. Therefore we incorporate them into a unified model.<br/></li>    <li id="list4" label="&#x2022;">Some conversation threads can be very long, containing tens of posts. Attention mechanisms are usually integrated into RNNs to solve the problem of long-term dependency &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. However, because of the fact that different conversations could vary greatly by length, we find that the standard attention mechanism over posts falls short of modeling the longer threads. Therefore, we specifically design an attention mechanism to handle this situation.<br/></li>    </ul>    <p>We propose a recurrent neural network model, called ConverNet, which implements the above design objectives and ideas. In the rest of this section, we will give a brief introduction to standard RNNs, followed by the description of our model.</p>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Background</h3>     </div>    </header>    <p>To facilitate readers with different levels of knowledge, we introduce the standard building blocks used by our model here.</p>    <section id="sec-12">     <p><em>3.1.1 LSTM And BiLSTM.</em> In ConverNet, we use BiLSTM as the basic building blocks of its architecture. LSTM (Long Short-term Memory) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>] units are widely used to build RNN models and BiLSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0011">11</a>] is one of its extensions.</p>     <p>Below we briefly introduce the basic formulation of a LSTM layer. Given <em>c</em>      <sub>0</sub> as the cell state&#x0027;s initial value and <em>x<sub>t</sub>      </em> as the input of time step <em>t</em>, LSTM can be formulated as follows: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ i_{t} = \sigma (W_{ix}x_{t}+W_{im}m_{t-1}), \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ f_{t} = \sigma (W_{fx}x_{t}+W_{fm}m_{t-1}), \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ o_{t} = \sigma (W_{ox}x_{t}+W_{om}m_{t-1}), \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ c_{t} = f_{t} \odot c_{t-1}+i_{t}\odot h(W_{cx}x_{t}+W_{cm}m_{t-1}) \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ m_{t} = o_{t} \odot c_{t} \] </span>       <br/>       </div>      </div> and <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \sigma (x) = \frac{1}{1+e^{-x}}, ~~~~~~h(x) = \frac{1-e^{-2x}}{1+e^{-2x}}, \] </span>       <br/>       </div>      </div> where <em>m<sub>t</sub>      </em> is the output of the LSTM layer at time step t, and the operator &#x2299; denotes the Hadamard product (entry-wise product).</p>     <p>Bidirectional LSTM is an extension of LSTM. It takes the information not only from the forward pass but also from the backward pass. There are two identical independent LSTM kernels in the BiLSTM. At time step <em>t</em>, one takes <em>x<sub>t</sub>      </em> and the other takes <em>x</em>      <sub>       <em>T</em> &#x2212; <em>t</em>      </sub> as its input, where <em>T</em> is the total time steps needed. The outputs of the two kernels are later aligned according to the time ordinal number and concatenated as the final output of the BiLSTM block.</p>     <p>Bidirectional LSTM can alleviate the problem that the LSTM kernel at the time step <em>t</em> is unaware of the following input sequences. However, it still cannot solve the paradox that the longer the input sequences, the more information the LSTM is about to forget. Therefore attention mechanisms are introduced as a remedy.</p>    </section>    <section id="sec-13">     <p><em>3.1.2 Layer Normalization.</em> The <strong>Layer Normalization</strong> technique introduced by Ba et. al [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>] showed that when training LSTM, layer normalization can have a significant influence on both the training speed and the performance. In a standard RNN, the summed inputs in the recurrent layer are computed from the current input <em>x<sup>t</sup>      </em> and previous vector of hidden states <em>h</em>      <sup>       <em>t</em> &#x2212; 1</sup> which are computed as <em>a<sup>t</sup>      </em> = <em>W<sub>hh</sub>h</em>      <sup>       <em>t</em> &#x2212; 1</sup> + <em>W<sub>xh</sub>x<sup>t</sup>      </em>. The layer normalized recurrent layer re-centers and re-scales it using the extra normalization terms: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ h^{t} = f[\frac{g}{\sigma ^ {t}} \odot (a^{t} - \mu ^{t})+b] \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \mu ^{t} = \frac{1}{H}\sum _{i=1}^{H}a_{i}^{t},\sigma ^{t} = \sqrt {\frac{1}{H}\sum _{i=1}^{H}(a_{i}^{t}-\mu ^{t})^{2}}, \] </span>       <br/>       </div>      </div> where <em>W<sub>hh</sub>      </em> are the recurrent hidden to hidden weights and <em>W<sub>xh</sub>      </em> are the bottom up input to hidden weights. &#x2299; is the element-wise multiplication between two vectors. <em>b</em> and <em>g</em> are defined as the bias and gain parameters of the same dimension as <em>h<sup>t</sup>      </em>.</p>     <p>In a layer normalized RNN, the normalization terms make it invariant to re-scaling all of the summed inputs to a layer, thus resulting in much more stable hidden-to-hidden dynamics.</p>    </section>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Context Information for Predicting Thread-ending Posts</h3>     </div>    </header>    <p>As mentioned above, in addition to the text content, we investigate a set of context information of a conversation thread that could contribute to the prediction task. To incorporate them into a unified model, they are implemented as features listed as follows. Generally speaking, there are four types, <em>length information, sentiment information, background information, and replying property</em>.</p>    <p>     <strong>Length information</strong>    </p>    <p>     <em>Post length</em>: the number of words in a given post.</p>    <p>     <em>Thread length</em>: the number of posts in a given thread.</p>    <p>     <strong>Sentiment information</strong>    </p>    <p>     <em>Sentiment</em>: the intensity scores of <em>neutral</em>, <em>positive</em>, and <em>negative</em> sentiments of a given post. In this work we simply adopt the scores provided by the VADER Lexicon&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>].</p>    <p>     <strong>Background information</strong>    </p>    <p>     <em>Conversation background</em>: the context where the conversation happens. For example in the movie-dialog data set, the background is the movie in which the conversation happens.</p>    <p>     <em>Author features</em>: background and historical information of the author of the post. For example, the number of times the author ends a conversation thread in the past.</p>    <p>     <strong>Reply information</strong>    </p>    <p>     <em>Reply structure</em>: basic reply-to information of a thread, consisting of every post&#x0027;s parent post in this thread.</p>    <p>     <em>Post time</em>: the post time interval between each post and its previous one, classified into categories of <em>within an hour</em>, <em>within a day</em>, <em>within a week</em>, and <em>no later than a month</em>.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> ConverNet</h3>     </div>    </header>    <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186013/images/www2018-22-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">A version of the ConverNet designed for the prediction task. Some submodules are numbered and described in details correspondingly in the text.</span>     </div>    </figure>    <p>We now introduce the framework of our proposed neural network model, shown in Figure &#x00A0;<a class="fig" href="#fig1">1</a>. For simplicity, we will focus on illustrating three main components in our network.</p>    <p>     <strong>(1) Input Processing Component</strong>    </p>    <p>The input of ConverNet is a flattened sequence of posts in a thread sorted by their post time, regardless of whether one is replying to the previous one. The replying tree structure is handled along with other context information.</p>    <p>For the content information, we first use an embedding layer to get <em>N</em> (the total number of posts in a thread) embedding vectors <span class="inline-equation"><span class="tex">$C_{p}^{i}$</span>     </span> (which represents the embedding for the <em>i<sub>th</sub>     </em> word of the <em>p<sub>th</sub>     </em> post) for each word in a post from a given thread. After that we are going to generate a post embedding vector based on all its words. <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ x_{t} = [\frac{\sum _{i=1}^{N} C_{t}^{i}}{N};\overline{S_{p}}], \] </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$\overline{S_{p}}$</span>     </span> is the corresponding context information for the <em>p<sub>th</sub>     </em> post in a given thread. This is done through an average pooling layer on top of the word embedding layer in ConverNet.</p>    <p>All types of context information are merged with the pooling result by a simple concatenation, composing the input data for LNBiLSTM layer (BiLSTM with layer normalization).</p>    <p>     <strong>(2) Encoding Component</strong>    </p>    <p>The encoding component consists of a BiLSTM with the layer normalization technique and our proposed Dwdl attention layer, which will be described in details in the next subsection. Firstly, the LNBiLSTM encodes the input <em>x<sub>i</sub>     </em> in the following way, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ i_t = \sigma _i(LN(x_t W_{xi}; \alpha _{xi}, \beta _{xi}) + LN(h_{t-1} W_{hi}; \alpha _{hi}, \beta _{hi}) + w_{ci} \odot c_{t-1} + b_i) \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ f_t = \sigma _f(LN(x_t W_{xf}; \alpha _{xf}, \beta _{xf}) + LN(h_{t-1} W_{hf}; \alpha _{hf}, \beta _{hf}) + w_{cf} \odot c_{t-1} + b_f) \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ c_t = f_t \odot c_{t - 1} + i_t \odot \sigma _c(LN(x_t W_{xc}; \alpha _{xc}, \beta _{xc}) + h_{t-1} W_{hc}; \alpha _{hc}, \beta _{hc}) + b_c) \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ o_t = \sigma _o(LN(x_t W_{xo}; \alpha _{xo}, \beta _{xo}) + LN(h_{t-1} W_{ho}; \alpha _{ho}, \beta _{ho}) + w_{co} \odot c_t + b_o) \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ h_t = o_t \odot \sigma _h(LN(c_t; \alpha _c, \beta _c)), \] </span>       <br/>      </div>     </div> where <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ LN(z;\alpha , \beta) = \frac{(z-\mu)}{\sigma } \odot \alpha + \beta . \] </span>       <br/>      </div>     </div>    </p>    <p>The parameters are as follows:</p>    <p>For the input gate: <em>i<sub>t</sub>     </em>: <em>W<sub>xi</sub>     </em>, <em>W<sub>hi</sub>     </em>, <em>w<sub>ci</sub>     </em>, <em>b<sub>i</sub>     </em>, and <em>&#x03C3;<sub>i</sub>     </em>.</p>    <p>For the forget gate: <em>f<sub>t</sub>     </em>: <em>W<sub>xf</sub>     </em>, <em>W<sub>hf</sub>     </em>, <em>w<sub>cf</sub>     </em>, <em>b<sub>f</sub>     </em>, and <em>&#x03C3;<sub>f</sub>     </em>.</p>    <p>For the cell computation: <em>c<sub>t</sub>     </em>: <em>W<sub>xc</sub>     </em>, <em>W<sub>hc</sub>     </em>, <em>b<sub>c</sub>     </em>, and <em>&#x03C3;<sub>c</sub>     </em>.</p>    <p>For the output gate: <em>o<sub>t</sub>     </em>: <em>W<sub>xo</sub>     </em>, <em>W<sub>ho</sub>     </em>, <em>w<sub>co</sub>     </em>, <em>b<sub>o</sub>     </em>, and <em>&#x03C3;<sub>o</sub>     </em>.</p>    <p>The post to be predicted is positioned at the end of the input sequence. However, instead of using only the final output of the LNBiLSTM layer, <em>h<sub>T</sub>     </em>, we make full use of the outputs {<em>h<sub>t</sub>     </em>} from all time steps. The attention layer further encodes the output sequence [<em>h<sub>t</sub>     </em>] of LNBiLSTM into a vector that has the same dimensionality as the hidden units in LNBiLSTM, where [<em>h<sub>t</sub>     </em>] is a matrix <strong>vertically</strong> stacked by the LNBiLSTM output <em>h<sub>t</sub>     </em> at every time step.</p>    <p>In the end, there is a merge operation to combine the result of the attention layer and the LNBiLSTM&#x0027;s last output vector: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ z_{0} = tanh([c(s, H); h_{T}]\cdot W). \] </span>       <br/>      </div>     </div> In this way, theoretically, the performance of adding the attention layer will not be any worse than a single LNBiLSTM kernel. As for the merge operation, we implement it using <em>concatenation</em>.</p>    <p>     <strong>(3) Decoding Component</strong>    </p>    <p>After getting the result from the encoding component, the decoding component does the classification. It consists of several MLP layers, followed by the final layer that has only one output unit, predicting whether the given post is a conversation killer or not: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ z_{i} = Relu(z_{i-1}\cdot W+b) \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \hat{y} = Sigmoid(z_{n}\cdot W+b). \] </span>       <br/>      </div>     </div>    </p>    <p>All MLP layers are followed by batch normalization layers[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. All layers are activated with ReLU [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>], with the exception of the last one, which is activated with the <em>Sigmoid</em> function. This guarantees that the final output is either 0 or 1.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Dwdl Attention Layer</h3>     </div>    </header>    <p>The motivation to use an attention layer is to take full usage of the information generated by the LSTM kernel. However, one difficulty of our task is that the length of threads ranges from a considerable scale. The standard attention mechanism learns attention weights uniformly over posts. That is, the <em>i</em>-th post in different threads always receive the same attention. Unfortunately, this assumption does not hold in reality, as the learned weights do not fit universally to posts in threads of various lengths.</p>    <p>As a solution, one may want to apply different attention weights for each thread. This leads to another problem by introducing a large number of parameters to be learned.</p>    <p>To resolve both issues, we propose an attention mechanism that applies <strong>d</strong>ifferent attention <strong>w</strong>eights for <strong>d</strong>ifferent <strong>l</strong>engths of input <strong>(Dwdl)</strong>, while weights are shared among threads of identical length. In this way, the attention mechanism outputs the result <em>c</em>(<em>s</em>, <em>H</em>) as <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ c(s, H) = \frac{\sum _{k=1}^{s}[exp(w_{ks}) \cdot h_{k}]}{\sum _{k=1}^{s}exp(w_{ks})}, \] </span>       <br/>      </div>     </div> where <em>H</em> is the output sequence from the LNBiLSTM layer with length <em>s</em>. And <em>W</em> is the attention weight matrix that will be learned.</p>    <p>Despite its simplicity, Dwdl attention not only solves the problem of learning thread representations with a large variance in length but also avoids the introduction of too many parameters. The design of the Dwdl attention layer is a major innovation of the proposed ConverNet model in the context of deep learning architectures.</p>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Loss function</h3>     </div>    </header>    <p>We use binary cross entropy loss to train our model. The objective is to minimize the loss function: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \mathcal {L} = - \sum _{i} [g_{i} log(\hat{y}_{i}) + (1 - g_{i}) log(1 - \hat{y}_{i})], \] </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$\hat{y}_{i}$</span>     </span> is the predicted probability of the <em>i</em>-th post ending a conversation, and <em>g<sub>i</sub>     </em> is the ground-truth label.</p>    </section>   </section>   <section id="sec-18">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> EXPERIMENT SETUP</h2>    </div>    </header>    <p>We empirically compare our model with various alternative approaches on two public datasets. The following experiments are aimed to demonstrate the effectiveness of ConverNet in general and the effectiveness of different kinds of content and context information in predicting thread-ending posts.</p>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Data Sets</h3>     </div>    </header>    <p>We accomplish this task on two representative datasets. One contains threads of Reddit posts, which is extracted from Reddit.com, one of the largest online forums that cover a variety of topics. This dataset is representative for online conversations. The other is a collection of conversations extracted from movie scripts. We include this dataset because movie dialogs are closer to offline, everyday conversations, which would be a good reference for understanding the properties of online conversations. The statistics of these datasets are listed in Table&#x00A0;<a class="tbl" href="#tab1">1</a>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Statistics of each data set.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Properties</th>       <th style="text-align:center;">Reddit-Threads</th>       <th>Movie-Dialogs</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Threads</td>       <td style="text-align:center;">83,097</td>       <td>100,000</td>       </tr>       <tr>       <td style="text-align:center;">Vocabulary</td>       <td style="text-align:center;">29,729</td>       <td>107,354</td>       </tr>       <tr>       <td style="text-align:center;">Max post len.</td>       <td style="text-align:center;">673</td>       <td>2689</td>       </tr>       <tr>       <td style="text-align:center;">Avg. post len.</td>       <td style="text-align:center;">13.02 words</td>       <td>43.83 words</td>       </tr>       <tr>       <td style="text-align:center;"># train threads</td>       <td style="text-align:center;">63,097</td>       <td>80,000</td>       </tr>       <tr>       <td style="text-align:center;"># val threads</td>       <td style="text-align:center;">10,000</td>       <td>10,000</td>       </tr>       <tr>       <td style="text-align:center;"># test threads</td>       <td style="text-align:center;">10,000</td>       <td>10,000</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Reddit-Threads Data Set</strong>    </p>    <p>     <em>Source.</em> This data set is generated based on the public Reddit-Comments data set provided by Reddit user Stuck_In_the_Matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. The original data set consists of all of the posts and comments available on Reddit since early 2006. In our experiment, we focus on the threads in the political domain (from 2007.8 &#x223C; 2009.8), a major topic of interest on Reddit.</p>    <p>     <em>Processing.</em> By utilizing the <em>parent</em> post information provided by Reddit, we recover the tree structure of each thread, where leaf nodes are considered as thread-ending posts. As mentioned before, we only focus on threads with more than one post &#x2013; a thread with only one post is not a conversation. The length distribution of threads is shown in Figure&#x00A0;<a class="fig" href="#fig2">2</a>, which generally follows the power law distribution.</p>    <p>     <em>Prediction Task.</em> With these tree-structured threads, our prediction task is equivalent to predicting whether a given node is a leaf node or not. Note that in a Reddit thread, there might be more than two actors (authors) engaged in a conversation.</p>    <p>     <strong>Movie-Dialogs Corpus</strong>    </p>    <p>     <em>Sources.</em> We use the Cornell Movie-Dialogs Corpus, which is widely used for text generation tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. It contains more everyday words and involves in total 617 movies with 10,292 movie characters. Compared with the Reddit-Threads data set, the posts (or sentences) are simpler, shorter, and more normal. A major difference is that every dialog happens between two speakers, so the number of users in threads is a constant. The length distribution of movie dialog threads is shown in Figure&#x00A0;<a class="fig" href="#fig3">3</a>.</p>    <p>     <em>Prediction Task.</em> We treat every movie dialog also as a chatting thread with two participators. These chatting threads are organized as a sequence of &#x201C;posts&#x201D; (sentences) instead of a tree structure. As a result, only the last post (sentence) ends the conversation.</p>    <p>     <strong>Sampling and Preprocessing</strong>    </p>    <p>In both datasets, we randomly sample one post (sentence) from each thread (dialog) and predict whether it is a thread-ending post. We call these posts <strong>target posts</strong>. Since the information after the target post will reveal the ground truth for the prediction task, all posts after the target post must be omitted before feeding into the model. The Reddit-threads data set are split into training, validation, and test set according to submission time of the first post in each thread. The first 80,000 threads are assigned to the training set and the rest 20,000 are equally separated into validation and test set. For the movie data set, we do a random permutation of all the threads and assign them into training/validation/test sets as Table&#x00A0;<a class="tbl" href="#tab1">1</a> shows. <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186013/images/www2018-22-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">The length distribution of threads in Reddit-Threads data set.</span>      </div>     </figure>     <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186013/images/www2018-22-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">The length distribution of threads in Movie-Dialogs data set.</span>      </div>     </figure>    </p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Metrics</h3>     </div>    </header>    <p>Since the label distribution of our binary classification task is skewed, we adopt metrics in addition to the commonly used <strong>accuracy</strong>, including <em>AUC</em> and <em>MAP</em> (mean-average precision), because in reality, it is more important to make sure the top-ranked posts are truly thread-ending posts (so that notices can be sent). To achieve high scores in these two additional metrics, both precision and recall are important.</p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Competing methods</h3>     </div>    </header>    <p>We compare ConverNet with a handful of baseline methods in two categories: conventional machine learning methods and alternative deep learning methods.</p>    <section id="sec-22">     <p><em>4.3.1 Conventional Baselines.</em>      <strong>SVM+/-[features].</strong> As a conventional method that has demonstrated superior performance in many kinds of classification tasks, SVM can sometimes achieve comparable performance to the deep learning methods. Besides, it plays a key role in deciding which kind of hand-crafted features are helpful in our prediction task. Therefore, we include all kinds of features potentially related to this problem across domains and use a linear kernel SVM implemented by <em>sklearn</em> for classification.</p>     <p>In addition to all the features mentioned in the method section, we include additional features extracted from text content. They include word unigrams, bigrams, trigrams, and post embeddings. Specifically, post embeddings are the averages of word vectors in a post. The word vectors are generated by word2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>] skip-gram and CBOW models. We concatenate all the features from all the posts in a thread. When a method is named <strong>SVM+[features]</strong>, it refers to an SVM model trained using only the corresponding set of features. When a method is named <strong>SVM-[features]</strong>, it refers to an SVM model that uses all but the corresponding set of features.</p>    </section>    <section id="sec-23">     <p><em>4.3.2 Deep learning baselines.</em>      <strong>BiLSTM</strong>, <strong>LNBiLSTM</strong>, <strong>Parallel LNBiLSTM</strong>. We use bi-directional LSTM (<strong>BiLSTM</strong>) that is widely used for classification tasks. Considering the recent success of layer normalization, we include the bi-directional LSTM with layer normalization (<strong>LNBiLSTM</strong>) as a baseline. We also stack multiple LNBiLSTM to learn deeper representations (<strong>Stacked LNBiLSTM</strong>).</p>     <p>      <strong>LNBiLSTM+features</strong> and <strong>LNBiLSTM+features+SA</strong>. One major innovation of ConverNet is the newly designed Dwdl attention mechanism (see Section 3.4) to handle context information in a thread. For comparison, we also add context information to LNBiLSTM using several ways. <strong>LNBiLSTM+features</strong> simply concatenate the representation output by LNBiLSTM and features extracted from context information. <strong>LNBiLSTM+features+SA</strong> applies a standard attention over the output of the hidden state by LNBiLSTM. For all LNBiLSTM-related models, LSTMs are stacked using horizontal or vertical ways. But the number of stacked layers can vary from model to model. In this task, we use 2 &#x223C; 5 layers.</p>    </section>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Training Details</h3>     </div>    </header>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Performance of competing methods: LNBiLSTM+All features+Dwdl attention achieves top performance.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;"/>       <th colspan="3" style="text-align:center;">Reddit-Threads Data Set<hr/>       </th>       <th colspan="3" style="text-align:center;">Movie-Dialogs Data Set<hr/>       </th>       </tr>       <tr>       <th style="text-align:center;">Method</th>       <th style="text-align:center;">Accuracy</th>       <th style="text-align:center;">AUC</th>       <th style="text-align:center;">MAP</th>       <th style="text-align:center;">Accuracy</th>       <th style="text-align:center;">AUC</th>       <th>MAP</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">SVM-Text content(Embedding, N-grams)</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$75.95\scriptstyle \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$81.26\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$68.84\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$74.57\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$83.12\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$64.97\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">SVM-Lengths info</td>       <td style="text-align:center;">76.45</td>       <td style="text-align:center;">83.05</td>       <td style="text-align:center;">72.31</td>       <td style="text-align:center;">75.70</td>       <td style="text-align:center;">84.67</td>       <td>69.50</td>       </tr>       <tr>       <td style="text-align:center;">SVM-Background info</td>       <td style="text-align:center;"> &#x2212;</td>       <td style="text-align:center;"> &#x2212;</td>       <td style="text-align:center;"> &#x2212;</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$75.43\scriptstyle \diamond$</span>        </span>       </td>       <td style="text-align:center;">84.56</td>       <td>        <span class="inline-equation"><span class="tex">$69.09\scriptstyle \diamond$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">SVM-Post time</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$75.55\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$81.36\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$69.85\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;"> &#x2212;</td>       <td style="text-align:center;"> &#x2212;</td>       <td>&#x2212;</td>       </tr>       <tr>       <td style="text-align:center;">SVM-Replying structures</td>       <td style="text-align:center;">76.15</td>       <td style="text-align:center;">83.13</td>       <td style="text-align:center;">72.67</td>       <td style="text-align:center;"> &#x2212;</td>       <td style="text-align:center;"> &#x2212;</td>       <td>&#x2212;</td>       </tr>       <tr>       <td style="text-align:center;">SVM-Sentiment</td>       <td style="text-align:center;">76.31</td>       <td style="text-align:center;">83.06</td>       <td style="text-align:center;">72.84</td>       <td style="text-align:center;">75.60</td>       <td style="text-align:center;">84.59</td>       <td>69.59</td>       </tr>       <tr>       <td style="text-align:center;">SVM+All features</td>       <td style="text-align:center;">76.39</td>       <td style="text-align:center;">83.30</td>       <td style="text-align:center;">72.60</td>       <td style="text-align:center;">75.84</td>       <td style="text-align:center;">84.67</td>       <td>69.63</td>       </tr>       <tr>       <td style="text-align:center;">BiLSTM+Text content (only the target post)</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$60.80\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$64.36\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$58.20\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$61.62\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$61.40\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$50.30\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">BiLSTM+Text content</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$76.02\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$83.42\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">73.33</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$76.26\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$85.22\scriptstyle \star \star \star$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$70.63\scriptstyle \star \star \star$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">LNBiLSTM+Text content</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$76.59\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$84.22\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$74.07\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$76.75\scriptstyle \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$85.55\scriptstyle \star \star \star$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$70.85\scriptstyle \star \star \star$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">Stacked LNBiLSTM+Text content</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$76.42\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$84.46\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$74.44\scriptstyle \star \star \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$76.98\scriptstyle \star$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$85.87\scriptstyle \star \star$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$71.67\scriptstyle \star \star$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">LNBiLSTM+All features</td>       <td style="text-align:center;">78.05</td>       <td style="text-align:center;">85.91</td>       <td style="text-align:center;">77.39</td>       <td style="text-align:center;">77.51</td>       <td style="text-align:center;">86.47</td>       <td>72.95</td>       </tr>       <tr>       <td style="text-align:center;">LNBiLSTM+All features+Standard attention</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$78.05\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$85.97\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$77.70\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$77.45\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$86.32\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$72.63\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       </tr>       <tr>       <td style="text-align:center;">        <strong>ConverNet</strong>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$78.27\scriptstyle \diamond \diamond \diamond$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$86.22\scriptstyle ^{\star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$78.21\scriptstyle ^{\star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$78.04\scriptstyle ^{\star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$86.82\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       <td>        <span class="inline-equation"><span class="tex">$73.76\scriptstyle ^{\star \star \star }_{\diamond \diamond \diamond }$</span>        </span>       </td>       </tr>      </tbody>      <tfoot>       <tr>       <td colspan="7">All numbers are in percentage. <span class="inline-equation"><span class="tex">$\scriptstyle \star (\star \star ,\star \star \star)$</span>        </span> indicates that one method is statistically significantly better or worse than <em>LNBiLSTM+All features+Standard attention</em> (which is in general the best configuration among all non-ConverNet, LSTM-related models) according to Random Permutation Test [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0008">8</a>] at the significance level of 0.05(0.01,0.001). And <span class="inline-equation"><span class="tex">$\scriptstyle \diamond (\diamond \diamond ,\diamond \diamond \diamond)$</span>        </span> indicates one method is statistically significantly better or worse than <em>SVM+All features</em> at also the significance level of 0.05(0.01,0.001). Results are indicated by &#x201C;-&#x201D; if a feature category is not available for a particular data set.</td>       </tr>      </tfoot>     </table>    </div>    <p>All hyper-parameters are tuned to obtain the best performance of AUC score on the validation set. For LSTM-related methods, the candidate word embedding sizes are set as {16, 32, 64, 128, 256} and the candidate numbers of hidden/cell units in the LSTM-related layer are {16, 32, 64, 128}. The embedding size for context information is selected from {2, 4, 8, 16, 32, 64}. The initial learning rate is selected from {10<sup>&#x2212; 1</sup>, 10<sup>&#x2212; 2</sup>, ..., 10<sup>&#x2212; 5</sup>}. For SVM, the candidate embedding size is from {50, 100, 200, 500} and the relaxing parameter C for SVM model is chosen from {10<sup>3</sup>, 10<sup>2</sup>, ..., 10<sup>&#x2212; 3</sup>}.</p>    <p>We initialize parameters in neural networks using a zero-mean Gaussian with standard deviation selected from {0.01, 0.05, 0.1, 0.2}. For parallel network models, the number of stacked layers is selected from {2, 3,..., 5}. All deep learning models are optimized by RmsProp&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. We stop training when the performance converges on the validation set.</p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> experiment results</h2>    </div>    </header>    <section id="sec-26">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Overall Performance</h3>     </div>    </header>    <p>The overall performance of all competing methods are shown in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. The proposed method ConverNet outperforms all competing methods in all three metrics, <em>AUC</em>, <em>Accuracy</em>, and <em>MAP</em>. The improvements are all statistically significant except for one case (accuracy on Reddit-Threads, where LNBiLSTM+All features already performs very well). This empirically confirms that a well designed deep learning model can achieve the best result in predicting thread-ending posts in online conversations.</p>    <p>Comparing different versions of SVM models, the content of the thread and the target post appears to be the most important. When content features are included, there is a 5% improvement in MAP on Reddit dataset (0.688 -> 0.726) and 7% improvement on the Movie-dialog dataset (0.650 -> 0.696). Certain context information is also useful on top of textual features, especially the time of the posts in Reddit threads (0.699 -> 0.726). A more detailed comparison of the features is deferred to Section 6. A consistent conclusion can be made by comparing ConverNet with the best deep learning baseline that is purely based on content information. Overall, comparing ConverNet to the best performing SVM baseline, there is another 8% improvement on Reddit-Threads (0.726 -> 0.782) and 6% improvement on Movie-Dialogs (0.696 -> 0.738).</p>    <p>When using the standard attention layer to handle context information, the deep learning model does not perform better than the model based on content only, sometimes even less effective. This is possibly due to the large variation of the thread length in both data sets. Threads with fewer posts could have a totally different attention distribution from those with more posts. With the newly designed Dwrl attention layer, ConverNet is able to significantly outperform the content based models.</p>    <p>The experiment results of using different LSTM-related kernels also prove that BiLSTM with layer normalization can make a significant improvement, while repeatedly stacking this layer can also slightly enhance the prediction result.</p>    <p>It is interesting to note that a deep learning model learned only based on the content of the target post (instead of the whole thread), <strong>BiLSTM+Text content (only the target post)</strong>, performs significantly worse even compared to the same model that considers the content of both the target post and other posts in the thread (with no context information). This assures that information in the whole thread is important for predicting whether a post will be replied to, which again distinguishes our problem setting with existing work that predicts retweets. Indeed, whether a post will carry on a conversation highly depends on whether what it says is relevant to the topic of the discussion.</p>    </section>    <section id="sec-27">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Training Time Analysis</h3>     </div>    </header>    <p>In order to measure the training speed of each model, we train all deep learning methods on a server with a single TITAN X GPU (12 GB GDDR5X, Graphics Card Power of 250 W).</p>    <p>For all deep learning methods, the numbers of epochs required for training are all quite close, which is around 15 epochs averaged over all datasets.</p>    <p>Comparing the training time per epoch, stacking more LNBiLSTM will decrease the time efficiency. And the basic BiLSTM model takes around 55 seconds per epoch. Using layer normalization will slightly reduce training time (by 2 to 3 seconds). Adding attention layer increases the training time. Our ConverNet model consumes around 60 seconds per epoch. The total training time of ConverNet model is around 900 seconds with about 18 training epochs.</p>    </section>   </section>   <section id="sec-28">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Discussion</h2>    </div>    </header>    <p>The overall performances of the competing algorithms have demonstrated that thread-ending posts are predictable through integrating content and rich context information, and through a carefully designed deep learning architecture. Beyond the numbers, we are also interested in the implications of the experiment on how to avoid being a conversation killer. We approach this by a more detailed analysis of the features and interpretation of the model results.</p>    <section id="sec-29">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Feature Analysis</h3>     </div>    </header>    <p>Based on the features extracted for SVM, we first conduct a simple correlation analysis to understand what features of content and context are positively or negatively correlated with the outcome, whether a post in a thread-ending post or not. Correlations of some selected features (measured in Pearson&#x0027;s coefficient) and the outcome label are shown in Table&#x00A0;<a class="tbl" href="#tab3">3</a>.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Correlation of features to ending a thread.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;"/>       <th style="text-align:center;">Reddit-Threads</th>       <th style="text-align:center;">Movie-Dialogs</th>       </tr>       <tr>       <th style="text-align:center;">Features</th>       <th style="text-align:center;">Correlation</th>       <th>Correlation</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Word Embeddings(-)</td>       <td colspan="2" style="text-align:center;">&#x2018;Mr.&#x2019;,&#x2018;Mrs.&#x2019;,&#x2018;like&#x2019;,&#x2018;talked&#x2019;,&#x2018;heard&#x2019;,&#x2018;seen&#x2019;,&#x2018;care&#x2019;<hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">Word Embeddings(+)</td>       <td colspan="2" style="text-align:center;">&#x2018;Wait&#x2019;,&#x2018;Christ&#x2019;,&#x2018;ass&#x2019;,&#x2018;but&#x2019;,&#x2018;YOU&#x2019;,&#x2018;THE&#x2019;<hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">Length of Thread</td>       <td style="text-align:center;">+</td>       <td>+</td>       </tr>       <tr>       <td style="text-align:center;">Length of Post</td>       <td style="text-align:center;">-</td>       <td>+</td>       </tr>       <tr>       <td style="text-align:center;">Post Time Difference</td>       <td style="text-align:center;">+</td>       <td>x</td>       </tr>       <tr>       <td style="text-align:center;">Neutral Sentiment Score</td>       <td style="text-align:center;">x</td>       <td>-</td>       </tr>       <tr>       <td style="text-align:center;">Positive Sentiment Score</td>       <td style="text-align:center;">+</td>       <td>+</td>       </tr>       <tr>       <td style="text-align:center;">Negative Sentiment Score</td>       <td style="text-align:center;">-</td>       <td>+</td>       </tr>      </tbody>      <tfoot>       <tr>       <td colspan="3">For a feature, we report the sign of Pearson&#x0027;s coefficient to class label. The mark &#x201C;x&#x201D; means that feature does not exist in the dataset, &#x201C;/&#x201D; means the coefficient is trivial, + and - represent significantly (p < 0.01) positive (likely to end thread) or negative correlation (unlikely to end thread).</td>       </tr>      </tfoot>     </table>    </div>    <p>Embeddings of certain words are identified as significantly correlated to thread-ending posts, either positively or negatively (<em>positively</em> means higher probabilities to cause endings, <em>negatively</em> means otherwise). We see that the most correlated words are likely to be sentimental or particular expressions instead of topical words. For example, polite addresses like &#x2018;Mr.&#x2019; and &#x2018;Mrs&#x2019; will more successfully lead to further communications. Key Words indicating inclines of sharing experiences like &#x2018;seen&#x2019;, &#x2018;heard&#x2019; will also draw the attention of other users. However, insulting words like &#x2018;ass&#x2019; or words with an intense sentiment like &#x2018;YOU&#x2019; and &#x2018;THE&#x2019; will be more likely to end a conversation.</p>    <p>Length of a thread is positively correlated with thread-ending posts, indicating that being the first few posts in a conversation is more likely to be replied to, and when the conversations are already lengthy, it is less likely to prolong the discussion. Post time (time since the previous post) is also positively correlated to the outcome, indicating that the longer one waits to reply to a thread, the more likely that they will never be replied to.</p>    <p>These findings are quite intuitive and pretty much consistent on two datasets. There are other features that are more intriguing. For example, in online conversations (Reddit threads), the more words a post has, the more likely that it brings in a reply. In everyday conversations (movie dialogs), a long speech does not necessarily bring in responses. Saying too much might result in a silence. Perhaps reading (a forum post) is indeed more efficient than listening? In movie dialogs, a post with positive sentiment is more likely to end a conversation and so it is in the Reddit threads. Our general intuition is that being more polite will be easier to get people respond to you. But in movie dialogs, there are lots of conventional conversation-ending words like &#x2018;Thanks&#x2019;, &#x2018;Nice!&#x2019;, and &#x2018;Great&#x2019;, which take a big part in the data set. In online conversations (Reddit threads), many threads are asking questions. When a satisfactory answer is provided, these threads are usually ended with a short post of a simple appreciation. Therefore, a positive sentiment is also linked to thread-ending. Negative posts in movie dialogs are more likely to end conversations which is consistent with our intuition. However, this correlation is the opposite in Reddit threads. Considering the unique nature of online forums and the topic (politics), it is perhaps not too surprising. On one hand, political discussions in online forums are known to be intense and controversial, where an attacking or uncivil (usually with extremely negative sentiment) post is likely to bring another negative reply [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. Apparently, in these two cases, ending a thread politely is not a bad thing, and prolonging an uncivil discussion is much more undesirable. This does intrigue us to rethink about the difference between thread-ending posts and conversation killers, and the difference between conversation killers and killers of a &#x201C;good conversation.&#x201D;</p>    <p>Given the correlation analysis of individual features, we are also interested to know how the features work together (see Table&#x00A0;<a class="tbl" href="#tab2">2</a>). Because different types of content and context features can be highly correlated, when they are fed into SVM, the signs and coefficients of the features may or may not be consistent with the correlation results.</p>    <p>In the Movie-Dialog data set, apart from the most predictive content features, movie background features (such as the theme of the movie) also contribute significantly. On the contrary, sentiment and length features are less useful on top of content features, the reason for which may be that such information might have already been captured by the content features. The background feature represents the circumstances under which conversations happen. One may also utilize such background information for analyzing online forums if it is available (e.g., politics vs. entertainment).</p>    <p>In the Reddit-Threads data set, post time brings in a significant improvement, while replying structures and sentiment scores can only slightly improve on top of other features. This is because sentiment scores and replying structures may have become redundant when other kinds of features are already used. For example, SVMs might have already learned the sentiment features from the text content. However, context information like post time is relatively orthogonal to the other types of features, thus bringing in a more noticeable improvement.</p>    </section>    <section id="sec-30">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> What ConverNet Learns</h3>     </div>    </header>    <p>To gain a better understanding of the behavior of ConverNet, we manually analyzed cases where ConverNet performs better than SVMs. These cases can be put into the following categories.</p>    <p>     <em>Posts with an intense tone.</em>    </p>    <p>Empirically speaking, a post with an intense tone tends to create a serious chatting atmosphere, where other conversation participants may get nervous or shocked to say anything, thus increasing the possibility to end a conversation. It is hard for SVMs to identify these cases even with the help of sentiment analysis, while ConverNet can have a significantly higher chance to give the right prediction. An example is shown in Figure&#x00A0;<a class="fig" href="#fig4">4</a>. <em>Sit</em> and <em>down</em> are both everyday words. Sentiment lexicons might fail to detect any sentiments, but we can sense a commanding tone of the expression, especially for the last utterance. Such subtlety, which is context dependent, can be detected by neural network&#x0027;s recurrent mechanism and the attention mechanism. <figure id="fig4">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186013/images/www2018-22-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">ConverNet performs better than SVM on thread-ending posts with an intense tone.</span>      </div>     </figure>    </p>    <p>     <em>Posts with an inquiry tone.</em>    </p>    <p>If someone is asking for other people&#x0027;s opinions or proposing questions, the conversation may have a much higher possibility to carry on. When these posts are the targets, ConverNet is more likely to detect these questions and give a correct prediction. Some examples are shown in the Figure <a class="fig" href="#fig5">5</a>. <figure id="fig5">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186013/images/www2018-22-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">ConverNet performs better than SVM on thread-ending posts with an asking tone.</span>      </div>     </figure>    </p>    <p>     <em>Posts with a vague tone.</em>    </p>    <p>We find that a large number of thread-ending posts carry a vague tone, with examples listed in the Figure <a class="fig" href="#fig6">6</a>. These posts convey ambiguous meanings, giving no direct response to the questions raised by others before. This kind of vague tone can make other participants think that the speaker is not interested, or is not giving enough attention, which in turn terminates the conversation. ConverNet&#x0027;s recurrent mechanism and attention mechanism can help better extract these ambiguous words from given comments, thus yielding a higher precision. <figure id="fig6">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186013/images/www2018-22-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">ConverNet performs better than SVM on thread-ending posts with a vague tone.</span>      </div>     </figure>    </p>    <p>Based on the analysis of features and the interpretation of the models, if advice have to be given to avoid being a conversation killer, some general implications may be: keep to the point (content), act fast (post time and thread length), be elaborative (post length), be positive (sentiment), and pay attention to your tone (deep patterns in language).</p>    </section>   </section>   <section id="sec-31">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion</h2>    </div>    </header>    <p>How to improve the quality of conversations and engage user participation in online communities is a critical problem that may be relevant to every Internet user. Our work focuses on a novel data mining problem, to identify what type of posts are likely to end a thread of conversation online. We find that while a standard SVM can effectively identify useful signals from both content and context information that are predictive for thread-ending posts, a carefully designed recurrent neural network model, ConverNet, is able to maximize the predictive power of these signals. ConverNet outperforms all the competing baselines in data sets from two representative domains. The results of ConverNet also provides practical implications for improving the quality of online conversations. Our work opens up interesting directions towards understanding the quality of online conversations and increasing user engagement, and towards a deeper understanding of the functionality of language in a conversation.</p>    <section id="sec-32">    <header>     <div class="title-info">      <h3>Acknowledgment</h3>     </div>    </header>    <p>This work was supported in part by the National Science Foundation under grant numbers 1054199, 1633370, 1131500, and 1620319, and in part by the Chinese 973 program (2015CB352302), NSFC (U1611461), and key program of Zhejiang Province (2015C01027).</p>    </section>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012. Predicting Responses to Microblog Posts. In <em>      <em>Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 3-8, 2012, Montr&#x00E9;al, Canada</em>     </em>. The Association for Computational Linguistics, 602&#x2013;606. <a href="http://www.aclweb.org/anthology/N12-1074" target="_blank">http://www.aclweb.org/anthology/N12-1074</a></li>    <li id="BibPLXBIB0002" label="[2]">Lei&#x00A0;Jimmy Ba, Ryan Kiros, and Geoffrey&#x00A0;E. Hinton. 2016. Layer Normalization. <em>      <em>CoRR</em>     </em>abs/1607.06450(2016). <a href="http://arxiv.org/abs/1607.06450" target="_blank">http://arxiv.org/abs/1607.06450</a></li>    <li id="BibPLXBIB0003" label="[3]">A Balali, H Faili, and M Asadpour. 2014. A supervised approach to predict the hierarchical structure of conversation threads for comments. <em>      <em>The Scientific World Journal</em>     </em>2014 (2014).</li>    <li id="BibPLXBIB0004" label="[4]">Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter. In <em>      <em>43rd Hawaii International International Conference on Systems Science (HICSS-43 2010), Proceedings, 5-8 January 2010, Koloa, Kauai, HI, USA</em>     </em>. IEEE Computer Society, 1&#x2013;10. <a class="link-inline force-break" href="https://doi.org/10.1109/HICSS.2010.412"      target="_blank">https://doi.org/10.1109/HICSS.2010.412</a></li>    <li id="BibPLXBIB0005" label="[5]">Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2017. Anyone can become a troll: Causes of trolling behavior in online discussions. <em>      <em>arXiv preprint arXiv:1702.01119</em>     </em>(2017).</li>    <li id="BibPLXBIB0006" label="[6]">Kyunghyun Cho, Bart van Merrienboer, &#x00C7;aglar G&#x00FC;l&#x00E7;ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In <em>      <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>     </em>, Alessandro Moschitti, Bo&#x00A0;Pang, and Walter Daelemans (Eds.). ACL, 1724&#x2013;1734. <a href="http://aclweb.org/anthology/D/D14/D14-1179.pdf" target="_blank">http://aclweb.org/anthology/D/D14/D14-1179.pdf</a></li>    <li id="BibPLXBIB0007" label="[7]">Cristian Danescu-Niculescu-Mizil and Lillian Lee. 2011. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.. In <em>      <em>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011</em>     </em>.</li>    <li id="BibPLXBIB0008" label="[8]">RA Fisher. 1935. The design of experiments. (1935).</li>    <li id="BibPLXBIB0009" label="[9]">Anindya Ghose and Panagiotis&#x00A0;G. Ipeirotis. 2011. Estimating the Helpfulness and Economic Impact of Product Reviews: Mining Text and Reviewer Characteristics. <em>      <em>IEEE Trans. Knowl. Data Eng.</em>     </em>23, 10 (2011), 1498&#x2013;1512. <a class="link-inline force-break" href="https://doi.org/10.1109/TKDE.2010.188"      target="_blank">https://doi.org/10.1109/TKDE.2010.188</a></li>    <li id="BibPLXBIB0010" label="[10]">Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep Sparse Rectifier Neural Networks. In <em>      <em>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011</em>     </em>(JMLR Proceedings), Geoffrey&#x00A0;J. Gordon, David&#x00A0;B. Dunson, and Miroslav Dud&#x00ED;k (Eds.). Vol.&#x00A0;15. JMLR.org, 315&#x2013;323. <a class="link-inline force-break" href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf"      target="_blank">http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf</a></li>    <li id="BibPLXBIB0011" label="[11]">Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. 2013. Hybrid speech recognition with deep bidirectional LSTM. In <em>      <em>Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on</em>     </em>. IEEE, 273&#x2013;278.</li>    <li id="BibPLXBIB0012" label="[12]">Sepp Hochreiter and J&#x00FC;rgen Schmidhuber. 1997. Long Short-Term Memory. <em>      <em>Neural Comput.</em>     </em>9, 8 (Nov. 1997), 1735&#x2013;1780. 0899-7667 <a class="link-inline force-break"      href="https://doi.org/10.1162/neco.1997.9.8.1735"      target="_blank">https://doi.org/10.1162/neco.1997.9.8.1735</a></li>    <li id="BibPLXBIB0013" label="[13]">Courtenay Honeycutt and Susan&#x00A0;C. Herring. 2009. Beyond Microblogging: Conversation and Collaboration via Twitter. In <em>      <em>42st Hawaii International International Conference on Systems Science (HICSS-42 2009), Proceedings (CD-ROM and online), 5-8 January 2009, Waikoloa, Big Island, HI, USA</em>     </em>. IEEE Computer Society, 1&#x2013;10. <a class="link-inline force-break" href="https://doi.org/10.1109/HICSS.2009.89"      target="_blank">https://doi.org/10.1109/HICSS.2009.89</a></li>    <li id="BibPLXBIB0014" label="[14]">Liangjie Hong, Ovidiu Dan, and Brian&#x00A0;D Davison. 2011. Predicting popular messages in twitter. In <em>      <em>Proceedings of the 20th international conference companion on World wide web</em>     </em>. ACM, 57&#x2013;58.</li>    <li id="BibPLXBIB0015" label="[15]">Clayton&#x00A0;J. Hutto and Eric Gilbert. 2014. VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. In <em>      <em>Proceedings of the Eighth International Conference on Weblogs and Social Media, ICWSM 2014, Ann Arbor, Michigan, USA, June 1-4, 2014.</em>     </em>, Eytan Adar, Paul Resnick, Munmun&#x00A0;De Choudhury, Bernie Hogan, and Alice&#x00A0;H. Oh (Eds.). The AAAI Press. <a class="link-inline force-break"      href="http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109"      target="_blank">http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109</a></li>    <li id="BibPLXBIB0016" label="[16]">Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In <em>      <em>Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015</em>     </em>(JMLR Workshop and Conference Proceedings), Francis&#x00A0;R. Bach and David&#x00A0;M. Blei (Eds.). Vol.&#x00A0;37. JMLR.org, 448&#x2013;456. <a class="link-inline force-break" href="http://jmlr.org/proceedings/papers/v37/ioffe15.html"      target="_blank">http://jmlr.org/proceedings/papers/v37/ioffe15.html</a></li>    <li id="BibPLXBIB0017" label="[17]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. <em>      <em>arXiv preprint arXiv:1301.3781</em>     </em>(2013).</li>    <li id="BibPLXBIB0018" label="[18]">Krish Perumal and Graeme Hirst. 2016. Semi-supervised and unsupervised categorization of posts in Web discussion forums using part-of-speech information and minimal features. In <em>      <em>Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, WASSA@NAACL-HLT 2016, June 16, 2016, San Diego, California, USA</em>     </em>, Alexandra Balahur, Erik&#x00A0;Van der Goot, Piek Vossen, and Andr&#x00E9;s Montoyo (Eds.). The Association for Computer Linguistics, 100&#x2013;108. <a class="link-inline force-break"      href="http://aclweb.org/anthology/W/W16/W16-0417.pdf"      target="_blank">http://aclweb.org/anthology/W/W16/W16-0417.pdf</a></li>    <li id="BibPLXBIB0019" label="[19]">Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised Modeling of Twitter Conversations. In <em>      <em>Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 2-4, 2010, Los Angeles, California, USA</em>     </em>. The Association for Computational Linguistics, 172&#x2013;180. <a class="link-inline force-break"      href="http://www.aclweb.org/anthology/N10-1020"      target="_blank">http://www.aclweb.org/anthology/N10-1020</a></li>    <li id="BibPLXBIB0020" label="[20]">Matthew Rowe and Harith Alani. 2014. Mining and comparing engagement dynamics across multiple social media platforms. In <em>      <em>ACM Web Science Conference, WebSci &#x2019;14, Bloomington, IN, USA, June 23-26, 2014</em>     </em>, Filippo Menczer, Jim Hendler, William&#x00A0;H. Dutton, Markus Strohmaier, Ciro Cattuto, and Eric&#x00A0;T. Meyer (Eds.). ACM, 229&#x2013;238. <a class="link-inline force-break" href="https://doi.org/10.1145/2615569.2615677"      target="_blank">https://doi.org/10.1145/2615569.2615677</a></li>    <li id="BibPLXBIB0021" label="[21]">Matthew Rowe, Sofia Angeletou, and Harith Alani. 2011. Predicting discussions on the social semantic web. <em>      <em>The Semanic Web: Research and Applications</em>     </em>(2011), 405&#x2013;420.</li>    <li id="BibPLXBIB0022" label="[22]">Iulian&#x00A0;Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron&#x00A0;C. Courville, and Joelle Pineau. 2016. Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models. In <em>      <em>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.</em>     </em>, Dale Schuurmans and Michael&#x00A0;P. Wellman (Eds.). AAAI Press, 3776&#x2013;3784. <a class="link-inline force-break"      href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11957"      target="_blank">http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11957</a></li>    <li id="BibPLXBIB0023" label="[23]">Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural Responding Machine for Short-Text Conversation. In <em>      <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers</em>     </em>. The Association for Computer Linguistics, 1577&#x2013;1586. <a class="link-inline force-break"      href="http://aclweb.org/anthology/P/P15/P15-1152.pdf"      target="_blank">http://aclweb.org/anthology/P/P15/P15-1152.pdf</a></li>    <li id="BibPLXBIB0024" label="[24]">Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl, and Jos&#x00E9;&#x00A0;San Pedro. 2010. How useful are your comments?: analyzing and predicting youtube comments and comment ratings. In <em>      <em>Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010</em>     </em>, Michael Rappa, Paul Jones, Juliana Freire, and Soumen Chakrabarti (Eds.). ACM, 891&#x2013;900. <a class="link-inline force-break" href="https://doi.org/10.1145/1772690.1772781"      target="_blank">https://doi.org/10.1145/1772690.1772781</a></li>    <li id="BibPLXBIB0025" label="[25]">Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob&#x00A0;Grue Simonsen, and Jian-Yun Nie. 2015. A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion. <em>      <em>CoRR</em>     </em>abs/1507.02221(2015). <a class="link-inline force-break"      href="http://arxiv.org/abs/1507.02221"      target="_blank">http://arxiv.org/abs/1507.02221</a></li>    <li id="BibPLXBIB0026" label="[26]">Stuck_In_the_Matrix. 2015. Reddit-Comments dataset. (2015). <a class="link-inline force-break"      href="https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/"      target="_blank">https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/</a></li>    <li id="BibPLXBIB0027" label="[27]">Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed&#x00A0;H. Chi. 2010. Want to be Retweeted? Large Scale Analytics on Factors Impacting Retweet in Twitter Network. In <em>      <em>Proceedings of the 2010 IEEE Second International Conference on Social Computing, SocialCom / IEEE International Conference on Privacy, Security, Risk and Trust, PASSAT 2010, Minneapolis, Minnesota, USA, August 20-22, 2010</em>     </em>, Ahmed&#x00A0;K. Elmagarmid and Divyakant Agrawal (Eds.). IEEE Computer Society, 177&#x2013;184. <a class="link-inline force-break"      href="https://doi.org/10.1109/SocialCom.2010.33"      target="_blank">https://doi.org/10.1109/SocialCom.2010.33</a></li>    <li id="BibPLXBIB0028" label="[28]">Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The effect of wording on message propagation: Topic-and author-controlled natural experiments on Twitter. <em>      <em>ACL</em>     </em> (2014).</li>    <li id="BibPLXBIB0029" label="[29]">Yla&#x00A0;R. Tausczik and James&#x00A0;W. Pennebaker. 2011. Predicting the perceived quality of online mathematics contributions from users&#x2019; reputations. In <em>      <em>Proceedings of the International Conference on Human Factors in Computing Systems, CHI 2011, Vancouver, BC, Canada, May 7-12, 2011</em>     </em>, Desney&#x00A0;S. Tan, Saleema Amershi, Bo&#x00A0;Begole, Wendy&#x00A0;A. Kellogg, and Manas Tungare (Eds.). ACM, 1885&#x2013;1888. <a class="link-inline force-break" href="https://doi.org/10.1145/1978942.1979215"      target="_blank">https://doi.org/10.1145/1978942.1979215</a></li>    <li id="BibPLXBIB0030" label="[30]">T. Tieleman and G. Hinton. 2012. Lecture 6.5&#x2014;RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning. (2012).</li>    <li id="BibPLXBIB0031" label="[31]">Li Wang, Marco Lui, Su&#x00A0;Nam Kim, Joakim Nivre, and Timothy Baldwin. 2011. Predicting Thread Discourse Structure over Technical Web Forums. In <em>      <em>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL</em>     </em>. ACL, 13&#x2013;25. <a class="link-inline force-break"      href="http://www.aclweb.org/anthology/D11-1002"      target="_blank">http://www.aclweb.org/anthology/D11-1002</a></li>    <li id="BibPLXBIB0032" label="[32]">Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander&#x00A0;J Smola, and Eduard&#x00A0;H Hovy. 2016. Hierarchical Attention Networks for Document Classification.. In <em>      <em>HLT-NAACL</em>     </em>. 1480&#x2013;1489.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>This work was done when the first author was visiting the University of Michigan.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break"    href="https://www.merriam-webster.com/dictionary/conversation">https://www.merriam-webster.com/dictionary/conversation</a>, retrieved on February 17, 2018.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186013">https://doi.org/10.1145/3178876.3186013</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
