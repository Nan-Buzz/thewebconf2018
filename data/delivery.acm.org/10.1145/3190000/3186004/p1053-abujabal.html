<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Never-Ending Learning for Open-Domain Question Answering over Knowledge Bases</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Never-Ending Learning for Open-Domain Question Answering over Knowledge Bases</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Abdalghani</span>     <span class="surName">Abujabal</span>,     Max Planck Institute for Informatics Saarland Informatics Campus, Germany, <a href="mailto:abujabal@mpi-inf.mpg.de">abujabal@mpi-inf.mpg.de</a>    </div>    <div class="author">     <span class="givenName">Rishiraj Saha</span>     <span class="surName">Roy</span>,     Max Planck Institute for Informatics Saarland Informatics Campus, Germany, <a href="mailto:rishiraj@mpi-inf.mpg.de">rishiraj@mpi-inf.mpg.de</a>    </div>    <div class="author">     <span class="givenName">Mohamed</span>     <span class="surName">Yahya</span>,     Bloomberg L.P. London, United Kingdom, <a href="mailto:myahya@myahya.org">myahya@myahya.org</a>    </div>    <div class="author">     <span class="givenName">Gerhard</span>     <span class="surName">Weikum</span>,     Max Planck Institute for Informatics Saarland Informatics Campus, Germany, <a href="mailto:weikum@mpi-inf.mpg.de">weikum@mpi-inf.mpg.de</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186004" target="_blank">https://doi.org/10.1145/3178876.3186004</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Translating natural language questions to semantic representations such as SPARQL is a core challenge in open-domain question answering over knowledge bases (KB-QA). Existing methods rely on a clear separation between an offline training phase, where a model is learned, and an online phase where this model is deployed. Two major shortcomings of such methods are that (i) they require access to a large annotated training set that is not always readily available and (ii) they fail on questions from before-unseen domains. To overcome these limitations, this paper presents NEQA, a <em>continuous learning paradigm</em> for KB-QA. Offline, NEQA automatically learns templates mapping syntactic structures to semantic ones from a <em>small</em> number of training question-answer pairs. Once deployed, continuous learning is triggered on cases where templates are insufficient. Using a semantic similarity function between questions and by judicious invocation of non-expert user feedback, NEQA learns new templates that capture previously-unseen syntactic structures. This way, NEQA gradually extends its template repository. NEQA periodically re-trains its underlying models, allowing it to adapt to the language used after deployment. Our experiments demonstrate NEQA&#x0027;s viability, with steady improvement in answering quality over time, and the ability to answer questions from new domains.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Question answering; Never-ending learning; User feedback</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, and Gerhard Weikum. 2018. Never-Ending Learning for Open-Domain Question Answering over Knowledge Bases. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 12 Pages. <a href="https://doi.org/10.1145/3178876.3186004" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186004</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <figure id="fig1">    <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186004/images/www2018-13-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Continuous learning: if a new question <em>u<sub>new</sub>      </em> cannot be satisfactorily answered via templates, we utilize user feedback on the output of a semantic similarity function to learn a new template <span class="inline-equation"><span class="tex">$(u^{t}_{new}$</span>      </span>, <em>q<sup>t</sup>      </em>) based on <em>u<sub>new</sub>      </em>.</span>    </div>    </figure>    <p>    <strong>Motivation.</strong> Open-domain question answering over knowledge bases (KB-QA) is an active research area where the goal is to provide crisp answers to natural language questions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0057">57</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0062">62</a>] or telegraphic queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>]. An important direction in KB-QA performs this answering via semantic parsing: translating a user&#x0027;s <em>question</em> to a SPARQL <em>query</em> that is subsequently executed over a KB like Freebase [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>], DBPedia&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] or YAGO&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>]. Existing approaches rely on a clear separation between an offline training phase, where a model is either learned or manually crafted, and an online phase where this model is deployed to answer users&#x2019; questions. Such approaches suffer from three major shortcomings: (i) they require access to reasonably large training sets with sufficient syntactic and lexical coverage representative of the kinds of questions users pose, which are expensive to construct, (ii) they provide no mechanism for improving their performance over time by learning from failure cases on questions received after deployment, and (iii) they are limited to the language learned at training time, therefore, they fail on questions from domains not observed previously.</p>    <p>In this work, we present a continuous-learning framework for template-based KB-QA called NEQA (<em>N</em>ever <em>E</em>nding <em>QA</em>) that (i) is initialized with a <em>small</em> training set, (ii) improves its performance over time by judiciously invoking user feedback on answers from non-expert users on the failure cases of the underlying template-based answering mechanism, and (iii) adapts to the language used after deployment by periodically retraining its underlying models. A simplified workflow is shown in Figure&#x00A0;<a class="fig" href="#fig1">1</a>.</p>    <p>Training a well-performing open-domain KB-QA system requires a massive annotation effort, in terms of cost, time and expertise. Some methods use labeled SPARQL queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], while others train their systems on question/answer pairs as a form of weak supervision&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>], which has been proven to work well. We adopt this form of supervision, however, for only a small training seed to minimize the annotation effort required. We rely on non-expert user feedback to acquire more question/answer pairs over time. In our experiments, we show that NEQA was able to successfully answer questions from domains it has not seen before.</p>    <p>We harness non-expert user feedback on answer sets generated as a response to a given question, which is related to a number of recent ideas in semantic parsing and natural language interfaces to databases (NLIDB). Li et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] invoke user feedback to resolve ambiguous words/phrases in the users&#x2019; questions, while Iyer et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] ask expert users to provide a full SQL query that answers a question over a database. In Wang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>], an end user teaches the model new concepts through direct interaction. Other approaches utilize crowdsourcing as a sort of interactivity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0052">52</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0053">53</a>].</p>    <p>NEQA builds on an established line of work that performs semantic parsing by translating syntactic dependency structures of utterances to semantic predicate-argument structures using templates that are either manually crafted&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0057">57</a>], or automatically learned&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. By exploiting syntax, such template-based approaches achieve better generalization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. As an example, such systems can use a template generated from <em>u</em>    <sub>1</sub> = <em>&#x201C;which film awards was bill carraro nominated for?&#x201D;</em> to answer the syntactically isomorphic question <em>&#x201C;which president was lincoln succeeded by?&#x201D;</em>, despite the fact that it invokes a different semantic KB predicate.</p>    <p>The main drawback of such systems is their inability to handle new syntactic structures beyond those observed in the static training set. Take, for example, a new question <em>u<sub>new</sub>    </em> = <em>&#x201C;what are the film award nominations that bill carraro received?&#x201D;</em>. Even if the above systems had seen <em>u</em>    <sub>1</sub> during training, they cannot answer this new semantically related (but syntactically different) question. This problem is exacerbated if these systems are trained on a small number of training examples. NEQA rectifies this limitation by using a state-of-art similarity function&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0063">63</a>] to find a correctly answered and semantically-similar question from its history, and subsequently learns a new template based on the new question.</p>    <p>    <strong>Approach.</strong> NEQA is driven by two intuitions. First, syntactic isomorphism of questions is a strong cue for the isomorphism of their respective predicate-argument structures (SPARQL queries). This intuition underlies template-based approaches outlined above. Second, where syntactic isomorphism fails, NEQA invokes a semantic similarity function together with user feedback to transfer semantics across syntactic structures and triggers the learning of new templates. NEQA combines these intuitions into a <em>continuous learning framework</em> that gradually overcomes the limitations of small training sets and evolves over time.</p>    <p>NEQA starts by automatically learning a few templates from a small number of questions offline, using the approach of Abujabal et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. Figure&#x00A0;<a class="fig" href="#fig1">1</a> shows what happens when NEQA receives a new question <em>u<sub>new</sub>    </em> online. It adds satisfactorily answered questions to an ever-growing bank of question-query (<em>u</em>, <em>q</em>) pairs, initially composed of a small training set. Questions in this bank will be called upon when our template-based answering mechanism fails to satisfactorily answer a new question. In such cases, NEQA learns new syntactic structures to improve its future answering performance.</p>    <p>When <em>u<sub>new</sub>    </em> is unsatisfactorily answered using our template-based answering mechanism, NEQA triggers the learning of a new template from this question. It first consults a semantic similarity function to find the <em>k</em> previously answered questions closest to <em>u<sub>new</sub>    </em>. NEQA then instantiates the corresponding queries with entities from <em>u<sub>new</sub>    </em>. Leveraging user feedback on answer sets generated by executing these queries over the KB, one of the resulting queries (<em>q</em>    <sub>2</sub> in Figure&#x00A0;<a class="fig" href="#fig1">1</a>) is determined to be the best fit for <em>u<sub>new</sub>    </em>. NEQA then uses a lexicon and an Integer Linear Program to align the constituents of <em>u<sub>new</sub>    </em> and <em>q</em>    <sub>2</sub>. A new template (<span class="inline-equation"><span class="tex">$u^{t}_{new}$</span>    </span>, <span class="inline-equation"><span class="tex">$q_2^t$</span>    </span>) is created from this pair, which is then added to the template bank.</p>    <p>    <strong>Contributions.</strong> We present NEQA, the first continuous learning framework for KB-QA, and make four novel contributions:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">a KB-QA system that can be seeded with a small number of training examples and supports continuous learning to improve its answering performance over time;<br/></li>    <li id="list2" label="&#x2022;">a similarity function-based answering mechanism that enables NEQA to answer questions with previously-unseen syntactic structures, thereby extending its coverage;<br/></li>    <li id="list3" label="&#x2022;">a user feedback component that judiciously asks non-expert users to select satisfactory answers, thus closing the loop between users and the system and enabling continuous learning;<br/></li>    <li id="list4" label="&#x2022;">extensive experimental results on two benchmarks demonstrating the viability of our continuous learning approach, and the ability to answer questions from previously-unseen domains.<br/></li>    </ul>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Setup</h2>    </div>    </header>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Basic Concepts</h3>     </div>    </header>    <figure id="fig2">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186004/images/www2018-13-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">A subgraph from a knowledge base. Nodes correspond to entities and classes, and edges represent predicates.</span>     </div>    </figure>    <p>     <strong>Knowledge base.</strong> A knowledge base (KB) is a collection of facts represented as a graph where nodes correspond to (i) entities <em>e</em> &#x2208; <em>E</em> (e.g., <tt>BillCarraro</tt>), (ii) classes or types <em>c</em> &#x2208; <em>C</em>, (e.g., <tt>movieAward</tt>), and (iii) literals <em>h</em> &#x2208; <em>H</em> (e.g., dates). Two nodes are connected by an edge labeled with a predicate <em>p</em> &#x2208; <em>P</em> (for example, <tt>nominatedFor</tt>), which together form a triple (e.g., <tt>BillCarraro nominatedFor BlackReel</tt>). Any <em>s</em> &#x2208; <em>S</em> = <em>E</em>&#x222A;<em>C</em>&#x222A;<em>P</em> is called a <em>semantic item</em>. Figure&#x00A0;<a class="fig" href="#fig2">2</a> shows a sample KB subgraph.</p>    <p>     <strong>Query.</strong> To query a KB, we use graph-matching based on SPARQL triple patterns. A triple pattern is a triple with one or more of its components replaced by variables (e.g., <tt>?x type movieAward</tt>). A query <em>q</em> is a set of triple patterns (e.g., <tt>BillCarraro nominatedFor ?x . ?x type movieAward</tt>). The variable <tt>?x</tt> is designated as the <em>projection variable</em>.</p>    <p>     <strong>Answer.</strong> An answer <em>a</em> to a query <em>q</em> over a KB is an entity (possibly a set) which is obtained by mapping variables of <em>q</em> to KB items where the projection variable maps to <em>a</em>. For example, the answer to the above query is <tt>BlackReel</tt> (Figure&#x00A0;<a class="fig" href="#fig2">2</a>).</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Question and Query Templates</h3>     </div>    </header>    <figure id="fig3">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186004/images/www2018-13-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">An aligned question-query pair (<em>u</em>, <em>q</em>). Alignment is indicated by shared <em>ent</em>, <em>pred</em>, and <em>class</em> annotations.</span>     </div>    </figure>    <figure id="fig4">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186004/images/www2018-13-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">A template composed of aligned question and query templates (<em>u<sup>t</sup>       </em>, <em>q<sup>t</sup>       </em>) generated from the question-query pair in Figure&#x00A0;<a class="fig" href="#fig3">3</a>. Shared <em>ent</em>, <em>pred</em>, and <em>class</em> annotations indicate alignment between <em>u<sup>t</sup>       </em> and <em>q<sup>t</sup>       </em>.</span>     </div>    </figure>    <p>Templates play an important role in KB-QA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0048">48</a>]. They guide the mapping of the syntactic structures of natural language utterances to semantic predicate-argument structures in SPARQL queries. Figure&#x00A0;<a class="fig" href="#fig4">4</a> shows an example template from Abujabal et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], which has the same form as the templates we use in this work. It consists of a question template <em>u<sup>t</sup>     </em> and its corresponding query template <em>q<sup>t</sup>     </em>, where <em>u<sup>t</sup>     </em> and <em>q<sup>t</sup>     </em> are derived from generalizations over the dependency parse and the query, respectively. Alignment of the constituents of <em>u<sup>t</sup>     </em> and <em>q<sup>t</sup>     </em> is indicated by shared <em>ent</em>, <em>pred</em>, and <em>class</em> annotations. <strong>Generating templates.</strong>We follow the approach of Abujabal et al. (Section 3 in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]) for learning templates. We briefly summarize the relevant parts of the method here for completeness. The approach is designed for a weakly supervised setting where a training instance is a question <em>u</em> paired with its answer set <em>A<sub>u</sub>     </em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]. NEQA uses this form of supervision for the initial training phase. The approach heuristically generates a query <em>q</em> that captures each training question <em>u</em> from the corresponding training pair (<em>u</em>, <em>A<sub>u</sub>     </em>). For <em>u</em> = <em>&#x201C;Which film awards was Bill Carraro nominated for?&#x201D;</em>, the corresponding query would be <em>q</em> = <tt>BillCarraro nominatedFor ?x . ?x type movieAward</tt>. We now have a question query pair (<em>u</em>, <em>q</em>). The rest of the discussion explains how a template is generated from such a pair. This process is invoked in NEQA both as part of initial training (where we start with (<em>u</em>, <em>A<sub>u</sub>     </em>) pairs), and during continuous learning, where NEQA generalizes a (<em>u</em>, <em>q</em>) pair resulting from the similarity function and user feedback to a template (Figure&#x00A0;<a class="fig" href="#fig1">1</a>).</p>    <p>Next, nodes in the dependency parse of <em>u</em> are aligned with semantic items in <em>q</em>. A dependency parse is a tree whose nodes correspond to words in a sentence and edges represent grammatical relations between words. We use the Stanford dependency parser&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] in this work. For example, <em>&#x2018;film awards&#x2019;</em> in <em>u</em> above is aligned with the KB class <tt>movieAward</tt> in the corresponding <em>q</em>. A weighted <em>lexicon L</em> (Section&#x00A0;<a class="sec" href="#sec-9">2.3</a>) is used to connect phrases in <em>u</em> to all candidate semantic items in <em>q</em>, forming a weighted bipartite graph. The alignment problem is formulated as a constrained optimization problem solved using an Integer Linear Program (ILP)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. The solution to the ILP is a role-aligned question-query pair (Figure&#x00A0;<a class="fig" href="#fig3">3</a>) where phrases in <em>u</em> that are not part of the alignment are dropped (e.g., <em>&#x2018;Which&#x2019;</em>). Finally, concrete values in both <em>u</em> and <em>q</em> are dropped to produce a role-aligned question-query template pair (<em>u<sup>t</sup>     </em>, <em>q<sup>t</sup>     </em>) (Figure&#x00A0;<a class="fig" href="#fig4">4</a>).</p> 				 <p><strong>Using templates.</strong> During answering, when a new question <em>u<sub>new</sub>     </em> is encountered, question templates matching its dependency parse are identified (see Section&#x00A0;<a class="sec" href="#sec-12">3.2</a>). Corresponding query templates are then instantiated using alignment information and the lexicon. This step potentially generates multiple query candidates due to lexicon ambiguity, which are ranked using a learning-to-rank (LTR) model. Finally, the answer of the top-ranked query is presented to the user.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Predicate and Class Lexicons</h3>     </div>    </header>    <p>To connect utterance vocabulary to semantic items in the KB, we construct a lexicon <em>L</em> that consists of a predicate lexicon <em>L<sub>P</sub>     </em> and a class lexicon <em>L<sub>C</sub>     </em>. <em>L<sub>P</sub>     </em> and <em>L<sub>C</sub>     </em> are constructed from ClueWeb09-FACC1, a corpus of 500<em>M</em> Web pages annotated with Freebase entities&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. To construct <em>L<sub>P</sub>     </em> we run the extraction pattern <em>&#x201C;<em>e</em>      <sub>1</sub>~<em>r</em>~<em>e</em>      <sub>2</sub>&#x201D;</em> over the corpus, where <em>e</em>     <sub>1</sub> and <em>e</em>     <sub>2</sub> are entities and <em>r</em> is a phrase. Following the <em>distant supervision hypothesis</em> in Mintz et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>], we assume that if (<em>e</em>     <sub>1</sub>~<em>p</em>~<em>e</em>     <sub>2</sub>) is a triple in the KB, then <em>r</em> expresses <em>p</em> and we add <em>r</em>&#x21A6;<em>p</em> to <em>L<sub>P</sub>     </em>. <em>L<sub>C</sub>     </em> is constructed by running Hearst patterns&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] over the corpus, where one argument is an entity and the other is a noun phrase. For example, for <em>&#x201C;<em>e</em>~and&#x2009;other~<em>np</em>&#x201D;</em>, the entry <em>np</em>&#x21A6;<em>c</em> for each <em>c</em> is added to <em>L<sub>C</sub>     </em> such that <span class="inline-equation"><span class="tex">$(e \texttt {~type~} c) \in KB$</span>     </span>. Each entry in <em>L</em> is assigned a mapping weight proportional to its corpus frequency. For handling entities in questions, we use an existing named entity recognition and disambiguation (NERD) system&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0059">59</a>].</p>    </section>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> The NEQA Framework</h2>    </div>    </header>    <p>Initially, NEQA goes through an offline training stage that populates its question-query and template banks with their seed data (Section&#x00A0;<a class="sec" href="#sec-11">3.1</a>). Online, when NEQA is deployed, a stream of questions arrives from users. NEQA attempts to answer each incoming question using the templates it has learned so far (Section&#x00A0;<a class="sec" href="#sec-12">3.2</a>). If this fails, it falls back to answering using the semantic similarity function against the set of already answered questions in the question-query bank (Section&#x00A0;<a class="sec" href="#sec-13">3.3</a>). In both cases, NEQA utilizes user feedback on answer sets to extend its banks (Section&#x00A0;<a class="sec" href="#sec-14">3.4</a>). After each <em>batch</em> of questions, NEQA retrains its learning-to-rank (LTR) ranking model on the accumulated data in its banks to improve system performance for subsequent questions.</p>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Initial Training</h3>     </div>    </header>    <p>NEQA is initialized through an automated template generation stage, as discussed in Section&#x00A0;<a class="sec" href="#sec-8">2.2</a>. This stage relies on weak supervision through a small number of questions, each paired with its answer set. This training stage results in populating the question-query and template banks (Figure&#x00A0;<a class="fig" href="#fig1">1</a>) with their seed data that are used for bootstrapping the continuous learning process. Moreover, it results in NEQA&#x0027;s first LTR model. NEQA&#x0027;s continuous learning improves all three components once the system goes online.</p>    <p>Questions in the question-query bank are stored in a generalized form that facilitates improved matching by the semantic similarity function. Specifically, entities in both questions and queries in the question-query bank are replaced by placeholders. For example, <em>&#x201C;Which film award was ENTITY nominated for?&#x201D;</em> (question) is paired with <tt>ENTITY nominatedFor ?x . ?x type movieAward</tt> (query).</p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Answering with Templates</h3>     </div>    </header>    <p>Once the system goes online, it starts receiving new question utterances from users. Given a new question <em>u<sub>new</sub>     </em>, NEQA identifies matching question templates {<em>u<sup>t</sup>     </em>} in its template bank. A match is deemed successful if edge labels and POS tags in the dependency parse of <em>u<sub>new</sub>     </em> and <em>u<sup>t</sup>     </em> agree. For example, the dependency parse of <em>&#x201C;which president was lincoln succeeded by?&#x201D;</em> matches the utterance template in Figure&#x00A0;<a class="fig" href="#fig4">4</a>. When this happens, the associated query template <em>q<sup>t</sup>     </em> is instantiated with concrete semantic items using the phrases in <em>u<sub>new</sub>     </em>, the alignment information between <em>u<sup>t</sup>     </em> and <em>q<sup>t</sup>     </em>, and the underlying lexicon <em>L</em>. For example, based on the alignment information in Figure&#x00A0;<a class="fig" href="#fig4">4</a>, the verb phrase <em>&#x2018;succeeded by&#x2019;</em> is used to instantiate a KB predicate.</p>    <p>Note that a single utterance may match multiple utterance templates, and these templates may result in multiple queries due to ambiguity in <em>L</em>. The learning-to-rank (LTR) model is used to rank this set of candidate queries. Features for training the LTR model are borrowed from past work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], and are derived from lexicon weights, entity popularity scores, answer type constraints, and sizes of answer sets, among others. The top-ranked queries, as detailed below, are then executed over the KB to fetch answer sets.</p>    <p>Next, user feedback is obtained on these retrieved answer sets. To be realistic, note that we obtain feedback on answer sets of the top-<em>k</em> queries where <em>k</em> is small. If an answer set is chosen (e.g., <tt>{AndrewJohnson}</tt>) by the user, then this validates the choice of the query <em>q</em>     <sup>*</sup> that generated this answer set as correct (e.g., <tt>AbrahamLincoln succeededBy ?x . ?x type president</tt>). On the other hand, when none of the shown answer sets is chosen, NEQA proceeds differently (Section&#x00A0;<a class="sec" href="#sec-13">3.3</a>). Finally, the correct query <em>q</em>     <sup>*</sup> is paired with <em>u<sub>new</sub>     </em> and is then added to our question-query bank after entity generalization. For the example above, <em>q</em>     <sup>*</sup> after entity generalization is: <tt>ENTITY succeededBy ?x . ?x type president</tt>. Such an augmentation of the question-query bank potentially results in the system gaining questions with unseen KB predicates. We validate this postulate in our experiments on open-domain answering. When a batch of questions has been received, we use the questions answered satisfactorily by the templates to retrain the LTR model to further boost the performance of the system on subsequent questions.</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Answering via Similarity Function</h3>     </div>    </header>    <p>A core contribution of NEQA is to extend coverage of template-based answering using a semantic similarity function. A typical template-based KB-QA system fails when an input utterance represents previously unseen syntactic structure&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0048">48</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0057">57</a>]. Further, even when a matching utterance template is identified, the KB-QA system might fail to deliver answers due to errors in the alignment information between the question and the query templates.</p>    <p>NEQA, on the other hand, builds on failure cases to improve its future QA performance. Whenever a question cannot be answered satisfactorily using templates, NEQA uses a <em>semantic similarity function</em> to retrieve the <em>k</em> most semantically similar questions to <em>u<sub>new</sub>     </em> from its question-query bank. For example, say, the utterance <em>u<sub>new</sub>     </em> = <em>&#x201C;what are the film award nominations that bill carraro received?&#x201D;</em> represents a syntactic structure beyond the coverage of our current templates. However, our question-query bank contains a similar question: <em>&#x201C;which film awards was bill carraro nominated for?&#x201D;</em>. The goal of our similarity function is to identify such questions and allow the <em>transfer of semantics</em> across syntactic structures.</p>    <p>We first use an off-the-shelf NERD system to link mentions of entities in <em>u<sub>new</sub>     </em> to KB entities&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0059">59</a>]. Identified entities in <em>u<sub>new</sub>     </em> are then replaced by placeholders to ensure better generalization. Similar generalization is also done on (<em>u</em>, <em>q</em>) pairs in our question-query bank (Section&#x00A0;<a class="sec" href="#sec-11">3.1</a>). Next, the corresponding queries {<em>q</em>     <sub>1</sub>&#x2026;<em>q<sub>k</sub>     </em>} for these similar utterances are instantiated with entities from <em>u<sub>new</sub>     </em> and then executed over the KB to retrieve answer sets.</p>    <p>Next, we obtain user feedback on the answer sets of the <em>k</em> queries. If an answer set is chosen (e.g., <tt>{BlackReel}</tt>), the corresponding query <em>q</em>     <sup>*</sup> (e.g., <tt>BillCarraro nominatedFor ?x . ?x type movieAward</tt>) is paired with <em>u<sub>new</sub>     </em>. The newly generated pair (<em>u<sub>new</sub>     </em>, <em>q</em>     <sup>*</sup>) is then added to our question-query bank after entity generalization. A vital step of NEQA is the subsequent on-the-fly alignment and generalization of <em>u<sub>new</sub>     </em> and <em>q</em>     <sup>*</sup>, to obtain a new template (<em>u<sup>t</sup>     </em>, <em>q<sup>t</sup>     </em>). This is performed by casting the problem as an integer linear program (Section&#x00A0;<a class="sec" href="#sec-8">2.2</a>). The new template (<em>u<sup>t</sup>     </em>, <em>q<sup>t</sup>     </em>) is then added to NEQA&#x0027;s template bank. By acquiring more templates, the system&#x0027;s capability to handle syntactic variation increases, i.e., it learns how to <em>directly</em> answer questions with new syntactic structures. <strong>Similarity function.</strong>Following recent work on <em>question retrieval</em> in community question answering&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0063">63</a>], we opt for an unsupervised semantic similarity function. Note that we treat the similarity function as a plug-in, where supervised methods can also be used if required. Our similarity function consists of two components: (i) question likelihood based on a language model, and (ii) word embedding-based similarity obtained through <em>word2vec</em>.</p>    <p>Given a new question <em>u<sub>new</sub>     </em> and a question <em>u<sub>i</sub>     </em> from our question-query bank, our first component, based on language model, computes question likelihood as follows: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} score_{LM}(u_{new}, u_i) = \prod _{w \in u_{new}} [(1 - \lambda) \cdot P_{ml}(w|u_i) + \lambda \cdot P_{ml}(w|C)] \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <em>P<sub>ml</sub>     </em>(<em>w</em>|<em>u<sub>i</sub>     </em>) represents the maximum likelihood probability estimate of <em>w</em> estimated from <em>u<sub>i</sub>     </em> and <em>w</em> is a unigram, bigram or trigram generated directly from <em>u<sub>new</sub>     </em> or from paths of lengths one and two in the dependency parse of <em>u<sub>new</sub>     </em>. <em>P<sub>ml</sub>     </em>(<em>w</em>|<em>C</em>) is a smoothing term calculated as the maximum likelihood of <em>w</em> in a corpus <em>C</em> of questions from our question-query bank, and <em>&#x03BB;</em> &#x2208; [0, 1] is a smoothing parameter.</p>    <p>The second component uses a <em>word2vec</em> model pre-trained on Google News corpora&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} score_{w2v}(u_{new}, u_i) = \dfrac{1}{|\mathcal {P}|} \sum _{(w_j, w_k) \in \mathcal {P}} cos(w2v(w_j), w2v(w_k)), \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where, <em>w<sub>j</sub>     </em> &#x2208; <em>u<sub>new</sub>     </em>, <em>w<sub>k</sub>     </em> &#x2208; <em>u<sub>i</sub>     </em>, <em>w</em>2<em>v</em>(<em>w</em>) is the <em>word2vec</em> embedding vector of <em>w</em>, and <span class="inline-equation"><span class="tex">$\mathcal {P}$</span>     </span> is the set of word pairs from <em>u<sub>new</sub>     </em> and <em>u<sub>i</sub>     </em> whose cosine similarity is above a threshold <em>&#x03C4;</em>.</p>    <p>The final score is a linear combination of the two components presented above, where <em>&#x03B1;</em> is a trade-off parameter: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} score_{sim}(u_{new}, u_i) &#x0026; = \; \alpha \cdot score_{LM}(u_{new}, u_i) \\ &#x0026; \;\; \;\;+ (1 - \alpha) \cdot score_{w2v}(u_{new}, u_i) \\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>    </p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Harnessing User Feedback</h3>     </div>    </header>    <p>NEQA resorts to user feedback in two cases. The first is when an incoming utterance <em>u<sub>new</sub>     </em> is answered using templates in the template bank. In this case, the user is asked to give feedback on the relevance of the answer sets shown to her by either choosing the one that satisfies her information needs or none of them, if none is satisfactory. By propagating answer quality back to queries, this feedback is leveraged to extend the question-query bank. The second case is when NEQA returns answers using the semantic similarity function. The answers obtained from the top-<em>k</em> previously answered questions that are most similar to <em>u<sub>new</sub>     </em> are shown to the user for assessment. This feedback is used to extend both the template and the question-query banks.</p>    <p>In both cases above, it is important to keep <em>k</em> small to ensure the feasibility of asking for user feedback. In the experiments, we show that this is the case for our choices of LTR and semantic similarity functions. Additionally, we look at the extreme case where <em>k</em> = 1 and user feedback is bypassed by making the assumption that answers returned by our system are correct, and can be used for continuous learning.</p>    </section>   </section>   <section id="sec-15">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>    </div>    </header>    <p>We present extensive experimental evaluation and analysis of continuous learning in NEQA. Our experiments demonstrate NEQA&#x0027;s ability to continuously improve its answering performance over time starting with a very limited training set. We show that the answering performance of traditional state-of-the-art QA systems where periodic re-training is done is inferior to that of NEQA, which was designed specifically to support continuous learning. We also show that the manner in which NEQA exploits the interaction between syntax and semantics allows it to support truly open-domain QA by answering questions requiring predicates it has not seen before.</p>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Setup</h3>     </div>    </header>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">NEQA initialization statistics.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Property</strong>       </th>       <th style="text-align:center;">        <strong>WQ</strong>       </th>       <th style="text-align:center;">        <strong>CQ</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Size of initial training set</td>       <td style="text-align:center;">300</td>       <td style="text-align:center;">105</td>       </tr>       <tr>       <td style="text-align:left;">Size of development set</td>       <td style="text-align:center;">300</td>       <td style="text-align:center;">300</td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Initial templates acquired</td>       <td style="text-align:center;">223</td>       <td style="text-align:center;">85</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Benchmarks.</strong>We use the following KB-QA benchmarks over Freebase to evaluate NEQA:</p>    <ul class="list-no-style">     <li id="list5" label="&#x2022;"><strong>WebQuestions (WQ)</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>]: This benchmark was created using Google&#x0027;s suggest API and crowdsourcing, and is composed of 5810 questions, each paired with its answer set. These are split into 3778 training and 2032 test instances.<br/></li>     <li id="list6" label="&#x2022;"><strong>ComplexQuestions (CQ)</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>]: This very recent benchmark focuses on more challenging multi-constraint questions. It contains 2100 question-answer pairs from (i) a commercial search engine, (ii) WebQuestions and, (iii) the benchmark released by Yin et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0062">62</a>]. It is split into 1300 training and 800 test cases.<br/></li>    </ul>    <p>We base our extended analyses and comparisons with baseline systems on WQ due to the lack of publicly available KB-QA systems designed for handling complex questions in CQ. As detailed below, small subsets of the respective training sets are used for initial training, followed by streaming the complete test sets in batches to simulate online answering and continuous learning. <strong>Training.</strong> Table <a class="tbl" href="#tab1">1</a> gives a summary of the initial training stage. A main motivation for resorting to continuous learning is the cost associated with obtaining a large training set upfront. To simulate small seed training sets, we randomly sample only about 8% of the standard WQ and CQ training sets. These seed training sets were used to initialize the (i) question-query and template banks (Section&#x00A0;<a class="sec" href="#sec-11">3.1</a>), (ii) learning-to-rank (LTR) models (Section&#x00A0;<a class="sec" href="#sec-12">3.2</a>), and (iii) language model component of the similarity function (Section&#x00A0;<a class="sec" href="#sec-13">3.3</a>). The development sets (randomly sampled from the training set) were used to tune the <em>&#x03BB;</em> and <em>&#x03B1;</em> parameters of the similarity function. Crucially, NEQA is never exposed to the full WQ or CQ training sets in our experiments beyond the above seed examples. After initial training, NEQA is deployed to answer incoming questions, performing continuous learning when necessary. <strong>Continuous learning.</strong> During answering, NEQA receives test questions from the respective benchmark in batches. At the end of each batch, we retrain the LTR component and re-estimate the language model with the data seen thus far. We set our batch size to 100 questions, so that we can observe the effect of continued learning over a larger number of batches (20 for WQ, 8 for CQ). Varying batch size did not have any significant effect on the observed trends.</p>    <p>We report system performance in two modes, which differ in their invocation of user feedback during continuous learning:</p>    <ul class="list-no-style">     <li id="list7" label="&#x2022;"><strong>NEQA</strong> (with user feedback): Here, we invoke user feedback to select the most appropriate answer set among the top-<em>k</em> answer sets obtained using templates and, if none are appropriate, among the top-<em>k</em> obtained using the similarity function. We use gold answer labels provided with the benchmarks to simulate user feedback. We use <em>k</em> = 5 in both cases, as we find that it provides a good balance between recall and the number of answer sets a user needs to look at.<br/></li>     <li id="list8" label="&#x2022;"><strong>NEQA-No-User-Feedback</strong>: In this configuration, we perform continuous learning without user feedback. Instead, we take the top-ranked answer set in either of the two lists of answer sets above to be the correct one. We consider the list of answer sets obtained using the similarity function only if the list obtained using templates (ranked by the LTR function) is empty. This configuration demonstrates the quality of our template-based and similarity function-based answering mechanisms and helps understand the gap filled by user feedback.<br/></li>    </ul>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Results</h3>     </div>    </header>    <figure id="fig5">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186004/images/www2018-13-fig5.jpg" class="img-responsive" alt="Figure 5"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Performance of NEQA with and without user feedback as a function of batch number, on the WebQuestions (WQ) and ComplexQuestions (CQ) datasets.</span>     </div>    </figure>    <p>     <strong>Answering performance over time.</strong> Figure&#x00A0;<a class="fig" href="#fig5">5</a> shows how NEQA performs after deployment, as it receives user questions and invokes continuous learning where necessary. The cumulative average F1 scores in Figures&#x00A0;5a and&#x00A0;5d show an improvement over time for both benchmarks and for both feedback configurations. We compute the F1 score of a given batch after observing all questions in that batch, but before updating our ranking models based on that batch. The cumulative F1 score at batch <em>n</em> is the mean of these scores over all <em>n</em> batches. On both benchmarks, the general trend is for the cumulative F1 to increase as NEQA sees more questions and invokes continuous learning as needed to learn new templates and improve its ranking model. In general, the numbers on CQ are lower due to its more challenging nature stemming from multi-relation questions. We can see some fluctuation in the initial batches for both datasets. We attribute such variations to the modest ranking performance of the underlying LTR model during the very first iterations due to the small number of instances it was trained on. As expected, NEQA&#x0027;s F1 increases significantly when user feedback is invoked. We observe an F1 increase of 3.8 and 2.8 points over the no-feedback configuration for WQ and CQ, respectively.</p>    <p>     <strong>Augmentation of banks.</strong> NEQA extends its banks with new templates and question-query pairs over time. Figures&#x00A0;5b and&#x00A0;5e show the number of templates learned over time. Each template captures a distinct syntactic structure and its mapping to the appropriate semantic predicate-argument structure. Each template in the template bank corresponds to one or more question-query pairs in the question-query bank: it was either generated from such a pair during training or continuous learning, or was used to answer <em>u</em> in that pair by mapping it to <em>q</em>. In general, having more correct (<em>u</em>, <em>q</em>) pairs in the question-query bank means: (i) NEQA has learned more correct templates, and, (ii) NEQA can better transfer these new templates to new syntactic structures with semantics similar to that of <em>q</em>. Figures&#x00A0;5c and&#x00A0;5f show the numbers of <em>correctly answered</em> new (<em>u</em>, <em>q</em>) pairs for WQ and CQ, respectively with the two modes of feedback. A (<em>u</em>, <em>q</em>) pair is deemed correct if the gold answer set of <em>u</em> overlaps with the answer set of <em>q</em> when executed over the KB. For 1393 out of 2032 test questions in WQ (338 out of 800 in CQ), user feedback indeed yielded the ground-truth answer set.</p>    <p>Contrasting these figures gives interesting insights. For WQ, the number of templates obtained from user feedback is higher than that obtained without. This is because the similarity function does a good job at surfacing the correct answer set to the top-<em>k</em> from which the user selects the correct one. However, without feedback, the top-1 may be incorrect, in which case alignment between the corresponding query and the question at hand fails (as opposed to generating a spurious alignment), resulting in no templates. The question-query bank in the configuration with feedback contains more correct (<em>u</em>, <em>q</em>) pairs, meaning that we have more correct alignments (and hence less spurious templates). When looking at CQ, we see that the no-feedback configuration results in more templates. The reason here is that with the complexity of the dataset and the limitations of the similarity function, users are more likely to decide that no answer set produced through the similarity function is appropriate. In the no-feedback case, the topmost answer set from the similarity function is always chosen, and alignments (including spurious ones) are more likely here due to the length of the questions. Despite the large number of templates for the no-feedback configuration, we can see that NEQA with user feedback results in more correct (<em>u</em>, <em>q</em>) pairs (Figure&#x00A0;5f), indicating that the no-feedback configuration has more spurious templates than the one with feedback, as expected.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Performance of continuous learning-based methods on the WebQuestions test set. User Feedback is used to re-train the systems after each batch.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Method</strong>       </th>       <th style="text-align:center;">        <strong>Avg.</strong>       </th>       <th style="text-align:center;">        <strong>Avg.</strong>       </th>       <th style="text-align:center;">        <strong>Avg.</strong>       </th>       </tr>       <tr>       <th style="text-align:left;"/>       <th style="text-align:center;">        <strong>Prec.</strong>       </th>       <th style="text-align:center;">        <strong>Rec.</strong>       </th>       <th style="text-align:center;">        <strong>F1</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">QUINT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0001">1</a>] - No Feedback</td>       <td style="text-align:center;">25.5</td>       <td style="text-align:center;">30.2</td>       <td style="text-align:center;">25.7</td>       </tr>       <tr>       <td style="text-align:left;">QUINT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0001">1</a>] - Feedback</td>       <td style="text-align:center;">35.2</td>       <td style="text-align:center;">44.1</td>       <td style="text-align:center;">35.9</td>       </tr>       <tr>       <td style="text-align:left;">AQQU&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0006">6</a>] - No Feedback</td>       <td style="text-align:center;">24.5</td>       <td style="text-align:center;">29.6</td>       <td style="text-align:center;">24.8</td>       </tr>       <tr>       <td style="text-align:left;">AQQU&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0006">6</a>] - Feedback</td>       <td style="text-align:center;">36.3</td>       <td style="text-align:center;">45.2</td>       <td style="text-align:center;">37.6</td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">NEQA-No-User-Feedback</td>       <td style="text-align:center;">36.6</td>       <td style="text-align:center;">45.4</td>       <td style="text-align:center;">37.0</td>       </tr>       <tr>       <td style="text-align:left;">NEQA</td>       <td style="text-align:center;">40.6</td>       <td style="text-align:center;">49.5</td>       <td style="text-align:center;">40.8</td>       </tr>      </tbody>     </table>    </div>    <figure id="fig6">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186004/images/www2018-13-fig6.jpg" class="img-responsive" alt="Figure 6"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 6:</span>      <span class="figure-title">Performance of AQQU&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>], QUINT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>] and NEQA over the 20 batches of the WQ test set.</span>     </div>    </figure>    <p>     <strong>Comparison with state-of-the-art.</strong> An intuitive baseline for evaluating continuous learning in NEQA is to extend existing static-learning based methods to our setting of continuous learning through user feedback and periodic retraining. Concretely, we trained both QUINT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], and AQQU&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] on the same initial training seed as NEQA (300 question-answer pairs for WQ). Then, we streamed the 2032 test questions in batches of size 100, where user feedback is harnessed on the top-5 answer sets generated by the two systems. After each batch, the baseline systems were re-trained using the initial seed plus those questions from the batches seen thus far. Additionally, similar to NEQA-No-User-Feedback configuration, we performed continuous learning without user feedback for the two baselines.</p>    <p>Aggregate results over the WQ test questions are shown in Table&#x00A0;<a class="tbl" href="#tab2">2</a>, where NEQA outperformed both QUINT and AQQU in the two modes of operation (with and without user feedback). To gain more insights, the per-batch cumulative average F1 for the three systems in both configurations is depicted in Figures&#x00A0;6a and&#x00A0;6b . NEQA starts off with a high F1 score (36.5) in batch one, compared to AQQU and QUINT with F1 scores of 20.0 and 25.5, respectively. This is due to two reasons: (i) NEQA learns templates online and adds them directly to the template bank, while QUINT, for example, learns new templates after each batch and (ii) when a question has no matching templates, our similarity function is invoked and might be able to correctly answer the question at hand, and hence, positively affects the performance. AQQU disregards the syntax of questions and relies on three query templates with exhaustive instantiation. This makes the underlying ranking module of AQQU very crucial to the overall performance, which explains the low values for the very first batches (the ranking module was trained on a relatively small number of training instances). When user feedback is bypassed, the performance of the baselines deteriorates, as shown in Figure&#x00A0;6b, where the two baselines were not able to improve their performance over time. On the other hand, NEQA showed improvement in performance over time. We attribute this to our <em>unsupervised</em> similarity function, which is used to answer questions when the template-based answering mechanism fails and subsequently trigger the learning of new templates with new syntactic structures. The similarity function plays a vital role in distinguishing our system, built with continuous learning in mind, from systems where continuous learning is achieved through simple periodic retraining.</p>    <p>For completeness, we also show results for NEQA when used as a traditional static-learning based QA system, with distinct training and testing phases and with <em>continuous learning disabled</em>. Table <a class="tbl" href="#tab3">3</a> shows the results for NEQA and baseline systems on the WQ test set after training each on the full WQ training set. The results show that NEQA achieves competitive results, with no significant differences from the best system, but with the added advantage of being able to perform continuous learning when limited training data is available. As described in Sections&#x00A0;<a class="sec" href="#sec-6">4.1</a> and&#x00A0;<a class="sec" href="#sec-10">3</a>, NEQA is a continuous-learning extension of Abujabal et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], hence when used in the classical QA setup it obtains the same results as that work.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Performance of state-of-the-art static learning-based methods on the WebQuestions test set.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Method</strong>       </th>       <th style="text-align:center;">        <strong>Avg.</strong>       </th>       <th style="text-align:center;">        <strong>Avg.</strong>       </th>       <th style="text-align:center;">        <strong>Avg.</strong>       </th>       </tr>       <tr>       <th style="text-align:left;"/>       <th style="text-align:center;">        <strong>Prec.</strong>       </th>       <th style="text-align:center;">        <strong>Rec.</strong>       </th>       <th style="text-align:center;">        <strong>F1</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Berant et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0009">9</a>] (2013)</td>       <td style="text-align:center;">48.0</td>       <td style="text-align:center;">41.3</td>       <td style="text-align:center;">35.7</td>       </tr>       <tr>       <td style="text-align:left;">Yao and Van Durme&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0060">60</a>] (2014)</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">33.0</td>       </tr>       <tr>       <td style="text-align:left;">Bordes et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0013">13</a>] (2014)</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">39.2</td>       </tr>       <tr>       <td style="text-align:left;">Bast and Haussmann&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0006">6</a>] (2015)</td>       <td style="text-align:center;">49.8</td>       <td style="text-align:center;">60.4</td>       <td style="text-align:center;">49.4</td>       </tr>       <tr>       <td style="text-align:left;">Yih et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0061">61</a>] (2015)</td>       <td style="text-align:center;">52.8</td>       <td style="text-align:center;">60.7</td>       <td style="text-align:center;">52.5</td>       </tr>       <tr>       <td style="text-align:left;">Reddy et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0040">40</a>] (2016)</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">50.3</td>       </tr>       <tr>       <td style="text-align:left;">Savenkov et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0042">42</a>] (2016) (w/o text)</td>       <td style="text-align:center;">49.8</td>       <td style="text-align:center;">60.4</td>       <td style="text-align:center;">49.4</td>       </tr>       <tr>       <td style="text-align:left;">Xu et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0055">55</a>] (2016) (w/o text)</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">47.1</td>       </tr>       <tr>       <td style="text-align:left;">Abujabal et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0001">1</a>] (2017)</td>       <td style="text-align:center;">52.1</td>       <td style="text-align:center;">60.3</td>       <td style="text-align:center;">51.0</td>       </tr>       <tr>       <td style="text-align:left;">NEQA</td>       <td style="text-align:center;">52.1</td>       <td style="text-align:center;">60.3</td>       <td style="text-align:center;">51.0</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Open-domain question answering.</strong> NEQA exploits the interaction between syntax and semantics to perform continuous learning on questions coming while the system is deployed. This interaction allows NEQA to perform truly open-domain question answering, where it answers questions that require previously-unseen semantic predicates. To test how well NEQA performs on this task, we restrict the test questions from WQ to 693 questions whose corresponding query contains predicates from the following three domains: <tt>sports</tt>, <tt>government</tt> and <tt>people</tt>. Note that the domain information is encoded in the names of Freebase predicates (e.g., <tt>sports.sports_team.championships</tt>), allowing us to systematically make this restriction. We then removed the 56 questions with queries containing predicates from the above domains from the seed training set used for NEQA (resulting in 300 &#x2212; 56 = 244 new seed training examples). To provide a baseline, we used the best publicly available traditional KB-QA system of Bast and Haussmann&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] as a baseline. We train this system on the standard WQ training set with the 1315 questions from the three domains above excluded (a total of 3778 &#x2212; 1315 = 2463 training samples).</p>    <p>NEQA achieved an F1 score of 50.3 and 41.5 with and without user feedback, respectively, on the above test set. The system of Bast and Haussmann&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], AQQU, had an F1 score of 20.3. The exhaustive instantiation of the three query templates with all possible KB predicates explains the F1 score achieved by AQQU. This experiment shows that NEQA, in both modes of feedback, answered questions from domains it had never seen during the initial training with a high F1 on par with the results obtained without filtering the seed training set. We attribute these gains to a combination of (i) using templates that account for syntax and using lexicons for instantiating predicate-argument queries as a foundation for NEQA, (ii) re-training the underlying models using test questions which helps NEQA to adapt to the terminology of the new domain, and (ii) the extension of these methods to allow for continuous learning to account for new syntactic structures.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Analysis</h3>     </div>    </header>    <p>     <strong>Impact of templates and similarity function.</strong> We studied the two branches of NEQA individually: answering with templates, and via the similarity function when the basic answering with templates fails and continuous learning is triggered. Note that we cannot completely decouple both branches since the similarity function feeds our template-based answering with new templates over time, resulting in better coverage and performance. On WQ, with user feedback, 1184 questions were answered with templates, while 848 were answered via the similarity function. For the no-feedback configuration, 1788 out of 2032 were handled by the learned templates, and the similarity function answered 244 questions. The contrast between 1184 and 1788 shows cases where feedback helped weed out erroneous answers obtained through templates.</p>    <p>There are various possible failure cases in our pipeline. During the very first batches, the LTR model had a modest ranking performance due to the small number of examples used to train it. However, as more questions were observed, the ranking performance of the LTR model improved substantially, especially when user feedback was harnessed. The impact of this was either incorrect answering, triggering continuous learning, or, once continuous learning was triggered, no answer sets in the top-<em>k</em> being correct, triggering answering via the similarity function. In some cases, none of the generated queries that were fed to the LTR model was correct to start with. This is explained either by the lack of appropriate templates, especially in early batches, or the incompleteness of the underlying lexicons used to instantiate our templates with concrete SPARQL queries (Section&#x00A0;<a class="sec" href="#sec-9">2.3</a>). As future work, we plan to add textual resources to build better lexicons. In other cases, the NERD system failed to link mentions to the correct KB entities.</p>    <p>For some questions, our similarity function failed to retrieve semantically similar questions from our question-query bank. In some of these cases, our bank did not contain any question that is semantically similar to the question at hand. In other cases, although a correct similar question was retrieved, no new template was generated. Again, this is explained by deficiencies in our lexicons, or the NERD system we use.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">F1 scores for a component ablation analysis of our similarity function.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Components</strong>       </th>       <th style="text-align:center;">        <strong>NEQA</strong>       </th>       <th style="text-align:center;">        <strong>NEQA-No-User-Feedback</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Both</td>       <td style="text-align:center;">40.8</td>       <td style="text-align:center;">37.0</td>       </tr>       <tr>       <td style="text-align:left;">Only LM</td>       <td style="text-align:center;">38.3</td>       <td style="text-align:center;">35.1</td>       </tr>       <tr>       <td style="text-align:left;">Only word2vec</td>       <td style="text-align:center;">35.0</td>       <td style="text-align:center;">33.4</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Similarity function ablation study.</strong> Our similarity function consists of two components: (i) a language model (LM), and (ii) <em>word2vec</em> similarity. We conducted an ablation study to measure the effect of each component on the overall performance of the system. F1-scores are shown in Table&#x00A0;<a class="tbl" href="#tab4">4</a>. The highest F1 score is achieved when the two components are used. While the LM plays the most vital role, the word2vec component also contributes significantly to the final performance.</p>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Sample questions correctly answered via templates learned online.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:left;">        <em>&#x201C;what is the name of the currency used in italy?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x201C;what is the head judge of the supreme court called?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x201C;where did the battle of waterloo occur?&#x201D;</em>       </td>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tab6">     <div class="table-caption">      <span class="table-number">Table 6:</span>      <span class="table-title">Sample questions correctly answered using the similar questions retrieved from the question-query bank. Entities are generalized using [&#x2026;] placeholders.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:left;">        <strong>Question:</strong>       </td>       <td style="text-align:left;">        <em>&#x201C;what is the currency in [italy?]&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Most similar:</strong>       </td>       <td style="text-align:left;">        <em>&#x201C;what kind of money is used in [israel]?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">        <strong>Question:</strong>       </td>       <td style="text-align:left;">        <em>&#x201C;what films has [scarlett johansson] been in?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Most similar:</strong>       </td>       <td style="text-align:left;">        <em>&#x201C;what movies did [zoe saldana] play in?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">        <strong>Question:</strong>       </td>       <td style="text-align:left;">        <em>&#x201C;what was [sir isaac newton]&#x2019;s inventions?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Most similar:</strong>       </td>       <td style="text-align:left;">        <em>&#x201C;what inventions did [robert hooke] made?&#x201D;</em>       </td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Anecdotal results.</strong> Table <a class="tbl" href="#tab5">5</a> shows sample test questions from the WebQuestions that were correctly answered using templates learned online. These questions represent new syntactic structures that NEQA learned online. Table&#x00A0;<a class="tbl" href="#tab6">6</a> shows questions that were correctly answered using our similarity function together with the top-1 most similar question retrieved by the similarity function. For every pair of questions in this table, note the differing syntactic structures conveying similar semantics, e.g., <em>&#x2018;what is the currency&#x2019;</em> and <em>&#x2018;what kind of money&#x2019;</em>.</p>    </section>   </section>   <section id="sec-19">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Related Work</h2>    </div>    </header>    <p>    <strong>Question answering.</strong> KB-QA has seen broad interest in recent years with the wide availability and rapid growth of KBs and voice-based interaction with devices (e.g., Alexa, Cortana). We adopt a template-based approach for mapping syntactic structures to semantic predicate-argument structures&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0057">57</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0065">65</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0067">67</a>]. Another way of exploiting syntax is to use grammatical formalisms that derive syntax and semantics in tandem, most prominent among these being CCGs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>]. Some techniques disregard syntax altogether, and rely on combinatorial over-generation of queries followed by a ranking of such candidate queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0060">60</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0061">61</a>]. Finally, with the recent popularity of deep learning, some methods use large amounts of training data to learn a function for embedding questions and answer entities in a shared latent space&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0058">58</a>]. We opted to base NEQA on the systems that use syntax as they achieve better generalization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>], allowing the transfer of semantics between syntactic structures. We rely on dependency parsing for capturing syntax to exploit the rapid progress on this task&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. In contrast to all the above, our system uses continuous learning that allows starting from small training sets and improving over time.</p>    <p>The framing of the QA task depends on the type of the underlying data and associated annotations. An important QA setting is answering over textual corpora. One way to approach this is using traditional IR methods to retrieve relevant documents and extract passages or phrases that answer the question&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. Another way has been to use OpenIE to turn such corpora into open-vocabulary knowledge bases and answer over these&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. Finally, in a setting where both textual and structured data are used, hybrid approaches have been explored for QA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0055">55</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>].</p>    <p>In <em>entity search</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>], the user searches for a list of entities using keyword-based queries (e.g., <em>&#x2018;dutch artists paris&#x2019;</em>). The underlying corpus for retrieval may be Wikipedia pages&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], documents with Freebase annotations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>], or general RDF-stores&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>]. Techniques vary from probabilistic language models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>], query segmentation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>], to category models for entities&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>].</p>    <p>    <strong>Continuous learning and user feedback.</strong> Our work draws inspiration from the never-ending learning paradigm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] and its use case NELL (Never-Ending Language Learning) in machine reading [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. NEQA also leans on the principle of <em>online learning</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] where incoming questions are fed into the system in a sequential order, thus improving the system&#x0027;s performance over time.</p>    <p>User feedback has always been vital for IR systems: be it solely for evaluation as relevance judgments in the early days&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0049">49</a>], or in more implicit forms like clicks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] and reformulations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>] for improving personalized ranking models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] and automatically completing queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>]. User interactions play a key role in closing the loop in a continuous learning framework, where they improve the system iteratively. In the NELL system[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>], feedback is incorporated as periodic expert judgment on extracted beliefs. Recently, user feedback was leveraged on graph queries, and evaluated with simulated judgments&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>]. User feedback has been leveraged in natural language interfaces to databases (NLIDB), where Li et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] invoke user feedback to resolve ambiguous words/phrases in the users&#x2019; questions, while Iyer et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] ask expert users to provide a full SQL query that answers a question over a database.</p>    <p>    <strong>Question retrieval.</strong> NEQA relies on question retrieval to drive continuous learning when templates fail, where it looks for previously answered questions most similar to the current one. This is a central task in community question answering (CQA)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0063">63</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0064">64</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0066">66</a>], where the goal is to answer a user&#x0027;s question by presenting answers to similar questions that have already been answered. Various methods have been proposed, including those that use syntactic parse trees&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>] and language models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0063">63</a>]. Our method takes inspiration from the latter work.</p>    <p>Notions of similarity have been leveraged in KB-QA systems based on paraphrasing. Berant and Liang&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] use a supervised paraphrasing model that finds the logical form whose machine-generated verbalization best paraphrases the input question. Fader et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] perform QA over an open-predicate KB by learning a paraphrase model for rewriting a question to a set of similar canonical question forms, each of which maps to a unique query.</p>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>We presented NEQA, a continuous learning framework for KB-QA that relies on a combination of syntax-aware templates, a semantic similarity function, and judicious invocation of non-expert user feedback on answer sets. NEQA starts with a small seed training set, and exploits failure cases to improve its coverage, by using a similarity function to learn new templates with previously-unseen syntactic structures. Our experiments showed that (i) NEQA has a steady improvement in performance over time, (ii) NEQA is superior to the static learning-based methods with re-training, and (iii) NEQA performs truly open-domain QA.</p>    <p>Improving the similarity function to handle more implicit semantics is a promising direction of future work. Furthermore, we plan to investigate ways to bypass direct user feedback by relying on question reformulations.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Abdalghani Abujabal, Mohamed Yahya, Mirek Riedewald, and Gerhard Weikum. 2017. Automated Template Generation for Question Answering over Knowledge Graphs. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0002" label="[2]">Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving Web search ranking by incorporating user behavior information. In <em>      <em>SIGIR</em>     </em>.</li>    <li id="BibPLXBIB0003" label="[3]">Eugene Agichtein, Eric Brill, Susan Dumais, and Robert Ragno. 2006. Learning user interaction models for predicting Web search result preferences. In <em>      <em>SIGIR</em>     </em>.</li>    <li id="BibPLXBIB0004" label="[4]">Krisztian Balog, Marc Bron, and Maarten de Rijke. 2011. Query modeling for entity search based on terms, categories, and examples. <em>      <em>TOIS</em>     </em> (2011).</li>    <li id="BibPLXBIB0005" label="[5]">Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-Based Question Answering with Knowledge Graph. In <em>      <em>COLING</em>     </em>.</li>    <li id="BibPLXBIB0006" label="[6]">Hannah Bast and Elmar Haussmann. 2015. More Accurate Question Answering on Freebase. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0007" label="[7]">Emily&#x00A0;M. Bender, Dan Flickinger, Stephan Oepen, Woodley Packard, and Ann&#x00A0;A. Copestake. 2015. Layers of Interpretation: On Grammar and Compositionality. In <em>      <em>International Conference on Computational Semantics</em>     </em>.</li>    <li id="BibPLXBIB0008" label="[8]">Michael Bendersky, Xuanhui Wang, Donald Metzler, and Marc Najork. 2017. Learning from user interactions in personal search via attribute parameterization. In <em>      <em>WSDM</em>     </em>.</li>    <li id="BibPLXBIB0009" label="[9]">Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0010" label="[10]">Jonathan Berant and Percy Liang. 2014. Semantic Parsing via Paraphrasing. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0011" label="[11]">Roi Blanco, Peter Mika, and Sebastiano Vigna. 2011. Effective and Efficient Entity Search in RDF Data. In <em>      <em>ISWC</em>     </em>.</li>    <li id="BibPLXBIB0012" label="[12]">Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. In <em>      <em>SIGMOD</em>     </em>.</li>    <li id="BibPLXBIB0013" label="[13]">Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question Answering with Subgraph Embeddings. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0014" label="[14]">Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. <em>      <em>arXiv</em>     </em> (2015).</li>    <li id="BibPLXBIB0015" label="[15]">Eric Brill, Susan Dumais, and Michele Banko. 2002. An analysis of the AskMSR question-answering system. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0016" label="[16]">Qingqing Cai and Alexander Yates. 2013. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0017" label="[17]">Danqi Chen and Christopher&#x00A0;D. Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0018" label="[18]">Long Chen, Joemon&#x00A0;M. Jose, Haitao Yu, Fajie Yuan, and Dell Zhang. 2016. A Semantic Graph based Topic Model for Question Retrieval in Community Question Answering. In <em>      <em>WSDM</em>     </em>.</li>    <li id="BibPLXBIB0019" label="[19]">Jeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In <em>      <em>SIGIR</em>     </em>.</li>    <li id="BibPLXBIB0020" label="[20]">Andrew&#x00A0;Carlson et al.2010. Toward an Architecture for Never-Ending Language Learning. In <em>      <em>AAAI</em>     </em>.</li>    <li id="BibPLXBIB0021" label="[21]">S&#x00F6;ren&#x00A0;Auer et al.2007. DBpedia: A Nucleus for a Web of Open Data. In <em>      <em>ISWC</em>     </em>.</li>    <li id="BibPLXBIB0022" label="[22]">Tom M.&#x00A0;Mitchell et al.2015. Never-Ending Learning. In <em>      <em>Conference on AI</em>     </em>.</li>    <li id="BibPLXBIB0023" label="[23]">Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-Driven Learning for Open Question Answering. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0024" label="[24]">Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open Question Answering over Curated and Extracted Knowledge Bases. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0025" label="[25]">Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora. (2013).</li>    <li id="BibPLXBIB0026" label="[26]">Sanda&#x00A0;M. Harabagiu, Marius Pasca, and Steven&#x00A0;J. Maiorano. 2000. Experiments with Open-Domain Textual Question Answering. In <em>      <em>COLING</em>     </em>.</li>    <li id="BibPLXBIB0027" label="[27]">Marti&#x00A0;A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In <em>      <em>COLING</em>     </em>.</li>    <li id="BibPLXBIB0028" label="[28]">Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a Neural Semantic Parser from User Feedback. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0029" label="[29]">Jiwoon Jeon, W.&#x00A0;Bruce Croft, and Joon&#x00A0;Ho Lee. 2005. Finding similar questions in large question and answer archives. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0030" label="[30]">Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0031" label="[31]">Mandar Joshi, Uma Sawant, and Soumen Chakrabarti. 2014. Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0032" label="[32]">Jyrki Kivinen, Alexander&#x00A0;J Smola, and Robert&#x00A0;C Williamson. 2004. Online learning with kernels. <em>      <em>IEEE Trans. on Signal Processing</em>     </em>.</li>    <li id="BibPLXBIB0033" label="[33]">Jayant Krishnamurthy and Tom&#x00A0;M. Mitchell. 2015. Learning a Compositional Semantics for Freebase with an Open Predicate Vocabulary. <em>      <em>TACL</em>     </em> (2015).</li>    <li id="BibPLXBIB0034" label="[34]">Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke&#x00A0;S. Zettlemoyer. 2013. Scaling Semantic Parsers with On-the-Fly Ontology Matching. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0035" label="[35]">Fei Li and Hosagrahar&#x00A0;Visvesvaraya Jagadish. 2014. NaLIR: an interactive natural language interface for querying relational databases. In <em>      <em>SIGMOD</em>     </em>.</li>    <li id="BibPLXBIB0036" label="[36]">Liangda Li, Hongbo Deng, Anlei Dong, Yi Chang, Ricardo Baeza-Yates, and Hongyuan Zha. 2017. Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0037" label="[37]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. <em>      <em>arXiv</em>     </em> (2013).</li>    <li id="BibPLXBIB0038" label="[38]">Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0039" label="[39]">Filip Radlinski and Thorsten Joachims. 2005. Query chains: learning to rank from implicit feedback. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0040" label="[40]">Siva Reddy, Oscar T&#x00E4;ckstr&#x00F6;m, Michael Collins, Tom Kwiatkowski, Dipanjan Das, Mark Steedman, and Mirella Lapata. 2016. Transforming Dependency Structures to Logical Forms for Semantic Parsing. <em>      <em>TACL</em>     </em>.</li>    <li id="BibPLXBIB0041" label="[41]">Kiril Ribarov. 2004. Review: Open-Domain Question Answering from Large Text Collections, by Marius Pasca. <em>      <em>Prague Bull. Math. Linguistics</em>     </em>(2004).</li>    <li id="BibPLXBIB0042" label="[42]">Denis Savenkov and Eugene Agichtein. 2016. When a Knowledge Base Is Not Enough: Question Answering over Knowledge Bases with External Text Data. In <em>      <em>SIGIR</em>     </em>.</li>    <li id="BibPLXBIB0043" label="[43]">Uma Sawant and Soumen Chakrabarti. 2013. Learning Joint Query Interpretation and Response Ranking. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0044" label="[44]">Yu Su, Shengqi Yang, Huan Sun, Mudhakar Srivatsa, Sue Kase, Michelle Vanni, and Xifeng Yan. 2015. Exploiting relevance feedback in knowledge graph search. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0045" label="[45]">Fabian&#x00A0;M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0046" label="[46]">Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai, Jingjing Liu, and Ming-Wei Chang. 2015. Open Domain Question Answering via Semantic Enrichment. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0047" label="[47]">Thanh Tran, Philipp Cimiano, Sebastian Rudolph, and Rudi Studer. 2007. Ontology-Based Interpretation of Keywords for Semantic Search. In <em>      <em>ISWC</em>     </em>.</li>    <li id="BibPLXBIB0048" label="[48]">Christina Unger, Lorenz B&#x00FC;hmann, Jens Lehmann, Axel-Cyrille&#x00A0;Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0049" label="[49]">Ellen&#x00A0;M. Voorhees. 2001. The Philosophy of Information Retrieval Evaluation. In <em>      <em>CLEF</em>     </em>.</li>    <li id="BibPLXBIB0050" label="[50]">Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009. A syntactic tree matching approach to finding similar questions in community-based QA services. In <em>      <em>SIGIR</em>     </em>.</li>    <li id="BibPLXBIB0051" label="[51]">Sida&#x00A0;I. Wang, Percy Liang, and Christopher&#x00A0;D. Manning. 2016. Learning Language Games through Interaction. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0052" label="[52]">Yushi Wang, Jonathan Berant, and Percy Liang. 2015. Building a Semantic Parser Overnight. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0053" label="[53]">Keenon Werling, Arun&#x00A0;Tejasvi Chaganty, Percy Liang, and Christopher&#x00A0;D. Manning. 2015. On-the-Job Learning with Bayesian Decision Theory. In <em>      <em>Conference on Neural Information Processing Systems</em>     </em>.</li>    <li id="BibPLXBIB0054" label="[54]">Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Hybrid Question Answering over Knowledge Base and Free Text. In <em>      <em>COLING</em>     </em>.</li>    <li id="BibPLXBIB0055" label="[55]">Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Question Answering on Freebase via Relation Extraction and Textual Evidence. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0056" label="[56]">Mohamed Yahya, Denilson Barbosa, Klaus Berberich, Qiuyue Wang, and Gerhard Weikum. 2016. Relationship Queries on Extended Knowledge Graphs. In <em>      <em>WSDM</em>     </em>.</li>    <li id="BibPLXBIB0057" label="[57]">Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2013. Robust question answering over the web of linked data. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0058" label="[58]">Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-Chang Rim. 2014. Joint Relational Embeddings for Knowledge-based Question Answering. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0059" label="[59]">Yi Yang and Ming-Wei Chang. 2015. S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0060" label="[60]">Xuchen Yao and Benjamin&#x00A0;Van Durme. 2014. Information Extraction over Structured Data: Question Answering with Freebase. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0061" label="[61]">Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0062" label="[62]">Pengcheng Yin, Nan Duan, Ben Kao, Junwei Bao, and Ming Zhou. 2015. Answering Questions with Complex Semantic Constraints on Open Knowledge Bases. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0063" label="[63]">Kai Zhang, Wei Wu, Fang Wang, Ming Zhou, and Zhoujun Li. 2016. Learning Distributed Representations of Data in Community Question Answering for Question Retrieval. In <em>      <em>WSDM</em>     </em>.</li>    <li id="BibPLXBIB0064" label="[64]">Kai Zhang, Wei Wu, Haocheng Wu, Zhoujun Li, and Ming Zhou. 2014. Question Retrieval with High Quality Answers in Community Question Answering. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0065" label="[65]">Weiguo Zheng, Lei Zou, Xiang Lian, Jeffrey&#x00A0;Xu Yu, Shaoxu Song, and Dongyan Zhao. 2015. How to Build Templates for RDF Question/Answering: An Uncertain Graph Similarity Join Approach. In <em>      <em>Conference on Management of Data</em>     </em>.</li>    <li id="BibPLXBIB0066" label="[66]">Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0067" label="[67]">Lei Zou, Ruizhe Huang, Haixun Wang, Jeffrey&#x00A0;Xu Yu, Wenqiang He, and Dongyan Zhao. 2014. Natural language question answering over RDF: A graph data driven approach. In <em>      <em>Conference on Management of Data</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186004">https://doi.org/10.1145/3178876.3186004</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
