<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Sharing Deep Neural Network Models with Interpretation</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Sharing Deep Neural Network Models with Interpretation</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Huijun</span>     <span class="surName">Wu</span>,     UNSW and Data61, CSIRO Sydney, Australia, NSW 2032, <a href="mailto:huijunw@cse.unsw.edu.au">huijunw@cse.unsw.edu.au</a>    </div>    <div class="author">     <span class="givenName">Chen</span>     <span class="surName">Wang</span>,     Data61,CSIRO Sydney, Australia, NSW 2015, <a href="mailto:chen.wang@data61.csiro.au">chen.wang@data61.csiro.au</a>    </div>    <div class="author">     <span class="givenName">Jie</span>     <span class="surName">Yin</span>,     The University of Sydney Sydney, Australia, <a href="mailto:jie.yin@sydney.edu.au">jie.yin@sydney.edu.au</a>    </div>    <div class="author">     <span class="givenName">Kai</span>     <span class="surName">Lu</span>,     NUDT Changsha, China, <a href="mailto:kailu@nudt.edu.cn">kailu@nudt.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Liming</span>     <span class="surName">Zhu</span>,     UNSW and Data61, CSIRO Sydney, Australia, <a href="mailto:limingz@cse.unsw.edu.au">limingz@cse.unsw.edu.au</a>    </div>                        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3185995" target="_blank">https://doi.org/10.1145/3178876.3185995</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Despite outperforming humans in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, where the model is developed by a third party. For a supervised machine learning model, sharing training process including training data is a way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight into a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated deep neural network models with high fidelity. We show that data point pairs in the tree give users significantly better understanding of the model decision boundaries and paves the way for trustworthy model sharing.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Neural networks;</strong> &#x2022;<strong> Software and its engineering </strong>&#x2192; <em>Open source model;</em></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Deep Neural Networks</small>, </span>     <span class="keyword">      <small> Model Sharing</small>, </span>     <span class="keyword">      <small> Interpretability</small>, </span>     <span class="keyword">      <small> Decision Boundary</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Huijun Wu, Chen Wang, Jie Yin, Kai Lu, and Liming Zhu. 2018. Sharing Deep Neural Network Models with Interpretation. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3185995" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3185995</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Complicated machine learning models like deep neural networks (DNN) have achieved great success in image classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>], speech recognition&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>], and classic games such as Go&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] in recent years. The success inspires the use of DNN in a rapidly increasing number of applications. Training a DNN model, however, often requires large amounts of labeled data and non-trivial efforts of tuning. Sharing a trained model is cost-effective. As shown in Figure <a class="fig" href="#fig1">1</a>, machine learning models can be hosted in the cloud and run as a service in a Pay-As-You-Go model. The model developers train models using the data they collect. They may use machine learning as service platforms (MLaaS) such as Google CloudML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], AmazonML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] or Microsoft AzureML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] to develop models and manage training data in the cloud. The application developers may then integrate these models into their applications through prediction APIs. However, a complicated machine learning model, particularly a DNN model remains a black-box and the model quality is difficult to assess, especially when the distribution of application data differs from that of the training data. Commonly used accuracy and confidence values are not sufficient to reveal model behaviors on the data collected from different sources. A simple example is that the confidence value of classifying a data point belonging to an unseen class by the model can be high (see Fig.&#x00A0;<a class="fig" href="#fig13">13</a>). MLaaS platforms do not have effective means to address this problem, thus making it challenging for users to trust a shared model. <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">The Model Sharing Scenario: A model user calls prediction APIs to get predictions of her/his input data. The prediction is based on deep neural network models developed by the third party model developers on the data invisible to the model user.</span>     </div>    </figure>    </p>    <p>Providing means to interpret a model is effective to improve model transparency and help model users to understand the potential weakness of the models on their data. There has been a growing interest in exploring the interpretability for DNNs.</p>    <p>A common approach is to find examples or prototypes to guide decision making, which is along the line of case-based reasoning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. This approach identifies training data points that are close to data points within their own classes and far away from those in different classes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. Prototype finding is solved as a set cover optimization problem in &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. This approach is complemented by criticism finding as prototypes give little information to interpret the data points that do not fit a model well. MMD-critic&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] intends to find outliers in a class that differ the most to other data points belonging to the same class. These outliers are called criticisms. However, criticisms do not contain sufficient information to give users hints about why a model classifies a data point into one class, not another. Without information about similar data points in the neighboring classes, it is still difficult for a user to understand crucial model predictions around class boundaries. In other words, characterizing the differences between classes is able to explain these predictions.</p>    <p>Another approach is to mimic a complicated model using an interpretable simple model such as decision tree&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>]. The limitation of this approach is that it often requires structured training data. When the structure of data is complicated, a decision tree itself can be difficult to interpret.</p>    <p>Visualization is also used to examine features extracted by hidden neurons in DNNs for users to infer the relationship between a classification decision and these features &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>]. The sample perturbation approach&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] also attempts to understand features leading to certain predictions in image classification. The basic idea is to learn an image perturbation mask that minimizes a class score. The computational cost for perturbation based methods is non-trivial. Moreover, these methods rely on the change of confidence values to infer the influence of a changing individual feature. The change in confidence values may not accurately reflect the influence of features on predictions when there is a <em>concept drift</em> in data, which can be particularly common in model sharing or open set classification scenarios&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. Moreover, in model sharing scenarios, model users may not have the access to the internal structure of a model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>].</p>    <p>Along a similar line, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] proposes to use influence functions to identify the most important training data leading to certain predictions. This method requires the access to the whole training dataset, which is unrealistic in the model sharing settings.</p>    <p>In this paper, we propose a method to enhance users&#x2019; understanding of a shared DNN model by providing a small set of training data that characterizes the model decision boundaries. These data points are informative for users to infer how a prediction is made in relation to them. Our method is relatively simple yet effective. Specifically, we employ a max-margin based approach to select the most representative training data that largely contributes to the forming of the decision boundaries of a DNN model. These training data points are organized via an Explicable Boundary Tree (EB-tree) based on the distances in the DNN transformed space. The EB-tree data structure embodies DNN decision boundaries. Thus, traversing data points in the EB-tree is able to approximate the predictions of the DNN model on these points with high fidelity.</p>    <p>EB-tree has the following advantages: First, it extracts training data points that characterize decision boundaries so that it is able to explain why a test data point is classified into one class, not the other through the traversal of the test point along training data pairs with different labels in the tree. These pairs reveal the difference between a test data point to representative training data belonging to different classes to understand how the classification decision is made by the DNN model. Second, interpreting classification results by traversing the EB-tree does not require the access of the internal structure of the DNN model, thus suitable for model sharing scenarios. Third, EB-tree is computationally efficient since it only needs to pass the training data once, which makes it scalable for dealing with large models.</p>    <p>Our experimental results show that EB-tree is able to achieve a high fidelity to the DNN model by only disclosing a small set of training data. In addition, we show through a human pilot study that the traversal process of a test point in the tree clearly improves model users&#x2019; understanding about how predictions are made by DNN models. Compared to methods like MMD-critic, the boundary traversal in the tree can help users to better understand the classification results. We also demonstrate that in addition to giving model users insight into DNN model decision boundaries, EB-tree can also be used for identifying mislabeled training data and improving the efficiency of emerging new class detection, both of which are useful for understanding shared models.</p>    <p>The rest of the paper is organized as follows: Section&#x00A0;<a class="sec" href="#sec-7">2</a> describes the decision boundary of a DNN model and the process of constructing an explicable boundary tree (EB-tree) associated with a shared deep learning model; Section&#x00A0;<a class="sec" href="#sec-11">3</a> gives algorithms to approximate a model using EB-tree; Section&#x00A0;<a class="sec" href="#sec-14">4</a> presents evaluation results and Section&#x00A0;<a class="sec" href="#sec-22">5</a> concludes the paper.</p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Explicable Boundary Tree</h2>    </div>    </header>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Interpretable Decision Boundaries</h3>     </div>    </header>    <p>To interpret a DNN model, one effective way is to link the numerical values in a prediction made by the model to the training data related to this prediction. This process characterizes model decision boundaries using these training data points and ensures these boundaries are consistent with model predictions. We call the boundaries <em>interpretable decision boundaries</em>.</p>    <p>In the DNN transformed space, the interpretable decision boundaries formed by the training data points with different labels can be computed using a Voronoi diagram&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. Specifically, let <em>P</em> = {<em>P<sub>k</sub>     </em>|<em>k</em> &#x2208; <em>K</em>} denote a set of distinct points in space <em>X</em>, a Voronoi diagram of <em>P</em> is the subdivision of the space into <em>K</em> regions. Data points in a region, denoted by <em>R<sub>k</sub>     </em>, have the following property: their distance to <em>P<sub>k</sub>     </em> is not greater than their distance to any <em>P<sub>j</sub>     </em>, <em>j</em> &#x2260; <em>k</em>. Formally, given a distance function <em>d</em>(<em>x</em>, <em>P<sub>k</sub>     </em>), a region in a Voronoi diagram is defined as below: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ R_{k} = \lbrace x \in X | \forall j\ne k, d(x, P_{k} \le d(x, P_{j}))\rbrace . \] </span>       <br/>      </div>     </div>    </p>    <p>We give an example to show how an interpretable boundary between two classes in a DNN transformed space is characterized. As shown in Figure <a class="fig" href="#fig2">2</a>, training data points in grey and white region have different labels. We build a Voronoi diagram for all these points (<em>K</em> equals to the total number of training data points) through Delaunay triangulation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>], shown by orange lines in Figure&#x00A0;<a class="fig" href="#fig2">2</a>. Delaunay triangulation is the dual graph of a Voronoi diagram and is commonly used for Voronoi diagram construction. With a Voronoi diagram for all the points constructed, the interpretable decision boundary between the two classes is a set of connected edges shared by the neighboring regions of different classes. To gain a different level of insight into the DNN model, the interpretable decision boundaries can be constructed in the latent or the output layer of the DNN transformed space. <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">An interpretable decision boundary constructed using a Voronoi diagram.</span>      </div>     </figure>    </p>    <p>However, computing Delaunay triangulation on <em>n</em> points in <span class="inline-equation"><span class="tex">$\mathbb {R}^{d}$</span>     </span> has the time complexity of <em>O</em>(<em>n</em>     <sup>&#x2308;<em>d</em>/2&#x2309;</sup>)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. It scales poorly with the increase of dimensions (<em>d</em>) and takes a long time if not impossible to compute for a moderate dataset. To address this problem, we give a method called Explicable Boundary Tree (EB-tree) to approximate interpretable decision boundaries (referred as decision boundaries for simplicity in the rest of the paper). <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Architecture of Explicable Boundary Tree: A model user calls the prediction APIs to calculate the DNN transformed representations of input data. The representations traverse the boundary tree for predictions and interpretations.</span>      </div>     </figure>    </p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Boundary Tree</h3>     </div>    </header>    <p>We use edges that cross the boundaries and their associated nodes in the Delaunay triangulation to approximate the decision boundaries. The boundary tree is a data structure to achieve this. The boundary tree (forest) algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] is initially proposed for fast online learning. Each node of the boundary tree represents a training point. For a query of training point <em>y</em>, the algorithm looks up the closest node <em>x</em> to <em>y</em> and uses the label of <em>x</em> to predict the class of <em>y</em>. If <em>x</em> has the same label with <em>y</em>, <em>y</em> is discarded. Otherwise, it is added to the tree. The process repeats for each training data point. A test data point is classified by the label of its closest node in the boundary tree. As each edge in the tree crosses a decision boundary, all the nodes on the boundary tree, in essence, sketches the contours of the decision boundaries. Similar to the edges that cross the decision boundary in Delaunay triangulation, the difference between two end nodes of an edge on the boundary tree can, therefore, serve as a local explainer to a prediction close to the boundary.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Explicable Boundary Tree</h3>     </div>    </header>    <p>Even though its edges provide certain hints about the decision boundary between two classes, a basic boundary tree has three main limitations to support the interpretability of a shared DNN model: firstly, the tree may have relatively low fidelity to the DNN model in decision making; secondly, the number of training data points in the tree is not optimized and there is plenty of room to reduce the number to avoid unnecessary training data disclosure; thirdly, the training data point selection does not characterize a boundary clearly as two data points far away from the boundary may be connected by an edge. Some recent work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] intends to further learn a distance metric of boundary tree edges using DNN. Specifically, for a given boundary tree structure, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] provides a DNN to transform the data into a boundary tree-friendly representation. Note that the DNN used in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] is not optimized for classification directly. By contrast, for a given DNN optimized for classification, EB-tree aims to provide a boundary tree that approximates the decision boundaries of the DNN classifier accurately with a small number of training data points. EB-tree intends to address these limitations of the basic boundary tree.</p>    <p>Figure <a class="fig" href="#fig3">3</a> shows the architecture of EB-tree consisting of a DNN classifier <em>f</em> and an optimized boundary tree. The model translation module is responsible for constructing a boundary tree <em>T</em> to mimic the decision making of the DNN classifier. Each node of <em>T</em> is a training data point that is close to the decision boundary in the training dataset <em>D</em>. The DNN classifier transforms training data <em>t</em> &#x2208; <em>D</em> to representation <em>f</em>(<em>t</em>), which is a transformed feature vector. The features can be derived from different hidden layers or the output layer. We use the output of <em>softmax</em> layer in the DNN as the transformed representation of a training data point. The Euclidean distance is used to measure the distance between <em>f</em>(<em>t</em>1) and <em>f</em>(<em>t</em>2).</p>    <p>EB-tree selects a small portion of training data points that are close to the decision boundaries to approximate a DNN model. These data points are helpful for model users to gain insight into the key differences between different classes. For a test data point, traversing the tree to reach its closest training point pairs provides an interpretation of the decision choice of the model. To construct an EB-tree with good interpretability, it is important to ensure that the distance between two nodes connected by an edge is short. As an edge crosses the boundary, a short edge approximates the boundary with a narrow margin and gives a model user better visualization of the boundary. The vanilla boundary tree is trained in an online manner and it is impossible to change the order of the nodes fed into the tree. EB-tree, however, can leverage a carefully-designed training order to improve its ability to mimic the decision boundary of a DNN model. Specifically, we give a training data ordering algorithm for better boundary characterization in the following section. The algorithm produces high accuracy and fidelity to the DNN model.</p>    <p>Once constructed, the EB-tree is able to answer queries of test data points from a model user as follows: A query sample <em>y</em> is firstly transformed by the DNN model to <em>f</em>(<em>y</em>); then a traversal process locates the closest node to <em>f</em>(<em>y</em>) in the tree, denoted by <em>x</em>; finally, the label of <em>x</em> is used to predict the class of <em>y</em>. The traversal path is used as an explainer of the prediction.</p>    </section>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Model Translation</h2>    </div>    </header>    <p>The model translation module in Figure&#x00A0;<a class="fig" href="#fig3">3</a> is responsible for identifying the most representative training data that characterizes the decision boundaries of a DNN model and constructing an EB-tree to approximate the decision boundaries.</p>    <p>The basic boundary tree algorithm is not able to achieve sufficient fidelity as shown in our experiment (Table&#x00A0;<a class="tbl" href="#tab1">1</a>), neither does it optimize the size of the tree, which may lead to unnecessary training data disclosure. We address this problem by training point reordering and boundary stitching described as below.</p>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Training Data Point Reordering</h3>     </div>    </header>    <p>Figure <a class="fig" href="#fig4">4</a> gives an example to illustrate the training point selection problem. Node <em>B</em> may add either node <em>A</em> or <span class="inline-equation"><span class="tex">$A^{^{\prime }}$</span>     </span> as its child. The choice leads to different classification results for query <em>Q</em>. Specifically, when <span class="inline-equation"><span class="tex">$A^{^{\prime }}$</span>     </span> is selected, query of test data point <em>Q</em> is classified as class A because it is closer to node <em>A</em> (on the left of dash line <span class="inline-equation"><span class="tex">$l^{^{\prime }}$</span>     </span>); otherwise, it is misclassified because <em>Q</em> is closer to node <em>B</em> than <em>A</em>. <figure id="fig4">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">A boundary tree for a two-class classification problem. <em>A</em> and <span class="inline-equation"><span class="tex">$A^{^{\prime }}$</span>       </span> are two possible children for node <em>B</em>. <em>Q</em> is a test point.</span>      </div>     </figure>    </p>    <p>Selecting training data pairs close to the boundary from a different side and close to each other reduces the probability a test data point is misclassified, thereby enabling the constructed tree to achieve high fidelity to the DNN model. In order to do this, we need to derive a metric to characterize the distance between a data point and a boundary.</p>    <p>EB-tree uses a support vector machine to measure the distance. Since the DNN-transformed space can often be considered linearly separable, we first use a support vector machine to get the decision boundaries. Let <em>w</em> denote a vector orthogonal to a decision boundary, <em>b</em> denote a scalar &#x201C;offset&#x201D; and {<em>x<sub>i</sub>     </em>, <em>y<sub>i</sub>     </em>} denote the DNN transformed representations of training points and the corresponding predicted label given by the DNN model. Given a set of training data points <em>x</em>, the decision boundary of two classes can be represented as <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w^{T}x + b = 0. \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> The margin from the boundary to the nearest data points on each side of the boundary satisfies <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} (w^{T}x_{i}+b)\cdot y_{i} \ge 1. \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>    </p>    <p>The margin between two classes is, therefore, <span class="inline-equation"><span class="tex">$d = \frac{2}{||w||}$</span>     </span>. We identify a boundary that maximizes the margin between two classes, which is equivalent to minimizing ||<em>w</em>||. We use the one-vs-all scheme&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>] to obtain a boundary for each class based on max-margin. For each training data point (<em>x</em>, <em>y</em>) in the DNN transformed space, we compute its minimal distance to the decision boundary of its corresponding class. Training points are sorted in ascending order according to their distances to boundaries.</p>    <p>The EB-tree construction process fetches sorted data points to insert into the tree. The order has a significant impact on both node number and interpretability of the boundary tree constructed. Consider that training points are randomly included in the tree, a training data point that is far away from a decision boundary is likely to be added to the tree first. This may result in the discard of the subsequent adjacent data points that are closer to the boundary because they share the same label of the node already in the tree. Our experiments show that random order tends to include many nodes in the tree but achieving sub-optimal model mimicking performance.</p>    <p>On the interpretability aspect, randomly ordered data points are likely to include many long edges in the boundary tree, which make the feature or visual difference between a parent and a child node difficult to infer. In contrast, an ascending order can keep the data points near a boundary in the constructed tree, meanwhile avoiding the inclusion of long edges. It is also likely to discard data points far away from boundaries. The classification of these data points is often consistent with human intuition and including them in the tree does not contribute to boundary characterization much. This approach helps to minimize the training data disclosure. <figure id="fig5">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">The comparison of distance increasing order and boundary stitching order on constructed boundary trees: the numbers on nodes follow the increasing order of the distances to decision boundaries.</span>      </div>     </figure>    </p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Boundary Stitching Algorithm</h3>     </div>    </header>    <p>Simply inserting training data points according to the ascending order of their distances to boundaries is not sufficiently effective for constructing a boundary tree with good interpretability. It is mainly due to that two training points with similar distances to the boundary may be located at different ends of the boundary and far away from each other. The features of such two points are unlikely to share sufficient commonality for a model user to understand the decision boundary. Figure 5a illustrates the case with an example, in which data points like node 3, 6 and 7 are not included in the tree because their corresponding closest nodes with the same label but closer to the boundary are already in the tree at the time they are being processed. In the following, we give a boundary stitching algorithm to address this problem and the algorithm aims o construct a tree well approximating the boundary as illustrated in Figure&#x00A0;5b .</p>    <p>The boundary stitching algorithm (Algorithm 1) considers the distances between the current node in the tree and the candidate points to insert. It prioritizes candidate points with a different label based on their distances to the current node in the tree. The algorithm first computes the DNN transformed representations of training data points and their distances to the boundaries via max-margin. It then sorts these data points according to the ascending order of their distances to the boundaries. The construction of the boundary tree starts from the node with the shortest distance to a boundary. The node is inserted into the tree as the root. A search for <em>k</em>-nearest neighbors (kNNs) of the newly added node in the tree is then performed. If there is a neighboring data point belonging to a different class compared with the current node, the closest one to the current node is selected to insert into the tree. The insertion process traverses the selected data point to its closest node. If the data point has a different label with its closest node in the tree, it is inserted as a child of this node; otherwise, the data point is discarded as a similar node belonging to the same class has already been added to the tree. The process continues until all data points are processed.</p>    <p>     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>Finding <em>k</em> nearest neighbors for massive high-dimensional data incurs high computing cost. Hence, we use <em>locality sensitive hashing</em> (LSH) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] to reduce the cost. For an n-point dataset in <em>d</em>-dimensional space, doing so achieves query computing complexity of <em>O</em>(<em>dn</em>     <sup>      <em>&#x03C1;</em> + <em>O</em>(1)</sup>) where <span class="inline-equation"><span class="tex">$\rho = \frac{1}{2c^{2} - 1}$</span>     </span> for the Euclidean distance and approximation <em>c</em> > 1.</p>    </section>   </section>   <section id="sec-14">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Evaluation</h2>    </div>    </header>    <p>We evaluate the effectiveness of EB-tree on three image classification tasks. The shared models are a convolutional net (CNN)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] for MNIST hand-written digit dataset, all convolutional net (All-CNN) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] for CIFAR-10 dataset and Inception-v3 network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] for ImageNet dataset&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. We set <em>k</em> in Algorithm&#x00A0;1 and the maximum number of children for each node in EB-trees based on the number of classes in the datasets. We evaluate our algorithm on the following two aspects: model mimicking performance and the interpretability of the constructed EB-tree. The model mimicking performance is measured by the decision consistency between an EB-tree and its corresponding DNN classifier, or <em>fidelity</em>. The metric we use is the <em>F-measure</em> of the EB-tree predictions against the DNN model predictions. We measure the interpretability of EB-trees by conducting a human pilot study.</p>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Model Accuracy and Fidelity</h3>     </div>    </header>    <section id="sec-16">     <header>      <div class="title-info">       <h4>       <span class="section-number">4.1.1</span> Boundary Tree and EB-tree</h4>      </div>     </header>     <div class="table-responsive" id="tab1">      <div class="table-caption">       <span class="table-number">Table 1:</span>       <span class="table-title">Comparison of boundary tree, EB-tree and the original DNN classifiers. <em>SD</em> is standard deviation.</span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:center;">Dataset</th>        <th style="text-align:center;">Model</th>        <th style="text-align:center;">Error Rate</th>        <th style="text-align:center;">Fidelity</th>        <th style="text-align:center;">Error Fidelity</th>        <th style="text-align:center;">Avg. Edge Length</th>        <th style="text-align:center;">Parameters</th>        <th style="text-align:center;"># of Nodes</th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">CNN</td>        <td style="text-align:center;">0.53%</td>        <td style="text-align:center;">100%</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;"/>        <td style="text-align:center;">1.66M</td>        <td style="text-align:center;">-</td>       </tr>       <tr>        <td style="text-align:center;">MNIST</td>        <td style="text-align:center;">Boundary Tree</td>        <td style="text-align:center;">0.57%(SD=0.04%)</td>        <td style="text-align:center;">99.81(SD=0.07%)</td>        <td style="text-align:center;">92.45% (SD=3.7%)</td>        <td style="text-align:center;">0.998 (SD=0.023)</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;">46(SD=7)</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">EB-tree</td>        <td style="text-align:center;">0.53%</td>        <td style="text-align:center;">99.90%</td>        <td style="text-align:center;">94.33%</td>        <td style="text-align:center;">0.716</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;">21</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">ALL-CNN</td>        <td style="text-align:center;">7.90%</td>        <td style="text-align:center;">100%</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;"/>        <td style="text-align:center;">1.37M</td>        <td style="text-align:center;">-</td>       </tr>       <tr>        <td style="text-align:center;">CIFAR-10</td>        <td style="text-align:center;">Boundary Tree</td>        <td style="text-align:center;">7.87%(SD=0.33%)</td>        <td style="text-align:center;">98.88%(SD=0.27%)</td>        <td style="text-align:center;">86.96% (SD = 4.05%)</td>        <td style="text-align:center;">0.259 (SD=0.055)</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;">277(SD=34)</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">EB-tree</td>        <td style="text-align:center;">7.73%</td>        <td style="text-align:center;">99.12%</td>        <td style="text-align:center;">93.79%</td>        <td style="text-align:center;">0.165</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;">145</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Inception-v3</td>        <td style="text-align:center;">22.05%</td>        <td style="text-align:center;">100%</td>        <td style="text-align:center;"/>        <td style="text-align:center;"/>        <td style="text-align:center;">24.7M</td>        <td style="text-align:center;">-</td>       </tr>       <tr>        <td style="text-align:center;">ImageNet</td>        <td style="text-align:center;">Boundary Tree</td>        <td style="text-align:center;">24.97%(SD=0.51%)</td>        <td style="text-align:center;">87.82%(SD=4.16%)</td>        <td style="text-align:center;">66.29% (SD = 7.03%)</td>        <td style="text-align:center;">0.501 (SD = 0.046)</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;">43986(SD=3572)</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">EB-tree</td>        <td style="text-align:center;">22.51%</td>        <td style="text-align:center;">97.05%</td>        <td style="text-align:center;">91.87%</td>        <td style="text-align:center;">0.371</td>        <td style="text-align:center;">-</td>        <td style="text-align:center;">11927</td>       </tr>       </tbody>      </table>     </div>     <p>As shown by the error rate in Table <a class="tbl" href="#tab1">1</a>, EB-tree achieves a comparable accuracy of the DNN classifiers. EB-tree also approximates the DNN classifiers with high fidelity (99.90% with MNIST-CNN and 99.12% with CIFAR-ALL-CNN). Expectedly, EB-tree significantly outperforms the original boundary tree algorithm in terms of accuracy and fidelity with a smaller number of nodes in the tree. It is worth noting that, for the MNIST dataset, the resulting EB-tree only needs to disclose 21 out of 60,000 training data points (0.035%) to show model users interpretable decision boundaries. For CIFAR-10 dataset, EB-tree only needs to disclose 145 out of the 50,000 training data points (0.29%) to model users. For the ImageNet dataset with 1.28 million images and 1,000 classes, EB-tree achieves a low error rate (22.51%) and a high fidelity (97.05%) with 11,927 training data points to disclose. Note that for an EB-tree with a large number of children at each node (e.g., EB-tree for ImageNet-Inception-v3), traversing the tree may lead to a local closest node, so that the final node might share little similarity with the test data point. The boundary tree algorithm solves this by building a forest. This approach is infeasible for EB-tree as it aims to reduce the number of training data points to disclose. To address this, we allow a global nearest neighbor search when the final node of the traversal path has a different label to the prediction of the test data point.</p>     <p>We further examine classification errors made by EB-tree and the original boundary tree. We measure whether a test data point is likely to be misclassified to the same class as the model. We call this the <em>error fidelity</em>, defined as <em>N<sub>c</sub>      </em>/<em>N<sub>m</sub>      </em> where <em>N<sub>m</sub>      </em> is the total number of misclassified test data points and <em>N<sub>c</sub>      </em> is the number of consistent predictions of the misclassified test data points between the interpretation model and the DNN model. As shown in Table <a class="tbl" href="#tab1">1</a>, the original boundary tree has a lower error fidelity, indicating it does not mimic the model faithfully on misclassified data points. With a better boundary characterization mechanism, EB-tree outperforms the original boundary tree significantly.</p>     <p>We measure the average edge length of an EB-tree and a vanilla boundary tree. As shown in Table <a class="tbl" href="#tab1">1</a>, an EB-tree has a much shorter average edge length than a vanilla boundary tree. Figure <a class="fig" href="#fig6">6</a> shows a part of the vanilla boundary tree and a part of the EB-tree built for MNIST-CNN. It is clear that the nodes sharing the same edges in the EB-tree have better visual similarity than those in the boundary tree. An EB-tree characterizes boundaries in a more interpretable manner and is capable of explaining how a data point is classified into one class, but not another by the model.</p>     <p>We also measure the EB-tree construction and traversal time. The support vector machines are used to measure the distance to decision boundaries for training data points. For a complex model (Inception-v3) built on a large dataset (ImageNet), the training time for a support vector machine is within 6 hours with 60 Intel Xeon E5 cores and 64GB memory on an HPC cluster. The training time is short compared to that of training the DNN model. Moreover, it takes 47.61s, 35.65s, and 3,812.71s to construct an EB-tree for CNN (MNIST), ALL-CNN (CIFAR-10) and Inception-v3 (ImageNet), respectively. For Inception-v3 (ImageNet), the EB-tree construction time is mainly dominated by execution time of LSH queries. The EB-tree traversal time depends on the depth and number of children nodes. For the above three EB-trees, the average traversal time for a test data point is 0.062s, 0.065s, and 0.194s, respectively. <figure id="fig6">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig6.jpg" class="img-responsive" alt="Figure 6"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Comparison of the boundary tree built by random training data order and the EB-tree for MNIST-CNN.</span>       </div>      </figure>      <figure id="fig7">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig7.jpg" class="img-responsive" alt="Figure 7"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Error rate vs. number of prototypes (nodes).</span>       </div>      </figure>     </p>    </section>    <section id="sec-17">     <p><em>4.1.2 MMD-critic and EB-tree.</em> We also compare the training data selection performance between EB-tree and MMD-critic. MMD-critic is a representative method for interpreting deep learning models with examples. We first compare the error rates of 1-NN (the nearest neighbor) classifiers built from the MMD-selected prototypes and EB-tree nodes. For MMD-critic, image embeddings before the fully connected layer are used for the prototype selection. As shown in Figure&#x00A0;<a class="fig" href="#fig7">7</a>, the classifier built on EB-tree selected data achieves a lower error rate with a significantly smaller number of training data points. This indicates that the EB-tree has a stronger capability of model mimicking and reflects model decision boundaries better. <figure id="fig8">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig8.jpg" class="img-responsive" alt="Figure 8"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">Sample prototypes and criticisms (MNIST-CNN).</span>       </div>      </figure>     </p>     <p>Apart from the prototypes, MMD-critic also selects criticisms to assist in interpreting the outlier points as shown in Figure&#x00A0;<a class="fig" href="#fig8">8</a>. However, MMD-critic does not provide any data structure to help users to explain a prediction. For a large dataset, it is difficult and time-consuming for a user to go through these selected data points one by one to get insight into a prediction. For an EB-tree, similar nodes belonging to different classes are organized as parent-children pairs in the tree. Traversing a test data point along these pairs is effective for a user to understand why certain predictions are made by a DNN model. We show our experimental results with human judgments in the following use cases.</p>    </section>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Use Cases of EB-tree</h3>     </div>    </header>    <section id="sec-19">     <p><em>4.2.1 Case 1: Explaining Predictions of Individual Data Points.</em> To evaluate the effectiveness of an EB-tree in helping a user understanding the decision making of a model, we conduct a human pilot study. We define their effectiveness as follows: a method is effective for assisting a user in interpreting a DNN model when the user is able to identify the data points that support the most likely classes in a prediction. For example, if a test data point is confidently classified into class A, a training data point from class A that is visually similar to the test data point should be identified as the explainer. If the prediction of a test data point is ambiguous between class A and class B, an effective data point selection method should be able to assist the user in identifying training data points belonging to each class to demonstrate the ambiguity.</p>     <p>The study involves 20 users without machine learning research and development experience. We randomly choose 50 test images in each of the three datasets (MNIST, CIFAR-10, and ImageNet). Among each dataset, 25 are chosen from misclassified images while the other 25 are chosen from images that are correctly classified. For each dataset, we provide two means for supporting the interpretation. One is the EB-tree constructed based on the models and the other is the prototypes/criticisms selected by MMD-critic. We design tasks to ask a user to identify &#x201C;explainers&#x201D; among MMD-selected training data points or from the EB-tree. To make a fair comparison, the node number of the tree and the data point number of prototypes/criticisms are set to equal. For ImageNet, the computation of prototypes/criticisms overwhelms MMD-critic and we only select the classes related to the classification of the 50 test images. To avoid the interference of the MMD-critic and EB-tree method, we randomly split participants into two groups with one group using MMD-critic method and the other using EB-tree method.</p>     <div class="table-responsive" id="tab2">      <div class="table-caption">       <span class="table-number">Table 2:</span>       <span class="table-title">The comparison of number of test data points whose supporting training data points can be identified via MMD-critic and EB-tree.</span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:center;">Dataset/Tool</th>        <th style="text-align:center;">Correctly classified data</th>        <th style="text-align:center;">Incorrectly classified data</th>        <th>Time/image</th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;">MNIST/MMD-critic</td>        <td style="text-align:center;">21</td>        <td style="text-align:center;">14</td>        <td>17s</td>       </tr>       <tr>        <td style="text-align:center;">MNIST/EB-tree</td>        <td style="text-align:center;">17</td>        <td style="text-align:center;">20</td>        <td>6s</td>       </tr>       <tr>        <td style="text-align:center;">CIFAR-10/MMD-critic</td>        <td style="text-align:center;">17</td>        <td style="text-align:center;">13</td>        <td>88s</td>       </tr>       <tr>        <td style="text-align:center;">CIFAR-10/EB-tree</td>        <td style="text-align:center;">15</td>        <td style="text-align:center;">18</td>        <td>25s</td>       </tr>       <tr>        <td style="text-align:center;">ImageNet/MMD-critic</td>        <td style="text-align:center;">19</td>        <td style="text-align:center;">12</td>        <td>72s</td>       </tr>       <tr>        <td style="text-align:center;">ImageNet/EB-tree</td>        <td style="text-align:center;">18</td>        <td style="text-align:center;">17</td>        <td>12s</td>       </tr>       </tbody>      </table>     </div>     <figure id="fig9">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig9.jpg" class="img-responsive" alt="Figure 9"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 9:</span>       <span class="figure-title">A test image with the label 3 (Figure 9a) in MNIST dataset and its traversal path in the EB-tree (Figure 9b).</span>      </div>     </figure>     <figure id="fig10">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig10.jpg" class="img-responsive" alt="Figure 10"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 10:</span>       <span class="figure-title">Misclassified test samples in ImageNet.</span>      </div>     </figure>     <figure id="fig11">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig11.jpg" class="img-responsive" alt="Figure 11"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 11:</span>       <span class="figure-title">Sample A (Figure&#x00A0;10a) with label &#x201C;appenzeller&#x201D; is misclassified as &#x201C;greater swiss mountain dog&#x201D; by Inception-v3. Sample B (Figure&#x00A0;10b) with label &#x201C;impala, aepyceros melampus&#x201D; is misclassified as &#x2019;gazelle&#x2019;. Sample C (Figure&#x00A0;10c) with label &#x201C;microphone&#x201D; is misclassified as &#x201C;trilobite&#x201D;.</span>      </div>     </figure>     <p>The results of our experiment are shown in Table <a class="tbl" href="#tab2">2</a>. Overall, users take significantly shorter time to identify supporting training data points in EB-trees than in MMD-critic selected data points. The results on CIFAR-10 dataset are relatively worse than the other two datasets mainly because that the image resolution of the CIFAR-10 dataset is low and participants of the pilot study have difficulty in recognizing some images. For correctly classified test data, MMD-critic exhibits better performance on identifying their supporting training points. For incorrectly classified test data, EB-tree outperforms MMD-critic. As EB-tree mainly selects the boundary nodes, correctly classified data points are likely to be further away from these nodes than from prototypes selected by MMD-critic. However, we argue that model interpretability can be better achieved through the understanding of boundaries. Incorrectly classified data points reveal the weakness of a model and a user is able to infer the model decision boundaries through training data points that support the misclassification. The insight into the boundaries is important for model sharing scenarios. On the other hand, users find it straightforward to interpret the correctly classified results even without giving supporting training data.</p>     <p>EB-tree also shows model predictions of the training data points in the tree. This reveals a few training data labeling errors and model training errors. To further illustrate the training data points identified in the EB-tree, we show examples of randomly chosen test data points and their corresponding traversal paths in Figure <a class="fig" href="#fig9">9</a>, Figure&#x00A0;<a class="fig" href="#fig10">10</a> and Figure&#x00A0;<a class="fig" href="#fig11">11</a>. CNN classifies the &#x201C;3&#x201D; in Figure 9a as &#x201C;5&#x201D;. Its traversal path marked in red is shown in Figure 9b . A traversal path moving from node <em>x<sub>i</sub>      </em> to node <em>x<sub>j</sub>      </em> indicates that node <em>x<sub>j</sub>      </em> is closer to the test point based on the DNN model. One may easily discover that the misclassification is because that the closest boundary node to the test data point is visually similar to &#x201C;3&#x201D;, but mislabeled as &#x201C;5&#x201D;.</p>     <p>The &#x201C;Appenzeller&#x201D; in Figure&#x00A0;10a is misclassified as &#x2019;Greater Swiss Mountain Dog&#x2019; by Inception-v3. When looking at the traversal path, the dog in the final node is indeed similar to the test data point. The traversal paths of EB-trees indeed provide participants with hints to understand the prediction results.</p>     <p>Moreover, the distances between the test image to the training points on the traversal path can be used to compute the confidence vector for a prediction. For EB-tree, given a test point <em>y</em>, let <em>x<sub>i</sub>      </em> denote the features of nodes on the traversal path of <em>y</em> in the EB-tree. <em>C<sub>x</sub>      </em> indicates the label of node <em>x</em>. Hence, the probability that the test point belongs to class <em>c</em> (<em>p</em>(<em>C<sub>y</sub>      </em> = <em>c</em>)) is computed as follows: <div class="table-responsive" id="eq3">       <div class="display-equation">       <span class="tex mytex">\begin{equation} p(C_{y}=C_{x_{i}}) = \textsf {SoftMax}(-d(x_{i},y)), \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>       </div>      </div>      <div class="table-responsive" id="eq4">       <div class="display-equation">       <span class="tex mytex">\begin{equation} p(C_{y} = c) = \sum \nolimits _{x_{i}=c} p(C_{y}=C_{x_{i}}), \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>       </div>      </div> where <em>d</em>(<em>x</em>, <em>y</em>) is the distance between <em>x</em> and <em>y</em>. For the &#x201C;Appenzeller&#x201D; example, the confidence vector given by the DNN model is [Greater Swiss Mountain Dog: 0.812, EntleBucher: 0.069, other classes: 0.119]. Following Equation&#x00A0;<a class="eqn" href="#eq3">3</a> and &#x00A0;<a class="eqn" href="#eq4">4</a>, we obtain the confidence of EB-tree: [Greater Swiss Mountain Dog: 0.891, EntleBucher: 0.073, other classes: 0.0359]. This indicates that the EB-tree makes predictions in a similar way to the original DNN model. For the test data in Figure&#x00A0;10b, nearly all the participants write the interpretation like &#x2019;The direction and the profile of the gazelle look much more similar to the test image than the &#x2019;impala&#x2019; (the 1st child of the final node). The test data C in Figure&#x00A0;10c is interesting since before looking at the traversal path, none of the participants have an idea about why the image (true label is microphone) is classified with the confidence vector like [&#x2019;trilobite&#x2019;:0.603, &#x2019;isopod&#x2019;:0.036, ...]. The traversal path clearly gives visual hints about the prediction results; the similar texture of the trilobite and the isopod leads to the misclassification.</p>     <p>The results indicate that EB-tree is able to give model users explanation about why a data point is classified into a particular class, not other classes along the traveral path. A model user can also traverse their own data in the tree to gain insight into the model. For model providers, EB-tree is also helpful for debugging their models and identifying mislabeled training data.</p>    </section>    <section id="sec-20">     <p><em>4.2.2 Case 2: Understanding the Model Weakness via Visualizing Decision Boundaries.</em> The decision boundaries learned through DNN classifiers are difficult to &#x201C;see&#x201D; by a human without examples. EB-tree provides a means for model users to see a small number of training data points that characterize the boundaries. It is also helpful for improving the model training. We implement an operation called <em>boundary projection</em> in the EB-tree to visualize boundaries. This operation traverses the EB-tree and finds all the edges with two end nodes from each pair of neighboring classes. A sequence of these node pairs along a boundary produces a visualization of the boundary between two classes.</p>     <p>Figure 12a shows the decision boundary between class &#x201C;chimpanzee&#x201D; and class &#x201C;gorilla&#x201D; for the EB-tree built for the Inception-v3 model on ImageNet. One may notice that some training points are at the wrong side of the boundary. For example, the third image on the &#x201C;chimpanzee&#x201D; side has a label of &#x201C;gorilla&#x201D;. We identify that these training points are not correctly classified due to model training errors. We also find that if a training point is mislabeled, it is likely to also appear at the decision boundary. The reason is that when a training point of class B is mislabeled as class A, the data point still has significant similarity with many samples in class B in the DNN transformed space. Therefore, it is likely to be inserted into the EB-tree as a neighbor of some training points in class B. This enables us to identify incorrectly labeled training data through unexplainable observations on the decision boundaries. Similarly, as shown in Figure&#x00A0;6b, a few training points (e.g., the root the tree) are clearly mislabeled as &#x201C;3&#x201D;. To further evaluate EB-tree&#x0027;s capability of discovering mislabeled training points, we randomly mislabel 100 images (10 for each class) in MNIST and CIFAR-10. Through the boundary projection, there are 99 and 97 mislabeled training points being identified respectively by the projected boundaries.</p>     <p>The separability of decision boundaries can reveal the weakness of a model. EB-tree links training data points that are close to each other but belong to different classes together. If a decision boundary contains many nodes but the differences among these nodes are unclear, the model is weak on differentiating these classes and misclassification is likely to happen among these classes. In contrast, if the difference of nodes is clear along a boundary, the model can generalize the difference between two classes well and is able to differentiate the corresponding data points belonging to these classes. <figure id="fig12">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig12.jpg" class="img-responsive" alt="Figure 12"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 12:</span>       <span class="figure-title">Projected boundaries for Inception-v3 (ImageNet) (a) the boundary between class &#x201C;chimpanzee&#x201D; and &#x201C;gorilla&#x201D;; (b) the boundary between &#x201C;goose&#x201D; and &#x201C;black swan&#x201D;.</span>       </div>      </figure>     </p>     <p>For test images in class &#x201C;goose&#x201D; and &#x201C;black swan&#x201D; in Figure&#x00A0;12b, the projected boundary is simple and the two classes can be separated by the color. As expected, we rarely observe misclassification between the two classes. For &#x201C;chimpanzee&#x201D; and &#x201C;gorilla&#x201D;, the projected boundary contains many nodes and the majority of our participants claim that the decision boundary is difficult to recognize. Correspondingly, we find that one-third of the misclassified images in class &#x201C;chimpanzee&#x201D; are classified as &#x201C;gorilla&#x201D;.</p>    </section>    <section id="sec-21">     <p><em>4.2.3 Case 3: Detecting Emerging New Classes.</em> Using a pre-trained model on a new dataset faces challenges that the dataset may contain classes unseen in the training process, which may result in incorrect predictions or missing important discoveries. This is the emerging new class detection problem in model sharing.</p>     <p>Existing new class detection methods require massive distance computation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0022">22</a>] to compare the incoming data with the existing training data points. EB-tree is able to reduce the computational complexity by only computing distances between a test data point and the training data points that share the same traversal path with the test data in the EB-tree. Formally, for an incoming data point <em>z</em> with a predicted class <em>D</em> that reaches node <em>N</em> in an EB-tree traversal, we are able to detect new classes by only comparing a subset <em>D<sub>N</sub>      </em> that reach node <em>N</em> in the EB-tree through the same traversal path. As in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>], we use conformal evaluation to compute the similarity between <em>z</em> and data points in <em>D<sub>N</sub>      </em> through p-values. The p-value <span class="inline-equation"><span class="tex">$p_z^{D_N}$</span>      </span> of a test data point <em>z</em> is calculated as below: <div class="table-responsive" id="eq5">       <div class="display-equation">       <span class="tex mytex">\begin{equation} p_{z}^{D_{N}} = \frac{|\lbrace \forall i \in D_N: A(D_N \backslash z_i, z_i) \ge A(D_N, z)\rbrace |}{|D_{N}|}, \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>       </div>      </div> where <em>A</em> is a distance function. The distance between two data points can be computed by the Euclidean distance of their confidence values just as in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>]. The p-value <span class="inline-equation"><span class="tex">$p_{z}^{D_{N}}$</span>      </span> for <em>z</em> indicates how different the new data point is from existing data points that share the same traversal path. A low p-value indicates that the prediction of the test data lacks statistical support for fitting the prediction model. <figure id="fig13">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3185995/images/www2018-4-fig13.jpg" class="img-responsive" alt="Figure 13"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 13:</span>       <span class="figure-title">(a) and (c) are test data from the unseen class &#x201C;9&#x201D;. (b) and (d) show the traversal paths of the two samples.</span>       </div>      </figure>     </p>     <p>To demonstrate the effectiveness of EB-tree in detecting emerging new classes, we simulate a model sharing scenario with the MNIST dataset. Initially, the DNN model is only trained by data points from class &#x201C;0&#x201D; to &#x201C;8&#x201D;. The EB-tree constructed for the DNN model contains 52 nodes. We then mix samples from class &#x201C;9&#x201D; in the test data and check whether they can be accurately identified. We observe that the test points in class &#x201C;9&#x201D; are misclassified in different ways. For instance, Figure&#x00A0;13a is classified as &#x201C;8&#x201D; with a confidence value of 93.76% while Figure&#x00A0;13c is misclassified as &#x201C;4&#x201D; with a confidence value of 96.43%.</p>     <p>For Figure&#x00A0;13a, we compare it with all nodes that also reach node &#x201C;8&#x201D; in the EB-tree shown in Figure&#x00A0;13b . Although all these nodes are predicted as &#x201C;8&#x201D;, we get <em>p<sub>S</sub>      </em> = 0.0073. This value indicates that sample A is statistically different from previous data points being classified as &#x201C;8&#x201D; through the same node. Therefore it is likely to belong to a new class. Similarly, we get <em>p<sub>S</sub>      </em> = 0.0036 for Figure&#x00A0;13c by comparing with all nodes that reach node &#x201C;4&#x201D; shown in Figure&#x00A0;13d . Using this method, we correctly identify 991 out of 1,000 data points belonging to class &#x201C;9&#x201D; in the test data (99.1% accuracy). Comparing to the existing method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>] that achieves an accuracy of 99.2%, EB-tree reduces the computation cost by up to 97.1% through only computing the distances between an incoming sample and existing data points ending their EB-tree traversals on the same node. EB-tree provides a unique advantage for detecting emerging new classes and significantly reduces the detection cost.</p>     <p>In practice, each node in the EB-tree of a trained model is associated with a list of DNN-transformed features (e.g., confidence values) to track how the training data points hit the node for model users to detect new classes.</p>    </section>    </section>   </section>   <section id="sec-22">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusion and Discussion</h2>    </div>    </header>    <p>We presented an Explicable Boundary Tree (EB-tree) to improve the trustworthiness of DNN model sharing. EB-tree supports model interpretability by disclosing a small set of representative training data. A model user gained insight into the decision making of a DNN model via EB-tree traversal. We showed that an EB-tree approximated the corresponding DNN model with high fidelity and improved a model user&#x0027;s understanding of a complicated model. We also showed an EB-tree was effective in detecting mislabeled training data and model training errors. EB-tree also significantly reduced the computation cost of detecting emerging new classes.</p>    <p>    <strong>Discussions</strong>. An EB-tree can be built on the information at a different hidden layer of a DNN model. A tree built on the output of the last layer (softmax) often approximates the model the best with the least number of nodes, while a tree built on the output of one of the hidden layers often shows details of boundaries with additional nodes. This helps users to better understand the similarity or dissimilarity of features embodied in a boundary and in the test data. The EB-tree built on the output of the last fully connected layer of the MNIST-CNN model is available<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> for comparison. There exists a tradeoff between achieving interpretability and optimal model approximation.</p>    <p>In addition, the construction of a tree relies on the distance measure between data points. Euclidean distance shows good performance in the output of the last layer even though some work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] shows the advantage of cosine similarity based distance metric in softmax output. There is still lots of room to exploit on what distances are suitable for the output of different hidden layers to provide an optimal model approximation.</p>   </section>   <section id="sec-23">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Acknowledgment</h2>    </div>    </header>    <p>The authors would like to thank Richard Nock and Xifeng Guo for insightful discussions. This work is partially supported by the The National Key Research and Development Program of China (2016YFB0200401), by the program for New Century Excellent Talents in University, by National Science Foundation (NSF) China 61402492, 61402486, 61379146, by the laboratory pre-research fund (9140C810106150C-81001), by the HUNAN Province Science Foundation 2017RS3045.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Agnar Aamodt and Enric Plaza. 1994. Case-based reasoning: Foundational issues, methodological variations, and system approaches. <em>      <em>AI communications</em>     </em>7, 1 (1994), 39&#x2013;59.</li>    <li id="BibPLXBIB0002" label="[2]">Amazon. 2017. Machine Learning on AWS. https://aws.amazon.com/machine-learning/. (2017). [Online; accessed 07-July-2017].</li>    <li id="BibPLXBIB0003" label="[3]">Alexandr Andoni and Ilya Razenshteyn. 2015. Optimal data-dependent hashing for approximate near neighbors. In <em>      <em>Proceedings of the forty-seventh annual ACM symposium on Theory of computing</em>     </em>. ACM, 793&#x2013;801.</li>    <li id="BibPLXBIB0004" label="[4]">Franz Aurenhammer. 1991. Voronoi diagrams&#x2014;a survey of a fundamental geometric data structure. <em>      <em>ACM Computing Surveys (CSUR)</em>     </em>23, 3 (1991), 345&#x2013;405.</li>    <li id="BibPLXBIB0005" label="[5]">Jacob Bien and Robert Tibshirani. 2011. Prototype selection for interpretable classification. <em>      <em>The Annals of Applied Statistics</em>     </em>(2011), 2403&#x2013;2424.</li>    <li id="BibPLXBIB0006" label="[6]">Olcay Boz. 2002. Extracting decision trees from trained neural networks. In <em>      <em>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</em>     </em>. ACM, 456&#x2013;461.</li>    <li id="BibPLXBIB0007" label="[7]">Zhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. 2016. Interpretable deep models for icu outcome prediction. In <em>      <em>AMIA Annual Symposium Proceedings</em>     </em>, Vol.&#x00A0;2016. American Medical Informatics Association, 371.</li>    <li id="BibPLXBIB0008" label="[8]">Mark Craven and Jude&#x00A0;W Shavlik. 1996. Extracting tree-structured representations of trained networks. In <em>      <em>Advances in neural information processing systems</em>     </em>. 24&#x2013;30.</li>    <li id="BibPLXBIB0009" label="[9]">Ruth Fong and Andrea Vedaldi. 2017. Interpretable Explanations of Black Boxes by Meaningful Perturbation. <em>      <em>arXiv preprint arXiv:1704.03296</em>     </em>(2017).</li>    <li id="BibPLXBIB0010" label="[10]">Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In <em>      <em>Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</em>     </em>. IEEE, 6645&#x2013;6649.</li>    <li id="BibPLXBIB0011" label="[11]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In <em>      <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>     </em>. 770&#x2013;778.</li>    <li id="BibPLXBIB0012" label="[12]">Geoffrey Hinton, Li Deng, Dong Yu, George&#x00A0;E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara&#x00A0;N Sainath, <em>et al.</em> 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. <em>      <em>IEEE Signal Processing Magazine</em>     </em>29, 6 (2012), 82&#x2013;97.</li>    <li id="BibPLXBIB0013" label="[13]">Shota Horiguchi, Daiki Ikami, and Kiyoharu Aizawa. 2016. Significance of softmax-based features over metric learning-based features. (2016).</li>    <li id="BibPLXBIB0014" label="[14]">Roberto Jordaney, Kumar Sharad, Santanu&#x00A0;K. Dash, Zhi Wang, Davide Papini, Ilia Nouretdinov, and Lorenzo Cavallaro. 2017. Transcend: Detecting Concept Drift in Malware Classification Models. In <em>      <em>26th USENIX Security Symposium (USENIX Security 17)</em>     </em>. USENIX Association, Vancouver, BC, 625&#x2013;642. 978-1-931971-40-9https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jordaney</li>    <li id="BibPLXBIB0015" label="[15]">Mayank Kabra, Alice Robie, and Kristin Branson. 2015. Understanding classifier errors by examining influential neighbors. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 3917&#x2013;3925.</li>    <li id="BibPLXBIB0016" label="[16]">Been Kim, Rajiv Khanna, and Oluwasanmi&#x00A0;O Koyejo. 2016. Examples are not enough, learn to criticize! criticism for interpretability. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 2280&#x2013;2288.</li>    <li id="BibPLXBIB0017" label="[17]">Pang&#x00A0;Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. <em>      <em>arXiv preprint arXiv:1703.04730</em>     </em>(2017).</li>    <li id="BibPLXBIB0018" label="[18]">Alex Krizhevsky, Ilya Sutskever, and Geoffrey&#x00A0;E. Hinton. 2017. ImageNet Classification with Deep Convolutional Neural Networks. <em>      <em>Commun. ACM</em>     </em>60, 6 (May 2017), 84&#x2013;90. 0001-0782 <a class="link-inline force-break" href="https://doi.org/10.1145/3065386"      target="_blank">https://doi.org/10.1145/3065386</a></li>    <li id="BibPLXBIB0019" label="[19]">Der-Tsai Lee and Bruce&#x00A0;J Schachter. 1980. Two algorithms for constructing a Delaunay triangulation. <em>      <em>International Journal of Computer &#x0026; Information Sciences</em>     </em>9, 3(1980), 219&#x2013;242.</li>    <li id="BibPLXBIB0020" label="[20]">Charles Mathy, Nate Derbinsky, Jos&#x00E9; Bento, Jonathan Rosenthal, and Jonathan&#x00A0;S Yedidia. 2015. The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning.. In <em>      <em>AAAI</em>     </em>. 2864&#x2013;2870.</li>    <li id="BibPLXBIB0021" label="[21]">Microsoft. 2017. Microsoft Azure Machine Learning Studio. https://studio.azureml.net/. (2017). [Online; accessed 07-July-2017].</li>    <li id="BibPLXBIB0022" label="[22]">Xin Mu, Feida Zhu, Juan Du, Ee-Peng Lim, and Zhi-Hua Zhou. 2017. Streaming Classification with Emerging New Class by Class Matrix Sketching.. In <em>      <em>AAAI</em>     </em>. 2373&#x2013;2379.</li>    <li id="BibPLXBIB0023" label="[23]">Google&#x00A0;Cloud Platform. 2018. Cloud Machine Learning Engine. https://cloud.google.com/ml-engine/. (2018). [Online; accessed 01-Feb-2018].</li>    <li id="BibPLXBIB0024" label="[24]">Marco&#x00A0;Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why Should I Trust You?: Explaining the Predictions of Any Classifier. In <em>      <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 1135&#x2013;1144.</li>    <li id="BibPLXBIB0025" label="[25]">Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-vs-all classification. <em>      <em>Journal of machine learning research</em>     </em>5, Jan (2004), 101&#x2013;141.</li>    <li id="BibPLXBIB0026" label="[26]">Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander&#x00A0;C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. <em>      <em>International Journal of Computer Vision (IJCV)</em>     </em>115, 3 (2015), 211&#x2013;252. <a class="link-inline force-break"      href="https://doi.org/10.1007/s11263-015-0816-y"      target="_blank">https://doi.org/10.1007/s11263-015-0816-y</a></li>    <li id="BibPLXBIB0027" label="[27]">W.&#x00A0;J. Scheirer, A. de Rezende&#x00A0;Rocha, A. Sapkota, and T.&#x00A0;E. Boult. 2013. Toward Open Set Recognition. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>35, 7 (July 2013), 1757&#x2013;1772. 0162-8828 <a class="link-inline force-break" href="https://doi.org/10.1109/TPAMI.2012.256"      target="_blank">https://doi.org/10.1109/TPAMI.2012.256</a></li>    <li id="BibPLXBIB0028" label="[28]">Gregor&#x00A0;PJ Schmitz, Chris Aldrich, and Francois&#x00A0;S Gouws. 1999. ANN-DT: an algorithm for extraction of decision trees from artificial neural networks. <em>      <em>IEEE Transactions on Neural Networks</em>     </em>10, 6 (1999), 1392&#x2013;1401.</li>    <li id="BibPLXBIB0029" label="[29]">David Silver, Aja Huang, Chris&#x00A0;J Maddison, Arthur Guez, Laurent Sifre, George Van Den&#x00A0;Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, <em>et al.</em> 2016. Mastering the game of Go with deep neural networks and tree search. <em>      <em>Nature</em>     </em>529, 7587 (2016), 484&#x2013;489.</li>    <li id="BibPLXBIB0030" label="[30]">J Springenberg, Alexey Dosovitskiy, Thomas Brox, and M Riedmiller. 2015. Striving for Simplicity: The All Convolutional Net. In <em>      <em>ICLR (workshop track)</em>     </em>.</li>    <li id="BibPLXBIB0031" label="[31]">Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 2818&#x2013;2826.</li>    <li id="BibPLXBIB0032" label="[32]">Tensorflow. 2017. Deep MNIST for Experts. https://www.tensorflow.org/get_started/mnist/pros. (2017). [Online; accessed 07-July-2017].</li>    <li id="BibPLXBIB0033" label="[33]">Csaba&#x00A0;D Toth, Joseph O&#x0027;Rourke, and Jacob&#x00A0;E Goodman. 2004. <em>      <em>Handbook of discrete and computational geometry</em>     </em>. CRC press.</li>    <li id="BibPLXBIB0034" label="[34]">Florian Tram&#x00E8;r, Fan Zhang, Ari Juels, Michael&#x00A0;K Reiter, and Thomas Ristenpart. 2016. Stealing machine learning models via prediction apis. In <em>      <em>USENIX Security</em>     </em>.</li>    <li id="BibPLXBIB0035" label="[35]">Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. 2015. Understanding neural networks through deep visualization. <em>      <em>arXiv preprint arXiv:1506.06579</em>     </em>(2015).</li>    <li id="BibPLXBIB0036" label="[36]">Matthew&#x00A0;D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In <em>      <em>European conference on computer vision</em>     </em>. Springer, 818&#x2013;833.</li>    <li id="BibPLXBIB0037" label="[37]">Daniel Zoran, Balaji Lakshminarayanan, and Charles Blundell. 2017. Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees. <em>      <em>arXiv preprint arXiv:1702.08833</em>     </em>(2017).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break"    href="https://github.com/sktzwhj/explicable_boundary_tree_sample/blob/master/MNIST_EB_tree_hidden_layer.png">https://github.com/sktzwhj/explicable_boundary_tree_sample/blob/master/MNIST_EB_tree_hidden_layer.png</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3185995">https://doi.org/10.1145/3178876.3185995</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
