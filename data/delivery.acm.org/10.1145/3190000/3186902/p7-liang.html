<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>AttAE-RL2: Attention based Autoencoder for Rap Lyrics Representation Learning</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">AttAE-RL<sup>2</sup>: Attention based Autoencoder for Rap Lyrics Representation Learning</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Hongru</span>      <span class="surName">Liang</span>     College of Computer and Control Engineering, Nankai University, Tianjin, China, 300350     </div>     <div class="author">     <span class="givenName">Qian</span>      <span class="surName">Li</span>     College of Computer and Control Engineering, Nankai University, Tianjin, China, 300350     </div>     <div class="author">     <span class="givenName">Haozheng</span>      <span class="surName">Wang</span>     College of Computer and Control Engineering, Nankai University, Tianjin, China, 300350     </div>     <div class="author">     <span class="givenName">Hang</span>      <span class="surName">Li</span>     College of Computer and Control Engineering, Nankai University, Tianjin, China, 300350     </div>     <div class="author">     <span class="givenName">Jin-Mao</span>      <span class="surName">Wei</span>     College of Computer and Control Engineering, Nankai University, Tianjin, China, 300350     </div>     <div class="author">     <span class="givenName">Zhenglu</span>      <span class="surName">Yang</span>     College of Computer and Control Engineering, Nankai University, Tianjin, China, 300350, <a href="mailto:lianghr@mail.nankai.edu.cn, liqian515@mail.nankai.edu.cn, hzwang@mail.nankai.edu.cn, hangl@mail.nankai.edu.cn, weijm@nankai.edu.cn, yangzl@nankai.edu.cn">lianghr@mail.nankai.edu.cn, liqian515@mail.nankai.edu.cn, hzwang@mail.nankai.edu.cn, hangl@mail.nankai.edu.cn, weijm@nankai.edu.cn, yangzl@nankai.edu.cn</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186902" target="_blank">https://doi.org/10.1145/3184558.3186902</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Learning rap lyrics is an important area of music information retrieval because it is the basis of many applications, such as recommendation systems, automatic classification. In this paper, we tackle the issue pertaining to the lack of an effective approach to aggregate various features of lyrics by proposing an <span style="text-decoration: underline;">att</span>ention-based <span style="text-decoration: underline;">a</span>uto<span style="text-decoration: underline;">e</span>ncoder for <span style="text-decoration: underline;">r</span>ap <span style="text-decoration: underline;">l</span>yrics <span style="text-decoration: underline;">r</span>epresentation <span style="text-decoration: underline;">l</span>earning (AttAE-RL<sup>2</sup>). The proposed method appropriately integrates the semantic and prosodic features of rap lyrics. The preliminary experimental results demonstrate that our approach outperforms the state-of-the-art ones.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>rap lyrics</small>, </span>     <span class="keyword">      <small> representation learning</small>, </span>     <span class="keyword">      <small> autoencoder</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Hongru Liang, Qian Li, Haozheng Wang, Hang Li, Jin-Mao Wei, and Zhenglu Yang. 2018. AttAE-RL<sup>2</sup>: Attention based Autoencoder for Rap Lyrics Representation Learning. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 3 Pages. <a href="https://doi.org/10.1145/3184558.3186902" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186902</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Among the various music genres, rap is one of the most popular types&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. Learning rap lyrics is an important task of music information retrieval that has attracted growing interest from researchers. However, rap lyrics are unstructured, and directly using the off-the-shelf natural language processing techniques is infeasible in conducting phonological analysis.</p>    <p>In recent years, many studies have been conducted on rap lyrics analysis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. However, the effectiveness of these studies is far from satisfactory for the users, because such studies have either partially utilized the features, e.g., semantic features, or learn ineffective representation of the features, e.g., statistical representations. As far as we know, no study has yet produced general representations of rap lyrics involving both semantic and prosodic information.</p>    <p>To address the aforementioned issues, we introduce an attention-based autoencoder to appropriately aggregate the semantic and prosodic information from the lyrics. Our goal is to learn the overall representation of rap lyrics. The prosodic features are effectively represented by a novel strategy, i.e., rhyme2vec. Moreover, attention mechanism is used to balance the importance among various types of information. All these strategies are integrated into a general framework called the attention-based autoencoder for rap lyrics representation learning (AttAE-RL<sup>2</sup>). The performance of our proposed approach is experimentally demonstrated to be superior to the state-of-the-art ones by a large margin. To the best of our knowledge, this is the first study to learn the integrated and distributed representation of rap lyrics.</p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Model Description</h2>     </div>    </header>    <p>The model consists of three modules, i.e., Input module, Encoder module, and Decoder module (Fig.&#x00A0;<a class="fig" href="#fig1">1</a>). The Encoder and the Decoder modules comprise the attention-based autoencoder. Our target is to learn a latent representation for a piece of rap lyric with both semantic and prosodic information. <figure id="fig1">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186902/images/www18companion-142-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">The architecture of AttAE-RL<sup>2</sup>      </span>     </div>     </figure>     <strong>Input module.</strong>As shown at the top of Fig.&#x00A0;<a class="fig" href="#fig1">1</a>, we utilize rap lyrics as the input. For each piece of rap lyric, let L<sub>s</sub> denote its raw lyrics and L<sub>r</sub> denote the corresponding phonetic transcriptions, translated by eSpeak<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. Table&#x00A0;<a class="tbl" href="#tab1">1</a> shows an example of four consecutive rap lyrics and phonetic transcription from Fort Minor&#x0027;s <em>Remember the Name</em>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Rap lyrics with their phonetic transcription</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:left;">        <strong>raw lyrics (L<sub>s</sub>)</strong>       </th>       <th style="text-align:left;">        <strong>phonetic transcription (L<sub>r</sub>)</strong>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">Put it together himself</td>       <td style="text-align:left;">p,Ut It t@g,ED3 hIms&#x0027;Elf</td>      </tr>      <tr>       <td style="text-align:left;">now the picture connects</td>       <td style="text-align:left;">n&#x0027;aU D@ p&#x0027;IktS3 k@n&#x0027;Ekts</td>      </tr>      <tr>       <td style="text-align:left;">Never asking for someone&#x0027;s help</td>       <td style="text-align:left;">n&#x0027;Ev3r- &#x2019;aaskIN fO@ s&#x0027;Vmw0nz h&#x0027;Elp</td>      </tr>      <tr>       <td style="text-align:left;">to get some respect</td>       <td style="text-align:left;">t@ gEt s,Vm rI2sp&#x0027;Ekt</td>      </tr>     </tbody>     </table>    </div>    <p>We assume that rap lyrics have both monorhyme and alternate rhyme<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>. For the monorhyme, we treat all consecutive lines as a prosodic block, i.e., L<sub>r</sub>. For the alternate rhyme, we split the rap lines into two prosodic blocks, namely, <span class="inline-equation"><span class="tex">$ {L_{r}^{o}}$</span>     </span>, which includes the odd lines, and <span class="inline-equation"><span class="tex">$ {L_{r}^{e}}$</span>     </span>, which includes the even lines.</p>    <p>     <strong>Attention based autoencoder.</strong>Instead of using one-hot codes, we utilize pretrained embeddings as the input. Doc2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] is employed to extract the semantic vector of L<sub>s</sub>, i.e., v<sub>s</sub>. Here, <span class="inline-equation"><span class="tex">$ {v_r^m}$</span>     </span>, <span class="inline-equation"><span class="tex">$ {v_r^o}$</span>     </span>, and <span class="inline-equation"><span class="tex">$ {v_r^e}$</span>     </span> are embedded in a similar way of modeling the semantic information, yet at the character level.</p>    <p>In the Encoder module, an E-attention layer, to be explained later, is applied on <span class="inline-equation"><span class="tex">$ {v_r^m}$</span>     </span>, <span class="inline-equation"><span class="tex">$ {v_r^o}$</span>     </span>, and <span class="inline-equation"><span class="tex">$ {v_r^e}$</span>     </span> to obtain a comprehensive rhyme vector v<sub>r</sub>. Both v<sub>r</sub> and v<sub>s</sub> are vectors in different high dimension spaces. To learn the integrated representations of rap lyrics, v<sub>r</sub> and v<sub>s</sub> are mapped into a shared latent space through fully-connected layers, i.e., R-encoder and S-encoder, respectively. As can be seen, u<sub>r</sub> and u<sub>s</sub> are the corresponding vectors to v<sub>r</sub> and v<sub>s</sub> in the shared high dimension space, respectively. Instead of simply adding u<sub>r</sub> and u<sub>s</sub> up, we employ another E-attention layer to fuse them and obtain a latent vector, u. The target representation of rap lyrics, v<sub>t</sub>, is learned from u through fully-connected layers, L-encoder.</p>    <p>The Decoder module is the inversion of the Encoder module. In this module, the intermediate results and input vectors of the Encoder module should be reconstructed from v<sub>t</sub>.</p>    <p>     <em>Attention mechanism.</em> For an attention layer, noted as E-attention layer in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>, <em>n m</em>-dimensional vectors serve as inputs. In the E-attention layer, we stack the input vectors as an <em>n</em> &#x00D7; <em>m</em> matrix, M. And an attention vector a is calculated by feeding the concatenation of the input vectors into a fully-connected layer. Every element of a corresponds to one input vector. The output s of an attention layer is calculated with s = a &#x00B7; M. A D-attention layer can be regarded as an inverse operation of an E-attention layer. The output of a D-attention layer is calculated as <span class="inline-equation"><span class="tex">$ {\hat{M}} = {a^{+}} \cdot {\hat{s}}$</span>     </span>, where a<sup>+</sup> is the left pseudo inverse of the attention vector a of the corresponding E-attention layer.</p>    <p>     <strong>Loss function.</strong>The loss function contains reconstruction loss and label loss. The reconstruction loss is formulated as <span class="inline-equation"><span class="tex">$\ell _{ae}=mse({v_s}, {\hat{v}_s})+mse({v_r^m}, {\hat{v}_r^m})+mse({v_r^o}, {\hat{v}_r^o})+mse({v_r^e}, {\hat{v}_r^e})$</span>     </span>, where <em>mse</em> stands for mean squared error. To incorporate the label information, we deploy a classifier over v<sub>t</sub> as an external tool in fine tuning the model. The label loss, denoted as &#x2113;<sub>     <em>label</em>     </sub>, is a cross entropy function over the classifier. The overall objective function is &#x2113; = <em>&#x03B1;</em>*&#x2113;<sub>     <em>ae</em>     </sub> + (1 &#x2212; <em>&#x03B1;</em>)*&#x2113;<sub>     <em>label</em>     </sub>, where <em>&#x03B1;</em> is a hyper parameter to balance the importance of the two objectives.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experimental Evaluation</h2>     </div>    </header>    <p>To evaluate the effectiveness of the proposed framework on representing rap lyrics, we conduct two experimental tasks, i.e., NextLine prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] and rap genre classification. The dataset and source code are available at <a class="link-inline force-break" href="https://github.com/mengshor/attaerl2">https://github.com/mengshor/attaerl2</a>. </p>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> NextLine Prediction</h3>     </div>     </header>     <p>Given a rap song with a sequence of <em>m</em> lines, the task of NextLine prediction is to predict the (<em>m</em>+1)th line from a set of candidate lines&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. We obtain a corpus of rap lyrics crawled from the Internet<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>, which includes 810567 lines from 16697 songs.</p>     <p>We compare AttAE-RL<sup>2</sup> with other state-of-the-art methods, namely, DopeSemantic, DopeRhyme, and DopeLearning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>], which are collectively known as &#x201C;Dopes&#x201D;. Doc2vec-rhyme2vec concatenates doc2vec and rhyme2vec representations together.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Performance of different models</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <strong>Algorithm</strong>        </td>        <td style="text-align:center;">        <strong>Mean rank</strong>        </td>        <td style="text-align:center;">        <strong>MRR</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">        <strong>DopeSemantic</strong>        </td>        <td style="text-align:center;">116.4178</td>        <td style="text-align:center;">0.0822</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>DopeRhyme</strong>        </td>        <td style="text-align:center;">103.2068</td>        <td style="text-align:center;">0.1303</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>DopeLearning</strong>        </td>        <td style="text-align:center;">79.9964</td>        <td style="text-align:center;">0.1680</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>rhyme2vec</strong>        </td>        <td style="text-align:center;">45.8172</td>        <td style="text-align:center;">0.1906</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>doc2vec</strong>        </td>        <td style="text-align:center;">38.8388</td>        <td style="text-align:center;">0.2066</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>doc2vec-rhyme2vec</strong>        </td>        <td style="text-align:center;">15.1046</td>        <td style="text-align:center;">0.4627</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>AttAE-RL</strong>        <sup>2</sup>        </td>        <td style="text-align:center;">        <strong>6.6024</strong>        </td>        <td style="text-align:center;">        <strong>0.7278</strong>        </td>       </tr>      </tbody>     </table>     </div>     <p>The performance is evaluated by mean rank and mean reciprocal rank (MRR), as shown in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. Among all methods, AttAE-RL<sup>2</sup> achieves the best performance with a mean rank of 6.6024. This value is much better than that of Dopes&#x00A0;(79.9964). Moreover, the MRR of AttAE-RL2 is 0.7278, which is superior to that of Dopes&#x00A0;(0.168). The result demonstrates the superiority of AttAE-RL<sup>2</sup> over other methods in that it can represent rap lyrics by fusing more features more effectively .</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Rap Genre Classification</h3>     </div>     </header>     <p>Given a set of rap songs and a set of genre labels, the rap genre classification task is to predict proper labels for every song based upon the representations learned from raw rap lyrics. We create a dataset consisting of 10167 songs from 9 rap genres, such as alternative rap, grime rap, and so forth. <figure id="fig2">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186902/images/www18companion-142-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Classification results on micro-F1</span>      </div>     </figure>     </p>     <p>The baselines evaluated are <strong>RhymeAPP</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], a lyrical analysis tool that calculates the statistical features of a rap song; <strong>rhyme2vec</strong>, which employs prosodic representations; <strong>doc2vec</strong>, which utilizes merely semantic representations; and <strong>HAN-L</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>], which is a state-of-the-art approach for genre classification of intact lyrics.</p>     <p>The micro-F1 and macro-F1 are employed to evaluate the classification results, as shown in Fig.&#x00A0;<a class="fig" href="#fig2">2</a>, respectively. As can be seen, AttAE-RL<sup>2</sup> significantly outperforms the other methods, indicating that it has a strong ability to capture prosodic and semantic information in effectively representing rap lyrics.</p>    </section>   </section>  </section>  <section class="back-matter">   <section id="sec-7">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work was supported in part by the National Natural Science Foundation of China under Grant No.U1636116, 11431006, 61772288, and the Research Fund for International Young Scientists under Grant No. 61650110510 and 61750110530.</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Hussein Hirjee and Daniel&#x00A0;G Brown. 2010. Rhyme analyzer: An analysis tool for rap lyrics. In <em>      <em>Proceedings of the 11th International Society for Music Information Retrieval Conference</em>     </em>.</li>     <li id="BibPLXBIB0002" label="[2]">Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, and Aristides Gionis. 2016. DopeLearning: A Computational Approach to Rap Lyrics Generation. In <em>      <em>Proceedings of KDD&#x2019;16</em>     </em>.</li>     <li id="BibPLXBIB0003" label="[3]">Matthias Mauch, Robert&#x00A0;M MacCallum, Mark Levy, and Armand&#x00A0;M Leroi. 2015. The evolution of popular music: USA 1960&#x2013;2010. <em>      <em>Royal Society open science</em>     </em>2, 5 (2015), 150081.</li>     <li id="BibPLXBIB0004" label="[4]">V.&#x00A0;Le Quoc and Mikolov Tomas. 2014. Distributed Representations of Sentences and Documents. In <em>      <em>Proceedings of ICML&#x2019;14</em>     </em>.</li>     <li id="BibPLXBIB0005" label="[5]">Alexandros Tsaptsinos. 2017. Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network. In <em>      <em>Proceedings of ISMIR&#x2019;17</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://espeak.sourceforge.net/.">http://espeak.sourceforge.net/.</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Monorhyme is a rhyme scheme in which each line has an identical rhyme. In alternate rhyme, the rhyme is on alternate lines. <em>Wikipedia</em>.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="http://ohhla.com/">http://ohhla.com/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186902">https://doi.org/10.1145/3184558.3186902</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
