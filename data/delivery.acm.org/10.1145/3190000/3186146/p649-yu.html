<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Aesthetic-based Clothing Recommendation</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Aesthetic-based Clothing Recommendation</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Wenhui</span>     <span class="surName">Yu<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a></span>,     Tsinghua University, Beijing, China, <a href="mailto:yuwh16@mails.tsinghua.edu.cn">yuwh16@mails.tsinghua.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Huidi</span>     <span class="surName">Zhang<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a></span>,     Tsinghua University, Beijing, China, <a href="mailto:zhd16@mails.tsinghua.edu.cn">zhd16@mails.tsinghua.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Xiangnan</span>     <span class="surName">He</span>,     National University of Singapore, Singapore, 117417, <a href="mailto:xiangnanhe@gmail.com">xiangnanhe@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Xu</span>     <span class="surName">Chen</span>,     Tsinghua University, Beijing, China, <a href="mailto:xu-ch14@mails.tsinghua.edu.cn">xu-ch14@mails.tsinghua.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Li</span>     <span class="surName">Xiong</span>,     Emory University, Atlanta, USA, <a href="mailto:lxiong@emory.edu">lxiong@emory.edu</a>    </div>    <div class="author">     <span class="givenName">Zheng</span>     <span class="surName">Qin<a class="fn" href="#fn2" id="foot-fn2"><sup>&#x2020;</sup></a></span>,     Tsinghua University, Beijing, China, <a href="mailto:qingzh@mail.tsinghua.edu.cn">qingzh@mail.tsinghua.edu.cn</a>    </div>        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186146" target="_blank">https://doi.org/10.1145/3178876.3186146</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Recently, product images have gained increasing attention in clothing recommendation since the visual appearance of clothing products has a significant impact on consumers&#x2019; decision. Most existing methods rely on conventional features to represent an image, such as the visual features extracted by convolutional neural networks (CNN features) and the scale-invariant feature transform algorithm (SIFT features), color histograms, and so on. Nevertheless, one important type of features, the <em>aesthetic features</em>, is seldom considered. It plays a vital role in clothing recommendation since a users&#x2019; decision depends largely on whether the clothing is in line with her aesthetics, however the conventional image features cannot portray this directly. To bridge this gap, we propose to introduce the aesthetic information, which is highly relevant with user preference, into clothing recommender systems. To achieve this, we first present the aesthetic features extracted by a pre-trained neural network, which is a brain-inspired deep structure trained for the aesthetic assessment task. Considering that the aesthetic preference varies significantly from user to user and by time, we then propose a new tensor factorization model to incorporate the aesthetic features in a personalized manner. We conduct extensive experiments on real-world datasets, which demonstrate that our approach can capture the aesthetic preference of users and significantly outperform several state-of-the-art recommendation methods.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Collaborative filtering;</strong> <strong>Social recommendation;</strong> <strong>Recommender systems;</strong> &#x2022;<strong> Human-centered computing </strong>&#x2192; <em>Social recommendation;</em></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Clothing recommendation</small>, </span>     <span class="keyword">      <small> side information</small>, </span>     <span class="keyword">      <small> aesthetic features</small>, </span>     <span class="keyword">      <small> tensor factorization</small>, </span>     <span class="keyword">      <small> dynamic collaborative filtering.</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Wenhui Yu<sup><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a></sup>, Huidi Zhang<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a>, Xiangnan He, Xu Chen, Li Xiong, and Zheng Qin&#x2020;. 2018. Aesthetic-based Clothing Recommendation. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186146" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186146</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <figure id="fig1">    <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Comparison of CNN features and aesthetic features. The CNN is inputted with the RGB components of an image and trained for the classification task, while the aesthetic network is inputted with raw aesthetic features and trained for the aesthetic assessment task.</span>    </div>    </figure>    <p>When shopping for clothing on the Web, we usually look through product images before making the decision. Product images provide abundant information, including design, color schemes, decorative pattern, texture, and so on; we can even estimate the thickness and quality of a product from its images. As such, product images play a key role in the clothing recommendation task.</p>    <p>To leverage this information and enhance the performance, existing clothing recommender systems use image data with various image features, like features extracted by convolutional neural networks (CNN features) and the scale-invariant feature transform algorithm (SIFT features), color histograms, etc. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] utilized the CNN features extracted by a deep convolutional neural network. Trained for the classification task, CNN features contain semantic information to distinguish items and have been widely used in recommendation tasks. However, one important factor, aesthetics, has yet been considered in previous research. When purchasing clothing products, what consumers concern is not only &#x201C;What is the product?&#x201D;, but also &#x201C;Is the product good-looking?&#x201D;.</p>    <p>Taking the product shown in Figure <a class="fig" href="#fig1">1</a> as an example. A consumer will notice that the dress is of colors black and white, of simple but elegant design, and has a delightful proportion. She will purchase it only if she is satisfied with all these aesthetic factors. In fact, for some consumers, especially young females, aesthetic factor could be the primary factor, even more important than others like quality, comfort, and prices. As such, we need novel features to capture this indispensable information. Unfortunately, CNN features do not encode the aesthetic information by nature. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>] used color histograms to portray consumers&#x2019; intuitive perception about an image while it is too crude and primitive. To provide quality recommendation for the clothing domain, comprehensive and high-level aesthetic features are greatly desired.</p>    <p>In this paper, we leverage the aesthetic network to extract relevant features. The differences between an aesthetic network and a CNN are demonstrated in Figure <a class="fig" href="#fig1">1</a>. Recently, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] proposed a <strong>B</strong>rain-inspired <strong>D</strong>eep <strong>N</strong>etwork (<strong>BDN</strong>), which is a deep structure trained for image aesthetic assessment. The inputs are several raw features that are indicative of aesthetic feelings, like hue, saturation, value, duotones, complementary color, etc. It then extracts high-level aesthetic features from the raw features. In this paper, BDN is utilized to extract the holistic features to represent the aesthetic elements of a clothing product (taking Figure <a class="fig" href="#fig1">1</a> as an example, the aesthetic elements can be color, structure, proportion, style, etc.).</p>    <p>It is obvious that the aesthetic preference shows a significant diversity among different people. For instance, children prefer colorful and lovely products while adults prefer those can make them look mature and elegant; women may prefer exquisite decorations while men like concise designs. Moreover, the aesthetic tastes of consumers also change with time, either in short term, or in long term. For example, the aesthetic tastes vary in different seasons periodically&#x2014;in spring or summer, people may prefer clothes with light color and fine texture, while in autumn or winter, people tend to buy clothes with dark color, rough texture, and loose style. In the long term, the fashion trend changes all the time and the popular color and design may be different by year.</p>    <p>To capture the diversity of the aesthetic preference among consumers and over time, we exploit tensor factorization as a basic model. There are several ways to decompose a tensor [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], however, there are certain drawbacks in existing models. To address the clothing recommendation task better, we first propose a <strong>D</strong>ynamic <strong>C</strong>ollaborative <strong>F</strong>iltering (<strong>DCF</strong>) model trained with coupled matrices to mitigate the <em>sparsity</em> problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. We then combine it with the additional image features (concatenated aesthetic and CNN features) and term the method as <strong>D</strong>ynamic <strong>C</strong>ollaborative <strong>F</strong>iltering model with <strong>A</strong>esthetic Features (called <strong>DCFA</strong>). We optimize the models with bayesian personalized ranking (BPR) optimization criterion [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] and evaluated their performance on an <em>Amazon clothing</em> dataset. Extensive experiments show that we improve the performance significantly by incorporating aesthetic features.</p>    <p>To summarize, our main contributions are as follows:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">We leverage novel aesthetic features in recommendation to capture consumers&#x2019; aesthetic preference. Moreover, we compare the effect with several conventional features to demonstrate the necessity of the aesthetic features.<br/></li>    <li id="list2" label="&#x2022;">We propose a novel DCF model to portray the purchase events in three dimensions: users, items, and time. We then incorporate aesthetic features into DCF and train it with coupled matrices to alleviate the sparsity problem.<br/></li>    <li id="list3" label="&#x2022;">We conduct comprehensive experiments on real-world datasets to demonstrate the effectiveness of our DCFA method.<br/></li>    </ul>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>This paper develops aesthetic-aware clothing recommender systems. Specifically, we incorporate the features extracted from the product images by an aesthetic network into a tensor factorization model. As such, we review related work on aesthetic networks, image-based recommendation, and tensor factorization.</p>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Aesthetic Networks</h3>     </div>    </header>    <p>The aesthetic networks are proposed for image aesthetic assessment. After [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] first proposed the aesthetic assessment problem, many research efforts exploited various handcrafted features to extract the aesthetic information of images [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>]. To portray the subjective and complex aesthetic perception, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>] exploited deep networks to emulate the underlying complex neural mechanisms of human perception, and displayed the ability to describe image content from the primitive level (low-level) features to the abstract level (high-level) features.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Image-based Recommendations</h3>     </div>    </header>    <p>Recommendation has been widely studied due to its extensive use, and many effective methods have been proposed [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>]. The power of recommender systems lies on their ability to model the complex preference that consumers exhibit toward items based on their past interactions and behavior. To extend their expressive power, various works exploited image data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>]. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] infused product images and item descriptions together to make dynamic predictions, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>] leveraged textual and visual information to recommend tweets and personalized key frames respectively. Image data can also mitigate the sparsity problem and cold start problem. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] used CNN features of product images while [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>] recommended movies with color histograms of posters and frames. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] recommended clothes by considering the clothing fashion style.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Tensor Factorization</h3>     </div>    </header>    <p>Time is an important contextual information in recommender systems since the sales of commodities show a distinct time-related succession. In context-aware recommender systems, tensor factorization has been extensively used. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>] introduced two main forms of tensor decomposition, the <strong>C</strong>ANDECOMP/<strong>P</strong>ARAFAC (<strong>CP</strong>) and Tucker decomposition. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] first utilized tensor factorization for context-aware collaborative filtering. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] proposed a <strong>P</strong>airwise <strong>I</strong>nteraction <strong>T</strong>ensor <strong>F</strong>actorization (<strong>PITF</strong>) model to decompose the tensor with a linear complexity. Nevertheless, tensor-based methods suffer from several drawbacks like poor convergence in sparse data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] and not scalable to large-scale datasets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. To address these limitations, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] formulated recommendation models with the <strong>C</strong>oupled <strong>M</strong>atrix and <strong>T</strong>ensor <strong>F</strong>actorization (<strong>CMTF</strong>) framework. <figure id="fig2">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Brain-inspired Deep Network (BDN) architecture.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Preliminaries</h2>    </div>    </header>    <p>This section introduces some preliminaries about the aesthetic neural network, which is used to extract the aesthetic features of clothing images. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>], the authors introduced the Brain-inspired Deep Networks (BDN, shown in Figure <a class="fig" href="#fig2">2</a>), a deep CNN structure consists of several parallel pathways (sub-networks) and a high-level synthesis network. It is trained on the <em>Aesthetic Visual Analysis (AVA)</em> dataset, which contains 250,000 images with aesthetic ratings and tagged with 14 photographic styles (e.g., complementary colors, duotones, rule of thirds, etc.). The pathways take the form of convolutional networks to exact the abstracted aesthetic features by <em>pre-trained</em> with the individual labels of each tag. For example, when training the pathway for complementary colors, the individual label is 1 if the sample is tagged with &#x201C;complementary colors&#x201D; and is 0 if not. We input the raw features, which include low-level features (hue, saturation, value) and abstracted features (feature maps of the pathways), into the high-level synthesis network and <em>jointly tune</em> it with the pathways for aesthetic rating prediction. Considering that the <em>AVA</em> is a photography dataset and the styles are for photography, so not all the raw features extracted by the pathways are desired in our recommendation task. Thus we only reserve the pathways that are relevant to the clothing aesthetic. Finally, we use the output of the second fully-connected layer of the synthesis network as our aesthetic features.</p>    <p>We then analyze several extensively used features and demonstrate the superiority of our aesthetic features.</p>    <p>    <strong>CNN Features:</strong> These are the most extensively used features due to their extraordinary representation ability. Typically the output of certain fully-connected layer of a deep CNN structure is used. For example, a common choice is the Caffe reference model with 5 convolutional layers followed by 3 fully-connected layers (pre-trained on the ImageNet dataset); the features are the output of FC7, namely, the second fully-connected layer, which is a feature vector of length 4096.</p>    <p>CNN features mainly contain semantic information, which contributes little to evaluate the aesthetics of an image. Recall the example in Figure <a class="fig" href="#fig1">1</a>, it can encode &#x201C;There is a skirt in the image.&#x201D; but cannot express &#x201C;The clothing is beautiful and fits the consumer&#x0027;s taste.&#x201D;. Devised for aesthetic assessment, BDN can capture the high-level aesthetic information. As such, our aesthetic features can do better in beauty estimating and complement CNN features in clothing recommendation.</p>    <p>    <strong>Color Histograms:</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>] exploited color histograms to represent human&#x0027;s feeling about the posters and frames for movie recommendation. Though can get the aesthetic information roughly, the low-level handcrafted features are crude, unilateral, and empirical. BDN can get abundant visual features by the pathways. Also, it is data-driven, since the rules to extract features are learned from the data. Compared with the intuitive color histograms, our aesthetic features are more objective and comprehensive. Recall the example in Figure <a class="fig" href="#fig1">1</a> again, color histograms can tell us no more than &#x201C;The clothes in the image is white and black&#x201D;.</p>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Clothing Recommendation with Aesthetic Features</h2>    </div>    </header>    <p>In this section, we first introduce the basic tensor factorization model (DCF). We next construct a hybrid model that integrates image features into the basic model (DCFA).</p>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Basic Model</h3>     </div>    </header>    <p>Considering the impact of time on aesthetic preference, we propose a context-aware model as the basic model to account for the temporal factor. We use a <em>P</em> &#x00D7; <em>Q</em> &#x00D7; <em>R</em> tensor A to indicate the purchase events among the user, clothes, and time dimensions (where <em>P</em>, <em>Q</em>, <em>R</em> are the number of users, clothes, and time intervals, respectively). If user <em>p</em> purchased item <em>q</em> in time interval <em>r</em>, A<sub>      <em>pqr</em>     </sub> = 1, otherwise A<sub>      <em>pqr</em>     </sub> = 0. Tensor factorization has been widely used to predict the missing entries (i.e., zero elements) in A, which can be used for recommendation. There are several approaches and we introduce the most common ones:</p>    <section id="sec-15">     <p><em>4.1.1 Existing Methods and Their Limitations.</em> In this subsection, we summarize the motivation of proposing our novel tensor factorization model.</p>     <p>      <strong>Tucker Decomposition:</strong> This method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0023">23</a>] decomposes the tensor A into a tensor core and three matrices, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \hat{ {{\rm A}}}_{pqr}=\sum _{i=1}^{K_1} \sum _{j=1}^{K_2} \sum _{k=1}^{K_3} { {{\rm a}}}_{ijk} { {{\rm U}}}_{ip} { {{\rm V}}}_{jq} { {{\rm T}}}_{kr}, \] </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">${ {{\rm a}}} \in \mathbb {R}^{K_1 \times K_2 \times K_3}$</span>      </span> is the tensor core, <span class="inline-equation"><span class="tex">${ {{\rm U}}} \in \mathbb {R}^{K_1 \times P}$</span>      </span>, <span class="inline-equation"><span class="tex">${ {{\rm V}}} \in \mathbb {R}^{K_2 \times Q}$</span>      </span>, and <span class="inline-equation"><span class="tex">${ {{\rm T}}} \in \mathbb {R}^{K_3 \times R}$</span>      </span>. Tucker decomposition has very strong representation ability, but it is very time consuming, and hard to converge.</p>     <p>      <strong>CP Decomposition:</strong> The tensor A is decomposed into three matrices in CP decomposition, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \hat{ {{\rm A}}}_{pqr}=\sum _{k=1}^{K} { {{\rm U}}}_{kp} { {{\rm V}}}_{kq} { {{\rm T}}}_{kr}, \] </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">${ {{\rm U}}} \in \mathbb {R}^{K \times P}$</span>      </span>, <span class="inline-equation"><span class="tex">${ {{\rm V}}} \in \mathbb {R}^{K \times Q}$</span>      </span>, and <span class="inline-equation"><span class="tex">${ {{\rm T}}} \in \mathbb {R}^{K \times R}$</span>      </span>. This model has been widely used due to its linear time complexity, especially in Coupled Matrix and Tensor Factorization (CMTF) structure model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>]. However, all dimensions (users, clothes, time) are related by the same latent features. Intuitively, we want the latent features relating users and clothes to contain the information about users&#x2019; preference, like aesthetics, prices, quality, brands, etc., and the latent features relating clothes and time to contain the information about the seasonal characteristics and fashion elements of clothes like colors, thickness, design, etc.</p>     <p>      <strong>PITF Decomposition:</strong> The Pairwise Interaction Tensor Factorization (PITF) model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0034">34</a>] decomposes A into three pair of matrices, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \hat{ {{\rm A}}}_{pqr}=\sum _{k=1}^{K} { {{\rm U}}}_{kp}^{ {{\rm V}}} { {{\rm V}}}_{kq}^{ {{\rm U}}} +\sum _{k=1}^{K} { {{\rm U}}}_{kp}^{ {{\rm T}}} { {{\rm T}}}_{kr}^{ {{\rm U}}} + \sum _{k=1}^{K} { {{\rm V}}}_{kq}^{ {{\rm T}}} { {{\rm T}}}_{kr}^{ {{\rm V}}}, \] </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">${ {{\rm U}}}^{ {{\rm V}}}, { {{\rm U}}}^{ {{\rm T}}} \in \mathbb {R}^{K \times P}$</span>      </span>; <span class="inline-equation"><span class="tex">${ {{\rm V}}}^{ {{\rm U}}}, { {{\rm V}}}^{ {{\rm T}}} \in \mathbb {R}^{K \times Q}$</span>      </span>; <span class="inline-equation"><span class="tex">${ {{\rm T}}}^{ {{\rm U}}}, { {{\rm T}}}^{ {{\rm V}}} \in \mathbb {R}^{K \times R}$</span>      </span>. PIFT has a linear complexity and strong representation ability. Yet, it is not in line with practical applications due to the additive combination of each pair of matrices. For example, in PIFT, for certain clothes <em>q</em> liked by the user <em>p</em> but not fitting the current time <em>r</em>, <em>q</em> gets a high score for <em>p</em> and a low score for <em>r</em>. Intuitively it should not be recommended to the user since we want to recommend the right item in the right time. However, the total score can be high enough if <em>p</em> likes <em>q</em> so much that <em>q</em>&#x2019;s score for <em>p</em> is very high. In this case, <em>q</em> will be returned even it does not fit the time. In addition, PITF model is inappropriate to be trained with coupled matrices.</p>    </section>    <section id="sec-16">     <p><em>4.1.2 Dynamic Collaborative Filtering (DCF) Model.</em> To address the limitations of the aforementioned models, we propose a new tensor factorization method. When a user makes a purchase decision on a clothing product, there are two primary factors: if the product fits the user&#x0027;s preference and if it fits the time. A clothing product fits a user&#x0027;s preference if the appearance is appealing, the style fits the user&#x0027;s tastes, the quality is good, and the price is acceptable. And a clothing product fits the time if it is in-season and fashionable. For user <em>p</em>, clothing <em>q</em>, and time interval <em>r</em>, we use the scores <em>S</em>      <sub>1</sub> and <em>S</em>      <sub>2</sub> to indicate how the user likes the clothing and how the clothing fits the time respectively. <em>S</em>      <sub>1</sub> = 1 when the user likes the clothing and <em>S</em>      <sub>1</sub> = 0 otherwise. Similarly, <em>S</em>      <sub>2</sub> = 1 if the clothing fits the time and <em>S</em>      <sub>2</sub> = 0 otherwise. The consumer will buy the clothing only if <em>S</em>      <sub>1</sub> = 1 and <em>S</em>      <sub>2</sub> = 1, so, <span class="inline-equation"><span class="tex">$\hat{{ {{\rm A}}}}_{pqr} = S_1 \&#x0026; S_2$</span>      </span>. To make the formula differentiable, we can approximately formulate it as <span class="inline-equation"><span class="tex">$\hat{{ {{\rm A}}}}_{pqr} = S_1 \cdot S_2$</span>      </span>. We present <em>S</em>      <sub>1</sub> and <em>S</em>      <sub>2</sub> in the form of matrix factorization: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ S_1 = \sum _{i = 1}^{K_1} { {{\rm U}}}_{ip} { {{\rm V}}}_{iq} \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ S_2 = \sum _{j = 1}^{K_2} { {{\rm T}}}_{jr} { {{\rm W}}}_{jq}, \] </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">${ {{\rm U}}} \in \mathbb {R}^{K_1 \times P}$</span>      </span>, <span class="inline-equation"><span class="tex">${ {{\rm V}}} \in \mathbb {R}^{K_1 \times Q}$</span>      </span>, <span class="inline-equation"><span class="tex">${ {{\rm T}}} \in \mathbb {R}^{K_2 \times R}$</span>      </span>, and <span class="inline-equation"><span class="tex">${ {{\rm W}}} \in \mathbb {R}^{K_2 \times Q}$</span>      </span>. The prediction is then given by: <div class="table-responsive" id="eq1">       <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \hat{{ {{\rm A}}}}_{pqr} = \left({ {{\rm U}}}_{*p}^\mathsf {T} { {{\rm V}}}_{*q}\right) \left({ {{\rm T}}}_{*r}^\mathsf {T} { {{\rm W}}}_{*q}\right). \end{eqnarray} </span>       <br/>       <span class="equation-number">(1)</span>       </div>      </div> We can see that in Equation (<a class="eqn" href="#eq1">1</a>), the latent features relating users and clothes are independent with those relating clothes and time. Though <em>K</em>      <sub>1</sub>-dimensional vector V<sub>*<em>q</em>      </sub> and <em>K</em>      <sub>2</sub>-dimensional vector W<sub>*<em>q</em>      </sub> are all latent features of clothing <em>q</em>, V<sub>*<em>q</em>      </sub> captures the information about users&#x2019; preference intuitively whereas W<sub>*<em>q</em>      </sub> captures the temporal information of the clothing. Compared with CP decomposition, our model is more expressive in capturing the underlying latent patterns in purchases. Compared with PITF, combining <em>S</em>      <sub>1</sub> and <em>S</em>      <sub>2</sub> with &#x0026; (approximated by multiplication) is helpful to recommend right clothing in right time. Moreover, our model is efficient and easy to train compared with the Tucker decomposition.</p>    </section>    <section id="sec-17">     <p><em>4.1.3 Coupled Matrix and Tensor Factorization.</em> Though widely used to portray the context information in recommendation, tensor factorization suffers from poor convergence due to the sparsity of the tensor. To relieve this problem, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>] proposed a CMTF model, which decomposes the tensor with coupled matrices. In this subsection, we couple our tensor factorization model with restrained matrices during training.</p>     <p>      <strong>User &#x00D7; Clothing Matrix:</strong> We use matrix <span class="inline-equation"><span class="tex">${ {{\rm B}}} \in \mathbb {R}^{P \times Q}$</span>      </span> to indicate the purchase activities between users and clothes. B<sub>       <em>pq</em>      </sub> = 1 if the user <em>p</em> purchased clothing <em>q</em> and B<sub>       <em>pq</em>      </sub> = 0 if not.</p>     <p>      <strong>Time &#x00D7; Clothing Matrix:</strong> We use matrix <span class="inline-equation"><span class="tex">${ {{\rm C}}} \in \mathbb {R}^{R \times Q}$</span>      </span> to record when the clothing was purchased. Since the characteristics of clothing change steadily with time, we do a coarse-grained discretization on time to avoid the tensor from being extremely sparse. Time is divided into <em>R</em> intervals in total. C<sub>       <em>rq</em>      </sub> = 1 if the clothing <em>q</em> is purchased in time interval <em>r</em> and C<sub>       <em>rq</em>      </sub> = 0 if not.</p>     <p>      <strong>Objective Function Formulation:</strong> In existing works [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0043">43</a>], CMTF models are optimized by minimizing the sum of the squared error of each simulation (<span class="inline-equation"><span class="tex">$\rm MSE\_O\scriptstyle PT$</span>      </span>). It is represented as: <div class="table-responsive" id="eq2">       <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \left.\begin{aligned} \rm MSE\_O\scriptstyle PT = &#x0026;\frac{1}{2} {\left\Vert {{\rm A}} \!-\! \hat{ {{\rm A}}} \right\Vert }_{\rm F}^2 \!+\! \frac{\lambda _1}{2} {\left\Vert {{\rm B}} \!-\! \hat{ {{\rm B}}} \right\Vert }_{\rm F}^2 \!+\! \frac{\lambda _2}{2} {\left\Vert {{\rm C}} \!-\! \hat{ {{\rm C}}} \right\Vert }_{\rm F}^2\\ &#x0026;+\! \frac{\lambda _3}{2} {\left\Vert {{\rm U}} \right\Vert }_{\rm F}^2 \!+\! \frac{\lambda _4}{2} {\left\Vert {{\rm V}} \right\Vert }_{\rm F}^2 \!+\! \frac{\lambda _5}{2} {\left\Vert {{\rm T}} \right\Vert }_{\rm F}^2 \!+\! \frac{\lambda _6}{2} {\left\Vert {{\rm W}} \right\Vert }_{\rm F}^2, \end{aligned} \right. \qquad \end{eqnarray} </span>       <br/>       <span class="equation-number">(2)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$\hat{ {{\rm A}}}$</span>      </span> is defined in Equation (<a class="eqn" href="#eq1">1</a>), <span class="inline-equation"><span class="tex">$\hat{ {{\rm B}}} = {{\rm U}}^\mathsf {T} {{\rm V}}$</span>      </span>, <span class="inline-equation"><span class="tex">$\hat{ {{\rm C}}} = {{\rm T}}^\mathsf {T} {{\rm W}}$</span>      </span>, and &#x2016;&#x2009;&#x2009;&#x2016;<sub>F</sub> is the Frobenius norm of the matrix. The last four terms of Equation (<a class="eqn" href="#eq2">2</a>) are the regularization terms to prevent overfitting. Although the pointwise squared loss has been widely used in recommendation, it is not directly optimized for ranking. To get better top-<em>n</em> performance, we next introduce our hybrid model with BPR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0033">33</a>] optimization criterion.</p>    </section>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Hybrid Model</h3>     </div>    </header>    <section id="sec-19">     <p><em>4.2.1 Problem Formulation.</em> Combined with image features, we formulate the predictive model as: <div class="table-responsive" id="eq3">       <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \hat{{ {{\rm A}}}}_{pqr} = \left({ {{\rm U}}}_{*p}^\mathsf {T} { {{\rm V}}}_{*q} + { {{\rm M}}}_{*p}^\mathsf {T} { {{\rm F}}}_{*q}\right) \left({ {{\rm T}}}_{*r}^\mathsf {T} { {{\rm W}}}_{*q} + { {{\rm N}}}_{*r}^\mathsf {T} { {{\rm F}}}_{*q}\right), \end{eqnarray} </span>       <br/>       <span class="equation-number">(3)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">${ {{\rm F}}} \in \mathbb {R}^{K \times Q}$</span>      </span> is the feature matrix, F<sub>*<em>q</em>      </sub> is the image features of clothing <em>q</em>, which is the concatenation of CNN features (f<sub>       <em>CNN</em>      </sub>) and aesthetic features (f<sub>       <em>AES</em>      </sub>), <span class="inline-equation"><span class="tex">${ {{\rm F}}}_{*q} = \left[ {\begin{array}{*10c} {{\rm f}}_{CNN}\\ { {{\rm f}}}_{AES} \end{array}} \right]$</span>      </span> and <em>K</em> = 8192. <span class="inline-equation"><span class="tex">${ {{\rm M}}} \in \mathbb {R}^{K \times P}$</span>      </span> and <span class="inline-equation"><span class="tex">${ {{\rm N}}} \in \mathbb {R}^{K \times R}$</span>      </span> are aesthetic preference matrices. M<sub>*<em>p</em>      </sub> encodes the preference of user <em>p</em> and N<sub>*<em>r</em>      </sub> encodes the preference in time interval <em>r</em>. In our model, both the latent features and image features contribute to the final prediction. Though the latent features can uncover any relevant attribute theoretically, they usually cannot in real-world applications on account of the sparsity of the data and lack of information. So the assistance of image information can highly enhance the model. Also, recommender systems often suffer from the <em>cold start</em> problem. It is hard to extract information from users and clothes without consumption records. In this case, content and context information can alleviate this problem. For example, for certain &#x201C;cold&#x201D; clothing <em>q</em>, we can decide whether to recommend it to certain consumer <em>p</em> in current time <em>r</em> according to if <em>q</em> looks satisfying to the consumer (determined by M<sub>*<em>p</em>      </sub>) and to the time (determined by N<sub>*<em>r</em>      </sub>). <figure id="fig3">       <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig3.jpg" class="img-responsive" alt="Figure 3"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Diagram of our preference predictor.</span>       </div>      </figure>     </p>    </section>    <section id="sec-20">     <p><em>4.2.2 Model Learning.</em> The model is optimized with BPR optimization criterion from users&#x2019; <em>implicit feedback</em> (purchase record) with mini-batch gradient descent, which calculates the gradient with a small batch of samples. BPR is a pairwise ranking optimization framework and we represent the training set <em>D</em> into three different forms: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ D_{pr} = \lbrace (p,q,q^{\prime },r)|p\in \mathcal {P} \,\wedge \, r\in \mathcal {R} \,\wedge \, q\in \mathcal {Q}^+_{pr} \,\wedge \, q^{\prime }\in \mathcal {Q} \setminus \mathcal {Q}^+_{pr}\rbrace , \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ D_{p} = \lbrace (p,q,q^{\prime })|p\in \mathcal {P} \,\wedge \, q\in \mathcal {Q}^+_{p} \,\wedge \, q^{\prime }\in \mathcal {Q} \setminus \mathcal {Q}^+_{p}\rbrace , \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ D_{r} = \lbrace (r,q,q^{\prime })|r\in \mathcal {R} \,\wedge \, q\in \mathcal {Q}^+_{r} \,\wedge \, q^{\prime }\in \mathcal {Q} \setminus \mathcal {Q}^+_{r}\rbrace , \] </span>       <br/>       </div>      </div> where <em>u</em> denotes the user, <em>r</em> represents the time, <em>q</em> represents the positive feedback, and <em>q</em>&#x2032; represents the non-observed item. The objective function is formulated as: <div class="table-responsive" id="eq4">       <div class="display-equation">       <span class="tex mytex">\begin{align} \rm BPR\_O\scriptstyle PT = &#x0026;\sum _{(p,q,q^{\prime },r)\in D_{pr}} \!\! {\rm ln} \, \sigma (\hat{ {{\rm A}}}_{pqq^{\prime }r}) + \lambda _1 \!\! \sum _{(p,q,q^{\prime })\in D_{p}} \!\! {\rm ln} \, \sigma (\hat{ {{\rm B}}}_{pqq^{\prime }}) \nonumber \\ +&#x0026;\lambda _2 \!\! \sum _{(r,q,q^{\prime })\in D_{r}} \!\! {\rm ln} \, \sigma (\hat{ {{\rm C}}}_{rqq^{\prime }}) - { \lambda }_{\rm \Theta } {\left\Vert {\rm \Theta } \right\Vert }_{\rm F}^2 \, , \end{align} </span>       <br/>       <span class="equation-number">(4)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$\hat{ {{\rm A}}}$</span>      </span> is defined in the Equation (<a class="eqn" href="#eq3">3</a>), <span class="inline-equation"><span class="tex">$\hat{ {{\rm B}}} = { {{\rm U}}}^\mathsf {T} { {{\rm V}}} + { {{\rm M}}}^\mathsf {T} { {{\rm F}}}$</span>      </span>, and <span class="inline-equation"><span class="tex">$\hat{ {{\rm C}}} = { {{\rm T}}}^\mathsf {T} { {{\rm W}}} + { {{\rm N}}}^\mathsf {T} { {{\rm F}}}$</span>      </span>; <span class="inline-equation"><span class="tex">$\hat{ {{\rm A}}}_{pqq^{\prime }r} = \hat{ {{\rm A}}}_{pqr} - \hat{ {{\rm A}}}_{pq^{\prime }r}$</span>      </span>, <span class="inline-equation"><span class="tex">$\hat{ {{\rm B}}}_{pqq^{\prime }} = \hat{ {{\rm B}}}_{pq} - \hat{ {{\rm B}}}_{pq^{\prime }}$</span>      </span>, <span class="inline-equation"><span class="tex">$\hat{ {{\rm C}}}_{rqq^{\prime }} = \hat{ {{\rm C}}}_{rq} - \hat{ {{\rm C}}}_{rq^{\prime }}$</span>      </span>; <em>&#x03C3;</em> is the sigmoid function; &#x0398; = {U, V, T, W, M, N} and &#x03BB;<sub>&#x0398;</sub> = {<em>&#x03BB;</em>      <sub>3</sub>, &#x2026;, <em>&#x03BB;</em>      <sub>8</sub>} respectively. We then calculate the gradient of Equation (<a class="eqn" href="#eq4">4</a>). To maximize the objective function, we take the first-order derivatives with respect to each model parameter: <div class="table-responsive" id="eq5">       <div class="display-equation">       <span class="tex mytex">\begin{align} {\nabla _{ {{\rm \Theta }}}{\rm BPR\_O\scriptstyle PT}} = &#x0026;\sigma (-\hat{ {{\rm A}}}_{pqq^{\prime }r})\frac{\partial \hat{ {{\rm A}}}_{pqq^{\prime }r}}{\partial {\rm \Theta }} + \lambda _1 \sigma (-\hat{ {{\rm B}}}_{pqq^{\prime }})\frac{\partial \hat{ {{\rm B}}}_{pqq^{\prime }}}{\partial {\rm \Theta }} \nonumber \\ + &#x0026;\lambda _2 \sigma (-\hat{ {{\rm C}}}_{rqq^{\prime }})\frac{\partial \hat{ {{\rm C}}}_{rqq^{\prime }}}{\partial {\rm \Theta }} - { \lambda }_{\rm \Theta }{\rm \Theta }. \end{align} </span>       <br/>       <span class="equation-number">(5)</span>       </div>      </div> We use &#x03B8; to denote certain column of &#x0398;. For our DCFA model, the derivatives are: <div class="table-responsive" id="eq6">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \frac{\partial \hat{ {{\rm A}}}_{pqq^{\prime }r}}{\partial {\rm \theta }} = \left\lbrace \begin{array}{lcl}\hat{ {{\rm C}}}_{rq} { {{\rm V}}}_{*q} - \hat{ {{\rm C}}}_{rq^{\prime }} { {{\rm V}}}_{*q^{\prime }} &#x0026;\text{if} &#x0026;{\rm \theta } = { {{\rm U}}}_{*p} \\ {\hat{ {{\rm C}}}_{rq} { {{\rm U}}}_{*p} / -\hat{ {{\rm C}}}_{rq^{\prime }} { {{\rm U}}}_{*p}} &#x0026;\text{if} &#x0026;{\rm \theta } = { {{\rm V}}}_{*q} / { {{\rm V}}}_{*q^{\prime }} \\ {\hat{ {{\rm C}}}_{rq} { {{\rm F}}}_{*q} - \hat{ {{\rm C}}}_{rq^{\prime }} { {{\rm F}}}_{*q^{\prime }}} &#x0026;\text{if} &#x0026;{\rm \theta } = { {{\rm M}}}_{*p} \end{array} \right. \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>       </div>      </div>      <div class="table-responsive" id="eq7">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \frac{\partial \hat{ {{\rm B}}}_{pqq^{\prime }}}{\partial {\rm \theta }} = \left\lbrace \begin{array}{lcl}{ {{\rm V}}}_{*q} - { {{\rm V}}}_{*q^{\prime }} &#x0026;\text{if} &#x0026;{\rm \theta } = { {{\rm U}}}_{*p} \\ {{ {{\rm U}}}_{*p} / -{ {{\rm U}}}_{*p}} &#x0026;\text{if} &#x0026;{\rm \theta } = { {{\rm V}}}_{*q} / { {{\rm V}}}_{*q^{\prime }} \\ {{ {{\rm F}}}_{*q} - { {{\rm F}}}_{*q^{\prime }}} &#x0026;\text{if} &#x0026;{\rm \theta } = { {{\rm M}}}_{*p} \end{array} \right. \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>       </div>      </div> Equations (<a class="eqn" href="#eq6">6</a>) and (<a class="eqn" href="#eq7">7</a>) give the derivatives for &#x0398; = {U, V, M}, and we can get the similar form for &#x0398; = {T, W, N}.</p>     <p>      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-img1.svg" class="img-responsive" alt=""       longdesc=""/>     </p>     <p>We exploit the mini-batch gradient descent to maximize the objective function. For each iteration, all positive samples are enumerated (lines 3-12). We compute the gradients with a batch, including <em>b</em> positive samples (line 5) and 5<em>b</em> negative samples (lines 7-9) to construct 5<em>b</em> preference pairs, and update the parameters (line 11). To calculate the gradients (line 10), we combine Equations (<a class="eqn" href="#eq5">5</a>) with (<a class="eqn" href="#eq6">6</a>) and (<a class="eqn" href="#eq7">7</a>). Of special note is that <span class="inline-equation"><span class="tex">$\frac{\partial \hat{ {{\rm A}}}_{pqq^{\prime }r}}{\partial {\rm \theta }}$</span>      </span> in Equation (<a class="eqn" href="#eq6">6</a>) is certain column of <span class="inline-equation"><span class="tex">$\frac{\partial \hat{ {{\rm A}}}_{pqq^{\prime }r}}{\partial {\rm \Theta }}$</span>      </span> in Equation (<a class="eqn" href="#eq5">5</a>), for example, the <em>p</em>-th column when &#x03B8; = U<sub>*<em>p</em>      </sub>.</p>    </section>    </section>   </section>   <section id="sec-21">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiment</h2>    </div>    </header>    <p>In this section, we conduct experiments on real-world datasets to verify the feasibility of our proposed model. We then analyze the experiment results and demonstrate the precision promotion by comparing it with various baselines. We focus on answering the following three key research questions:</p>    <p>    <strong>RQ1:</strong> How is the performance of our final framework for the clothing recommendation task?</p>    <p>    <strong>RQ2:</strong> What are the advantages of the aesthetic features compared with conventional image features?</p>    <p>    <strong>RQ3:</strong> Is it reasonable to transfer the knowledge gained from <em>AVA</em>, which is a dataset of photographic competition works, to the clothing aesthetics assessment task?</p>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Experimental Setup</h3>     </div>    </header>    <section id="sec-23">     <p><em>5.1.1 Datasets.</em> We use the <em>AVA</em> dataset to train the aesthetic network and use the <em>Amazon</em> dataset to train the recommendation models.</p>     <ul class="list-no-style">      <li id="list4" label="&#x2022;"><strong>Amazon clothing:</strong> The <em>Amazon</em> dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>] is the consumption records from <em>Amazon.com</em>. In this paper, we use the <em>clothing shoes and jewelry</em> category filtered with <em>5-score</em> (remove users and items with less than 5 purchase records) to train all recommendation models. There are 39,371 users, 23,022 items, and 278,677 records in total (after 2010). The sparsity of the dataset is 99.969%.<br/></li>      <li id="list5" label="&#x2022;"><strong>Aesthetic Visual Analysis (AVA):</strong> We train the aesthetic network with the <em>AVA</em> dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>], which is the collection of images and meta-data derived from <em>DPChallenge.com</em>. It contains over 250,000 images with aesthetic ratings from 1 to 10, 66 textual tags describing the semantics of images, and 14 photographic styles (complementary colors, duotones, high dynamic range, image grain, light on white, long exposure, macro, motion blur, negative image, rule of thirds, shallow DOF, silhouettes, soft focus, and vanishing point).<br/></li>     </ul>    </section>    <section id="sec-24">     <p><em>5.1.2 Baselines.</em> To demonstrate the effectiveness of our model, we adopt the following methods as baselines for performance comparison:</p>     <ul class="list-no-style">      <li id="list6" label="&#x2022;"><strong>Random (RAND):</strong> This baseline ranks items randomly for all users.<br/></li>      <li id="list7" label="&#x2022;"><strong>Most Popular (MP):</strong> This baseline ranks items according to their popularity and is non-personalized.<br/></li>      <li id="list8" label="&#x2022;"><strong>MF:</strong> This <strong>M</strong>atrix <strong>F</strong>actorization method ranks items according to the prediction provided by a singular value decomposition structure. It is the basis of many state-of-the-art recommendation approaches.<br/></li>      <li id="list9" label="&#x2022;"><strong>VBPR:</strong> This is a stat-of-the-art visual-based recommendation method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>]. The image features are pre-generated from the product image using the Caffe deep learning framework.<br/></li>      <li id="list10" label="&#x2022;"><strong>CMTF:</strong> This is a stat-of-the-art context-aware recommendation method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>]. The tensor factorization is jointly learned with several coupled matrices.<br/></li>     </ul>    </section>    <section id="sec-25">     <p><em>5.1.3 Experiment Settings.</em> In the <em>Amazon</em> dataset, we remove the record before 2010 and discretize the time by weeks. There are 237 time intervals, the sparsity of the tensor is 99.99987%. We randomly split the dataset into training (80%), validation (10%), and test (10%) sets. The validation set was used for tuning hyper-parameters and the final performance comparison was conducted on the test set. We do the prediction and recommend the top-<em>n</em> items to consumers. The Recall and the normalized discounted cumulative gain (NDCG) are calculated to evaluate the performance of the baselines and our model. When <em>n</em> is fixed, the Precision is only determined by true positives whereas the Recall is determined by both true positives and positive samples. To give a more comprehensive evaluation, we exhibit the Recall rather than the Precision and <em>F</em>      <sub>1</sub>-score (<em>F</em>      <sub>1</sub>-score is almost determined by the Precision since the Precision is much smaller than the Recall in our experiments). Our experiments are conducted by predicting Top-5, 10, 20, 50, and 100 favorite clothing.</p>    </section>    </section>    <section id="sec-26">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Performance of Our Model (RQ1)</h3>     </div>    </header>    <figure id="fig4">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Performance with training iterations (test set).</span>     </div>    </figure>    <p>We iterate 200 times to train all models (except RAND and MP). In each iteration, we enumerate all positive records to optimize models and select 1000 users in test (or validation) set to calculate evaluation metrics, then show the best performance every 10 iterations. Figure 4(a) shows the Recall and Figure 4(b) shows the NDCG during training. We set <em>n</em> = 50 when representing the Recall and <em>n</em> = 5 when representing the NDCG, due to the relatively large value respectively (represented in Figure <a class="fig" href="#fig5">5</a>). We can see that NDCG@5 shows a heavier fluctuation than Recall@50 (Figure <a class="fig" href="#fig4">4</a> and Figure <a class="fig" href="#fig7">7</a>) since a smaller <em>n</em> leads to a more random prediction. Compared with MP, personalized methods show stronger ability to represent the preference of users and outperform MP several times. By recommending clothes that fit the current season, CMTF can outperform MF on both Recall and NDCG. Enhanced by side information, VBPR performs the best among all baselines. The proposed DCFA model outperforms VBPR about 8.53% on Recall@50 and 8.73% on NDCG@5. <figure id="fig5">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Performance with different <em>n</em> (test set).</span>      </div>     </figure>    </p>    <p>Figure <a class="fig" href="#fig5">5</a> represents the variation of the Recall and the NDCG with different <em>n</em>. In Figure 5(a), we can see that the Recall increases almost linearly with the increasing of <em>n</em> while in Figure 5(b), for most methods (except RAND), the NDCG decreases with the increasing of <em>n</em>. Since for most models (except RAND), the higher-rated clothing is with more possibility to be chosen by consumers. So the ordering quality decreases with the increasing of <em>n</em>. To the contrary, since RAND orders all items randomly, its ordering quality keeps constant. <figure id="fig6">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Impacts of hyperparameters (validation set).</span>      </div>     </figure>    </p>    <p>In our experiments, we tune all hyperparameters sequentially on the validation set (include those in our model and in baselines). There are 8 hyperparameters in Equation (<a class="eqn" href="#eq4">4</a>) and the sensitivity analysis is shown in Figure <a class="fig" href="#fig6">6</a>. We can see that when <em>&#x03BB;</em>     <sub>1</sub> = 0.1, <em>&#x03BB;</em>     <sub>2</sub> = 0.1, <em>&#x03BB;</em>     <sub>3</sub> = 0.3, <em>&#x03BB;</em>     <sub>4</sub> = 0.3, <em>&#x03BB;</em>     <sub>5</sub> = 0.5, <em>&#x03BB;</em>     <sub>6</sub> = 0.2, <em>&#x03BB;</em>     <sub>7</sub> = 0.5, <em>&#x03BB;</em>     <sub>8</sub> = 0.5, DCFA can achieve the best performance. Influences of hyperparameters in baselines are also shown in Figure <a class="fig" href="#fig6">6</a>. For all models, <em>&#x03BB;</em>     <sub>1</sub> and <em>&#x03BB;</em>     <sub>2</sub> are used to represent weights of the coupled user-item matrix and time-item matrix. <em>&#x03BB;</em>     <sub>3</sub> to <em>&#x03BB;</em>     <sub>8</sub> are regularization coefficients of the user matrix, item matrix (connecting with user), time matrix, item matrix (connecting with time), aesthetic preference matrix of consumers, and aesthetic preference matrix of time respectively. For example, we can see that the performance of MF varies with regularization coefficients of the user matrix (<em>&#x03BB;</em>     <sub>3</sub>) and the item matrix (connecting with user, <em>&#x03BB;</em>     <sub>4</sub>), while keeps constant with the variation of <em>&#x03BB;</em>     <sub>5</sub> because there is no time matrix in MF. Specially, in CMTF, the item matrix connects both the user and time matrices, we use <em>&#x03BB;</em>     <sub>3</sub>, <em>&#x03BB;</em>     <sub>4</sub>, <em>&#x03BB;</em>     <sub>5</sub> to represent the regularization coefficients of the user, item, and time matrices respectively.</p>    </section>    <section id="sec-27">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Necessity of the aesthetic features (RQ2)</h3>     </div>    </header>    <p>In this subsection, we discuss the necessity of the aesthetic features. We combine various widely used features to our basic model and compare the effect of each features by constructing five models:</p>    <ul class="list-no-style">     <li id="list11" label="&#x2022;"><strong>DCF:</strong> This is our basic <strong>D</strong>ynamic <strong>C</strong>ollaborative <strong>F</strong>iltering model without any image features, which is represented in the subsection 4.1.<br/></li>     <li id="list12" label="&#x2022;"><strong>DCFH:</strong> This is a <strong>D</strong>ynamic <strong>C</strong>ollaborative <strong>F</strong>iltering model with Color <strong>H</strong>istograms.<br/></li>     <li id="list13" label="&#x2022;"><strong>DCFCo:</strong> This is a <strong>D</strong>ynamic <strong>C</strong>ollaborative <strong>F</strong>iltering model with <strong>C</strong>NN Features <strong>o</strong>nly.<br/></li>     <li id="list14" label="&#x2022;"><strong>DCFAo:</strong> This is a <strong>D</strong>ynamic <strong>C</strong>ollaborative <strong>F</strong>iltering model with <strong>A</strong>esthetics Features <strong>o</strong>nly.<br/></li>     <li id="list15" label="&#x2022;"><strong>DCFA:</strong> This is our proposed model represented in the subsection 4.2, utilizing both CNN features and aesthetic features.<br/></li>    </ul>    <figure id="fig7">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig7.jpg" class="img-responsive" alt="Figure 7"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 7:</span>      <span class="figure-title">Performance of various features (test set).</span>     </div>    </figure>    <p>Figures 7(a) and 7(b) show the distribution of 10 maximum on Recall@50 and the NDCG@5 of each model during the 200 iterations. As shown in Figure <a class="fig" href="#fig7">7</a>, DCF performs the worst since no image features are involved to provide the extra information. With the information of color distribution, DCFH performs better, though still worse than DCFCo and DCFAo, because the low-level features are too crude and unilateral, and can provide very limited information about consumers&#x2019; aesthetic preference. DCFCo and DCFAo show the similar performance because both CNN features and aesthetic features have strong ability to mine the user&#x0027;s preference. Our DCFA model, capturing both semantic information and aesthetic information, performs the best on the <em>Amazon</em> dataset since those two kinds of information mutually enhance each other to a certain extent. Give an intuitive example, if a consumer want to purchase a skirt, she needs to tell whether there is a skirt in the image (semantic information) when look through products, and then she needs to evaluate if the skirt is good-looking and fits her tastes (aesthetic information) to make the final decision. We can see that in the actual scene, semantic information and aesthetic information are both important for decision making and the two kinds of features complement each other in modeling this procedure. Though CNN features also contain some aesthetic information (like color, texture, etc.), it is far from a comprehensive description, which can be provide by the aesthetic features on account of the abundant raw aesthetic features inputted and training for aesthetic assessment tasks. Also, aesthetic features contain some holistic information (like structure and portion), while cannot provide a complete semantic description. So, these two kind of features cannot replace each other and are supposed to model users&#x2019; preference collaboratively. In our experiments, DCFA outperforms DCFCo and DCFAo about 5.06% and 8.79% on Recall@50, 4.89% and 8.51% on NDCG@5 respectively. We can see that though the aesthetic features and CNN features do not perform the best separately, they mutually enhance each other and achieve improvement together. <figure id="fig8">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186146/images/www2018-155-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">Items purchased by consumers and recommended by different models.</span>      </div>     </figure>    </p>    <p>Several purchased and recommended items are represented in the Figure <a class="fig" href="#fig8">8</a>. Items in the first row are purchased by certain consumer (training data, the number is random). To illustrate the effect of the aesthetic features intuitively, we choose the consumers with explicit style preference and single category of items. Items in the second row and third row are recommended by DCFCo and DCFA respectively. For these two rows, we choose five best items from the 50 recommendations to exhibit. Comparing the first and the second row, we can see that leveraging semantic information, DCFCo can recommend the congeneric (with the CNN features) and relevant (with tensor factorization) commodities. Though can it recommend the pertinent products, they are usually not in the same style with what the consumer has purchased. Capturing both aesthetic and semantic information, DCFA performs much better. We can see that items in the third row have more similar style with the training samples than items in the second row. Take Figure 8(f) as an example, we can see that what the consumer likes are vibrant watches for young men. However, watches in the second row are in pretty different styles, like digital watches for children, luxuriantly-decorated ones for ladies, old-fashioned ones for adults. Evidently, watches in the third row are in similar style with the train samples. They have similar color schemes and design elements, like the intricatel-designed dials, nonmetallic watchbands, small dials, and tachymeters. It is also obvious in Figure 8(c), we can see that the consumer prefers boots, ankle boots or thigh boots. However, products recommended by DCFCo are some different type of women&#x0027;s shoes, like high heels, snow boots, thigh boots, and cotton slippers. Though there is a thigh boot, it is not in line with the consumer&#x0027;s aesthetics due to the gaudy patterns and stumpy proportion, which rarely appears in her choices. Products recommended by DCFA are better. First, almost all recommendations are boots. Then, thigh boots in the third row are in the same style with the training samples, like leather texture, slender proportions, simple design and some design elements of detail like straps and buckles (the second and third ones). Though the last one seems a bit different with the training samples, it is in the uniform style with them intuitively, since they are all designed for young ladies. As we can see, with the aesthetic features and the CNN features complementing each other, DCFA performs much better than DCFCo.</p>    </section>    <section id="sec-28">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Rationality of using the <em>AVA</em> dataset (RQ3)</h3>     </div>    </header>    <p>The BDN is trained on the <em>AVA</em> dataset, which contains photographic works labeled with aesthetic ratings, textual tags, and photographic styles. We utilize aesthetic ratings and photographic styles to train the aesthetic network. In this subsection, we simply discuss if it is reasonable to estimate clothing by the features trained for photographic assessment.</p>    <p>With no doubt that there are many similarities between esthetical photographs and well-designed clothing, like delightful color combinations, saturation, brightness, structures, proportion, etc. Of course, there are also many differences. To address this gap, we modify the BDN. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>], there are 14 pathways to captures all photographic styles. In this paper, we remove several pathways for the photographic styles which contribute little in clothing estimation, like high dynamic range, long exposure, macro, motion blur, shallow DOF, and soft focus. These features mainly describe the camera parameters setting or photography skills but not the image, so they help little in our clothing aesthetic assessment task. Experiments show that our proposed model can uncover consumers&#x2019; aesthetic preference and recommend the clothing that are in line with their aesthetics, and the performance is obviously promoted.</p>    <p>There are many works recommending clothing or garments with fashion information [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] and there are several datasets for clothing fashion style. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] utilized three datasets containing street fashion images and annotations by fashionistas to train phase, input queries, and return ranked list respectively. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] proposed a novel dataset crawled from <em>chictopia.com</em> containing photographs, text in the form of descriptions, votes, and garment tags. However, these datasets are mainly for fashion style and not appropriate for BDN training because of the lack of aesthetic ratings and style tags, so we choose <em>AVA</em>. There are abundant images and tags to provide raw aesthetic features. Though not all raw features are needed due to the gap of photographic works and clothing, many of them are important in clothing aesthetic assessment. Beyond that, our model should have ability to extend to a wider range of application scenarios, like the recommendation of electronic products, movies, toys, etc., so a general dataset for aesthetic network training is important.</p>    </section>   </section>   <section id="sec-29">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>In this paper, we investigated the usefulness of aesthetic features for personalized recommendation on implicit feedback datasets. We proposed a novel model that incorporates aesthetic features into a tensor factorization model to capture the aesthetic preference of consumers at a particular time. Experiments on challenging real-word datasets show that our proposed method dramatically outperforms state-of-the-art models, and succeeds in recommending items that fit consumers&#x2019; style.</p>    <p>For future work, we will establish a large dataset for product aesthetic assessment, and train the networks to extract the aesthetic information better. Moreover, we will investigate the effectiveness of our proposed method in the setting of explicit feedback. Lastly, we are interested in integrating the domain knowledge about aesthetic assessment, e.g., in the form of decision rules&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>], into the recommender model.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Evrim Acar, Tamara&#x00A0;G. Kolda, and Daniel&#x00A0;M. Dunlavy. 2011. All-at-once Optimization for Coupled Matrix and Tensor Factorizations. <em>      <em>Computing Research Repository - CORR</em>     </em>abs/1105.3422 (2011). arxiv:1105.3422</li>    <li id="BibPLXBIB0002" label="[2]">Evrim Acar, Tamara&#x00A0;G Kolda, Daniel&#x00A0;M Dunlavy, and Morten Morup. 2010. Scalable Tensor Factorizations for Incomplete Data. <em>      <em>Chemometrics and Intelligent Laboratory Systems</em>     </em>106, 1 (2010), 41&#x2013;56.</li>    <li id="BibPLXBIB0003" label="[3]">Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Steffen Rendle. 2017. A Generic Coordinate Descent Framework for Learning from Implicit Feedback. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web</em>     </em>(<em>WWW &#x2019;17</em>). 1341&#x2013;1350.</li>    <li id="BibPLXBIB0004" label="[4]">Yoshua Bengio. 2009. Learning Deep Architectures for AI. <em>      <em>Found. Trends Mach. Learn.</em>     </em>2, 1 (Jan. 2009), 1&#x2013;127.</li>    <li id="BibPLXBIB0005" label="[5]">Preeti Bhargava, Thomas Phan, Jiayu Zhou, and Juhan Lee. 2015. Who, What, When, and Where: Multi-Dimensional Collaborative Recommendations Using Tensor Factorization on Sparse User-Generated Data. In <em>      <em>Proceedings of the 24th International Conference on World Wide Web</em>     </em>(<em>WWW &#x2019;15</em>). 130&#x2013;140.</li>    <li id="BibPLXBIB0006" label="[6]">A.&#x00A0;M. Buchanan and A.&#x00A0;W. Fitzgibbon. 2005. Damped Newton algorithms for matrix factorization with missing data. In <em>      <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>     </em>(<em>CVPR &#x2019;05</em>), Vol.&#x00A0;2. 316&#x2013;322 vol. 2.</li>    <li id="BibPLXBIB0007" label="[7]">Da Cao, Liqiang Nie, Xiangnan He, Xiaochi Wei, Shunzhi Zhu, and Tat-Seng Chua. 2017. Embedding Factorization Models for Jointly Recommending Items and User Generated Lists. In <em>      <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;17</em>). 585&#x2013;594.</li>    <li id="BibPLXBIB0008" label="[8]">Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-Seng Chua. 2017. Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention. In <em>      <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;17</em>). 335&#x2013;344.</li>    <li id="BibPLXBIB0009" label="[9]">Tao Chen, Xiangnan He, and Min-Yen Kan. 2016. Context-aware Image Tweet Modelling and Recommendation. In <em>      <em>Proceedings of the 2016 ACM on Multimedia Conference</em>     </em>(<em>MM &#x2019;16</em>). 1018&#x2013;1027.</li>    <li id="BibPLXBIB0010" label="[10]">Xu Chen, Zheng Qin, Yongfeng Zhang, and Tao Xu. 2016. Learning to Rank Features for Recommendation over Multiple Categories. In <em>      <em>Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;16</em>). 305&#x2013;314.</li>    <li id="BibPLXBIB0011" label="[11]">Xu Chen, Pengfei Wang, Zheng Qin, and Yongfeng Zhang. 2016. HLBPR: A Hybrid Local Bayesian Personal Ranking Method. In <em>      <em>Proceedings of the 25th International Conference Companion on World Wide Web</em>     </em>(<em>WWW &#x2019;16 Companion</em>). 21&#x2013;22.</li>    <li id="BibPLXBIB0012" label="[12]">Xu Chen, Yongfeng Zhang, Qingyao Ai, Hongteng Xu, Junchi Yan, and Zheng Qin. 2017. Personalized Key Frame Recommendation. In <em>      <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;17</em>). 315&#x2013;324.</li>    <li id="BibPLXBIB0013" label="[13]">Qiang Cui, Shu Wu, Qiang Liu, and Liang Wang. 2016. A Visual and Textual Recurrent Neural Network for Sequential Prediction. <em>      <em>arXiv preprint arXiv:1611.06668</em>     </em>(2016).</li>    <li id="BibPLXBIB0014" label="[14]">Ritendra Datta, Dhiraj Joshi, Jia Li, and James&#x00A0;Z Wang. 2006. Studying Aesthetics in Photographic Images Using a Computational Approach. In <em>      <em>Proceedings of the 9th European Conference on Computer Vision</em>     </em>(<em>ECCV &#x2019;06</em>). 288&#x2013;301.</li>    <li id="BibPLXBIB0015" label="[15]">Ruining He and Julian McAuley. 2016. VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback. In <em>      <em>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</em>     </em>(<em>AAAI &#x2019;16</em>). 144&#x2013;150.</li>    <li id="BibPLXBIB0016" label="[16]">Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse Predictive Analytics. In <em>      <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;17</em>). 355&#x2013;364.</li>    <li id="BibPLXBIB0017" label="[17]">Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web</em>     </em>(<em>WWW &#x2019;17</em>). 173&#x2013;182.</li>    <li id="BibPLXBIB0018" label="[18]">Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast Matrix Factorization for Online Recommendation with Implicit Feedback. In <em>      <em>Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;16</em>). 549&#x2013;558.</li>    <li id="BibPLXBIB0019" label="[19]">Bal&#x00E1;zs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos Tikk. 2016. Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations. In <em>      <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>     </em>(<em>RecSys &#x2019;16</em>). 241&#x2013;248.</li>    <li id="BibPLXBIB0020" label="[20]">Vignesh Jagadeesh, Robinson Piramuthu, Anurag Bhardwaj, Wei Di, and Neel Sundaresan. 2014. Large Scale Visual Recommendations from Street Fashion Images. In <em>      <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>(<em>KDD &#x2019;14</em>). 1925&#x2013;1934.</li>    <li id="BibPLXBIB0021" label="[21]">Alexandros Karatzoglou, Xavier Amatriain, Linas Baltrunas, and Nuria Oliver. 2010. Multiverse Recommendation: N-dimensional Tensor Factorization for Context-aware Collaborative Filtering. In <em>      <em>Proceedings of the Fourth ACM Conference on Recommender Systems</em>     </em>(<em>RecSys &#x2019;10</em>). 79&#x2013;86.</li>    <li id="BibPLXBIB0022" label="[22]">Yan Ke, Xiaoou Tang, and Feng Jing. 2006. The Design of High-Level Features for Photo Quality Assessment. In <em>      <em>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>     </em>(<em>CVPR &#x2019;06</em>), Vol.&#x00A0;1. 419&#x2013;426.</li>    <li id="BibPLXBIB0023" label="[23]">Tamara&#x00A0;G. Kolda and Brett&#x00A0;W. Bader. 2009. Tensor Decompositions and Applications. <em>      <em>Siam Review</em>     </em>51, 3 (2009), 455&#x2013;500.</li>    <li id="BibPLXBIB0024" label="[24]">Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. <em>      <em>Computer</em>     </em>42, 8 (2009), 30&#x2013;37.</li>    <li id="BibPLXBIB0025" label="[25]">Xin Lu, Zhe Lin, Hailin Jin, Jianchao Yang, and James&#x00A0;Zijun Wang. 2014. RAPID: Rating Pictorial Aesthetics using Deep Learning. In <em>      <em>Proceedings of the ACM International Conference on Multimedia</em>     </em>(<em>MM &#x2019;14</em>). 457&#x2013;466.</li>    <li id="BibPLXBIB0026" label="[26]">Wei Luo, Xiaogang Wang, and Xiaoou Tang. 2013. Content-Based Photo Quality Assessment. <em>      <em>IEEE Transactions on Multimedia</em>     </em>15, 8 (Dec 2013), 1930&#x2013;1943.</li>    <li id="BibPLXBIB0027" label="[27]">Shuang Ma, Jing Liu, and Chang&#x00A0;Wen Chen. 2017. A-Lamp: Adaptive Layout-Aware Multi-patch Deep Convolutional Neural Network for Photo Aesthetic Assessment. In <em>      <em>2017 IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>(<em>CVPR &#x2019;17</em>). 722&#x2013;731.</li>    <li id="BibPLXBIB0028" label="[28]">Luca Marchesotti, Florent Perronnin, Diane Larlus, and Gabriela Csurka. 2011. Assessing the aesthetic quality of photographs using generic image descriptors. In <em>      <em>2011 International Conference on Computer Vision</em>     </em>(<em>ICCV &#x2019;06</em>). 1784&#x2013;1791.</li>    <li id="BibPLXBIB0029" label="[29]">Benjamin&#x00A0;M. Marlin. 2003. Modeling User Rating Profiles For Collaborative Filtering. In <em>      <em>International Conference on Neural Information Processing Systems</em>     </em>(<em>NIPS &#x2019;03</em>). 627&#x2013;634.</li>    <li id="BibPLXBIB0030" label="[30]">Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van&#x00A0;den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In <em>      <em>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;15</em>). 43&#x2013;52.</li>    <li id="BibPLXBIB0031" label="[31]">Naila Murray, Luca Marchesotti, and Florent Perronnin. 2012. AVA: A large-scale database for aesthetic visual analysis. In <em>      <em>2012 IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>(<em>CVPR &#x2019;12</em>). 2408&#x2013;2415.</li>    <li id="BibPLXBIB0032" label="[32]">Dmitry Pavlov and David&#x00A0;M. Pennock. 2002. A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains. In <em>      <em>International Conference on Neural Information Processing Systems</em>     </em>(<em>NIPS &#x2019;02</em>). 1441&#x2013;1448.</li>    <li id="BibPLXBIB0033" label="[33]">Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In <em>      <em>Conference on Uncertainty in Artificial Intelligence</em>     </em>(<em>UAI &#x2019;09</em>). 452&#x2013;461.</li>    <li id="BibPLXBIB0034" label="[34]">Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise Interaction Tensor Factorization for Personalized Tag Recommendation. In <em>      <em>Proceedings of the Third ACM International Conference on Web Search and Data Mining</em>     </em>(<em>WSDM &#x2019;10</em>). 81&#x2013;90.</li>    <li id="BibPLXBIB0035" label="[35]">Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic Matrix Factorization. In <em>      <em>Proceedings of the 20th International Conference on Neural Information Processing Systems</em>     </em>(<em>NIPS&#x2019;07</em>). 1257&#x2013;1264.</li>    <li id="BibPLXBIB0036" label="[36]">Katharina Schwarz, Patrick Wieschollek, and Hendrik P.&#x00A0;A. Lensch. 2016. Will People Like Your Image?<em>      <em>CoRR</em>     </em>abs/1611.05203(2016). arxiv:1611.05203</li>    <li id="BibPLXBIB0037" label="[37]">Dandan Sha, Daling Wang, Xiangmin Zhou, Shi Feng, Yifei Zhang, and Ge Yu. 2016. An Approach for Clothing Recommendation Based on Multiple Image Attributes. In <em>      <em>Web-Age Information Management: 17th International Conference</em>     </em>(<em>WAIM &#x2019;16</em>). 272&#x2013;285.</li>    <li id="BibPLXBIB0038" label="[38]">Nicholas Sidiropoulos, Lieven&#x00A0;De Lathauwer, Xiao Fu, Kejun Huang, Evangelos Papalexakis, and Christos Faloutsos. 2017. Tensor Decomposition for Signal Processing and Machine Learning. <em>      <em>IEEE Transactions on Signal Processing</em>     </em>65, 13 (July 2017), 3551&#x2013;3582.</li>    <li id="BibPLXBIB0039" label="[39]">Edgar Simoserra, Sanja Fidler, Francesc Morenonoguer, and Raquel Urtasun. 2015. Neuroaesthetics in fashion: Modeling the perception of fashionability. In <em>      <em>2015 IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>(<em>CVPR &#x2019;15</em>). 869&#x2013;877.</li>    <li id="BibPLXBIB0040" label="[40]">Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie, and Tat-Seng Chua. 2018. TEM: Tree-enhanced Embedding Model for Explainable Recommendation. In <em>      <em>Proceedings of the 27th International Conference on World Wide Web</em>     </em>(<em>WWW &#x2019;18</em>).</li>    <li id="BibPLXBIB0041" label="[41]">Xiang Wang, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2017. Item Silk Road: Recommending Items from Information Domains to Social Users. In <em>      <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;17</em>). 185&#x2013;194.</li>    <li id="BibPLXBIB0042" label="[42]">Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, and Thomas&#x00A0;S. Huang. 2016. Brain-Inspired Deep Networks for Image Aesthetics Assessment. <em>      <em>Michigan Law Review</em>     </em>52, 1 (2016), 123&#x2013;128.</li>    <li id="BibPLXBIB0043" label="[43]">Liang Xiong, Xi Chen, Tzu&#x00A0;Kuo Huang, Jeff&#x00A0;G. Schneider, and Jaime&#x00A0;G. Carbonell. 2010. Temporal Collaborative Filtering with Bayesian Probabilistic Tensor Factorization. In <em>      <em>Siam International Conference on Data Mining</em>     </em>(<em>SDM &#x2019;10</em>). 211&#x2013;222.</li>    <li id="BibPLXBIB0044" label="[44]">Yongfeng Zhang, Min Zhang, Yiqun Liu, Shaoping Ma, and Shi Feng. 2013. Localized Matrix Factorization for Recommendation Based on Matrix Block Diagonal Forms. In <em>      <em>Proceedings of the 22Nd International Conference on World Wide Web</em>     </em>(<em>WWW &#x2019;13</em>). 1511&#x2013;1520.</li>    <li id="BibPLXBIB0045" label="[45]">Lili Zhao, Zhongqi Lu, Sinno&#x00A0;Jialin Pan, and Qiang Yang. 2016. Matrix Factorization+ for Movie Recommendation. In <em>      <em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</em>     </em>(<em>IJCAI &#x2019;16</em>). 3945&#x2013;3951.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x002A;</sup></a>Both authors contributed equally to this work.</p>   <p id="fn2"><a href="#foot-fn2"><sup>&#x2020;</sup></a>The corresponding author.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186146">https://doi.org/10.1145/3178876.3186146</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
