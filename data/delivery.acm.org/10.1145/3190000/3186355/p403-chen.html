<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Learning the Chinese Sentence Representation with LSTM Autoencoder</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Learning the Chinese Sentence Representation with LSTM Autoencoder</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Mu-Yen</span>     <span class="surName">Chen</span>     Department of Information Management, National Taichung University of Science and Technology, Taiwan, <a href="mailto:mychen@nutc.edu.tw">mychen@nutc.edu.tw</a>        </div>        <div class="author">     <span class="givenName">Tien-Chi</span>     <span class="surName">Huang</span>     Department of Information Management, National Taichung University of Science and Technology, Taiwan, <a href="mailto:tchuang@nutc.edu.tw">tchuang@nutc.edu.tw</a>        </div>        <div class="author">     <span class="givenName">Yu</span>     <span class="surName">Shu</span>     Department of Industrial Education and Technology, National Changhua University of Education, Taiwan, <a href="mailto:vera.yushu@gmail.com">vera.yushu@gmail.com</a>        </div>        <div class="author">     <span class="givenName">Chia-Chen</span>     <span class="surName">Chen</span>     Department of Management Information Systems, National Chung Hsing University, Taiwan, <a href="mailto:emily@nchu.edu.tw">emily@nchu.edu.tw</a>        </div>        <div class="author">     <span class="givenName">Tsung-Che</span>     <span class="surName">Hsieh</span>     Department of Information Management, National Taichung University of Science and Technology, Taiwan, <a href="mailto:s4851798@gmail.com">s4851798@gmail.com</a>        </div>        <div class="author">     <span class="givenName">Neil Y.</span>     <span class="surName">Yen</span>     School of Computer Science and Engineering, University of Aizu, Japan, <a href="mailto:neilyyen@u-aizu.ac.jp">neilyyen@u-aizu.ac.jp</a>        </div>        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3186355" target="_blank">https://doi.org/10.1145/3184558.3186355</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>This study retains the meanings of the original text using Autoencoder (AE) in this regard. This study uses the different loss (includes three types) to train the neural network model, hopes that after compressing sentence features, it can still decompress the original input sentences and classify the correct targets, such as positive or negative sentiment. In this way, it supposed to get the more relative features (compressing sentence features) in the sentences to classify the targets, rather than using the classification loss that may classify by the meaningless features (words). In the result, this study discovers that adding additional features for correction of errors does not interfere with the learning. Also, not all words are needed to be restored without distortion after applying the AE method.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>KEYWORDS:</small>     </span>     <span class="keyword">      <small>Deep learning</small>, </span>     <span class="keyword">      <small>Autoencoder</small>, </span>     <span class="keyword">      <small>Long Short-Term Memory (LSTM)</small>, </span>     <span class="keyword">      <small>Chinese Sentence Representation</small>, </span>     <span class="keyword">      <small>Sentiment classification</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>Mu-Yen Chen, Tien-Chi Huang, Yu Shu, Chia-Chen Chen, Tsung-Che Hsieh and Neil Y. Yen. 2018. Learning the Chinese Sentence Representation with LSTM Autoencoder. In <em>Proceedings of The 2018 Web Conference Companion (WWW'2018 Companion). ACM, New York, NY, USA, 7 pages.</em> <a href="https://doi.org/10.1145/3184558.3186355" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3186355</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec1">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> INTRODUCTION</h2>    </div>    </header>    <p>Sentiment analysis is mainly applied in sentimental data mining to determine the current status. Therefore, it is mostly used by hotels, restaurants, and online shopping platforms for collection of customer opinions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib1">1</a>]. To start with, one common problem such methods face is how to determine the sentimental implications of users&#x2019; messages and comments. To solve this problem, machine should be trained to understand the meaning of sentences. The most traditional way to do this is by finding key feature words and categorized them. For example the widely known term frequency&#x2013;inverse document frequency (TF-IDF). However, it does not take into account the complex relations between lexical words. In order to solve the implicit relations between the words [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib2">2</a>], researchers attempted using singular value decomposition (SVD), principal component analysis (PCA), and other methods of reduction of dimension for feature extraction. By doing so, rather than simply analyzing the features of keywords, sentences are projected onto other vectors and their implicit features can be shown subsequently. This concept can be interpreted as researchers attempting to use continuous vectors to denote discrete sentences. Yet this method could not really show the implicit features of sentences. It merely targets the composition of lexical words shown in texts. Researches on the extraction of text sentence have continued to be conducted. In recent years, Google made its word2vec an open source tool [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib3">3</a>], giving new meanings to the word vector definition, in that it creates an implicit influential relation between word vectors built upon sentence structures. The idea opens up a new path for research on text analysis and feature extraction.</p>    <p>In recent years, deep learning methods have become popular in extracting complex features from large amounts of data. It is also commonly used in research fields concerning natural language processing (NLP). Text features can be extracted from many aspects, such as keyword features, the influential relations between lexical words, or grammar structures of sentences. Using traditional statistical methods for extraction of the complex relations in texts under large data dimension, however, may require a staggering amount of research fund. In addition, the extracted features should be in line with the hypothetical situation proposed by the user. Deep learning methods, on the other hand, greatly simplify many complex relations, while adopting iterative methods to approximate undiscovered features within and beyond traditional statistical methods. In using deep learning methods to conduct text analysis, convolutional neural network (CNN) and recurrent neural network (RNN) are two commonly used structures. Kim [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib4">4</a>] adopted CNN in his experiments, in which features of lexical words in a sentence were collected in accordance with the filter size of each window for categorization into positive or negative sentiments. Dos Santos &amp; Gatti [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib5">5</a>] used multiple CNN structures to perform efficiency experiment on short-text categorization. Li &amp; Qian [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib6">6</a>] used Long Short-Term Memory (LSTM) to conduct research on multiple categorizations of text sentiments It is worth noting that using deep learning methods for extraction of features may result in overfitting. Intuitively, it is not reasonable for a categorizer to correctly determine positive or negative sentiments based on meaningless lexical words. Therefore, this study retains the meanings of the original text using Autoencoder (AE) in this regard. It serves as an unsupervised neural model frequently used for compression. The study hopes that after compressing sentence features, it can still decompress and restore the original input sentences. This way, such compression feature could show the implicit meanings of a sentence, rather than a feature composition of meaningless lexical words.</p>    <p>The study uses LSTM for extraction of implicit meanings of sentences. With a view to ensuring that the extracted features can provide better interpretations, the study incorporated the concepts of Autoencoder. The hypothesis of the study is that, when all categorization models achieve a certain level of accuracy, the difference between implicit meanings extracted by LSTM with AE and implicit meanings directly extracted by LSTM lie in the fact that the former has deeper and more intuitive meanings, and is thereby closer to the intended meanings.</p>   </section>   <section id="sec2">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> LITERATURE REVIEW</h2>    </div>    </header>    <section id="sec2Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Sentiment Analysis</h3>     </div>    </header>    <p>Sentiment analysis is used to understand the sentiment of a text by extracting features from said text. Currently, there are two methods. One is by consulting sentiment dictionary to search from texts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib7">7</a>]. Searched lexical words from sentences or articles are weighted or scored. Determination of the type of sentiment is made according to the weighting or scores. Another is by conducting machine learning to perform feature learning and categorization. The study uses the later. Ortigosa et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib8">8</a>] mined the English lexical database of Facebook to map out a person&#x0027;s mood swing. The whole process includes data collection, data cleaning, and establishment of lexical database (to acquire feature lexical words). Lastly, Support Vector Machine (SVM), C4.5, and Naive Bayes were used for categorization of sentences. The input pattern, based on traditional vector representation methods, is shown by a comparison between sentences and feature lexical words shown and not shown in the lexical database. Kim [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib4">4</a>] adopted CNN to train semantic models. Due to the fact that CNN would automatically collect features using slide windows, it mainly functions as a collector of neighboring lexical words at intervals for input. Li et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib9">9</a>] conducted semantic mining on Weibo, mainly collecting texts from disaster-related articles. First, word vectors of texts were pre-trained by Google&#x0027;s word2vec. Later, CNN was used for categorization and setting up models. The main purpose of his article is to mine features of lexical words. Methods similar to word vector compression were adopted to categorize sentences within set-up word vectors. Sentences were categorized into positive or negative sentiment. Filtering properties of CNN were used to observe the selected lexical words to pinpoint important feature lexical words in the &#x2018;positive&#x2019; and &#x2018;negative&#x2019; categories.</p>    </section>    <section id="sec2Z2">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Deep Learning for Semantic Extraction</h3>     </div>    </header>    <p>     <em>2.2.1&#x2003; Semantic Vector.</em> Semantic vector should first be viewed through small word vectors, which can be viewed as a denotation of the conversion from discrete lexical words to vectors with meanings. Among all traditional conversion methods, the one-hot representation of BOW (Bag of word) is the most frequently used. However, the meanings denoted by this method could only recognize different lexical words, while similar words may require the consultation of synonym dictionaries, making actual processing more complicated. Hence, researches that followed tend to use matrix factorization to extract the relation between lexical words and data. Word vectors should contain more meanings. Therefore, researches on semantics in recent years have begun to focus more in this regard. The word vector proposed by Mikolov et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib10">10</a>] refers to the use of slide windows to collect words and their neighboring words (can be viewed as target words and feature words next to target words) for conducting training for shallow neural networks. After the training, the hidden layer would represent the word vector of the input words (similar to the concept of compression). The concept of word vector and its granularity may increase, meaning that it may go beyond the relations between words and words. The concept may also be applied in sentences and articles for examining the relations between them.</p>    <p>     <em>2.2.2&#x2003; Deep Learning for Text Analysis.</em> Cheng et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib11">11</a>] trained lexical data collected from Weibo and adopted RNN for categorization of text features based on input between each interval. Zhou et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib12">12</a>] conducted research on QA(Question Answer) systems, with an eye to making clear the implicit relations between questions and answers. Their data were provided by YAHOO API. This study hopes to approximate questions and their best answers after projecting them onto a certain vector, therefore it uses Autoencoder to compress semantic meanings. Finally, SVMRank is used for sequencing. The study hopes that by doing so, best answers can be recommended first. Chen et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib13">13</a>] also conducted research on QA system. It used data collected from WIKI for training and the RNN structure as the model for extraction of implicit features between questions and answers. Araque et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib14">14</a>] used six open source data sets, all of which sourced from, Twitter and movie reviews. The data sets were all labeled either &#x2018;positive&#x2019; or &#x2018;negative&#x2019;. The data was pre-processed; punctuation marks, URLs, numbers, user names and other irrelevant words were deleted before the data was input. Convolutional vectors were used to extract in-depth features for sentimental polarity prediction. During the prediction, single-logic features in new data could be extracted. Each surface feature would be put into one category, which served as its complete feature. Then, features of the target fields were defined for new input data to determine target fields based upon their trainings for feature recognition. In the experimental result, the accuracy of six data sets was evaluated based on different indicators. Gui et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib15">15</a>] adopted linear equations to match the statistical features of product reviews and words of previous posts by product users. Then, they reconstructed a new feature vector, which is categorized according to CNN. The results showed that this conversion is valid. Park et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib16">16</a>] collected 611,590 news articles from 2010 to 2014. During a baseball game, every movement of player is recorded for statistical purposes. Their data was collected from the official score sheet provided by Korea Baseball Organization (KBO). The study proposed a player evaluation model based on deep learning. It is combined with quantitative statistical analysis for sports and qualitative analysis for news articles. Sports statistical data was used to categorize and label players&#x2019; performance as &#x2018;positive&#x2019; or &#x2018;negative&#x2019;. The labels were used as target fields of articles related to rated players. Labeled articles were used in Deep Neural Network (DNN) categorizer for training of sentence-by-sentence recognition. Based on the results of QA system, their study proved that it is reasonable to use polarity scores in predicting the result of a baseball game. Ronnqvist &amp; Sarlin [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib17">17</a>] proposed the use of deep learning methods in examining relevant discussions of texts. They also extract natural language descriptive features by using DNN. The model is based on the unsupervised learning mechanism denoted by semantic vectors of extensive text data and validated the applicability of news articles in financial risk research. Furthermore, they gave examples to explain how texts and constantly-updating, and extensively applicable descriptive data may serve as a helpful supplementary sources for financial and system analysis. Kraus &amp; Feuerriegel [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib18">18</a>] used RNN to extract features from time sequence and found highly nonlinear relations. They used different lexical databases to predict the stock trend, and proved that the accuracy compared with traditional machine learning methods is higher. Therefore, it is helpful to reveal the commercial value of deep learning.</p>    </section>   </section>   <section id="sec3">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> LSTM WITH AUTOENCODER CLASSIFIER</h2>    </div>    </header>    <p>This chapter explains the structure of this study. First, the study extracts features of sentences before restoring them based on said features, to ensure that said features hold deeper meanings, rather than simply modifying the weighting. The errors taken into account in this study include: errors arising from word restoration and errors arising from categorization, as shown in the following formula: <span class="inline-equation"><span class="tex">${\rm{Los}}{{\rm{s}}_{total}} = Los{s_{AE}} + Los{s_{class}}$ </span>    </span>. <span class="inline-equation"><span class="tex">$Los{s_{AE}}$ </span>    </span> is the loss of Autoencoder.<span class="inline-equation"><span class="tex">${\rm{\;}}Los{s_{class}}$ </span>    </span> is the loss of the targets, including positive and negative sentiment. In theory, if the error of each word so serious as to affect the efficiency of categorization, no deeper meanings can be extracted from words in sentences being compressed, or the extracted semantic meanings are completely irrelevant to targets set for categorization. This means that the hypothesis of this study is not valid in regards to machine learning. In contrast, if the errors, after being aggregated, do not have too much effect on categorization, the extracted semantic meanings hold a certain level of interpretation for abstract ideas.</p>    <figure id="fig1">    <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186355/images/image1.png" class="img-responsive" alt="Figure 1:" longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">LSTM with Autoencoder classifier. Left: Feature extraction layer based on Autoencoder, performed through LSTM. Right: Target for categorization (positive or negative sentiment); output layers all linked to softmax layer.</span>    </div>    </figure>    <p>First, this study uses LSTM for extraction of sentence concepts and features. The reason is that using traditional RNN often leads to a serious problem of vanishing gradient. Chung et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib19">19</a>] also proposed the same, thereby assuming the importance of words sequence in every sentence. And owing to the fact that humans tend to understand the meaning of a sentence from a single perspective, sentences are input from the beginning of the sentence into LSTM. The image below shows the step-by-step process of LSTM modification of weighting. Concepts of Autoencoder (AE) are incorporated; therefore its final purpose is to input itself in hopes of restoring itself after being compressed. The compressed semantic vectors are stored in 256 neurons. The number of target node point being output at last shall equal to words in the dictionary. The formula for calculating errors of LSTM and AE is Cross Entropy, shown as <span class="inline-equation"><span class="tex">$Los{s_{AE}} = \mathop \sum \limits_{w = 0}^{| S |} ( { - \mathop \sum \limits_{i = 1}^{| {BOW} |} ( {real_i^w\log ( {predict_i^w} )} )} )$ </span>    </span>&#x0F0C; <span class="inline-equation"><span class="tex">$| S |$ </span>    </span> is the length of a sentence; w is the index for the current word in a sentence; i is the BOW index of words; 0 is the reserved value, therefore the calculation starts from 1; |BOW| is the total amount of words in the dictionary; real is the actual target value; predict is the predicted value. In all, the formula aggregates the errors of all words in one sentence.</p>    <figure id="fig2">    <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186355/images/image2.png" class="img-responsive" alt="Figure 2:" longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 2:</span>     <span class="figure-title">LSTM with Autoencoder. One sentence input at a time for compression in LSTM. Sentences are later decompressed error feedbacks are given. During the prediction, the targets are the most likely candidates among all words in the dictionary.</span>    </div>    </figure>    <p>Based on the above method, reconstructed features can be extracted from sentences. Vectors of the size of 256 dimensions are used. This study uses this feature to perform categorization of positive and negative sentiments, proposes the following formula for calculating errors: <span class="inline-equation"><span class="tex">$Los{s_{class}} = - \mathop \sum \limits_{i = 0}^{| {class} |} ( {rea{l_i}\log ( {predic{t_i}} )} )$ </span>    </span>. |class| is the total number of target categories, which means positive and negative sentiments.</p>    <figure id="fig3">    <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186355/images/image3.png" class="img-responsive" alt="Figure 3:" longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 3:</span>     <span class="figure-title">Simple Neural Model. Input: implicit meanings compressed by LSTM.</span>    </div>    </figure>   </section>   <section id="sec4">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> EXPERIMENT DESIGN &amp; PROCESS</h2>    </div>    </header>    <section id="sec4Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Experiment environment</h3>     </div>    </header>    <p>This chapter introduces the experiment environment of this study, which include the hardware part and the software part. Regarding hardware, the operating system is Ubuntu (64-bit), which comes with 4 CPUs and a 12G RAM. Regarding software, the study uses Python 3.6 for development. Software used for development kit includes Jieba and Pytorch. Jieba kit is an open source software from github for segmentation of Chinese characters. It is also compatible with a wide array of programming languages such as PHP, Python and R. The study uses Jieba 0.39 for segmentation and performs random manual inspection to check whether the segmented sentences make sense. Pytorch is a deep learning framework that evolved from the deep learning software developed by Torch7 for Lua. Later it was transferred to Python, hence named Pytorch. In recent years, tech giants such as Facebook, twitter and others have relied on Pytorch for software development, therefore it has become widely known. The study uses Pytorch because of its user-friendly framework design.</p>    </section>    <section id="sec4Z2">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Experiment Process</h3>     </div>    </header>    <p>This section briefly explains the process of the experiment, which includes four stages: data collection, data pre-processing, categorization and modeling experiment, and model evaluation. Details of each stage are given in the followings.</p>    <p>     <em>4.2.1&#x2003;Data Collection.</em> The Chinese lexical data is collected from datatang. After deleting repeated ones, there are 16680 sets of data, among which positive sentiments for 8680 and negative sentiments for 8000. It can be inferred that data amount of both categories are on the same level. In addition, because they have yet been processed, every sentence is a complete comment and can be put into either one of the category. It is worth noting, however, that every sentence varies in length, and the sentimental lexical words in a comment may conflict with the category of the comment. The reason is that the labeling of each comment is based on the narrative of the whole sentence. Therefore, while the comment goes on, it may mix positive words with negative ones. Use traditional methods and view the weighting of each word as equal may result in great errors in constructing the modeling learning. Thankfully, models like LSTM would adjust the weighting of each word sequence, making learning easier.</p>    <p>     <em>4.2.2&#x2003;Data Pre-Processing.</em> This study uses dictionaries provided by Jieba for segmentation. Punctuation marks are all deleted. The study also makes the length of all sentences uniform, in that although sentences of different lengths can be input into LSTM for training, sentences that are too lengthy get higher weighting. Therefore, to solve the problem, the study makes all sentence lengths equal. If a sentence exceeds the maximum length, the maximum length would be extracted; insufficient parts are made up by padding. Furthermore, because this method may seem unfair in certain circumstances, the study adopts the trial-and-error method to figure out the threshold for said batch of data.</p>    <p>     <em>4.2.3&#x2003;Categorization and Modeling Experiment.</em> Modeling is performed in this stage for input of processed sentences. Here, sentences can be categorized into positive sentiment or negative sentiment based on the model. This stage is mainly divided into two parts: feature extraction and sentiment categorization. Feature extraction is performed by one-way feature matching. Autoencoder is also used for addition of extracted features modified by compressed and restored errors. The positive and negative sentiment categorization is performed after determining the semantic features. The study compares proposed models and LSTM without Autoencoder. In addition, regarding the definition of loss function, the study proposes 2 hypotheses. One is that the weighting of all words equals that of the final sentiment targets; therefore, errors in <span class="inline-equation"><span class="tex">$Los{s_{AE}}$ </span>     </span> are directly aggregated. Another is that the weighting of all sentences equals that of the final sentiment targets, while the weighting of each word in every sentence are the same. The study slightly modifies the original formula into: <span class="inline-equation"><span class="tex">$\frac{1}{{| S |}}\mathop \sum \limits_{w = 0}^{| S |} ( { - \mathop \sum \limits_{i = 1}^{| {BOW} |} ( {real_i^w\log ( {predict_i^w} )} )} )$ </span>     </span>. The purpose is to simply examine whether the modifications made according to the errors arising from newly added words would positively or negatively affect the targets for categorization.</p>    <p>     <em>4.2.4&#x2003;Model Evaluation.</em> Owing to the fact that the balanced data set used merely shows whether the returned errors would positively or negatively affect the categorization into positive sentiment or negative sentiment, the study adopts overall accuracy for examination. Loss values come from each batch of data, but for clearer presentation they are shown in the form of average error value of each example.</p>    </section>   </section>   <section id="sec5">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> EXPERIMENT RESULTS</h2>    </div>    </header>    <section id="sec5Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Data Description</h3>     </div>    </header>    <p>The original data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib20">20</a>] distribution of the 20000-plus sets of open lexical data used in this study is shown below. After examining the data, the study deleted the repeated and invalid sentences, leaving only 16680 sets of data, among which those categorized as positive sentiment for 8680 and negative sentiment for 8000. The average length of sentence is 54.79 words. Data sequence is also randomized. Data for training purposes is total of 14440. The remaining becomes data for testing is 2240. The ratio of training and testing is approximately 9:1.</p>    </section>    <section id="sec5Z2">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Experiment Results and Analysis</h3>     </div>    </header>    <p>In the study, the epoch is set at 10. To shorten the time required for the experiment, batch sizes are uniformly set at 32, to which data at the beginning are distributed. It is worth noting that if the batch size is too large, a few features may be distributed to the same batch and the training efficiency will be reduced; too small, calculation time may increase, and the algorithm may become more sensitive to few features. These types of neural model include LSTM. The input dimension of the parameter of LSTM in this study is set at 50 to correspond with the dimension of word embeddings. Only one hidden layer is on the inner part. The number of node is 256. Dropout size is set at 0.3. The adopted optimizer is Adam, primarily because traditional gradient descent methods may result in local optimum; therefore the more advanced algorithm Adam is used. Regarding loss function, the study selected the Cross Entropy Loss method mainly because it provides better calculation for errors of discrete categories. The effect of batch learning is better when the learning rate is set at 0.001. The learning rate cannot be converged when it is too high.</p>    <p>The table below shows the experiment results. It is worth noting that the study uses random seeds to anchor random functions; therefore, each epoch is provided with the same training data to perform back-propagation to modify neural work network models. The same data is used for model testing. Using these data equally would not result in back-propagation or modification of parameters. The results are shown in Table <a class="tbl" href="#tb1">1</a>. Among them, average loss is the aggregation of all batch loss values divided by the number of samples. It can be inferred from the table that after the 10<sup>th</sup> epoch, the overall accuracy of each experiment method converges above 85%, with the average error rate decreasing. Epoch seems to have yet achieved its optimal convergence. But apparently, although the correction performed for errors arising from word compression affect the speed of convergence, it is conducive to the categorization. After the 10<sup>th</sup> epoch, the error rate of LSTM with AE decreases significantly. Even if the weighting of each word equals that of both &#x2018;positive&#x2019; and &#x2018;negative&#x2019; categories, errors can be largely decreased without interfering the learning. After being weighted, LSTM with AE seems to be no different than LSTM. This is probably because of the insufficient reduction of weighting of each word, and the word weighting of the whole sentence equaling that of the target weighting of the sentiment categories, causing the neural model to benefit more in the categorization of sentiments. Therefore, it is also important to set up proper weightings for each word, which may be able to strike a balance between training efficiency and concept interpretation.</p>    <div class="table-responsive" id="tb1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Each Epoch of Categorization Models.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Epochs</th>       <th style="text-align:center;" colspan="2">LSTM with AE<hr/>       </th>       <th style="text-align:center;" colspan="2">LSTM with AE(weight loss)<hr/>       </th>       <th style="text-align:center;" colspan="2">LSTM<hr/>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">-</td>       <td style="text-align:center;">acc</td>       <td style="text-align:center;">avg loss</td>       <td style="text-align:center;">acc</td>       <td style="text-align:center;">avg loss</td>       <td style="text-align:center;">acc</td>       <td style="text-align:center;">avg loss</td>       </tr>       <tr>       <td style="text-align:center;">1</td>       <td style="text-align:center;">51.79</td>       <td style="text-align:center;">13.23</td>       <td style="text-align:center;">64.78</td>       <td style="text-align:center;">2.19</td>       <td style="text-align:center;">53.44</td>       <td style="text-align:center;">2.13</td>       </tr>       <tr>       <td style="text-align:center;">2</td>       <td style="text-align:center;">51.79</td>       <td style="text-align:center;">11.64</td>       <td style="text-align:center;">70.09</td>       <td style="text-align:center;">2.00</td>       <td style="text-align:center;">71.74</td>       <td style="text-align:center;">2.03</td>       </tr>       <tr>       <td style="text-align:center;">3</td>       <td style="text-align:center;">51.79</td>       <td style="text-align:center;">11.04</td>       <td style="text-align:center;">78.30</td>       <td style="text-align:center;">1.53</td>       <td style="text-align:center;">84.46</td>       <td style="text-align:center;">1.53</td>       </tr>       <tr>       <td style="text-align:center;">4</td>       <td style="text-align:center;">52.99</td>       <td style="text-align:center;">9.81</td>       <td style="text-align:center;">84.46</td>       <td style="text-align:center;">1.15</td>       <td style="text-align:center;">84.91</td>       <td style="text-align:center;">1.10</td>       </tr>       <tr>       <td style="text-align:center;">5</td>       <td style="text-align:center;">67.14</td>       <td style="text-align:center;">7.15</td>       <td style="text-align:center;">83.62</td>       <td style="text-align:center;">0.96</td>       <td style="text-align:center;">85.27</td>       <td style="text-align:center;">0.89</td>       </tr>       <tr>       <td style="text-align:center;">6</td>       <td style="text-align:center;">81.79</td>       <td style="text-align:center;">4.91</td>       <td style="text-align:center;">84.42</td>       <td style="text-align:center;">0.81</td>       <td style="text-align:center;">85.31</td>       <td style="text-align:center;">0.74</td>       </tr>       <tr>       <td style="text-align:center;">7</td>       <td style="text-align:center;">80.63</td>       <td style="text-align:center;">3.46</td>       <td style="text-align:center;">86.43</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">83.93</td>       <td style="text-align:center;">0.64</td>       </tr>       <tr>       <td style="text-align:center;">8</td>       <td style="text-align:center;">81.78</td>       <td style="text-align:center;">2.67</td>       <td style="text-align:center;">85.71</td>       <td style="text-align:center;">0.59</td>       <td style="text-align:center;">86.29</td>       <td style="text-align:center;">0.55</td>       </tr>       <tr>       <td style="text-align:center;">9</td>       <td style="text-align:center;">86.16</td>       <td style="text-align:center;">2.19</td>       <td style="text-align:center;">87.54</td>       <td style="text-align:center;">0.52</td>       <td style="text-align:center;">87.14</td>       <td style="text-align:center;">0.48</td>       </tr>       <tr>       <td style="text-align:center;">10</td>       <td style="text-align:center;">86.96</td>       <td style="text-align:center;">1.82</td>       <td style="text-align:center;">87.68</td>       <td style="text-align:center;">0.46</td>       <td style="text-align:center;">87.28</td>       <td style="text-align:center;">0.41</td>       </tr>       <tr>       <td colspan="7" style="text-align:right;">Unit: percentage (%)<hr/>       </td>       </tr>      </tbody>     </table>    </div>    <p>This study used the above models that underwent 10 times of epoch to examine their output in testing. The following is one of the batches that includes 32 samples. The accuracy of this batch is 75%. Among them, one sample A was mistakenly predicted as negative when in fact it should be positive. Another sample B was correctly predicted as positive, as it should be. After being compressed by AE, sample A was distorted. This is normal. Because the test data wasn&#x0027;t used for training. Therefore, a combination following a small paragraph (or words) should appear frequently in data training. From sample A, it is known that distorted words do not affect human categorization of sentiments. But for machine, they slightly do. However, judging from the fact that a sentence contains 100 words, they do not have significant influence over machine categorization, and is not likely to be labeled as both &#x2018;positive&#x2019; and &#x2018;negative&#x2019;. Hence, the study infers that this is because the AE has not picked up the weighting of stressed tones. Although sample B was correctly predicted, its distortion rate is higher than that of sample A. The study reckons that each word has different influence over the weighting of targets. Therefore, decompression without distortion isn&#x0027;t required. As long as important word features and sentence structures (e.g. stressed tone) can be compressed, positive and negative sentimental categorization can be performed based on abstract semantic concepts.</p>    </section>   </section>   <section id="sec6">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> CONCLUSIONS</h2>    </div>    </header>    <p>After conducting the experiment, the study discovers that adding additional features for correction of errors does not interfere with the learning. Also, not all words are needed to be restored without distortion after applying the AE method. What matters most is that important combinations of word features (meanings) can be learned. The study merely used 10 epochs to perform experiments, but the results are worth delving deeper into, especially the parts of interpretative capabilities. In the future development, to prove that the semantic compression of autoencoder is related to the training of generalized features, the study may further adopt attention-based models for visualized interpretations. In addition, to speed up the improvement in accuracy of these types of models, the study will consider using external pre-trained word vectors for initialization.</p>   </section>  </section>  <section class="back-matter">   <section id="ack-001">    <header>    <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>    </div>    </header>    <p>The authors wish to thank the Ministry of Science and Technology of the Republic of China for financially supporting this research under Contract Grants No. MOST106-2634-F-025-001, MOST 106-2511-S-025-003-MY3, MOST105-2410-H-025-015-MY2, MOST105-2511-S-005-001-MY3, and MOST104-2511-S-005-003.</p>   </section>   <section id="bib-sec-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="bib1" label="[1]">Q. Gan, B. H. Ferns, Y. Yu, &amp; L. Jin. 2017. A text mining and multidimensional sentiment analysis of online restaurant reviews. <em>Journal of Quality Assurance in Hospitality &amp; Tourism</em>, 18, 465-492.</li>    <li id="bib2" label="[2]">S. M. Patel, V. K. Dabhi, &amp; H. B. Prajapati. 2017. Extractive Based Automatic Text Summarization. <em>JCP</em>, 12, 550-563.</li>    <li id="bib3" label="[3]">Q. Le, &amp; T. Mikolov. 2014. Distributed representations of sentences and documents. <em>In Proceedings of the 31st International Conference on Machine Learning</em>, 1188-1196.</li>    <li id="bib4" label="[4]">Y. Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</li>    <li id="bib5" label="[5]">C. Dos Santos, &amp; M. Gatti. 2014. Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. <em>In COLING</em>, 69-78.</li>    <li id="bib6" label="[6]">D. Li, &amp; J. Qian. 2016. Text sentiment analysis based on long short-term memory. <em>In Computer Communication and the Internet (ICCCI)</em>, 2016 IEEE International Conference on, 471-475.</li>    <li id="bib7" label="[7]">S. Zhang, Z. Wei, Y. Wang, &amp; T. Liao. 2018. Sentiment analysis of Chinese micro-blog text based on extended sentiment dictionary. <em>Future Generation Computer Systems</em>, 81, 395-403.</li>    <li id="bib8" label="[8]">A. Ortigosa, J. M. Mart&#x00ED;n, &amp; R. M. Carro. 2014. Sentiment analysis in Facebook and its application to e-learning. <em>Computers in Human Behavior</em>, 31, 527-541.</li>    <li id="bib9" label="[9]">Q. Li, Z. Jin, C. Wang, &amp; D. D. Zeng. 2016. Mining opinion summarizations using convolutional neural networks in Chinese microblogging systems. <em>Knowledge-Based Systems</em>, 107, 289-300.</li>    <li id="bib10" label="[10]">T. Mikolov, K. Chen, G. Corrado, &amp; J. Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</li>    <li id="bib11" label="[11]">J. Cheng, P. Li, Z. Ding, S. Zhang, &amp; H. Wang. 2016. Sentiment Classification of Chinese Microblogging Texts with Global RNN. <em>In Data Science in Cyberspace (DSC), IEEE International Conference on</em>, 653-657.</li>    <li id="bib12" label="[12]">G. Zhou, Y. Zhou, T. He, &amp; W. Wu. 2016. Learning semantic representation with neural networks for community question answering retrieval. <em>Knowledge-Based Systems</em>, 93, 75-83.</li>    <li id="bib13" label="[13]">D. Chen, A. Fisch, J. Weston, &amp; A. Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. arXiv preprint arXiv:1704.00051.</li>    <li id="bib14" label="[14]">O. Araque, I. Corcuera-Platas, J. F. S&#x00E1;nchez-Rada, &amp; C. A. Iglesias. 2017. Enhancing deep learning sentiment analysis with ensemble techniques in social applications. <em>Expert Systems with Applications</em>, 77, 236-246.</li>    <li id="bib15" label="[15]">L. Gui, Y. Zhou, R. Xu, Y. He, &amp; Q. Lu. 2017. Learning representations from heterogeneous network for sentiment classification of product reviews. <em>Knowledge-Based Systems</em>, 124, 34-45.</li>    <li id="bib16" label="[16]">Y. J. Park, H. S. Kim, H. Lee, D. Kim, S. B. Kim, &amp; P. Kang. 2017. A deep learning-based sports player evaluation model based on game statistics and news articles. <em>Knowledge-Based Systems</em>, 138, 15-26.</li>    <li id="bib17" label="[17]">S. R&#x00F6;nnqvist, &amp; P. Sarlin. 2017. Bank distress in the news: Describing events through deep learning. <em>Neurocomputing</em>, 264, 57-70</li>    <li id="bib18" label="[18]">M. Kraus, &amp; S. Feuerriegel. 2017. Decision support from financial disclosures with deep neural networks and transfer learning. <em>Decision Support Systems</em>, 104, 38-48.</li>    <li id="bib19" label="[19]">J. Chung, C. Gulcehre, K. Cho, &amp; Y. Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</li>    <li id="bib20" label="[20]">DATATANG. <a class="link-inline force-break" href="http://www.datatang.com/index.html">http://www.datatang.com/index.html</a> (2017)</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186355">https://doi.org/10.1145/3184558.3186355</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
