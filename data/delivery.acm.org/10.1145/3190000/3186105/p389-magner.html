<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>TIMES: Temporal Information Maximally Extracted from Structures</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">TIMES: Temporal Information Maximally Extracted from Structures</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Abram N.</span>     <span class="surName">Magner</span>,     UIUC, Urbana, IL, USA<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>, <a href="mailto:anmagner@illinois.edu">anmagner@illinois.edu</a>    </div>    <div class="author">     <span class="givenName">Jithin K.</span>     <span class="surName">Sreedharan</span>,     Purdue University, <a href="mailto:jithinks@purdue.edu">jithinks@purdue.edu</a>    </div>    <div class="author">     <span class="givenName">Ananth Y.</span>     <span class="surName">Grama</span>,     Purdue University, <a href="mailto:ayg@cs.purdue.edu">ayg@cs.purdue.edu</a>    </div>    <div class="author">     <span class="givenName">Wojciech</span>     <span class="surName">Szpankowski</span>,     Purdue University, <a href="mailto:spa@cs.purdue.edu">spa@cs.purdue.edu</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186105" target="_blank">https://doi.org/10.1145/3178876.3186105</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Inferring the node arrival sequence from a snapshot of a dynamic network is an important problem, with applications ranging from identifying sources of contagion to flow of capital in financial transaction networks. Variants of this problem have received significant recent research attention, including results on infeasibility of solution for prior formulations. We present a new formulation of the problem that admits probabilistic solutions for broad classes of dynamic network models. Instantiating our framework for a preferential attachment model, we present effectively computable and practically tight bounds on the tradeoff curve between optimal achievable precision and <em>density</em>/recall. We also present efficient algorithms for partial recovery of node arrival orders and derive theoretical and empirical performance bounds on the precision and density/recall of our methods in comparison to the best possible. We validate our methods through experiments on both synthetic and real networks to show that their performance is robust to model changes, and that they yield excellent results in practice. We also demonstrate their utility in the context of a novel application in analysis of the human brain connectome to draw new insights into the functional and structural organization and evolution of the human brain.</small>    </p>    <p>     <small>      <strong>Keywords.</strong> Preferential attachment graphs, partial order, linear extension, integer programming</small>    </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Abram N. Magner, Jithin K. Sreedharan, Ananth Y. Grama, and Wojciech Szpankowski. 2018. TIMES: Temporal Information Maximally Extracted from Structures. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018 (WWW 2018),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186105" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186105</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>The problem of inferring the arrival order of nodes in a dynamic network, from its observed current state, is of considerable significance. In a network of financial transactions, the arrival order tracks the flow of capital. In mapping spread of infectious diseases, arrival order allows one to identify early patients, yielding clues to genetic origin, evolution, and mechanisms of transmission. In social networks, one can map the spread of information, and use it to design personalized channels by prioritizing information of shared interest. In networks of biochemical interactions (e.g., protein interaction networks), one can identify early biomolecules (e.g., proteins in prokaryotic species), which are known to be preferentially implicated in cancers and other diseases&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>].</p>    <p>Owing to this significance, and the availability of large datasets in diverse application domains, there has been considerable recent research interest in variants of this problem. Despite the apparent simplicity of the informal problem statement, the node arrival inference problem is highly complex, both from analysis and methodological points of view. This complexity stems, in part, from the symmetries inherent in graphs and the models that generate them, which admit multiple (partial) node orderings with equal likelihood. Characterizing these symmetries and suitably reformulating the problem to establish limits on inference is at the core of our analysis framework. Efficient methods that achieve the prescribed limits of inference characterize the algorithmic challenges associated with the problem.</p>    <p>Dynamic graph models underlying a large class of applications primarily consider node arrivals, along with corresponding edge insertions (without corresponding node departures)<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>. Starting from a snapshot of a dynamic network, the problem of inferring the arrival order inverts the arrival process. Consequently, a specific inference technique is intimately coupled with a corresponding model of the arrival process. In this paper, we assume that networks evolve according to some stochastic model, which provides the basis for node order inference, with associated probabilistic guarantees. We present a general framework for inferring node orders, and a specific instantiation of our framework in the context of a preferential attachment model. We note that our framework is general, and capable of admitting a large class of dynamic graph models. Our specific instantiation of preferential attachment graphs is directly applicable to a large number of applications, since the model has been extensively studied in literature. We will primarily consider the following form of the preferential attachment model:</p>    <p>    <strong>Barab&#x00E1;si-Albert model.</strong>Let <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n,m)$</span>    </span> denote the following version of the Barab&#x00E1;si-Albert model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]: the parameter <em>n</em> represents the number of nodes in the network and <em>m</em> is the number of connections a new node makes to existing nodes when it is added to the network. To form graph <em>G<sub>t</sub>    </em> at time <em>t</em>, for <em>t</em> = 1, a single vertex (called 1) is created with <em>m</em> self loops. At time <em>t</em> > 1, vertex <em>t</em> joins the network and makes <em>m</em> connections to existing nodes in graph <em>G</em>    <sub>     <em>t</em> &#x2212; 1</sub> with probability proportional to their current degrees as follows: each connection choice is conditionally independent of the others and satisfies <span class="inline-equation"><span class="tex">${\mathrm{Pr}}[t \text{ connects to } k | G_{t-1}] = \frac{\text{deg}_{t-1}(k)}{2m(t-1)},$</span>    </span> where deg<sub>     <em>t</em> &#x2212; 1</sub>(<em>k</em>) is the degree of node <em>k</em> at time <em>t</em> &#x2212; 1. This process continues until <em>t</em> equals <em>n</em><a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>. This paper primarily focuses on <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n,m)$</span>    </span>, unless otherwise stated explicitly. We remark that the existence of self-loops on the initial vertex allows for clean proofs; our theoretical and empirical results extend without significant changes to models in which these self-loops are not present.</p>    <p>In our previous work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], we present a formulation of the problem of recovering the temporal order of nodes in preferential attachment graphs. Focusing on deriving fundamental limits on inference of temporal order, we show that any estimator that outputs a <em>total</em> ordering of vertices, with high probability, makes a large number of mistakes (to be made precise later) &#x2013; owing to inherent symmetries in the network model. This negative result provides the motivation for this work.</p>    <p>    <strong>Our Contributions.</strong>In this paper, we present the following key results:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">We present a relaxed formulation of the problem that aims to infer a <em>partial order</em> on nodes (recall that a partial order satisfies antisymmetry and transitivity), which is meant to approximate the true arrival order. The intuition is that we allow an estimator to make fewer vertex pair order inferences, in exchange for higher precision.<br/>We present detailed analytical results on the limits achievable in this new formulation in terms of the tradeoff between expected precision and partial order density. In particular, we derive tight, efficiently computable linear programming bounds on the optimal precision achievable when our estimator is required to make a certain minimum number of vertex pair order inferences.<br/></li>    <li id="list2" label="&#x2022;">We show that maximum likelihood estimation of a total order of arrivals leads to an exponentially large set of equiprobable solutions, making it an unfavorable notion of optimality.<br/></li>    <li id="list3" label="&#x2022;">Our computable bounds on the optimal precision curve do not yield efficient partial order estimators. For this reason, we introduce and analyze, both theoretically and empirically, efficient estimators: the first is optimal in the sense that it yields an estimator with <em>perfect</em> precision (see Theorem&#x00A0;<a class="enc" href="#enc8">3.5</a>): that is, given a relabeled preferential attachment graph, it infers all vertex order relations that hold with probability 1. We rigorously analyze the number of such relations and find them to be asymptotically small (Theorem&#x00A0;<a class="enc" href="#enc11">5.1</a>). This motivates our investigation of other algorithms (the <SmallCap>Peeling</SmallCap> and <SmallCap>Peeling+</SmallCap> algorithms), which sacrifice some precision in order to achieve higher density.<br/></li>    <li id="list4" label="&#x2022;">We present experimental evaluation on both real and synthetic datasets. These experiments demonstrate the robustness of our methods to variations in the preferential attachment model (see Section&#x00A0;<a class="sec" href="#sec-13">4</a>) and provide guidance for future theoretical exploration. We also present a novel application of our method to the analysis of the human brain connectome to identify regions of &#x201C;early&#x201D; and &#x201C;late&#x201D; development. This analysis could separate regions with core functionality from those involved in functional specializations, giving information about evolution of various structural and functional elements.<br/></li>    </ul>    <p>    <strong>Prior Related Work.</strong>Several prior results focus on variants of the problem of finding the oldest node in a graph [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]. Bubeck et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] consider uniform attachment and preferential attachment trees, and find a set that contains the root node with a prescribed error probability. They prove that the set size is independent of the number of nodes. However, their technique cannot be readily extended to the case <em>m</em> > 1. Our proposed algorithms target a more general problem, and can be used to find such a set for the oldest node. For preferential attachment graphs, Frieze et al.[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] study the problem of identifying the oldest node. However, their setting and goal are rather different: they assume that the arrival order of nodes is known, and their goal is to arrive at the oldest node by a process that starts at a different node and only uses local information to advance.</p>    <p>A related problem of detecting information sources in epidemic networks has been studied by Shah et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] for the Susceptible-Infected model and Zhu et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] for the Susceptible-Infected-Recovered model. The more general problem of inferring node arrival orders has been shown to be particularly challenging by us in a recent work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], in which we derive fundamental bounds for exact and approximate recovery of vertex ordering in preferential attachment graphs. Recognizing this complexity, in this paper we focus on analysis and algorithmic approaches for partial ordering of vertices. Mahantesh et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] empirically studied inference of arrival order via a binning method. In this context, we provide a rigorous formulation and analysis, a simpler solution without having to generate samples from an estimated random graph model. We also provide a precise characterization of the performance of our methods. To the best of our knowledge, our work presents the first feasible and rigorous approach to the problem of inferring partial orders in preferential attachment graphs.</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Problem Formulation</h2>    </div>    </header>    <p>Let <em>G</em> be a graph drawn from the model <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n,m)$</span>    </span>. Note that, by definition, the vertices of <em>G</em> are precisely the integers [<em>n</em>] = {1, ..., <em>n</em>}, where node <em>j</em> was the <em>j</em>th node to arrive. Next, the nodes are subjected to a permutation <em>&#x03C0;</em> drawn uniformly at random from the symmetric group <em>S<sub>n</sub>    </em>, and we are given the graph <em>G</em>&#x2032; &#x2254; <em>&#x03C0;</em>(<em>G</em>); that is, the nodes of <em>G</em> are randomly <em>relabeled</em>. This paper aims to develop methods for inferring arrival order in <em>G</em> after observing <em>G</em>&#x2032;, i.e., to find <em>&#x03C0;</em>    <sup>&#x2212; 1</sup>. The permutation <em>&#x03C0;</em>    <sup>&#x2212; 1</sup> can be considered as the true arrival order of the nodes of the given graph; that is, <em>&#x03C0;</em>    <sup>&#x2212; 1</sup>(<em>u</em>) for a node <em>u</em> in <em>G</em>&#x2032; gives the arrival time of that node in the original graph process.</p>    <p>In our recent work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], a solution to this problem takes the form of an estimator function <span class="inline-equation"><span class="tex">$\phi : {\mathfrak {G}}_n \rightarrow S_n$</span>    </span>, where <em>S<sub>n</sub>    </em> is the symmetric group on <em>n</em> letters and <span class="inline-equation"><span class="tex">$ {\mathfrak {G}}_n$</span>    </span> is the set of graphs on <em>n</em> vertices. We evaluated estimators using the expected Kendall <em>&#x03C4;</em> distance between <em>&#x03D5;</em>(<em>&#x03C0;</em>(<em>G</em>)) and <em>&#x03C0;</em>    <sup>&#x2212; 1</sup> as a metric. In particular, they showed that <em>any</em> such estimator makes, with high probability, <em>&#x0398;</em>(<em>n</em>    <sup>2</sup>) inversion errors (note that the maximum possible number of such errors is <span class="inline-equation"><span class="tex">${n\atopwithdelims ()2} = \Theta (n^2)$</span>    </span>), owing to the existence of a superexponentially large number of equiprobable graphs with the same structure as <em>&#x03C0;</em>(<em>G</em>).</p>    <p>In view of this negative result, we relax the problem: instead of requiring that an estimator infer a total order (i.e., a permutation), we study estimators that output <em>partial</em> orders on the set of vertices. For a partial order <em>&#x03C3;</em>, a relation <em>u</em> < <sub>     <em>&#x03C3;</em>    </sub>    <em>v</em> indicates a guess that <em>&#x03C0;</em>    <sup>&#x2212; 1</sup>(<em>u</em>) < <em>&#x03C0;</em>    <sup>&#x2212; 1</sup>(<em>v</em>). The advantage of allowing partial orders is that the ability of an estimator to refrain from guessing the relative order of some vertex pairs allows for the possibility of greater <em>precision</em>. An example of our new setting is given in Figure&#x00A0;<a class="fig" href="#fig1">1</a>. <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186105/images/www2018-114-fig1.svg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">An example scenario. The estimator sees only <em>&#x03C0;</em>(<em>G</em>) and must infer <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>. E.g., it may output the order <em>&#x03C3;</em> = {4&#x227A;1&#x227A;2} The relation 4&#x227A;1 is correct, since <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(4) = 3 < <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(1) = 4, but the relations 4&#x227A;2 and 1&#x227A;2 are incorrect, since <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(4) = 3 > <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(2) = 2 and <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(1) = 4 > <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(2) = 2. The density is <span class="inline-equation"><span class="tex">$\delta (\sigma) = 3 / {4\atopwithdelims ()2} = 3/6 = 1/2$</span>       </span>, the precision is <em>&#x03B8;</em>(<em>&#x03C3;</em>) = 1/<em>K</em>(<em>&#x03C3;</em>) = 1/3, and the recall is <em>&#x03C1;</em>(<em>&#x03C3;</em>) = <em>&#x03B8;</em>(<em>&#x03C3;</em>)<em>&#x03B4;</em>(<em>&#x03C3;</em>) = 1/6. (See below for definitions of <em>&#x03B4;</em>, <em>&#x03B8;</em> and <em>&#x03C1;</em>.)</span>     </div>    </figure>    </p>    <p>In evaluating a partial order <em>&#x03C3;</em> as a solution to our problem, we say that an ordered pair of vertices (<em>u</em>, <em>w</em>) in <em>&#x03C0;</em>(<em>G</em>) satisfying <em>u</em> < <sub>     <em>&#x03C3;</em>    </sub>    <em>w</em> forms a <em>correct pair</em> if <em>&#x03C0;</em>    <sup>&#x2212; 1</sup>(<em>u</em>) < <em>&#x03C0;</em>    <sup>&#x2212; 1</sup>(<em>w</em>). A pair of vertices is <em>unguessed</em> with respect to <em>&#x03C3;</em> if they are unrelated in <em>&#x03C3;</em>. Note that, given a partial order <em>&#x03C3;</em>, we can always algorithmically find a total order consistent with <em>&#x03C3;</em> (i.e., a linear extension of <em>&#x03C3;</em>) and containing at least as many correct pairs (but generally with more incorrect pairs as well).</p>    <p>A partial order may be described by a directed, acyclic graph (DAG) on the set of vertices. We find it useful to describe a partial order by a <em>binning</em>, which is simply a more compact representation of a DAG: vertices within a given bin are unrelated in the partial order, while an edge from a bin <span class="inline-equation"><span class="tex">$\mathcal {B}_1$</span>    </span> to another one, say, <span class="inline-equation"><span class="tex">$\mathcal {B}_2$</span>    </span>, indicates that <em>v</em>    <sub>1</sub> < <sub>     <em>&#x03C3;</em>    </sub>    <em>v</em>    <sub>2</sub> for every <span class="inline-equation"><span class="tex">$v_1 \in \mathcal {B}_1$</span>    </span> and <span class="inline-equation"><span class="tex">$v_2 \in \mathcal {B}_2$</span>    </span>.</p>    <p>    <strong>Why is maximum likelihood estimation not a good approach?</strong>In the case of total order estimators, a natural way to approach the problem is to frame it in terms of maximum likelihood estimation (MLE) as follows: <span class="inline-equation"><span class="tex">$\mathcal {C}_{\text{ ML}}(H) = \mathop{arg\,max}_{\sigma \in S_n} {\mathrm{Pr}}[G = \sigma ^{-1}(H)| \pi (G) = H].$</span>    </span> The following proposition gives a characterization of the optimal solution set <span class="inline-equation"><span class="tex">$\mathcal {C}_{\text{ML}}$</span>    </span>. Its proof sketch can be found in the Section&#x00A0;<a class="sec" href="#sec-19">5.3</a>.</p>    <p>    <div class="proposition" id="enc1">     <Label>Proposition 2.1 (MLE of total orders).</Label>     <p> The maximum likelihood estimation solution set <span class="inline-equation"><span class="tex">$\mathcal {C}_{\text{ML}} = \mathcal {C}_{\text{ML}}(\pi (G))$</span>      </span> satisfies <span class="inline-equation"><span class="tex">$\displaystyle \vert \mathcal {C}_{\text{ML}} \vert = e^{n \log n - O(n\log \log n)}$</span>      </span> with high probability.</p>    </div>    </p>    <p>Thus, <span class="inline-equation"><span class="tex">$\mathcal {C}_{\text{ML}}$</span>    </span> gives a large number of equiprobable solutions and the maximum likelihood formulation is not the appropriate approach for the problem.</p>    <p>    <strong>Measures for evaluating partial order estimators.</strong>In order to evaluate estimators that output partial orders, we define a few measures that capture important aspects of their performance: for a partial order <em>&#x03C3;</em>, let <em>K</em>(<em>&#x03C3;</em>) denote the number of pairs (<em>u</em>, <em>v</em>) that are comparable under <em>&#x03C3;</em>: i.e., <em>K</em>(<em>&#x03C3;</em>) = |{(<em>u</em>, <em>v</em>)~: ~<em>u</em> < <sub>     <em>&#x03C3;</em>    </sub>    <em>v</em>}|, where the notation <em>u</em> < <sub>     <em>&#x03C3;</em>    </sub>    <em>v</em> means that <em>u</em> is less than <em>v</em> according to the partial order <em>&#x03C3;</em>.</p>    <p>    <em>Density</em>: The density of a partial order <em>&#x03C3;</em> is simply the number of comparable pairs, normalized by the total possible number, <span class="inline-equation"><span class="tex">$n\atopwithdelims ()2$</span>    </span>. That is, <span class="inline-equation"><span class="tex">$\delta (\sigma) = \frac{K(\sigma)}{ {n\atopwithdelims ()2}}.$</span>    </span> Note that <em>&#x03B4;</em>(<em>&#x03C3;</em>) &#x2208; [0, 1]. Then the density of a partial order estimator <em>&#x03D5;</em> is simply its minimum possible density <em>&#x03B4;</em>(<em>&#x03D5;</em>) = min&#x2009;<sub>     <em>H</em>    </sub>[<em>&#x03B4;</em>(<em>&#x03D5;</em>(<em>H</em>))].</p>    <p>    <em>Measure of precision</em>: This measures the expected fraction of <em>correct</em> pairs out of all pairs that are guessed by the partial order. It is given by <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \theta (\sigma) = \mathbb {E}\left[ \frac{1}{K(\sigma)} | \lbrace u, v \in [n] ~:~ u {\lt}_{\sigma } v, \pi ^{-1}(u) {\lt} \pi ^{-1}(v) \rbrace | \right]. \] </span>      <br/>     </div>    </div> For an estimator <em>&#x03D5;</em>, we also denote by <em>&#x03B8;</em>(<em>&#x03D5;</em>) the quantity <span class="inline-equation"><span class="tex">$\mathbb {E}[\theta (\phi (\pi (G)))]$</span>    </span>.</p>    <p>The previous two measures capture the salient behaviors of interest in an estimator. For convenience, we will sometimes refer to an additional measure, which differs from the precision in the choice of normalization:</p>    <p>    <em>Measure of recall</em>: This measure gives the expected fraction of correct pairs (out of the total number, <span class="inline-equation"><span class="tex">$n\atopwithdelims ()2$</span>    </span>) given by the algorithm that outputs the partial order <em>&#x03C3;</em>. It can be interpreted as a modified version of the Kendall <em>&#x03C4;</em> metric for partial orders. It is defined as <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \rho (\sigma) = \mathbb {E}\left[ \frac{1}{\binom{n}{2}} |\lbrace u, v \in [n] ~:~ u {\lt}_\sigma v, \pi ^{-1}(u) {\lt} \pi ^{-1}(v) \rbrace | \right].\end{align*} </span>      <br/>     </div>    </div>    </p>    <p>There is a simple relation among the different measures: for a given partial order <em>&#x03C3;</em>, we have <em>&#x03C1;</em>(<em>&#x03C3;</em>) = <em>&#x03B8;</em>(<em>&#x03C3;</em>) &#x00B7; <em>&#x03B4;</em>(<em>&#x03C3;</em>).</p>    <p>Our main objective is then to seek an estimator that is optimal in the following sense: for an input parameter &#x025B; &#x2208; [0, 1], we wish to find an estimator <em>&#x03D5;</em> which has <em>&#x03B4;</em>(<em>&#x03D5;</em>) &#x2265; &#x025B; and maximum possible precision <em>&#x03B8;</em>(<em>&#x03D5;</em>). This then yields an <em>optimal curve &#x03B8;</em>    <sub>*</sub>(&#x025B;), that characterizes the tradeoff between precision and density. The main results of this paper (explained in the next section) derives computable bounds on this curve and efficient heuristic estimators that approach it.</p>    <p>    <div class="remark" id="enc2">     <Label>Remark 1.</Label>     <p> We could have defined a tradeoff curve between precision and recall, but this would have the undesirable feature that the optimal precision is not necessarily defined for every value of recall in [0, 1], since some values for recall are not achievable [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>].</p>    </div>    </p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Main Results</h2>    </div>    </header>    <p>Our main results concern the approximation of the optimal precision function <em>&#x03B8;</em>    <sub>*</sub>(&#x025B;) defined in the previous section. We start by expressing the precision of a given estimator <em>&#x03D5;</em> as a sum over all graphs <em>H</em>: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray*} &&\theta (\phi) = \sum _{H} {\mathrm{Pr}}[\pi (G)=H] \frac{1}{K(\phi (H))} \times {\nonumber }\\ && \sum _{1 \le i {\lt} j \le n} \mathbb {E}\left[ |\lbrace u,v\in [n] : u {\lt}_{\phi (H)} v, \pi ^{-1}(u) {\lt} \pi ^{-1}(v) \rbrace | \Big | \pi (G)=H \right],\end{eqnarray*} </span>      <br/>     </div>    </div> where the conditional expectation is with respect to the randomness in <em>&#x03C0;</em> and <em>G</em>. Note, however, that, under the conditioning, once <em>&#x03C0;</em> is fixed, then so is <em>G</em>. Note that we were able to take <span class="inline-equation"><span class="tex">$\frac{1}{K(\phi (H))}$</span>    </span> outside the expectation because it is constant with respect to <em>&#x03C0;</em>.</p>    <p>This equation shows that, to exhibit an optimal estimator, it is sufficient to choose, for each <em>H</em>, a value for <em>&#x03D5;</em>(<em>H</em>) (i.e., a partial order) that maximizes the expression <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{align} \frac{\mathbb {E}\left[ |\lbrace u,v\in [n] ~:~ u {\lt}_{\phi (H)} v, \pi ^{-1}(u) {\lt} \pi ^{-1}(v) \rbrace | ~\Big | \pi (G)=H \right]}{ K(\phi (H)) } \end{align} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div> subject to the density constraint that <span class="inline-equation"><span class="tex">$K(\phi (H)) \ge \varepsilon {n\atopwithdelims ()2}$</span>    </span>.</p>    <p>To do this, we derive a useful representation for this optimization problem as an integer program. There are various ways to do this, but our method allows for simple upper bounds via linear programming relaxations.</p>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Integer program formulation</h3>     </div>    </header>    <p>To each ordered pair (<em>u</em>, <em>v</em>) of vertices of <em>H</em>, we associate a binary variable <em>x</em>     <sub>      <em>u</em>, <em>v</em>     </sub>, where setting <em>x</em>     <sub>      <em>u</em>, <em>v</em>     </sub> = 1 indicates that <em>u</em> < <sub>      <em>&#x03D5;</em>(<em>H</em>)</sub>     <em>v</em>. Then, we can rewrite the objective function (<a class="eqn" href="#eq1">1</a>) in terms of these variables by expressing the cardinalities in the numerator and denominator as sums of these indicators. Using linearity of expectation, we set: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{align} J_{\varepsilon }(\phi) = \frac{\sum _{1 \le u {\lt} v \le n } p_{u,v}(H) x_{u,v} }{ \sum _{1 \le u \ne v \le n} x_{u,v}}, \end{align} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>) = Pr[<em>&#x03C0;</em>     <sup>&#x2212; 1</sup>(<em>u</em>) < <em>&#x03C0;</em>     <sup>&#x2212; 1</sup>(<em>v</em>)|<em>&#x03C0;</em>(<em>G</em>) = <em>H</em>]. We have the following constraints, coming from the fact that the outputs of our estimators are partial orders and from our constraint on a given minimum density:</p>    <ol class="list-no-style">     <li id="list5" label="(1)">Antisymmetry: <em>x</em>      <sub>       <em>u</em>, <em>v</em>      </sub> + <em>x</em>      <sub>       <em>v</em>, <em>u</em>      </sub> &#x2264; 1.<br/></li>     <li id="list6" label="(2)">Transitivity: <em>x</em>      <sub>       <em>u</em>, <em>w</em>      </sub> &#x2265; <em>x</em>      <sub>       <em>u</em>, <em>v</em>      </sub> + <em>x</em>      <sub>       <em>v</em>, <em>w</em>      </sub> &#x2212; 1 for all <em>u</em>, <em>v</em>, <em>w</em> &#x2208; [<em>n</em>].<br/></li>     <li id="list7" label="(3)">Minimum density: <span class="inline-equation"><span class="tex">$\sum _{1 \le u \ne v \le n} x_{u,v} \ge \varepsilon {n\atopwithdelims ()2}$</span>      </span>.<br/></li>     <li id="list8" label="(4)">Domain restriction: <em>x</em>      <sub>       <em>u</em>, <em>v</em>      </sub> &#x2208; {0, 1} for all <em>u</em>, <em>v</em> &#x2208; [<em>n</em>].<br/></li>    </ol>    <p>The above integer program is rather explicit, except for the coefficient <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>). The next lemma gives a combinatorial formula for the probability <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>). Associated with permutations of the random graph model, we define the following terms: <em>Set of feasible permutations of a graph <em>H</em>     </em>: the subset <em>&#x0393;</em>(<em>H</em>)&#x2286;<em>S<sub>n</sub>     </em>, which consists of permutations <em>&#x03C3;</em>, such that <em>&#x03C3;</em>(<em>H</em>) has positive probability under the distribution <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n,m)$</span>     </span>. An example of a permutation that is <em>not</em>feasible for a graph <em>G</em> generated by preferential attachment is <em>&#x03C0;</em> = (1, <em>n</em>), which swaps the first and last vertices. This is because the degree of the vertex labeled <em>n</em> in the resulting graph <em>&#x03C0;</em>(<em>G</em>) is > <em>m</em>, which happens with probability zero. <em>Set of admissible graphs of <em>H</em>     </em>: Adm(<em>H</em>) = {<em>&#x03C3;</em>(<em>H</em>)~: ~<em>&#x03C3;</em> &#x2208; <em>&#x0393;</em>(<em>H</em>)}. These are the graphs obtained by applying elements of <em>&#x0393;</em>(<em>H</em>) to <em>H</em>.</p>    <div class="lemma" id="enc3">     <Label>Lemma 3.1 (Expression for coefficients of the optimal precision integer program).</Label>     <p> For all <em>v</em>, <em>w</em> &#x2208; [<em>n</em>] and graphs <em>H</em>, <span class="inline-equation"><span class="tex">${\mathrm{Pr}}[\pi ^{-1}(v) {\lt} \pi ^{-1}(w) | \pi (G) = H] = \frac{ | \lbrace \sigma ~:~ \sigma ^{-1} \in \Gamma (H), \sigma ^{-1}(v) {\lt} \sigma ^{-1}(w) \rbrace | }{|\Gamma (H)|}.$</span>      </span>     </p>    </div>    <p>The proof is given in Section&#x00A0;<a class="enc" href="#enc3">3.1</a>.</p>    <p>We thus end up with an integer program written in terms of coefficients that are (in principle) explicitly computable, and the expression in Lemma&#x00A0;<a class="enc" href="#enc3">3.1</a> can be used to guide intuition in the design of more efficient heuristic methods. In the next subsection, we define the directed version of a sampled graph <em>&#x03C0;</em>(<em>G</em>) and explain how to efficiently recover it from <em>&#x03C0;</em>(<em>G</em>). This will be essential in the calculation of the coefficients <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>), and it will yield efficient heuristic partial order estimators.</p>    <div class="remark" id="enc4">     <Label>Remark 2.</Label>     <p> The integer program formulation and Lemma&#x00A0;<a class="enc" href="#enc3">3.1</a> hold for any random graph model in which two positive-probability graphs that are isomorphic are equiprobable.</p>    </div>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Recovering edge directions</h3>     </div>    </header>    <p>To efficiently calculate <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>), we need to further characterize the set <em>&#x0393;</em>(<em>H</em>) (note that its cardinality is invariant under relabeling). At a high level, given <em>H</em> = <em>&#x03C0;</em>(<em>G</em>), we can recover a natural directed, acyclic version Dir(<em>H</em>), which induces a partial order on its vertices. We can then show that <em>&#x0393;</em>(<em>H</em>) is precisely the set of <em>linear extensions</em> of this partial order. We can then use algorithms developed for approximate counting of linear extensions to estimate <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>).</p>    <p>We first formalize the DAG in question:</p>    <div class="definition" id="enc5">     <Label>Definition 3.2 (DAG of G).</Label>     <p> For <em>G</em> distributed according to the preferential attachment distribution (for any <em>m</em>), we define DAG(<em>G</em>) to be the directed acyclic graph defined on the same vertex set as <em>G</em>, such that there is an edge from <em>u</em> to <em>v</em> < <em>u</em> if and only if there is an edge between <em>u</em> and <em>v</em> in <em>G</em>. This is just <em>G</em> with the directions of edges marked in accordance with the graph evolution (incident on an existing node from a new node). <a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>. For an arbitrary permutation <em>&#x03C3;</em> with <em>H</em> = <em>&#x03C3;</em>(<em>G</em>), we denote by Dir(<em>H</em>) = <em>&#x03C3;</em>(DAG(<em>G</em>)).</p>    </div>    <section id="sec-10">     <header>      <div class="title-info">       <h4>       <span class="section-number">3.2.1</span>        <strong>Exact recovery of <em>&#x03C0;</em>(DAG(<em>G</em>))</strong>       </h4>      </div>     </header>     <p>Next, we show that we can efficiently recover <em>&#x03C0;</em>(DAG(<em>G</em>)), given access to <em>H</em> = <em>&#x03C0;</em>(<em>G</em>), via a method that we call the <SmallCap>Peeling</SmallCap> technique. <em>We note here that DAG(<em>G</em>) and <em>&#x03C0;</em>(DAG(<em>G</em>)) are exactly the same in structure and relabeling will not affect the directions.</em> Hence recovering the directions of edges from a younger node to an older node is the first step toward our goal. The algorithm starts by identifying the lowest-degree nodes (in our model, the nodes of degree exactly <em>m</em>) and places them in the youngest bin. Then, it removes all of these nodes and their edges from the graph. The process proceeds recursively, removing the lowest-degree nodes and placing them in the next-youngest bin, until there are no more nodes to remove from the graph. To construct Dir(<em>H</em>) = <em>&#x03C0;</em>(DAG(<em>G</em>)) during this process, we simply note that all of the edges of a given degree-<em>m</em> node in a given step of the peeling process must be to older nodes; hence, their orientations can be recovered. See Lemma&#x00A0;<a class="enc" href="#enc6">3.3</a> and Section&#x00A0;<a class="sec" href="#sec-21">5.5</a> for its proof. An example graph and its DAG with the indication of bins are shown in Figure&#x00A0;<a class="fig" href="#fig2">2</a>.</p>     <p>      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186105/images/www2018-114-img1.svg" class="img-responsive" alt="" longdesc=""/>     </p>     <div class="lemma" id="enc6">      <Label>Lemma 3.3 (Reconstruction of &#x03C0;(DAG(G))).</Label>      <p> The <SmallCap>Peeling</SmallCap> algorithm exactly recovers <em>&#x03C0;</em>(DAG(<em>G</em>)) from <em>&#x03C0;</em>(<em>G</em>).</p>     </div>     <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186105/images/www2018-114-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Graph (left) and its DAG (right) where the same bin nodes have the same colors.</span>      </div>     </figure>     <p>The directed, acyclic graph <em>&#x03C0;</em>(DAG(<em>G</em>)) conveniently encodes the set of all vertex pairs whose true order relationship can be inferred with <em>complete certainty</em>. To make this precise, for a graph <em>H</em> or the resulting DAG Dir(<em>H</em>), we define a <em>vertex pair order event</em>(<em>u</em>, <em>v</em>), for vertices <em>u</em>, <em>v</em> &#x2208; <em>H</em>, to be simply an ordered pair of distinct vertices. We interpret this as a claim that <em>u</em> came before <em>v</em> according to <em>&#x03C0;</em>      <sup>&#x2212; 1</sup>.</p>     <div class="definition" id="enc7">      <Label>Definition 3.4 (Perfect vertex pair order event).</Label>      <p> A vertex pair order event (<em>u</em>, <em>v</em>) is <em>perfect</em> for a graph <em>H</em> if, for all <em>&#x03C3;</em> &#x2208; <em>&#x0393;</em>(<em>H</em>), we have <em>&#x03C3;</em>(<em>u</em>) < <em>&#x03C3;</em>(<em>v</em>). Equivalently, recalling that <em>&#x03C0;</em> was the uniformly random permutation used to relabel the vertices of <em>G</em>, Pr[<em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(<em>u</em>) < <em>&#x03C0;</em>       <sup>&#x2212; 1</sup>(<em>v</em>)|<em>&#x03C0;</em>(<em>G</em>) = <em>H</em>] = 1.</p>     </div>     <p>The following result formalizes our intuition that Dir(<em>H</em>) captures all probability-1 information about vertex ordering in <em>H</em>:</p>     <div class="theorem" id="enc8">      <Label>Theorem 3.5 (Dir(H) captures perfect vertex pair information).</Label>      <p> Consider a graph <em>H</em> on the vertex set [<em>n</em>] satisfying <em>&#x0393;</em>(<em>H</em>) &#x2260; &#x2205;. Let its DAG Dir(<em>H</em>) be denoted by <em>K</em>.</p>      <p>For any <em>u</em>, <em>v</em> &#x2208; [<em>n</em>], (<em>u</em>, <em>v</em>) is perfect for <em>H</em> if and only if there exists a directed path in <em>K</em> from <em>v</em> to <em>u</em> (denoted by <span class="inline-equation"><span class="tex">$v \leadsto u$</span>       </span>).</p>     </div>     <div class="proof" id="proof1">      <Label>Proof.</Label>      <p>First, we will show that if <span class="inline-equation"><span class="tex">$v \leadsto u$</span>       </span>, then (<em>u</em>, <em>v</em>) is perfect. This follows by</p>      <p>showing the simpler claim that if there is an edge from <em>v</em> to <em>u</em> (denoted by <em>v</em> &#x2192; <em>u</em>), then (<em>u</em>, <em>v</em>) is perfect.</p>      <p>Let <em>&#x03C3;</em> &#x2208; <em>&#x0393;</em>(<em>K</em>). This means that <em>&#x03C3;</em>(<em>K</em>) is a positive-probability DAG under the preferential attachment model. Note also that <em>&#x03C3;</em>(<em>v</em>) &#x2192; <em>&#x03C3;</em>(<em>u</em>) in <em>&#x03C3;</em>(<em>K</em>), which implies that we certainly must have <em>&#x03C3;</em>(<em>v</em>) > <em>&#x03C3;</em>(<em>u</em>) (vertices only choose to connect to older vertices). Since <em>&#x03C3;</em> was arbitrary, we have that (<em>u</em>, <em>v</em>) is perfect for <em>H</em>, as desired.</p>      <p>Now we show the converse claim: if (<em>u</em>, <em>v</em>) is perfect for <em>K</em>, then <span class="inline-equation"><span class="tex">$v\leadsto u$</span>       </span> in <em>K</em>. We will do this by proving the contrapositive: assume that there is no directed path from <em>v</em> to <em>u</em>. Then we will construct a permutation <em>&#x03C3;</em> satisfying i).&#x00A0;<em>&#x03C3;</em> &#x2208; <em>&#x0393;</em>(<em>K</em>), and ii).&#x00A0;<em>&#x03C3;</em>(<em>u</em>) > <em>&#x03C3;</em>(<em>v</em>). This is equivalent to producing a feasible <em>schedule</em> of the vertices of <em>K</em> (i.e., a sequence of distinct vertices <em>v</em>       <sub>1</sub>, <em>v</em>       <sub>2</sub>, ..., <em>v<sub>n</sub>       </em> of <em>K</em>, such that, for each <em>j</em> &#x2208; [<em>n</em>], all <em>m</em> of the the descendants of <em>v<sub>j</sub>       </em> in <em>K</em> are contained in the set {<em>v</em>       <sub>1</sub>, ..., <em>v</em>       <sub>       <em>j</em> &#x2212; 1</sub>}). We will require that <em>v<sub>u</sub>       </em> > <em>v<sub>v</sub>       </em> in the schedule. Such a schedule gives a permutation satisfying the properties above as follows: for each <em>j</em>, <em>&#x03C3;</em>(<em>j</em>) = <em>v<sub>j</sub>       </em>.</p>      <p>To do this, we start by considering the sub-DAG <em>K<sub>v</sub>       </em>, consisting of <em>v</em> and all of its descendants. Now, we set <em>v</em>       <sub>1</sub> to be the bottom vertex of <em>K</em> (which is also the bottom vertex of <em>K<sub>v</sub>       </em>). We will add subsequent vertices to our schedule as follows: at each time step <em>t</em> > 1, [<em>n</em>] is partitioned into three parts: <em>S</em>       <sub>       <em>p</em>, <em>t</em>       </sub> (the vertices already in the schedule), <em>S</em>       <sub>       <em>a</em>, <em>t</em>       </sub> (the <em>active</em> vertices; i.e., those vertices not in <em>S</em>       <sub>       <em>p</em>, <em>t</em>       </sub>, all of whose children in <em>K</em> are contained in <em>S</em>       <sub>       <em>p</em>, <em>t</em>       </sub>), and <em>S</em>       <sub>       <em>d</em>, <em>t</em>       </sub> (the <em>dormant</em> vertices; i.e., those vertices that are not active or already processed). So <em>S</em>       <sub>       <em>p</em>, 1</sub> = {<em>v</em>       <sub>1</sub>}, <em>S</em>       <sub>       <em>a</em>, 1</sub> is the set of neighbors of <em>v</em>       <sub>1</sub>, and <em>S</em>       <sub>       <em>d</em>, 1</sub> consists of the rest of the vertices.</p>      <p>We observe that <em>S</em>       <sub>       <em>a</em>, <em>t</em>       </sub> is nonempty unless <em>t</em> = <em>n</em>: otherwise, there are less than <em>n</em> vertices in <em>S</em>       <sub>       <em>p</em>, <em>t</em>       </sub> (in fact, precisely <em>t</em> of them), and the rest are in <em>S</em>       <sub>       <em>d</em>, <em>t</em>       </sub>. In this case, consider the <em>lowest</em> vertex &#x2113; in <em>S</em>       <sub>       <em>d</em>, <em>t</em>       </sub>; &#x2113; cannot have any children in <em>S</em>       <sub>       <em>d</em>, <em>t</em>       </sub>, since it is the lowest, so all of its children must be in <em>S</em>       <sub>       <em>p</em>, <em>t</em>       </sub>. But this means precisely that &#x2113; &#x2208; <em>S</em>       <sub>       <em>a</em>, <em>t</em>       </sub>. Thus, <em>S</em>       <sub>       <em>a</em>, <em>t</em>       </sub> must be nonempty.</p>      <p>Note that it is clear that at any time <em>t</em>, we can designate any active vertex as the next one in our schedule; we would then move it to the processed set, potentially resulting in some vertices in <em>S</em>       <sub>       <em>d</em>, <em>t</em>       </sub> becoming active.</p>      <p>Now, observe that at time <em>t</em> = 1, some vertex from <em>K<sub>v</sub>       </em> must be active (by the same reasoning that established that the active set must be nonempty). In fact, until all vertices of <em>K<sub>v</sub>       </em> have been processed, there remains at least one such vertex that is active. We thus choose active vertices <em>K<sub>v</sub>       </em> until it is entirely processed (note that we do not process the vertex <em>u</em>&#x2209;<em>K<sub>v</sub>       </em> yet, since there is no directed path from <em>v</em> to <em>u</em>). Then we process active vertices until a complete schedule has been generated. By construction, <em>v</em> comes earlier in the schedule than <em>u</em>, which completes the proof.</p>     </div>    </section>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span>       <strong>Integer program coefficients via Dir(<em>H</em>)</strong>      </h3>     </div>    </header>    <p>The discussion in the previous subsection particularly implies that <em>&#x0393;</em>(<em>H</em>) is precisely the set of linear extensions of the partial order defined by Dir(<em>H</em>).</p>    <p>Coming back to computing the coefficients <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>) in the integer program, we see that this task can be reduced to the problem of counting linear extensions of Dir(<em>H</em>) and of Dir(<em>H</em>) with the additional relation that <em>u</em>&#x227A;<em>v</em>.</p>    <p>In full generality, the problem of counting linear extensions of an arbitrary partial order is classically known to be #P-complete [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. However, there exist fully polynomial-time approximation schemes for the problem, which allow us to approximate the coefficients up to an arbitrarily small relative error.</p>    <div class="proposition" id="enc9">     <Label>Proposition 3.6 (FPTAS for approximating pu, v(H)).</Label>     <p> There exists a randomized algorithm which, on input <em>H</em> and positive number <em>&#x03BB;</em>, outputs a sequence <span class="inline-equation"><span class="tex">$\hat{p}_{u,v}(H)$</span>      </span> satisfying <span class="inline-equation"><span class="tex">$ |\hat{p}_{u,v}(H) - p_{u,v}(H)| \le \lambda p_{u,v}(H)$</span>      </span> for all <em>u</em>, <em>v</em> &#x2208; [<em>n</em>] with probability 1 &#x2212; <em>o</em>(1), in time <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} O(n^5 (\log n)^3 \lambda ^{-2} \log (1/\lambda)).\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <p>The time complexity given in the above proposition is based on the worst case running times of the fastest known schemes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>].</p>    <p>Given that we can approximate the coefficients <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>) by <span class="inline-equation"><span class="tex">$\hat{p}_{u,v}(H)$</span>     </span> = (1 &#x00B1; <em>&#x03BB;</em>)<em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>) uniformly for arbitrarily small <em>&#x03BB;</em> > 0, the next lemma bounds the effect of this approximation on the optimal value of the integer program.</p>    <div class="lemma" id="enc10">     <Label>Lemma 3.7 (Effect of perturbation of pu, v(H)).</Label>     <p> Consider the integer program whose objective function is given by <span class="inline-equation"><span class="tex">$\hat{J}_{\varepsilon ,\lambda }(\phi) = \frac{\sum _{1 \le u {\lt} v \le n} \hat{p}_{u,v}(H) x_{u,v} }{\sum _{1\le u \ne v \le n} x_{u,v}}$</span>      </span>, with the same constraints as in the original IP. Let <em>&#x03D5;</em>      <sub>*</sub> and <span class="inline-equation"><span class="tex">$\hat{\phi }_*$</span>      </span> denote optimal points for the original and modified integer programs, respectively. Then we have <span class="inline-equation"><span class="tex">$ |\hat{J}_{\varepsilon ,\lambda }(\hat{\phi }_*) - J_{\varepsilon }(\phi _*)| \le 2.5\lambda ,$</span>      </span> for arbitrary <em>&#x03BB;</em> > 0.</p>    </div>    <p>This particularly implies that we can approximate the optimal objective function value to an arbitrarily small relative error, provided that the true value is bounded away from 0.</p>    <p>Given these results, we can efficiently approximately upper bound the optimal precision for any given density constraint as follows: on a randomly generated input graph <em>H</em>, we recover its edge directions and use them to approximate the coefficients <em>p</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>) up to some relative error <em>&#x03BB;</em> (using Proposition&#x00A0;<a class="enc" href="#enc9">3.6</a>). Now, at this point, we have a rational linear integer program with estimates for the coefficients. We can convert this to an equivalent truly linear integer program using a standard renormalization transformation (we will not explicitly indicate this transformation in what follows).</p>    <p>We then consider the natural linear programming relaxation, obtained by replacing the binarity constraint with <em>x</em>     <sub>      <em>u</em>, <em>v</em>     </sub>(<em>H</em>) &#x2208; [0, 1] for all <em>u</em>, <em>v</em>. This can be solved in polynomial time using standard tools. The result is depicted in Figure&#x00A0;<a class="fig" href="#fig3">3</a>. <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186105/images/www2018-114-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">LP relaxation to the optimal precision curve <em>&#x03B8;</em>       <sub>*</sub>(&#x025B;) and estimators for <span class="inline-equation"><span class="tex">$G\sim \mathcal {P}\!\mathcal {A}(n=50,m=3)$</span>       </span>. The bold points indicate averaged value.</span>      </div>     </figure>    </p>    <p>We thus have an effectively computable approximate upper bound on the precision-density curve, which we can use as a baseline for comparison with efficient heuristics.</p>    <p>Due to the high polynomial time complexity involved in solving the optimal scheme (Proposition&#x00A0;<a class="enc" href="#enc9">3.6</a> and complexity of linear programming), we now provide efficient estimators whose performance is close to the optimal curve. They stand as efficiently computable lower bounds on the optimal precision for particular densities, which empirically show the tightness of our upper bound.</p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Estimators</h3>     </div>    </header>    <p>We propose three estimators here. A detailed analysis of the estimators is given in Section&#x00A0;<a class="sec" href="#sec-16">5</a>. Figure&#x00A0;<a class="fig" href="#fig3">3</a> shows a comparison of the following estimators with the optimal estimator. All these estimators are based on the <SmallCap>Peeling</SmallCap> procedure. <em>The time complexity of the estimators is based only on the formation of DAG, and is <em>O</em>(<em>n</em>log&#x2009;<em>n</em>).</em> Note that one must take care in designing an estimator to ensure that the resulting set of vertex pair order relations satisfies transitivity and antisymmetry.</p>    <p>     <strong>Maximum-density precision 1 estimator.</strong>The estimator itself takes as input a graph <em>H</em> and outputs the partial order as Dir(<em>H</em>) as recovered by the <SmallCap>Peeling</SmallCap> algorithm. This estimator gives the maximum density among all estimators that have precision one.</p>    <p>     <SmallCap>      <strong>Peeling</strong>     </SmallCap>     <strong>: A linear binning estimator via peeling.</strong>Another natural estimator arises from the bins given by the <SmallCap>Peeling</SmallCap> procedure. In particular, the sequence of subsets of vertices removed during each step naturally gives a partial order: each such subset forms a bin, and bins that are removed earlier are considered to contain younger vertices (recall the definition of a binning, given near the beginning of Section&#x00A0;<a class="sec" href="#sec-6">2</a>). This forms a <em>linear binning</em>, which is a special type of partial order, defined as follows: a partial order &#x227A; is a linear binning if its elements may be partitioned into subsets <span class="inline-equation"><span class="tex">$\mathcal {B}_1, ..., \mathcal {B}_j$</span>     </span>, for some <em>j</em>, such that <em>u</em>&#x227A;<em>v</em> if and only if there is a pair <em>i</em> < <em>j</em> such that <span class="inline-equation"><span class="tex">$u \in \mathcal {B}_i$</span>     </span> and <span class="inline-equation"><span class="tex">$v \in \mathcal {B}_j$</span>     </span>. This is equivalent to a graded partial order. With a slight abuse of notaion, when the term <SmallCap>Peeling</SmallCap> is used as an estimator, we mean the above linear binning estimator.</p>    <p>     <SmallCap>      <strong>Peeling</strong>     </SmallCap>     <strong>+: Peeling with deduction of same bin pairs.</strong> This estimator first runs the <SmallCap>Peeling</SmallCap> algorithm, and classifies the vertices into bins. For each node except those in bin <span class="inline-equation"><span class="tex">$\mathcal {B}_1$</span>     </span>, we find the averaged value of its neighbors&#x2019; bin numbers (levels), which we call in what follows the node&#x0027;s <em>average neighbor level</em>. A high value of average neighbor level indicates youngness of the node. For each pair of nodes inside each bin, we infer the order between them based on the the averaged neighbor level of the respective nodes. Moreover, its performance is very close to the optimal curve at around &#x025B; = 1. (The density may not be exactly 1 because there may still exist some pairs inside bins that have the same average neighbor level and thus makes them incomparable. Moreover we cannot order the nodes inside bin 0).</p>    <p>The proposed three estimators serve different purposes. The perfect-precision estimator outputs pairs with full accuracy, but only a few. The <SmallCap>Peeling</SmallCap>+ gives a total order, but with less accuracy (which is much better than random guessing, and close to the optimal algorithm). The <SmallCap>Peeling</SmallCap> stands in the middle with better accuracy than <SmallCap>Peeling</SmallCap>+, and yet recovers a constant fraction of number of pairs. We will primarily focus on <SmallCap>Peeling</SmallCap> estimator.</p>    </section>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>    </div>    </header>    <p>In this section, we evaluate our methods on synthetic and real-world networks. In what follows, <span class="inline-equation"><span class="tex">$\sigma _{\text{perf}}, \sigma _{\text{peel}}, \sigma _{\text{peel+}}$</span>    </span> denote the partial orders produced by the Perfect-precision, <SmallCap>Peeling</SmallCap>, and <SmallCap>Peeling</SmallCap>+ algorithms.</p>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Synthetic graphs</h3>     </div>    </header>    <p>We derive significant insight by studying the <SmallCap>Peeling</SmallCap> method empirically.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Comparison for <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n = 5000,m)$</span>       </span>.      </span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <em>m</em>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\theta (\sigma _{\textrm{peel}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\rho (\sigma _{\textrm{peel}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\delta (\sigma _{\textrm{peel}})$</span>        </span>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">5</td>       <td style="text-align:center;">0.954</td>       <td style="text-align:center;">0.758</td>       <td style="text-align:center;">0.794</td>       </tr>       <tr>       <td style="text-align:left;">10</td>       <td style="text-align:center;">0.971</td>       <td style="text-align:center;">0.858</td>       <td style="text-align:center;">0.884</td>       </tr>       <tr>       <td style="text-align:left;">25</td>       <td style="text-align:center;">0.986</td>       <td style="text-align:center;">0.936</td>       <td style="text-align:center;">0.949</td>       </tr>       <tr>       <td style="text-align:left;">50</td>       <td style="text-align:center;">0.993</td>       <td style="text-align:center;">0.967</td>       <td style="text-align:center;">0.974</td>       </tr>      </tbody>     </table>    </div>    <p>Table&#x00A0;<a class="tbl" href="#tab1">1</a> and Figure&#x00A0;<a class="fig" href="#fig4">4</a> shows simulation results for samples from the Barab&#x00E1;si-Albert model. The performance improves greatly with even small increases in <em>m</em>. This can be explained intuitively as follows: for small <em>m</em>, as the graph evolves, each new vertex is likely to connect to high-degree nodes, which are already in older bins in DAG(<em>G</em>). Thus, bins tend to be large, resulting in low precision and recall. For larger <em>m</em>, each new node chooses a degree-<em>m</em> node (say, <em>v</em>) with higher probability, since a constant fraction of all nodes have degree <em>m</em> and the number of choices that each new vertex makes increases with <em>m</em>. A new vertex choosing <em>v</em> results in shifting the bin indices of all descendants of <em>v</em>. Thus, larger <em>m</em> leads to a DAG with a more equitable distribution of vertices across bins, which yields higher precision and recall. <figure id="fig4">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186105/images/www2018-114-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Recall of the <SmallCap>Peeling</SmallCap> algorithm remains constant when the number of nodes varies.</span>      </div>     </figure>     <strong>Robustness of the</strong>     <SmallCap>      <strong>Peeling</strong>     </SmallCap>     <strong>algorithm.</strong> Table&#x00A0;<a class="tbl" href="#tab2">2</a> demonstrates robustness of our <SmallCap>Peeling</SmallCap> algorithm for various generalizations of the model: preferential attachment model with variable <em>m</em> (denoted by <em>M</em> and &#x223C; unif{<em>a</em>, <em>b</em>} denote discrete uniform distribution), uniform attachment model (denoted by <span class="inline-equation"><span class="tex">$\mathcal {U}\!\mathcal {A}$</span>     </span>), and the very general Cooper-Frieze model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]. In our instance of Cooper-Frieze model, at each time instant, number of new edges at a time step <em>m</em> is drawn from &#x223C; unif{5, 50} , the model allows either addition of a new node (with probability 0.75) or addition of edges between exiting nodes, and the endpoints of new edges can be selected either preferentially (with probability 0.5) or uniformly among existing nodes.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">A general comparison: <em>n</em> = 5000.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Technique</th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\theta (\sigma _{\textrm{peel}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\rho (\sigma _{\textrm{peel}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\delta (\sigma _{\textrm{peel}})$</span>        </span>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n,m=25)$</span>        </span>       </td>       <td style="text-align:center;">0.958</td>       <td style="text-align:center;">0.936</td>       <td style="text-align:center;">0.977</td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n,M)$</span>        </span>, <em>M</em> &#x223C; unif{5, 50}</td>       <td style="text-align:center;">0.691</td>       <td style="text-align:center;">0.683</td>       <td style="text-align:center;">0.988</td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\mathcal {U}\!\mathcal {A}(n,m=25)$</span>        </span>       </td>       <td style="text-align:center;">0.977</td>       <td style="text-align:center;">0.967</td>       <td style="text-align:center;">0.99</td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\mathcal {U}\!\mathcal {A}(n,M)$</span>        </span>, <em>M</em> &#x223C; unif{5, 50}</td>       <td style="text-align:center;">0.827</td>       <td style="text-align:center;">0.823</td>       <td style="text-align:center;">0.995</td>       </tr>       <tr>       <td style="text-align:left;">Cooper-Frieze (Web graph) model</td>       <td style="text-align:center;">0.828</td>       <td style="text-align:center;">0.822</td>       <td style="text-align:center;">0.993</td>       </tr>      </tbody>      <tfoot>       <tr>       <td>.</td>       <td/>       <td/>       <td/>       </tr>      </tfoot>     </table>    </div>    <p>These results suggest that the proposed DAG-based methods can simultaneously achieve high precision and recall/density.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Real-world networks</h3>     </div>    </header>    <p>     <strong>Datasets.</strong> We use the following datasets. All the datasets except human brain network are publicly available. We explain later how the brain network is formed.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Network statistics.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Dataset</th>       <th style="text-align:center;"># Nodes</th>       <th style="text-align:center;"># Edges</th>       <th style="text-align:center;">Genre</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">ArXiv High Energy Physics</td>       <td style="text-align:center;">7, 464</td>       <td style="text-align:center;">116, 268</td>       <td style="text-align:center;">Citation</td>       </tr>       <tr>       <td style="text-align:left;">Simple English Wikipedia</td>       <td style="text-align:center;">100, 312</td>       <td style="text-align:center;">1, 627, 472</td>       <td style="text-align:center;">Hyperlink</td>       </tr>       <tr>       <td style="text-align:left;">DBLP CS bibliography</td>       <td style="text-align:center;">1, 137, 114</td>       <td style="text-align:center;">5, 018, 065</td>       <td style="text-align:center;">Coauthorship</td>       </tr>       <tr>       <td style="text-align:left;">Human protein interaction</td>       <td style="text-align:center;">14, 867</td>       <td style="text-align:center;">126, 593</td>       <td style="text-align:center;">Biological</td>       </tr>       <tr>       <td style="text-align:left;">Human brain</td>       <td style="text-align:center;">56</td>       <td style="text-align:center;">1, 164</td>       <td style="text-align:center;">Biological</td>       </tr>      </tbody>      <tfoot>       <tr>       <td>.</td>       <td/>       <td/>       <td/>       </tr>      </tfoot>     </table>    </div>    <p>The ArXiv, Wikipedia and DBLP are collected from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>], and human protein network is available from BioGRID (<a class="link-inline force-break" href="https://thebiogrid.org">https://thebiogrid.org</a>). The first three networks have partial temporal information available, and he original ranking of the network is deduced from the time stamp of the edges. For the protein network, only a prediction is presented in previous works, and for the brain network no explicit temporal information is available.</p>    <p>     <em>The ArXiv network:</em> Directed network; the nodes are the publications and the edges are formed when a publication cite another. The original ranking is not a full ranking; it has 1457 bins.</p>    <p>     <em>The Simple English Wikipedia dynamic network:</em> Directed network; it shows the evolution of hyperlinks between articles of the Simple English Wikipedia. Nodes represent articles and an edge indicates that a hyperlink was added or removed.</p>    <p>     <em>DBLP computer science bibliography:</em> Undirected; an edge between two authors represents a common publication.</p>    <p>     <em>Human protein interaction network:</em> Directed network; the nodes are the proteins, and the links indicate interaction between two proteins in gene formation. The original ranking is taken as the approximate ranking provided by ProteinHistorian&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], and has only 16 bins. Note that the ranking provided in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] is only an approximation, and not the ground truth.</p>    <p>     <em>Human brain network:</em> It is an undirected network. The nodes here are the regions in human brain, and the edges represent communication between two regions while performing an activity. The network is formed as follows. We gathered the human brain fMRI data at resting state from the Cambridge-Buckner dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>]. An initial network is first formed with nodes as voxels, and there are 243,648 voxels. Each voxel is associated with a time series data that lasted approximately 350 seconds. We compute the Pearson correlation coefficient between the time series of every pair of voxels. If the correlation is greater than 0.8 we form an edge between the voxels. Finally we form a network of 56 regions, which is a contracted network of voxels - all voxels corresponding to a region are merged to form a node, and an edge is created between two regions when there exists at least one edge the between member voxels.</p>    <p>     <strong>Results.</strong> The results for all the datasets except human brain network are presented in Table&#x00A0;<a class="tbl" href="#tab4">4</a>. For all real networks above, with the exception of the protein interaction network, our algorithms yield excellent precision and recall results, consistent with our theoretical predictions. Note that these results would have improved if we have complete temporal information instead of partial original ranking with bins. The above table shows the use of <SmallCap>Peeling</SmallCap>+ algorithm. When the density of the recovered partial order by <SmallCap>Peeling</SmallCap> algorithm is low, the recall can be improved via <SmallCap>Peeling</SmallCap>+ with a slight loss in precision (see the Wikipedia result).</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Results for real-world networks: <span class="inline-equation"><span class="tex">$\theta (\sigma _{\textrm{peel+}}) \approx \rho (\sigma _{\textrm{peel+}})$</span>       </span> and <span class="inline-equation"><span class="tex">$\delta ({\sigma _{\textrm{peel+}}}) \approx 1$</span>       </span>.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;"/>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\theta (\sigma _{\textrm{peel}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\rho (\sigma _{\textrm{peel}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\delta ({\sigma _{\textrm{peel}}})$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\rho (\sigma _{\textrm{peel+}})$</span>        </span>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">ArXiv</td>       <td style="text-align:center;">0.708</td>       <td style="text-align:center;">0.681</td>       <td style="text-align:center;">0.961</td>       <td style="text-align:center;">0.707</td>       </tr>       <tr>       <td style="text-align:center;">Wikipedia</td>       <td style="text-align:center;">0.624</td>       <td style="text-align:center;">0.548</td>       <td style="text-align:center;">0.878</td>       <td style="text-align:center;">0.609</td>       </tr>       <tr>       <td style="text-align:center;">DBLP</td>       <td style="text-align:center;">0.785</td>       <td style="text-align:center;">0.728</td>       <td style="text-align:center;">0.927</td>       <td style="text-align:center;">0.764</td>       </tr>       <tr>       <td style="text-align:center;">Protein interaction</td>       <td style="text-align:center;">0.5526</td>       <td style="text-align:center;">0.511</td>       <td style="text-align:center;">0.925</td>       <td style="text-align:center;">0.54</td>       </tr>      </tbody>     </table>    </div>    <p>     <em>Human protein interaction network:</em> In the case of protein interaction networks, our definitions of precision and recall are overly pessimistic for a few reasons, the first of which is that the &#x201C;ground truth&#x201D; that we use is a very sparse binning. A pair whose order is inferred by our algorithm and not inferred in this ground truth is counted as an error. Thus, we extend the ground truth ordering to a total order by randomly guessing the order of incomparable pairs, resulting in the numbers given. Moreover, importantly, the protein interaction network is much noisier than the others that we analyzed: while we have reasonable confidence that all edges are accounted for in the citation and Wikipedia networks, there is no such guarantee that this holds for the protein interaction network, as the presence or absence of each edge is costly to determine.</p>    <p>     <em>Human brain network:</em> The purpose here is to predict the evolutionary order among the important regions inside the brain. Figure&#x00A0;5a presents the bins of regions we found via <font style="font-variant: small-caps">Peeling</font>     </span> algorithm. Figure&#x00A0;5b shows an image of the human brain with prominent regions annotated with bin number. We note here that there is no available ground truth of the ranking, and we propose our order for further study in this area. Our ranking can be backed up with the results we derived in this paper and with the prior studies that show preferential attachment involvement in brain networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]. Moreover a quick consistency check can be done on some of the brain regions. E.g., the corpus callosum which joins the two hemispheres of the brain is supposed to develop at the earliest stages of brain evolution. Another example is the uncinate fasciculus, the last white matter tract to be evolved in the human brain, which is in compliance with our result. We consider this as a first step towards finding proper evolutionary order of brain regions. <figure id="fig5">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186105/images/www2018-114-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">       <SmallCap>Peeling</SmallCap> bins of regions in human brain network.</span>      </div>     </figure>    </p>    <p>Finally, in general, the more preferential attachment plays a role in the formation of a real network, the closer the performance will be to our theoretical and synthetic results. In some of the networks that we analyzed, previous studies report preferential attachment involvement, for instance see [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>]. V&#x00E9;rtes et al. proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] a widely used variation of preferential attachment model for brain functional networks.</p>    </section>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Analysis of Algorithms, Proofs</h2>    </div>    </header>    <p>Here we derive certain properties of the estimators and provide proof sketches for the results in the previous sections.</p>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Maximum-density precision 1 estimator</h3>     </div>    </header>    <p>In this subsection, we show that the point (density, precision) = (0, 1) is a point on the optimal precision curve: namely, we prove perfect-precision estimator estimator has precision one, but asymptotically negligible density, and this estimator has the maximum density among all precision one estimators. From Theorem&#x00A0;<a class="enc" href="#enc8">3.5</a>, we know that this achieves precision 1. So, in order to prove the remainder of our claim, we need to analyze the density of this estimator; that is, we analyze the typical number of perfect pairs, culminating in the following theorem.</p>    <div class="theorem" id="enc11">     <Label>Theorem 5.1 (Typical number of perfect pairs).</Label>     <p> With high probability, for arbitrary fixed <em>m</em> &#x2265; 1, the number of perfect pairs associated with <em>G</em> is <em>&#x03A9;</em>(<em>n</em>log&#x2009;<em>n</em>) and <em>o</em>(<em>n</em>      <sup>2</sup>) (uniformly in <em>m</em>). When <em>m</em> = 1, we have the matching upper bound of <em>O</em>(<em>n</em>log&#x2009;<em>n</em>), where the hidden constant in the asymptotic notation can be explicitly calculated.</p>    </div>    <div class="proof" id="proof2">     <Label>Proof.</Label>     <p>      <strong>Upper bound:</strong> Let <em>X<sub>t</sub>      </em> denote the number of perfect pairs in the graph immediately after time <em>t</em>. We will prove the claimed upper bound by upper bounding <em>X<sub>t</sub>      </em> in expectation, then using Markov&#x0027;s inequality. To bound <span class="inline-equation"><span class="tex">$\mathbb {E}[X_t]$</span>      </span>, we will show that it is sufficient to upper bound <em>X</em>(<em>v</em>) in expectation for each <em>v</em> < <em>t</em>, where <em>X</em>(<em>v</em>) denotes the number of descendants of <em>v</em> in the DAG.</p>     <p>Using Proposition&#x00A0;1 of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>] with &#x2113; = <em>&#x0398;</em>(log&#x2009;<sup>4/5</sup>      <em>n</em>log&#x2009;(log&#x2009;<em>n</em>)), we can show that the total number of descendants <em>X</em>(<em>u</em>), for all <em>u</em> &#x2264; <em>n</em>, is at most <em>O</em>(<em>n</em>/log&#x2009;<sup>1/5</sup>      <em>n</em>), with high probability.</p>     <p>Now, we translate this to an upper bound on <em>X<sub>t</sub>      </em> as follows: we have <span class="inline-equation"><span class="tex">$ \mathbb {E}[X_t] \le \mathbb {E}[X_{t-1}] + m + m O(t/\log ^{1/5} t).$</span>      </span>      <em>Thisupperboundisfromthefollowingfacts</em>: <em>attime</em>t, <em>allperfectvertexpairsfromtime</em>t-1<em>arestillperfect</em>, <em>contributingthe</em>Xt-1<em>term</em>.<em>Next</em>, <em>vertex</em>t<em>makesatmost</em>m<em>choices</em>, <em>creatingatmost</em>m<em>newperfectpairs</em>(<em>whichexplainsthesecondterm</em>).<em>Finally</em>, <em>thethirdtermcomesfromthefactthatif</em>t<em>chooses</em>v, <em>and</em>u<em>isadescendantof</em>v(<em>sothat</em>(u, v)<em>isaperfectpair</em>), <em>then</em>(u, t)<em>isalsoperfect</em>.Solving this recurrence, we find that <span class="inline-equation"><span class="tex">$\mathbb {E}[X_t] = o(t^2)$</span>      </span>, as desired, and the proof is completed using Markov&#x0027;s inequality.</p>     <p>In the case where <em>m</em> = 1, we have a much better upper bound on <em>X</em>(<em>v</em>), for arbitrary <em>v</em>: with high probability, at time <em>t</em>, <em>X</em>(<em>v</em>) = <em>O</em>(log&#x2009;<em>t</em>), as a result of the height of the tree being <em>O</em>(log&#x2009;<em>t</em>). This gives the improved bound of <span class="inline-equation"><span class="tex">$\mathbb {E}[X_t] = O(t\log t)$</span>      </span>.</p>     <p>      <strong>Lower bound:</strong> We only sketch the lower bound proof. In a nutshell, for <em>m</em> = 1, we can show that <em>X<sub>t</sub>      </em> is the <em>total path length</em> of the tree. We then use the results of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0018">18</a>] to finish the proof. For general <em>m</em>, we can reduce to the <em>m</em> = 1 case using the correspondence between <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(n, m)$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {P}\!\mathcal {A}(nm, 1)$</span>      </span>.</p>    </div>    <p>The above theorem implies that the density of the perfect pair estimator is asymptotically 0, for arbitrary <em>m</em>.</p>    <p>A scrutiny of the numerical evidence leads to a conjecture about the more precise behavior of the number of perfect pairs as a function of <em>m</em>: we conjecture that for <em>m</em> > 1, the number of perfect pairs is <em>O</em>(<em>n</em>     <sup>1 + <em>&#x03B4;</em>(<em>m</em>)</sup>), for some function 1 > <em>&#x03B4;</em>(<em>m</em>) > 0. For lack of space, we omit the explanation of the conjecture.</p>    <p>Theorem&#x00A0;<a class="enc" href="#enc11">5.1</a> gives us a single point on the optimal precision curve, and it is very simple to show that the curve is decreasing as &#x025B; increases. In the next section, we show <SmallCap>Peeling</SmallCap> estimator achieves nontrivial density and precision with high probability, which gives a lower bound on the optimal precision for another value of the density. We will use this to give an empirical indication of the tightness of our upper bound.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Linear binning estimator <SmallCap>Peeling</SmallCap> estimator</h3>     </div>    </header>    <p>We can start by showing a crude estimate of the precision and density of the <SmallCap>Peeling</SmallCap> estimator.</p>    <div class="theorem" id="enc12">     <Label>Theorem 5.2 (Non-negligible precision and density of the peeling estimator).</Label>     <p> For each <em>m</em> &#x2265; 1 the <SmallCap>Peeling</SmallCap> estimator has precision and density <em>&#x0398;</em>(1).</p>    </div>    <div class="proof" id="proof3">     <Label>Proof Proof (sketch)</Label>     <p> The precision claim follows by showing that, with high probability, there exist <em>&#x0398;</em>(<em>n</em>) vertices in the last <em>&#x0398;</em>(<em>n</em>) timesteps that are never chosen (so that they are removed in the first layer and, thus, correctly declared to be younger than <em>&#x0398;</em>(<em>n</em>) other vertices). This also proves the density claim.</p>    </div>    <p>We now discuss further analysis of the procedure. In the case <em>m</em> = 1, the original graph <em>G</em> is a tree, and we are able to precisely characterize certain parameters of the <SmallCap>Peeling</SmallCap> procedure, owing to the large amount of literature surrounding the preferential attachment distribution for this case (the structure is (essentially) identically distributed to a random <em>plane-oriented recursive tree</em>): for instance, the asymptotic size of the <em>j</em>th youngest bin, for each fixed <em>j</em>, may be precisely determined by writing it as an additive parameter and using results from Wagner et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>]. This gives us a complicated but explicitly computable convergent series expression for the typical asymptotic density of the estimator in this case. Precise theoretical results for <em>m</em> > 1 (and for the precision in the <em>m</em> = 1 case) are much more elusive, owing to the fact that the precision and the DAG structure of the graph fail to lead to clean recursive formulas for parameters of interest (though a P&#x00F3;lya urn approach allows us to derive very complicated expressions involving arbitrarily high moments and covariances of quantities of interest); however, we can show that the peeling process recovers all perfect pairs, in addition to many imperfect ones.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Proof sketch of Proposition <a class="enc" href="#enc1">2.1</a>      </h3>     </div>    </header>    <p>First, by definition of <em>&#x0393;</em>(<em>H</em>) (see Section&#x00A0;<a class="sec" href="#sec-8">3.1</a> for definitions), we must have that <span class="inline-equation"><span class="tex">$\mathcal {C}_{ML}(H) \subseteq \Gamma (H)$</span>     </span>. We will show, in fact, that the likelihoods given to all elements of <em>&#x0393;</em>(<em>H</em>) are equal. This will then imply that <span class="inline-equation"><span class="tex">$\mathcal {C}_{ML}(H) = \Gamma (H)$</span>     </span>. Next, from a result of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], we have that with high probability |<em>&#x0393;</em>(<em>&#x03C0;</em>(<em>G</em>))| = <em>e</em>     <sup>      <em>n</em>log&#x2009;<em>n</em> &#x2212; <em>O</em>(<em>n</em>log&#x2009;log&#x2009;<em>n</em>)</sup>, which will complete the proof.</p>    <p>So it is sufficient to show that, for each <em>&#x03C3;</em> &#x2208; <em>&#x0393;</em>(<em>H</em>), Pr[<em>G</em> = <em>&#x03C3;</em>(<em>H</em>)|<em>&#x03C0;</em>(<em>G</em>) = <em>H</em>] depends only on the structure <em>S</em>(<em>H</em>). To do this, note that by definition of Adm(<em>H</em>) and <em>&#x0393;</em>(<em>H</em>), we must have <em>&#x03C3;</em>(<em>H</em>) &#x2208; Adm(<em>H</em>). So it is enough to show that for any two positive-probability isomorphic graphs <em>G</em>     <sub>1</sub> and <em>G</em>     <sub>2</sub>, we have Pr[<em>G</em> = <em>G</em>     <sub>1</sub>] = Pr[<em>G</em> = <em>G</em>     <sub>2</sub>]. We omit the proof of this fact for the sake of brevity.</p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Proof of Lemma&#x00A0;<a class="enc" href="#enc3">3.1</a>      </h3>     </div>    </header>    <p>We can express the conditional probability in question as a sum, as follows: Pr[<em>&#x03C0;</em>     <sup>&#x2212; 1</sup>(<em>v</em>) < <em>&#x03C0;</em>     <sup>&#x2212; 1</sup>(<em>w</em>)|<em>&#x03C0;</em>(<em>G</em>) = <em>H</em>] <span class="inline-equation"><span class="tex">$= \sum _{\sigma ~:~ \sigma ^{-1} \in \Gamma (H), \sigma ^{-1}(v) {\lt} \sigma ^{-1}(w)} {\mathrm{Pr}}[\pi = \sigma | \pi (G)=H]$</span>     </span>.</p>    <p>Now, recall that <em>&#x03C0;</em>     <sup>&#x2212; 1</sup> &#x2208; <em>&#x0393;</em>(<em>H</em>) under this conditioning, since <em>&#x03C0;</em>(<em>G</em>) = <em>H</em> and <em>G</em> is admissible. Moreover, it is uniformly distributed on Iso(<em>G</em>, <em>H</em>) (the set of isomorphisms from <em>G</em> to <em>H</em>), so we have <span class="inline-equation"><span class="tex">$ {\mathrm{Pr}}[\pi = \sigma | \pi (G) = H] = \frac{{\mathrm{Pr}}[G = \sigma ^{-1}(H) | \pi (G)=H]}{| {\mathrm{Aut}}(H)|} = \frac{ 1 }{| {\mathrm{Aut}}(H)| | {\mathrm{Adm}}(H)|}.$</span>     </span> Taking the sum, this becomes <span class="inline-equation"><span class="tex">$ \frac{ |\lbrace \sigma ~:~ \sigma ^{-1}\in \Gamma (H), \sigma ^{-1}(v) {\lt} \sigma ^{-1}(w) \rbrace | }{| {\mathrm{Aut}}(H)| | {\mathrm{Adm}}(H)|}.$</span>     </span> Finally, recall that |Adm(<em>H</em>)| = |<em>&#x0393;</em>(<em>H</em>)|/|Aut(<em>H</em>)| [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. <span class="inline-equation"><span class="tex">$\Box$</span>     </span>    </p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.5</span> Proof of Lemma&#x00A0;<a class="enc" href="#enc6">3.3</a>      </h3>     </div>    </header>    <p>The <SmallCap>Peeling</SmallCap> algorithm maintains the following invariant at the beginning of each step: every degree-<em>m</em> node connects only to vertices older than itself in the remaining graph. This is clear in the initial step, since a node can only have degree exactly <em>m</em> in the full graph if its neighbors are all older than it is. In subsequent steps (assuming by induction that the invariant holds for all previous ones), if some edge incident with a degree-<em>m</em> node <em>u</em> is incident with a younger node <em>v</em> in the current graph, then this implies that some node <em>w</em> older than <em>u</em> and adjacent to <em>u</em> in a previous step has already been removed. This, in turn, implies that in that previous step, <em>w</em> had degree <em>m</em> and was connected to <em>u</em>, a younger vertex. This yields a contradiction, completing the proof. <span class="inline-equation"><span class="tex">$\Box$</span>     </span>    </p>    </section>   </section>   <section id="sec-22">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions</h2>    </div>    </header>    <p>We considered approaches to the problem of estimating the temporal ordering of nodes in a dynamically grown network, based on modeling by the preferential attachment distribution. To overcome the limitations imposed by the structure of such models, we dealt with partial ordering schemes. The proposed algorithms showed good performance both in analysis and simulations, that is close to the optimal estimator.</p>    <p>We expect that this work will open up many interesting questions on the inference of node ages in growing networks. </p>   </section>  </section>  <section class="back-matter">   <section id="sec-23">    <header>    <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>    </div>    </header>    <p>This work was supported by NSF Center for Science of Information (CSoI) Grant CCF-0939370, NSF Grants CCF-1524312 and CSR-1422338, and by NIH Grant 1U01CA198941-01. We thank Vikram Ravindra for providing network correlation matrix from the brain data. </p>   </section>   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Jacqueline Banks, Scott Garrabrant, Mark&#x00A0;L Huber, and Anne Perizzolo. 2010. Using TPA to count linear extensions. <em>      <em>arXiv preprint arXiv:1010.4981</em>     </em>(2010).</li>    <li id="BibPLXBIB0002" label="[2]">Albert-L&#x00E1;szl&#x00F3; Barab&#x00E1;si and R&#x00E9;ka Albert. 1999. Emergence of Scaling in Random Networks. <em>      <em>Science</em>     </em>286, 5439 (1999), 509&#x2013;512.</li>    <li id="BibPLXBIB0003" label="[3]">Bharat&#x00A0;B Biswal, Maarten Mennes, Xi-Nian Zuo, Suril Gohel, Clare Kelly, Steve&#x00A0;M Smith, Christian&#x00A0;F Beckmann, Jonathan&#x00A0;S Adelstein, Randy&#x00A0;L Buckner, Stan Colcombe, <em>et al.</em> 2010. Toward discovery science of human brain function. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>107, 10(2010), 4734&#x2013;4739.</li>    <li id="BibPLXBIB0004" label="[4]">Graham Brightwell and Peter Winkler. 1991. Counting Linear Extensions is #P-complete. In <em>      <em>Proceedings of the Twenty-third Annual ACM Symposium on Theory of Computing (STOC)</em>     </em>. ACM, New York, NY, USA, 175&#x2013;181.</li>    <li id="BibPLXBIB0005" label="[5]">S&#x00E9;bastien Bubeck, Luc Devroye, and G&#x00E1;bor Lugosi. 2016. Finding Adam in random growing trees. <em>      <em>Random Structures &#x0026; Algorithms</em>     </em>(2016).</li>    <li id="BibPLXBIB0006" label="[6]">John&#x00A0;A. Capra, Alexander&#x00A0;G. Williams, and Katherine&#x00A0;S. Pollard. 2012. ProteinHistorian: Tools for the Comparative Analysis of Eukaryote Protein Origin. <em>      <em>PLOS Computational Biology</em>     </em>8 (06 2012), 1&#x2013;9.</li>    <li id="BibPLXBIB0007" label="[7]">KONECT The Koblenz&#x00A0;Network Collection. 2016. Preferential attachment exponent. Available on http://konect.uni-koblenz.de/statistics/prefatt [Accessed: 30 April 2017]. (2016).</li>    <li id="BibPLXBIB0008" label="[8]">Colin Cooper. 2006. Distribution of vertex degree in web-graphs. <em>      <em>Combinatorics, Probability and Computing</em>     </em>15, 05 (2006), 637&#x2013;661.</li>    <li id="BibPLXBIB0009" label="[9]">Colin Cooper and Alan Frieze. 2003. A general model of web graphs. <em>      <em>Random Structures &#x0026; Algorithms</em>     </em>22, 3 (2003), 311&#x2013;335.</li>    <li id="BibPLXBIB0010" label="[10]">Alan Frieze, Wesley Pegden, <em>et al.</em> 2017. Looking for vertex number one. <em>      <em>The Annals of Applied Probability</em>     </em>27, 1 (2017), 582&#x2013;630.</li>    <li id="BibPLXBIB0011" label="[11]">International Neuroimaging Data-Sharing Initiative. 2017. The 1000 Functional Connectomes Project. http://fcon_1000.projects.nitrc.org/index.html. (2017). [Online; accessed 11-February-2018].</li>    <li id="BibPLXBIB0012" label="[12]">Brian&#x00A0;J Jellison, Aaron&#x00A0;S Field, Joshua Medow, Mariana Lazar, M&#x00A0;Shariar Salamat, and Andrew&#x00A0;L Alexander. 2004. Diffusion tensor imaging of cerebral white matter: a pictorial review of physics, fiber tract anatomy, and tumor imaging patterns. <em>      <em>American Journal of Neuroradiology</em>     </em>25, 3 (2004), 356&#x2013;369.</li>    <li id="BibPLXBIB0013" label="[13]">Alexander Karzanov and Leonid Khachiyan. 1991. On the conductance of order Markov chains. <em>      <em>Order</em>     </em>8, 1 (1991), 7&#x2013;15.</li>    <li id="BibPLXBIB0014" label="[14]">J&#x00E9;r&#x00F4;me Kunegis, Marcel Blattner, and Christine Moser. 2013. Preferential attachment in online networks: Measurement and explanations. In <em>      <em>Proceedings of the 5th Annual ACM Web Science Conference</em>     </em>. ACM, 205&#x2013;214.</li>    <li id="BibPLXBIB0015" label="[15]">Tomasz &#x0141;uczak, Abram Magner, and Wojciech Szpankowski. 2017. Asymmetry and structural information in preferential attachment graphs. <em>      <em>arXiv preprint arXiv:1607.04102</em>     </em>(2017).</li>    <li id="BibPLXBIB0016" label="[16]">Abram Magner, Ananth Grama, Jithin Sreedharan, and Wojciech Szpankowski. 2017. Recovery of Vertex Orderings in Dynamic Graphs. In <em>      <em>Proceedings of the IEEE International Symposium on Information Theory (ISIT)</em>     </em>.</li>    <li id="BibPLXBIB0017" label="[17]">S.&#x00A0;M.&#x00A0;Vijay Mahantesh, Sudarshan Iyengar, M. Vijesh, Shruthi&#x00A0;R. Nayak, and Nikitha Shenoy. 2012. Prediction of Arrival of Nodes in a Scale Free Network. In <em>      <em>Proceedings of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</em>     </em>. 517&#x2013;521.</li>    <li id="BibPLXBIB0018" label="[18]">Dimbinaina Ralaivaosaona and Stephan Wagner. 2016. Additive functionals of <em>d</em>-ary increasing trees. In <em>      <em>Proceedings of the 27th International Conference on Probabilistic, Combinatorial, and Asymptotic Methods for the Analysis of Algorithms</em>     </em>. 1&#x2013;12.</li>    <li id="BibPLXBIB0019" label="[19]">Devavrat Shah and Tauhid Zaman. 2011. Rumors in a network: Who&#x0027;s the culprit?<em>      <em>IEEE Transactions on information theory</em>     </em>57, 8 (2011), 5163&#x2013;5181.</li>    <li id="BibPLXBIB0020" label="[20]">Mansi Srivastava, Oleg Simakov, Jarrod Chapman, Bryony Fahey, Marie&#x00A0;EA Gauthier, Therese Mitros, Gemma&#x00A0;S Richards, Cecilia Conaco, Michael Dacre, Uffe Hellsten, <em>et al.</em> 2010. The Amphimedon queenslandica genome and the evolution of animal complexity. <em>      <em>Nature</em>     </em>466, 7307 (2010), 720&#x2013;726.</li>    <li id="BibPLXBIB0021" label="[21]">Petra&#x00A0;E V&#x00E9;rtes, Aaron&#x00A0;F Alexander-Bloch, Nitin Gogtay, Jay&#x00A0;N Giedd, Judith&#x00A0;L Rapoport, and Edward&#x00A0;T Bullmore. 2012. Simple models of human brain functional networks. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>109, 15(2012), 5868&#x2013;5873.</li>    <li id="BibPLXBIB0022" label="[22]">Kai Zhu and Lei Ying. 2016. Information source detection in the SIR model: A sample-path-based approach. <em>      <em>IEEE/ACM Transactions on Networking</em>     </em>24, 1 (2016), 408&#x2013;421.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>These authors contributed equally to this work.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>In many applications, for instance, brain coactivity, protein interaction, and citation networks, departures do not play a significant role. In other domains, it is important to consider departures &#x2013; we do not directly address these domains in our current work.</p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a>We will drop the subscript <em>n</em> in <em>G<sub>n</sub>    </em> if it is implied from the context.</p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a>In reference to the DAG of a preferential attachment graph, we ignore self-loops, without loss of generality.</p>   <p id="fn5">    <Label>3</Label>The original figure without bin number is taken from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0012">12</a>].</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186105">https://doi.org/10.1145/3178876.3186105</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
