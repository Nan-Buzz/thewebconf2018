<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Urban Perception of Commercial Activeness from Satellite Images and Streetscapes</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Urban Perception of Commercial Activeness from Satellite Images and Streetscapes</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Wenshan</span>      <span class="surName">Wang</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China, <a href="mailto:wswang14@fudan.edu.cn">wswang14@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Su</span>      <span class="surName">Yang</span><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China, <a href="mailto:suyang@fudan.edu.cn">suyang@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Zhiyuan</span>      <span class="surName">He</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China, <a href="mailto:16210240032@fudan.edu.cn">16210240032@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Minjie</span>      <span class="surName">Wang</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China, <a href="mailto:minjiewang12@fudan.edu.cn">minjiewang12@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Jiulong</span>      <span class="surName">Zhang</span>,     School of Computer Science, Xi&#x0027;an University of Technology, Xi&#x2019; an, China, <a href="mailto:zhangjiulong@xaut.edu.cn">zhangjiulong@xaut.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Weishan</span>      <span class="surName">Zhang</span>,     Department of Software Engineering, China University of Petroleum, Qingdao, China, <a href="mailto:zhangws@upc.edu.cn">zhangws@upc.edu.cn</a>     </div>                             </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186581" target="_blank">https://doi.org/10.1145/3184558.3186581</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>People can percept social attributes from streetscapes such as safety, richness, and happiness by means of visual perception, which inspires the research in terms of urban perception. To the best of our knowledge, this is the first work focused on revealing the relationship between visual patterns of satellite images as well as streetscapes and commercial activeness. We propose to make use of bag of features (BoF) in the context of computer vision and sparse representation in the sense of machine learning to predict commercial activeness of urban commercial districts. After obtaining the urban commercial districts via clustering, we predict the commercial activeness degrees of them using four image features, namely, Histogram of Oriented Gradients (HOG), Autoencoder, GIST, and multifractal spectra for satellite images and street view images, respectively. The performance evaluation with four large-scale datasets demonstrates that the presented computational framework can not only predict the commercial activeness with satisfactory precision compared with that based on Point of Interest (POI) data but also discover the visual patterns related.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Applied computing </strong>&#x2192; <em>Sociology;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Urban perception; social intelligence; pervasive computing; computer vision; data mining</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Wenshan Wang, Su Yang, Zhiyuan He, Minjie Wang, Jiulong Zhang, and Weishan Zhang. 2018. Urban Perception of Commercial Activeness from Satellite Images and Streetscapes. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA, 8 Pages. <a href="https://doi.org/10.1145/3184558.3186581" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186581</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>In terms of intelligent information processing over urban big data, an emerging trend is vision-based urban perception. For example, people can judge the richness and safety of a place according to the scene images of this place [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], which shows that there exist correlations between the visual patterns of scene images and the social contexts indicated by such visual patterns. The goal of urban perception is to discover such visual patterns that can act as signals to infer the social attributes of the corresponding scene images, and reveal the association rules to correlate the visual patterns with the social attributes of scene images.</p>    <p>In the literature, Arietta et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] propose to predict location-aware crime rate and population from Google streetscapes based on visual pattern analysis, and learn prediction models with the visual patterns and the corresponding social attributes as input and output, respectively. Khosla et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] propose to predict the distance to McDonald or hospital from the visual patterns of scene images, where they make use of a variety of visual pattern analysis techniques including convolutional neural networks (CNN) and the prediction results turned out from support vector regression (SVR) are comparable to those from human intelligence. Furthermore, they classify city functions into seven categories for urban planning based on about two million scene images collected from 21 cities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. Quercia et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] discover the visual patterns to make London beautiful, quiet, and happy based on crowdsourcing and image processing. Recently, Dubey et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] introduce the new dataset focused on liveliness and depression for urban perception. In particular, Nadai et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] investigate the correlation between the safety and the call frequency of mobile phone users in a region by using convolutional neural network based streetscape analysis. Gebru et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] propose a deep learning-based method to estimate socioeconomic characteristics of regions using 50 million Google street view images. Nikhil et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] use a computer vision method to model the dynamics of physical urban change from time-series streetscapes.    </p>    <p>Although a couple of works have been done to reveal the correlation between scene images and certain social attributes, business intelligence in terms of location-aware business planning in regard to surrounding social contexts, a significant issue in terms of socioeconomics and smart cities, remains a missing topic so far. It is known that the visual patterns of streetscapes are informative indicators of population [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>], city functions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], call frequency of mobile phone users reflecting human mobility[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>], and psychological perception of beautifulness and happiness [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], which are intuitively key factors to determine commercial activeness of a region, so the correlation between the visual patterns of streetscapes and commercial activeness is analyzed in this study. Aside from streetscapes, satellite images should be a natural means to observe a city from a global point of view and a recent trend is to leverage remote-sensing images to reveal the correlation between the visual patterns and the socioeconomic property of a region. Jean et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] propose to use convolutional neural networks along with transferring learning to learn features for predicting the poverty across a country from nighttime light intensities over satellite images. However, how and what kinds of visual features are correlated to city functions for a broad-spectrum social-economic issues is an open problem yet. In this study, we make use of satellite images to infer commercial activeness due to the intuition that city functions and infrastructures should be exhibited more straightforwardly in the global view of satellite images, which are the key issues among the social contexts to determine the commercial activeness of a region.</p>    <p>In this paper, we investigate the connection between the visual appearances of urban commercial district (UCD) and the corresponding commercial activeness, by applying computer vision and machine learning techniques to fit the visual features of UCD with the corresponding commercial activeness, which is quantized by means of the online reviews on commercial entities as proxy. To the best of our knowledge, this is the first work focused on revealing the relationship between visual patterns of urban regions and commercial activeness based on satellite images and street view images. Fig. <a class="fig" href="#fig1">1</a> and Fig. <a class="fig" href="#fig2">2</a> show some examples of street view images and satellite images in Beijing, respectively, which indicate different levels of commercial activeness. Intuitively, visual features can reflect which one corresponds with high/low commercial activeness. The motivation to fuse satellite images and streetscapes for commercial activeness perception is: It is known that city functions play an important role in attracting people&#x0027;s visiting to a region [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], which enables figuring out regionally geographic profiles in terms of data mining [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], and city infrastructures can be identified in remote-sensing images based on ground objects and structures via deep learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. However, the detailed visual appearances of the ground objects are not visible in satellite images but streetscapes can provide such details complementary to the global view of these objects. Therefore, by fusing visual information from satellite images and streetscapes, higher performance for commercial activeness perception can be expected, which has been experimentally confirmed in this study. The advantage of applying visual perception to prediction of commercial activeness lies in that we can reach a visually straightforward understanding about how commercial activeness is boomed, which reveals the phenomenon from not only the city infrastructure perspective but also the psychology perspective on what kinds of visual patterns affect people&#x0027;s favor. <figure id="fig1">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186581/images/www18companion-135-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Some examples of the street view images indicating different levels of commercial activeness. The levels of images are (highest) B > C > A (lowest).</span>     </div>     </figure> <figure id="fig2">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186581/images/www18companion-135-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Some examples of the satellite images in Beijing indicating different levels of commercial activeness. The levels are: (highest) B > C > A (lowest).</span>     </div>     </figure> </p>    <p>The problem in regard to urban perception is two-fold: (i) Discovering the physical law that correlates visual patterns with certain social attributes as well as detecting the corresponding visual patterns. (ii) Developing analytical toolkits in the context of computer vision and machine learning to enable sound analysis over visual big data for urban perception.    </p>    <p>In order to predict social attributes, most of the existing research works use classifical image features such as HOG, Scale Invariant Feature Transform (SIFT), Local Binary Patterns (LBP) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], and deep image features (convolutional neural network) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>]. On the basis of these works, we present a novel computational framework based on bag of features to predict commercial activeness of urban commercial districts (UCDs), where four image features are incorporated into the bag-of-features model for satellite images and street view images, say, GIST, HOG, Autoencoder, and multifractal spectra, respectively. Specifically, we aim to address the following three questions:</p>    <p>     <strong>RQ1</strong>: Can commercial activeness of UCDs be automatically predicted using classical image features (GIST, HOG, multifractal spectra) as well as deep features (Autoencoder) extracted from satellite images and streetscapes, respectively?</p>    <p>     <strong>RQ2</strong>: Can the combined features from satellite images and street view images predict the commercial activeness more accurately than using those based on either modality of images alone?</p>    <p>     <strong>RQ3</strong>: What visual patterns are highly correlated to commercial activeness?</p>    <p>To address these questions, we collect four datasets for Beijing and Shanghai, respectively: a) Point-of-Interest data; b) A dataset of large-scale streetscapes collected from web; c) A dataset of satellite images from Google Earth; d) A dataset of customers&#x2019; comments on commercial entities. We describe the visual appearances of UCDs using bag of features based on GIST, HOG, Autoencoder, and multifractal spectra for satellite images and street view images, respectively. Furthermore, we reveal the visual patterns related to the commercial activeness based on sparse representation.</p>    <p>The contributions are summarized as follows:</p>    <p>(1) To the best of our knowledge, it is the first work focused on revealing the relationship between visual patterns of scene images and commercial activeness of urban regions.</p>    <p>(2) The experimental results show the feasibility to automatically predict commercial activeness using bag-of-features model over the visual features of satellite images and street view images for each UCD. Based on bag-of-features model, moreover, we are able to find out the visual patterns that relate to commercial activeness.</p>    <p>(3) We aim to discover the correlation between the visual patterns and the commercial activeness in a city so as to predict urban commercial activeness by analyzing large-scale remote-sensing images. As far as we know, it is the first study focused on remote-sensing images and streetscapes rendered business intelligence in the context of urban perception. Interestingly, we identify some semantically meaningful objects only under the supervision of commercial activeness.</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Methods</h2>     </div>    </header>    <p>In this work, we consider predicting commercial activeness of UCDs as a regression problem based on BoF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>]. First, we employ the BoF model to obtain a representation of the UCDs&#x2019; visual appearances of interest. Second, we define and compute the proxy of the commercial activeness using the online review data for each UCD. Third, we apply the regression methods in terms of machine learning to model the association rules between the BoF descriptors and the commercial activeness of the UCDs of interest for the sake of predicting the activeness. Moreover, four generic image features are combined to form a single feature vector prior to computing the BoF model so as to examine how the prediction performance can be improved by fusing different visual features as a whole. The flowchart of the pipelined computational procedure is illustrated in Fig. <a class="fig" href="#fig3">3</a>.     <figure id="fig3">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186581/images/www18companion-135-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">The flowchart of the computational framework based on bag of features.</span>     </div>     </figure> </p>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Measures of visual appearances of urban regions</h3>     </div>     </header>     <p>Here, we use bag of features as our model to generate descriptors of urban appearances for each UCD. First, we collect the satellite images and the street view images for every UCD. Second, we partition every image into 50 patches to focus on the local contents. We randomly extract 256 &#x00D7; 256 patches for each modality of images. Third, we perform four kinds of feature extraction methods on every image patch, namely, GIST, HOG, Autoencoder, and multifractal spectra for street view images and satellite images, respectively. Fourth, we employ the BoF model to obtain a histogram-based overall profile of the UCD of interest, where similar visual features are grouped into identical histogram bins [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. The details are described below.</p>     <section id="sec-10">     <p><em>2.1.1 Feature extraction.</em>      <strong>GIST:</strong> GIST is a biologically inspired feature that globally encodes an image while ignores local details. Previous studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0021">21</a>] suggest that GIST simulates the process of human visual observation of scenes. Here, we use the popular GIST<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> for scene recognition over satellite images and street view images, respectively, which leads to 512-dimentional descriptor for each patch.</p>     <p>      <strong>HOG</strong>: Some studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>] suggest that the low-level visual perception mechanism in human visual systems is based on gradient features. Here, we choose to use the powerful feature referred to as HOG [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>]. In our experiments, we use 124-dimensional HOG descriptors for 2 &#x00D7; 2 image cells. We randomly sample 100 HOG descriptors and concatenate them together, which generates 12400-dimentional feature vector for each patch of satellite images as well as streetscapes.</p>     <p>      <strong>Multifractal spectra:</strong> Fractal dimensions provide clues to discriminate natural contexts from man-made objects [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0023">23</a>] and can measure the randomness of the object of interest. Multifractal theory extends classical fractal theory to enable more powerful analytical tools in image processing. In particular, multifractal spectra [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>] have been applied in a variety of practical applications such as texture analysis [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>]. Intuitively, economic activities are correlated to the distribution of infrastructures and natural plants in a city. To characterize the density of man-made infrastructures and natural plants as textures, we employ multifractal spectra to analyze satellite images and street view images. To encode each patch, we make use of multifractal spectra [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0011">11</a>] to generate 40-dimentional feature vector.</p>     <p>      <strong>Autoencoder</strong>: Artificial neural networks have been widely used in various challenging computer vision tasks. An autoencoder is a neural network, which is used to train its input to approach its output. It can then be used for feature representation and dimensionality reduction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>]. Sparse autoencoder is one of the variations of autoencoder with an additional sparse penalty term. In addition, autoencoders can be stacked to form a deep network by feeding the output of the previous layer as the input to the current layer.</p>     <p>In our experiments, we train stacked sparse autoencoders [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>] for predicting commercial activeness. The first sparse autoencoder learns a representation of the original input. Then, we treat the representation as a new input and use the second sparse autoencoder to learn a new representation for it. The third and fourth sparse autoencoders are learnt from the representations of the previous autoencoders. Here, we train a 4-layer stacked autoencoders. The number of the neurons in the 4-layer neural network is 80, 100, 100, and 100, respectively. We train the model using the stochastic gradient descent with a learning rate of 0.01. Every iteration in the learning procedure treats a collection of 25,000 patches. We extract features from the fourth layer to encode each image patch, resulting in the dimensionality of 100.</p>     </section>     <section id="sec-11">     <p><em>2.1.2 Bag-of-features model.</em> In order to build a dictionary, we use the K-means algorithm to cluster all the descriptors (GIST, HOG, multifractal spectra, and Autoencoder) of the image patches of the UCDs in the training set. Then, we apply vector quantization to assign the descriptors of each UCD to the dictionary to obtain a final BOF vector for each UCD&#x0027;s visual appearance. Here, we achieve optimal hyperparameters of K-means through grid search.</p>     </section>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Proxy for commercial activeness</h3>     </div>     </header>     <p>We define the proxy that represents the level of commercial activeness or popularity for an UCD as follows: In general, the key attribute of online review data is the number of reviews, which reflects users&#x2019; interests in commercial entities, namely, popularity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. Here, we sum the number of user-generated comments as the indictor of commercial activeness since it reflects how much a UCD attracts the attentions of users, who have truly experienced the interactions with the corresponding entities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>].</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Predicting methods (regression model)</h3>     </div>     </header>     <p>Here, we learn to predict commercial activeness of UCDs in the sense of regression. Given the image feature vector <em>x<sub>i</sub>     </em> &#x2208; <em>R<sup>n</sup>     </em> and the corresponding target value <em>y<sub>i</sub>     </em> &#x2208; <em>R</em>, our goal is to make the predicted value <span class="inline-equation"><span class="tex">${\hat{y}_i} \in R$</span>     </span> approximate the ground truth such that, <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} {\hat{y}_i} = {{{\bf \tilde{w}}}^T}\phi ({x_i}) + b \quad b \in R, i = 1,...,l \end{eqnarray} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${{\hat{y}}_i}$</span>     </span> is the predicted value, <em>&#x03D5;</em>(<em>x<sub>i</sub>     </em>) maps <em>x<sub>i</sub>     </em> into a higher-dimensional space, and <span class="inline-equation"><span class="tex">${{{\bf \tilde{w}}}}$</span>     </span> is the optimal weight vector to be learnt.</p>     <p>We use SVR with RBF kernel. In practical situation, the <em>&#x03BD;</em>-SVR in libsvm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] tries to minimize the following function: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} {{\bf \tilde{w}}} = \mathop {\arg \min }\limits _{{\bf w}} \frac{1}{2}{{{\bf w}}^T}{{\bf w}} + \frac{C}{l}\sum \limits _{i = 1}^l {(\max (0,\left| {{y_i} - {{\hat{y}}_i}} \right| - \varepsilon))} \end{eqnarray} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <strong>w</strong> is the weight vector, &#x025B; the constant, and <em>C</em> the regularization hyperparameter.</p>    </section>   </section>   <section id="sec-14">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experimental results</h2>     </div>    </header>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Datasets</h3>     </div>     </header>     <p>We use four datasets to conduct this research: a) Point-of-Interests data from Baidu Maps; b) A dataset of large-scale streetscapes collected from web using Baidu API; c) A dataset of satellite images from Google earth<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>; d) A dataset with customers&#x2019; comments on commercial entities collected from <a class="link-inline force-break" href="http://www.dianping.com">http://www.dianping.com</a>. </p>     <p>     <strong>Point of Interest</strong>: A record of point of interest (POI) contains the following items: ID of POI, the longitude and latitude of the location of interest, the address in plain text, and the category to denote the associated city function such as shopping mall, transportation facility, or, hotel. We collect 1,007,373 POI records for Beijing and 1,363,709 POI records for Shanghai via Baidu Maps API<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>.</p>     <p>     <strong>Streetscapes</strong>: We hold a large-scale dataset with 272,552 and 204,161 street view images for Beijing and Shanghai, respectively. Here, we use Baidu Maps API<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a> to collect images with 400 grid-point intervals within each UCD. We collect four images from per grid-point, which are located 50 meters apart in the four different directions.</p>     <p>     <strong>Satellite images:</strong> We collect 456 and 429 satellite images for Beijing and Shanghai, respectively, via Google Static Maps API<a class="fn" href="#fn6" id="foot-fn6"><sup>5</sup></a>. The resolution of each satellite image is 1024 &#x00D7; 768 pixels at zoom level 17, covering every UCD with the radius of around one kilometer.</p>     <p>     <strong>Comments on commercial entities</strong>: Some websites such as Yelp archive comments and reviews from customers in terms of their favor on an entity such as a restaurant. In China, the best-known website to record users&#x2019; comments on commercial entities is Dianping<a class="fn" href="#fn7" id="foot-fn7"><sup>6</sup></a>, where we collect 100,313 and 110,490 comments for Beijing and Shanghai, respectively. The review data are from December 9, 2014 to February 11, 2015. Each comment includes ID of POI, address of POI, category of POI, total number of users&#x2019; comments, average price of services or products, and users&#x2019; rating score. The number of reviews reflects users&#x2019; interests, say, popularity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>].</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Urban commercial districts</h3>     </div>     </header>     <p>We apply a clustering algorithm to obtain urban commercial districts from POIs data. The POIs with categories to be shopping malls or commercial streets are considered as the seeds for clustering. First, a few seeds are selected to initialize the centroids of the corresponding groups. Then, each POI is assigned to the group the centroid of which is the closest one to the POI among all the competitors. After that, the centroids of the corresponding groups should be updated by taking into account the newly included POIs. A new group will be created if the distance between the POI and the closest centroid is greater than a predefined threshold. Repeat the above procedure until convergence. We obtain the optimal threshold based on grid search.</p>     <p>We find that there are 456 and 429 UCDs in Beijing and Shanghai, respectively, by applying the clustering algorithm. As shown in Fig. <a class="fig" href="#fig4">4</a>, the areas of the circles represent the relative commercial activeness. <figure id="fig4">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186581/images/www18companion-135-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">The areas of the circles represent the relative commercial activeness.</span>      </div>     </figure>     </p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Performance in predicting commercial activeness</h3>     </div>     </header>     <p>Here, we evaluate the performance of a regression model using the metric referred to as Mean Absolute Percentage Error (MAPE) by which the precision is defined as follows: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} Accuracy = 1-\frac{1}{n}\sum _{i=1}^{n}\mid \frac{\hat{y}_i-y_i}{y_i}\mid \end{eqnarray} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <em>y<sub>i</sub>     </em> is the true value, <span class="inline-equation"><span class="tex">${{{\hat{y}}_i}}$</span>     </span> the predicted value, and <em>n</em> the number of the testing examples.</p>     <p>In this work, we perform four experiments. In general, we randomly partition the dataset into two parts, one for training and the other for testing. We compute the average accuracy over 10 tests with random train/test splits. In order to obtain optimal hyperparameters, we make use of grid search.</p>     <section id="sec-18">     <p><em>3.3.1 Baseline method.</em> It is known that people can percept some socioeconomic attributes from visual indicators of streetscape, such as city function [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0033">33</a>], safety [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>], and population [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>]. These attributes can also be predicted from POI data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>]. Therefore, to demonstrate the effectiveness of the proposed method, we employ POI data to predict commercial activeness as the baseline method. In this work, we utilize bag-of-features model to extract the 13-dimensional POI feature, which reflects the distribution of 13 categories of POIs within each UCD. The 13 different categories of POIs are listed in Table <a class="tbl" href="#tab1">1</a>. Then, we train the SVR-based predictive model with the 13-dimensional POI feature and the UCDs&#x2019; commercial activeness as input and output, respectively. The accuracy of the baseline method on Beijing and Shanghai datasets are shown in Table <a class="tbl" href="#tab2">2</a>.</p>     <div class="table-responsive" id="tab1">      <div class="table-caption">       <span class="table-number">Table 1:</span>       <span class="table-title">The POI features.      </span>      </div>      <table class="table">       <thead>        <tr>        <th style="text-align:center;">         <em>No</em>.</th>        <th style="text-align:left;">Types</th>        </tr>       </thead>       <tbody>        <tr>        <td style="text-align:center;">1</td>        <td style="text-align:left;">Catering services</td>        </tr>        <tr>        <td style="text-align:center;">2</td>        <td style="text-align:left;">Road traffic facilities</td>        </tr>        <tr>        <td style="text-align:center;">3</td>        <td style="text-align:left;">Address information</td>        </tr>        <tr>        <td style="text-align:center;">4</td>        <td style="text-align:left;">Famous scenery</td>        </tr>        <tr>        <td style="text-align:center;">5</td>        <td style="text-align:left;">Company enterprise</td>        </tr>        <tr>        <td style="text-align:center;">6</td>        <td style="text-align:left;">Shopping services</td>        </tr>        <tr>        <td style="text-align:center;">7</td>        <td style="text-align:left;">Financial insurance services</td>        </tr>        <tr>        <td style="text-align:center;">8</td>        <td style="text-align:left;">Science, education, and cultural services</td>        </tr>        <tr>        <td style="text-align:center;">9</td>        <td style="text-align:left;">Automobile service</td>        </tr>        <tr>        <td style="text-align:center;">10</td>        <td style="text-align:left;">Living service</td>        </tr>        <tr>        <td style="text-align:center;">11</td>        <td style="text-align:left;">Sports entertainment services</td>        </tr>        <tr>        <td style="text-align:center;">12</td>        <td style="text-align:left;">Health care services</td>        </tr>        <tr>        <td style="text-align:center;">13</td>        <td style="text-align:left;">Government agencies and social organizations</td>        </tr>       </tbody>      </table>     </div>     <div class="table-responsive" id="tab2">      <div class="table-caption">       <span class="table-number">Table 2:</span>       <span class="table-title">The predicted accuracy (%) using SVR with 13-dimensional POI features.</span>      </div>      <table class="table">       <thead>        <tr>        <th style="text-align:center;">City</th>        <th style="text-align:center;">POI features</th>        </tr>       </thead>       <tbody>        <tr>        <td style="text-align:center;">         <tt>Beijing</tt>        </td>        <td style="text-align:center;">55.68</td>        </tr>        <tr>        <td style="text-align:center;">         <tt>Shanghai</tt>        </td>        <td style="text-align:center;">54.47</td>        </tr>       </tbody>      </table>     </div>     </section>     <section id="sec-19">     <p><em>3.3.2 Satellite images.</em> As shown in Table <a class="tbl" href="#tab3">3</a>, we evaluate the precision of the regression model on four image feature representations for Beijing and Shanghai based on satellite images, respectively. We observe that both Autoencoder and multifractal spectra achieve the superior prediction results on Beijing and Shanghai datasets in the sense of MAPE. This is because Autoencoder is capable of representing high-level concepts and multifractal spectra have the ability to distinguish between natural and artificial image contents. They should be good representations of satellite images. Besides, we find that the combined features achieve the best performance. In addition, the MAPE value is from 55% to 60%, indicating that the regression model can achieve better prediction results compared with the baseline method.</p>     <div class="table-responsive" id="tab3">      <div class="table-caption">       <span class="table-number">Table 3:</span>       <span class="table-title">Prediction results (%) using regression model on Beijing and Shanghai using satellite images.</span>      </div>      <table class="table">       <thead>        <tr>        <th style="text-align:center;">City</th>        <th style="text-align:center;">HOG</th>        <th style="text-align:center;">Autoencoder</th>        <th style="text-align:center;">GIST</th>        <th style="text-align:center;">Multifractal</th>        <th style="text-align:left;">All</th>        </tr>       </thead>       <tbody>        <tr>        <td style="text-align:center;">         <tt>Beijing</tt>        </td>        <td style="text-align:center;">56.30</td>        <td style="text-align:center;">58.80</td>        <td style="text-align:center;">57.67</td>        <td style="text-align:center;">57.88</td>        <td style="text-align:left;">60.17</td>        </tr>        <tr>        <td style="text-align:center;">         <tt>Shanghai</tt>        </td>        <td style="text-align:center;">55.27</td>        <td style="text-align:center;">57.30</td>        <td style="text-align:center;">56.78</td>        <td style="text-align:center;">57.12</td>        <td style="text-align:left;">59.83</td>        </tr>       </tbody>      </table>     </div>     </section>     <section id="sec-20">     <p><em>3.3.3 Street view images.</em> In terms of streetscapes, we evaluate the accuracy of the regression model on four image feature representations for Beijing and Shanghai, respectively. As shown in Table <a class="tbl" href="#tab4">4</a>, we observe that the Autoencoder feature achieves the best prediction result for both Beijing and Shanghai datasets in the sense of MAPE. Autoencoder is a hierarchical feature that can represent abstract concepts well. Besides, we find that the combined features (HOG, Autoencoder, GIST, and multifractal spectra) lead to better performance for both Beijing and Shanghai. Compared with the baseline method, the MAPE is from 56% to 62%, which indicates that the regression model gains advantage over POI based prediction.</p>     <div class="table-responsive" id="tab4">      <div class="table-caption">       <span class="table-number">Table 4:</span>       <span class="table-title">Prediction of Commercial Activeness (%) in Beijing and Shanghai using street view images.</span>      </div>      <table class="table">       <thead>        <tr>        <th style="text-align:center;">City</th>        <th style="text-align:center;">HOG</th>        <th style="text-align:center;">Autoencoder</th>        <th style="text-align:center;">GIST</th>        <th style="text-align:center;">Multifractal</th>        <th style="text-align:left;">All</th>        </tr>       </thead>       <tbody>        <tr>        <td style="text-align:center;">         <tt>Beijing</tt>        </td>        <td style="text-align:center;">61.40</td>        <td style="text-align:center;">62.53</td>        <td style="text-align:center;">61.10</td>        <td style="text-align:center;">60.85</td>        <td style="text-align:left;">62.66</td>        </tr>        <tr>        <td style="text-align:center;">         <tt>Shanghai</tt>        </td>        <td style="text-align:center;">58.18</td>        <td style="text-align:center;">59.35</td>        <td style="text-align:center;">57.94</td>        <td style="text-align:center;">56.97</td>        <td style="text-align:left;">60.46</td>        </tr>       </tbody>      </table>     </div>     </section>     <section id="sec-21">     <p><em>3.3.4 Combine satellite images and street view images.</em> Here, we perform the experiment based on feature fusion. We use the aforementioned fusion strategy to concatenate all the four image feature representations including HOG, Autoencoder, GIST, and multifractal spectra. As shown in Table <a class="tbl" href="#tab5">5</a>, we observe that it improves the prediction accuracy for Beijing and Shanghai, respectively.</p>     <div class="table-responsive" id="tab5">      <div class="table-caption">       <span class="table-number">Table 5:</span>       <span class="table-title">Concatenate all the image feature representations (%).</span>      </div>      <table class="table">       <thead>        <tr>        <th style="text-align:center;">City</th>        <th style="text-align:left;">concatenate all the image features</th>        </tr>       </thead>       <tbody>        <tr>        <td style="text-align:center;">         <tt>Beijing</tt>        </td>        <td style="text-align:left;">64.07</td>        </tr>        <tr>        <td style="text-align:center;">         <tt>Shanghai</tt>        </td>        <td style="text-align:left;">63.33</td>        </tr>       </tbody>      </table>     </div>     </section>    </section>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Visual patterns correlated to commercial activeness</h2>     </div>    </header>    <p>Here, we aim to answer: What visual patterns are correlated with commercial activeness? What visual cues contribute to predict commercial activeness?</p>    <p>In order to find out the visual patterns that correlated to the commercial activeness, we make use of sparse representation. The goal is to make <span class="inline-equation"><span class="tex">${{\hat{y}}_i} \in R$</span>     </span> approach the ground truth <em>y<sub>i</sub>     </em> &#x2208; <em>R</em>. Then, we add a <em>l</em>     <sup>1</sup>-norm term to the optimization function as follows: <div class="table-responsive" id="eq4">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} {{\bf \tilde{w}}} = \mathop {\arg \min }\limits _{{\bf w}} (\lambda {\left| {{\bf w}} \right|_1} + \frac{1}{2}\left| {{y_i} - {{\hat{y}}_i}} \right|_2^2) \end{eqnarray} </span>      <br/>      <span class="equation-number">(4)</span>     </div>     </div> where <strong>w</strong> is the weight vectors, <span class="inline-equation"><span class="tex">${{{\bf \tilde{w}}}}$</span>     </span> the optimal weight vectors, and <em>&#x03BB;</em> the Lagrange multiplier. Specifically, sufficiently large <em>&#x03BB;</em> makes some of the coefficients in <strong>w</strong> to be zeros. Furthermore, the <em>l</em>     <sup>1</sup>-norm solution has sparse property, which is verified [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>].</p>    <p>In the experiment, we train the sparse representation model with the bag-of-features vector and the commercial activeness of each UCD as input and output, respectively, with the help of the open-source software SPAMS [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. Then, we sort the weights resulting from sparse representation in descending order, which indicates the corresponding contribution of every component in the BOF model. Note that every component in the BOF model corresponds with a cluster of a couple of image patches with similar visual features. We figure out the image patches that are closest to the centroids of the clusters possessing high weights in terms of sparse representation, which are identified as the representatives of the patterns highly relevant to commercial activeness. The visual patterns related to commercial activeness in Beijing and Shanghai are illustrated in Fig. <a class="fig" href="#fig5">5</a> and Fig. <a class="fig" href="#fig6">6</a>, respectively.    <figure id="fig5">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186581/images/www18companion-135-fig5.jpg" class="img-responsive" alt="Figure 5"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 5:</span>     <span class="figure-title">The visual patterns of satellite images related to commercial activeness. (Beijing: top two rows and Shanghai: bottom two rows).</span>     </div>    </figure> <figure id="fig6">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186581/images/www18companion-135-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 6:</span>      <span class="figure-title">The visual patterns of street view images related to commercial activeness. (Beijing: top two rows and Shanghai: bottom two rows).</span>     </div>     </figure> </p>    <p>In Fig. <a class="fig" href="#fig5">5</a>, we find that there are several semantically meaningful features such as buildings, houses, urban roads, factories, lakes, farmlands, residential areas, and barren lands. Surprisingly, these semantic meaningful objects are found only under the supervision of commercial activeness. Specifically, the objects that show strong positive impact on commercial activeness are buildings, residential areas, and sports playgrounds. Intuitively, the commercial activeness should be high near these objects, since there are many people living there. Furthermore, factories, urban roads, and houses are the objects that show medium positive impact on commercial activeness while the objects that contribute lowly to commercial activeness are lakes, farmlands, and suburban districts. It is surprising to find that the patch in row 1 and column 2 of Fig. <a class="fig" href="#fig5">5</a> is the airport in Beijing. In general, the airport is built in suburbs, where the commercial activeness should be low.</p>    <p>On the other hand, some semantically meaningful objects related to commercial activeness can be found in Fig. <a class="fig" href="#fig6">6</a>. We observe that cars, roads, pedestrians, and brands have strong positive impact on commercial activeness. Intuitively, higher commercial activeness corresponds with more cars and more people. Moreover, the objects that show medium positive impact on commercial activeness are apartments, office buildings, and small business streets while the objects that contribute lowly to commercial activeness are industrial areas and suburban districts.    </p>   </section>   <section id="sec-23">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusions</h2>     </div>    </header>    <p>It is the first effort to apply heterogeneous image data to infer the correlation between commercial hotness and visual patterns of satellite images as well as streetscapes for the sake of commercial hotness prediction. Besides, we obtain visually straightforward knowledge regarding how city infrastructures and people&#x0027;s psychological favors in terms of visual perception affect commercial hotness. We present a novel computational framework based on bag of features in the context of computer vision and sparse representation in terms of machine learning to predict commercial activeness of urban commercial districts, where we use four image features, namely, GIST, HOG, Autoencoder, and multifractal spectra for street view images and satellite images, respectively. The performance evaluation demonstrates that the predictor is promising compared with the baseline method and the visual patterns correlated to commercial activeness are revealed.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-24">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work is supported by NSFC (grant NO. 61472087) and Shanghai Science and Technology Commission (grant No. 1751110420).</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">S.&#x00A0;M. Arietta, A.&#x00A0;A. Efros, R. Ramamoorthi, and M. Agrawala. 2014. City Forensics: Using Visual Elements to Predict Non-Visual City Attributes. <em>      <em>IEEE Transactions on Visualization and Computer Graphics</em>     </em>20, 12 (Dec 2014), 2624&#x2013;2633. <a class="link-inline force-break"      href="https://doi.org/10.1109/TVCG.2014.2346446"      target="_blank">https://doi.org/10.1109/TVCG.2014.2346446</a></li>     <li id="BibPLXBIB0002" label="[2]">S. Bakhshi, P. Kanuparthy, and E. Gilbert. 2014. Demographics, Weather and Online Reviews: A Study of Restaurant Recommendations. In <em>      <em>Proceedings of the 23rd International Conference on World Wide Web</em>     </em>(<em>WWW &#x2019;14</em>). ACM, New York, NY, USA, 443&#x2013;454. <a class="link-inline force-break" href="https://doi.org/10.1145/2566486.2568021"      target="_blank">https://doi.org/10.1145/2566486.2568021</a></li>     <li id="BibPLXBIB0003" label="[3]">Ssb. Chen, S. Ma., and D. Dl.2001. Atomic decomposition by basis pursuit. <em>      <em>Siam Review</em>     </em>43, 1 (2001), 33&#x2013;61.</li>     <li id="BibPLXBIB0004" label="[4]">Y. Chen, Z. Lin, X. Zhao, G. Wang, and Y. Gu. 2014. Deep Learning-Based Classification of Hyperspectral Data. <em>      <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>     </em>7, 6 (June 2014), 2094&#x2013;2107. <a class="link-inline force-break"      href="https://doi.org/10.1109/JSTARS.2014.2329330"      target="_blank">https://doi.org/10.1109/JSTARS.2014.2329330</a></li>     <li id="BibPLXBIB0005" label="[5]">G. Csurka, C.&#x00A0;R. Dance, L. Fan, J. Willamowski, and C. Bray. 2004. Visual categorization with bags of keypoints. <em>      <em>Workshop on Statistical Learning in Computer Vision Eccv</em>     </em> (2004), 1&#x2013;22.</li>     <li id="BibPLXBIB0006" label="[6]">N. Dalal and B. Triggs. 2005. Histograms of oriented gradients for human detection. In <em>      <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#x2019;05)</em>     </em>, Vol.&#x00A0;1. 886&#x2013;893 vol. 1. <a class="link-inline force-break" href="https://doi.org/10.1109/CVPR.2005.177"      target="_blank">https://doi.org/10.1109/CVPR.2005.177</a></li>     <li id="BibPLXBIB0007" label="[7]">M. De&#x00A0;Nadai, R.&#x00A0;L. Vieriu, G. Zen, S. Dragicevic, N. Naik, M. Caraviello, C.&#x00A0;A. Hidalgo, N. Sebe, and B. Lepri. 2016. Are Safer Looking Neighborhoods More Lively?: A Multimodal Investigation into Urban Life. In <em>      <em>Proceedings of the 2016 ACM on Multimedia Conference</em>     </em>(<em>MM &#x2019;16</em>). ACM, New York, NY, USA, 1127&#x2013;1135. <a class="link-inline force-break" href="https://doi.org/10.1145/2964284.2964312"      target="_blank">https://doi.org/10.1145/2964284.2964312</a></li>     <li id="BibPLXBIB0008" label="[8]">A. Dubey, N. Naik, D. Parikh, R. Raskar, and C.&#x00A0;A. Hidalgo. 2016. Deep Learning the City: Quantifying Urban Perception at a Global Scale. In <em>      <em>Computer Vision &#x2013; ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11&#x2013;14, 2016, Proceedings, Part I</em>     </em>. Springer International Publishing, Cham, 196&#x2013;212. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-46448-0_12"      target="_blank">https://doi.org/10.1007/978-3-319-46448-0_12</a></li>     <li id="BibPLXBIB0009" label="[9]">P.&#x00A0;F. Felzenszwalb, R.&#x00A0;B. Girshick, D. McAllester, and D. Ramanan. 2010. Object Detection with Discriminatively Trained Part-Based Models. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>32, 9 (Sept 2010), 1627&#x2013;1645. <a class="link-inline force-break" href="https://doi.org/10.1109/TPAMI.2009.167"      target="_blank">https://doi.org/10.1109/TPAMI.2009.167</a></li>     <li id="BibPLXBIB0010" label="[10]">T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, E.&#x00A0;L. Aiden, and L. Fei-Fei. 2017. Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>114, 50(2017), 13108&#x2013;13113. <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.1700035114"      target="_blank">https://doi.org/10.1073/pnas.1700035114</a>arXiv:<a class="link-inline force-break" href="http://www.pnas.org/content/114/50/13108.full.pdf"      target="_blank">http://www.pnas.org/content/114/50/13108.full.pdf</a></li>     <li id="BibPLXBIB0011" label="[11]">D. Gimenez, A. Posadas, and M. Cooper. 2003. Multifractal Characterization of Soil Pore Shapes. <em>      <em>Soil Science Society of America Journal</em>     </em>08901, 12 (2003), 6388&#x2013;6394.</li>     <li id="BibPLXBIB0012" label="[12]">G.&#x00A0;E. Hinton and R.&#x00A0;R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. <em>      <em>Science</em>     </em>313, 5786 (2006), 504&#x2013;507. <a class="link-inline force-break" href="https://doi.org/10.1126/science.1127647"      target="_blank">https://doi.org/10.1126/science.1127647</a>arXiv:http://science.sciencemag.org/content/313/5786/504.full.pdf</li>     <li id="BibPLXBIB0013" label="[13]">M. Hu and B. Liu. 2004. Mining and Summarizing Customer Reviews. In <em>      <em>Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>(<em>KDD &#x2019;04</em>). ACM, New York, NY, USA, 168&#x2013;177. <a class="link-inline force-break" href="https://doi.org/10.1145/1014052.1014073"      target="_blank">https://doi.org/10.1145/1014052.1014073</a></li>     <li id="BibPLXBIB0014" label="[14]">N. Jean, M. Burke, M. Xie, W.&#x00A0;M. Davis, D.&#x00A0;B. Lobell, and S. Ermon. 2016. Combining satellite imagery and machine learning to predict poverty.<em>      <em>Science</em>     </em>353, 6301 (2016), 790&#x2013;794.</li>     <li id="BibPLXBIB0015" label="[15]">A. Khosla, B. An, J.&#x00A0;J. Lim, and A. Torralba. 2014. Looking Beyond the Visible Scene. In <em>      <em>2014 IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 3710&#x2013;3717. <a class="link-inline force-break" href="https://doi.org/10.1109/CVPR.2014.474"      target="_blank">https://doi.org/10.1109/CVPR.2014.474</a></li>     <li id="BibPLXBIB0016" label="[16]">J. Levy&#x00A0;Vehel, P. Mignot, and J. Berroir. 1992. Multifractals, texture, and image analysis. In <em>      <em>Computer Vision and Pattern Recognition, 1992. Proceedings CVPR &#x2019;92., 1992 IEEE Computer Society Conference on</em>     </em>. 661&#x2013;664.</li>     <li id="BibPLXBIB0017" label="[17]">R. Lopes and N. Betrouni. 2009. Fractal and multifractal analysis: A review. <em>      <em>Medical Image Analysis</em>     </em>13, 4 (2009), 634&#x2013;649.</li>     <li id="BibPLXBIB0018" label="[18]">J. Mairal, F. Bach, J. Ponce, and G. Sapiro. 2009. Online dictionary learning for sparse coding. In <em>      <em>International Conference on Machine Learning</em>     </em>. 689&#x2013;696.</li>     <li id="BibPLXBIB0019" label="[19]">N. Naik, S.&#x00A0;D. Kominers, R. Raskar, E.&#x00A0;L. Glaeser, and C.&#x00A0;A. Hidalgo. 2017. Computer vision uncovers predictors of physical urban change. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>114, 29(2017), 7571&#x2013;7576. <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.1619003114"      target="_blank">https://doi.org/10.1073/pnas.1619003114</a>arXiv:http://www.pnas.org/content/114/29/7571.full.pdf</li>     <li id="BibPLXBIB0020" label="[20]">N. Naik, J. Philipoom, R. Raskar, and C. Hidalgo. 2014. Streetscore &#x2013; Predicting the Perceived Safety of One Million Streetscapes. In <em>      <em>2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops</em>     </em>. 793&#x2013;799. <a class="link-inline force-break" href="https://doi.org/10.1109/CVPRW.2014.121"      target="_blank">https://doi.org/10.1109/CVPRW.2014.121</a></li>     <li id="BibPLXBIB0021" label="[21]">A. Oliva and A. Torralba. 2001. Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope. <em>      <em>International Journal of Computer Vision</em>     </em>42, 3 (2001), 145&#x2013;175.</li>     <li id="BibPLXBIB0022" label="[22]">V. Ordonez and T.&#x00A0;L. Berg. 2014. Learning High-Level Judgments of Urban Perception. In <em>      <em>Computer Vision &#x2013; ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI</em>     </em>. Springer International Publishing, Cham, 494&#x2013;510. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-10599-4_32"      target="_blank">https://doi.org/10.1007/978-3-319-10599-4_32</a></li>     <li id="BibPLXBIB0023" label="[23]">A.&#x00A0;P. Pentland. 1984. Fractal-based description of natural scenes.<em>      <em>IEEE Transactions on Pattern Analysis &#x0026; Machine Intelligence</em>     </em>6, 6(1984), 661&#x2013;74.</li>     <li id="BibPLXBIB0024" label="[24]">L. Porzi, S. Rota&#x00A0;Bul&#x00F2;, B. Lepri, and E. Ricci. 2015. Predicting and Understanding Urban Perception with Convolutional Neural Networks. In <em>      <em>Proceedings of the 23rd ACM International Conference on Multimedia</em>     </em>(<em>MM &#x2019;15</em>). ACM, New York, NY, USA, 139&#x2013;148. <a class="link-inline force-break" href="https://doi.org/10.1145/2733373.2806273"      target="_blank">https://doi.org/10.1145/2733373.2806273</a></li>     <li id="BibPLXBIB0025" label="[25]">D. Quercia, N.&#x00A0;K. O&#x0027;Hare, and H. Cramer. 2014. Aesthetic Capital: What Makes London Look Beautiful, Quiet, and Happy?. In <em>      <em>Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work; Social Computing</em>     </em>(<em>CSCW &#x2019;14</em>). ACM, New York, NY, USA, 945&#x2013;955. <a class="link-inline force-break" href="https://doi.org/10.1145/2531602.2531613"      target="_blank">https://doi.org/10.1145/2531602.2531613</a></li>     <li id="BibPLXBIB0026" label="[26]">P. Salesses, K. Schechtner, and C.&#x00A0;A. Hidalgo. 2013. The Collaborative Image of The City: Mapping the Inequality of Urban Perception. <em>      <em>PLOS ONE</em>     </em>8, 7 (07 2013), 1&#x2013;12. <a class="link-inline force-break"      href="https://doi.org/10.1371/journal.pone.0068400"      target="_blank">https://doi.org/10.1371/journal.pone.0068400</a></li>     <li id="BibPLXBIB0027" label="[27]">B. Scholkopf, A.&#x00A0;J. Smola, R.&#x00A0;C. Williamson, and P.&#x00A0;L. Bartlett. 2000. New support vector algorithms. <em>      <em>Neural Computation</em>     </em>12, 5 (2000), 1207&#x2013;1245.</li>     <li id="BibPLXBIB0028" label="[28]">J. Sivic and A. Zisserman. 2003. Video Google: a text retrieval approach to object matching in videos. In <em>      <em>Proceedings Ninth IEEE International Conference on Computer Vision</em>     </em>. 1470&#x2013;1477 vol.2. <a class="link-inline force-break"      href="https://doi.org/10.1109/ICCV.2003.1238663"      target="_blank">https://doi.org/10.1109/ICCV.2003.1238663</a></li>     <li id="BibPLXBIB0029" label="[29]">P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.&#x00A0;A. Manzagol. 2010. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. <em>      <em>J. Mach. Learn. Res.</em>     </em>11 (Dec. 2010), 3371&#x2013;3408. <a class="link-inline force-break"      href="http://dl.acm.org/citation.cfm?id=1756006.1953039"      target="_blank">http://dl.acm.org/citation.cfm?id=1756006.1953039</a></li>     <li id="BibPLXBIB0030" label="[30]">M. Wang, S. Yang, Y. Sun, and J. Gao. 2017. Human mobility prediction from region functions with taxi trajectories. <em>      <em>PLOS ONE</em>     </em>12, 11 (11 2017), 1&#x2013;23. <a class="link-inline force-break"      href="https://doi.org/10.1371/journal.pone.0188735"      target="_blank">https://doi.org/10.1371/journal.pone.0188735</a></li>     <li id="BibPLXBIB0031" label="[31]">J. Yuan, Y. Zheng, and X. Xie. 2012. Discovering Regions of Different Functions in a City Using Human Mobility and POIs. In <em>      <em>Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>(<em>KDD &#x2019;12</em>). ACM, New York, NY, USA, 186&#x2013;194. <a class="link-inline force-break" href="https://doi.org/10.1145/2339530.2339561"      target="_blank">https://doi.org/10.1145/2339530.2339561</a></li>     <li id="BibPLXBIB0032" label="[32]">Y. Zhong, N.&#x00A0;J. Yuan, W. Zhong, F. Zhang, and X. Xie. 2015. You Are Where You Go: Inferring Demographic Attributes from Location Check-ins. In <em>      <em>Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</em>     </em>(<em>WSDM &#x2019;15</em>). ACM, New York, NY, USA, 295&#x2013;304. <a class="link-inline force-break" href="https://doi.org/10.1145/2684822.2685287"      target="_blank">https://doi.org/10.1145/2684822.2685287</a></li>     <li id="BibPLXBIB0033" label="[33]">B. Zhou, L. Liu, A. Oliva, and A. Torralba. 2014. Recognizing City Identity via Attribute Analysis of Geo-tagged Images. In <em>      <em>Computer Vision &#x2013; ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III</em>     </em>. Springer International Publishing, Cham, 519&#x2013;534. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-10578-9_34"      target="_blank">https://doi.org/10.1007/978-3-319-10578-9_34</a></li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x002A;</sup></a>Correspondence author. The author is also with School of Computer Science, Xi&#x0027;an University of Technology, Xi&#x0027;an, China.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break"     href="http://people.csail.mit.edu/torralba/code/spatialenvelope/">http://people.csail.mit.edu/torralba/code/spatialenvelope/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://www.google.com/earth/">https://www.google.com/earth/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break"     href="http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi">http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class="link-inline force-break"     href="http://developer.baidu.com/map/viewstatic.htm">http://developer.baidu.com/map/viewstatic.htm</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class="link-inline force-break"     href="https://developers.google.com/maps/documentation/static-maps/intro?hl=zh-cn">https://developers.google.com/maps/documentation/static-maps/intro?hl=zh-cn</a>   </p>   <p id="fn7"><a href="#foot-fn7"><sup>6</sup></a><a class="link-inline force-break" href="https://www.dianping.com/">https://www.dianping.com/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18 Companion, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. <br/>ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186581">https://doi.org/10.1145/3184558.3186581</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
