<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Deep Collective Classification in Heterogeneous Information Networks</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Deep Collective Classification in Heterogeneous Information Networks</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Yizhou</span>      <span class="surName">Zhang</span>,     Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China, <a href="mailto:yizhouzhang14@fudan.edu.cn">yizhouzhang14@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Yun</span>      <span class="surName">Xiong</span>,<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>     Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China; Shanghai Institute for Advanced Communication and Data Science, Fudan University, China, <a href="mailto:yunx@fudan.edu.cn">yunx@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Xiangnan</span>      <span class="surName">Kong</span>,     Worcester Polytechnic Institute, Worcester, MA, USA, <a href="mailto:xkong@wpi.edu">xkong@wpi.edu</a>     </div>     <div class="author">     <span class="givenName">Shanshan</span>      <span class="surName">Li</span>,     School of Computer, National University of Defense Technology, China, <a href="mailto:shanshanli@nudt.edu.cn">shanshanli@nudt.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Jinhong</span>      <span class="surName">Mi</span>,     Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China, <a href="mailto:mij@fudan.edu.cn">mij@fudan.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Yangyong</span>      <span class="surName">Zhu</span>,     Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China; Shanghai Institute for Advanced Communication and Data Science, Fudan University, China, <a href="mailto:shanshanli@nudt.edu.cn">shanshanli@nudt.edu.cn</a>, <a href="mailto:yyzhu@fudan.edu.cn">yyzhu@fudan.edu.cn</a>     </div>         </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186106" target="_blank">https://doi.org/10.1145/3178876.3186106</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Collective classification has attracted considerable attention in the last decade, where the labels within a group of instances are correlated and should be inferred collectively, instead of independently. Conventional approaches on collective classification mainly focus on exploiting simple relational features (such as <em>count</em> and <em>exists</em> aggregators on neighboring nodes). However, many real-world applications involve complex dependencies among the instances, which are obscure/hidden in the networks. To capture these dependencies in collective classification, we need to go beyond simple relational features and extract deep dependencies between the instances. In this paper, we study the problem of deep collective classification in <em>Heterogeneous Information Networks</em> (HINs), which involves different types of autocorrelations, from simple to complex relations, among the instances. Different from conventional autocorrelations, which are given explicitly by the links in the network, complex autocorrelations are obscure/hidden in HINs, and should be inferred from existing links in a hierarchical order. This problem is highly challenging due to the multiple types of dependencies among the nodes and the complexity of the relational features. In this study, we proposed a deep convolutional collective classification method, called <em>GraphInception</em>, to learn the deep relational features in HINs. The proposed method can automatically generate a hierarchy of relational features with different complexities. Extensive experiments on four real-world networks demonstrate that our approach can improve the collective classification performance by considering deep relational features in HINs.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Collective Classification</small>, </span>     <span class="keyword">      <small> Graph Convolution</small>, </span>     <span class="keyword">      <small> Heterogeneous Information Networks</small>, </span>     <span class="keyword">      <small> Graph Mining</small>, </span>     <span class="keyword">      <small> Deep Learning</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yizhou Zhang, Yun Xiong, Xiangnan Kong, Shanshan Li, Jinhong Mi, and Yangyong Zhu. 2018. Deep Collective Classification in Heterogeneous Information Networks. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186106" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186106</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Collective classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] aims at exploiting the label autocorrelation among a group of inter-connected instances and predicting their class labels collectively. In many relational data, the labels of different instances can be related. For example, in bibliographic networks, the papers written by the same author are more likely to share similar topics than those written by different authors. An effective model for relational data should be able to capture the dependencies among different instances and perform classification collectively. Motivated by this challenge, collective classification has been extensively studied in recent years&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>].</p>    <p>Previous works on collective classification are concentrated on conventional relational models, which depend heavily on the design of relational features by the experts. On one hand, conventional relational features are usually defined as a simple aggregation of a node&#x0027;s direct neighbors, such as the average or the count of their labels. On the other hand, recent works on deep learning models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] offer automatic end-to-end feature learning in a variety of domains, such as vision&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], speech and NLP&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. However, current research on deep learning mainly focuses on content features, <em>e.g.</em>, visual features in image data. They have not yet been used to extract deep relational features in collective classification, to capture the complex autocorrelation among instances in relational learning.</p>    <p>In this paper, we study the problem of <em>deep collective classification</em> in HINs, which involves a hierarchy of different types of autocorrelations among the instances. For example, in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, we show a deep relational learning task, <em>i.e.</em>, predicting the research area (label) of the authors in <em>bibliographic networks</em>. Different authors are not only explicitly inter-connected through co-author relations, but also implicitly connected through latent relations, such as adviser-advisee, share-advisor, colleague. The dotted lines in Figure&#x00A0;<a class="fig" href="#fig1">1</a> represent implicit relationships between authors that exist in real world, but can only be inferred in the DBLP network. Although there exist previous methods on collective classification in HINs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>], deep collective classification is still an open and challenging problem due to the following reasons:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>Deep Relational Features</strong>: One major challenge of the problem is that HINs can involve a hierarchy of different types of autocorrelations, from simple to complex ones. The complex relationships are not given directly by the links in the network, but can be inferred by a hierarchy of relational features. For instance, in the DBLP network in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, there are co-author relations (simple relationships) between authors; advisor-advisee relations (hidden relationships), and share-adviser relations (complex relationships). The complex relations, <em>e.g.</em>, share-advisor, cannot be directly modeled by shallow relational features like co-author relations, but can potentially be inferred from a hierarchy of deep relational features, from simple ones (co-authors), medium ones (advisor-advisee), to complex ones (share-advisor), as shown in Figure&#x00A0;<a class="fig" href="#fig2">2</a>. For example, to infer the advisor-advisee relationships, we could find the two authors sharing similar neighboring nodes (these neighbors are most likely to be their adviser and other students in the research group). Because of the complex and obscure relationships between the instances in heterogeneous networks, we need a deep relational learning model to extract a hierarchy of deep dependencies among the instances. <figure id="fig1">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">An example of <em>simple</em> links and <em>hidden</em> links in DBLP network. Here the solid lines represent the <em>simple</em> relations exist in DBLP network, and the dotted lines represent the <em>hidden</em> relations that exist in real world but should be inferred from DBLP network, <em>i.e.</em>, co-author (blue line), adviser-advisee (pink line) and share-adviser (red line).</span>      </div>     </figure>     <br/></li>     <li id="list2" label="&#x2022;"><strong>Mixed Complexity in Relational Features</strong>: The second major challenge of the problem is the diversity of the complexity levels in the relational features. As shown in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, there are usually a mixture of both simple and complex dependencies among the instances, which can all be related to the collective classification task. In these networks, a simple relational model can only capture simple relations, but will be underfitting on complex relations. On the other hand, a conventional deep learning model with deep layers may only capture complex relations, but will be overfitting on simple relations. Therefore, an ideal model should be able to automatically balance its model complexity with respect to the mixture of complexities.<br/></li>     <li id="list3" label="&#x2022;"><strong>Heterogeneous Dependencies</strong>: The third major challenge with applying deep learning models on collective classification lies in the diversity of the node types and link types in HINs. The properties of different types of nodes (links) are very different, which makes it difficult to apply deep learning models directly. For example, graph convolution models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], assume each node in the network sharing the same convolution kernels, which is untenable in HINs. Although there exist some researches for collective classification in HINs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], however, most of them are shallow models which ignore the deep relational features in the network.<br/></li>    </ul>    <figure id="fig2">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 2:</span>     <span class="figure-title">An example of a hierarchical relations between authors in the DBLP network. Here the complex/obscure relations (above) can be inferred from the existing/simple relations (bottom).</span>     </div>    </figure>    <figure id="fig3">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 3:</span>     <span class="figure-title">The difference between the proposed method and conventional methods. We divided the algorithms according to the difficulty of the content features and the relational features. In particular, the deep learning models include highway network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0033">33</a>], inception module&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0035">35</a>], graph convolution model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>], and HNE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>].</span>     </div>    </figure>    <p>In order to address the above challenges, we present a deep graph convolutional model, called GraphInception, for collective classification in HINs. Figure&#x00A0;<a class="fig" href="#fig3">3</a> compares the difference between GraphInception and other conventional methods. Considering the diversity of the complexity levels in the relational features (some are very simple and some are complex), in order to study the relational features more efficiently, we propose the <em>graph inception module</em> to balance relational features with different complexities. This model is inspired by the inception module&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], a highly efficient deep convolutional model for CNN with fewer parameters and deeper layers.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Important Notations.</span>     </div>     <table class="table"> 		 <thead>      <tr>       <th style="text-align:right;">        <strong>Symbol</strong>       </th>       <th>        <strong>Definition</strong>       </th>      </tr> 		 </thead>      <tbody>      <tr>       <td style="text-align:right;">        <span class="inline-equation"><span class="tex">${G}=\left(\mathcal {V}, \mathcal {E}\right)$</span>        </span>       </td>       <td>A heterogeneous information network.</td>      </tr>      <tr>       <td style="text-align:right;">        <span class="inline-equation"><span class="tex">$\mathcal {V}=\lbrace \mathcal {V}_1,\cdots ,\mathcal {V}_m\rbrace$</span>        </span>       </td>       <td>The set of nodes, involving <em>m</em> types of nodes. The target node type is <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>        </span>.</td>      </tr>      <tr>       <td style="text-align:right;">        <span class="inline-equation"><span class="tex">$\mathcal {X}=\lbrace {\bf x}_1,\cdots ,{\bf x}_n\rbrace$</span>        </span>       </td>       <td>The set of local features for all instances in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>        </span>.</td>      </tr>      <tr>       <td style="text-align:right;">        <span class="inline-equation"><span class="tex">$\mathcal {Y} = \lbrace Y_1, \cdots , Y_{n}\rbrace$</span>        </span>       </td>       <td>The set of label values for all instances in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>        </span>, <em>Y<sub>i</sub>        </em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>C</em>}.</td>      </tr>      <tr>       <td style="text-align:right;">        <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>        </span> and <span class="inline-equation"><span class="tex">$\mathcal {U}$</span>        </span>       </td>       <td>The training set and test set, where <span class="inline-equation"><span class="tex">$\mathcal {L}\bigcup \mathcal {U}=\mathcal {V}_1$</span>        </span>, <span class="inline-equation"><span class="tex">$\mathcal {L}\bigcap \mathcal {U}=\emptyset$</span>        </span>.</td>      </tr>      <tr>       <td style="text-align:right;">        <span class="inline-equation"><span class="tex">$\mathcal {S}=\lbrace \mathcal {P}_1,\cdots ,\mathcal {P}_{|\mathcal {S}|}\rbrace$</span>        </span>       </td>       <td>The set of meta paths type. Each <span class="inline-equation"><span class="tex">$\mathcal {P}_l$</span>        </span> denotes a composite relationship between instances in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>        </span>.</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>X</strong>       </td>       <td>The convolutional signal of nodes, where <span class="inline-equation"><span class="tex">$\mathbf {X}\in \mathbb {R}^{n\times C}$</span>        </span>.</td>      </tr>      <tr>       <td style="text-align:right;">        <em>K</em>       </td>       <td>The convolutional kernel size.</td>      </tr>      <tr>       <td style="text-align:right;">        <em>F</em>       </td>       <td>Number of convolutional filters, <em>i.e.</em>, the hidden dimension of the filter.</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>H</strong>        <sup>        <em>t</em>        </sup> and T</td>       <td>        <strong>H</strong>        <sup>        <em>t</em>        </sup> denotes the convolution result at <em>t</em>-th layer in neural networks. T is the number of convolutional layers.</td>      </tr>     </tbody>     </table>    </div>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>     </div>    </header>    <p>     <em>Notation</em> <em>convention</em>: We use upper-case letters for matrices, bold lower-case letters for vectors and handwritten letters for sets. The operator &#x2297; is used to denote convolution operation. We summarized all notations in Table 1.</p>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Heterogeneous Information Network</h3>     </div>     </header>     <p>In many real-world applications, the networks include multiple types of nodes and links, which are called <em>heterogeneous information networks</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>].</p>     <div class="definition" id="enc1">     <Label>Definition 1.</Label>     <p> Heterogeneous information network is a special kind of information network, which can be represented as a directed graph <span class="inline-equation"><span class="tex">$G=(\mathcal {V},\mathcal {E})$</span>      </span>. <span class="inline-equation"><span class="tex">$\mathcal {V}$</span>      </span> denotes the set of nodes, including <em>m</em> types of nodes: <span class="inline-equation"><span class="tex">$\mathcal {V}_1=\lbrace v_{11}, \cdots , v_{1n_1} \rbrace , \cdots , \mathcal {V}_m=\lbrace v_{m1}, \cdots , v_{mn_m}\rbrace$</span>      </span>, where <em>v<sub>ji</sub>      </em> represents the <em>i</em>-th instance of type <em>j</em>. <span class="inline-equation"><span class="tex">$\mathcal {E}\subseteq \mathcal {V}\times \mathcal {V}$</span>      </span> denotes the links between the nodes in <span class="inline-equation"><span class="tex">$\mathcal {V}$</span>      </span>, which involves multiple types of links.</p>     </div>     <p>For example, as shown in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, the DBLP network includes three types of nodes, <em>e.g.</em>, authors, papers, conferences, which are connected through two types of links, <em>e.g.</em>, authoredBy, PublishedIn.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Collective Classification in HINs</h3>     </div>     </header>     <p>In this paper, we focus on studying the collective classification problem on one type of nodes, instead of on all of them in HINs. The reason is the label space of different types of nodes are quite different, so it&#x0027;s unreasonable to assume all types of nodes share the same set of label concepts. For instance, in movie networks, <em>e.g.</em>, IMDB&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>], the label concepts for movie genres classification task are only defined on movie nodes, instead of director nodes or actor nodes. In a specific inference task, we usually only care about the inference results on one type of nodes.</p>     <p>Without loss of generality, we suppose the first node type <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>     </span> in the HIN <em>G</em> as the type of target nodes we need to inference, and suppose we have <em>n</em> nodes in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>     </span>. On each node <span class="inline-equation"><span class="tex">$v_{1i}\in \mathcal {V}_1$</span>     </span>, we have a features vector <span class="inline-equation"><span class="tex">$\mathbf {x}_i \in \mathbb {R}^d$</span>     </span> in the <em>d</em>-dimensional space; and we also have a label variable <em>Y<sub>i</sub>     </em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>C</em>} indicating the class label assigned to node <em>v</em>     <sub>1<em>i</em>     </sub>. <span class="inline-equation"><span class="tex">$\mathcal {X} = \lbrace \mathbf {x}_1, \cdots , \mathbf {x}_{n}\rbrace$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {Y} = \lbrace Y_1, \cdots , Y_{n}\rbrace$</span>     </span> represent the set of features and the set of labels for all instances in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>     </span>.</p>     <p>The instances in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>     </span> are then divided into a training set <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>     </span> and a test set <span class="inline-equation"><span class="tex">$\mathcal {U}$</span>     </span>, where <span class="inline-equation"><span class="tex">$\mathcal {L}\bigcup \mathcal {U}=\mathcal {V}_1$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}\bigcap \mathcal {U}=\emptyset$</span>     </span>. We use <span class="inline-equation"><span class="tex">$\mathcal {Y}_{\mathcal {L}}=\lbrace Y_i|v_{1i}\in \mathcal {L}\rbrace$</span>     </span> represents the labels set of the nodes in the training set, and use <span class="inline-equation"><span class="tex">$\mathcal {Y}_{\mathcal {U}}=\lbrace Y_i|v_{1i}\in \mathcal {U}\rbrace$</span>     </span> represents the labels set of the nodes in the test set. Collective inference methods assume that the instances linked in the network are related&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. Let <span class="inline-equation"><span class="tex">$\mathcal {N}_i$</span>     </span> (<span class="inline-equation"><span class="tex">$\mathcal {N}_i\subseteq \mathcal {V}_1$</span>     </span>) represents the set of nodes associated with <em>v</em>     <sub>1<em>i</em>     </sub>, <span class="inline-equation"><span class="tex">$\mathcal {Y}_{\mathcal {N}_i} = \lbrace Y_i | v_{1i} \in \mathcal {N}_i \rbrace$</span>     </span>. Then the task of <em>collective classification in HINs</em> is to estimate: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr \left(\mathcal {Y}_{\mathcal {U}}|\mathcal {X}, \mathcal {Y}_{\mathcal {L}}\right)\propto \prod _{v_{1i}\in \mathcal {U}}Pr(Y_i|\mathbf {x}_i,\mathcal {Y}_{\mathcal {N}_i}) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div>     </p>     <p>It is challenging to learn and infer about <span class="inline-equation"><span class="tex">$Pr(Y_i|\mathbf {x}_i,\mathcal {Y}_{\mathcal {N}_i})$</span>     </span>, especially to learn the deep relational features of the nodes with complex correlations in HIN. In next section, we propose a framework to solve the problem.</p>    </section>   </section>   <section id="sec-13">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Graph Convolution-based Deep Relational Feature Learning</h2>     </div>    </header>    <p>For learning the deep relational features in HINs, we present the graph convolution-based relational feature learning model in this section. The model includes two phases: i) <em>multi-channel network translation</em>, which translates the HIN to a multi-channel network so that we can do convolution in the HIN; ii) <em>graph convolution based relational feature learning</em>, which learns the deep relational features from multi-channel networks based on graph convolution. The architecture of our method is shown in Figure&#x00A0;<a class="fig" href="#fig5">5</a>. <figure id="fig4">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Examples of different types of Meta-path between two authors in DBLP Network.</span>     </div>     </figure>     <figure id="fig5">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Architecture of the proposed GraphInception in HINs. The details of the graph inception module is shown in Figure&#x00A0;<a class="fig" href="#fig6">6</a>.</span>     </div>     </figure>    </p>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Multi-channel Network Translation</h3>     </div>     </header>     <p>There are abundant types of nodes in a HIN, while the diversities between different node types vary widely, which greatly increased the difficulty of convolution operation on the network. Usually, we only care about one type of nodes, instead of all of them in HINs. To simplify the learning curve, we propose the <em>multi-channel network</em>, each channel of which is a homogeneous network consisting of the target nodes type, and the links (relationships) are extracted from the HIN with different semantic meaning. In this subsection, we first introduce a concept named <em>meta path</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>], which is often used to extract relationships among the instances in HINs. Then we propose how to translate the HIN to a multi-channel network based on meta paths.</p>     <p>The instances in HIN are inter-connected through multiple types of links. Each type of links from node type <span class="inline-equation"><span class="tex">$\mathcal {V}_i$</span>     </span> to node type <span class="inline-equation"><span class="tex">$\mathcal {V}_j$</span>     </span> corresponds to a binary relation <em>R</em>, where <em>R</em>(<em>v<sub>ip</sub>     </em>, <em>v<sub>jq</sub>     </em>) holds if the node <em>v<sub>ip</sub>     </em> and <em>v<sub>jq</sub>     </em> are linked in <em>R</em>. For example, in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, the link type &#x201C;authoredBy&#x201D; is a relation between <em>paper</em> nodes and <em>author</em> nodes, where <em>R</em>(<em>v<sub>ip</sub>     </em>, <em>v<sub>jq</sub>     </em>) holds if the <em>paper</em> node <em>v<sub>ip</sub>     </em> has a link of type &#x201C;authoredBy&#x201D; to the <em>author</em> node <em>v<sub>jq</sub>     </em> in the network. We can write the link type as &#x201C;paper <span class="inline-equation"><span class="tex">$\xrightarrow {authoredBy}$</span>     </span> author&#x201D;. The <em>meta path</em> is defined as a sequence of relations in the network schema. For instance, a meta path &#x201C;author <span class="inline-equation"><span class="tex">$\xrightarrow {authoredBy^{-1}}$</span>     </span> paper <span class="inline-equation"><span class="tex">$\xrightarrow {authoredBy}$</span>     </span> author&#x201D; denotes a composite relationship between <em>author</em> nodes, where the semantic meaning of this meta-path is that the two authors are <em>Co-author</em>. Here the link type &#x201C;authoredBy<sup>&#x2212; 1</sup>&#x201D; represents the inverted relation of &#x201C;authoredBy&#x201D;. We give two types of meta path between <em>author</em> nodes in Figure&#x00A0;<a class="fig" href="#fig4">4</a>.</p>     <p>Each meta path defines a unique relationship between nodes, and can be used as a link type to a specific channel in the multi-channel network. For learning the dependencies among instances more effectively, we translate the HIN to the multi-channel network, where each channel of the network is connected via a certain type of meta path. Formally, given a set of meta paths <span class="inline-equation"><span class="tex">${\mathcal {S}}$</span>     </span> = <span class="inline-equation"><span class="tex">$\lbrace \mathcal {P}_1, \cdots , \mathcal {P}_{|\mathcal {S}|}\rbrace$</span>     </span>, the translated multi-channel network <em>G</em>&#x2032; is defined as: <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} G^{\prime }=\lbrace G_{\ell }^{\prime }| G_{\ell }^{\prime }=(\mathcal {V}_1,\mathcal {E}_{1{\ell }}), {\ell }=1, \cdots , |\mathcal {S}|.\rbrace \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathcal {E}_{1{\ell }}\subseteq \mathcal {V}_1\times \mathcal {V}_1$</span>     </span> denotes the links between the instances in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>     </span>, which connected through the meta path <span class="inline-equation"><span class="tex">$\mathcal {P}_{\ell }$</span>     </span>.</p>     <p>There are many ways to construct meta paths: At the beginning, meta paths were constructed manually by the experts; Afterwards, several efficient methods were presented to construct meta paths including using the breadth-first search within a limited path length&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] and greedy tree-based model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>]. In this paper, We choose the breadth-first search to construct meta paths.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Graph Convolution-based Relational Feature Learning</h3>     </div>     </header>     <p>In this subsection, we first propose the idea of learning the relational features from homogeneous networks based on graph convolution, then extend the idea into HINs.</p>     <p>In this study, we focus on relational feature learning, while conventional graph convolution models mainly focus on content feature learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. We summarized the significant differences between them as shown in Table&#x00A0;<a class="tbl" href="#tab2">2</a> and the last paragraph in this subsection. Considering the graph convolution on a homogeneous network <em>G</em>     <sub>homo</sub>, the convolution theorem defines convolutions as linear operators that diagonalize in the Fourier basis. We use the eigenvectors of the graph transition probability matrix <strong>P</strong> as Fourier basis. Then the convolution on <em>G</em>     <sub>homo</sub> is defined as the multiplication of a signal <span class="inline-equation"><span class="tex">$\mathbf {X} \in \mathbb {R}^{n\times C}$</span>     </span> (a <em>C</em>-dimensional feature vectors for each node) with a filter <em>g<sub>&#x03B8;</sub>     </em> on <strong>P</strong> in the Fourier domain, <em>i.e.</em>, <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} g_{\theta }\otimes _{G_{\text{homo}}} \mathbf {X} = g_{\theta }(\mathbf {P})\mathbf {X} = g_{\theta }(\mathbf {U} \mathbf {\Lambda } \mathbf {U}^{\top })\mathbf {X} = \mathbf {U}g_{\theta }(\mathbf {\Lambda })\mathbf {U}^{\top }\mathbf {X} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <strong>U</strong> is the eigenvector matrix of <strong>P</strong>, <strong>      <em>&#x039B;</em>     </strong> is the diagnal matrix of eigenvalues of <strong>P</strong>, <em>g<sub>&#x03B8;</sub>     </em>(<strong>      <em>&#x039B;</em>     </strong>) is the vector of Fourier coefficients, and <strong>U</strong>     <sup>&#x22A4;</sup>     <strong>X</strong> is the graph Fourier transform of <strong>X</strong>. Note that <strong>U</strong> and <strong>      <em>&#x039B;</em>     </strong> are complex matrices because <strong>P</strong> is unsymmetrical matrix. However, recent work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] has successfully applied the complex valued in traditional CNN. And we will see later, our model has nothing to do with whether <strong>U</strong> is complex matrix.</p>     <p>For selecting the local neighbors of a given node, we define <em>g<sub>&#x03B8;</sub>     </em> as a polynomial-parametric filter&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} g_{\theta }(\mathbf {\Lambda }) = \sum _{k=1}^K {\theta }_k {\mathbf {\Lambda }}^k \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where the parameter <span class="inline-equation"><span class="tex">$\theta \in \mathbb {R}^K$</span>     </span> is a vector of polynomial coefficients. Then we have: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} g_{\theta }\otimes _{G_{\text{homo}}} \mathbf {X} = \mathbf {U}\sum _{k=1}^K {\theta }_k {\mathbf {\Lambda }}^k\mathbf {U}^{\top }\mathbf {X}=\sum _{k=1}^K {\theta }_k \mathbf {P}^k \mathbf {X} \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> Note that this expression is now <em>K</em>-order neighborhood since it is a <em>K</em>-th order polynomial of the transition probability matrix, <em>i.e.</em>, it depends only on nodes that are at maximum <em>K</em> steps away from the target node.</p>     <p>Now, we extend the Eq.&#x00A0;<a class="eqn" href="#eq2">5</a> to HINs. Previously, we introduced how to translate a HIN <em>G</em> to a multi-channel network <em>G</em>&#x2032;, where each channel of the network represents a particular relationship between the nodes in <span class="inline-equation"><span class="tex">$\mathcal {V}_1$</span>     </span>. In this subsection, we learned the relational features on HIN via convolution on each channel of <em>G</em>&#x2032;, <em>i.e.</em>, <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} g_{\theta }\otimes _{G} \mathbf {X} = \left(g_{\theta _1}\otimes _{G^{\prime }_1} x,\cdots , g_{\theta _{|\mathcal {S}|}}\otimes _{G^{\prime }_{|\mathcal {S}|}} \mathbf {X}\right)\\ = \left(g_{\theta _1}(\mathbf {P}_1) \mathbf {X},\cdots , g_{\theta _{|\mathcal {S}|}}(\mathbf {P}_{|\mathcal {S}|}) \mathbf {X}\right) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathbf {P}_{\ell }~({\ell }=1,\cdots ,|\mathcal {S}|)$</span>     </span> represents the transition probability matrix of <span class="inline-equation"><span class="tex">$G^{\prime }_{\ell }$</span>     </span>. Note that we use different convolutional filters on different channels, and finally concat the convolution results. The reason is that the nodes have different neighbor nodes in each channel, thus are not suitable for convolution on all channels with one filter. Moreover, we generalize Eq.&#x00A0;<a class="eqn" href="#eq3">6</a> to <em>F</em> filters for feature maps, <em>i.e.</em>, the hidden dimension of the convolutional filter is <em>F</em>. Then we have: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} \mathbf {H} = g_{\Theta }\otimes _{G} \mathbf {X} = \left(g_{\Theta _1}(\mathbf {P}_1) \mathbf {X},\cdots , g_{\Theta _{|\mathcal {S}|}}(\mathbf {P}_{|\mathcal {S}|}) \mathbf {X}\right)\\ = r\left(\sum _{k=1}^K \mathbf {P}_1^k\mathbf {X}{\Theta }_{1k},\cdots ,\sum _{k=1}^K \mathbf {P}_{|\mathcal {S}|}^k\mathbf {X}{\Theta }_{|\mathcal {S}|k}\right) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>      <span class="inline-equation"><span class="tex">${\Theta }_{{\ell }k}\in \mathbb {R}^{C\times F}~({\ell }=1,\cdots ,|\mathcal {S}|)$</span>     </span> is now a matrix of filter parameters. A function <em>r</em> of a vector <strong>x</strong> is defined as <em>r</em>(<strong>x</strong>) = (<em>r</em>(<em>x</em>     <sub>1</sub>), &#x22C5;&#x22C5;&#x22C5;, <em>r</em>(<em>x<sub>n</sub>     </em>)), here <em>r</em>(<em>x<sub>i</sub>     </em>) = max&#x2009;(0, <em>x<sub>i</sub>     </em>) is the Relu function. The <em>i</em>-th row vector of <strong>H</strong> represents the learned relational features of the node <em>v</em>     <sub>1<em>i</em>     </sub>. <figure id="fig6">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Graph inception module. (a) the framework of the proposed module, and the red chart on the left is an enlarged view of each layer. (b) a toy example of the graph inception module in DBLP network, which generate a hierarchy of relationships among authors via graph inception (The green arrow indicates the neural network in the left parentheses).</span>      </div>     </figure>     </p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">The differences between our model and graph convolution models.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">        <strong>Graph convolution models</strong>        </th>        <th style="text-align:center;">        <strong>Our model</strong>        </th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:center;">Target problem</td>        <td style="text-align:center;">Graph classification</td>        <td style="text-align:center;">Collective classification</td>       </tr>       <tr>        <td style="text-align:center;">Convolution</td>        <td style="text-align:center;">Laplacian matrix</td>        <td style="text-align:center;">Transition probability</td>       </tr>       <tr>        <td style="text-align:center;">matrix</td>        <td style="text-align:center;">        <strong>L</strong>        </td>        <td style="text-align:center;">matrix <strong>P</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">Convolution</td>        <td style="text-align:center;">Neighbor nodes</td>        <td style="text-align:center;">Neighbor nodes</td>       </tr>       <tr>        <td style="text-align:center;">Input</td>        <td style="text-align:center;">with itself</td>        <td style="text-align:center;">only</td>       </tr>       <tr>        <td style="text-align:center;">Convolution</td>        <td style="text-align:center;">Content</td>        <td style="text-align:center;">Relational</td>       </tr>       <tr>        <td style="text-align:center;">Output</td>        <td style="text-align:center;">features</td>        <td style="text-align:center;">features</td>       </tr>       <tr>        <td style="text-align:center;">Networks</td>        <td style="text-align:center;">Homogeneous networks</td>        <td style="text-align:center;">Heterogeneous networks</td>       </tr>      </tbody>     </table>     </div>     <p>In summary, the differences between conventional graph convolution models and our model include the following aspects: 1) Conventional models concentrate on graph classification problem, while our model focus on collective classification problem; 2) Most graph convolution models are content feature learning models, therefore, they use the eigenvectors of the laplacian matrix <strong>L</strong> as the Fourier basis. Instead, our model is a relational feature learning model, so we use the eigenvectors of the transition probability matrix <strong>P</strong> as the Fourier basis; 3) Since we aim at learning the relational features, our convolutional filters donot need the attributes of the node itself, <em>i.e.</em>, do not need to consider the case of <em>k</em> = 0 in Eq.&#x00A0;<a class="eqn" href="#eq2">5</a>; 4) The convolution output of our model is relational features, and most conventional models are content features; 5) Our model can deal with HINs, while traditional graph convolution models cannot work on HINs.</p>    </section>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Graph Inception Module-based Deep Relational Feature Learning</h2>     </div>    </header>    <p>In order to balance the complexity levels of relational features, we proposed the <em>graph inception module</em>, to automatically generate a hierarchy of relational features from simple to complex features.</p>    <p>Conventional inception modules can only work on Euclidean grids data, <em>e.g.</em>, image data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], and can not be used on general graph structure, <em>e.g.</em>, network data. Therefore, we propose the <em>graph inception module</em>, in order to learn the relational features in networks more effectively. As shown in Figure&#x00A0;<a class="fig" href="#fig6">6</a>(a), graph inception module combines convolutional kernels with different size to extract relational features, and generate a hierarchy of relational features from simple to complex features through stacking such network layers. We also give a toy example in Figure&#x00A0;<a class="fig" href="#fig6">6</a>(b), which generate relationships among authors in the DBLP network, from simple to complex ones. For instance, we take two convolutional kernels for each channel on every layer, and set the kernel sizes as 1 and 2 respectively. Then the graph inception module in the <em>t</em>-th layer is defined as: <div class="table-responsive" id="eq5">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} \mathbf {C}_{{\ell }1}^t=\mathbf {P}_{\ell }\sigma (\hat{\mathbf {H}}^{t-1}){\Theta }^t_{{\ell }1}\\ \mathbf {C}_{{\ell }2}^t=\mathbf {P}_{\ell }\sigma (\hat{\mathbf {H}}^{t-1}){\Theta }^t_{{\ell }1^{\prime }}+\mathbf {P}_{\ell }^2\sigma (\hat{\mathbf {H}}^{t-1}){\Theta }^t_{{\ell }2}\\ \hat{\mathbf {H}}^t=r\left(\mathbf {C}^t_{11},\mathbf {C}^t_{12},\cdots ,\mathbf {C}^t_{|\mathcal {S}|1},\mathbf {C}^t_{|\mathcal {S}|2}\right) \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(8)</span>     </div>     </div> Here <span class="inline-equation"><span class="tex">${\mathbf {C}}_{{\ell }1}^t/\mathbf {C}_{{\ell }2}^t$</span>     </span> are the convolution kernels of size 1/2, by combining them to construct the <em>t</em>-th layer of the neural network. And <span class="inline-equation"><span class="tex">${\mathbf {C}}_{{\ell }1}^t,\mathbf {C}_{{\ell }2}^t \in \mathbb {R}^{ n \times F}$</span>     </span>. <em>&#x03C3;</em>(&#x00B7;) is a 1 &#x00D7; 1 convolutional filter for dimensionality reduction, and <span class="inline-equation"><span class="tex">$\hat{\mathbf {H}}^0=(X) \in \mathbb {R}^{1 \times n \times C}$</span>     </span>. When <em>t</em> is 0, <span class="inline-equation"><span class="tex">${\Theta }^t_{{\ell }k}\in \mathbb {R}^{ C \times F}$</span>     </span> (<em>k</em> = 1, 2), otherwise <span class="inline-equation"><span class="tex">${\Theta }^t_{{\ell }k}\in \mathbb {R}^{ F \times F}$</span>     </span>. To reduce the number of parameters, we replace <span class="inline-equation"><span class="tex">${C}_{{\ell }2}^t$</span>     </span> as: <div class="table-responsive" id="Xeq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \mathbf {C}_{{\ell }2}^t=\mathbf {P}_{\ell }^2\sigma (\hat{\mathbf {H}}^{t-1}){\Theta }^t_{{\ell }2} \end{equation} </span>      <br/>      <span class="equation-number">(9)</span>     </div>     </div> We can extract complex relational features through stacking multiple graph inception layers, and control the complexity of the learned relational features by adjusting the number of layers.</p>    <p>Compared with Eq.&#x00A0;<a class="eqn" href="#eq4">7</a>, the graph inception module-based method has three strengths: 1) It can significantly reduce the required storage space when <em>K</em> is a large number (Eq.&#x00A0;<a class="eqn" href="#eq4">7</a> need to store <em>K</em> matrices while Eq.&#x00A0;<a class="eqn" href="#eq5">8</a> only need to store <strong>P</strong> and <strong>P</strong>     <sup>2</sup>); 2) It requires fewer parameters than Eq.&#x00A0;<a class="eqn" href="#eq4">7</a> when the network is very deep; 3) It can enhance the ability of the model to extract the relational features. We intuitively expect that such a model can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions, such as social networks, citation networks, etc.</p>    <p>Suppose the graph inception module has <em>T</em> layers, then the convolution operation has complexity <span class="inline-equation"><span class="tex">$\mathcal {O}(|\mathcal {S}|\cdot C \cdot F \cdot T \cdot |\mathcal {E}|)$</span>     </span>, here <span class="inline-equation"><span class="tex">$|\mathcal {S}|$</span>     </span> is the number of the meta paths, <em>C</em> is the dimension of the input signals, and <em>F</em> is the hidden dimension of convolutional filters. As <span class="inline-equation"><span class="tex">$\mathbf {P}^k_{\ell }\mathbf {X}$</span>     </span> can be efficiently implemented as a product of a sparse matrix with a dense matrix, and <span class="inline-equation"><span class="tex">$|\mathcal {S}|,C,F,T\ll |\mathcal {E}|$</span>     </span>, the complexity is linear in the number of graph edges. <figure id="fig7">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig7.svg" class="img-responsive" alt="Figure 7"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 7:</span>      <span class="figure-title">The GraphInception 			algorithm.</span>     </div>     </figure>    </p>   </section>   <section id="sec-17">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Proposed Solution</h2>     </div>    </header>    <p>After learning the relational features for all nodes, we predict the label <span class="inline-equation"><span class="tex">$Y_i\in \mathcal {Y}_{\mathcal {U}}$</span>     </span> via a softmax layer with relational features <span class="inline-equation"><span class="tex">$\mathbf {H}^T_i$</span>     </span> and local features <strong>x</strong>     <sub>     <em>i</em>     </sub>, <em>i.e.</em>, <div class="table-responsive" id="eq6">     <div class="display-equation">      <span class="tex mytex">\begin{equation} Pr(Y_i=c|\mathcal {Y}_{\mathcal {N}_i},\mathbf {x}_i)=\text{softmax}({\bf vec}(W_{c\mathbf {H}}\mathbf {H}_i^T)+W_{c\mathbf {x}}\mathbf {x}_i+\mathbf {b}_c) \end{equation} </span>      <br/>      <span class="equation-number">(10)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$W_{c\mathbf {H}}\in \mathbb {R}^{2|\mathcal {S}|\times F}$</span>     </span> and <span class="inline-equation"><span class="tex">$W_{c\mathbf {x}}\in \mathbb {R}^d$</span>     </span> are weight matrices and <strong>b</strong>     <sub>     <em>c</em>     </sub> is a bias. <em>T</em> is the top layer of Eq.&#x00A0;<a class="eqn" href="#eq5">8</a>, and <em>c</em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>C</em>} indicates the label of nodes. Here <strong>vec</strong>(&#x00B7;) is a function which scales the input matrix into a vector. We use the labels of all nodes as the input signal <strong>X</strong> of Eq.&#x00A0;<a class="eqn" href="#eq5">8</a>, and use <strong>0</strong> to initialize the labels of the test (unknown) nodes. There are two reasons why we only use the labels instead of both labels and local features as the input signal <strong>X</strong>: 1) It can significantly reduce the number of parameters; 2) Conventional collective classification methods confirmed that there is only little association between the label of the target node and the local features of the neighboring nodes.</p>    <p>Inspired by the success of iterative classification method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], we propose an algorithm, called GraphInception, to solve the collective classification problem in HINs. The framework of GraphInception is shown in Figure&#x00A0;<a class="fig" href="#fig7">7</a>. The algorithm includes following steps:</p>    <p>     <strong>Multi-channel Network Construction:</strong>Given a HIN <em>G</em>, we first extract a meta-path set <span class="inline-equation"><span class="tex">$\mathcal {S} = \lbrace \mathcal {P}_1, \cdots , \mathcal {P}_{|\mathcal {S}|} \rbrace$</span>     </span> within the maximum path length <em>p</em>     <sub>max&#x2009;</sub>. Then we use the meta-path set to construct the multi-channel network <em>G</em>&#x2032;.</p>    <p>     <strong>Training Model:</strong>In the training step, we construct an extended training set <span class="inline-equation"><span class="tex">$\mathcal {D}=\left\lbrace (\mathbf {x}^{\prime }_i, Y_i)\right\rbrace$</span>     </span> by converting each instance <strong>x</strong>     <sub>     <em>i</em>     </sub> to <span class="inline-equation"><span class="tex">$\mathbf {x}^{\prime }_i=({\bf vec}(\mathbf {H}^T_i),\mathbf {x}_i)$</span>     </span> using local fecatures <strong>x</strong>     <sub>     <em>i</em>     </sub> and relational features learned from Eq.&#x00A0;<a class="eqn" href="#eq5">8</a>. Then we train the neural network on the extended training set based on Eq.&#x00A0;<a class="eqn" href="#eq6">10</a>.</p>    <p>     <strong>Iterative Inference:</strong>In the inference step, we iteratively update the label values of neighboring nodes based on latest predict results, and then use these new labels to make a prediction. The iterative process terminates when the convergence criteria are met. In the end, we will get <span class="inline-equation"><span class="tex">$\mathcal {Y}_{\mathcal {U}}$</span>     </span> for the test instances.</p>    <p>Our proposed GraphInception model can not only be applied to ICA framework, but can also be easily extended to other collective classification frameworks including stack learning and label propagation by replacing the traditional relational features with Eq.&#x00A0;<a class="eqn" href="#eq5">8</a>. <figure id="fig8">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 8:</span>      <span class="figure-title">Schema of datasets.</span>     </div>     </figure>    </p>   </section>   <section id="sec-18">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Related Work</h2>     </div>    </header>    <p>This paper sits at the intersection of two developed areas: collective classification and deep learning models. We provide a brief overview of related works in both fields.</p>    <p>Collective classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] of relational data, has been investigated by many researchers. Basic collective classification problems mainly focus on homogeneous network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. Ji&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] studied a specialized classification problem on HINs, where different types of nodes share a same set of label concepts. Kong et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] proposed methods based on meta-path to solve the collective classification problem on one-type nodes in HINs. Kou&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] proposed a method based on stacked model to solve collective classification problem. Choetkiertikul&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] extends the stacked model into multi-relational networks, which can be considered as one of multi-channel networks proposed in this paper. However, all above are shallow models. Nandanwar&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] proposed a deep random walk-based collective classification model, but it focuses on homogeneous networks. In this work, we chose three representative algorithms in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] as competing algorithms.</p>    <p>On the other hand, in deep learning models, there exist many related works on graph convolution recently&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. There are two strategies to define convolutional filters on graph: either from a spatial approach or from a spectral approach. By construction, spatial approaches provide filter localization via the finite size of the kernel. However, although graph convolution directly in the spatial domain is conceivable, it also faces the challenge of matching local neighborhoods, as pointed out in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. On the other hand, the convolution theorem defines convolutions as linear operators that diagonalize in the Fourier basis (usually represented by the eigenvectors of the Laplacian operator), which provide a well-defined translation operator on graphs. However, it is difficult to represent the local neighborhoods in the spectral domain. Recent works&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] tried some localized filters to overcome the problem, which inspired our algorithm. There also exist other deep models which are applied to collective classification problem. Wang&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>] proposed a deep network embedding method, which can be used to solve the classification problem on networks. Kipf&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] proposed a semi-supervised classification method based on graph convolution, while Moore&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] proposed a semi-supervised classification method based on RNN model. However, all of them focused on homogeneous networks. Chang&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] proposed a deep architecture for heterogeneous network embedding, which can be used to classified nodes in HINs. Pham&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] proposed a collective classification method on multi-relational networks based on stacked model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] and highway networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. In this work, we also chose three representative algorithms in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] as competing algorithms.</p>   </section>   <section id="sec-19">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Experiments</h2>     </div>    </header>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.1</span> Data Collection</h3>     </div>     </header>     <p>In order to validate the collective classification performances, we apply our algorithm to four real-world HINs. Note that the IMDB dataset is a multi-label dataset, and the rest are multi-class datasets. (Summarized in Table&#x00A0;<a class="tbl" href="#tab3">3</a>).</p>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Summary of datasets.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:center;">        <strong>Dataset</strong>        </th>        <th style="text-align:center;">        <strong>Node Type</strong>        </th>        <th style="text-align:center;">        <strong>Link Type</strong>        </th>        <th style="text-align:center;">        <strong>Feature</strong>        </th>        <th style="text-align:center;">        <strong>Label</strong>        </th>        <th style="text-align:center;">        <strong>Instance</strong>        </th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:center;">        <strong>DBLP</strong>        </td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">209</td>        <td style="text-align:center;">4</td>        <td style="text-align:center;">14.5K</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>IMDB</strong>        </td>        <td style="text-align:center;">4</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">9</td>        <td style="text-align:center;">18.4K</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>SLAP</strong>        </td>        <td style="text-align:center;">5</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">2695</td>        <td style="text-align:center;">15</td>        <td style="text-align:center;">20.4K</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>ACM</strong>        </td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">300</td>        <td style="text-align:center;">11</td>        <td style="text-align:center;">12.5K</td>       </tr>      </tbody>     </table>     </div>     <ul class="list-no-style">     <li id="list4" label="&#x2022;"><strong>DBLP Dataset:</strong>The first dataset, <em>i.e.</em>, DBLP four areas&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>], is a bibliographic information network extracted from DBLP<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>, which involves three types of nodes: conference, paper and author, connected by two types of relations/links: authoredBy link and publishedIn link. We treat authors as our target instances, with the research area of the authors as the instances labels. We also extract a bag-of-words representation of all the paper titles published by the author as local features, which include 209 words (terms). For detailed description of the DBLP dataset, please refer to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>], and the network schema is shown in Figure&#x00A0;<a class="fig" href="#fig8">8</a>(a).<br/></li>     <li id="list5" label="&#x2022;"><strong>IMDB Dataset:</strong>The second dataset is a movieLens dataset<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>, which contains four types of nodes: movie, director, actor and actress, connected by two types of relations/links: directed link and actor/actress staring link. The target instance type is movie instance, which assigned with a set of class labels, indicating genres of the movie. For each movie, we also extract a bag-of-words vector of all the plot summary about the movie as local features, which include 1000 words. The network schema of IMDB dataset is shown in Figure&#x00A0;<a class="fig" href="#fig8">8</a>(b). For detailed description of the IMDB dataset, please refer to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>].<br/></li>     <li id="list6" label="&#x2022;"><strong>SLAP Dataset:</strong>The third dataset is a bioinformatic dataset SLAP&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0003">3</a>]. As showed in Figure&#x00A0;<a class="fig" href="#fig8">8</a>(c), the SLAP dataset contains integrated data related to chemical compound, gene, disease, tissue, pathway etc. We treat genes as our target instances. Each gene can belong to one of the gene family. We extract 15 most frequent gene families as the instances labels, and extract 3000 gene ontology terms (GO terms) as the features of each gene instance. For detailed description of the SLAP dataset, please refer to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>].<br/></li>     <li id="list7" label="&#x2022;"><strong>ACM Conference Dataset:</strong>The last dataset is also a bibliographic information network, ACM Conference dataset<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>, and the network schema is shown in Figure&#x00A0;<a class="fig" href="#fig8">8</a>(d). This network includes 196 conference proceedings (<em>e.g.</em>, KDD&#x2019;10, KDD&#x2019;09, etc.), 12.5K papers and 17K authors. On each paper node, we extract a bag-of-words representation of the paper title and abstract to use as local features, which include 300 words. Each paper node in the network is assigned with a class label, indicating the ACM index term of the paper including 11 categories. For detailed description of the ACM Conference dataset, please refer to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>].<br/></li>     </ul>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Types of models, based on the kinds of features used.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:left;">        <strong>Method</strong>        </tdh>        <th style="text-align:center;">        <strong>Self</strong>        </th>        <th style="text-align:center;">        <strong>Neigh.</strong>        </th>        <th style="text-align:center;">        <strong>deep</strong>        </th>        <th style="text-align:center;">        <strong>HINs</strong>        </th>        <th style="text-align:center;">        <strong>Publication</strong>        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;">        <strong>attr.</strong>        </th>        <th style="text-align:center;">        <strong>labels</strong>        </th>        <th style="text-align:center;">        nets        </th>        <th style="text-align:center;"/>        <th style="text-align:center;">&#x0026;<strong>year</strong>        </th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:left;">LR</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;"/>        <td style="text-align:center;"/>        <td style="text-align:center;">&#x2014;</td>       </tr>       <tr>        <td style="text-align:left;">Highway Network</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;">Srivastava. 2015</td>       </tr>       <tr>        <td style="text-align:left;">ICA</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;"/>        <td style="text-align:center;">Sen. 2008</td>       </tr>       <tr>        <td style="text-align:left;">HCC</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">Kong. 2012</td>       </tr>       <tr>        <td style="text-align:left;">Stacked Learning</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">Choetkiertikul. 2015</td>       </tr>       <tr>        <td style="text-align:left;">CLN</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">Pham. 2017</td>       </tr>       <tr>        <td style="text-align:left;">GCN</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"/>        <td style="text-align:center;">Kipf. 2017</td>       </tr>       <tr>        <td style="text-align:left;">GCN (metapath)</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">This paper</td>       </tr>       <tr>        <td style="text-align:left;">GraphInception</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">This paper</td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Results on the DBLP, SLAP and ACM Conference datasets. &#x201C;&#x2193;&#x201D; indicates the smaller the value the better performance; &#x201C;&#x2191;&#x201D; indicates the larger the value the better performance.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">4c(a) Results (mean (rank)) on the DBLP dataset.</th>        <th style="text-align:center;" colspan="4">(b) Results (mean (rank)) on the SLAP dataset.<hr/>        </th>        <th style="text-align:center;" colspan="4">(c) Results (mean (rank)) on the ACM dataset.<hr/>        </th>        <th/>        <th/>        <th/>       </tr>       <tr>        <th style="text-align:left;">Algorithms</th>        <th style="text-align:center;" colspan="2">Evaluation Criteria<hr/>        </th>        <th style="text-align:center;">Ave.</th>        <th style="text-align:left;">Algorithms</th>        <th style="text-align:center;" colspan="2">Evaluation Criteria<hr/>        </th>        <th style="text-align:center;">Ave.</th>        <th style="text-align:left;">Algorithms</th>        <th style="text-align:center;" colspan="2">Evaluation Criteria<hr/>        </th>        <th style="text-align:center;">Ave.</th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;">Acc.&#x2191;</th>        <th style="text-align:left;">F1&#x2191;</th>        <th style="text-align:center;">Rank</th>        <th style="text-align:left;"/>        <th style="text-align:left;">Acc.&#x2191;</th>        <th style="text-align:left;">F1&#x2191;</th>        <th style="text-align:center;">Rank</th>        <th style="text-align:left;"/>        <th style="text-align:left;">Acc.&#x2191;</th>        <th style="text-align:left;">F1&#x2191;</th>        <th style="text-align:center;">Rank</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">GraphInception</td>        <td style="text-align:left;">        <strong>0.923</strong> (1)</td>        <td style="text-align:left;">        <strong>0.917</strong> (1)</td>        <td style="text-align:center;">1</td>        <td style="text-align:left;">GraphInception</td>        <td style="text-align:left;">        <strong>0.349</strong> (1)</td>        <td style="text-align:left;">        <strong>0.317</strong> (1)</td>        <td style="text-align:center;">1</td>        <td style="text-align:left;">GraphInception</td>        <td style="text-align:left;">0.733 (2)</td>        <td style="text-align:left;">        <strong>0.369</strong> (1)</td>        <td style="text-align:center;">1.5</td>       </tr>       <tr>        <td style="text-align:left;">Stacked learning</td>        <td style="text-align:left;">0.916 (2)</td>        <td style="text-align:left;">0.911 (2)</td>        <td style="text-align:center;">2</td>        <td style="text-align:left;">HCC</td>        <td style="text-align:left;">0.345 (2)</td>        <td style="text-align:left;">0.312 (2)</td>        <td style="text-align:center;">2</td>        <td style="text-align:left;">HCC</td>        <td style="text-align:left;">        <strong>0.740</strong> (1)</td>        <td style="text-align:left;">0.298 (5)</td>        <td style="text-align:center;">3</td>       </tr>       <tr>        <td style="text-align:left;">CLN</td>        <td style="text-align:left;">0.912 (3)</td>        <td style="text-align:left;">0.906 (3)</td>        <td style="text-align:center;">3</td>        <td style="text-align:left;">ICA</td>        <td style="text-align:left;">0.344 (3)</td>        <td style="text-align:left;">0.310 (3)</td>        <td style="text-align:center;">3</td>        <td style="text-align:left;">GCN (metapath)</td>        <td style="text-align:left;">0.664 (5)</td>        <td style="text-align:left;">0.366 (2)</td>        <td style="text-align:center;">3.5</td>       </tr>       <tr>        <td style="text-align:left;">HCC</td>        <td style="text-align:left;">0.906 (4)</td>        <td style="text-align:left;">0.898 (4)</td>        <td style="text-align:center;">4</td>        <td style="text-align:left;">Highway network</td>        <td style="text-align:left;">0.338 (4)</td>        <td style="text-align:left;">0.307 (4)</td>        <td style="text-align:center;">4</td>        <td style="text-align:left;">Stacked learning</td>        <td style="text-align:left;">0.724 (3)</td>        <td style="text-align:left;">0.310 (4)</td>        <td style="text-align:center;">3.5</td>       </tr>       <tr>        <td style="text-align:left;">ICA</td>        <td style="text-align:left;">0.807 (5)</td>        <td style="text-align:left;">0.799 (5)</td>        <td style="text-align:center;">5</td>        <td style="text-align:left;">LR</td>        <td style="text-align:left;">0.337 (5)</td>        <td style="text-align:left;">0.305 (5)</td>        <td style="text-align:center;">5</td>        <td style="text-align:left;">ICA</td>        <td style="text-align:left;">0.682 (4)</td>        <td style="text-align:left;">0.233 (6)</td>        <td style="text-align:center;">5</td>       </tr>       <tr>        <td style="text-align:left;">GCN</td>        <td style="text-align:left;">0.803 (6)</td>        <td style="text-align:left;">0.797 (6)</td>        <td style="text-align:center;">6</td>        <td style="text-align:left;">Stacked Learning</td>        <td style="text-align:left;">0.297 (7)</td>        <td style="text-align:left;">0.216 (6)</td>        <td style="text-align:center;">6.5</td>        <td style="text-align:left;">GCN</td>        <td style="text-align:left;">0.618 (8)</td>        <td style="text-align:left;">0.314 (3)</td>        <td style="text-align:center;">5.5</td>       </tr>       <tr>        <td style="text-align:left;">GCN (metapath)</td>        <td style="text-align:left;">0.794 (7)</td>        <td style="text-align:left;">0.788 (7)</td>        <td style="text-align:center;">7</td>        <td style="text-align:left;">GCN</td>        <td style="text-align:left;">0.263 (8)</td>        <td style="text-align:left;">0.222 (6)</td>        <td style="text-align:center;">7</td>        <td style="text-align:left;">LR</td>        <td style="text-align:left;">0.653 (6)</td>        <td style="text-align:left;">0.212 (7)</td>        <td style="text-align:center;">6.5</td>       </tr>       <tr>        <td style="text-align:left;">Highway network</td>        <td style="text-align:left;">0.781 (8)</td>        <td style="text-align:left;">0.773 (8)</td>        <td style="text-align:center;">8</td>        <td style="text-align:left;">CLN</td>        <td style="text-align:left;">0.282 (7)</td>        <td style="text-align:left;">0.191 (8)</td>        <td style="text-align:center;">7.5</td>        <td style="text-align:left;">Highway network</td>        <td style="text-align:left;">0.624 (7)</td>        <td style="text-align:left;">0.176 (8)</td>        <td style="text-align:center;">7.5</td>       </tr>       <tr>        <td style="text-align:left;">LR</td>        <td style="text-align:left;">0.776 (9)</td>        <td style="text-align:left;">0.770 (9)</td>        <td style="text-align:center;">9</td>        <td style="text-align:left;">GCN (metapath)</td>        <td style="text-align:left;">0.182 (9)</td>        <td style="text-align:left;">0.175 (9)</td>        <td style="text-align:center;">9</td>        <td style="text-align:left;">CLN</td>        <td style="text-align:left;">0.521 (9)</td>        <td style="text-align:left;">0.065 (9)</td>        <td style="text-align:center;">9</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.2</span> Compared Algorithms</h3>     </div>     </header>     <p>To demonstrate the effectiveness of our method, we compare with the following state-of-the-art algorithms (summarized in Table&#x00A0;<a class="tbl" href="#tab4">4</a>):</p>     <ul class="list-no-style">     <li id="list8" label="&#x2022;">Logistic Regression (LR for short): baseline algorithm.<br/></li>     <li id="list9" label="&#x2022;">Highway Network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0033">33</a>]: A type of neural network that uses a gating mechanism to control the information flow through a layer. Stacking multiple highway layers allow for training of deep networks. We share parameters for each layer.<br/></li>     <li id="list10" label="&#x2022;">ICA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0030">30</a>]: A basic collective classification method in homogeneous networks.<br/></li>     <li id="list11" label="&#x2022;">HCC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>]: This is a collective classification approach, which works on HIN by exploiting dependencies based on multiple meta paths in the network.<br/></li>     <li id="list12" label="&#x2022;">Stacked Learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0004">4</a>]: This is a multi-step learning procedure for collective classification. In each step, the predict labels of the neighbor nodes and the local features of the target node are fed into a standard classifier (LR) to make predictions.<br/></li>     <li id="list13" label="&#x2022;">Column Network (CLN for short)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>]: This is a deep feedforward network for collective classification in multi-relational domains, with shared parameters for each layer. It also implements the highway layer into the model.<br/></li>     <li id="list14" label="&#x2022;">GCN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>]: This is a graph convolution-based semi-supervised classification algorithm. The model extracts the 1-localized information for each node in each convolution layer, and extracts the deeper relational features through stacked multiple convolution layers. However, it focuses on homogeneous networks.<br/></li>     <li id="list15" label="&#x2022;">GCN (metapath): In order to compare with the GCN method more fairly, we extend the GCN method into HINs with meta path, as described in section&#x00A0;<a class="sec" href="#sec-14">3.1</a>.<br/></li>     <li id="list16" label="&#x2022;">GraphInception: This is our method proposed in Section&#x00A0;<a class="sec" href="#sec-17">5</a>.<br/></li>     </ul>     <p>In practice, we make use of Keras for an efficient GPU-based implementation of all algorithms. For a fair comparison, we train all models for 1500 epochs (training iterations) using RMSprop with a learning rate of 0.01. All neural nets use ReLU in the hidden layers. The maximum inference iteration <em>MAX<sub>It</sub>     </em> is 10, we also search for the number of stacked layers for stacked-based methods (CLN, stacked learning, highway network): 5,10,15,20. The maximum of the metapath length <em>p<sub>max</sub>     </em> is 4. The hidden dimension of the convolutional filters is 4 times the number of label types, <em>i.e.</em>, <em>F</em> = 4*<em>C</em>. Each inception module has two convolutional filters, and the kernel size <em>K</em> is 1 and 2 respectively. For hyper-parameter tuning, we search for the number of inception layers <em>T</em>: 1,2,3,4. 5-fold cross validation is used in all experiments, and the results are reported by the mean results of 10 runs. Code for our model can be found on Github<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a>.</p>     <div class="table-responsive" id="tab6">     <div class="table-caption">      <span class="table-number">Table 6:</span>      <span class="table-title">Results (mean (rank)) on the IMDB dataset. &#x201C;&#x2193;&#x201D; indicates the smaller the value the better performance; &#x201C;&#x2191;&#x201D; indicates the larger the value the better performance.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Algorithms</th>        <th style="text-align:center;" colspan="3">Evaluation Criteria<hr/>        </th>        <th style="text-align:center;">Ave.</th>       </tr>       <tr>        <th style="text-align:left;">(lr)2-4</th>        <th style="text-align:center;">Hamming</th>        <th style="text-align:center;">Micro F1&#x2191;</th>        <th style="text-align:center;">Subset 0/1</th>        <th style="text-align:center;">Rank</th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;"> loss&#x2193;</th>        <th style="text-align:center;"/>        <th style="text-align:center;"> loss &#x2193;</th>        <th style="text-align:center;"/>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">GraphInception</td>        <td style="text-align:center;">        <strong>0.227</strong> (1)</td>        <td style="text-align:center;">        <strong>0.551</strong> (1)</td>        <td style="text-align:center;">        <strong>0.879</strong> (1)</td>        <td style="text-align:center;">1</td>       </tr>       <tr>        <td style="text-align:left;">Stacked learning</td>        <td style="text-align:center;">0.251 (3)</td>        <td style="text-align:center;">0.533 (3)</td>        <td style="text-align:center;">0.901 (2)</td>        <td style="text-align:center;">2.67</td>       </tr>       <tr>        <td style="text-align:left;">GCN (metapath)</td>        <td style="text-align:center;">0.242 (2)</td>        <td style="text-align:center;">0.512 (5)</td>        <td style="text-align:center;">0.903 (3)</td>        <td style="text-align:center;">3.33</td>       </tr>       <tr>        <td style="text-align:left;">HCC</td>        <td style="text-align:center;">0.289 (4)</td>        <td style="text-align:center;">0.550 (2)</td>        <td style="text-align:center;">0.945 (4)</td>        <td style="text-align:center;">3.33</td>       </tr>       <tr>        <td style="text-align:left;">ICA</td>        <td style="text-align:center;">0.317 (5)</td>        <td style="text-align:center;">0.524 (4)</td>        <td style="text-align:center;">0.957 (6)</td>        <td style="text-align:center;">5</td>       </tr>       <tr>        <td style="text-align:left;">LR</td>        <td style="text-align:center;">0.341 (6)</td>        <td style="text-align:center;">0.503 (6)</td>        <td style="text-align:center;">0.967 (7)</td>        <td style="text-align:center;">6.33</td>       </tr>       <tr>        <td style="text-align:left;">CLN</td>        <td style="text-align:center;">0.406 (8)</td>        <td style="text-align:center;">0.417 (9)</td>        <td style="text-align:center;">0.954 (5)</td>        <td style="text-align:center;">7.33</td>       </tr>       <tr>        <td style="text-align:left;">GCN</td>        <td style="text-align:center;">0.388 (7)</td>        <td style="text-align:center;">0.479 (7)</td>        <td style="text-align:center;">0.968 (8)</td>        <td style="text-align:center;">7.33</td>       </tr>       <tr>        <td style="text-align:left;">Highway network</td>        <td style="text-align:center;">0.481 (9)</td>        <td style="text-align:center;">0.435 (8)</td>        <td style="text-align:center;">0.994 (9)</td>        <td style="text-align:center;">8.67</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-22">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.3</span> Multi-class Classification Performance</h3>     </div>     </header>     <p>In our first experiment, we evaluate the effectiveness of the proposed GraphInception method on three multi-class classification problem: predicting the research area for authors in DBLP network, predicting the gene family for genes in SLAP network, and predicting the ACM index for papers in ACM Conference network. The evaluation metrics include accuracy and F1-score. The results are reported in Table&#x00A0;<a class="tbl" href="#tab5">5</a>. Performance ranks of each model on each of the evaluation criteria are also listed.</p>     <p>On DBLP dataset, GraphInception works best with 4 inception layers. On SLAP dataset, GraphInception works best with 1 inception layers. On ACM Conference dataset, GraphInception works best with 4 inception layers. The first observation in the Table&#x00A0;<a class="tbl" href="#tab5">5</a> is: Almost all have better performance than the baseline logistic regression, which demonstrates that both deep learning models and collective classification models can improve classification performance. We also find that HCC, Stacked learning are significantly outperform than Highway network and ICA method. These results support that the heterogeneous dependencies among instances can improve classification performance. Although the CLN and GCN methods can capture the deep relational features in networks which use the local features as input, but they don&#x0027;t perform well on some datasets. One possible reason is that the node labels are more closely related to the labels of the neighboring nodes, rather than the features of the neighboring nodes. Compared with all above algorithms, GraphInception always has best performance in the multi-class classification task.</p>    </section>    <section id="sec-23">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.4</span> Multi-label Classification Performance</h3>     </div>     </header>     <p>In our second experiment, we evaluate the effectiveness of the proposed GraphInception method on a multi-label classification problem: predicting the film genres for movies in IMDB network. The evaluation metrics include: hamming loss, micro F1-score and subset 0/1 loss. The results are reported in Table&#x00A0;<a class="tbl" href="#tab6">6</a>. Performance ranks of each model on each of the evaluation criteria are also listed.</p>     <p>On IMDB dataset, GraphInception works best with 1 inception layers. The first observation in Table&#x00A0;<a class="tbl" href="#tab6">6</a> we have is that most collective classification methods have better performance than the methods which do not consider the correlations among instances, <em>i.e.</em>, LR and highway network. We also find that HCC, Stacked learning and GCN (with metapath) are significantly outperform than GCN and ICA method. These results support that the heterogeneous dependencies among instances can improve classification performance. Overall, GraphInception has best performance on all metrics in the multi-label classification task. <figure id="fig9">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig9.jpg" class="img-responsive" alt="Figure 9"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 9:</span>       <span class="figure-title">Visualize the hidden layer activations on DBLP dataset.</span>      </div>     </figure>     </p>    </section>    <section id="sec-24">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.5</span> Relational Features Visualization</h3>     </div>     </header>     <p>To better understand the learned relational features of our model, we use <em>t</em> &#x2212; <em>SNE</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] to visualize the hidden layer activations of Eq.&#x00A0;<a class="eqn" href="#eq5">8</a> be trained on DBLP dataset. We also compare the visualization results with other deep learning models, including: highway network, stacked learning and CLN. The results are shown in Figure&#x00A0;<a class="fig" href="#fig9">9</a>, colors denote research area class of authors. We can find our GraphInception model can effectively gather the same type nodes together (same color). The results demonstrate the effectiveness of the proposed Eq.&#x00A0;<a class="eqn" href="#eq5">8</a> on learning the deep relational features. We do not compare with GCN which utilizes both local features and neighbor nodes labels in hidden layer, while other models (including our model) only use neighbor nodes labels in hidden layer, which is unfair to other models.</p>    </section>    <section id="sec-25">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.6</span> Parameters Sensitivity</h3>     </div>     </header>     <p>There exist two essential hyper-parameters in GraphInception: the convolutional kernel size <em>K</em> and the hidden dimension of convolutional filters <em>F</em>. To test the stability of the performances of GraphInception method, we test different values of <em>K</em> and <em>F</em> on both DBLP and IMDB dataset. Similar trend holds for the other two datasets which cannot be shown due to space limitation. The results are shown in Figure&#x00A0;<a class="fig" href="#fig10">10</a> and Figure&#x00A0;<a class="fig" href="#fig11">11</a>. In Figure&#x00A0;<a class="fig" href="#fig10">10</a>, we can find that it is not sensitive to the kernel sizes on both DBLP dataset and IMDB dataset. In Figure&#x00A0;<a class="fig" href="#fig11">11</a>, we can find that more hidden filters can achieve better performance on both DBLP dataset and IMDB dataset. <figure id="fig10">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig10.jpg" class="img-responsive" alt="Figure 10"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 10:</span>       <span class="figure-title">Performances of different kernel sizes.</span>      </div>     </figure>     <figure id="fig11">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186106/images/www2018-115-fig11.jpg" class="img-responsive" alt="Figure 11"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 11:</span>       <span class="figure-title">Performances of different hidden dimension.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-26">    <header>     <div class="title-info">     <h2>      <span class="section-number">8</span> Conclusion</h2>     </div>    </header>    <p>In this paper, we proposed a graph convolution-based model for learning the deep relational features in HINs, which mainly focus on collective classification problem. We further proposed the <em>graph inception module</em> to mix both complex and simple dependencies among the instances. Empirical studies on real-world tasks demonstrate the effectiveness of the proposed GraphInception algorithm in learning deep relational features in HINs.</p>   </section>   <section id="sec-27">    <header>     <div class="title-info">     <h2>      <span class="section-number">9</span> Acknowledgments</h2>     </div>    </header>    <p>This work is supported in part by the National Natural Science Foundation of China Projects No.91546105, No.U1636207, the National Science Foundation through grand IIS-1718310, the National High Technology Research and Development Program of China No.2015AA020105, the Shanghai Science and Technology Development Fund No.16JC1400801, No.17511105502, No.17511101702, No.16511102204, Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase) under Grant No.U1501501.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">I. Alodah and J. Neville. Combining gradient boosting machines with collective inference to predict continuous values. <em>      <em>CoRR</em></em>, abs/1607.00110, 2016.</li>     <li id="BibPLXBIB0002" label="[2]">S. Chang, W. Han, J. Tang, G. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In <em>      <em>KDD</em></em>, 2015.</li>     <li id="BibPLXBIB0003" label="[3]">B. Chen, D. Ying, and D. J. Wild. Assessing drug target association using semantic linked data. <em>      <em>Plos Computational Biology</em></em>, 8(7), 2012.</li>     <li id="BibPLXBIB0004" label="[4]">M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose. Predicting delays in software projects using networked classification. In <em>      <em>ASE</em></em>, 2015.</li>     <li id="BibPLXBIB0005" label="[5]">A. Coates. Deep learning for machine vision. In <em>      <em>BMVC</em></em>, 2013.</li>     <li id="BibPLXBIB0006" label="[6]">M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In <em>      <em>NIPS</em></em>, 2016.</li>     <li id="BibPLXBIB0007" label="[7]">M. Edwards and X. Xie. Graph based convolutional neural network. <em>      <em>CoRR</em></em>, abs/1609.08965, 2016.</li>     <li id="BibPLXBIB0008" label="[8]">N. Guberman. On complex valued convolutional neural networks. <em>      <em>CoRR</em></em>, abs/1602.09046, 2016.</li>     <li id="BibPLXBIB0009" label="[9]">F. M. Harper and J. A. Konstan. The movielens datasets: History and context. <em>      <em>TiiS</em></em>, 5(4), 2016.</li>     <li id="BibPLXBIB0010" label="[10]">M. Henaff, J. Bruna, and Y. Lecun. Deep convolutional networks on graph-structured data. <em>      <em>CoRR</em></em>, abs/1506.05163, 2015.</li>     <li id="BibPLXBIB0011" label="[11]">D. Jensen, J. Neville, and B. Gallagher. Why collective inference improves relational classification. In <em>      <em>KDD</em></em>, 2004.</li>     <li id="BibPLXBIB0012" label="[12]">M. Ji, J. Han, and M. Danilevsky. Ranking-based classification of heterogeneous information networks. In <em>      <em>KDD</em></em>, 2011.</li>     <li id="BibPLXBIB0013" label="[13]">M. Ji, Y. Sun, J. H. M. Danilevsky, and J. Gao. Graph regularized transductive classification on heterogeneous information networks. In <em>      <em>PKDD</em></em>, 2010.</li>     <li id="BibPLXBIB0014" label="[14]">T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In <em>      <em>ICLR</em></em>, 2017.</li>     <li id="BibPLXBIB0015" label="[15]">X. Kong, B. Cao, and P. S. Yu. Multi-label classification by mining label and instance correlations from heterogeneous information networks. In <em>      <em>KDD</em></em>, 2013.</li>     <li id="BibPLXBIB0016" label="[16]">X. Kong, P. S. Yu, Y. Ding, and D. J. Wild. Meta path-based collective classification in heterogeneous information networks. In <em>      <em>CIKM</em></em>, 2012.</li>     <li id="BibPLXBIB0017" label="[17]">Z. Kou and W. W. Cohen. Stacked graphical models for efficient inference in markov random fields. In <em>      <em>SDM</em></em>, 2007.</li>     <li id="BibPLXBIB0018" label="[18]">V. D. M. Laurens and G. Hinton. Visualizing data using t-sne. <em>      <em>JMLR</em></em>, 9(2605), 2008.</li>     <li id="BibPLXBIB0019" label="[19]">Y. Lecun, Y. Bengio, and G. Hinton. Deep learning. <em>      <em>Nature</em></em>, 521(7553), 2015.</li>     <li id="BibPLXBIB0020" label="[20]">C. Loglisci, A. Appice, and D. Malerba. Collective regression for handling autocorrelation of network data in a transductive setting. <em>      <em>J. Intell. Inf. Syst.</em></em>, 46(3), 2015.</li>     <li id="BibPLXBIB0021" label="[21]">Q. Lu and L. Getoor. Link-based classification. In <em>      <em>ICML</em></em>, 2003.</li>     <li id="BibPLXBIB0022" label="[22]">S. Mallat. <em>      <em>A wavelet tour of signal processing (2. ed.)</em></em>. Academic Press, 1999.</li>     <li id="BibPLXBIB0023" label="[23]">L. K. Mcdowell and D. W. Aha. Labels or attributes?: rethinking the neighbors for collective classification in sparsely-labeled networks. In <em>      <em>CIKM</em></em>, 2013.</li>     <li id="BibPLXBIB0024" label="[24]">C. Meng, R. Cheng, S. Maniu, P. Senellart, and W. Zhang. Discovering meta-paths in large heterogeneous information networks. In <em>      <em>WWW</em></em>, 2015.</li>     <li id="BibPLXBIB0025" label="[25]">J. Moore and J. Neville. Deep collective inference. In <em>      <em>AAAI</em></em>, 2017.</li>     <li id="BibPLXBIB0026" label="[26]">S. Nandanwar and M. N. Murty. Structural neighborhood based classification of nodes in a network. In <em>      <em>KDD</em></em>, 2016.</li>     <li id="BibPLXBIB0027" label="[27]">J. Neville and D. Jensen. Iterative classification in relational data. In <em>      <em>AAAI</em></em>, 2000.</li>     <li id="BibPLXBIB0028" label="[28]">M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. <em>      <em>CoRR</em></em>, abs/1605.05273, 2016.</li>     <li id="BibPLXBIB0029" label="[29]">T. Pham, T. Tran, D. Q. Phung, and S. Venkatesh. Column networks for collective classification. In <em>      <em>AAAI</em></em>, 2017.</li>     <li id="BibPLXBIB0030" label="[30]">P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective classification in network data. <em>      <em>AI Magazine</em></em>, 29(3), 2008.</li>     <li id="BibPLXBIB0031" label="[31]">Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson. Structured sequence modeling with graph convolutional recurrent networks. <em>      <em>CoRR</em></em>, abs/1612.07659, 2016.</li>     <li id="BibPLXBIB0032" label="[32]">R. Socher, Y. Bengio, and C. D. Manning. Deep learning for NLP (without magic). In <em>      <em>ACL</em></em>, 2012.</li>     <li id="BibPLXBIB0033" label="[33]">R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. <em>      <em>CoRR</em></em>, abs/1505.00387, 2015.</li>     <li id="BibPLXBIB0034" label="[34]">Y. Sun and J. Han. <em>      <em>Mining Heterogeneous Information Networks: Principles and Methodologies</em></em>. Morgan &#x0026; Claypool Publishers, 2012.</li>     <li id="BibPLXBIB0035" label="[35]">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In <em>      <em>CVPR</em></em>, 2015.</li>     <li id="BibPLXBIB0036" label="[36]">D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In <em>      <em>KDD</em></em>, 2016.</li>     <li id="BibPLXBIB0037" label="[37]">Y. Zhang, Y. Xiong, X. Kong, and Y. Zhu. Netcycle:collective evolution inference in heterogeneous information networks. In <em>      <em>KDD</em></em>, 2016.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>is corresponding author</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" 	 href="http://dblp.uni-trier.de/db/">http://dblp.uni-trier.de/db/</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="http://www.imdb.com">http://www.imdb.com</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="http://dl.acm.org/">http://dl.acm.org/</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class="link-inline force-break" 	 href="https://github.com/zyz282994112/GraphInception.git">https://github.com/zyz282994112/GraphInception.git</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186106">https://doi.org/10.1145/3178876.3186106</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
