<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>MusicLynx: Exploring Music Through Artist Similarity Graphs</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">MusicLynx: Exploring Music Through Artist Similarity Graphs</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Alo</span>      <span class="surName">Allik</span>,     Queen Mary University of London, <a href="mailto:a.allik@qmul.ac.uk">a.allik@qmul.ac.uk</a>     </div>     <div class="author">     <span class="givenName">Florian</span>      <span class="surName">Thalmann</span>,     Queen Mary University of London, <a href="mailto:f.thalmann@qmul.ac.uk">f.thalmann@qmul.ac.uk</a>     </div>     <div class="author">     <span class="givenName">Mark</span>      <span class="surName">Sandler</span>,     Queen Mary University of London, <a href="mailto:mark.sandler@qmul.ac.uk">mark.sandler@qmul.ac.uk</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186970" target="_blank">https://doi.org/10.1145/3184558.3186970</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>MusicLynx is a web application for music discovery that enables users to explore an artist similarity graph constructed by linking together various open public data sources. It provides a multifaceted browsing platform that strives for an alternative, graph-based representation of artist connections to the grid-like conventions of traditional recommendation systems. Bipartite graph filtering of the Linked Data cloud, content-based music information retrieval, machine learning on crowd-sourced information and Semantic Web technologies are combined to analyze existing and create new categories of music artists through which they are connected. The categories can uncover similarities between artists who otherwise may not be immediately associated: for example, they may share ethnic background or nationality, common musical style or be signed to the same record label, come from the same geographic origin, share a fate or an affliction, or have made similar lifestyle choices. They may also prefer similar musical keys, instrumentation, rhythmic attributes, or even moods their music evokes. This demonstration is primarily meant to showcase the graph-based artist discovery interface of MusicLynx: how artists are connected through various categories, how the different graph filtering methods affect the topology and geometry of linked artists graphs, and ways in which users can connect to external services for additional content and information about objects of their interest.</small>     </p>    </div>     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small></span>     <span class="keyword"> 				 <small>music discovery and recommendation</small>, </span>     <span class="keyword">      <small> music information retrieval</small>, </span>     <span class="keyword">      <small> machine learning</small>, </span>     <span class="keyword">      <small> graph theory</small>, </span>     <span class="keyword">      <small> web programming</small>, </span>     <span class="keyword">      <small> Semantic Web</small>, </span>     <span class="keyword">      <small> Linked Data</small>, </span>     <span class="keyword">      <small> similarity modeling</small>, </span>     <span class="keyword">      <small> affective computing</small></span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Alo Allik, Florian Thalmann, and Mark Sandler. 2018. MusicLynx: Exploring Music Through Artist Similarity Graphs. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA, 4 Pages. <a href="https://doi.org/10.1145/3184558.3186970" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186970</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Recommendation and discovery</h2>     </div>    </header>    <p>Music recommendation systems, whether commercial or experimental, either based on collaborative filtering, content-based information retrieval or both, have a tendency to present suggestions to users in a linear list format without providing details about the nature of similarity. This is possibly due to the tendency towards efficiency and accuracy of recommendations in such services, in fear of overwhelming users with too much content. This may sound like a reasonable approach in a profit-minded music streaming or download environment, however, valuable information, which otherwise might enable a more enhanced and illuminating discovery of music, is concealed from users. MusicLynx - available as a web application at <a class="link-inline force-break" href="https://musiclynx.github.io">https://musiclynx.github.io</a> - is being developed as an alternative way to browse and discover music, based on different methods of modeling artist similarity. The primary method relies on analyzing Wikipedia categories<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> found at the bottom of each artist page and deriving from these meaningful connections between artists. These connections are enhanced by creating new categories based on music information retrieval data and crowd-sourced mood tag statistics. The result is a similarity graph for each artist who has an entry in one or more of the available datasets. The goal is to create a multifaceted representation of artist similarity for enhanced music exploration and discovery. The similarity graph is displayed on each artist&#x0027;s page which also includes a brief biography, links to external web resources about the artist, potentially including metadata, videos, TV and radio shows, recordings of live performances and other information. The page also features a music player that streams 30 second previews of top tracks by the artist if found in the streaming service database.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Linking data sources</h2>     </div>    </header>    <p>There are a number of openly accessible music-related knowledge bases and datasets, that crowd-source different types of information and make it available to the community. These come in many different formats and varying levels of programmatic accessibility, which makes it sometimes challenging to link them together. The linked artist dataset that was created in the process of MusicLynx development uses and makes connections between the following services:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>MusicBrainz</strong> (<a class="link-inline force-break" href="https://musicbrainz.org">https://musicbrainz.org</a>): an open music encyclopedia of music publishing metadata, primarily used in MusicLynx for global identifiers that it provides for musical entities<br/></li>     <li id="list2" label="&#x2022;"><strong>Dbpedia</strong> (<a class="link-inline force-break" href="https://dbpedia.org">https://dbpedia.org</a>): a Semantic Web triple store that contains structured information extracted from Wikipedia; in MusicLynx this enables querying and analysis of Wikipedia artist categories<br/></li>     <li id="list3" label="&#x2022;"><strong>AcousticBrainz</strong> (<a class="link-inline force-break" href="https://acousticbrainz.org">https://acousticbrainz.org</a>): a crowd-sourced publicly accessible dataset of content-based music information retrieval data, which in this context is used to build similarity models based on musical tonality, rhythm and timbre features<br/></li>     <li id="list4" label="&#x2022;"><strong>Million Song Dataset</strong> (<a class="link-inline force-break"      href="https://labrosa.ee.columbia.edu/millionsong/">https://labrosa.ee.columbia.edu/millionsong/</a>): a popular data<br/>set in the Music Information Retrieval community<br/></li>     <li id="list5" label="&#x2022;"><strong>Last.FM</strong> (<a class="link-inline force-break" href="https://www.last.fm/">https://www.last.fm/</a>): is a social music user site that collects information about users&#x2019; listening habits and musical tastes; among other information it provides access to user tags from which a mood-based similarity model has been derived for the MusicLynx artist graph<br/></li>     <li id="list6" label="&#x2022;"><strong>BBC Music</strong> (<a class="link-inline force-break" href="https://www.bbc.co.uk/music">https://www.bbc.co.uk/music</a>): BBC Music provides a page for most artists in MusicBrainz; used in the process of linking an artist MusicBrainz identifier to Dbpedia URI.<br/></li>     <li id="list7" label="&#x2022;"><strong>Sameas</strong> (<a class="link-inline force-break" href="http://sameas.org">http://sameas.org</a>): a service that finds co-references between different data sets, primary linking tool between MusicBrainz and Dbpedia.<br/></li>     <li id="list8" label="&#x2022;"><strong>Wikidata</strong> (<a class="link-inline force-break" href="https://www.wikidata.org">https://www.wikidata.org</a>): a free and open knowledge base that can be read and edited by both humans and machines. Wikidata acts as central storage for the structured data of its Wikimedia sister projects including Wikipedia, Wikivoyage, Wikisource, and others. MusicLynx API uses Wikidata as a backup linking tool between MusicBrainz and Dbpedia.<br/></li>    </ul>    <figure id="fig1">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186970/images/www18companion-210-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Linking artist identifiers from different data sources in the MusicLynx API.</span>     </div>    </figure>    <p>Figure <a class="fig" href="#fig1">1</a> highlights services where various types of data originate from and how artist and recording entities are linked by Semantic Web properties. MusicBrainz GUIDs and Dbpedia URIs are used as the primary identifiers of artists in the MusicLynx system, therefore making reliable links between these datasets is crucial. There are 3 methods that accomplish this: (1) using Sameas.org service and BBC Music artist URIs, since Sameas does not contain MusicBrainz URIs; (2) using Wikidata entity identifiers; (3) if the previous methods fail, we can try to convert the artist name into a Dbpedia URI, which is the least reliable way of achieving the connection. Listing 1 is an example of RDF representation of an artist in the MusicLynx dataset in Notation3 syntax. Concepts from the Music Ontology, Dbpedia Ontology and YAGO are combined to link the different sources into a Semantic Web fragment of music artists. The properties <strong>owl:sameAs</strong> and <strong>mo:musicbrainz_id</strong> are used to link the structured data from Dbpedia to information from any service that uses MusicBrainz identifiers, including, for example, Last.FM. AcousticBrainz feature data can be linked to the Semantic Web graph as well, even though the service does not support Semantic Web formats; however, it does use MusicBrainz identifiers for recordings. This can be achieved by employing an inverse of the property <strong>dbo:artist</strong> of class <strong>dbo:MusicalWork</strong> to associate recordings with artists. Mappings are also provided for some of the tracks in the Million Song Dataset using the generic identifier linking mechanism <strong>afo:identifier</strong> from the Audio Feature Ontology (AFO) as shown in the listing.</p>    <p>Links between data sources enable analyzing the existing categories of artists that can be queried from Dbpedia and adding new ones to the artist graphs using content-based audio features from AcousticBrainz and applying machine learning techniques to Last.FM mood tags in order to calculate 2-dimensional mood coordinates on the arousal-valence plane for each artist.</p>    <p>     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186970/images/www18companion-210-img1.jpg" class="img-responsive" alt=""      longdesc=""/>    </p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Graph filtering</h2>     </div>    </header>    <p>The Wikipedia artist categories, that can be queried from Dbpedia in the form of structured Semantic Web triples, organize artists according to style, preferred instrument, record label, geographic location or origin, ethnicity, nationality, lifestyle choices, occupation, and so on. There is a significant disparity between category degrees (i.e. number of artists that belong to a category), which implies a difference in meaningfulness of a connection between artists. For example, the connections due to the category &#x201D;Living People&#x201D; are arguably not as interesting, and thereby less meaningful, as ones made through &#x201D;Artists Who Died On Stage&#x201D;. The MusicLynx API provides a number different methods of graph ranking to facilitate filtering that, on the one hand, offer a measure of &#x201D;meaningfulness&#x201D; of links between artists, and a balance between accuracy and diversity of similarity on the other. In the paradigm of bipartite graph similarity, which means there are only connections between 2 types of nodes (in this case artists and categories), there are a number of different ways to calculate and rank similarity, primarily using the degrees of either types of nodes as similarity weights in order to boost the ranking of nodes with smaller degrees. The simplest measure of similarity is <em>global ranking</em> or, in this case, the number of categories shared between any pair of artists. Global ranking in itself is not a very accurate or diverse measure of similarity, but rather is used as a component in more complex calculations. The MusicLynx API supports a number of well-established graph-based similarity measures, including Jaccard similarity, the Sorensen index, collaborative filtering, Maximum Degree Weighted (MDW), and a hybrid of heat and probability spreading algorithms. For a detailed overview of different bipartite graph similarity methods, see [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. These measures are used to rank similar artists as well as limit the number of artists displayed to the user. Since many artists are linked to potentially thousands of other artists, methods that boost selection of meaningful links and that balance between accuracy and diversity become significant. The MDW[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] method accomplishes this task more successfully than other methods, because it uses category degrees to weight global ranking which, in effect, diminishes the contribution to the similarity of large categories like &#x201D;Living People&#x201D;. Most other measures do not take into account category degrees, with the exception of the hybrid heat-probability spreading algorithm[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. Furthermore, the similarity measure is divided by the maximum of the degrees of the artists being compared: <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} s_i,j^u,MWD = \frac{1}{max\lbrace k_i, k_j\rbrace } \sum _{\alpha =1,k_\alpha {\gt}1}^{M} \frac{a_{i,\alpha } a_{j,\alpha }}{k_{\alpha }-1} \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div>    </p>    <p>The effect of this is diminished contribution to similarity of artists who belong to many categories. This combination of artist and category degrees in such a way consequently promotes artists who belong to fewer categories with smaller memberships, which arguably increases diversity. The heat-probability spreading algorithm is very similar to MDW with the exception of dividing the weighted global ranking by the degree of either the target artist (probability spreading) or each similar artist (heat spreading). That allows a choice in the system between encouraging either accuracy or diversity.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Content-based retrieval</h2>     </div>    </header>    <p>Content-based Music Information Retrieval (MIR)[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] facilitates applications that rely on perceptual, statistical, semantic or musical features derived from audio using digital signal processing and machine learning methods. These features may include statistical aggregates computed from time-frequency representations extracted over short time windows. For instance, spectral centroid is said to correlate with the perceived brightness of a sound [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>], therefore it may be used in the characterization in timbral similarity between musical pieces. More complex representations include features that are extracted using perceptually modeled algorithms. Mel-Frequency Cepstral Coefficients (MFCCs) for instance are often used in speech recognition as well as in estimating music similarity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. Higher-level musical features include keys, chords, tempo, rhythm, as well as semantic features like genre or mood, with specific algorithms to extract this information from audio. Content-based features are increasingly used in music recommendation systems to overcome issues such as infrequent access of lesser known pieces in large music catalogues (the &#x201C;long tail&#x201D; problem) or the difficulty of recommending new pieces without user ratings in systems that employ collaborative filtering (&#x201C;cold start&#x201D; problem) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>].</p>    <p>MusicLynx is designed to support music discovery by encouraging users to engage in interesting journeys through the artist graph. The aim is to create links between artists based on stylistic elements of their music derived from a collection of recordings and add to the social, biographical and cultural categories from Dbpedia. It is not trivial to extract high-level stylistic descriptors directly from audio, however, correlations with lower level features can be detected such as the average tempo of a track, the frequency of musical event onsets, the most commonly occurring keys or chords, or overall spectral dynamics that indicate which instruments are playing or whether the audio contains a singing voice. Three main categories of audio descriptors are used to model similarity between artists in the domains of rhythm, tonality and timbre. Audio features in each category of interest are queried from the AcousticBrainz service using MusicBrainz recording identifiers. The features selected for the content-based similarity experiments include:</p>    <ul class="list-no-style">     <li id="list9" label="&#x2022;">beats-per-minute and onset rate for rhythm<br/></li>     <li id="list10" label="&#x2022;">chord histograms for tonality<br/></li>     <li id="list11" label="&#x2022;">MFCC-s for timbre<br/></li>    </ul>    <p>There are tracks by approximately 20,000 artists included in the dataset, a number arrived at after applying the constraint that requires an artist to be represented by a minimum of 10 tracks in AcousticBrainz. At the other end of the track count spectrum, there are artists who have features for well over 1,000 tracks in the database, which raises the question of how to meaningfully compare artists with such disparate representations. Two methods have been tested to solve this problem: Gaussian Mixture Models (GMM) and calculating maximum similarity between artists. The GMM method is described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. While the GMM method includes all tracks available in the dataset for each artist, thus potentially comparing feature sets of significantly different sizes, the maximum similarity method has been developed specifically to alleviate this disparity. The significant variance in the number of tracks from artist to artist, of course, reflects a general truth: some artists are more prolific than others. However, in the case of AcousticBrainz, this phenomenon is made even more complicated, since artist representation depends on the unpredictability of crowd-sourced data, particularly since the domain of music information retrieval is relatively unknown to the general public. The solution we propose here is that only the most similar tracks between two artists are included in the comparison; the number of tracks considered for each pair of artists is simply determined by which artist has fewer tracks in the dataset. The maximum similarity method stores track features for each category of each track in a separate track vector of fixed dimension <em>M</em> (i.e. number of features). Then, we define a proximity function as the squared Euclidean distance that will determine if two track vectors are close and therefore similarity between tracks.</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Crowd-sourced tag statistics</h2>     </div>    </header>    <p>While automatic feature extraction has significantly enhanced organization and categorization of large music collections, it is still rather challenging to derive high level semantic information relating to mood or genre. Complementing signal processing and machine learning methods with crowd-sourced social tagging data from platforms like Last.fm can enrich and inform understanding of general listening habits and connections between artists. Mood-based similarity is another experimental enhancement to MusicLynx. This method involves using the Semantic Web version of ILM10K music mood dataset originally created for a mood-based interactive music player - Moodplay - that consists of over 4,000 unique artists. The dataset is based on crowd-sourced mood tag statistics from Last.fm users, which have been transformed to numerical coordinates in a Cartesian space. A more detailed account of this machine learning process is outlined in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. Every track in the collection is associated to 2-dimensional coordinates reflecting energy and pleasantness respectively. The similarity between artists in this case is measured by first calculating the location of the target artist in the mood space by averaging the coordinates of all the associated tracks. The same procedure is repeated for all other artists which then enables computing Manhattan distances between the target from the rest and using the ranking as similarity metric. Each artist in the mood dataset can thereby be included in the new category &#x201D;Moodplay similar artists&#x201D; by linking to the top <em>N</em> closest artists by mood coordinates distance.</p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Building a similarity graph</h2>     </div>    </header>    <p>The similarity graph that is displayed to users is built utilizing the different graph filtering methods described above. The first step in the process of constructing a similarity graph for an artist involves sending a query to the Dbpedia SPARQL endpoint which retrieves all artists linked to the target artist, their degrees, i.e. how many categories does each artist belong to, and the global ranking or count of shared categories between the target and linked artists. In order to ensure that all categories that a given artist belongs to are presented in the interface, while at the same time curbing links to artists in larger categories, the system uses one of the filtering methods - the above-mentioned MDW by default - to boost artists and categories of smaller degrees for ranking. Even though artists typically belong to more than one category, each can only belong to one in this context, due to constraints of the graph structure. This means that artists that belong to small categories get assigned with priority and larger categories end up with smaller number of artists as a result. The preference here is given to uniqueness and diversity over accuracy. The maximum number of artists is fixed by a system parameter as it is not feasible to display graphs that exceed a certain number of nodes, which means that there is a cutoff point which is also taken into account when the graph is being calculated.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> The application</h2>     </div>    </header>    <p>The MusicLynx application consists of two components: the Angular2 front-end that serves content to users and the MusicLynx API implemented in ExpressJS which consists of modules that query the different services for data and process it. Figure <a class="fig" href="#fig2">2</a> shows a screenshot of an artist page. The application is deployed on publicly available and free infrastructure with consideration to sustainability and legacy. The front-end application is compiled into a static assembly and deployed to Github pages, which also provides a free URI for the application. The API makes use of the Heroku free server hosting plan and is also publicly accessible for querying: <a class="link-inline force-break" href="https://musiclynx-api.herokuapp.com/">https://musiclynx-api.herokuapp.com/</a>. MusicLynx is developed as an open source project. The source code is available on Github for both the front-end (<a class="link-inline force-break"     href="https://github.com/darkjazz/musiclynx/tree/static">https://github.com/darkjazz/musiclynx/tree/static</a>) and the server component (<a class="link-inline force-break"     href="https://github.com/darkjazz/musiclynx-server">https://github.com/darkjazz/musiclynx-server</a>) The API modules include SPARQL query interfaces for Dbpedia, Wikidata, and Sameas services, MusicBrainz, Youtube, and Deezer search functionality for external content, as well as SPARQL query builder, graph constructing, and graph filtering modules. AcousticBrainz and Moodplay data is provided as static datasets integrated into the API component. <figure id="fig2">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186970/images/www18companion-210-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Screenshot of an artist page on MusicLynx with the interactive artist similarity graph at the bottom.</span>     </div>     </figure>    </p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Mathieu Barthet, Gy&#x00F6;rgy Fazekas, Alo Allik, and Mark&#x00A0;B. Sandler. 2015. Moodplay: an interactive mood-based musical experience. In <em>      <em>Proceedings of the Audio Mostly 2015 on Interaction With Sound, AM &#x2019;15, Thessaloniki, Greece, October 7-9, 2015</em>     </em>. 3:1&#x2013;3:8. <a class="link-inline force-break" href="https://doi.org/10.1145/2814895.2814922"      target="_blank">https://doi.org/10.1145/2814895.2814922</a></li>     <li id="BibPLXBIB0002" label="[2]">Michael&#x00A0;A. Casey, Remco Veltkamp, Masataka Goto, Marc Leman, Christophe Rhodes, and Malcolm Slaney. 2008. Content-Based Music Information Retrieval: Current Directions and Future Challenges, Vol.&#x00A0;96. IEEE Proceedings.</li>     <li id="BibPLXBIB0003" label="[3]">&#x00D2;. Celma. 2010. <em>      <em>Music Recommendation and Discovery:The Long Tail, Long Fail, and Long Play in the Digital Music Space.</em>     </em>Springer Verlag.</li>     <li id="BibPLXBIB0004" label="[4]">A. Fiasconaro, M. Tumminello, V. Nicosia, V. Latora, and R.&#x00A0;N. Mantegna. 2015. Hybrid recommendation methods in complex networks. <em>      <em>Phys. Rev. E</em>     </em>92 (Jul 2015), 012811. Issue 1. <a class="link-inline force-break"      href="https://doi.org/10.1103/PhysRevE.92.012811"      target="_blank">https://doi.org/10.1103/PhysRevE.92.012811</a></li>     <li id="BibPLXBIB0005" label="[5]">Beth Logan. 2000. Mel Frequency Cepstral Coefficients for Music Modeling. In <em>      <em>Proc. Int. Symp. of Music Information Retrieval (ISMIR)</em>     </em>.</li>     <li id="BibPLXBIB0006" label="[6]">Linyuan L&#x00FC;, Matus Medo, Chi&#x00A0;Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao Zhou. 2012. Recommender Systems. <em>      <em>CoRR</em>     </em>abs/1202.1112(2012). <a class="link-inline force-break"      href="http://dblp.uni-trier.de/db/journals/corr/corr1202.html#abs-1202-1112"      target="_blank">http://dblp.uni-trier.de/db/journals/corr/corr1202.html#abs-1202-1112</a></li>     <li id="BibPLXBIB0007" label="[7]">Mariano Mora-Mcginity, Alo Allik, Gy&#x00F6;rgy Fazekas, and Mark&#x00A0;B. Sandler. 2016. MusicWeb: Music Discovery with Open Linked Semantic Metadata.. In <em>      <em>MTSR</em>     </em>(<em>Communications in Computer and Information Science</em>), Emmanouel Garoufallou, Imma&#x00A0;Subirats Coll, Armando Stellato, and Jane Greenberg (Eds.). Vol.&#x00A0;672. 291&#x2013;296. <a class="link-inline force-break"      href="http://dblp.uni-trier.de/db/conf/mtsr/mtsr2016.html#Mora-McginityAF16"      target="_blank">http://dblp.uni-trier.de/db/conf/mtsr/mtsr2016.html#Mora-McginityAF16</a></li>     <li id="BibPLXBIB0008" label="[8]">Emery Schubert and Joe Wolfe. 2006. Does Timbral Brightness Scale with Frequency and Spectral Centroid?<em>      <em>Acta Acustica united with Acustica</em>     </em>92, 5 (2006), 820&#x2013;825.</li>     <li id="BibPLXBIB0009" label="[9]">Tao Zhou, Zolt&#x00E1;n Kuscsik, Jian-Guo Liu, Mat&#x00FA;&#x0161; Medo, Joseph&#x00A0;Rushton Wakeling, and Yi-Cheng Zhang. 2010. Solving the apparent diversity-accuracy dilemma of recommender systems. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>107, 10(2010), 4511&#x2013;4515. <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.1000488107"      target="_blank">https://doi.org/10.1073/pnas.1000488107</a>arXiv:http://www.pnas.org/content/107/10/4511.full.pdf</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break"     href="https://en.wikipedia.org/wiki/Help:Category">https://en.wikipedia.org/wiki/Help:Category</a>   </p>   <div class="bibStrip">    <p>     <CopyrightStatement/>    </p>    <p>     <em>WWW '18 Companion, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186970">https://doi.org/10.1145/3184558.3186970</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
