<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Hierarchical Type Constrained Topic Entity Detection for Knowledge Base Question Answering</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Hierarchical Type Constrained Topic Entity Detection for Knowledge Base Question Answering</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Yunqi</span>      <span class="surName">Qiu</span>     CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences     </div>     <div class="author">     <span class="givenName">Manling</span>      <span class="surName">Li</span>     CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences     </div>     <div class="author">     <span class="givenName">Yuanzhuo</span>      <span class="surName">Wang</span>     CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences     </div>     <div class="author">     <span class="givenName">Yantao</span>      <span class="surName">Jia</span>     CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences     </div>     <div class="author">     <span class="givenName">Xiaolong</span>      <span class="surName">Jin</span>     CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, <a href="mailto:yunqi_qiu, lmlylc2013, jamaths.h@163.com, wangyuanzhuo, jinxiaolong@ict.ac.cn">yunqi_qiu, lmlylc2013, jamaths.h@163.com, wangyuanzhuo, jinxiaolong@ict.ac.cn</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186916" target="_blank">https://doi.org/10.1145/3184558.3186916</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Topic entity detection is to find out the main entity asked in a question, which is significant in question answering. Traditional methods ignore the information of entities, especially entity types and their hierarchical structures, restricting the performance. To take full advantage of Knowledge Base(KB) and detect topic entities correctly, we propose a deep neural model to leverage type hierarchy and relations of entities in KB. Experimental results demonstrate the effectiveness of the proposed method.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Question answering;</strong> <em>Information extraction;</em> &#x2022;<strong> Computing methodologies </strong>&#x2192; Knowledge representation and reasoning;</small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Question Answering; Topic Entity Detection; Hierarchical Types</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yunqi Qiu, Manling Li, Yuanzhuo Wang, Yantao Jia, and Xiaolong Jin. 2018. Hierarchical Type Constrained Topic Entity Detection for Knowledge Base Question Answering. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 3 Pages. <a href="https://doi.org/10.1145/3184558.3186916" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186916</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Knowledge Base Question Answering(KBQA) aims to answer questions by leveraging simple facts in KB. Generally, KBQA systems have two key components: (1) topic entity detection, which detects topic entities in questions and links them to KB; (2) answer selection, which identifies KB relations linked to topic entities and selects correct answers. This paper focuses on topic entity detection.<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>    </p>    <p>Previous studies identify topic entities by existing entity linkers like Freebase API using the n-grams in questions. To better measure the similarity between the entity name and the question context, a bunch of statistical features are leveraged&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. However, they only consider the string itself and ignore the question semantics and entity information in KB, such as entity types and relations, which are of great importance in topic entity detection, since the semantics of question context are always closely tied to the types and relations of topic entities. For example, the question &#x201C;Who is the CEO of Apple?&#x201D; implies the type &#x201C;<em>company</em>&#x201D; and the relation &#x201C;<em>CEO_of</em>&#x201D; for topic entity &#x201C;<em>Apple</em>&#x201D;, so the topic entity is the company Apple not the food. The effectiveness of relations has been demonstrated in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], which uses KB relations to re-rank candidate topic entities provided by &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] and achieves the state-of-the-art accuracy. However, it suffers from zero-shot problem, i.e., test entities may have unseen relations in the training data. This is caused by the huge number of relations in KB, e.g., more than 6,000 relations in a small KB Freebase2M. To solve this problem, Yu et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] breaks these relation names into 4,500 tokens, but it is still easy to find unseen tokens for test entities. As is known, types constitute a multi-layered hierarchical structure, i.e., type hierarchy. It can naturally ease the zero-shot problem, because a test entity always has a coarsest parent type (the type on the first layer of type hierarchy, e.g., person, location) and with adequate training, infrequent child types can get support from their parent types. For example, thousands of types in Freebase can be mapped to 112 hierarchical types&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>].</p>    <p>Considering that hierarchical types can not only benefit the semantic understanding of questions, but also avoid zero-shot problem, we propose Hierarchical Type constrained Topic Entity Detection (HTTED) to increase detection accuracy. It is a neural model to match questions and entities by learning the representation of question context, entity hierarchical types and entity relations.</p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> The HTTED Model</h2>     </div>    </header>    <p>The key idea of HTTED is to leverage the type hierarchy in KB by modeling the interplay of child and parent types. Given that the semantics of a parent type is closely bound to the semantics of its child types, we represent the parent type as a semantic composition of all its child types, i.e., <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} {\bf t}_{p} = {\bf t}_{c_1} \oplus {\bf t}_{c_2} \oplus \cdots \oplus {\bf t}_{c_k}, \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">${\bf t}_{p} \in \mathbb {R}^d$</span>     </span> is a parent type&#x0027;s embedding, <span class="inline-equation"><span class="tex">${\bf t}_{c_i} \in \mathbb {R}^d, i = 1, 2, \ldots , k$</span>     </span> are its child types&#x2019; embeddings, and <em>d</em> is the embedding dimension. &#x2295; is a composition operator. In this paper, we consider addition operator, i.e., the sum of vectors. This formulation enables the model to share parameters between parent and child types, so that it helps learn embeddings of child types, which suffers from dearth of training data. Due to the hierarchical structure of types, the training data for parent types is much larger than child types. For example, 30.7% entities in FB2M only have the coarsest types and are not linked to any child types. With assistance of the training data for parent types, the embeddings of child types, especially infrequent ones, will be trained more sufficiently.</p>    <p>The overall architecture of HTTED model is illustrated in Figure 1, which is comprised of three modules: (1) question context encoder; (2) entity type encoder; (3) entity relation encoder. Given a question <em>q</em> and its candidate topic entities <em>E<sub>q</sub>     </em>, HTTED learns the representations of question context, entity types and entity relations. Then, <em>E<sub>q</sub>     </em> is re-ranked in terms of the matching score between <em>q</em> and <em>e</em> &#x2208; <em>E<sub>q</sub>     </em>. <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186916/images/www18companion-156-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">The architecture of HTTED. For a given question <em>q</em> and each of its candidate topic entity <em>e</em> &#x2208; <em>E<sub>q</sub>       </em>, we generate the matching score <em>S</em>(<em>e</em>; <em>q</em>) by encoding the question <em>q</em>, the entity&#x0027;s hierarchical types <em>T<sub>e</sub>       </em> and relations <em>R<sub>e</sub>       </em>.</span>     </div>     </figure>    </p>    <p>     <strong>Question Context Encoder:</strong> Given a question <em>q</em>, we put word embeddings of question context {<strong>w</strong>     <sub>1</sub>, &#x2026;, <strong>w</strong>     <sub>     <em>n</em>     </sub>} into a deep LSTM network, where <em>n</em> is its length. The first layer is bidirectional. The left-LSTM works from <em>w</em>     <sub>1</sub> to <em>w<sub>n</sub>     </em> with the output <span class="inline-equation"><span class="tex">${\bf h}_n^{l}$</span>     </span>, while the right-LSTM works from <em>w<sub>n</sub>     </em> to <em>w</em>     <sub>1</sub> with the output <span class="inline-equation"><span class="tex">${\bf h}_n^{r}$</span>     </span>. We concatenate these outputs <span class="inline-equation"><span class="tex">$[{\bf h}_n^{l}, {\bf h}_n^{r}]$</span>     </span> and pass it through the second layer LSTM. The final output is the question context representation <span class="inline-equation"><span class="tex">${\bf q} \in \mathbb {R}^d$</span>     </span>.</p>    <p>     <strong>Entity Type Encoder:</strong> For each <em>e</em> &#x2208; <em>E<sub>q</sub>     </em>, we get its linked types <em>T<sub>e</sub>     </em> in KB. The embeddings of parent types are calculated by Eq.(<a class="eqn" href="#eq1">1</a>). A single layer LSTM network is applied on these types&#x2019; embeddings to generate entity type representation <span class="inline-equation"><span class="tex">${\bf e}_{t} \in \mathbb {R}^d$</span>     </span>.</p>    <p>     <strong>Entity Relation Encoder:</strong> For each <em>e</em> &#x2208; <em>E<sub>q</sub>     </em>, we get its linked relations <em>R<sub>e</sub>     </em> in KB, and the relation embeddings contribute to relation-level relation representations. Additionally, relation names are broken into tokens <em>RT<sub>e</sub>     </em> following&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], and the word embeddings of them are token-level relation representations. The two levels of relation representations are put into a LSTM network to get the last output as entity relation representation <span class="inline-equation"><span class="tex">${\bf e}_{r} \in \mathbb {R}^d$</span>     </span>.</p>    <p>     <strong>Matching Score:</strong> Intuitively, an entity could be represented by its relations and types. Thus, the matching score of <em>e</em> given <em>q</em> is <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} S(e; q) = \alpha \cdot \cos {({\bf e}_{r}, {\bf q})} + (1 - \alpha) \cdot \cos {({\bf e}_{t}, {\bf q})} \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>     </div> where <em>&#x03B1;</em> &#x2208; (0, 1). In training step, we maximize the margin between the positive topic entity <em>e</em>     <sup>+</sup> and other negative ones <em>e</em>     <sup>&#x2212;</sup> with a ranking loss as following, <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} L = max\lbrace 0, \gamma - S(e^{+}, q) + S(e^{-}, q) \rbrace . \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>     </div> where <em>&#x03B3;</em> is a constant parameter.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experiments and analysis</h2>     </div>    </header>    <p>We conducted experiments on a widely used single-relation KBQA dataset SimpleQuestions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] and the candidate topic entity set was the one provided by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. The KB we used was FB2M, with 112 hierarchical types introduced by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>].</p>    <p>All word embeddings were initialized with 300-d pre-trained ones proposed by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. Relation embeddings were randomly initialized, and type embeddings were initialized under the constraint of Eq.(<a class="eqn" href="#eq1">1</a>) with leaf node type embeddings randomly initialized. We set <em>&#x03B3;</em> = 0.5 and the following hyper-parameters: (1) size of hidden states for LSTMs = 300; (2) learning rate = 0.001; (3) weight <em>&#x03B1;</em> = 0.5.</p>    <p>Our evaluation metrics were Top-K accuracy (K = 1, 10, 20, 50) following&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. The baselines adopted were Freebase API, Yin et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] and Yu et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. To evaluate the effectiveness of hierarchical types, we also constructed a baseline without considering the hierarchical structure of types.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Topic Entity Detection Accuracy</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">Methods</td>       <td style="text-align:center;">Top-1</td>       <td style="text-align:center;">Top-10</td>       <td style="text-align:center;">Top-20</td>       <td>Top-50</td>      </tr>      <tr>       <td style="text-align:left;">Freebase API</td>       <td style="text-align:center;">40.9</td>       <td style="text-align:center;">64.3</td>       <td style="text-align:center;">69.7</td>       <td>75.7</td>      </tr>      <tr>       <td style="text-align:left;">Yin et al., 2016</td>       <td style="text-align:center;">72.7</td>       <td style="text-align:center;">86.9</td>       <td style="text-align:center;">88.4</td>       <td>90.2</td>      </tr>      <tr>       <td style="text-align:left;">Yu et al., 2017</td>       <td style="text-align:center;">79.0</td>       <td style="text-align:center;">89.5</td>       <td style="text-align:center;">90.9</td>       <td>92.5</td>      </tr>      <tr>       <td style="text-align:left;">        <strong>HTTED</strong>       </td>       <td style="text-align:center;">        <strong>81.1</strong>       </td>       <td style="text-align:center;">        <strong>91.7</strong>       </td>       <td style="text-align:center;">        <strong>93.4</strong>       </td>       <td>        <strong>95.1</strong>       </td>      </tr>      <tr>       <td style="text-align:left;"> &#x2212; Hierarchy</td>       <td style="text-align:center;">80.0</td>       <td style="text-align:center;">90.3</td>       <td style="text-align:center;">92.2</td>       <td>94.2</td>      </tr>     </tbody>     </table>    </div>    <p>It can be observed from Table 1 that HTTED outperforms all baselines on all evaluation metrics. (1) Compared to the state-of-the-art method, HTTED promotes Top-1 accuracy by 2.1%, and still performs well when the beam sizes are larger, i.e., with improvement 2.2% on top-10, 2.5% on top-20, 2.6% on top-50. It proves the ability of hierarchical types to help semantic understanding of questions and find correct topic entities. (2) Without type hierarchy, the model yields a performance drop, which demonstrates the effectiveness of hierarchical types in topic entity detection. With support from parent types, the child types can be trained more sufficiently, and thus the detection accuracy can be improved.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Correct predictions with hierarchical types</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:center;">Question</td>       <td>Topic Entity Ranking</td>      </tr>      <tr>       <td style="text-align:center;">What position does <strong>carlos</strong>        <strong>gomez</strong> play?</td>       <td>        <strong>carlos gomez(person.athelete)</strong>       </td>      </tr>      <tr>       <td style="text-align:center;"/>       <td>carlos gomez(person.actor)</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td>carlos gomez(person.artist)</td>      </tr>      <tr>       <td style="text-align:center;">What video game platform was <strong>ghostbusters</strong> released on?</td>       <td>        <strong>ghostbusters(product.game)</strong>       </td>      </tr>      <tr>       <td style="text-align:center;"/>       <td>ghostbusters(art.film)</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td>ghostbusters(art.music)</td>      </tr>     </tbody>     </table>    </div>    <p>It can be seen from Table 2 that with the help of hierarchical types, the entities with same names can be discerned, and the one more related to the semantics of questions can be selected, thus the correct topic entities are detected.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Conclusions</h2>     </div>    </header>    <p>In this paper, we propose a deep LSTM network HTTED for topic entity detection. Apart from relation information, we leverage hierarchical types, which could provide more constraints and enhance the performance.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-6">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work has been supported by National Natural Science Foundation of China (No. 91646120, 61572469, 61572473, 61772501 and 61402442), National Key Research and Development Program of China under grant (No.2016YFB1000902).</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale Simple Question Answering with Memory Networks. <em>      <em>CoRR</em>     </em>abs/1506.02075(2015).</li>     <li id="BibPLXBIB0002" label="[2]">Xiao Ling and Daniel&#x00A0;S. Weld. 2012. Fine-Grained Entity Recognition. <em>      <em>In Proceedings of AAAI</em>     </em>(2012).</li>     <li id="BibPLXBIB0003" label="[3]">Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation. <em>      <em>In Proceedings of the conference on EMNLP</em>     </em>(2014).</li>     <li id="BibPLXBIB0004" label="[4]">Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and Hinrich Sch&#x00FC;tze. 2016. Simple question answering by attentive convolutional neural network. <em>      <em>In Proceedings of COLING</em>     </em>(2016).</li>     <li id="BibPLXBIB0005" label="[5]">Mo Yu, Wenpeng Yin, Kazi&#x00A0;Saidul Hasan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2017. Improved Neural Relation Detection for Knowledge Base Question Answering. <em>      <em>In Proceedings of ACL</em>     </em>(2017).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>We assume that each question has only one topic entity.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186916">https://doi.org/10.1145/3184558.3186916</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
