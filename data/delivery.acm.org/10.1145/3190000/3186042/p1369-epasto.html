<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Incentive-Aware Learning for Large Markets&#x204E;&#x204E;We thank the anonymous reviewers for their helpful comments.</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Incentive-Aware Learning for Large Markets<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a>      </span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Alessandro</span>     <span class="surName">Epasto</span>,     Google Research, New York City, New York, <a href="mailto:aepasto@google.com">aepasto@google.com</a>    </div>    <div class="author">     <span class="givenName">Mohammad</span>     <span class="surName">Mahdian</span>,     Google Research, New York City, New York, <a href="mailto:mahdian@google.com">mahdian@google.com</a>    </div>    <div class="author">     <span class="givenName">Vahab</span>     <span class="surName">Mirrokni</span>,     Google Research, New York City, New York, <a href="mailto:mirrokni@google.com">mirrokni@google.com</a>    </div>    <div class="author">     <span class="givenName">Song</span>     <span class="surName">Zuo<a class="fn" href="#fn3" id="foot-fn3"><sup>&#x2020;</sup></a></span>,     Tsinghua University, Beijing, China, <a href="mailto:songzuo.z@gmail.com">songzuo.z@gmail.com</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186042" target="_blank">https://doi.org/10.1145/3178876.3186042</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>In a typical learning problem, one key step is to use training data to pick one model from a collection of models that optimizes an objective function. In many multi-agent settings, the training data is generated through the actions of the agents, and the model is used to make a decision (e.g., how to sell an item) that affects the agents. An illustrative example of this is the problem of learning the reserve price in an auction. In such cases, the agents have an incentive to influence the training data (e.g., by manipulating their bids in the case of an auction) to game the system and achieve a more favorable outcome. In this paper, we study such <em>incentive-aware learning</em> problem in a general setting and show that it is possible to approximately optimize the objective function under two assumptions: (i) each individual agent is a &#x201C;small&#x201D; (part of the market); and (ii) there is a cost associated with manipulation. For our illustrative application, this nicely translates to a mechanism for setting approximately optimal reserve prices in auctions where no individual agent has significant market share. For this application, we also show that the second assumption (that manipulations are costly) is not necessary since we can &#x201C;perturb&#x201D; any auction to make it costly for the agents to manipulate.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Theory of computation </strong>&#x2192; <strong>Algorithmic game theory and mechanism design;</strong> <em>Models of learning;</em> <em>Convergence and learning in games;</em> <em>Computational advertising theory;</em> Multi-agent learning;</small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Incentive-aware learning; learning with strategic agents; large markets; ad auctions</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, and Song Zuo. 2018. Incentive-Aware Learning for Large Markets. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186042" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186042</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Machine Learning is the science of computing a model or a hypothesis (from a fixed hypothesis space) that best describes data, where one key step is to optimize some objective function over the data. This is done using a collection of observed historical data points, called <em>training data</em>. Many of the applications of machine learning are in environments that are <em>game theoretic</em> in nature, i.e., they involve multiple self-interested agents whose actions affect the data points that the machine learning algorithm observes, and who are affected by the outcome selected by the underlying optimization algorithm. A prime example of this is automated auction marketplaces such as the online advertisements market, where the agents are bidders who can influence the observed data point through the act of bidding. In this example, optimization algorithms are often used to fine-tune various parameters in the auction (e.g., the reserve price) that have direct monetary impact on the bidders.</p>    <p>Learning in such game theoretic environments can be quite challenging: Agents are strategic and seek to optimize their own utility functions. If the outcome of the learning task has some impact on the utility of these agents, they might try to manipulate the training data to push the algorithm to make a decision to their advantage. The main challenge is to develop the underlying optimization algorithms that encourage agents to behave truthfully, resulting in a correct outcome. Learning in the presence of such strategic agents has been studied the past, however known results are either for specific estimation problems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], or mainly in the context of prior-free mechanism design [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] which does not take advantage of any training data, or with a small number of non-strategic samples&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. In this paper, we study this problem in a simple (general) model of <em>incentive-aware learning</em> for learning from a training data that strips away the learning complexity of the task to focus on the strategic component (Section&#x00A0;<a class="sec" href="#sec-6">2</a>). A motivating application for this model is the problem of learning auction parameters such as the reserve price. In our model, the goal is to select one mechanism from a (potentially infinite) family of mechanisms (called the <em>ground set</em>) that optimizes an objective function (e.g., the revenue). Our main result is that it is possible to approximately optimize the objective function in this setting under the following two assumptions:</p>    <ol class="list-no-style">    <li id="list1" label="(1)">Individual agents are &#x201C;small&#x201D;.<a class="fn" href="#fn4" id="foot-fn4"><sup>1</sup></a>     <br/></li>    <li id="list2" label="(2)">The mechanisms in the ground set are strongly incentive compatible, i.e., not only misreporting values is not better (for the agent) than reporting truthfully, but also it is worse by a specified margin.<br/></li>    </ol>    <p>A formal definition of the above properties in the general case is given in Section&#x00A0;<a class="sec" href="#sec-8">3</a> (Definition&#x00A0;<a class="enc" href="#enc3">3.2</a>). Under the above conditions, we can prove that there is an approximately incentive compatible mechanism whose value of the objective function is close to that of the best mechanism in the ground set (Theorem&#x00A0;<a class="enc" href="#enc5">3.4</a>).</p>    <p>We argue that these assumptions are reasonable in many practical settings. The first assumption holds in large markets where no agent has significant market power. For example, online advertisement marketplaces generally satisfy this. In fact, there is a growing literature in economics on market design for large markets that designs market mechanisms under similar assumptions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>].</p>    <p>The second assumption holds when inherent uncertainties exist in the environment so that a buyer (e.g., an advertiser) cannot safely misreport his data (e.g., his bids) without affecting the outcome to his detriment. For example, in a second price auction, if there is enough variance in the highest competing bid, overbidding runs the risk of winning an item at a price higher than its value, and underbidding runs the risk of not winning an item that is priced below its value. Such variance commonly exists in practice, which may, for example, come from the uncertainty in the prediction of the click-through-rates by the search engine in cost-per-click auctions, the changes of other buyers&#x2019; bidding strategies, the random throttling, etc. On the other hand, for clean theoretical models, we will prove that in the motivating application of reserve price learning for auctions, even if the mechanisms in the ground set initially do not satisfy this condition (Theorem&#x00A0;<a class="enc" href="#enc9">4.1</a>): It is possible to modify them by adding a small &#x201C;perturbation&#x201D; so that the resulting mechanisms satisfy the above condition and the amount of revenue loss due to the perturbation is small (Section&#x00A0;<a class="sec" href="#sec-13">4.1</a> and Section&#x00A0;<a class="sec" href="#sec-14">4.2</a>).</p>    <p>The algorithm we propose (Algorithm&#x00A0;1) is simple and practical: instead of picking the mechanism in the ground set that maximizes the value of the objective function (which would have been the natural thing to do in absence of incentive constraints), we use a randomized algorithm that picks a &#x201C;soft&#x201D; maximum among the mechanisms. Intuitively, this eliminates cases where a small change in the value of the objective function for a mechanism causes a sudden change in the output of the algorithm. The bulk of the work is in analyzing this mechanism and showing that it has the required incentive property and that it achieves an objective value close to the optimal.</p>    <p>    <em>Our contributions and techniques</em>. In summary, our contributions in this paper are as follows:</p>    <ul class="list-no-style">    <li id="list3" label="&#x2022;">We formalize a novel framework of <em>incentive-aware learning</em> for a major class of learning problems (Section&#x00A0;<a class="sec" href="#sec-6">2</a>).<br/></li>    <li id="list4" label="&#x2022;">Under this framework, if a certain <em>strong incentive compatibility</em>constraint is satisfied (cf. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]), we show that a general randomized optimization strategy that can &#x201C;learn&#x201D; an approximately optimal mechanism from a given ground set of truthful mechanisms satisfying the strong incentive compatible. In particular, the learning process is guaranteed to be approximately truthful and the degree of truthfulness as well as the approximate optimality becomes stronger as the buyer market power becomes weaker (Theorem&#x00A0;<a class="enc" href="#enc5">3.4</a>).<br/></li>    <li id="list5" label="&#x2022;">More specifically, we present how this learning strategy can be used in an important industry application, the reserve price learning in second price auctions (Theorem&#x00A0;<a class="enc" href="#enc9">4.1</a>). In particular, we will manually introduce the strong incentive compatibility of second price auctions with reserves by &#x201C;perturbing&#x201D; the auction a little bit (Section&#x00A0;<a class="sec" href="#sec-13">4.1</a> and Section&#x00A0;<a class="sec" href="#sec-14">4.2</a>).<br/></li>    </ul>    <p>From technical point of view, as discussed earlier, our general optimization algorithm is based on a &#x201C;soft&#x201D; maximum function that selects approximately optimal mechanisms from the ground set. In fact, such a &#x201C;soft&#x201D; maximum function stems from the exponential mechanism by McSherry and Talwar [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>]. However, we emphasize that in presence of the strongly incentive compatible property, we take a different approach for the approximately incentive compatible guaratnee. In particular, such a guarantee is stronger for the incentive-aware learning framework when the impact of one agent on the learning objective is sufficiently large (see Section&#x00A0;<a class="sec" href="#sec-15">5</a> for a concrete example). Nissim et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] show that the even stronger ex-post incentive compatible guarantee can be achieved, if sufficiently large constant misreporting costs exist for all the agents, which, however, is a quite strong assumption comparing with strong incentive compatibility adopted in this paper.</p>    <p>In order to apply our general result to the auction pricing application, we need the strong incentive compatibility, which is not automatically satisfied. To resolve this problem, we introduced some novel techniques to &#x201C;perturb&#x201D; the auctions given in the ground set. The high-level ideas therein are to remove the discontinuity and eliminate the weak dominance in the system by sacrificing a small fraction of revenue.</p>    <p>    <em>Related Work</em>. We discuss one closely related work here and more in detail in Section&#x00A0;<a class="sec" href="#sec-23">7</a>. Balcan et&#x00A0;al. studied the problem of designing digital good auctions (hence unlimited supply) via the technique of separating the market into two parts and applying the prices learned from each part to the other [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. Such a technique, in fact, creates many non-strategic samples. However, it is still limited to applications in separable environments. In particular, for single item auctions, the generalization &#x2014; (i) separating buyers into different groups, (ii) learning reserve prices from bids in each group, and (iii) applying the reserves for other groups &#x2014; can never be better than simply running a second price auction. Because the reserve price learned from each group can never be more than the highest bids within this group and applying such a reserve to other groups cannot be better than simply putting the buyers together and run a second price auction, where the second highest bid is no less than the reserve (for more details, see Example&#x00A0;<a class="enc" href="#enc13">6.1</a>).</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>    </div>    </header>    <p>We start by defining a general framework of optimizations in classic learning problems and then augment it with strategic agents who jointly hold the data points as their private types.</p>    <p>    <em>Optimization in Classic Learning</em>. The key step of a broad set of learning problems is the task of finding an (approximately) optimal function <em>h</em> from some given hypothesis space <span class="inline-equation"><span class="tex">$\mathcal {H}$</span>    </span> that maximizes some objective function (or equivalently minimizes some loss function) over a set of samples.</p>    <p>More specifically, there are <em>m</em> samples <em>d</em>    <sub>1</sub>, &#x2026;, <em>d<sub>m</sub>    </em> coming from the sample space <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>    </span> and a predefined objective function <span class="inline-equation"><span class="tex">$W: \mathcal {H}\times \mathcal {D}\rightarrow R_+$</span>    </span>. Then the goal is to find one <span class="inline-equation"><span class="tex">$h \in \mathcal {H}$</span>    </span> that maximizes (or sometimes approximately maximizes) the objective on average, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \textstyle h^* \in \mathop{argmax}_{h \in \mathcal {H}} \frac{1}{m}\sum _{j \in [m]} W(h, d_j).\end{align*} </span>      <br/>     </div>    </div> In fact, the samples are commonly assumed to be drawn from an underlying distribution, and the goal is to maximize, for example, the expected objective of the next sample(s) drawn from the same distribution. In contrast to the case of learning with non-strategic samples, the task of learning from strategic samples (meaning that samples are privately held by selfish agents) is in general even harder when there is no prior knowledge assumption on these samples. The reasons are that on one hand, the designer has less information in this case, and on the other hand, the agents have more freedom to behave strategically.</p>    <p>    <em>Learning with Strategic Agents.</em>. We augment the above classic learning framework by introducing strategic agents. As mentioned previously, in this case, the <em>m</em> samples are produced by the joint actions of <em>n</em> &#x2265; 1 different agents. In particular, each sample consists of <em>n</em> pieces that are privately held by these <em>n</em> agents, respectively. Note that this is probably the most general setting, where we don&#x0027;t enforce any specific relations between the pieces held by different agents for the same sample (e.g., these pieces could be <em>n</em> copies of the entire sample, or <em>n</em> disjoint components of the sample, etc.). Meanwhile, each agent can misreport the piece she holds arbitrarily, regardless of what other agents report to the designer.</p>    <p>To be consistent with the common mechanism design terminology, we call the pieces held by the agents <em>private types</em>, denoted by <em>&#x03B8;</em> &#x2208; <em>&#x0398;</em>. Hence in the strategic setting, each sample can be represented by a vector of reported private types <em>&#x03B8;</em> = (<em>&#x03B8;</em>    <sub>1</sub>, &#x2026;, <em>&#x03B8;<sub>n</sub>    </em>) &#x2208; <em>&#x0398;<sup>n</sup>    </em>.</p>    <p>As in a standard mechanism design setup, we assume that the designer selects a mechanism <em>M</em>: <em>&#x0398;<sup>n</sup>    </em> &#x2192; [0, 1] that maps from the vector of types reported by the agents (i.e., the representation of the sample) to an outcome <em>o</em> &#x2208; [0, 1]. This mechanism is announced publicly to all agents before asking them to report their private types. The objective <em>W</em> of the designer as well as the utility <em>u<sub>i</sub>    </em> of each agent <em>i</em> is defined based on this outcome <em>o</em>:</p>    <ul class="list-no-style">    <li id="list6" label="&#x2022;"><em>Designer objective W</em>: [0, 1] &#x2192; <em>R</em>     <sub>+</sub> is defined for each outcome;<br/></li>    <li id="list7" label="&#x2022;"><em>Agent utility u<sub>i</sub>     </em>: <em>&#x0398;</em> &#x00D7; [0, 1] &#x2192; <em>R</em>     <sub>+</sub> is defined for each pair of the private type <em>&#x03B8;<sub>i</sub>     </em> and the outcome <em>o</em>.<br/></li>    </ul>    <p>Finally, the goal of the designer in this strategic setting is to come up with an optimization algorithm that selects a mechanism <em>M</em> from a given ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span> that (approximately) maximizes the average objective, <span class="inline-equation"><span class="tex">$\bar{W}$</span>    </span>, over the <em>m</em> outcomes (each from one sample): <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \textstyle \bar{W} = \frac{1}{m}\sum _{j \in [m]} W(o_j) = \frac{1}{m}\sum _{j \in [m]} W\big (M(\hat{\theta }_{1j}, \ldots , \hat{\theta }_{nj})\big),\end{align*} </span>      <br/>     </div>    </div> where <span class="inline-equation"><span class="tex">$\hat{\theta }_{ij}$</span>    </span> is the reported type of agent <em>i</em> in sample <em>j</em> and <span class="inline-equation"><span class="tex">$M = \mathcal {A}(\hat{\theta })$</span>    </span> is the mechanism selected by the optimization algorithm <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span>. We allow <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span> to be randomized, therefore the selected mechanism can be thought of as a distribution over mechanism ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span> and the goal of the designer is to maximize the expected average objective, where the expectation is taken over the randomness of the optimization algorithm <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span> (as well as the randomness in the selected mechanism, if any).</p>    <p>Similarly, each strategic agent <em>i</em>, knowing the optimization algorithm ahead of time, aims at maximizing her own (expected) average utility, <span class="inline-equation"><span class="tex">$\bar{U}_i$</span>    </span>: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \textstyle \bar{U}_i = \frac{1}{m}\sum _{j \in [m]} u_i(\theta _{ij}, o_j) = \frac{1}{m}\sum _{j \in [m]} u_i\big (\theta _{ij}, M(\hat{\theta }_{1j}, \ldots , \hat{\theta }_{nj})\big).\end{align*} </span>      <br/>     </div>    </div> When the optimization algorithm is randomized, the expectation is taken over the randomness of <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span> and the chosen mechanism.</p>    <p>The final outcome of this learning task is evaluated at the equilibrium where each of the agents is reacting to the system according to some best response (or approximately best response) to the actions of other agents and the optimization algorithm that the designer committed to ahead of time.</p>    <p>    <em>Revelation Principle and Truthfulness</em>. For the optimization algorithm <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span>, so far we have only specified that its output should be one element of or a distribution over the ground set of mechanisms <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span>. We did not place any limitation on the type of inputs to <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span>: it can be adaptive inputs to <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span> via interactions between designer and agents, or any even more complicated schemes that <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>    </span> employs to elicit private informations from the agents.</p>    <p>Fortunately, the theory of mechanism design sheds light to how we can restrict the designing space of the optimization algorithm without loss of any generality. Observe that the optimization algorithm, in fact, can be thought as a mechanism at a higher level in this system. Then one general tool in mechanism design, the Revelation Principle [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, p.224], suggests that it is without loss of generality to restrict the designing space to direct mechanisms.</p>    <p>By applying the standard argument from mechanism design, we are able to focus on optimization algorithms directly taking the private types as inputs and satisfying the truthfulness property, which is usually termed <em>incentive compatibility</em>.</p>    <p>    <div class="definition" id="enc1">     <Label>Definition 2.1 (Incentive Compatible (IC)).</Label>     <p> An optimization algorithm <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span> is incentive compatible, if reporting her private type truthfully to <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span> is a dominant strategy of any agent <em>i</em> (regardless of other agents&#x2019; actions). Formally, the (expected) average utility of truthfully reporting her private type (vector) <em>&#x03B8;<sub>i</sub>      </em> = (<em>&#x03B8;</em>      <sub>       <em>i</em>1</sub>, &#x2026;, <em>&#x03B8;<sub>im</sub>      </em>) &#x2208; <em>&#x0398;<sup>m</sup>      </em> is no less than reporting any other type vector <span class="inline-equation"><span class="tex">$\theta ^{\prime }_i \in \Theta ^m$</span>      </span>, given any type vectors <em>&#x03B8;</em>      <sub>&#x2212; <em>i</em>      </sub> &#x2208; <em>&#x0398;</em>      <sup>(<em>n</em> &#x2212; 1) &#x00D7; <em>m</em>      </sup> reported by other agents, <div class="table-responsive" id="eq1">       <div class="display-equation">       <span class="tex mytex">\begin{align} \textstyle \frac{1}{m}\sum _{j} u_i\big (\theta _{ij}, \mathcal {A}_j(\theta _i, \theta _{-i})\big) \ge \frac{1}{m}\sum _{j} u_i\left(\theta _{ij}, \mathcal {A}_j(\theta ^{\prime }_i, \theta _{-i})\right), \end{align} </span>       <br/>       <span class="equation-number">(IC)</span>       </div>      </div> where we use <span class="inline-equation"><span class="tex">$\mathcal {A}_j(\theta)$</span>      </span> to denote the outcome on sample <em>j</em> of mechanism <em>M</em> selected by optimization algorithm <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span> on inputs <em>&#x03B8;</em>. When <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span> is randomized, expectations should be taken on both sides with respect to the randomness of <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span>.</p>    </div>    </p>    <p>We also adopt the standard notion of <em>&#x03F5;-approximate incentive compatible</em>, which means that truthfully reporting is never worse off by &#x03F5; comparing to any other reports. Formally, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \textstyle \frac{1}{m}\sum _{j \in [m]} u_i\big (\theta _{ij}, \mathcal {A}_j(\theta _i, \theta _{-i})\big) \ge \frac{1}{m}\sum _{j \in [m]} u_i\left(\theta _{ij}, \mathcal {A}_j(\theta ^{\prime }_i, \theta _{-i})\right) - \epsilon ,\end{align*} </span>      <br/>     </div>    </div> which is equivalent to incentive compatibility for &#x03F5; = 0. Moreover, outcomes achieved by an &#x03F5;-approximate incentive compatible algorithm correspond to the ones achieved at an &#x03F5;-equilibrium of some indirect mechanism by the Revelation Principle, and vise versa.</p>    <p>Finally, we require all the mechanisms in the ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span> to be incentive compatible, which can be defined through Definition&#x00A0;<a class="enc" href="#enc1">2.1</a> by thinking a mechanism <span class="inline-equation"><span class="tex">$M \in \mathcal {M}$</span>    </span> as an optimization algorithm that always selects <em>M</em> as the output.</p>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> A Specific Auction Setup</h3>     </div>    </header>    <p>As a concrete application of the general incentive-aware learning framework, we will consider the auction setting where the designer/seller (he) simultaneously sells <em>m</em> items to <em>n</em> agents/buyers (she) via <em>m</em> parallel single-item auctions. The items are heterogeneous and the buyers have additive values and quasi-linear utilities. Let the buyer <em>i</em>&#x2019;s private type <span class="inline-equation"><span class="tex">$v_{ij} = \theta _{ij} \in \Theta = \mathbb {R}_+$</span>     </span> be her value on item <em>j</em> &#x2208; [<em>m</em>]. As the standard notion for values, we will use <em>v<sub>ij</sub>     </em> for buyer&#x0027;s private type/value in the auction setup throughout the paper. Then her utility from auction <em>j</em> (the auction selling item <em>j</em>) is <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} u_{i}(v_{ij}, o_j) = x_{ij} \cdot v_{ij} - p_{ij},\end{align*} </span>       <br/>      </div>     </div> where the outcome <em>o<sub>j</sub>     </em> = &#x27E8;(<em>x</em>     <sub>1<em>j</em>     </sub>, &#x2026;, <em>x<sub>nj</sub>     </em>), (<em>p</em>     <sub>1<em>j</em>     </sub>, &#x2026;, <em>p<sub>nj</sub>     </em>)&#x27E9; consists of the allocation <em>x<sub>ij</sub>     </em> &#x2208; [0, 1] and the payment <span class="inline-equation"><span class="tex">$p_{ij} \in \mathbb {R}_+$</span>     </span> for each buyer <em>i</em> in this auction. In addition, the following feasibility constraints must be satisfied for each auction <em>j</em>, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \forall v\in \mathbb {R}^{n \times m}_+,~ \sum _{i \in [n]} x_{ij}(v) \le 1.\end{align*} </span>       <br/>      </div>     </div> For ease of notation, we abuse <em>x<sub>ij</sub>     </em> and <em>p<sub>ij</sub>     </em> by overloading as the allocation rule and the payment rule, i.e., functions mapping from reported values to allocations and payments, so that the auction (or mechanism) <em>M</em> can be simply defined by such a pair of functions.</p>    <p>For the specific application to auctions, the seller is often enforced to provide an option for each buyer to leave the auction. By leaving an auction, the buyer pays nothing and gets nothing. The combination of the special option and the <a class="eqn" href="#eq1">IC</a> constraint is called <em>individual rationality</em>, <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{align} \textstyle \frac{1}{m} \sum _{j \in [m]} u_i\big (v_{ij}, \mathcal {A}_j(v_i, v_{-i})\big) \ge 0. \end{align} </span>       <br/>       <span class="equation-number">(IR)</span>      </div>     </div> As we require each auction in <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> to be incentive compatible, we also require it to be individually rational. In particular, for the ease of presentation, we will mainly consider the set of auctions to be <em>second price auctions with anonymous reserve</em>, which are extensively studied in auction theory and widely adopted by the industry [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>].</p>    <p>We take the revenue as the seller&#x0027;s objective, i.e., the total payments collected from buyers, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle\textrm{R}{\small \rm EV}^{\mathcal {A}}(v) := \frac{1}{m}\sum _{j \in [m]} W(o_j) = \frac{1}{m} \sum _{j \in [m]} \sum _{i \in [n]} p_{ij}(v).\end{align*} </span>       <br/>      </div>     </div>    </p>    </section>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Learning with Relaxed Truthfulness Constraints</h2>    </div>    </header>    <p>We now present our first main result: We first introduce our concept of strong incentive compatibility, and based on this property, we show a general incentive-aware learning method that can be used to learn an approximately optimal mechanism (in terms of any given objective) from a given ground set while ensuring (approximate) incentive compatibility. To obtain an incentive compatible algorithm that selects one of the mechanisms in the ground set, we naturally need the mechanisms in the ground set to also be incentive compatible. It turns out that some stronger notion of incentive compatibility, such as strong incentive compatibility, is necessary for any optimization algorithm to work.</p>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Strong Incentive Compatibility</h3>     </div>    </header>    <p>To explain the notion of strong incentive compatibility, let us start with the example of auctions. Incentive compatibility requires that for each agent, truthfully reporting the value (as the agent&#x0027;s bid) is at least as profitable as any mis-report of the value. For strong incentive compatibility, we require that mis-reporting the value is <em>strictly</em> worse than reporting truthfully. Furthermore, we require the magnitude of this &#x201C;cost of mis-reporting&#x201D; to be at least a function of the magnitude of mis-report. More precisely,</p>    <div class="definition" id="enc2">     <Label>Definition 3.1.</Label>     <p> Let <em>&#x03B4;</em> be a function defined on positive reals. A mechanism <em>M</em> is <em>&#x03B4;</em>-strongly incentive compatible if for every the agent <em>i</em>, any types <em>&#x03B8;</em>, and any misreport <span class="inline-equation"><span class="tex">$\theta ^{\prime }_i$</span>      </span> of the type of agent <em>i</em>, we have <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle u_i\big (\theta _i, M(\theta ^{\prime }_i, \theta _{-i})\big) \le u_i\big (\theta _i, M(\theta _i, \theta _{-i})\big) - \delta (|\theta _i - \theta ^{\prime }_i|).\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <p>In the general case, we need to be more careful with the definition, since the &#x201C;bid&#x201D; in this case has no numerical meaning (e.g., it can even be a categorical value), and therefore we have to replace the magnitude of misreport (<span class="inline-equation"><span class="tex">$|\theta _i - \theta ^{\prime }_i|$</span>     </span>) by another quantity. As it turns out, the right quantity to measure the magnitude of misreport is the amount of difference the agent can make in the designer&#x0027;s objective function. This gives us the following definition.</p>    <div class="definition" id="enc3">     <Label>Definition 3.2 (Strong Incentive Compatibility).</Label>     <p> The set of mechanisms <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>      </span> is <em>&#x03B4;</em>-strong incentive compatible with respect to a given objective <em>W</em>, if to manipulate the objective value of any <span class="inline-equation"><span class="tex">$M \in \mathcal {M}$</span>      </span> by <em>&#x03BB;</em> > 0 via mis-reporting, the agent <em>i</em> must suffer a loss no less than <em>&#x03B4;</em>(<em>&#x03BB;</em>), where <span class="inline-equation"><span class="tex">$\delta : \mathbb {R}_+ \rightarrow \mathbb {R}_+$</span>      </span> is a weakly increasing and nonnegative function. Formally, <span class="inline-equation"><span class="tex">$\forall \theta \in \Theta ^n, \theta _i^{\prime } \in \Theta , M \in \mathcal {M}$</span>      </span>, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; u_i\left(\theta _i, M(\theta ^{\prime }_i, \theta _{-i})\right) \le u_i\left(\theta _i, M(\theta _i, \theta _{-i})\right) - \delta (\lambda _i), \\\text{where} \quad &#x0026; \textstyle \lambda _i := \max _{M^{\prime } \in \mathcal {M}} \big |W\big (M^{\prime }(\theta ^{\prime }_i, \theta _{-i})\big) - W\big (M^{\prime }(\theta _i, \theta _{-i})\big)\big |.\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <p>The intuition behind the definition of strong incentive compatibility comes from the observation in complex environments of large practical systems. While requiring incentive compatibility, agents are usually assumed to have unlimited knowledge and unbounded computational power. However, it is not the case with real applications. In practice, manipulations could be computationally hard for the agents [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>], hence a costly option for them.</p>    <p>More importantly, the uncertainty and dynamics in the system make it almost impossible for the agents to perfectly predict the actions of other agents, the inherent randomness in some of the systems, or the future. Therefore, mis-reporting in such partially observable environments would be risky for the agents. The greater deviation from truthful actions, the higher risk of loss they may suffer. Hence in real applications, the agent will suffer a cost of mis-reporting. The cost might be small (comparing with the potential gain through gaming the learning system) but still growing as the magnitude of mis-reports increases.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Strength of Incentive Compatibility and Learnability</h3>     </div>    </header>    <p>The notion of strong incentive compatibility in fact partially characterizes how hard it is to learn in this system. We then show two illustrative examples, one easy to learn, one hard to learn, both in the context of single-item auctions.</p>    <p>     <em>Maximizing Social Welfare</em>. Consider the situation where the designer&#x0027;s objective is social welfare and the set of mechanisms are VCG mechanisms. More specifically, let&#x0027;s focus on the single item auctions, where the VCG mechanisms are second price auctions and the designer&#x0027;s objective is social welfare, i.e., <em>W</em>(<em>o</em>) = &#x2211;<sub>      <em>i</em> &#x2208; [<em>n</em>]</sub>     <em>x<sub>i</sub>     </em> &#x00B7; <em>&#x03B8;<sub>i</sub>     </em>. In this case, <em>&#x03B4;</em>(<em>&#x03BB;</em>) = <em>&#x03BB;</em>, and learning task is easy in the sense that all the agents are incentivized to report truthfully.</p>    <p>     <em>Maximizing Revenue Against One Dominant Buyer</em>. Consider the auction setup described in Section&#x00A0;<a class="sec" href="#sec-7">2.1</a>. More specifically, there is only one buyer who has large value to the item while other buyers only have negligible values. In this case, <em>&#x03B4;</em>(<em>&#x03BB;</em>) = 0 and it is almost impossible to learn, since the dominant buyer can always pretend to have a small value that is slightly larger than the values of others.</p>    <p>The second example suggests that in order to have an optimization algorithm with reasonable approximation guarantees, some cost for an agent to manipulate the designer&#x0027;s objective is necessary.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> A General Optimization Algorithm</h3>     </div>    </header>    <p>Now we propose a general optimization algorithm, which is parameterized by a single real number <em>&#x03B1;</em> > 0 we call the <em>learning rate</em>. For the ease of presentation, we mainly focus on the cases with <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> being a finite ground set, yet we will later show a natural generalization to the cases with infinite ground sets (see Section&#x00A0;<a class="sec" href="#sec-22">6.6</a>).</p>    <p>     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186042/images/www2018-51-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>The optimization algorithm above essentially returns a distribution over the given ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span>, where the probability of each mechanism <span class="inline-equation"><span class="tex">$M \in \mathcal {M}$</span>     </span> is proportional to <span class="inline-equation"><span class="tex">$f(\bar{W}(M))$</span>     </span>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \Pr [M] \propto f(\bar{W}(M)) := e^{\alpha \bar{W}(M)},\end{align*} </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$\bar{W}(M) = \frac{1}{m}\sum _{j \in [m]} W(M(\hat{\theta }_{1j}, \ldots , \hat{\theta }_{nj}))$</span>     </span> is the average objective of mechanism <em>M</em>.</p>    <p>The next theorem states that the randomized mechanism returned by Algorithm&#x00A0;1 is &#x03F5;-incentive compatible and approximately optimal. In particular, &#x03F5; and the sub-optimality of the algorithm rely on three parameters: the learning rate <em>&#x03B1;</em>, the <em>&#x03B4;</em>(&#x00B7;)-strong incentive compatibility, and the agent market power <em>u</em>     <sup>*</sup> defined below.</p>    <div class="definition" id="enc4">     <Label>Definition 3.3 (Agent Market Power).</Label>     <p> The agent market power <em>u</em>      <sup>*</sup> is a quantity that measures how much the agents can benefit from the selection of the mechanism, i.e., <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \textstyle u^* := \max _{i \in [n]} u^*_i, \quad u^*_i := \max _{M^{\prime },M^{\prime \prime } \in \mathcal {M}} \left(\bar{u}_i(M^{\prime }) - \bar{u}_i(M^{\prime \prime })\right), \\ &#x0026; \textstyle \bar{u}_i(M) := \frac{1}{m} \sum _{j \in [m]} u_i\left(\theta _{ij}, M(\theta _{1j}, \ldots , \theta _{ij}, \ldots , \theta _{nj})\right).\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <p>An agent is called &#x201C;small&#x201D;, if her market power is small with respect to the optimal objective OPT achievable within <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span>.</p>    <div class="theorem" id="enc5">     <Label>Theorem 3.4.</Label>     <p> The randomized mechanism by Algorithm&#x00A0;1 is &#x03F5;-incentive compatible and is approximately optimal, where tanh&#x2009;(&#x00B7;) is the hyperbolic tangent function.<a class="fn" href="#fn5" id="foot-fn5"><sup>2</sup></a>: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \epsilon = \max _{\lambda \ge 0} \left[u^* \cdot \tanh (\alpha \lambda / 2) - \delta (\lambda)\right] ~~~ \text{and} ~~~ \mathrm{ALG}\ge \mathrm{OPT}- \frac{1}{\alpha }\ln |\mathcal {M}|,\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <p>Two messages from Theorem&#x00A0;<a class="enc" href="#enc5">3.4</a>: (i) the trade-off (controlled by learning rate <em>&#x03B1;</em>) between the robustness (strength of incentive compatibility) and the optimality of the algorithm and (ii) the hardness/efficiency of the trade-off characterized by the strength of incentive compatibility <em>&#x03B4;</em>(&#x00B7;) and the agent market power <em>u</em>     <sup>*</sup>:</p>    <p>     <strong>Learning rate <em>&#x03B1;</em>     </strong>: the larger the learning rate is, the stronger guarantee of the optimality and the weaker guarantee of incentive compatibility we can conclude, vice versa. Intuitively, when <em>&#x03B1;</em> is very close to zero, the randomized mechanism is almost uniformly random over the ground set. In this case, the agents have little incentive to misreport their private values (hence stronger IC guarantee), because the selection of the mechanisms is almost independent with their reports. However, the uniform-at-random mechanism could be extremely far from optimal. In contrast, when <em>&#x03B1;</em> is very large (tends to &#x221E;), the randomized mechanism assigns almost all the probability mass on the optimal mechanism. In this case, the algorithm is almost optimal, but IC guarantee is as bad as naively picking the mechanism that maximizes the objective from the ground set.</p>    <p>     <strong>Strength of incentive compatibility <em>&#x03B4;</em>(&#x00B7;)</strong>: the stronger incentive compatibility the system can guarantee, the better approximate incentive compatibility we can conclude for our algorithm. As we discussed by examples in Section&#x00A0;<a class="sec" href="#sec-10">3.2</a>, the larger <em>&#x03B4;</em>(&#x00B7;) is, the &#x201C;easier&#x201D; to learn with good IC guarantees.</p>    <p>     <strong>Agent market power <em>u</em>      <sup>*</sup>     </strong>: the larger the agent market power is, the weaker incentive compatibility we can guarantee (so harder to learn). Intuitively, when the agent market power is small (compared to OPT), meaning there is little difference among mechanisms in the ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> from the perspective of the agents, any optimization algorithm must be close to incentive compatible, and vise versa.</p>    <p>The proof of this theorem breaks into two steps: Lemma&#x00A0;<a class="enc" href="#enc6">3.5</a> for the approximately incentive compatibility and Lemma&#x00A0;<a class="enc" href="#enc7">3.6</a> for the approximate optimality. The main messages from these lemmas are rather intuitive, while the mathematical techniques therein are quite challenging. We place the proofs in Section&#x00A0;<a class="sec" href="#sec-16">6</a>.</p>    <div class="lemma" id="enc6">     <Label>Lemma 3.5.</Label>     <p> The randomized mechanism by Algorithm&#x00A0;1 is &#x03F5;-incentive compatible. In particular, &#x03F5; for each agent <em>i</em> is no more than <span class="inline-equation"><span class="tex">$\epsilon _i = \max _{\lambda _i \ge 0}[u^*_i \cdot \tanh (\alpha \lambda _i / 2) - \delta (\lambda _i)]$</span>      </span>.</p>    </div>    <div class="lemma" id="enc7">     <Label>Lemma 3.6.</Label>     <p> Algorithm&#x00A0;1 is approximately optimal, i.e., <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \mathrm{ALG}= \bar{W}(\mathcal {A}) := \frac{1}{m} \sum _{j \in [m]} W\left(\mathcal {A}_j(\theta)\right) \ge \mathrm{OPT}- \frac{1}{\alpha }\ln |\mathcal {M}|.\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <div class="proof" id="proof1">     <Label>Proof Proof of Theorem&#x00A0;3.4</Label>     <p> Directly by Lemma&#x00A0;<a class="enc" href="#enc6">3.5</a> and <a class="enc" href="#enc7">3.6</a>.</p>    </div>    <p>     <em>A concrete example for the trade-off</em>. Let&#x0027;s again consider an example under the single-item auction setting and see how much incentive compatibility we should sacrifice to achieve a (1 &#x2212; <em>&#x03C1;</em>)-approximately optimal mechanism, where <em>&#x03C1;</em> > 0 is a small constant.</p>    <p>Suppose that <em>&#x03B4;</em>(<em>&#x03BB;</em>) is a quadratic function (see further discussions on this assumption in Section&#x00A0;<a class="sec" href="#sec-12">4</a>), we have the following corollary by tanh&#x2009;(<em>x</em>) &#x2264; <em>x</em> (for <em>x</em> &#x2265; 0):</p>    <div class="corollary" id="enc8">     <Label>Corollary 3.7.</Label>     <p> When <em>&#x03B4;</em>(<em>&#x03BB;</em>) = <em>&#x03B4;</em>      <sup>(2)</sup> &#x00B7; <em>&#x03BB;</em>      <sup>2</sup> is a quadratic function, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \epsilon \le \max _{\lambda \ge 0} [u^* \cdot \lambda \alpha / 2 - \delta ^{(2)} \cdot \lambda ^2] = (\alpha {u^*})^2 / 4\delta ^{(2)}.\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    <p>Using Theorem&#x00A0;<a class="enc" href="#enc5">3.4</a>, we will need the learning rate to be at least <span class="inline-equation"><span class="tex">$\alpha = \ln |\mathcal {M}| / (\rho \cdot \mathrm{OPT})$</span>     </span>. Hence &#x03F5; is very small when OPT &#x226B; <em>u</em>     <sup>*</sup>. As analogy to the agent market power, in fact, one can think OPT as the market power of the designer. Then OPT &#x226B; <em>u</em> <sup>*</sup> means that the designer has superior market power over the agents.</p>    <p>One may have the concern that the &#x03F5;-incentive compatible is additive, which may not make much sense if the buyer utility of truthfully reporting is close to zero (or less than &#x03F5;). Fortunately, we can easily convert the mechanism to another mechanism with multiplicative approximate incentive compatible constraints by sacrifice at most a small constant fraction of the revenue. Note that under the single-item auction setting, the more desirable auction (from second price auctions with reserves) for the buyers is the vanilla second price auction (i.e., with no reserve price), where the buyer utility is at least <em>u</em>     <sup>*</sup>. By taking the convex combination of the original mechanism and the vanilla second price auction (the latter is selected with a small probability), the revenue is decreased by at most a small constant fraction, while a strong multiplicative incentive compatible guaranteed is obtained as long as OPT &#x226B; <em>u</em>     <sup>*</sup>.</p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Application to Learning Reserve for Second Price Auctions</h2>    </div>    </header>    <p>In this section, we show how to apply the general incentive-aware learning framework to an illustrative example &#x2014; learning reserve prices in second price auctions. As described in Section&#x00A0;<a class="sec" href="#sec-7">2.1</a>, we consider a single item auction setting, where each sample corresponds to a second price auction selling one item and the goal of the seller is to find reserve prices maximizing his revenue. The ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span> consists of second price auctions with different reserves.</p>    <p>For the ease of presentation, we consider the problem of learning a single anonymous reserve price for all the samples. Note that this is an easy task in the context of classic learning where the buyers are not strategic. However, it is in general challenging when the buyers are strategic because second price auctions with reserves in <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span> do not satisfy the strong incentive compatibility and hence we cannot directly use the optimization algorithm introduced in Section&#x00A0;<a class="sec" href="#sec-8">3</a>. In our second main theorem, we show that with some &#x201C;perturbation&#x201D; to the mechanisms we can apply our general learning method but suffer a small additional loss in revenue.</p>    <p>    <div class="theorem" id="enc9">     <Label>Theorem 4.1.</Label>     <p> At the cost of a small loss in revenue, we can &#x201C;perturb&#x201D; the mechanisms in <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>      </span> and then apply our general incentive-aware learning algorithm, while still guarantee approximate incentive compatibility and optimality (compared with the optimal auction in the original ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>      </span>).<a class="fn" href="#fn6" id="foot-fn6"><sup>3</sup></a>     </p>    </div>    </p>    <p>In the remaining of this section, we make some changes to the ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>    </span> in two steps to obtain the strong incentive compatibility, so that our general incentive-aware learning algorithm applies:</p>    <ul class="list-no-style">    <li id="list8" label="&#x2022;">such that any buyer manipulates the revenue of any candidate auction has to significantly change her bids as well;<br/></li>    <li id="list9" label="&#x2022;">such that the utility loss of any buyer is quadratic in her changes of bids .<br/></li>    </ul>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Step I: buyers must change bids significantly in order to affect the revenue</h3>     </div>    </header>    <p>In the second price auction with any fixed reserve, some buyers could change the revenue significantly with negligible changes in her bids. In what follows, we show how we can make some changes to the ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> such that one must change her bids significantly to affect the revenue of any <span class="inline-equation"><span class="tex">$M \in \mathcal {M}$</span>     </span>.</p>    <div class="definition" id="enc10">     <Label>Definition 4.2 (Second price auction with random anonymous reserve).</Label>     <p> A second price auction with random anonymous reserves is parameterized by two parameters, <em>r</em> &#x2265; 0 and <em>&#x03B3;</em> &#x2208; [0, 1). The auction simply runs a second price auction with an anonymous reserve <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span> randomly drawn from a uniform distribution from [<em>&#x03B3;r</em>, <em>r</em>], i.e., <span class="inline-equation"><span class="tex">$\tilde{r} \sim \mathrm{U}[\gamma r, r]$</span>      </span>.</p>    </div>    <p>Suppose that <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> is the original ground set of second price auctions with anonymous reserves. Then we construct a corresponding ground set <span class="inline-equation"><span class="tex">$\mathcal {M}^{\prime }$</span>     </span> of second price auction with random anonymous reserves. More specifically, with a fixed <em>&#x03B3;</em> &#x2208; [0, 1), for each second price auction with reserve <em>r</em> in <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span>, we construct a second price auction with random anonymous reserves drawn from U[<em>&#x03B3;r</em>, <em>r</em>].</p>    <p>Then the following lemma states that for any mechanism <span class="inline-equation"><span class="tex">$M^{\prime } \in \mathcal {M}^{\prime }$</span>     </span>, the average revenue change <em>&#x03BB;<sub>i</sub>     </em> by the misreporting of any buyer <em>i</em> is no more than a constant times the average absolute change in her bids. Meanwhile, the optimal revenue OPT&#x2032; achievable within this mutant ground set <span class="inline-equation"><span class="tex">$\mathcal {M}^{\prime }$</span>     </span> is close to OPT of the original <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span>. So we don&#x0027;t lose much with such mutant.</p>    <div class="lemma" id="enc11">     <Label>Lemma 4.3.</Label>     <p>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \lambda \le \frac{1}{(1 - \gamma)m} \sum _j |\hat{v}_{ij} - v_{ij}| \quad \text{and} \quad \mathrm{OPT}^{\prime } \ge \mathrm{OPT}- \gamma \mathrm{OPT}/ 2.\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Step II: quadratic utility loss in bid-changes</h3>     </div>    </header>    <p>In standard second price auctions, truthfully bidding is the weak dominant strategy, meaning bidding truthfully is weakly better than any other strategies. Such weak dominance brings the difficulty of electing the true values while learning the reserve price for strategic buyers. Because for some instances, buyers can misreport significantly without suffering any loss in terms of their utilities.</p>    <p>However, in practice, inherent to most auction settings are several sources of uncertainty for the bidders, such as the predicted click-through-rate in ad auctions, the randomized throttling strategies, even the Internet connection status, etc. Misreporting in the presence of such uncertainties would potentially bring significant utility losses for the buyers, which tend to grow super linearly in the magnitude of the misreports. Intuitively, the more the buyer raises her bid in one auction, the more (linear in the magnitude) likely she will win the item at a price higher than her will and the more (linear in the magnitude) expensive price she has to pay. So her utility loss tends to be quadratic in the magnitude of her misreport (similar argument applies for lowering her bid).</p>    <p>In what follows, we manually introduce such uncertainty into the system by &#x201C;perturbing&#x201D; the auction and bids, i.e., with some small probability, <em>&#x03B2;</em> &#x2208; [0, 1], (i) reset the reserve <em>r</em> to be randomly drawn from <span class="inline-equation"><span class="tex">$\mathrm{U}[0, \overline{r}]$</span>     </span>, and (ii) remove each buyer from the auction independently with probability 1/2, where <span class="inline-equation"><span class="tex">$\overline{r}$</span>     </span> is the maximum reserve used in ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span>. By doing so, we will have to sacrifice a small fraction <em>&#x03B2;</em> of the revenue, but it does not hurt our goal of achieving approximately optimal revenue.</p>    <div class="lemma" id="enc12">     <Label>Lemma 4.4.</Label>     <p>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \delta _i(\lambda _i) \ge \frac{(1 - \gamma)\beta }{8 m \overline{r}} \sum _j \lambda _{ij}^2 \quad \text{and} \quad \mathrm{OPT}^{\prime \prime } \ge \mathrm{OPT}- (\gamma / 2 + \beta) \mathrm{OPT}.\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    </section>   </section>   <section id="sec-15">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Comparing with Mechanism Design via Differential Privacy</h2>    </div>    </header>    <p>In this section, we compare our incentive guarantee (Theorem&#x00A0;<a class="enc" href="#enc5">3.4</a>) with McSherry and Talwar [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] under the incentive-aware learning framework. In particular, our guarantee is stronger than theirs when the impact of each agent on the learning objective is sufficiently large.</p>    <p>Suppose that the objective on two data sets differing on the reports of agent <em>i</em> is at most <span class="inline-equation"><span class="tex">$\lambda ^*_i$</span>    </span>, and <span class="inline-equation"><span class="tex">$\lambda ^* = \max _{i} \lambda ^*_i$</span>    </span>. Then according to McSherry and Talwar [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, Lemma 3 and Theorem 6], the incentive-aware learning algorithm is &#x03F5;-approximately incentive compatible, where <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \epsilon = u^*\big (\exp (2 \alpha ^{\mathsf {MT}} \cdot \lambda ^*) - 1\big).\end{align*} </span>      <br/>     </div>    </div> In other words, applying their bounds, the learning rate <span class="inline-equation"><span class="tex">$\alpha ^{\mathsf {MT}}$</span>    </span> should be at most <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \alpha ^{\mathsf {MT}} = \ln (1 + \epsilon / u^*) / 2\lambda ^*.\end{align*} </span>      <br/>     </div>    </div>    </p>    <p>Suppose that the function <em>&#x03B4;</em>(&#x00B7;) is quadratic, as we argued in Section&#x00A0;<a class="sec" href="#sec-12">4</a> for second price auctions, i.e., <em>&#x03B4;</em>(<em>&#x03BB;</em>) = <em>&#x03B4;</em>    <sup>(2)</sup>    <em>&#x03BB;</em>    <sup>2</sup>. Then to achieve the same &#x03F5;-approximate incentive compatibility, by Corollary&#x00A0;<a class="enc" href="#enc8">3.7</a>, we require the learning rate to be at most <span class="inline-equation"><span class="tex">$\alpha ^{\mathsf {IA}}$</span>    </span>: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \alpha ^{\mathsf {IA}} = 4\sqrt {\delta ^{(2)}\epsilon } / u^*.\end{align*} </span>      <br/>     </div>    </div> Therefore, when <span class="inline-equation"><span class="tex">$\lambda ^* \ge u^* \ln (1 + \epsilon / u^*) / 8\sqrt {\delta ^{(2)}\epsilon }$</span>    </span>, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} \textstyle \alpha ^{\mathsf {IA}} = 4\sqrt {\delta ^{(2)}\epsilon } / u^* \ge \ln (1 + \epsilon / u^*) / 2\lambda ^* = \alpha ^{\mathsf {MT}}.\end{align*} </span>      <br/>     </div>    </div> In practice, we will need &#x03F5; &#x226A; <em>u</em>    <sup>*</sup>, therefore, when <span class="inline-equation"><span class="tex">$\lambda ^* \ge u^* \ln (1 + \epsilon / u^*) / 8\sqrt {\delta ^{(2)}\epsilon } \approx \sqrt {\epsilon / \delta ^{(2)}} / 8$</span>    </span>, with our analysis for incentive-aware learning, a higher learning rate is acceptable. Hence the loss in terms of the optimality could be made smaller (cf. Theorem&#x00A0;<a class="enc" href="#enc5">3.4</a>).</p>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Examples and Missing proofs</h2>    </div>    </header>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> The limitation of separation tricks</h3>     </div>    </header>    <p>In single-item auctions, separating buyers into two groups cannot be better than simply running a second price auction.</p>    <div class="example" id="enc13">     <Label>Example 6.1.</Label>     <p> Consider a single item auction with <em>N</em> i.i.d. buyers, values drawn from uniform [0, 1]. For large <em>N</em>, by randomly splitting the buyers into 2 groups, the seller can almost perfectly estimate the best reserve, 0.5, but with very high probability only about <em>N</em>/2 buyers in each group. Since there is only one item for sale, the seller must decide (maybe randomly) to which group the item should be sold, where the expected revenue is roughly (<em>N</em> &#x2212; 2)/(<em>N</em> + 2). However, the expected revenue of second price auction is (<em>N</em> &#x2212; 1)/(<em>N</em> + 1), clearly higher than (<em>N</em> &#x2212; 2)/(<em>N</em> + 2).</p>    </div>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Proof of Lemma&#x00A0;<a class="enc" href="#enc6">3.5</a>      </h3>     </div>    </header>    <div class="proof" id="proof2">     <Label>Proof Proof of Lemma&#x00A0;3.5</Label>     <p> We start with the upper bound on one&#x0027;s benefit by misreporting her private types. For each agent <em>i</em>, we use <span class="inline-equation"><span class="tex">$\bar{u}_i(M)$</span>      </span> to denote her average utility from mechanism <span class="inline-equation"><span class="tex">$M \in \mathcal {M}$</span>      </span> when she reports truthfully and the reports of others are fixed, i.e., <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \bar{u}_i(M) = \frac{1}{m} \sum _{j \in [m]} u_i\left(\theta _{ij}, M(\hat{\theta }_{1j}, \ldots , \theta _{ij}, \ldots , \hat{\theta }_{nj})\right).\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>Similarly, we use <span class="inline-equation"><span class="tex">$\bar{u}^{\prime }_i(M)$</span>      </span> to denote her average utility when she misreports but the others unchanged, i.e., <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \bar{u}^{\prime }_i(M) = \frac{1}{m} \sum _{j \in [m]} u_i\left(\theta _{ij}, M(\hat{\theta }_{1j}, \ldots , \theta ^{\prime }_{ij}, \ldots , \hat{\theta }_{nj})\right),\end{align*} </span>       <br/>       </div>      </div> and <span class="inline-equation"><span class="tex">$\bar{W}(M)$</span>      </span> and <span class="inline-equation"><span class="tex">$\bar{W}^{\prime }(M)$</span>      </span> to denote the designer&#x0027;s average objective when agent <em>i</em> truthfully reports and misreports, respectively, i.e., <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \bar{W}(M) &#x0026; \textstyle = \frac{1}{m} \sum _{j \in [m]} W\left(M(\hat{\theta }_{1j}, \ldots , \theta _{ij}, \ldots , \hat{\theta }_{nj})\right), \\\bar{W}^{\prime }(M) &#x0026; \textstyle = \frac{1}{m} \sum _{j \in [m]} W\left(M(\hat{\theta }_{1j}, \ldots , \theta ^{\prime }_{ij}, \ldots , \hat{\theta }_{nj})\right).\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>Then we consider the benefit of agent <em>i</em> by misreporting under optimization algorithm <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span>. According to the construction of <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span>, her average utilities under <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span> when truthfully reporting and misreporting are <span class="inline-equation"><span class="tex">$\bar{u}_i(\mathcal {A})$</span>      </span> and <span class="inline-equation"><span class="tex">$\bar{u}^{\prime }_i(\mathcal {A})$</span>      </span>, respectively, i.e., <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \bar{u}_i(\mathcal {A}) = \frac{\int _{\mathcal {M}} \bar{u}_i(M)f(\bar{W}(M)) \mathrm{d}M}{\int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}M}, \quad \textstyle \bar{u}^{\prime }_i(\mathcal {A}) = \frac{\int _{\mathcal {M}} \bar{u}^{\prime }_i(M)f(\bar{W}^{\prime }(M)) \mathrm{d}M}{\int _{\mathcal {M}} f(\bar{W}^{\prime }(M)) \mathrm{d}M}.\end{align*} </span>       <br/>       </div>      </div> Their difference is, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;\bar{u}^{\prime }_i(\mathcal {A}) - \bar{u}_i(\mathcal {A}) \textstyle = \textstyle \frac{\int _{\mathcal {M}} \bar{u}^{\prime }_i(M)f(\bar{W}^{\prime }(M)) \mathrm{d}M}{\int _{\mathcal {M}} f(\bar{W}^{\prime }(M)) \mathrm{d}M} - \frac{\int _{\mathcal {M}} \bar{u}_i(M)f(\bar{W}(M)) \mathrm{d}M}{\int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}M} \\\le ~&#x0026; \textstyle \int _\mathcal {M}\bar{u}_i(M) \left(\frac{f(\bar{W}^{\prime }(M))}{\int _{\mathcal {M}} f(\bar{W}^{\prime }(M)) \mathrm{d}M} - \frac{f(\bar{W}(M))}{\int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}M}\right) \mathrm{d}M - \delta (\lambda _i),\end{align*} </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">$\bar{u}^{\prime }_i(M) \le \bar{u}_i(M) - \delta (\lambda _i)$</span>      </span> by the strong incentive compatibility of the system. By partitioning <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>      </span> into <span class="inline-equation"><span class="tex">$\mathcal {M}_+$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {M}_-$</span>      </span>, i.e., letting <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \mathcal {M}_+ &#x0026;\textstyle = \left\lbrace M \in \mathcal {M}\bigg | \frac{f(\bar{W}^{\prime }(M))}{\int _{\mathcal {M}} f(\bar{W}^{\prime }(M)) \mathrm{d}M} - \frac{f(\bar{W}(M))}{\int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}M} \ge 0 \right\rbrace ,\end{align*} </span>       <br/>       </div>      </div> and <span class="inline-equation"><span class="tex">$\mathcal {M}_- = \mathcal {M}\setminus \mathcal {M}_+$</span>      </span>, we obtain, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;\bar{u}^{\prime }_i(\mathcal {A}) - \bar{u}_i(\mathcal {A}) \\ =~&#x0026; \textstyle \int _{\mathcal {M}_+} \bar{u}_i(M)\left(\frac{f(\bar{W}^{\prime }(M))}{\int _{\mathcal {M}} f(\bar{W}^{\prime }(M)) \mathrm{d}M} - \frac{f(\bar{W}(M))}{\int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}M}\right) \mathrm{d}M \\ &#x0026; \textstyle + \int _{\mathcal {M}_-} \bar{u}_i(M)\left(\frac{f(\bar{W}^{\prime }(M))}{\int _{\mathcal {M}} f(\bar{W}^{\prime }(M)) \mathrm{d}M} - \frac{f(\bar{W}(M))}{\int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}M}\right) \mathrm{d}M - \delta (\lambda _i) \\\le ~&#x0026; \textstyle \left(\frac{\int _{\mathcal {M}_+}f(\bar{W}^{\prime }(M))\mathrm{d}M}{\int _{\mathcal {M}}f(\bar{W}^{\prime }(M))\mathrm{d}M} - \frac{\int _{\mathcal {M}_+} f(\bar{W}(M))\mathrm{d}M}{\int _{\mathcal {M}}f(\bar{W}(M)) \mathrm{d}M}\right) \bar{u}^{\max }_i \\ &#x0026; \textstyle + \left(\frac{\int _{\mathcal {M}_-}f(\bar{W}^{\prime }(M))\mathrm{d}M}{\int _{\mathcal {M}}f(\bar{W}^{\prime }(M))\mathrm{d}M} - \frac{\int _{\mathcal {M}_-} f(\bar{W}(M))\mathrm{d}M}{\int _{\mathcal {M}}f(\bar{W}(M)) \mathrm{d}M}\right)\bar{u}^{\min }_i - \delta (\lambda _i),\end{align*} </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">$\bar{u}^{\max }_i = \max _{M \in \mathcal {M}} \bar{u}_i(M)$</span>      </span> and <span class="inline-equation"><span class="tex">$\bar{u}^{\min }_i = \min _{M \in \mathcal {M}} \bar{u}_i(M)$</span>      </span>.</p>     <p>Define the following notations, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \textstyle A = \int _{\mathcal {M}_+} f(\bar{W}(M)) \mathrm{d}M,~ A^{\prime } = \int _{\mathcal {M}_+} f(\bar{W}^{\prime }(M)) \mathrm{d}M,~ \\ &#x0026; \textstyle B = \int _{\mathcal {M}_-} f(\bar{W}(M)) \mathrm{d}M,~ B^{\prime } = \int _{\mathcal {M}_-} f(\bar{W}^{\prime }(M)) \mathrm{d}M.\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>Therefore, by rewriting, we obtain, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \bar{u}^{\prime }_i(\mathcal {A}) - \bar{u}_i(\mathcal {A}) \\\le ~&#x0026;\textstyle \left(\frac{A^{\prime }}{A^{\prime } + B^{\prime }} - \frac{A}{A + B}\right)\bar{u}^{\max }_i + \left(\frac{B^{\prime }}{A^{\prime } + B^{\prime }} - \frac{B}{A + B}\right)\bar{u}^{\min }_i - \delta (\lambda _i) \\ =~&#x0026;\textstyle \left(\frac{A^{\prime }}{A^{\prime } + B^{\prime }} - \frac{A}{A + B}\right) (\bar{u}^{\max }_i - \bar{u}^{\min }_i) - \delta (\lambda _i).\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>Recall that by definition, <span class="inline-equation"><span class="tex">$|\bar{W}^{\prime }(M) - \bar{W}(M)| \le \lambda _i$</span>      </span>. Since <em>f</em>(<em>x</em>) = <em>e<sup>&#x03B1;x</sup>      </em>, <em>&#x03B1;</em> > 0, we have <em>f</em>(<em>x</em> + <em>y</em>) = <em>f</em>(<em>x</em>)<em>f</em>(<em>y</em>) and <em>f</em> is non-negative and weakly increasing. Therefore, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{gather*} \textstyle A^{\prime } \le \int _{\mathcal {M}_+} f(\bar{W}(M) + \lambda _i) \mathrm{d}M = Af(\lambda _i),~ \\ \textstyle B^{\prime } \ge \int _{\mathcal {M}_-} f(\bar{W}(M) - \lambda _i) \mathrm{d}M = B/f(\lambda _i).\end{gather*} </span>       <br/>       </div>      </div> Hence, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \bar{u}^{\prime }_i(\mathcal {A}) - \bar{u}_i(\mathcal {A}) &#x0026; \textstyle \le \left(\frac{Af(\lambda _i)}{Af(\lambda _i) + B/f(\lambda _i)} - \frac{A}{A + B}\right)(\bar{u}^{\max }_i - \bar{u}^{\min }_i) - \delta (\lambda _i) \\ &#x0026; \textstyle = \left(\frac{f(\lambda _i)^2}{f(\lambda _i)^2 + B/A} - \frac{1}{1 + B/A}\right)(\bar{u}^{\max }_i - \bar{u}^{\min }_i) - \delta (\lambda _i).\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>To bound the right-hand-side for any <em>B</em>/<em>A</em> > 0, let&#x0027;s consider the following function of <em>x</em>, where <em>c</em> = <em>f</em>(<em>&#x03BB;<sub>i</sub>      </em>)<sup>2</sup> &#x2265; <em>f</em>(0)<sup>2</sup> = 1: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \frac{c}{c + x} - \frac{1}{1 + x} = \frac{(c - 1)x}{(c + x)(x + 1)} = \frac{(c - 1)}{x + c/x + c + 1} \le \frac{c - 1}{(\sqrt {c} + 1)^2} = \frac{\sqrt {c} - 1}{\sqrt {c} + 1}.\end{align*} </span>       <br/>       </div>      </div> The upper bound is reached if and only if <em>x</em> = <em>c</em>/<em>x</em>, i.e., <span class="inline-equation"><span class="tex">$x = \sqrt {c}$</span>      </span>.</p>     <p>Thus, <span class="inline-equation"><span class="tex">$\bar{u}^{\prime }_i(\mathcal {A}) - \bar{u}_i(\mathcal {A}) \le \frac{f(\lambda _i) - 1}{f(\lambda _i) + 1} (\bar{u}^{\max }_i - \bar{u}^{\min }_i) - \delta (\lambda _i)$</span>      </span>.</p>     <p>In particular, for <em>f</em>(<em>x</em>) = <em>e<sup>&#x03B1;x</sup>      </em>, the right-hand-side equals to <span class="inline-equation"><span class="tex">$\tanh (\alpha \lambda _i / 2) \bar{u}^*_i - \delta (\lambda _i)$</span>      </span>, where tanh&#x2009;(&#x00B7;) is the hyperbolic tangent function and its Taylor series expression is, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \tanh (\alpha \lambda _i / 2) = \frac{\alpha \lambda _i}{2} - \frac{(\alpha \lambda _i)^3}{24} + \frac{(\alpha \lambda _i)^5}{240} - O(\lambda _i^7),\end{align*} </span>       <br/>       </div>      </div> where the residue <span class="inline-equation"><span class="tex">$- O(\lambda _i^7) {\lt} 0$</span>      </span> for all <em>&#x03BB;<sub>i</sub>      </em> > 0.</p>    </div>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Proof of Lemma&#x00A0;<a class="enc" href="#enc7">3.6</a>      </h3>     </div>    </header>    <div class="proof" id="proof3">     <Label>Proof Proof of Lemma&#x00A0;3.6</Label>     <p> For simplicity, let&#x0027;s consider the cases where <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>      </span> is a finite set. In this case, the average objective is <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \bar{W}(\mathcal {A}) = {\mathbb {E}}_M[\bar{W}(M)] = (\sum _{M \in \mathcal {M}}\bar{W}(M) e^{\alpha \bar{W}(M)}) / (\sum _{M \in \mathcal {M}} e^{\alpha \bar{W}(M)}).\end{align*} </span>       <br/>       </div>      </div> Suppose that the optimal objective achievable by mechanisms in <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>      </span> is OPT and then our goal is to find a lower bound of <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \frac{\bar{W}(\mathcal {A})}{\mathrm{OPT}} = \big (\sum _{M \in \mathcal {M}}\frac{\bar{W}(M)}{\mathrm{OPT}} e^{\alpha ^{\prime }\frac{\bar{W}(M)}{\mathrm{OPT}}}\big) / \big (\sum _{M \in \mathcal {M}} e^{\alpha ^{\prime }\frac{\bar{W}(M)}{\mathrm{OPT}}}\big),\end{align*} </span>       <br/>       </div>      </div> where <em>&#x03B1;</em>&#x2032; = <em>&#x03B1;</em> &#x00B7; OPT. In particular, the right-hand-side can be abstracted as a multidimensional function in the following form, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle g(x) = \frac{e^{\alpha ^{\prime }} + \sum _{k = 1}^K x_k e^{\alpha ^{\prime } x_k}}{e^{\alpha ^{\prime }} + \sum _{k = 1}^K e^{\alpha ^{\prime } x_k}} = \frac{1}{\alpha ^{\prime }} \cdot \frac{\alpha ^{\prime } e^{\alpha ^{\prime }} + \sum _{k = 1}^K y_k\ln y_k}{e^{\alpha ^{\prime }} + \sum _{k = 1}^K y_k} =: \tilde{g}(y),\end{align*} </span>       <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">$y_k := e^{\alpha ^{\prime } x_k}$</span>      </span>. Note that <em>y<sub>k</sub>      </em> > 0 and <em>t</em>ln&#x2009;<em>t</em> is a convex function. Then by Jensen&#x0027;s inequality, we obtain, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \frac{1}{K}\sum _{k = 1}^K y_k\ln y_k \stackrel{\text{Jensen}}{\ge } \frac{\sum _{k = 1}^K y_k}{K}\ln \frac{\sum _{k = 1}^K y_k}{K}.\end{align*} </span>       <br/>       </div>      </div> By writing <span class="inline-equation"><span class="tex">$Y = \sum _{k = 1}^K y_k$</span>      </span> and <span class="inline-equation"><span class="tex">$X = \frac{1}{\alpha ^{\prime }}\ln \frac{Y}{K}$</span>      </span>, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle g(x) = \tilde{g}(y) \ge \frac{e^{\alpha ^{\prime }} + \frac{Y}{\alpha ^{\prime }}\ln \frac{Y}{K}}{e^{\alpha ^{\prime }} + Y} = \frac{e^{\alpha ^{\prime }} + KX e^{\alpha ^{\prime }X}}{e^{\alpha ^{\prime }} + Ke^{\alpha ^{\prime }X}}.\end{align*} </span>       <br/>       </div>      </div> The minimum of the function with single variable <em>X</em> is reached when the first order condition is satisfied, i.e., <em>X</em>      <sub>0</sub> is the root of, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{gather*} \textstyle Ke^{\alpha ^{\prime }X_0} = (\alpha ^{\prime } - \alpha ^{\prime }X_0 - 1)e^{\alpha ^{\prime }} \\ \iff ~ \textstyle \alpha ^{\prime }X_0 = \alpha ^{\prime } + \ln (\alpha ^{\prime } - \alpha ^{\prime }X_0 - 1) - \ln K. \\ \Longrightarrow \textstyle g(x) \ge \frac{e^{\alpha ^{\prime }} + KX e^{\alpha ^{\prime }X}}{e^{\alpha ^{\prime }} + Ke^{\alpha ^{\prime }X}} \ge \frac{e^{\alpha ^{\prime }} + X_0(\alpha ^{\prime } - \alpha ^{\prime }X_0 - 1)e^{\alpha ^{\prime }}}{e^{\alpha ^{\prime }} + (\alpha ^{\prime } - \alpha ^{\prime }X_0 - 1)e^{\alpha ^{\prime }}} = X_0 + \frac{1}{\alpha ^{\prime }}.\end{gather*} </span>       <br/>       </div>      </div>     </p>     <p>In particular, when <em>&#x03B1;</em>&#x2032; &#x2212; <em>&#x03B1;</em>&#x2032;<em>X</em>      <sub>0</sub> &#x2212; 1 &#x2264; 1/<em>e</em>, or equivalently <em>X</em>      <sub>0</sub> &#x2265; 1 &#x2212; (1 + 1/<em>e</em>)/<em>&#x03B1;</em>&#x2032;, we have <em>g</em>(<em>x</em>) &#x2265; <em>X</em>      <sub>0</sub> + 1/<em>&#x03B1;</em>&#x2032; &#x2265; 1 &#x2212; 1/<em>e&#x03B1;</em>&#x2032;. Otherwise, when <em>&#x03B1;</em>&#x2032; &#x2212; <em>&#x03B1;</em>&#x2032;<em>X</em>      <sub>0</sub> &#x2212; 1 > 1/<em>e</em>, which implies <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{gather*} \textstyle \alpha ^{\prime }X_0 = \alpha ^{\prime } + \ln (\alpha ^{\prime } - \alpha ^{\prime }X_0 - 1) - \ln K {\gt} \alpha ^{\prime } - 1 - \ln K \\ \textstyle \Longrightarrow X_0 \ge 1 - (\ln K + 1) / \alpha ^{\prime },\end{gather*} </span>       <br/>       </div>      </div> hence <span class="inline-equation"><span class="tex">$g(x) \ge X_0 + \frac{1}{\alpha ^{\prime }} \ge 1 - \frac{\ln K}{\alpha ^{\prime }}$</span>      </span>. In summary, we conclude on the following lower bound for any <em>K</em> &#x2265; 1 (hence ln&#x2009;(<em>K</em> + 1) &#x2265; ln&#x2009;2 > 1/<em>e</em>), <em>g</em>(<em>x</em>) &#x2265; 1 &#x2212; ln&#x2009;(<em>K</em> + 1)/<em>&#x03B1;</em>&#x2032;. Therefore, for our original problem, we conclude that, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \bar{W}(\mathcal {A}) \ge \mathrm{OPT}- \frac{1}{\alpha }\ln |\mathcal {M}|.\end{align*} </span>       <br/>       </div>      </div> Recall that <span class="inline-equation"><span class="tex">$g(x) = \bar{W}(\mathcal {A}) / \mathrm{OPT}$</span>      </span>, <em>&#x03B1;</em>&#x2032; = <em>&#x03B1;</em> &#x00B7; OPT, and <span class="inline-equation"><span class="tex">$K = |\mathcal {M}| - 1$</span>      </span>.</p>    </div>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.4</span> Proof of Lemma&#x00A0;<a class="enc" href="#enc11">4.3</a>      </h3>     </div>    </header>    <div class="proof" id="proof4">     <Label>Proof Proof of Lemma&#x00A0;4.3</Label>     <p> We then prove that to manipulate the revenue of any <span class="inline-equation"><span class="tex">$M^{\prime } \in \mathcal {M}^{\prime }$</span>      </span>, one must make significant changes of her bids. Let&#x0027;s consider buyer <em>i</em> trying to manipulate the revenue by changing her bids in auction <em>j</em>, under the following cases:</p>     <p>      <strong>1.</strong>      <span class="inline-equation"><span class="tex">$v_{ij} \ge \tilde{r}$</span>      </span>      <em> and <em>v<sub>ij</sub>       </em> is the highest bid.</em> In this case, buyer <em>i</em> can only manipulate the revenue by lowering her bid. If the second highest bid is larger than <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span>, then the loss to the revenue is no more than <span class="inline-equation"><span class="tex">$(v_{ij} - \hat{v}_{ij})$</span>      </span>. Otherwise, the revenue loss can be as large as <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span> by bidding <span class="inline-equation"><span class="tex">$\hat{v}_{ij} {\lt} \tilde{r}$</span>      </span> so that the item is not sold at all. Hence the revenue loss in expectation is at most: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \int _{\max \lbrace \hat{v}_{ij}, \gamma r\rbrace }^{\min \lbrace v_{ij}, r\rbrace } s \cdot \frac{1}{(1 - \gamma)r} \mathrm{d}s \le \frac{(r + \gamma r)(v_{ij} - \hat{v}_{ij})}{2(1 - \gamma)r} = \frac{1}{1 - \gamma }(v_{ij} - \hat{v}_{ij}).\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>      <strong>2.</strong>      <span class="inline-equation"><span class="tex">$v_{ij} \ge \tilde{r}$</span>      </span>      <em> and <em>v<sub>ij</sub>       </em> is not the highest bid.</em> In this case, buyer <em>i</em> can manipulate the revenue by lowering her bid (to some value no less than <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span>) or raising her bid. In both case, the revenue change (either loss or gain) is bounded by <span class="inline-equation"><span class="tex">$|v_{ij} - \hat{v}_{ij}|$</span>      </span>.</p>     <p>      <strong>3.</strong>      <span class="inline-equation"><span class="tex">$v_{ij} {\lt} \tilde{r}$</span>      </span>      <em> and the highest bid is larger than </em>      <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span>      <em>.</em> In this case, buyer <em>i</em> can only manipulate the revenue by raising her bid and the revenue gain is at most <span class="inline-equation"><span class="tex">$(\hat{v}_{ij} - v_{ij})$</span>      </span>.</p>     <p>      <strong>4.</strong>      <span class="inline-equation"><span class="tex">$v_{ij} {\lt} \tilde{r}$</span>      </span>      <em> and the highest bid is less than </em>      <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span>      <em>.</em> In this case, buyer <em>i</em> can only manipulate the revenue by raising her bid above <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span> to win this item and the revenue gain is <span class="inline-equation"><span class="tex">$\tilde{r}$</span>      </span>. In expectation, at most: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \int _{\max \lbrace v_{ij}, \gamma r\rbrace }^{\min \lbrace \hat{v}_{ij}, r\rbrace } s \cdot \frac{1}{(1 - \gamma)r} \mathrm{d}s \le \frac{(r + \gamma r)(\hat{v}_{ij} - v_{ij})}{2(1 - \gamma)r} = \frac{1}{1 - \gamma }(\hat{v}_{ij} - v_{ij}).\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>In summary, the revenue change on each auction is bounded by a constant (1 &#x2212; <em>&#x03B3;</em>) times the absolute change of buyer <em>i</em>&#x2019;s bid, <span class="inline-equation"><span class="tex">$|v_{ij} - \hat{v}_{ij}|$</span>      </span>. For the entire market, the average revenue change <em>&#x03BB;<sub>i</sub>      </em> caused by buyer <em>i</em> is bounded by the absolute bid change across the whole market by buyer <em>i</em>: <span class="inline-equation"><span class="tex">$\lambda _i \le \frac{1}{(1 - \gamma)m} \sum _{j} |v_{ij} - \hat{v}_{ij}|$</span>      </span>.</p>     <p>The optimal revenue guarantee, however, is directly from the fact that the revenue of second price auction is a concave function as the reserve being the variable, i.e., &#x2200;<em>r</em> > <em>r</em>&#x2032; <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \text{revenue of SPA with }r^{\prime } \ge \frac{r^{\prime }}{r} \cdot \text{revenue of SPA with }r.\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.5</span> Proof of Lemma&#x00A0;<a class="enc" href="#enc12">4.4</a>      </h3>     </div>    </header>    <div class="proof" id="proof5">     <Label>Proof Proof of Lemma&#x00A0;4.4</Label>     <p> First of all, we only consider the effective misreports which have non-zero influence on the revenue of at least one mechanism. Otherwise, <em>&#x03BB;<sub>ij</sub>      </em> = 0 and we are done.</p>     <p>As we argued in Section&#x00A0;<a class="sec" href="#sec-14">4.2</a>, the more the buyer raises her bid, the more likely she will win the item at a price higher than her will and the more expensive price she will pay. So her utility loss tends to be quadratic in the magnitude of her misreport. Formally, consider the buyer <em>i</em> misreporting in auction <em>j</em>,</p>     <p>      <strong>1.</strong>      <em>She is the winner.</em> Then with probability <em>&#x03B2;</em>/2, the reserve is drawn from <span class="inline-equation"><span class="tex">$\mathrm{U}[0, \overline{r}]$</span>      </span> and she is not removed from the auction. Suppose she raises her bid to <span class="inline-equation"><span class="tex">$\hat{v}_{ij}$</span>      </span>. Then with probability <span class="inline-equation"><span class="tex">$(\hat{v}_{ij} - v_{ij}) \beta / 2\overline{r}$</span>      </span>, she will have to buy the item at a price <em>r</em> higher than her value <em>v<sub>ij</sub>      </em>. The utility loss in expectation is, <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \int _{v_{ij}}^{\hat{v}_{ij}} (s - v_{ij}) \cdot \frac{\beta }{2\overline{r}} \mathrm{d}s = \frac{\beta }{4\overline{r}} \cdot (\hat{v}_{ij} - v_{ij})^2.\end{align*} </span>       <br/>       </div>      </div> Note that if the original reserve is higher than <em>v<sub>ij</sub>      </em> and less than <span class="inline-equation"><span class="tex">$\hat{v}_{ij}$</span>      </span>, the utility loss will be much higher. Similarly, if she lowers her bid, the utility loss in expectation is at least this much.</p>     <p>      <strong>2.</strong>      <em>She is the buyer with the second highest value.</em> Then with probability <em>&#x03B2;</em>/4, the problem reduced to the previous case, i.e., the reserve price is reset, the highest bid is removed, and buyer <em>i</em> becomes the &#x201C;winner&#x201D; in among the remaining buyers. Hence we can establish similar lower bounds on utility losses.</p>     <p>Finally, the loss in revenue happens only when reserve price is reset with probability <em>&#x03B2;</em>, hence at most <em>&#x03B2;</em>OPT.</p>    </div>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.6</span> Generalization to continuous ground sets</h3>     </div>    </header>    <p>When the ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> is a continuous set (hence not finite), we can still extend the results for finite cases to such infinite cases as long as the objective function satisfies some continuity. Formally, the continuous ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> must be given with a non-negative measure <em>&#x03BC;</em> so that the randomized mechanisms can be defined, i.e., <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \textstyle \forall \mathcal {M}^{\prime } \subseteq \mathcal {M},~\Pr [M \in \mathcal {M}^{\prime }] = \int _{\mathcal {M}^{\prime }} f(\bar{W}(M)) \mathrm{d}\mu / \int _{\mathcal {M}} f(\bar{W}(M)) \mathrm{d}\mu .\end{align*} </span>       <br/>      </div>     </div> Note that without any continuous property, the probability measure of the exact optimal mechanism and any close-to-optimal mechanisms might be zero, so we cannot conclude any good approximation guarantee. But once some continuous property is satisfied by <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> the approximate optimality can be generalized to the continuous setting. For example, if the ground set <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span> satisfies the continuous property such that for any mechanism <em>M</em>, there always exists a small neighborhood of <em>M</em> with at least some constant probability measure that any mechanism in the neighborhood is approximately as good as <em>M</em>. Then intuitively, the ground set can be discretized with small loss in the objective and the techniques for the finite setting can be applied. In the auction setting we considered, by properly designing the ground set and the accompany probability measure, the sub-optimality can be bounded by <span class="inline-equation"><span class="tex">$O(\ln \ln (\overline{r} / \underline{r}))$</span>     </span>, where <span class="inline-equation"><span class="tex">$\overline{r}$</span>     </span> and <em>r</em> are the maximum and the minimum reserve prices being considered in <span class="inline-equation"><span class="tex">$\mathcal {M}$</span>     </span>.<a class="fn" href="#fn7" id="foot-fn7"><sup>4</sup></a>    </p>    </section>   </section>   <section id="sec-23">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Additional Related Work</h2>    </div>    </header>    <p>The learning problem in game theoretic environments has been studied in a variety of settings. The most related ones can be casted into three main categories.</p>    <p>    <em>Specific learning tasks with strategic agents.</em>Dekel et&#x00A0;al. and Meir et&#x00A0;al. study the most classic learning tasks, regression&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] and classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], in game theoretic environments with some mild assumptions. In particular, the objectives in both problems are restricted to minimizing the loss functions. In contrast to such specific learning tasks, we consider learning with arbitrary objective with strategic agents having arbitrary utility functions.</p>    <p>    <em>Revenue maximization with strategic agents and non-strategic samples.</em> Mainly two types of work on this setting. The major difference with ours is that their samples are not strategic.</p>    <p>The line of studies around prior-free auctions initiated by Cole and Roughgarden consider the design of approximately optimal mechanisms with oracle accesses to non-strategic samples of the prior distributions, while the number of such accesses is limited [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>].</p>    <p>Apart from the previous approach, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] consider mechanism design with accesses to samples of historical bids. In particular, the distribution of bids are different from the distribution of private values, because the mechanism could be non-truthful and the bids might only partially reflect the values when reserves are higher than the values. However, the bids are still assumed to be non-strategic with respect to the distribution learning process which would probably hurt the long term benefits of the agents.</p>    <p>    <em>Revenue maximization with strategic agents (no non-strategic samples).</em> There are more work that focus on the perspective of mechanism design. Lu et&#x00A0;al. study the revenue maximization under single-item and single-buyer setting without any prior assumption and provide a randomized mechanism that always guarantees logarithmic approximation to the optimal benchmark where the prior distribution is given [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>]. With the regularity assumption on prior distributions, Fu et&#x00A0;al. propose a randomized auction with better worst case approximation guarantee than second price auctions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>]. Amin et&#x00A0;al. consider learning better prices from historical bids in the repeated auction setting and show the revenue could be improved if the seller is more patient than buyers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. Kanoria and Nazerzadeh consider the long-term dynamic incentive compatibility in repeated auctions where the seller is learning the distribution from bids and show that substantial benefit can be obtained via learning when there is uncertainty in the distributions and competition among bidders [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. Recently, Tang and Zeng consider the repeated auction setting without making asymmetric patience assumptions and show that the game induced by Myerson&#x0027;s auction (among the buyers) is strategically equivalent to a first price auction under the standard model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>].</p>    <p>Finally, we also highlight the potential connections to literatures of mechanism design via differential privacy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>].</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Kareem Amin, Afshin Rostamizadeh, and Umar Syed. 2013. Learning prices for repeated auctions with strategic buyers. In <em>      <em>Advances in Neural Information Processing Systems</em></em>. 1169&#x2013;1177.</li>    <li id="BibPLXBIB0002" label="[2]">Kareem Amin, Afshin Rostamizadeh, and Umar Syed. 2014. Repeated contextual auctions with strategic buyers. In <em>      <em>Advances in Neural Information Processing Systems</em></em>. 622&#x2013;630.</li>    <li id="BibPLXBIB0003" label="[3]">M-F Balcan, Avrim Blum, Jason&#x00A0;D Hartline, and Yishay Mansour. 2005. Mechanism design via machine learning. In <em>      <em>Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on</em></em>. IEEE, 605&#x2013;614.</li>    <li id="BibPLXBIB0004" label="[4]">Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. 2017. Sample Complexity of Multi-Item Profit Maximization. <em>      <em>arXiv preprint arXiv:1705.00243</em>     </em>(2017).</li>    <li id="BibPLXBIB0005" label="[5]">Eric Budish. 2011. The combinatorial assignment problem: Approximate competitive equilibrium from equal incomes. <em>      <em>Journal of Political Economy</em>     </em>119, 6 (2011), 1061&#x2013;1103.</li>    <li id="BibPLXBIB0006" label="[6]">Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. 2015. Regret minimization for reserve prices in second-price auctions. <em>      <em>IEEE Transactions on Information Theory</em>     </em>61, 1 (2015), 549&#x2013;564.</li>    <li id="BibPLXBIB0007" label="[7]">Shuchi Chawla, Jason Hartline, and Denis Nekipelov. 2014. Mechanism design for data science. In <em>      <em>Proceedings of the fifteenth ACM conference on Economics and computation</em></em>. ACM, 711&#x2013;712.</li>    <li id="BibPLXBIB0008" label="[8]">Richard Cole and Tim Roughgarden. 2014. The sample complexity of revenue maximization. In <em>      <em>Proceedings of the 46th Annual ACM Symposium on Theory of Computing</em></em>. ACM, 243&#x2013;252.</li>    <li id="BibPLXBIB0009" label="[9]">Ofer Dekel, Felix Fischer, and Ariel&#x00A0;D Procaccia. 2010. Incentive compatible regression learning. <em>      <em>J. Comput. System Sci.</em>     </em>76, 8 (2010), 759&#x2013;777.</li>    <li id="BibPLXBIB0010" label="[10]">Nikhil&#x00A0;R Devanur, Zhiyi Huang, and Christos-Alexandros Psomas. 2015. The sample complexity of auctions with side information. <em>      <em>arXiv preprint arXiv:1511.02296</em>     </em>(2015).</li>    <li id="BibPLXBIB0011" label="[11]">Piotr Faliszewski, Edith Hemaspaandra, and Lane&#x00A0;A Hemaspaandra. 2010. Using complexity to protect elections. <em>      <em>Commun. ACM</em>     </em>53, 11 (2010), 74&#x2013;82.</li>    <li id="BibPLXBIB0012" label="[12]">Piotr Faliszewski and Ariel&#x00A0;D Procaccia. 2010. AI&#x0027;s war on manipulation: Are we winning?<em>      <em>AI Magazine</em>     </em>31, 4 (2010), 53&#x2013;64.</li>    <li id="BibPLXBIB0013" label="[13]">Amos Fiat, Anna Karlin, Elias Koutsoupias, and Angelina Vidali. 2013. Approaching utopia: strong truthfulness and externality-resistant mechanisms. In <em>      <em>Proceedings of the 4th conference on Innovations in Theoretical Computer Science</em></em>. ACM, 221&#x2013;230.</li>    <li id="BibPLXBIB0014" label="[14]">Hu Fu, Nicole Immorlica, Brendan Lucier, and Philipp Strack. 2015. Randomization beats second price as a prior-independent auction. In <em>      <em>Proceedings of the Sixteenth ACM Conference on Economics and Computation</em></em>. ACM, 323&#x2013;323.</li>    <li id="BibPLXBIB0015" label="[15]">Yannai&#x00A0;A Gonczarowski and Noam Nisan. 2016. Efficient Empirical Revenue Maximization in Single-Parameter Auction Environments. <em>      <em>arXiv preprint arXiv:1610.09976</em>     </em>(2016).</li>    <li id="BibPLXBIB0016" label="[16]">Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. 2016. Strategic classification. In <em>      <em>Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science</em></em>. ACM, 111&#x2013;122.</li>    <li id="BibPLXBIB0017" label="[17]">Zhiyi Huang, Yishay Mansour, and Tim Roughgarden. 2015. Making the most of your samples. In <em>      <em>Proceedings of the Sixteenth ACM Conference on Economics and Computation</em></em>. ACM, 45&#x2013;60.</li>    <li id="BibPLXBIB0018" label="[18]">Yash Kanoria and Hamid Nazerzadeh. 2014. Dynamic reserve prices for repeated auctions: Learning from bids. (2014).</li>    <li id="BibPLXBIB0019" label="[19]">Michael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ullman. 2014. Mechanism design in large games: Incentives and privacy. In <em>      <em>Proceedings of the 5th conference on Innovations in theoretical computer science</em></em>. ACM, 403&#x2013;410.</li>    <li id="BibPLXBIB0020" label="[20]">Fuhito Kojima and Parag&#x00A0;A Pathak. 2009. Incentives and stability in large two-sided matching markets. <em>      <em>The American Economic Review</em>     </em>99, 3 (2009), 608&#x2013;627.</li>    <li id="BibPLXBIB0021" label="[21]">Pinyan Lu, Shang-Hua Teng, and Changyuan Yu. 2006. Truthful auctions with optimal profit. In <em>      <em>International Workshop on Internet and Network Economics</em></em>. Springer, 27&#x2013;36.</li>    <li id="BibPLXBIB0022" label="[22]">Frank McSherry and Kunal Talwar. 2007. Mechanism design via differential privacy. In <em>      <em>Foundations of Computer Science, 2007. FOCS&#x2019;07. 48th Annual IEEE Symposium on</em></em>. IEEE, 94&#x2013;103.</li>    <li id="BibPLXBIB0023" label="[23]">Reshef Meir, Ariel&#x00A0;D Procaccia, and Jeffrey&#x00A0;S Rosenschein. 2012. Algorithms for strategyproof classification. <em>      <em>Artificial Intelligence</em>     </em>186 (2012), 123&#x2013;156.</li>    <li id="BibPLXBIB0024" label="[24]">Jamie&#x00A0;H Morgenstern and Tim Roughgarden. 2015. On the pseudo-dimension of nearly optimal auctions. In <em>      <em>Advances in Neural Information Processing Systems</em></em>. 136&#x2013;144.</li>    <li id="BibPLXBIB0025" label="[25]">Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay&#x00A0;V Vazirani. 2007. <em>      <em>Algorithmic game theory</em></em>. Vol.&#x00A0;1. Cambridge University Press Cambridge.</li>    <li id="BibPLXBIB0026" label="[26]">Kobbi Nissim, Rann Smorodinsky, and Moshe Tennenholtz. 2012. Approximately optimal mechanism design via differential privacy. In <em>      <em>Proceedings of the 3rd innovations in theoretical computer science conference</em></em>. ACM, 203&#x2013;213.</li>    <li id="BibPLXBIB0027" label="[27]">Michael Ostrovsky and Michael Schwarz. 2011. Reserve prices in internet advertising auctions: A field experiment. In <em>      <em>Proceedings of the 12th ACM conference on Electronic commerce</em></em>. ACM, 59&#x2013;60.</li>    <li id="BibPLXBIB0028" label="[28]">Pingzhong Tang and Yulong Zeng. 2016. How to manipulate truthful prior-dependent mechanisms?<em>      <em>CoRR</em>     </em>abs/1606.02409 (2016). <a class="link-inline force-break" target="_blank"     href="http://arxiv.org/abs/1606.02409">http://arxiv.org/abs/1606.02409</a></li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x002A;</sup></a>We thank the anonymous reviewers for their helpful comments.</p>   <p id="fn3"><a href="#foot-fn3"><sup>&#x2020;</sup></a>The work was done when this author was an intern at Google. This author was supported by grants: 2011CBA00300, 2011CBA00301, NSFC61033001, NSFC61361136003, NSFC61303077, NSFC61561146398, a Tsinghua Initiative Scientific Research Grant and a China Youth 1000-talent program.</p>   <p id="fn4"><a href="#foot-fn4"><sup>1</sup></a>A &#x201C;small&#x201D; agent might have significant impact to the optimization objective by adopting different strategies, but her willingness to misreport is <em>relatively small</em> compared with the optimal objective achievable by the designer within the ground set. Intuitively, it means that the power of the designer is relatively &#x201C;large&#x201D; compared with agents.</p>   <p id="fn5"><a href="#foot-fn5"><sup>2</sup></a>    <span class="inline-equation"><span class="tex">$\tanh (x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \le \min \lbrace 1, x\rbrace$</span>    </span>.</p>   <p id="fn6"><a href="#foot-fn6"><sup>3</sup></a>We omit the concrete approximation numbers because they involve some parameters for the &#x201C;perturbation&#x201D; that we have not introduced yet.</p>   <p id="fn7"><a href="#foot-fn7"><sup>4</sup></a>In the practice of ad auctions, <span class="inline-equation"><span class="tex">$\ln \ln (\overline{r} / \underline{r})$</span>    </span> will never be more than 3, or equivalently <span class="inline-equation"><span class="tex">$\overline{r} / \underline{r}$</span>    </span> never more than exp&#x2009;(<em>e</em>    <sup>3</sup>) &#x2248; 10<sup>8.7</sup>.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International (CC-BY-NC-ND&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY-NC-ND&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186042">https://doi.org/10.1145/3178876.3186042</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
