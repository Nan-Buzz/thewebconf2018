<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>RaRE: Social Rank Regulated Large-scale Network Embedding</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">RaRE: Social Rank Regulated Large-scale Network Embedding</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Yupeng</span>     <span class="surName">Gu</span>,     University of California, Los Angeles, Los Angeles, CA, <a href="mailto:ypgu@cs.ucla.edu">ypgu@cs.ucla.edu</a>    </div>    <div class="author">     <span class="givenName">Yizhou</span>     <span class="surName">Sun</span>,     University of California, Los Angeles, Los Angeles, CA, <a href="mailto:yzsun@cs.ucla.edu">yzsun@cs.ucla.edu</a>    </div>    <div class="author">     <span class="givenName">Yanen</span>     <span class="surName">Li</span>,     Snapchat Inc., Venice, CA, <a href="mailto:yanen.li@snap.com">yanen.li@snap.com</a>    </div>    <div class="author">     <span class="givenName">Yang</span>     <span class="surName">Yang</span>,     Zhejiang University, Hangzhou, China, <a href="mailto:yangya@zju.edu.cn">yangya@zju.edu.cn</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186102" target="_blank">https://doi.org/10.1145/3178876.3186102</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Network embedding algorithms that map nodes in a network into a low-dimensional vector space are prevalent in recent years, due to their superior performance in many network-based tasks, such as clustering, classification, and link prediction. The main assumption of existing algorithms is that the learned latent representation for nodes should preserve the structure of the network, in terms of first-order or higher-order connectivity. In other words, nodes that are more similar will have higher probability to connect to each other. This phenomena is typically explained as homophily in network science. However, there is another factor usually neglected by the existing embedding algorithms, which is the popularity of a node. For example, celebrities in a social network usually receive numerous followers, which cannot be fully explained by the similarity of the two users. We denote this factor with the terminology &#x201C;social rank&#x201D;. We then propose a network embedding model that considers both of the two factors in link generation, and learn proximity-based embedding and social rank-based embedding separately. Rather than simply treating these two factors independent with each other, a carefully designed link generation model is proposed, which explicitly models the interdependency between these two types of embeddings. Experiments on several real-world datasets across different domains demonstrate the superiority of our novel network embedding model over the state-of-the-art methods.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Network embedding</small>, </span>     <span class="keyword">      <small> social rank</small>, </span>     <span class="keyword">      <small> representation learning</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yupeng Gu, Yizhou Sun, Yanen Li, and Yang Yang. 2018. RaRE: Social Rank Regulated Large-scale Network Embedding. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018 (WWW 2018),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186102" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186102</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Studying the latent representation of nodes in information networks has been a prevalent topic recently. Latent representations, also known as latent features or embeddings, often reside in a lower dimensional continuous vector space, and are especially helpful in terms of understanding the nodes. A wide variety of applications can be achieved as a result, including classification, visualization, community detection and so on.</p>    <p>Early methods include mapping nodes onto a lower dimensional manifold by finding the intrinsic dimensionality using spectral methods on graph adjacency matrix, such as locally linear embedding (LLE) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], Isomap [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>], multidimensional scaling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] and so on. Most of these methods do not scale for large networks. Later on more principled statistical models have been developed where node parameters are deduced by optimizing some global objective function. The intuition is rather straightforward: similarity in the graph should be preserved in the lower dimensional space. Proximity in the graph is usually embodied as neighbors: according to the homophily assumption [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>], entities that are connected in the graph represent some sort of similarity. Neighbors are also essential in random walk based methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>], where information and label propagate. As a result, entities that are connected in the original graph are often adjacent to each other in the latent space.</p>    <p>However, this seemingly plausible approach has some severe drawbacks and naturally triggers several open questions. First of all, <em>is it consistently true that all links occur between similar nodes</em>? As per the preferential attachment process in network generation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], nodes are believed to have a higher chance to connect to (e.g. follow) high-degree nodes (e.g. celebrities) in general. For example, a new Twitter user may first choose to follow some well-known politicians or movie stars, regardless of being a fan or not. Those links are generated due to the high exposure and popularity of certain accounts, rather than their similar tastes (i.e. proximity of latent representation). For instance, famous politicians usually have a fair amount of followers on social networks, but certainly not 100% of the followers should be considered as similar to them in terms of political opinions. On the other hand, everyone has limited amount of energy and resources, which prevents many actually similar pairs from being present. During a literature review, famous groups of scholars in the corresponding field of study are usually considered first, while a vast majority of junior groups may be ignored, even though they are working on very similar topics. With all being said, node features inferred according to the homophily assumption will be a mixture of popularity and proximity factors, and thus are not desirable for clustering or classification tasks. In this work, we use a specific terminology &#x201C;social rank&#x201D; to denote the popularity factor, namely the position where an entity is ranked among the network, and &#x201C;proximity-based representation&#x201D; to denote the general embedding vector which denotes the opinions and preferences of an entity.</p>    <p>The second question is, <em>should these two factors be considered as totally independent with each other</em>? From the case studies in the above question, we notice that how much proximity will contribute to link formation depends on the relative social status/rank between a pair of nodes. On one hand, when a link from a node to a more popular one is observed, it is somewhat likely to be explained by the popularity of the latter. On the other hand, when a link from a node to a less popular one is present, proximity factor should account for most of the intentions. For example, in a bibliography citation network, it is often more common for papers to cite very famous papers due to their substantial public attention. However, when a less popular paper is cited, it is almost certainly the case that it is essentially very relevant to the work. In sum, homophily itself does not suffice to explain the reason behind link generation, and we should decide <em>the extent</em> to which homophily is trusted considering the social rank of nodes. A principled methodology is desired to balance the effect of social rank and proximity factor in terms of network generation.</p>    <p>In this paper we propose a Social <span style="text-decoration: underline;">R</span>    <span style="text-decoration: underline;">a</span>nk <span style="text-decoration: underline;">R</span>egulated Network <span style="text-decoration: underline;">E</span>mbedding (RaRE) model which incorporates both latent <em>rank factor</em> and latent <em>proximity-based factor</em> to interpret the network generation process. Our unified Bayesian framework models the probabilistic relationship between social rank, proximity-based representation and existence of a link. We discuss what portion is truly justified by the proximity between nodes given their social rank difference, which explains the generation of a link from a brand new perspective. Our method is also scalable to large networks. The contribution of our method can be summarized as follows:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">We propose to solve the network embedding problem from a novel Bayesian perspective, which integrates both social rank and proximity-based embedding.<br/></li>    <li id="list2" label="&#x2022;">A brand new probabilistic link formation model is formulated that explicitly models the extent of contribution of proximity-based embedding under different relative social rank difference.<br/></li>    <li id="list3" label="&#x2022;">Our method is easily scalable to real-world large-scale networks which consist of millions of nodes.<br/></li>    </ul>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Problem Definition</h2>    </div>    </header>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Background</h3>     </div>    </header>    <p>Extracting latent representation of nodes (also known as embedding) in an information network is essential in understanding the relative position of each node in the network. A natural embedding is the row vector in the adjacency matrix where each dimension denotes the link status between a pair of nodes. Nevertheless, this plain strategy is seldom applicable to real-world tasks due to the computational complexity brought by its high dimensional representation, as well as its low representation power in terms of preserving network structure. In order to tackle this problem, traditional approaches extract dimensions with the biggest contribution to the data (e.g. PCA, SVD, IsoMap), or find lower dimensional representation of nodes by factorizing the adjacency matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>].</p>    <p>More recent approaches introduce the notion of &#x201C;node embedding&#x201D;, a low-dimensional vector representation of node, which embodies the latent merits and characteristics of an individual. The concept of embedding is very similar to word embeddings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>], where every word is represented by a low dimensional vector. These embedding vectors are learned by preserving similarity in the corpus (i.e. between every word and its context) and similarity in the latent space (i.e. vector dot product). Levy and Goldberg [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] also reveal the connection between matrix factorization and word embedding, arguing that estimating word embedding is equivalent to factorizing a pointwise mutual information matrix. In the realm of information networks, the concept of &#x201C;context&#x201D; no longer exists, and many researchers have proposed ways to define similar nodes in the network, such as <em>n</em>-hop neighbors or the nodes reachable from a random walk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>]. Besides, the generation process also allows great flexibility in the modeling part, and different link generation approaches have been proposed [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>]. In sum, the lower dimensional representations are much more succinct while keeping the majority of information in the graph.</p>    <p>However, none of the network embedding approaches explicitly interpret the meaning of the representation, and assume the link is generated based on the proximity of representations, or that proximity can propagate through links. We argue that, there is another essential factor (&#x201C;social rank&#x201D;) other than proximity that leads to the formation of a link, which is not homogeneous for connected nodes and should be detached from the general proximity-based embedding. Some matrix factorization methods model popularity in terms of bias (e.g. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]), but they simply treat proximity and popularity as two independent factors. Instead, we model the interdependency of the two factors in link formation explicitly. We will explain the necessity of modeling their interdependency in Section <a class="sec" href="#sec-9">3</a> and define our problem formally in the next paragraph.</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Problem Definition</h3>     </div>    </header>    <p>We define our embedding problem as follows. An information network can be formatted as <em>G</em> = (<em>V</em>, <em>E</em>), where <span class="inline-equation"><span class="tex">$V = \lbrace u_n \rbrace _{n=1}^{N}$</span>     </span> is the set of vertices and <em>E</em>&#x2282;<em>V</em>     <sup>2</sup> is the set of edges. We use <em>e<sub>ij</sub>     </em> to denote the binary status of the link from <em>u<sub>i</sub>     </em> to <em>u<sub>j</sub>     </em> for unweighted networks, or the multiplicity of the link for weighted networks. Our goal is to infer both latent <em>proximity-based</em> representation <span class="inline-equation"><span class="tex">$\lbrace {z}_v | v \in V\rbrace \subset \mathbb {R}^{K}$</span>     </span> and latent <em>rank</em> representation <span class="inline-equation"><span class="tex">$\lbrace r_v | v \in V\rbrace \subset \mathbb {R^{+}}$</span>     </span> for nodes in the network. Similar to the ordinal numbers, our ranking assumes a higher social rank for a smaller value of <em>r</em>, and requires that all social ranks are positive. The rank <em>r</em> can also be considered as radius of a node in the visualization, where center nodes (smaller radius) are most influential.</p>    </section>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Approach</h2>    </div>    </header>    <p>In this section, we illustrate our approach by introducing a general form of network generation prototype for unweighted networks, investigate the details, instantiate our final model using mathematical derivations, and extend it to general networks. We will discuss its scalability and relationship to existing models as well.</p>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Base Model</h3>     </div>    </header>    <p>The most general model for the binary status of a link <em>e<sub>ij</sub>     </em> (1 if present, 0 otherwise) is a Bernoulli event with parameter <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(e_{ij}=1 | r_i, r_j, {z}_i, {z}_j) = f(r_i, r_j, {z}_i, {z}_j) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <em>f</em>() is a probability function to be designed. First, we would like <em>f</em>() to encode the difference of the social ranks, and the most natural measure is the difference <em>r<sub>i</sub>     </em> &#x2212; <em>r<sub>j</sub>     </em>. Besides, a recent work [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] has also found that the probability of a friendship is a function of the difference of two users&#x2019; social ranks (independent of their absolute values), meaning that the function should be translation-invariant. We adopt their assumption and thus always study the difference of two entities&#x2019; ranks (denoted by <em>dr</em> = <em>r<sub>i</sub>     </em> &#x2212; <em>r<sub>j</sub>     </em>) in function <em>f</em>(). For the proximity-based representation, the homophily assumption has been widely used and accepted in related studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>], which posits that in information networks, most interactions occur between nodes with similar merits and characteristics. Therefore, with the hope that the representation z is a reflection of people&#x0027;s hidden characteristics, we design the probability to be a function of their Euclidean distance <em>dz</em> = ||z<sub>      <em>i</em>     </sub> &#x2212; z<sub>      <em>j</em>     </sub>||<sub>2</sub> in the lower dimensional space. In sum, Equation <a class="eqn" href="#eq1">1</a> becomes: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(e_{ij}=1 | r_i, r_j, {z}_i, {z}_j) = f(dr, dz) \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>    </p>    <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig1.svg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Left: traditional embedding models (e.g. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>]), where <em>s</em>(&#x00B7;, &#x00B7;) measures vector similarity. Right: graphical model representation of our model. Shadowed unit <em>e<sub>ij</sub>       </em> represents the observed variable (i.e. the status of the link).</span>     </div>    </figure>    <p>A graphical model representation is illustrated in Figure <a class="fig" href="#fig1">1</a>. It would be controversial about the exact form of <em>f</em>() at this stage, therefore in order to have a more convincing formulation, we will investigate the distribution of parameters under different circumstances (i.e. when the link is present/absent) and derive <em>f</em>() from a Bayesian perspective.</p>    <p>To reach the concrete form of the probability function, we define the conditional distribution <em>p</em>(<em>dr</em>, <em>dz</em>|<em>e<sub>ij</sub>     </em>) = <em>p</em>(<em>dr</em>|<em>dz</em>, <em>e<sub>ij</sub>     </em>) &#x00B7; <em>p</em>(<em>dz</em>|<em>e<sub>ij</sub>     </em>) in Section <a class="sec" href="#sec-11">3.2</a> and the prior distribution <em>p</em>(<em>r</em>) and <em>p</em>(z) in Section <a class="sec" href="#sec-12">3.3</a>. Finally, the exact form of <em>f</em>() is determined by Bayes&#x2019; rule in Section <a class="sec" href="#sec-13">3.4</a>.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Conditional Distributions</h3>     </div>    </header>    <p>     <em>When the link is present,</em>. it is generally due to two reasons: (1) the link-receiver is famous (or at least more famous than the link-sender); or (2) the two individuals are similar (homophily). In other words, a majority of the links occur between pairs of nodes (<em>u<sub>i</sub>     </em>, <em>u<sub>j</sub>     </em>) where <em>dr</em> = <em>r<sub>i</sub>     </em> &#x2212; <em>r<sub>j</sub>     </em> is positive or <em>dz</em> = ||z<sub>      <em>i</em>     </sub> &#x2212; z<sub>      <em>j</em>     </sub>||<sub>2</sub> is small. From the graphical model in Figure <a class="fig" href="#fig1">1</a> (right), <em>dr</em> and <em>dz</em> are no longer independent when we have knowledge about the link <em>e<sub>ij</sub>     </em>, which is referred to as &#x201C;explaining away&#x201D; in Bayesian networks. In other words, <em>p</em>(<em>dr</em>, <em>dz</em>|<em>e<sub>ij</sub>     </em>) &#x2260; <em>p</em>(<em>dr</em>|<em>e<sub>ij</sub>     </em>) &#x00B7; <em>p</em>(<em>dz</em>|<em>e<sub>ij</sub>     </em>) because they are conditionally dependent on <em>e<sub>ij</sub>     </em>. In particular, given the presence of a link, dissimilarity of two users (i.e. large <em>dz</em>) will increase our belief that <em>j</em> is more popular than <em>i</em> (i.e. positive <em>dr</em>). For example, sportsmen followed by a non-sporty person are likely to be very influential. On the other hand, proximity of z (i.e. small <em>dz</em>) will eliminate some possibility that <em>j</em> is popular (i.e. positive <em>dr</em>), thus shifting the mean of <em>dr</em> towards left. For example, followees of a sportholic might as well be some unspectacular players in his/her home team. Therefore, we assume the distribution of <em>dr</em> is a Gaussian with a positive mean, and <em>dz</em> follows a (truncated) Gaussian distribution where the peak is at <em>dz</em> = 0: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(dr | dz, e_{ij} = 1) = \mathcal {N}(\mu \cdot h(dz), \sigma _R^2) \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>     <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(dz | e_{ij} = 1) = \frac{1}{\mathcal {Z}} \cdot I_{\mathbb {R}^{+}} (dz) \cdot \mathcal {N}(0, \sigma _1^2) \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <em>&#x03BC;</em> &#x00B7; <em>h</em>(<em>dz</em>) (<em>&#x03BC;</em> > 0) is the mean of <em>dr</em> conditioned on <em>dz</em> and <em>e<sub>ij</sub>     </em> = 1, <span class="inline-equation"><span class="tex">$\mathcal {Z}$</span>     </span> is the normalization term (<span class="inline-equation"><span class="tex">$\mathcal {Z}=1/2$</span>     </span> obviously) and <em>I<sub>S</sub>     </em>(<em>x</em>) is the indicator function (takes value 1 if <em>x</em> &#x2208; <em>S</em> and 0 otherwise). The hyper-parameter <em>&#x03BC;</em> controls the scale of the mean, while <em>h</em>(<em>dz</em>) adjusts the mean with respect to different values of <em>dz</em>.</p>    <p>Ideally, <em>dr</em> and <em>dz</em> in the above equations should follow our previous intuition exactly. Therefore, function <em>h</em>() should obey the following properties:</p>    <ul class="list-no-style">     <li id="list4" label="&#x2022;">Non-negative, bounded and supported on <span class="inline-equation"><span class="tex">$\mathbb {R}^{+}$</span>      </span>. In general, most of existing links are likely to occur from a node to a more influential one (i.e., <em>dr</em> = <em>r<sub>i</sub>      </em> &#x2212; <em>r<sub>j</sub>      </em> &#x2265; 0). In other words, no matter what value <em>dz</em> takes, the mean of the rank difference (given the link is present) <span class="inline-equation"><span class="tex">$\mathbb {E}[dr]=\mu \cdot h(dz)$</span>      </span> should always be non-negative.<br/></li>     <li id="list5" label="&#x2022;">Non-decreasing monotonic. Given the existence of a link, the relative rank difference between two nodes should be more significant as their dissimilarity increases. This corresponds to the &#x201C;explaining away&#x201D; effect above.<br/></li>     <li id="list6" label="&#x2022;">Concave on some right-unbounded interval. From our intuition, as <em>dz</em> increases, the marginal gain of <span class="inline-equation"><span class="tex">$\mathbb {E}[dr]=\mu \cdot h(dz)$</span>      </span> should be diminishing. In other words, <em>h</em> should be a concave function when <em>dz</em> exceeds some threshold.<br/></li>    </ul>    <p>Lots of families of functions have these properties, but a simple one of them we find to work well is <span class="inline-equation"><span class="tex">$h(dz) = \frac{dz^2}{1+dz^2}$</span>     </span>. An illustration of the conditional probability distributions of <em>dr</em> (given <em>dz</em> and <em>e<sub>ij</sub>     </em> = 1) and <em>dz</em> (given <em>e<sub>ij</sub>     </em> = 1) is shown in Figure <a class="fig" href="#fig2">2</a> (left).</p>    <p>     <em>When the link is absent,</em>. it does not necessarily mean that the two individuals are dissimilar, or one of them is not popular enough to be observed. In other words, there are various reasons why no link is observed between two nodes. Therefore, without much confidence in claiming any special characteristic of an absent link, we assume a Gaussian distribution on <em>dr</em> and <em>dz</em>, both centered at 0. In other words, <em>dr</em> and <em>dz</em> will have equal chance of being positive and negative. Meanwhile, the variance of <em>dz</em> should be large as we expect the distribution to be more uniform. <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(dr | dz, e_{ij} = 0) = \mathcal {N}(0, \sigma _R^2) \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>     <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(dz | e_{ij} = 0) = \frac{1}{\mathcal {Z}} \cdot I_{\mathbb {R}^{+}} (dz) \cdot \mathcal {N}(0, \sigma _2^2) \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathcal {Z}=1/2$</span>     </span> is the normalization term.</p>    <p>Note that we assume <span class="inline-equation"><span class="tex">$\sigma _2^2 {\gt} \sigma _1^2$</span>     </span> (Equation <a class="eqn" href="#eq4">4</a> and <a class="eqn" href="#eq6">6</a>), as the probability distribution function of <em>dz</em> should look much more flat when the link is absent (illustrated in Figure <a class="fig" href="#fig2">2</a> (b)). For simplicity, we assume the variance of <em>dr</em> remains the same as <span class="inline-equation"><span class="tex">$\sigma _R^2$</span>     </span> for the two scenarios. <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Conditional probability distributions of variable <em>dr</em> given <em>dz</em> and <em>e<sub>ij</sub>       </em>, and variable <em>dz</em> given <em>e<sub>ij</sub>       </em>.</span>      </div>     </figure>    </p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Prior Distributions</h3>     </div>    </header>    <p>Social rank <em>r</em> reflects the ranking of actors in a network, and the most popular entities are assumed to have the smallest <em>r</em> values. Intuitively, influential nodes should always be fewer than ordinary nodes, which means there should be more nodes with large <em>r</em> values. Since power law is usually utilized to model a node&#x0027;s characteristics (e.g. ranks of individuals, foraging patterns of many species, etc.), we use the (truncated) long-tailed power law distribution as the prior distribution of the inverse of social rank, namely <span class="inline-equation"><span class="tex">$p(r) \sim (1/r)^{-k_R}$</span>     </span> (<em>k<sub>R</sub>     </em> > 0). Mathematically, a power law cannot be a well-defined probability distribution, but a distribution that is a truncated power law is possible: <span class="inline-equation"><span class="tex">$p(r) = C \cdot (1/r)^{-k_R}$</span>     </span> when 1/<em>r</em> > <em>r<sub>min</sub>     </em>. It is easy to reveal that the normalization factor <span class="inline-equation"><span class="tex">$C = (k_R+1) \cdot r_{min}^{k_R+1}$</span>     </span>, and thus <span class="inline-equation"><span class="tex">$p(r) = (k_R+1) \cdot r_{min}^{k_R+1} \cdot (1/r)^{-k_R}$</span>     </span> (0 < <em>r</em> < 1/<em>r<sub>min</sub>     </em>).</p>    <p>Proximity-based representation z should generally lie more uniformly on the <em>K</em> dimensional space, preferably not too far away from the origin. Therefore, a Gaussian prior is assumed on z: <span class="inline-equation"><span class="tex">$p({z}) = \mathcal {N}(0, \sigma _Z^2 \cdot I_K)$</span>     </span>, where <em>I<sub>K</sub>     </em> denotes the <em>K</em> dimensional identity matrix. An illustration of the prior distributions is shown in Figure <a class="fig" href="#fig3">3</a>. <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Left: prior of <em>r</em>. Right: prior of z (2D is used for visualization purpose).</span>      </div>     </figure>    </p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Objective Function and Optimization</h3>     </div>    </header>    <p>Given the conditional and prior distribution of variables, we can formalize the function <em>f</em>() in the beginning (Equation (<a class="eqn" href="#eq2">2</a>)) using Bayes&#x2019; rule, and thus finalize our objective. <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026; p(e_{ij} = 1 | dr, dz) = \frac{p(dr, dz, e_{ij}=1)}{\sum _{e=0}^{1} p(dr, dz, e_{ij}=e)} \\ =&#x0026; \frac{p(dr | dz, e_{ij} = 1) \cdot p(dz | e_{ij} = 1) \cdot p(e_{ij} = 1)}{\sum _{e=0}^{1} p(dr | dz, e_{ij} = e) \cdot p(dz | e_{ij} = e) \cdot p(e_{ij} = e)} = sigmoid(f_{ij}) \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> where <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} f_{ij} = \log \frac{p(dr | dz, e_{ij} = 1) \cdot p(dz | e_{ij} = 1) \cdot p(e_{ij} = 1)}{p(dr | dz, e_{ij} = 0) \cdot p(dz | e_{ij} = 0) \cdot p(e_{ij} = 0)} \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> and <em>sigmoid</em>(<em>x</em>) = 1/(1 + <em>e</em>     <sup>&#x2212; <em>x</em>     </sup>). Combining Equations (<a class="eqn" href="#eq3">3</a>)-(<a class="eqn" href="#eq6">6</a>), we have <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} f_{ij} &#x0026;= \frac{\mu }{\sigma _R^2} \cdot dr \cdot h(dz) - \frac{\mu ^2}{2\sigma _R^2} \cdot h^2(dz) - (\frac{1}{2\sigma _1^2} - \frac{1}{2\sigma _2^2}) (dz)^2 \\ &#x0026;+ \log \frac{\sigma _2 \cdot p(e_{ij} = 1)}{\sigma _1 \cdot p(e_{ij} = 0)} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>    </p>    <p>Since <em>h</em>(<em>dz</em>) < 1, we ignore the insignificant second order term <em>h</em>     <sup>2</sup>(<em>dz</em>) as an approximation for now. For short, we use <em>&#x03BB;<sub>R</sub>     </em> to denote <span class="inline-equation"><span class="tex">$\frac{\mu }{\sigma _R^2}$</span>     </span> (<em>&#x03BC;</em> > 0&#x21D2;<em>&#x03BB;<sub>R</sub>     </em> > 0), <em>&#x03BB;<sub>Z</sub>     </em> to denote <span class="inline-equation"><span class="tex">$\frac{1}{2\sigma _1^2} - \frac{1}{2\sigma _2^2}$</span>     </span> (<span class="inline-equation"><span class="tex">$\sigma _1^2 {\lt} \sigma _2^2 \Rightarrow \lambda _Z {\gt} 0$</span>     </span>) and <em>&#x03BB;</em>     <sub>0</sub> to denote <span class="inline-equation"><span class="tex">$\log \frac{\sigma _2 \cdot p(e_{ij} = 1)}{\sigma _1 \cdot p(e_{ij} = 0)}$</span>     </span> (for sparse networks, <em>p</em>(<em>e<sub>ij</sub>     </em> = 1) &#x226A; <em>p</em>(<em>e<sub>ij</sub>     </em> = 0)&#x21D2;<em>&#x03BB;</em>     <sub>0</sub> < 0). Then the equation above is altered to the following: <div class="table-responsive" id="Xeq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} f_{ij} = \lambda _R \cdot dr \cdot h(dz) - \lambda _Z \cdot (dz)^2 + \lambda _0 \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> where <em>&#x03BB;<sub>R</sub>     </em>, <em>&#x03BB;<sub>Z</sub>     </em>, <em>&#x03BB;</em>     <sub>0</sub> are hyper-parameters that need to be pre-assigned. Intuitively, large <em>&#x03BB;<sub>R</sub>     </em> indicates the rank factor is more important in the network; while a large <em>&#x03BB;<sub>Z</sub>     </em> indicates the proximity-based factor is more important. <em>&#x03BB;</em>     <sub>0</sub> reflects the sparsity of the network. We find that our model is not sensitive to <em>&#x03BB;</em>     <sub>0</sub>, and the optimal values of <em>&#x03BB;<sub>R</sub>     </em> and <em>&#x03BB;<sub>Z</sub>     </em> can be found using cross validation.</p>    <p>As an extension to weighted graphs (denoting the weight of a link by <em>w<sub>ij</sub>     </em>), we treat the weight as multi-edges, and consider each edge independently. Thus the probability of observing a weighted edge is simply generalized to <div class="table-responsive" id="Xeq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(e_{ij}=w_{ij} | dr,dz) := p(e_{ij}=1 | dr,dz)^{w_{ij}} . \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div>    </p>    <p>The model parameters are inferred using maximum a posteriori (MAP) estimation, i.e., <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026; \lbrace r^*, {z}^*\rbrace = \mathop{argmax}\limits _{r, {z}} \log p(r, {z} | G) \\ =&#x0026; \mathop{argmax}\limits _{r, {z}} \log p(G | r, {z}) + \log p(r, {z}) \\ =&#x0026; \mathop{argmax}\limits _{r, {z}} \log p(G | r, {z}) + \sum _{i=1}^{N} \log p(r_i) + \sum _{i=1}^{N} \log p({z_i}) . \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(12)</span>      </div>     </div>    </p>    <p>Since each entity generally has more freedom to issue links (e.g. a webpage can contain arbitrary many hyperlinks; a person can follow/retweet as many people/tweets as she wants), in order to eliminate the effect of size, we define the weight <em>w<sub>ij</sub>     </em> as the number of occurrence from <em>i</em> to <em>j</em> normalized by <em>i</em>&#x2019;s out-degree: <span class="inline-equation"><span class="tex">$w_{ij} = \#(i \rightarrow j) / deg_{out}(i)$</span>     </span>.</p>    <p>Edges in the graph are assumed to be generated independently. Therefore the likelihood of the graph is simply the product of the probabilities of all edges. Negative sampling is adopted in consideration of the efficiency issue [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]; in other words, for each existing link <em>e<sub>ij</sub>     </em>, <em>k</em> non-existing links are sampled <span class="inline-equation"><span class="tex">$\lbrace e_{il}=0 \rbrace _{l \sim \mathcal {P}_0}$</span>     </span>. According to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>], the background probability <span class="inline-equation"><span class="tex">$\mathcal {P}_0$</span>     </span> is set to <span class="inline-equation"><span class="tex">$\mathcal {P}_0(n) \propto deg_{out}(n)^{0.75}$</span>     </span>. Denoting the sampled negative links by <span class="inline-equation"><span class="tex">$S_0 = \cup _i \lbrace (i,l) | e_{il}=0 \rbrace _{l \sim \mathcal {P}_0}$</span>     </span>, Equation (<a class="eqn" href="#eq7">12</a>) becomes <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026; \lbrace r^*, {z}^*\rbrace = \mathop{argmax}\limits _{r, {z}} \Big (\sum _{(i,j):e_{ij}{\gt}0} w_{ij} \cdot \log p(e_{ij}=1 | dr,dz) \\ +&#x0026; \sum _{(i,j) \in S_0} \log \big (1-p(e_{ij}=1 | dr,dz) \big) ~+ \sum _{i=1}^{N} \log p(r_i) + \sum _{i=1}^{N} \log p({z_i}) \Big) \\ =&#x0026; \mathop{argmax}\limits _{r, {z}} \Big (\sum _{(i,j):e_{ij}{\gt}0 | dr,dz} w_{ij} \cdot \log sigmoid(f_{ij}) \\ +&#x0026; \sum _{(i,j) \in S_0} \log sigmoid(-f_{ij}) ~+ \sum _{i=1}^{N} \log p(r_i) + \sum _{i=1}^{N} \log p({z_i}) \Big) . \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(13)</span>      </div>     </div>    </p>    <p>The optimization is done using stochastic gradient ascent.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Complexity</h3>     </div>    </header>    <p>The first two summations in Equation (<a class="eqn" href="#eq8">13</a>) consist of <em>O</em>(<em>E</em> &#x00B7; <em>K</em>) terms, where <em>E</em> is the number of edges in the graph, and <em>K</em> is the dimension of the proximity-based factor. The last two summations in Equation (<a class="eqn" href="#eq8">13</a>) consist of <em>O</em>(<em>N</em> &#x00B7; <em>K</em>) terms, where <em>N</em> is the number of nodes. Therefore the computational complexity for each epoch of the data is <em>O</em>((<em>E</em> + <em>N</em>) &#x00B7; <em>K</em>). In sum, the running time is linear to the number of edges and nodes, therefore is scalable to large networks. In practice, it takes only a few epoches to reach convergence.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.6</span> Relation to Other Models</h3>     </div>    </header>    <p>Our model can be treated as a generalization of the well established Bradley-Terry model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] in the realm of ranking and pairwise comparison, which has been successfully applied in learning to rank [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. One parametrization of the Bradley-Terry model estimates the probability that the pairwise comparison <em>i</em>&#x227B;<em>j</em> (interpreted as &#x201C;<em>i</em> is preferred to <em>j</em>&#x201D;, or &#x201C;<em>i</em> ranks higher than <em>j</em>&#x201D;) is true as <div class="table-responsive" id="Xeq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(i \succ j) = \frac{e^{s_i}}{e^{s_i}+e^{s_j}} = sigmoid(s_i-s_j) \end{equation} </span>       <br/>       <span class="equation-number">(14)</span>      </div>     </div> where <em>s<sub>i</sub>     </em> is a real-valued score assigned to <em>i</em> and will be inferred. <em>s<sub>i</sub>     </em> can be treated as the rank of <em>i</em>; for example, if <em>s<sub>i</sub>     </em> > <em>s<sub>j</sub>     </em>, then <em>P</em>(<em>i</em>&#x227B;<em>j</em>) > 0.5. This is essentially the same as the probability of a link <em>p</em>(<em>e<sub>ji</sub>     </em> = 1) in a special case of our model, where <em>&#x03BB;<sub>R</sub>     </em> = 1, <em>&#x03BB;<sub>Z</sub>     </em> = <em>&#x03BB;</em>     <sub>0</sub> = 0 and <em>h</em>(<em>dz</em>) = 1 (constant).</p>    </section>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>    </div>    </header>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>    </header>    <p>We use the following real-world datasets from different domains to test our new embedding algorithm:</p>    <ul class="list-no-style">     <li id="list7" label="&#x2022;">Snapchat Friendship. Snapchat is a US based ephemeral photo-messaging application developed by camera company Snap Inc. Users can make bi-directional friend links with others. We extract all friendship relations from a relatively small and isolated country, resulting in a network with about 1.5 million total nodes (users) and about 66 millions edges (bi-directional friendship links).<br/></li>     <li id="list8" label="&#x2022;">Tencent Weibo Retweet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0048">48</a>]. Tencent weibo is a Chinese microblogging service where users can post tweets and follow/retweet from others. We extract the complete retweet relationships on November 1st, 2011. Tweets that have been retweeted for less than 5 times are excluded. A directed edge (<em>u<sub>A</sub>      </em>, <em>u<sub>B</sub>      </em>) is added when a user <em>u<sub>A</sub>      </em> retweets from user <em>u<sub>B</sub>      </em>.<br/></li>     <li id="list9" label="&#x2022;">Venue Citation: We extract the paper citation links in the computer science domain and build a venue citations network from the Microsoft Academic Graph data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0036">36</a>]. A link from <em>venue<sub>A</sub>      </em> to <em>venue<sub>B</sub>      </em> indicates a citation from a paper published in <em>venue<sub>A</sub>      </em> to a paper published in <em>venue<sub>B</sub>      </em>. We do not differentiate each proceeding of the venues. Links are aggregated over a 10-year period of time (2007-2016) and it naturally becomes a weighted graph. Venues are manually labeled according to their field of study, and we look into the following eight categories: AI (artificial intelligence and machine learning), NET (network), SE (software engineering), CT (computer theory), CV (computer vision and graphics), DB (database), PL (programming languages) and DM (data mining). Some venues may have multiple labels.<br/></li>     <li id="list10" label="&#x2022;">Wikipedia Hyperlink<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. An edge from <em>i</em> to <em>j</em> represents a hyperlink from wikipage <em>i</em> to wikipage <em>j</em>. Wikipages and their categories are structured in a collaborative hierarchical framework, and the folksonomy information is further cleaned according to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0007">7</a>]. We only keep wikipages with clear hierarchical labels in the previous step. Top six categories that contain most webpages are picked for our classification task: <em>sports</em>, <em>politics</em>, <em>science</em>, <em>Christian</em>, <em>geography</em> and <em>musician</em>. Some wikipages may have multiple labels. Edge multiplicity is not available for this dataset.<br/></li>     <li id="list11" label="&#x2022;">Wikipedia Clickstream<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>. This dataset contains counts of (referer, resource) pairs extracted from the HTTP request logs of Wikipedia during Jan. 2017, where people navigate from one wikipage (i.e. referer) to another (i.e. resource). Node labels are obtained in the same way as the Wikipedia Hyperlink dataset.<br/></li>    </ul>    <p>The details about the above datasets can be found in Table <a class="tbl" href="#tab1">1</a>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Dataset Statistics.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Dataset</th>       <th style="text-align:center;">#Nodes</th>       <th style="text-align:center;">#Edges</th>       <th style="text-align:center;">Weighted?</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Snapchat Friendship</td>       <td style="text-align:center;">1.5M</td>       <td style="text-align:center;">66M</td>       <td style="text-align:center;">No</td>       </tr>       <tr>       <td style="text-align:center;">Tencent Weibo Retweet</td>       <td style="text-align:center;">842K</td>       <td style="text-align:center;">1.9M</td>       <td style="text-align:center;">Yes</td>       </tr>       <tr>       <td style="text-align:center;">Venue Citation</td>       <td style="text-align:center;">1.2K</td>       <td style="text-align:center;">91K</td>       <td style="text-align:center;">Yes</td>       </tr>       <tr>       <td style="text-align:center;">Wikipedia Hyperlink</td>       <td style="text-align:center;">488K</td>       <td style="text-align:center;">5.5M</td>       <td style="text-align:center;">No</td>       </tr>       <tr>       <td style="text-align:center;">Wikipedia Clickstream</td>       <td style="text-align:center;">2.4M</td>       <td style="text-align:center;">15M</td>       <td style="text-align:center;">No</td>       </tr>      </tbody>     </table>    </div>    <p>In all experiments, 90% of the links are randomly sampled as the training dataset. Hyper-parameters for prior distribution (introduced in Section <a class="sec" href="#sec-12">3.3</a>) are set to <em>k<sub>R</sub>     </em> = 1.5, <em>r<sub>min</sub>     </em> = 0.1 and <span class="inline-equation"><span class="tex">$\sigma _Z^2=10^6$</span>     </span>. We do not observe significance in terms of the evaluation metrics for different settings of <em>k<sub>R</sub>     </em> &#x2208; [1, 2], <em>r<sub>min</sub>     </em> &#x2264; 0.2 and <span class="inline-equation"><span class="tex">$\sigma _Z^2 \ge 10^4$</span>     </span>.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Applications</h3>     </div>    </header>    <section id="sec-19">     <p><em>4.2.1 Classification.</em> We demonstrate the advantage of our embedding in terms of multi-label classification results. We compare with the following baseline results.</p>     <ul class="list-no-style">      <li id="list12" label="&#x2022;">Matrix factorization techniques for recommender systems (MF) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0022">22</a>]. Although this method is designed for recommender systems, we apply their method on the user-user affinity matrix and treat the low dimensional feature for users as embedding. Note that, the popularity of a user is explicitly modeled by an additional bias factor.<br/></li>      <li id="list13" label="&#x2022;">Graph Factorization (GF) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>]. This is another matrix factorization based approach that factorizes the graph adjacency matrix in order to obtain the low dimensional vector representation for nodes.<br/></li>      <li id="list14" label="&#x2022;">Large-scale information network embedding (LINE) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>]. It embeds large information networks into lower dimensional vector spaces. The authors propose two measures of node proximity (i.e. 1st and 2nd order), and we will compare with both of them.<br/></li>      <li id="list15" label="&#x2022;">Node2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>]. This approach learns low-dimensional representations for nodes in a graph by optimizing a neighborhood preserving objective.<br/></li>     </ul>     <p>The lower dimensional feature vector for each entity is then used to predict its label. For fair comparison, we only use the proximity-based representation z as the feature in our model. We also try treating the concatenation of both <em>r</em> and z as the feature vector; however, since there is little correlation between rank <em>r</em> and label (e.g. the popularity of a person does not indicate her occupation), we report the performance using z only. The state-of-the-art Conditional Bernoulli Mixtures (CBM) model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0025">25</a>] is adopted as the multi-label classifier, where the authors kindly make their code available online<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. 90% of the nodes (labels as well as their vector representations) are randomly sampled for training. We use the following metrics for evaluation (denoting <em>Pred</em> as the set of predicted labels, and <em>True</em> as the set of ground truth labels for each entity):</p>     <ul class="list-no-style">      <li id="list16" label="&#x2022;">Jaccard Index: the number of correctly predicted labels divided by the union of predicted and true labels: <span class="inline-equation"><span class="tex">$J(Pred, True) = \frac{|Pred~ \cap ~True|}{|Pred~ \cup ~True|}$</span>       </span>. Larger values indicate better performance.<br/></li>      <li id="list17" label="&#x2022;">Hamming Loss: the fraction of the wrong labels to the total number of labels: <span class="inline-equation"><span class="tex">$\frac{1}{L} \sum _{i=1}^{L} xor(Pred_i, True_i)$</span>       </span>, where <em>L</em> is the number of total labels. Smaller values indicate better performance.<br/></li>      <li id="list18" label="&#x2022;">F1 score: the harmonic mean of precision <span class="inline-equation"><span class="tex">$\frac{|Pred~ \cap ~True|}{|Pred|}$</span>       </span> and recall <span class="inline-equation"><span class="tex">$\frac{|Pred~ \cap ~True|}{|True|}$</span>       </span>. Larger values indicate better performance.<br/></li>     </ul>     <p>The classification results (average of the above metrics for each entity) are shown in Figures <a class="fig" href="#fig4">4</a>-<a class="fig" href="#fig6">6</a>. We only evaluate datasets where ground truth user labels are available (i.e., Venue Citation, Wikipedia Hyperlink and Wikipedia Clickstream). <figure id="fig4">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig4.jpg" class="img-responsive" alt="Figure 4"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Multilabel classification results on Venue Citation dataset.</span>       </div>      </figure>      <figure id="fig5">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig5.jpg" class="img-responsive" alt="Figure 5"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Multilabel classification results on Wikipedia Hyperlink dataset.</span>       </div>      </figure>      <figure id="fig6">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig6.jpg" class="img-responsive" alt="Figure 6"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Multilabel classification results on Wikipedia Clickstream dataset.</span>       </div>      </figure>     </p>    </section>    <section id="sec-20">     <p><em>4.2.2 Link Prediction.</em> Although not all baseline methods explicitly mention the application in link prediction, they all assign a probability score to every pair of nodes, which can be sorted and evaluated using the area under the ROC curve (AUC) score. Specifically, 10% of the existing edges and non-existing edges are hidden from the training set, and their probabilities are examined by the model. For methods designed for undirected networks, the probability of a directed link <em>u<sub>i</sub>      </em> &#x2192; <em>u<sub>j</sub>      </em> is simply regarded as that of the undirected dyad (<em>u<sub>i</sub>      </em>, <em>u<sub>j</sub>      </em>). The evaluation results are reported in Table <a class="tbl" href="#tab2">2</a>.</p>     <p>Particularly, a significant improvement over the baseline methods is observed on Snapchat and Weibo dataset, as users tend to interact with others (especially celebrities) due to their popularity instead of proximity, which is never captured by most baseline methods. This observation agrees with our intuition. Note that by using our embedding algorithm (RaRE), a 2-dimensional proximity-based embedding can already beat much higher-dimensional embedding (e.g., <em>K</em> = 32) for all the other baselines in almost all the datasets.</p>     <div class="table-responsive" id="tab2">      <div class="table-caption">       <span class="table-number">Table 2:</span>       <span class="table-title">Link prediction AUC (%) on all datasets.</span>      </div>      <table class="table">      <thead>       <tr>        <th colspan="2" style="text-align:left;">        </th>        <th colspan="5" style="text-align:center;">Dimension of embedding <em>K</em>         <hr/>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">2</th>        <th style="text-align:center;">4</th>        <th style="text-align:center;">8</th>        <th style="text-align:center;">16</th>        <th style="text-align:center;">32</th>       </tr>      </thead>       <tbody>       <tr>        <td style="text-align:center;">Snapchat</td>        <td style="text-align:center;">MF</td>        <td style="text-align:center;">54.0</td>        <td style="text-align:center;">54.2</td>        <td style="text-align:center;">54.3</td>        <td style="text-align:center;">54.3</td>        <td style="text-align:center;">54.3</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">GF</td>        <td style="text-align:center;">63.1</td>        <td style="text-align:center;">66.7</td>        <td style="text-align:center;">69.5</td>        <td style="text-align:center;">72.1</td>        <td style="text-align:center;">73.5</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-1st</td>        <td style="text-align:center;">54.6</td>        <td style="text-align:center;">57.8</td>        <td style="text-align:center;">59.1</td>        <td style="text-align:center;">60.3</td>        <td style="text-align:center;">58.8</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-2nd</td>        <td style="text-align:center;">55.8</td>        <td style="text-align:center;">56.0</td>        <td style="text-align:center;">56.0</td>        <td style="text-align:center;">56.0</td>        <td style="text-align:center;">56.0</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">Node2vec</td>        <td style="text-align:center;">57.3</td>        <td style="text-align:center;">56.9</td>        <td style="text-align:center;">56.1</td>        <td style="text-align:center;">53.9</td>        <td style="text-align:center;">65.7</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">RaRE</td>        <td style="text-align:center;">         <strong>86.7</strong>        </td>        <td style="text-align:center;">         <strong>92.1</strong>        </td>        <td style="text-align:center;">         <strong>94.0</strong>        </td>        <td style="text-align:center;">         <strong>94.8</strong>        </td>        <td style="text-align:center;">         <strong>94.6</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">Tencent<br/> Weibo</td>        <td style="text-align:center;">MF</td>        <td style="text-align:center;">92.7</td>        <td style="text-align:center;">92.9</td>        <td style="text-align:center;">93.0</td>        <td style="text-align:center;">         <strong>93.1</strong>        </td>        <td style="text-align:center;">93.1</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">GF</td>        <td style="text-align:center;">71.7</td>        <td style="text-align:center;">76.9</td>        <td style="text-align:center;">76.7</td>        <td style="text-align:center;">76.3</td>        <td style="text-align:center;">76.8</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-1st</td>        <td style="text-align:center;">62.3</td>        <td style="text-align:center;">64.5</td>        <td style="text-align:center;">70.9</td>        <td style="text-align:center;">74.7</td>        <td style="text-align:center;">75.8</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-2nd</td>        <td style="text-align:center;">57.1</td>        <td style="text-align:center;">60.4</td>        <td style="text-align:center;">60.2</td>        <td style="text-align:center;">60.9</td>        <td style="text-align:center;">61.6</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">Node2vec</td>        <td style="text-align:center;">68.3</td>        <td style="text-align:center;">71.0</td>        <td style="text-align:center;">71.9</td>        <td style="text-align:center;">72.3</td>        <td style="text-align:center;">72.5</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">RaRE</td>        <td style="text-align:center;">         <strong>95.3</strong>        </td>        <td style="text-align:center;">         <strong>95.2</strong>        </td>        <td style="text-align:center;">         <strong>94.2</strong>        </td>        <td style="text-align:center;">         <strong>93.1</strong>        </td>        <td style="text-align:center;">         <strong>96.6</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">Venue<br/> Citation</td>        <td style="text-align:center;">MF</td>        <td style="text-align:center;">85.5</td>        <td style="text-align:center;">90.2</td>        <td style="text-align:center;">92.3</td>        <td style="text-align:center;">91.8</td>        <td style="text-align:center;">91.6</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">GF</td>        <td style="text-align:center;">78.0</td>        <td style="text-align:center;">87.1</td>        <td style="text-align:center;">92.6</td>        <td style="text-align:center;">93.7</td>        <td style="text-align:center;">         <strong>94.4</strong>        </td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-1st</td>        <td style="text-align:center;">55.0</td>        <td style="text-align:center;">56.4</td>        <td style="text-align:center;">63.0</td>        <td style="text-align:center;">79.8</td>        <td style="text-align:center;">80.0</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-2nd</td>        <td style="text-align:center;">64.4</td>        <td style="text-align:center;">74.2</td>        <td style="text-align:center;">80.0</td>        <td style="text-align:center;">81.2</td>        <td style="text-align:center;">81.5</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">Node2vec</td>        <td style="text-align:center;">81.0</td>        <td style="text-align:center;">85.3</td>        <td style="text-align:center;">89.4</td>        <td style="text-align:center;">90.9</td>        <td style="text-align:center;">91.2</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">RaRE</td>        <td style="text-align:center;">         <strong>91.4</strong>        </td>        <td style="text-align:center;">         <strong>93.7</strong>        </td>        <td style="text-align:center;">         <strong>94.0</strong>        </td>        <td style="text-align:center;">         <strong>94.3</strong>        </td>        <td style="text-align:center;">94.2</td>       </tr>       <tr>        <td style="text-align:center;">Wikipedia<br/> Hyperlink</td>        <td style="text-align:center;">MF</td>        <td style="text-align:center;">84.9</td>        <td style="text-align:center;">87.8</td>        <td style="text-align:center;">89.6</td>        <td style="text-align:center;">90.8</td>        <td style="text-align:center;">91.5</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">GF</td>        <td style="text-align:center;">80.4</td>        <td style="text-align:center;">88.7</td>        <td style="text-align:center;">93.5</td>        <td style="text-align:center;">95.6</td>        <td style="text-align:center;">96.6</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-1st</td>        <td style="text-align:center;">52.8</td>        <td style="text-align:center;">55.8</td>        <td style="text-align:center;">63.7</td>        <td style="text-align:center;">69.6</td>        <td style="text-align:center;">77.7</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-2nd</td>        <td style="text-align:center;">50.0</td>        <td style="text-align:center;">50.1</td>        <td style="text-align:center;">50.2</td>        <td style="text-align:center;">50.7</td>        <td style="text-align:center;">51.7</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">Node2vec</td>        <td style="text-align:center;">77.2</td>        <td style="text-align:center;">84.9</td>        <td style="text-align:center;">88.7</td>        <td style="text-align:center;">89.1</td>        <td style="text-align:center;">89.4</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">RaRE</td>        <td style="text-align:center;">         <strong>97.5</strong>        </td>        <td style="text-align:center;">         <strong>97.6</strong>        </td>        <td style="text-align:center;">         <strong>97.7</strong>        </td>        <td style="text-align:center;">         <strong>97.8</strong>        </td>        <td style="text-align:center;">         <strong>97.8</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">Wikipedia<br/> Clickstream</td>        <td style="text-align:center;">MF</td>        <td style="text-align:center;">63.4</td>        <td style="text-align:center;">68.6</td>        <td style="text-align:center;">72.1</td>        <td style="text-align:center;">74.5</td>        <td style="text-align:center;">76.9</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">GF</td>        <td style="text-align:center;">76.1</td>        <td style="text-align:center;">82.5</td>        <td style="text-align:center;">86.1</td>        <td style="text-align:center;">86.7</td>        <td style="text-align:center;">86.7</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-1st</td>        <td style="text-align:center;">73.8</td>        <td style="text-align:center;">78.1</td>        <td style="text-align:center;">78.6</td>        <td style="text-align:center;">78.8</td>        <td style="text-align:center;">78.8</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">LINE-2nd</td>        <td style="text-align:center;">69.3</td>        <td style="text-align:center;">71.2</td>        <td style="text-align:center;">72.4</td>        <td style="text-align:center;">73.0</td>        <td style="text-align:center;">73.5</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">Node2vec</td>        <td style="text-align:center;">82.9</td>        <td style="text-align:center;">87.2</td>        <td style="text-align:center;">88.1</td>        <td style="text-align:center;">89.6</td>        <td style="text-align:center;">89.0</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td style="text-align:center;">RaRE</td>        <td style="text-align:center;">         <strong>90.7</strong>        </td>        <td style="text-align:center;">         <strong>94.4</strong>        </td>        <td style="text-align:center;">         <strong>94.7</strong>        </td>        <td style="text-align:center;">         <strong>94.3</strong>        </td>        <td style="text-align:center;">         <strong>93.7</strong>        </td>       </tr>       </tbody>      </table>     </div>    </section>    <section id="sec-21">     <p><em>4.2.3 Embedding as Additional Features for Classification.</em> The lower dimensional feature vector for each entity can also serve as additional features for real-world applications. For example, in Snapchat, the gender information is not required at the time of registration, however, knowledge of gender is crucial for better user understanding, ADs targeting and content recommendation. Currently, gender information is predicted by a Gradient Boosted Decision Trees model with a few highly engineered features, which already has an accuracy of 93.5% (the true gender information is obtained from Bitmoji user avatar). The features are derived from first name of a user, country, historical usage behavior of the Snapchat app products such as Lens, User Story, Discover etc. In this classification task, we collected 258,014 labeled examples, and we split it into training set and test set using 0.7 to 0.3 ratio. We concatenate the learned embedding vector as additional features to the basic features we used for gender prediction, and report the accuracy in Table <a class="tbl" href="#tab3">3</a> (accuracy = 1.0 - error rate). Given the high accuracy of the baseline model (93.5%), absolute accuracy lift of greater than 1% is considered very challenging and significant. As we can see from Table <a class="tbl" href="#tab3">3</a> that adding embedding vectors produced by our proposed method RaRE significantly outperforms other baselines for the gender prediction task. For example, when using 32 dimensional embeddings trained by RaRE, we observed 1.5% lift on the gender prediction accuracy over the current production model. The results show that the embedding information of a user carries useful signals for predicting the basic profile of a user such as gender.</p>     <div class="table-responsive" id="tab3">      <div class="table-caption">       <span class="table-number">Table 3:</span>       <span class="table-title">Gender prediction accuracy (%) on Snapchat dataset.</span>      </div>      <table class="table">      <thead>       <tr>        <th style="text-align:center;">Method</th>        <th style="text-align:center;">         <em>K</em> = 2</th>        <th style="text-align:center;">         <em>K</em> = 4</th>        <th style="text-align:center;">         <em>K</em> = 8</th>        <th style="text-align:center;">         <em>K</em> = 16</th>        <th style="text-align:center;">         <em>K</em> = 32</th>       </tr>      </thead>       <tbody>       <tr>        <td style="text-align:center;">MF</td>        <td style="text-align:center;">93.3</td>        <td style="text-align:center;">93.4</td>        <td style="text-align:center;">93.5</td>        <td style="text-align:center;">93.7</td>        <td style="text-align:center;">94.0</td>       </tr>       <tr>        <td style="text-align:center;">GF</td>        <td style="text-align:center;">93.6</td>        <td style="text-align:center;">93.6</td>        <td style="text-align:center;">93.8</td>        <td style="text-align:center;">94.0</td>        <td style="text-align:center;">94.3</td>       </tr>       <tr>        <td style="text-align:center;">LINE-1st</td>        <td style="text-align:center;">93.5</td>        <td style="text-align:center;">93.5</td>        <td style="text-align:center;">93.6</td>        <td style="text-align:center;">93.7</td>        <td style="text-align:center;">93.7</td>       </tr>       <tr>        <td style="text-align:center;">LINE-2nd</td>        <td style="text-align:center;">93.5</td>        <td style="text-align:center;">93.5</td>        <td style="text-align:center;">93.7</td>        <td style="text-align:center;">93.7</td>        <td style="text-align:center;">93.8</td>       </tr>       <tr>        <td style="text-align:center;">Node2vec</td>        <td style="text-align:center;">94.2</td>        <td style="text-align:center;">94.2</td>        <td style="text-align:center;">94.3</td>        <td style="text-align:center;">94.3</td>        <td style="text-align:center;">94.5</td>       </tr>       <tr>        <td style="text-align:center;">RaRE</td>        <td style="text-align:center;">         <strong>94.5</strong>        </td>        <td style="text-align:center;">         <strong>94.6</strong>        </td>        <td style="text-align:center;">         <strong>94.7</strong>        </td>        <td style="text-align:center;">         <strong>94.9</strong>        </td>        <td style="text-align:center;">         <strong>95.0</strong>        </td>       </tr>       </tbody>      </table>     </div>    </section>    <section id="sec-22">     <p><em>4.2.4 A Novel Polar Coordinate-based Visualization.</em> Visualization is another way to demonstrate the effectiveness of learned representation. A good embedding algorithm should be able to distinguish nodes of different labels by separating them in the vector representation space. Conventionally, a 2D or 3D vector representation is learned for each individual, which is treated as his/her coordinates and thus can be displayed in a scatter plot. We list the results of a few visualization methods in Figure <a class="fig" href="#fig7">7</a>, and it is very clear that the proximity-based embedding of RaRE does well in detecting different computer science research communities.</p>     <p>It is also interesting to reveal the visualization of the nodes by combining social rank and proximity-based representation in a unified plot. While many of the visualization approaches are capable of capturing much of the local structure (e.g. neighbors) as well as the global structure (e.g. clusters), they fail to identify the influential entities in the plot. In our method, we depict the coordinate of a node using the polar system, where the radius is simply its social rank <em>r</em>, and the angle <em>&#x03B8;</em> is obtained from its proximity-based representation z by a simple transformation: <div class="table-responsive" id="Xeq7">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \lbrace \theta \rbrace ^{*} = \mathop{argmin}\limits _{\theta } \sum _{(i,j) \in E_Z} \log \big (1 + e^{-\cos (\theta _i - \theta _j)} \big) + \sum _{(i,j) \in \hat{E_Z}} \log \big (1 + e^{\cos (\theta _i - \theta _j)} \big) \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(15)</span>       </div>      </div> where cos&#x2009;(<em>&#x03B8;<sub>i</sub>      </em> &#x2212; <em>&#x03B8;<sub>j</sub>      </em>) reflects the similarity on the 2D sphere, <em>E<sub>Z</sub>      </em> is the new set of edges defined in the space of proximity-based representation: <em>E<sub>Z</sub>      </em> = {(<em>i</em>, <em>j</em>): ~||z<sub>       <em>i</em>      </sub> &#x2212; z<sub>       <em>j</em>      </sub>||<sup>2</sup> < <em>t</em>} and <span class="inline-equation"><span class="tex">$\hat{E_Z}$</span>      </span> is the corresponding non-existing pairs of nodes sampled using the same strategy as Section <a class="sec" href="#sec-13">3.4</a>. The equation above can be considered as preserving the proximity in both spaces (minimizing the logit loss), as similar to various dimension reduction approaches. Here we pick <em>t</em> = 0.5 and <span class="inline-equation"><span class="tex">$ {z} \in \mathbb {R}^K$</span>      </span> where <em>K</em> = 2, and we do not observe significant variance in terms of these parameters.</p>     <p>In the bottom right figure of Figure <a class="fig" href="#fig7">7</a> (polar coordinates from RaRE), we can clearly observe the most influential venues around the center, among which top conferences in different areas (e.g. CHI, WWW, ICSE, CVPR, SIGGRAPH, SIGMOD, INFOCOM, AAAI, KDD, VLDB, ICML, STOC) are successfully identified. <figure id="fig7">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig7.jpg" class="img-responsive" alt="Figure 7"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Visualization on Venue Citation dataset. These plots are best viewed in color<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>.</span>       </div>      </figure>     </p>    </section>    </section>   </section>   <section id="sec-23">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Related Work</h2>    </div>    </header>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Ranking</h3>     </div>    </header>    <p>Mapping entities in the network to a spectrum of importance scores has been a very popular research topic for decades. The notion of influential nodes emerged from the large-scale World Wide Web (WWW), where it is vital for crawlers to start from important pages first [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], or generally, an objective method is desired to measure human&#x0027;s interest in a collections of webpages. Various algorithms have been proposed, e.g. PageRank [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], HITS [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], and have been widely generalized towards various needs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>]. These algorithms have an assumption in common: a random surfer is assumed to browse the web and click hyperlinks randomly, and the probability distribution of a webpage being visited will converge to a score related to its rank. Similar ideas are also applicable to information networks by defining the weights between entities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]. In addition to random walk-based approaches, many ranking methods are proposed based on Bayesian network and inference. Pal and Counts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] generate a list of features for microblogs based on their followers, number of hashtags and so on. Clusters are then revealed using a Gaussian mixture model, and the rank of a microblog is an aggregation of the rank of its features. Ball and Newman [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] study several friendship networks, and find that unreciprocated friendships often consist of a lower-ranked individual claiming friendship with a higher-ranked one. Based on this assumption, they deduce such social rankings using maximum likelihood estimation.</p>    <p>All the above approaches provide a global ranking for each entity, however, the ranking makes little sense when entities in several categories are mingled together without distinguishing the clustering information. For example, we seldom mention comparisons such as &#x201C;a computer system conference is ranked higher than a database conference&#x201D;. For heterogeneous networks where multiple types of entities or relations may exist, determining the network becomes even tricker and multiple ranking systems may occur according to different schemas or topics. Sun et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>] associate heterogeneous network clustering and ranking together, and assume several rank distributions conditional on different cluster structures. Liu et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] propose a probabilistic generative model, which explains the network generation process from users and documents and is able to reveal the most related nodes with a given topic (query). However, additional information other than cluster labels is usually desired to understand the network. Our method, on the other hand, provides the lower dimensional proximity-based representation for each entity, which has wide applications including classification, clustering, visualization and so on.</p>    </section>    <section id="sec-25">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Network Embedding</h3>     </div>    </header>    <p>Detecting latent representation (embedding) of nodes in a network is essential in understanding the opinions of individuals and desired for various machine learning tasks. Traditional approaches usually utilize the adjacency matrix in order to extract essential dimensions of the data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>], which involve finding the eigenvalues of a matrix and thus not scalable for large networks. Matrix factorization finds an approximation of a matrix by the product of two lower rank matrices, and this technique has been popular especially in recommender systems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. In terms of graph data, matrix factorization can be applied on the affinity matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], and each row of the low dimensional matrix naturally becomes the vector representation of the corresponding node.</p>    <p>More recent approaches introduce embedding for nodes in a network, which is a low-dimensional vector that represents the latent characteristics of a node. These embedding vectors are learned by preserving similarity in the network and similarity in the latent Euclidean space. The notion of embedding originates from word embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>], and Levy and Goldberg [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] establishes the connection between matrix factorization and word embedding, arguing that estimating word embedding is equivalent to factorizing a pointwise mutual information matrix. Later on, researchers have discovered strategies to explain the generation of links from a probabilistic perspective, with the assumption that the likelihood of a link should be proportional to the similarity of both nodes (neighbors) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>] or entities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>]. More recent approaches generalize the notion of similar nodes to <em>n</em>-hop neighbors [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>]. Generally, links are assumed to be explained as the proximity between the representation of two actors (i.e. the &#x201C;homophily&#x201D; assumption [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]). However, we often observe links to highly-ranked nodes (e.g. many users follow <em>celebrities</em> on Twitter, and scholars tend to cite <em>popular</em> works and authors in a bibliographic network). As a result, some nodes are poorly modeled by the homophily assumption. Embedding-based approaches ignore this seminal factor in link generation, which may lead to inaccurate estimation as a result. Some matrix factorization methods consider the popularity factor by introducing a bias term [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>], but they model these two factors independently, while neglecting the fact that the knowledge of one can affect the distribution of the other.</p>    </section>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>In this paper we present a novel approach for information network embedding with consideration of individuals&#x2019; social ranks. From the graph generation perspective, we refine the latent representation of nodes on information network by analyzing the role of individuals in terms of their social rank. Moreover, we provide solid derivations on the reason behind a link in terms of both latent proximity-based representation as well as social rank of a node, which provides a brand new insight of the problem. We carefully design a framework that explicitly models the interdependency between these two types of embeddings. Finally, we evaluate our model on several real-world large-scale datasets, and the results on classification, link prediction and visualization demonstrate our advantage over the state-of-the-art network embedding methods.</p>   </section>   <section id="sec-27">    <header>    <div class="title-info">     <h2>Acknowledgement</h2>    </div>    </header>    <p>We would like to thank Snapchat Inc. for its gift funding. We would also like to share our gratitude to the anonymous reviewers for their precious comments. This work is partially supported by NSF CAREER Award #1741634 and #1453800.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">A.&#x00A0;Ahmed, N.&#x00A0;Shervashidze, S.&#x00A0;Narayanamurthy, V.&#x00A0;Josifovski, and A.&#x00A0;J. Smola. Distributed large-scale natural graph factorization. In <em>Proceedings of the 22nd international conference on World Wide Web</em>, pages 37&#x2013;48. ACM, 2013.</li>    <li id="BibPLXBIB0002" label="[2]">B.&#x00A0;Ball and M.&#x00A0;E. Newman. Friendship networks and social status. <em>Network Science</em>, 1(01):16&#x2013;30, 2013.</li>    <li id="BibPLXBIB0003" label="[3]">A.&#x00A0;Balmin, V.&#x00A0;Hristidis, and Y.&#x00A0;Papakonstantinou. Objectrank: Authority-based keyword search in databases. In <em>Proceedings of the Thirtieth international conference on Very large data bases-Volume 30</em>, pages 564&#x2013;575. VLDB Endowment, 2004.</li>    <li id="BibPLXBIB0004" label="[4]">A.-L. Barab&#x00E1;si and R.&#x00A0;Albert. Emergence of scaling in random networks. <em>science</em>, 286(5439):509&#x2013;512, 1999.</li>    <li id="BibPLXBIB0005" label="[5]">M.&#x00A0;Belkin and P.&#x00A0;Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In <em>Advances in neural information processing systems</em>, pages 585&#x2013;591, 2002.</li>    <li id="BibPLXBIB0006" label="[6]">R.&#x00A0;M. Bell, Y.&#x00A0;Koren, and C.&#x00A0;Volinsky. The bellkor 2008 solution to the netflix prize. <em>Statistics Research Department at AT&#x0026;T Research</em>, 2008.</li>    <li id="BibPLXBIB0007" label="[7]">P.&#x00A0;Boldi and C.&#x00A0;Monti. Cleansing wikipedia categories using centrality. In <em>Proceedings of the 25th International Conference Companion on World Wide Web</em>, pages 969&#x2013;974. International World Wide Web Conferences Steering Committee, 2016.</li>    <li id="BibPLXBIB0008" label="[8]">A.&#x00A0;Bordes, N.&#x00A0;Usunier, A.&#x00A0;Garcia-Duran, J.&#x00A0;Weston, and O.&#x00A0;Yakhnenko. Translating embeddings for modeling multi-relational data. In <em>Advances in neural information processing systems</em>, pages 2787&#x2013;2795, 2013.</li>    <li id="BibPLXBIB0009" label="[9]">A.&#x00A0;Bordes, J.&#x00A0;Weston, R.&#x00A0;Collobert, Y.&#x00A0;Bengio, et&#x00A0;al. Learning structured embeddings of knowledge bases. In <em>AAAI</em>, volume&#x00A0;6, page&#x00A0;6, 2011.</li>    <li id="BibPLXBIB0010" label="[10]">R.&#x00A0;A. Bradley and M.&#x00A0;E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. <em>Biometrika</em>, 39(3/4):324&#x2013;345, 1952.</li>    <li id="BibPLXBIB0011" label="[11]">U.&#x00A0;Brandes and S.&#x00A0;Cornelsen. Visual ranking of link structures. <em>J. Graph Algorithms Appl.</em>, 7(2):181&#x2013;201, 2003.</li>    <li id="BibPLXBIB0012" label="[12]">C.&#x00A0;Burges, T.&#x00A0;Shaked, E.&#x00A0;Renshaw, A.&#x00A0;Lazier, M.&#x00A0;Deeds, N.&#x00A0;Hamilton, and G.&#x00A0;Hullender. Learning to rank using gradient descent. In <em>Proceedings of the 22nd international conference on Machine learning</em>, pages 89&#x2013;96. ACM, 2005.</li>    <li id="BibPLXBIB0013" label="[13]">S.&#x00A0;Cao, W.&#x00A0;Lu, and Q.&#x00A0;Xu. Grarep: Learning graph representations with global structural information. In <em>Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</em>, pages 891&#x2013;900. ACM, 2015.</li>    <li id="BibPLXBIB0014" label="[14]">S.&#x00A0;Chang, W.&#x00A0;Han, J.&#x00A0;Tang, G.-J. Qi, C.&#x00A0;C. Aggarwal, and T.&#x00A0;S. Huang. Heterogeneous network embedding via deep architectures. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, pages 119&#x2013;128. ACM, 2015.</li>    <li id="BibPLXBIB0015" label="[15]">J.&#x00A0;Cho, H.&#x00A0;Garcia-Molina, and L.&#x00A0;Page. Efficient crawling through url ordering. <em>Computer Networks and ISDN Systems</em>, 30(1):161&#x2013;172, 1998.</li>    <li id="BibPLXBIB0016" label="[16]">A.&#x00A0;Grover and J.&#x00A0;Leskovec. node2vec: Scalable feature learning for networks. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, pages 855&#x2013;864. ACM, 2016.</li>    <li id="BibPLXBIB0017" label="[17]">T.&#x00A0;H. Haveliwala. Topic-sensitive pagerank. In <em>Proceedings of the 11th international conference on World Wide Web</em>, pages 517&#x2013;526. ACM, 2002.</li>    <li id="BibPLXBIB0018" label="[18]">P.&#x00A0;D. Hoff, A.&#x00A0;E. Raftery, and M.&#x00A0;S. Handcock. Latent space approaches to social network analysis. <em>Journal of the american Statistical association</em>, 97(460):1090&#x2013;1098, 2002.</li>    <li id="BibPLXBIB0019" label="[19]">Y.&#x00A0;Hu, Y.&#x00A0;Koren, and C.&#x00A0;Volinsky. Collaborative filtering for implicit feedback datasets. In <em>Data Mining, 2008. ICDM&#x2019;08. Eighth IEEE International Conference on</em>, pages 263&#x2013;272. Ieee, 2008.</li>    <li id="BibPLXBIB0020" label="[20]">C.&#x00A0;C. Johnson. Logistic matrix factorization for implicit feedback data. <em>Advances in Neural Information Processing Systems</em>, 27, 2014.</li>    <li id="BibPLXBIB0021" label="[21]">J.&#x00A0;M. Kleinberg. Authoritative sources in a hyperlinked environment. <em>Journal of the ACM (JACM)</em>, 46(5):604&#x2013;632, 1999.</li>    <li id="BibPLXBIB0022" label="[22]">Y.&#x00A0;Koren, R.&#x00A0;Bell, and C.&#x00A0;Volinsky. Matrix factorization techniques for recommender systems. <em>Computer</em>, 42(8):30&#x2013;37, 2009.</li>    <li id="BibPLXBIB0023" label="[23]">J.&#x00A0;B. Kruskal and M.&#x00A0;Wish. <em>Multidimensional scaling</em>, volume&#x00A0;11. Sage, 1978.</li>    <li id="BibPLXBIB0024" label="[24]">O.&#x00A0;Levy and Y.&#x00A0;Goldberg. Neural word embedding as implicit matrix factorization. In <em>Advances in neural information processing systems</em>, pages 2177&#x2013;2185, 2014.</li>    <li id="BibPLXBIB0025" label="[25]">C.&#x00A0;Li, B.&#x00A0;Wang, V.&#x00A0;Pavlu, and J.&#x00A0;Aslam. Conditional bernoulli mixtures for multi-label classification. In <em>Proceedings of The 33rd International Conference on Machine Learning</em>, pages 2482&#x2013;2491, 2016.</li>    <li id="BibPLXBIB0026" label="[26]">L.&#x00A0;Liu, J.&#x00A0;Tang, J.&#x00A0;Han, M.&#x00A0;Jiang, and S.&#x00A0;Yang. Mining topic-level influence in heterogeneous networks. In <em>Proceedings of the 19th ACM international conference on Information and knowledge management</em>, pages 199&#x2013;208. ACM, 2010.</li>    <li id="BibPLXBIB0027" label="[27]">L.&#x00A0;L&#x00FC;, Y.-C. Zhang, C.&#x00A0;H. Yeung, and T.&#x00A0;Zhou. Leaders in social networks, the delicious case. <em>PloS one</em>, 6(6):e21202, 2011.</li>    <li id="BibPLXBIB0028" label="[28]">L.&#x00A0;v.&#x00A0;d. Maaten and G.&#x00A0;Hinton. Visualizing data using t-sne. <em>Journal of Machine Learning Research</em>, 9(Nov):2579&#x2013;2605, 2008.</li>    <li id="BibPLXBIB0029" label="[29]">M.&#x00A0;McPherson, L.&#x00A0;Smith-Lovin, and J.&#x00A0;M. Cook. Birds of a feather: Homophily in social networks. <em>Annual review of sociology</em>, pages 415&#x2013;444, 2001.</li>    <li id="BibPLXBIB0030" label="[30]">A.&#x00A0;Menon and C.&#x00A0;Elkan. Link prediction via matrix factorization. <em>Machine Learning and Knowledge Discovery in Databases</em>, pages 437&#x2013;452, 2011.</li>    <li id="BibPLXBIB0031" label="[31]">T.&#x00A0;Mikolov, I.&#x00A0;Sutskever, K.&#x00A0;Chen, G.&#x00A0;S. Corrado, and J.&#x00A0;Dean. Distributed representations of words and phrases and their compositionality. In <em>Advances in neural information processing systems</em>, pages 3111&#x2013;3119, 2013.</li>    <li id="BibPLXBIB0032" label="[32]">L.&#x00A0;Page, S.&#x00A0;Brin, R.&#x00A0;Motwani, and T.&#x00A0;Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.</li>    <li id="BibPLXBIB0033" label="[33]">A.&#x00A0;Pal and S.&#x00A0;Counts. Identifying topical authorities in microblogs. In <em>Proceedings of the fourth ACM international conference on Web search and data mining</em>, pages 45&#x2013;54. ACM, 2011.</li>    <li id="BibPLXBIB0034" label="[34]">B.&#x00A0;Perozzi, R.&#x00A0;Al-Rfou, and S.&#x00A0;Skiena. Deepwalk: Online learning of social representations. In <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, pages 701&#x2013;710. ACM, 2014.</li>    <li id="BibPLXBIB0035" label="[35]">S.&#x00A0;T. Roweis and L.&#x00A0;K. Saul. Nonlinear dimensionality reduction by locally linear embedding. <em>science</em>, 290(5500):2323&#x2013;2326, 2000.</li>    <li id="BibPLXBIB0036" label="[36]">A.&#x00A0;Sinha, Z.&#x00A0;Shen, Y.&#x00A0;Song, H.&#x00A0;Ma, D.&#x00A0;Eide, B.-j.&#x00A0;P. Hsu, and K.&#x00A0;Wang. An overview of microsoft academic service (mas) and applications. In <em>Proceedings of the 24th international conference on world wide web</em>, pages 243&#x2013;246. ACM, 2015.</li>    <li id="BibPLXBIB0037" label="[37]">Y.&#x00A0;Sun, J.&#x00A0;Han, P.&#x00A0;Zhao, Z.&#x00A0;Yin, H.&#x00A0;Cheng, and T.&#x00A0;Wu. Rankclus: integrating clustering with ranking for heterogeneous information network analysis. In <em>Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology</em>, pages 565&#x2013;576. ACM, 2009.</li>    <li id="BibPLXBIB0038" label="[38]">Y.&#x00A0;Sun, Y.&#x00A0;Yu, and J.&#x00A0;Han. Ranking-based clustering of heterogeneous information networks with star network schema. In <em>Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, pages 797&#x2013;806. ACM, 2009.</li>    <li id="BibPLXBIB0039" label="[39]">J.&#x00A0;Tang, J.&#x00A0;Liu, M.&#x00A0;Zhang, and Q.&#x00A0;Mei. Visualizing large-scale and high-dimensional data. In <em>Proceedings of the 25th International Conference on World Wide Web</em>, pages 287&#x2013;297. International World Wide Web Conferences Steering Committee, 2016.</li>    <li id="BibPLXBIB0040" label="[40]">J.&#x00A0;Tang, M.&#x00A0;Qu, M.&#x00A0;Wang, M.&#x00A0;Zhang, J.&#x00A0;Yan, and Q.&#x00A0;Mei. Line: Large-scale information network embedding. In <em>Proceedings of the 24th International Conference on World Wide Web</em>, pages 1067&#x2013;1077. ACM, 2015.</li>    <li id="BibPLXBIB0041" label="[41]">J.&#x00A0;B. Tenenbaum, V.&#x00A0;De&#x00A0;Silva, and J.&#x00A0;C. Langford. A global geometric framework for nonlinear dimensionality reduction. <em>science</em>, 290(5500):2319&#x2013;2323, 2000.</li>    <li id="BibPLXBIB0042" label="[42]">L.&#x00A0;Van Der&#x00A0;Maaten. Accelerating t-sne using tree-based algorithms. <em>Journal of machine learning research</em>, 15(1):3221&#x2013;3245, 2014.</li>    <li id="BibPLXBIB0043" label="[43]">D.&#x00A0;Wang, P.&#x00A0;Cui, and W.&#x00A0;Zhu. Structural deep network embedding. In <em>Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</em>, pages 1225&#x2013;1234. ACM, 2016.</li>    <li id="BibPLXBIB0044" label="[44]">Z.&#x00A0;Wang, J.&#x00A0;Zhang, J.&#x00A0;Feng, and Z.&#x00A0;Chen. Knowledge graph embedding by translating on hyperplanes. In <em>AAAI</em>, pages 1112&#x2013;1119, 2014.</li>    <li id="BibPLXBIB0045" label="[45]">J.&#x00A0;Weng, E.-P. Lim, J.&#x00A0;Jiang, and Q.&#x00A0;He. Twitterrank: finding topic-sensitive influential twitterers. In <em>Proceedings of the third ACM international conference on Web search and data mining</em>, pages 261&#x2013;270. ACM, 2010.</li>    <li id="BibPLXBIB0046" label="[46]">Y.&#x00A0;Yamaguchi, T.&#x00A0;Takahashi, T.&#x00A0;Amagasa, and H.&#x00A0;Kitagawa. Turank: Twitter user ranking based on user-tweet graph analysis. In <em>International Conference on Web Information Systems Engineering</em>, pages 240&#x2013;253. Springer, 2010.</li>    <li id="BibPLXBIB0047" label="[47]">S.&#x00A0;Yan, D.&#x00A0;Xu, B.&#x00A0;Zhang, H.-J. Zhang, Q.&#x00A0;Yang, and S.&#x00A0;Lin. Graph embedding and extensions: A general framework for dimensionality reduction. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 29(1):40&#x2013;51, 2007.</li>    <li id="BibPLXBIB0048" label="[48]">Y.&#x00A0;Yang, J.&#x00A0;Tang, C.&#x00A0;W.-k. Leung, Y.&#x00A0;Sun, Q.&#x00A0;Chen, J.&#x00A0;Li, and Q.&#x00A0;Yang. Rain: Social role-aware information diffusion. In <em>AAAI</em>, pages 367&#x2013;373, 2015.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break"    href="https://snap.stanford.edu/data/enwiki-2013.html">https://snap.stanford.edu/data/enwiki-2013.html</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break"    href="https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream">https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://github.com/cheng-li/pyramid">https://github.com/cheng-li/pyramid</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Labels (colors) are identified by investigating the topic of venues within each class of KNN on the original one-hot encoding vector.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186102">https://doi.org/10.1145/3178876.3186102</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
