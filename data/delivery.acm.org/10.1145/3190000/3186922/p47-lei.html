<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Zeyang</span>      <span class="surName">Lei</span>     Graduate School at Shenzhen,, Tsinghua University, <a href="mailto:leizy16@mails.tsinghua.edu.cn">leizy16@mails.tsinghua.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Yujiu</span>      <span class="surName">Yang</span>     Graduate School at Shenzhen,, Tsinghua University<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>, <a href="mailto:yang.yujiu@sz.tsinghua.edu.cn">yang.yujiu@sz.tsinghua.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Yi</span>      <span class="surName">Liu</span>     Peking University Shenzhen Institute, <a href="mailto:eeyliu@gmail.com">eeyliu@gmail.com</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186922" target="_blank">https://doi.org/10.1145/3184558.3186922</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Sentiment analysis of social media and comment data is an important issue in opinion monitoring. In this work, we propose a Linguistic-Aware Attention Network (LANN) to enhance the performance of convolution neural network (CNN). LANN adopts a two-stage strategy to model the sentiment-specific sentence representation. First, an interactive attention mechanism is designed to model word-level semantics. Second, to capture phrase-level linguistic structure, a dynamic semantic attention is adopted to select the crucial phrase chunks in the sentence. The experiments demonstrate that LANN has robust superiority over competitors and has reached the state-of-the-art performance.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Sentiment analysis;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <em>Natural language processing;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Sentiment analysis; interactive attention; dynamic semantic attention</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Zeyang Lei, Yujiu Yang, and Yi Liu. 2018. LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 2 Pages. <a href="https://doi.org/10.1145/3184558.3186922" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186922</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Sentiment classification aims to assign the sentiment polarity towards a given text. It has broad applications such as public opinion monitoring, product recommendation and so on&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. Most existing studies build sentiment classifiers using deep learning approaches, such as convolutional neural network (CNN), long short-term memory (LSTM).</p>    <p>Despite the effectiveness of deep learning approaches, sentiment classification still remains a defect in real-world scenarios. The traditional sentiment lexicon plays an important role in sentiment classification. However, the sentiment lexicon has not been fully exploited in recent neural network models (e.g., CNN and LSTM) for sentiment classification.</p>    <p>In this paper, we propose a Linguistic-Aware Attention Network (LANN), which aims to integrate the high-quality sentiment lexicon into convolution neural network to further boost the performance of sentiment classification. More specifically, our model adopts a two-step strategy to model word-level and phrase-level semantics, respectively. First, we design an interactive attention mechanism to fully utilize the mutual information between context words and sentiment words in the sentence. The interactive attention mechanism helps highlight the relevance context words associated with the sentiment of the sentence. Second, to better capture the sentiment-specific semantics structure of the sentence, we design a new dynamic semantics attention over n-gram convolution results, which is capable of selecting the crucial phrase chunks to compose the sentiment-specific sentence representation.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> The Proposed Model</h2>     </div>    </header>    <p>LAAN mainly consists of two parts: the word-Level interactive attention module and the phrase-level dynamic semantic attention module.</p>    <p>     <strong>Word-Level Interactive Attention.</strong> Suppose that a sentence <em>x</em> consists of <em>n</em> context words <span class="inline-equation"><span class="tex">${W^c}=[w_1^c,w_2^c,...,w_n^c]$</span>     </span> and <em>m</em> sentiment words <span class="inline-equation"><span class="tex">${W^s}=[w_1^s,w_1^s,...,w_m^s]$</span>     </span>, where each <em>w</em> denotes a specific word. We embed each word into a low dimensional semantic space. This means, each word is mapped to its embedding <span class="inline-equation"><span class="tex">$w \in \mathbb {R}^d$</span>     </span>. Then we define the correlation matrix <span class="inline-equation"><span class="tex">$\mathbf {C}={W^c} \cdot {{(W^s)}}^T \in \mathbb {R}^{n\times m}$</span>     </span>, which represents the mutual information between context words and sentiment words. Here, each element <strong>C</strong>     <sub>     <em>i</em>, <em>j</em>     </sub> in the correlation matrix denotes the relevant degree between <em>i</em>-th word <span class="inline-equation"><span class="tex">$w_i^c$</span>     </span> of the context words and <em>j</em>-th word <span class="inline-equation"><span class="tex">$w_j^s$</span>     </span> of the sentiment words. Then we average all values in a row of <strong>C</strong> (&#x201C;col-wise average&#x201D;, resulting in the column vector of size n) and apply a softmax operation to obtain a single attention weight vector <span class="inline-equation"><span class="tex">$\alpha =softmax(\frac{\sum _{h=1}^m C[:,h]}{m})$</span>     </span> about the context words. Similarly, we average all values in a column of <strong>C</strong> (&#x201C;row-wise average&#x201D;, resulting in the row vector of size m) and apply a softmax operation to obtain a single attention weight vector <span class="inline-equation"><span class="tex">$\beta =softmax(\frac{\sum _{k=1}^n C[k,:]}{n})$</span>     </span> about the sentiment words. Next, with the attention weight vector <em>&#x03B1;</em> and the context words <em>W<sup>c</sup>     </em> as inputs, we can obtain the correlation vector <span class="inline-equation"><span class="tex">$w_{\alpha }^c$</span>     </span> about the context : <span class="inline-equation"><span class="tex">$w_{\alpha }^c=\sum _{i=1}^n \alpha _i w_i^c$</span>     </span>. <span class="inline-equation"><span class="tex">$w_{\alpha }^c$</span>     </span> denotes the overall relevance metric of the sentiment words towards the context words, which is beneficial to highlight the important words in the context for sentiment classification. Similarly, we can obtain the correlation vector <span class="inline-equation"><span class="tex">$w_{\beta }^s$</span>     </span> about the sentiment words: <span class="inline-equation"><span class="tex">$w_{\beta }^s=\sum _{i=1}^m \beta _i w_i^s$</span>     </span>. <span class="inline-equation"><span class="tex">$w_{\beta }^s$</span>     </span> denotes the overall relevance metric of the context words towards the sentiment words, which helps integrate contextual global information into the representation of sentiment words.</p>    <p>Finally, with the concatenation of each context word and the correlation vector <span class="inline-equation"><span class="tex">$w_{\alpha }^c$</span>     </span>, the sentiment-enhanced representation <span class="inline-equation"><span class="tex">$W^c_e \in \mathbb {R}^{2d\times n}$</span>     </span> of the context words can be computed as follows: <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} W^c_e=relu([U^cW^c \oplus \lbrace U^{\alpha }w_{\alpha }^c \otimes e_n\rbrace ]) \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div> where <em>U<sup>c</sup>     </em>, <em>U<sup>&#x03B1;</sup>     </em> are projection parameters, &#x2295; refers to the concatenation operator, <span class="inline-equation"><span class="tex">$w_{\alpha }^c \otimes e_n=[w_{\alpha }^c;w_{\alpha }^c;...;w_{\alpha }^c]$</span>     </span> denotes the operator which repeatedly concatenates <span class="inline-equation"><span class="tex">$w_{\alpha }^c$</span>     </span> for n times and <em>relu</em> is the semantic mapping function. Similarly, we can obtain the contextual enhanced representation <span class="inline-equation"><span class="tex">$W^s_e \in \mathbb {R}^{2d\times m}$</span>     </span> of the sentiment words as follows: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ W^s_e=relu([U^sW^s \oplus \lbrace U^{\beta }w_{\beta }^s \otimes e_m\rbrace ]) \] </span>      <br/>     </div>     </div>     <strong>Phrase-Level Dynamic Semantic Attention.</strong> After obtaining <span class="inline-equation"><span class="tex">$W^c_e$</span>     </span> and <span class="inline-equation"><span class="tex">$W^s_e$</span>     </span>, we adopt multi-gram convolution operation to capture different local semantic units&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. Formally, with different window size, we induce a combined multi-gram feature map <em>H<sup>c</sup>     </em> for context words and <em>H<sup>s</sup>     </em> for sentiment words.</p>    <p>After performing convolution operation, in order to select those crucial sentimental phrase chunks to model the sentiment-specific sentence representation, we design a new dynamic semantic attention. The details about dynamic semantics attention are formulated as follows: <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{align} &#x0026;\tilde{A}=softmax(H^c\lbrace H^s\rbrace ^T) \end{align} </span>      <br/>      <span class="equation-number">(2)</span>     </div>     </div>     <div class="table-responsive" id="eq2">     <div class="display-equation">      <span class="tex mytex">\begin{align} &#x0026;\tilde{T}=(H^c)^T \tilde{W} \tilde{A} \end{align} </span>      <br/>      <span class="equation-number">(3)</span>     </div>     </div>     <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{align} &#x0026;S=H^s \odot \tilde{T}, \bar{s}=\frac{\sum _{j=1}^m H^s[:,j]}{m} \end{align} </span>      <br/>      <span class="equation-number">(4)</span>     </div>     </div>     <div class="table-responsive" id="eq4">     <div class="display-equation">      <span class="tex mytex">\begin{align} \tilde{\alpha }=s&#x0026;oftmax(\upsilon ^T tanh(VH^c+\tilde{U}\bar{s}\otimes e_n)) \end{align} </span>      <br/>      <span class="equation-number">(5)</span>     </div>     </div>     <div class="table-responsive" id="eq5">     <div class="display-equation">      <span class="tex mytex">\begin{align} &#x0026;o=\sum _{i=1}^n\tilde{\alpha }_i H^s_i \end{align} </span>      <br/>      <span class="equation-number">(6)</span>     </div>     </div> where &#x2299; denotes element-wise multiplication, <em>o</em> is the final sentiment-specific sentence representation.</p>    <p>Finally, we feed the output vector <em>o</em> to a softmax layer to predict the sentiment distribution. The training objective is to minimize the cross-entropy error of the predicted and true class distributions.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experiments</h2>     </div>    </header>    <p>     <strong>Datasets and Sentiment Lexicon.</strong> Movie Review (MR)<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> and Stanford Sentiment Treebank (SST)<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a> are used for evaluating our proposed model. Sentiment lexicon combines the sentiment words from both [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] and&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>].</p>    <p>     <strong>Experiment Results.</strong> In our experiments, we use classification accuracy as the evaluation metric. The experimental results are shown in Table <a class="tbl" href="#tab1">1</a>. From the results, our proposed model is effective to improve the performance of sentence-level sentiment classification. To be specific, we have the following findings: (i) Our model significantly outperforms those methods that do not fully employ linguistic knowledge, such as RNN/ RNTN, LSTM, BiLSTM, CNN on both datasets, which verifies the effectiveness of the idea that integrates sentiment linguistic knowledge into the deep neural networks. (ii) Compared with LR-LSTM and LR-Bi-LSTM which employ linguistic role of sentiment, negation and intensity words to neural networks by linguistic regularization with the similar idea, our model outperforms LR-LSTM and LR-Bi-LSTM by a significant margin on the two datasets. It demonstrates the considerable superiority of our model.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Evaluation results. The best result on each dataset is in bold. Results with * are obtained either with the same codes shared by the original authors or by our own implementation.</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:center;">Methods</th>       <th style="text-align:center;">MR</th>       <th style="text-align:left;">SST(sent.-level)</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:center;">RNN(i.e.,RAE) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0005">5</a>]</td>       <td style="text-align:center;">77.7%</td>       <td style="text-align:left;">43.2%</td>      </tr>      <tr>       <td style="text-align:center;">RNTN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0006">6</a>]</td>       <td style="text-align:center;">75.9%*</td>       <td style="text-align:left;">45.7%</td>      </tr>      <tr>       <td style="text-align:center;">LSTM</td>       <td style="text-align:center;">77.4%*</td>       <td style="text-align:left;">45.6%*</td>      </tr>      <tr>       <td style="text-align:center;">BiLSTM</td>       <td style="text-align:center;">79.3%*</td>       <td style="text-align:left;">46.5%*</td>      </tr>      <tr>       <td style="text-align:center;">Tree-LSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0007">7</a>]</td>       <td style="text-align:center;">80.7%</td>       <td style="text-align:left;">48.1%</td>      </tr>      <tr>       <td style="text-align:center;">CNN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0003">3</a>]</td>       <td style="text-align:center;">81.5%</td>       <td style="text-align:left;">48.0%</td>      </tr>      <tr>       <td style="text-align:center;">NSCL [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0008">8</a>]</td>       <td style="text-align:center;">82.9%</td>       <td style="text-align:left;">47.1%</td>      </tr>      <tr>       <td style="text-align:center;">LR-LSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0004">4</a>]</td>       <td style="text-align:center;">81.5%</td>       <td style="text-align:left;">48.3%</td>      </tr>      <tr>       <td style="text-align:center;">LR-Bi-LSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0004">4</a>]</td>       <td style="text-align:center;">82.1%</td>       <td style="text-align:left;">48.6%</td>      </tr>      <tr>       <td style="text-align:center;">        <strong>LAAN(our model)</strong>       </td>       <td style="text-align:center;">        <strong>83.9</strong>%</td>       <td style="text-align:left;">        <strong>49.1</strong>%</td>      </tr>     </tbody>     </table>    </div>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work was supported in part by the Research Fund for the development of strategic emerging industries by ShenZhen city (No.JCYJ20160301151844537 and No. JCYJ20160331104524983)</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Mauro Dragoni. 2017. A Three-Phase Approach for Exploiting Opinion Mining in Computational Advertising. <em>      <em>IEEE Intelligent Systems</em>     </em>(2017), 21&#x2013;27.</li>     <li id="BibPLXBIB0002" label="[2]">Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In <em>      <em>Proceedings of SIGKDD</em>     </em>.</li>     <li id="BibPLXBIB0003" label="[3]">Yoon Kim. 2014. Convolutional neural networks for sentence classification. <em>      <em>Proceedings of EMNLP 2014</em>     </em>.</li>     <li id="BibPLXBIB0004" label="[4]">Qiao Qian, Minlie Huang, Jinhao Lei, and Xiaoyan Zhu. 2017. Linguistically Regularized LSTM for Sentiment Classification. In <em>      <em>Proceedings of ACL 2017</em>     </em>.</li>     <li id="BibPLXBIB0005" label="[5]">Richard Socher, Jeffrey Pennington, Eric&#x00A0;H Huang, Andrew&#x00A0;Y Ng, and Christopher&#x00A0;D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In <em>      <em>Proceedings of EMNLP 2011</em>     </em>.</li>     <li id="BibPLXBIB0006" label="[6]">Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher&#x00A0;D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In <em>      <em>Proceedings of EMNLP 2013</em>     </em>.</li>     <li id="BibPLXBIB0007" label="[7]">Kai&#x00A0;Sheng Tai, Richard Socher, and Christopher&#x00A0;D. Manning. 2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. In <em>      <em>Proceedings of ACL 2015</em>     </em>.</li>     <li id="BibPLXBIB0008" label="[8]">Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016. Context-Sensitive Lexicon Features for Neural Sentiment Analysis.. In <em>      <em>Proceedings of EMNLP 2016</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>Corresponding author</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break"     href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://nlp.stanford.edu/sentiment/">https://nlp.stanford.edu/sentiment/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186922">https://doi.org/10.1145/3184558.3186922</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
