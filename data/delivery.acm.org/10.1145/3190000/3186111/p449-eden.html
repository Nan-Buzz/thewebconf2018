<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Provable and Practical Approximations for the Degree Distribution using Sublinear Graph Samples</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Provable and Practical Approximations for the Degree Distribution using Sublinear Graph Samples</span><a class="fn" href="#fn2" id="foot-fn2"><sup>*</sup></a><a class="fn" href="#fn3" id="foot-fn3"><sup>&#x2020;</sup></a>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Talya</span>     <span class="surName">Eden</span>,     School of Computer Science, Tel Aviv University, Tel Aviv, Israel, <a href="mailto:talyaa01@gmail.com">talyaa01@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Shweta</span>     <span class="surName">Jain</span>,     University of California, Santa Cruz, Santa Cruz, CA, USA, <a href="mailto:sjain12@ucsc.edu">sjain12@ucsc.edu</a>    </div>    <div class="author">     <span class="givenName">Ali</span>     <span class="surName">Pinar</span>,     Sandia National Laboratories, Livermore, CA, <a href="mailto:apinar@sandia.gov">apinar@sandia.gov</a>    </div>    <div class="author">     <span class="givenName">Dana</span>     <span class="surName">Ron</span>,     School of Computer Science, Tel Aviv University, Tel Aviv, Israel, <a href="mailto:danaron@tau.ac.il">danaron@tau.ac.il</a>    </div>    <div class="author">     <span class="givenName">C.</span>     <span class="surName">Seshadhri</span>,     University of California, Santa Cruz, Santa Cruz, CA, <a href="mailto:sesh@ucsc.edu">sesh@ucsc.edu</a>    </div>        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186111" target="_blank">https://doi.org/10.1145/3178876.3186111</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>The degree distribution is one of the most fundamental properties used in the analysis of massive graphs. There is a large literature on <em>graph sampling</em>, where the goal is to estimate properties (especially the degree distribution) of a large graph through a small, random sample. Estimating the degree distribution of real-world graphs poses a significant challenge, due to their heavy-tailed nature and the large variance in degrees.</small>    </p>    <p>     <small>We design a new algorithm, <SmallCap>SADDLES</SmallCap>, for this problem, using recent mathematical techniques from the field of <em>sublinear algorithms</em>. The <SmallCap>SADDLES</SmallCap> algorithm gives provably accurate outputs for all values of the degree distribution. For the analysis, we define two fatness measures of the degree distribution, called the <em>       <em>h</em>-index</em> and the <em>       <em>z</em>-index</em>. We prove that <SmallCap>SADDLES</SmallCap> is sublinear in the graph size when these indices are large. A corollary of this result is a provably sublinear algorithm for any degree distribution bounded below by a power law.</small>    </p>    <p>     <small>We deploy our new algorithm on a variety of real datasets and demonstrate its excellent empirical behavior. In all instances, we get extremely accurate approximations for all values in the degree distribution by observing at most 1% of the vertices. This is a major improvement over the state-of-the-art sampling algorithms, which typically sample more than 10% of the vertices to give comparable results. We also observe that the <em>h</em> and <em>z</em>-indices of real graphs are large, validating our theoretical analysis.</small>    </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Talya Eden, Shweta Jain, Ali Pinar, Dana Ron, and C. Seshadhri. 2018. Provable and Practical Approximations for the Degree Distribution using Sublinear Graph Samples. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186111" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186111</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>In domains as diverse as social sciences, biology, physics, cybersecurity, graphs are used to represent entities and the relationships between them. This has led to the explosive growth of network science as a discipline over the past decade. One of the hallmarks of network science is the occurrence of specific graph properties that are common to varying domains, such as heavy tailed degree distributions, large clustering coefficients, and small-world behavior. The degree distribution is especially significant, since the early days of modern network science&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>].</p>    <p>Given an undirected graph <em>G</em>, the degree distribution (or technically, histogram) is the sequence of numbers <em>n</em>(1), <em>n</em>(2), &#x2026;, where <em>n</em>(<em>d</em>) is the number of vertices of degree <em>d</em>. In almost all real-world scenarios, the average degree is small, but the variance (and higher moments) is large. Even for relatively large <em>d</em>, <em>n</em>(<em>d</em>) is still non-zero, and <em>n</em>(<em>d</em>) typically has a smooth non-increasing behavior. In Fig. <a class="fig" href="#fig1">1</a>, we see the typical degree distribution behavior. The average degree in a Google web network is less than 10, but the maximum degree is more than 5000. There are also numerous vertices with all intermediate degrees. This is referred to as a &#x201C;heavy tailed&#x201D; distribution. The degree distribution, especially the tail, is of significant relevance to modeling networks, determining their resilience, spread of information, and for algorithmics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>].</p>    <p>With full access to <em>G</em>, the degree distribution can be computed in linear time, by simply determining the degree of each vertex. Yet in many scenarios, we only have <em>partial</em> access to the graph, provided through some graph samples. A naive extrapolation of the degree distribution can result in biased results. The seminal research paper of Faloutsos et al. claimed a power law in the degree distribution on the Internet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>]. This degree distribution was deduced by measuring a power law distribution in the graph sample generated by a collection of traceroute queries on a set of routers. Unfortunately, it was mathematically and empirically proven that traceroute responses can have a power law <em>even if the true network does not</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. In general, a direct extrapolation of the degree distribution from a graph subsample is not valid for the underlying graph. This leads to the primary question behind our work.</p>    <p>    <em>How can we provably and practically estimate the degree distribution without seeing the entire graph?</em>    <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186111/images/www2018-120-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">The output of <SmallCap>SADDLES</SmallCap> on a collection of networks: <tt>amazon0601</tt> (403K vertices, 4.9M edges), <tt>web-Google</tt> (870K vertices, 4.3M edges), <tt>cit-Patents</tt> (3.8M vertices, 16M edges), <tt>com-orkut</tt> social network (3M vertices, 117M edges). <SmallCap>SADDLES</SmallCap> samples 1% of the vertices and gives accurate results for the entire (cumulative) degree distribution. For comparison, we show the output of a number of sampling algorithms from past work, each run with the same number of samples. (Because of the size of <tt>com-Orkut</tt>, methods involving optimization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0046">46</a>] fail to produce an estimate in reasonable time.)</span>     </div>    </figure>    </p>    <p>There is a rich literature in statistics, data mining, and physics on estimating graph properties (especially the degree distribution) using a small subsample&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>]. Nonetheless, there is no provable algorithm for the entire degree distribution, with a formal analysis on when it is sublinear in the number of vertices. Furthermore, most empirical studies typically sample 10-30% of the vertices for reasonable estimates.</p>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.1</span> Problem description</h3>     </div>    </header>    <p>We focus on the <em>complementary cumulative degree histogram</em> (often called the cumulative degree distribution) or <em>ccdh</em> of <em>G</em>. This is the sequence {<em>N</em>(<em>d</em>)}, where <em>N</em>(<em>d</em>) = &#x2211;<sub>      <em>r</em> &#x2265; <em>d</em>     </sub>     <em>n</em>(<em>r</em>) is the number of vertices of degree at least <em>d</em>. The ccdh is typically used for fitting distributions, since it averages out noise and is monotonic&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. Our aim is to get an accurate bicriteria approximation to the ccdh of <em>G</em>, at all values of <em>d</em>.</p>    <div class="definition" id="enc1">     <Label>Definition 1.1.</Label>     <p> The sequence <span class="inline-equation"><span class="tex">$\lbrace \widetilde{N}(d)\rbrace$</span>      </span> is an (&#x025B;, &#x025B;)-estimate of the ccdh if &#x2200;<em>d</em>, <span class="inline-equation"><span class="tex">$(1-\varepsilon)N((1+\varepsilon)d) \le \widetilde{N}(d) \le (1+\varepsilon)N((1-\varepsilon)d)$</span>      </span>.</p>    </div>    <p>Computing an (&#x025B;, &#x025B;)-estimate is significantly harder than approximating the ccdh using standard distribution measures. Statistical measures, such as the KS-distance, <em>&#x03C7;</em>     <sup>2</sup>, &#x2113;<sub>      <em>p</em>     </sub>-norms, etc. tend to ignore the tail, since (in terms of probability mass) it is a negligible portion of the distribution. An (&#x025B;, &#x025B;)-estimate is accurate for all <em>d</em>.</p>    <p>     <strong>The query model:</strong> A formal approach requires specifying a <em>query model</em> for accessing <em>G</em>. We look to the subfields of property testing and sublinear algorithms within theoretical computer science for such models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>]. Consider the following three kinds of <em>queries</em>.</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">Vertex queries: acquire a uniform random vertex <em>v</em> &#x2208; <em>V</em>.<br/></li>     <li id="list2" label="&#x2022;">Neighbor queries: given <em>v</em> &#x2208; <em>V</em>, acquire a uniform random neighbor <em>u</em> of <em>V</em>.<br/></li>     <li id="list3" label="&#x2022;">Degree queries: given <em>v</em> &#x2208; <em>V</em>, acquire the degree <em>d<sub>v</sub>      </em>.<br/></li>    </ul>    <p>An algorithm is only allowed to make these queries to process the input. It has to make some number of queries, and finally produce an output. We discuss two query models, and give results for both.</p>    <ul class="list-no-style">     <li id="uid9" label="The Standard Model (SM)">All queries allowed: This is the standard model in numerous sublinear algorithms results&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0025">25</a>]. Furthermore, most papers on graph sampling implicitly use this model for generating subsamples. Indeed, any method involving crawling from a random set of vertices and collecting degrees is in the SM. This model is the primary setting for our work, and allows for comparison with rich body of graph sampling algorithms. It is worth noting that in the SM, one can determine the entire degree distribution in <em>O</em>(<em>n</em>log&#x2009;<em>n</em>) queries (the extra log&#x2009;<em>n</em> factor comes from the coupon collector bound of finding all the vertices through uniform sampling). Thus, it makes sense to express the number of queries made by an algorithm as a fraction of <em>n</em>. Alternately, the number of queries is basically the number of vertices encountered by the algorithm. Thus, a sublinear algorithm makes <em>o</em>(<em>n</em>) queries.<br/></li>     <li id="uid10" label="The Hidden Degrees Model (HDM)">Vertex and neighbor queries allowed, not degree queries: This is a substantially weaker model. In numerous cybersecurity and network monitoring settings, an algorithm cannot query for degrees, and has to infer them indirectly. Observe that this model is significantly harder than the SM. It takes <em>O</em>((<em>m</em> + <em>n</em>)log&#x2009;<em>n</em>) to determine all the degrees, since one has to at least visit all the edges to find degrees exactly. In this model, we express the number of queries as a fraction of <em>m</em>.<br/></li>    </ul>    <p>     <em>Regarding uniform random vertex queries:</em>. This is a fairly powerful query, that may not be realizable in all situations. Indeed, Chierichetti et al. explicitly study this problem in social networks and design (non-trivial) algorithms for sampling uniform random vertices&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>]. In a previous work, Dasgupta, Kumar, and Sarlos study algorithms for estimating average degree when only random walks are possible&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>]. Despite this power, we believe that SM is a good testbed for understanding <em>when</em> a small sample of a graph provably gives properties of the whole. Furthermore, in the context of graph sampling, access to uniform random vertices is commonly (implicitly) assumed&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]. The vast majority of experiments conducted often use uniform random vertices.</p>    <p>As a future direction, we believe it is important to investigate sampling models without random vertex queries.</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.2</span> Our contributions</h3>     </div>    </header>    <p>Our main theoretical result is a new sampling algorithm, the Sublinear Approximations for Degree Distributions Leveraging Edge Samples, or <SmallCap>SADDLES</SmallCap>. This algorithm provably provides (&#x025B;, &#x025B;)-approximations for the ccdh. We show how to design <SmallCap>SADDLES</SmallCap> under both the SM and the HDM. We apply <SmallCap>SADDLES</SmallCap> on a variety of real datasets and demonstrate its ability to accurately approximate the ccdh with a tiny sample of the graph.</p>    <ul class="list-no-style">     <li id="list4" label="&#x2022;"><strong>Sampling algorithm for estimating ccdh:</strong> Our algorithm combines a number of techniques in random sampling to get (&#x025B;, &#x025B;)-estimates for the ccdh. A crucial component is an application of an edge simulation technique, first devised by Eden et al. in the context of triangle counting&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>]. This (theoretical) technique shows how to get a collection of weakly correlated uniform random edges from independent uniform vertices. <SmallCap>SADDLES</SmallCap> employs a weighting scheme on top of this method to estimate the ccdh.<br/></li>     <li id="list5" label="&#x2022;"><strong>Heavy tails leads to sublinear algorithms:</strong> The challenge in analyzing <SmallCap>SADDLES</SmallCap> is in finding parameters of the ccdh that allow for sublinear query complexity. To that end, we discuss two parameters that measure &#x201C;heaviness&#x201D; of the distribution tail: the classic <em>h</em>-index and a newly defined <em>z</em>-index. We prove that the query complexity of <SmallCap>SADDLES</SmallCap> is sublinear (for both models) whenever these indices are large.<br/></li>     <li id="list6" label="&#x2022;"><strong>Excellent empirical behavior:</strong> We deploy an implementation of <SmallCap>SADDLES</SmallCap> on a collection of large real-world graphs. In all instances, we achieve extremely accurate estimates for the entire ccdh by sampling at most 1% of the vertices of the graph. Refer to Fig. <a class="fig" href="#fig1">1</a>. Observe how <SmallCap>SADDLES</SmallCap> tracks various jumps in the ccdh, for all graphs in Fig. <a class="fig" href="#fig1">1</a>.<br/></li>     <li id="list7" label="&#x2022;"><strong>Comparison with existing sampling methods:</strong> A number of graph sampling methods have been proposed in practice, such as vertex sampling (VS), snowball sampling (OWS), forest-fire sampling (FF), induced graph sampling (IN), random walk (RWJ), edge sampling (ES) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0046">46</a>]. A recent work of Zhang et al. explicitly addresses biases in these sampling methods, and fixes them using optimization techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0046">46</a>]. We run head-to-head comparisons with all these sampling methods, and demonstrate the <SmallCap>SADDLES</SmallCap> gives significantly better practical performance. Fig. <a class="fig" href="#fig1">1</a>1 shows the output of all these sampling methods with a total sample size of 1% of the vertices. Observe how across the board, the methods make erroneous estimates for most of the degree distribution. The errors are also very large, for all the methods. This is consistent with previous work, where methods sample more than 10% of the number of vertices.<br/></li>    </ul>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.3</span> Theoretical results in detail</h3>     </div>    </header>    <p>Our main theoretical result is a new sampling algorithm, the Sublinear Approximations for Degree Distributions Leveraging Edge Samples, or <SmallCap>SADDLES</SmallCap>.</p>    <p>We first demonstrate our results for power law degree distributions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]. Statistical fitting procedures suggest they occur to some extent in the real-world, albeit with much noise&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. The classic power law degree distribution sets <em>n</em>(<em>d</em>)&#x221D;1/<em>d<sup>&#x03B3;</sup>     </em>, where <em>&#x03B3;</em> is typically in [2, 3]. We build on this to define a power law lower bound.</p>    <div class="definition" id="enc2">     <Label>Definition 1.2.</Label>     <p> Fix <em>&#x03B3;</em> > 2. A degree distribution is bounded below by a power law with exponent <em>&#x03B3;</em>, if the ccdh satisfies the following property. There exists a constant <em>&#x03C4;</em> > 0 such that for all <em>d</em>, <em>N</em>(<em>d</em>) &#x2265; &#x230A;<em>&#x03C4;n</em>/<em>d</em>      <sup>       <em>&#x03B3;</em> &#x2212; 1</sup>&#x230B;.</p>    </div>    <p>The following is a corollary of our main result. For convenience, we will suppress query complexity dependencies on &#x025B; and log&#x2009;<em>n</em> factors, using <span class="inline-equation"><span class="tex">$\widetilde{O}(\cdot)$</span>     </span>.</p>    <div class="theorem" id="enc3">     <Label>Theorem 1.3.</Label>     <p> Suppose the degree distribution of <em>G</em> is bounded below by a power law with exponent <em>&#x03B3;</em>. Let the average degree be denoted by <span class="inline-equation"><span class="tex">$\overline{d}$</span>      </span>. For any &#x025B; > 0, the <SmallCap>SADDLES</SmallCap> algorithm outputs (with high probability) an (&#x025B;, &#x025B;)-approximation to the ccdh and makes the following number of queries. For SM: <span class="inline-equation"><span class="tex">$\widetilde{O}(n^{1-\frac{1}{\gamma }} + n^{1-\frac{1}{\gamma -1}} \overline{d})$</span>      </span>. For HDM: <span class="inline-equation"><span class="tex">$\widetilde{O}(n^{1-\frac{1}{2(\gamma -1)}} \overline{d})$</span>      </span>     </p>    </div>    <p>In most real-world instances, the average degree <span class="inline-equation"><span class="tex">$\overline{d}$</span>     </span> is typically constant. Thus, the complexities above are strongly sublinear. For example, when <em>&#x03B3;</em> = 2, we get <span class="inline-equation"><span class="tex">$\widetilde{O}(n^{1/2})$</span>     </span> for both models. When <em>&#x03B3;</em> = 3, we get <span class="inline-equation"><span class="tex">$\widetilde{O}(n^{2/3})$</span>     </span> and <span class="inline-equation"><span class="tex">$\widetilde{O}(n^{3/4})$</span>     </span>.</p>    <p>Our main result is more nuanced, and holds for all degree distributions. If the ccdh has a heavy tail, we expect <em>N</em>(<em>d</em>) to be reasonably large even for large values of <em>d</em>. We describe two formalisms of this notion, through <em>fatness indices</em>.</p>    <div class="definition" id="enc4">     <Label>Definition 1.4.</Label>     <p> The <em>       <em>h</em>-index</em> of the degree distribution is the largest <em>d</em> such that there are at least <em>d</em> vertices of degree at least <em>d</em>.</p>    </div>    <p>This is the exact analogy of the bibliometric <em>h</em>-index&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. As we show in the [sec:indices]&#x00A7; 2.1, <em>h</em> can be approximated by min&#x2009;<sub>      <em>d</em>     </sub>(<em>d</em> + <em>N</em>(<em>d</em>))/2. A more stringent index is obtained by replacing the arithmetic mean by the (smaller) geometric mean.</p>    <div class="definition" id="enc5">     <Label>Definition 1.5.</Label>     <p> The <em>       <em>z</em>-index</em> of the degree distribution is <span class="inline-equation"><span class="tex">$z = \min _{d: N(d) {\gt} 0} \sqrt {d\cdot N(d)}$</span>      </span>.</p>    </div>    <p>Our main theorem asserts that large <em>h</em> and <em>z</em> indices lead to a sublinear algorithm for degree distribution estimation. Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc3">1.3</a> is a direct corollary obtained by plugging in values of the indices for power laws.</p>    <div class="theorem" id="enc6">     <Label>Theorem 1.6.</Label>     <p> For any &#x025B; > 0, the <SmallCap>SADDLES</SmallCap> algorithm outputs (with high probability) an (&#x025B;, &#x025B;)-approximation to the ccdh, and makes the following number of queries. For SM: <span class="inline-equation"><span class="tex">$\widetilde{O}(n/h + m/z^2)$</span>      </span>. For HDM: <span class="inline-equation"><span class="tex">$\widetilde{O}(m/z)$</span>      </span>.</p>    </div>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.4</span> Challenges and Main Idea</h3>     </div>    </header>    <p>The heavy-tailed behavior of the real degree distribution poses the primary challenge to computing (&#x025B;, &#x025B;)-estimates to the ccdh. Sampling uniform random vertices is inefficient when <em>N</em>(<em>d</em>) is small. A random neighbor of a random vertex is more likely to be a high degree vertex. This is the idea behind methods like OWS, FF, RWJ graph sample-and-hold, etc.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]. But these lead to biased samples, since vertices with the same degree may be picked with differing probabilities.</p>    <p>A direct extrapolation/scaling of the degrees in the observed graph does not provide an accurate estimate. Our experiments show that existing methods always miss the head or the tail. From a mathematical standpoint, the vast majority of existing results tend to analyze the KS-statistic, or some &#x2113;<sub>      <em>p</em>     </sub>-norm. As we mentioned earlier, this does not work well for measuring the quality of the estimate at all scales. As shown by our experiments, none of these methods give accurate estimate for the entire ccdh with less than 5% of the vertices.</p>    <p>The main innovation in <SmallCap>SADDLES</SmallCap> comes through the use of a recent theoretical technique to simulate edge samples through vertex samples&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>]. The sampling of edges occurs through two stages. In the first stage, the algorithm samples a set of <em>r</em> vertices and sets up a distribution over the sampled vertices such that any edge adjacent to a sampled vertex may be sampled with uniform probability. In the second stage, it samples <em>q</em> edges from this distribution. While a single edge is uniform random, the set of edges are correlated.</p>    <p>For a given <em>d</em>, we define a weight function on the edges, such that the total weight is exactly <em>N</em>(<em>d</em>). <SmallCap>SADDLES</SmallCap> estimates the total weight by scaling up the average weight on a random sample of edges, generated as discussed above. The difficulty in the analysis is the correlation between the edges. Our main insight is that if the degree distribution has a fat tail, this correlation can be contained even for sublinear <em>r</em> and <em>q</em>. Formally, this is achieved by relating the concentration behavior of the average weight of the sample to the <em>h</em> and <em>z</em>-indices. The final algorithm combines this idea with vertex sampling to get accurate estimates for all <em>d</em>.</p>    <p>The HDM is dealt with using birthday paradox techniques formalized by Ron and Tsur&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>]. It is possible to estimate the degree <em>d<sub>v</sub>     </em> using <span class="inline-equation"><span class="tex">$O(\sqrt {d_v})$</span>     </span> neighbor queries. But this adds overhead to the algorithm, especially for estimating the ccdh at the tail. As discussed earlier, we need methods that bias towards higher degrees, but this significantly adds to the query cost of actually estimating the degrees.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.5</span> Related Work</h3>     </div>    </header>    <p>There is a rich body of literature on generating a graph sample that reveals graph properties of the larger &#x201C;true&#x201D; graph. We do not attempt to fully survey this literature, and only refer to results directly related to our work. The works of Leskovec &#x0026; Faloutsos&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>], Maiya &#x0026; Berger-Wolf&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], and Ahmed, Neville, &#x0026; Kompella&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] provide excellent surveys of multiple sampling methods.</p>    <p>There are a number of sampling methods based on random crawls: forest-fire&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>], snowball sampling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], and expansion sampling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]. As has been detailed in previous work, these methods tend to bias certain parts of the network, which can be exploited for more accurate estimates of various properties&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>]. A series of papers by Ahmed, Neville, and Kompella&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] have proposed alternate sampling methods that combine random vertices and edges to get better representative samples.</p>    <p>All these results aim to capture numerous properties of the graph, using a single graph sample. Nonetheless, there is much previous work focused on the degree distribution. Ribiero and Towsley&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] and Stumpf and Wiuf&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>] specifically study degree distributions. Ribiero and Towsley&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] do detailed analysis on degree distribution estimates (they also look at the ccdh) for a variety of these sampling methods. Their empirical results show significant errors either at the head or the tail. We note that almost all these results end up sampling up to 20% of the graph to estimate the degree distribution.</p>    <p>Zhang et al. observe that the degree distribution of numerous sampling methods is a random linear projection of the true distribution&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]. They attempt to invert this (ill-conditioned) linear problem, to correct the biases. This leads to improvement in the estimate, but the empirical studies typically sample more than 10% of the vertices for good estimates.</p>    <p>Some methods try to match the shape/family of the distribution, rather than estimate it as a whole&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>]. Thus, statistical methods can be used to estimate parameters of the distribution. But it is reasonably well-established that real-world degree distributions are rarely pure power laws in most instances&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. Indeed, fitting a power law is rather challenging and naive regression fits on log-log plots are erroneous, as results of Clauset-Shalizi-Newman showed &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>].</p>    <p>The subfield of <em>property testing and sublinear algorithms for sparse graphs</em> within theoretical computer science can be thought of as a formalization of graph sampling to estimate properties. Indeed, our description of the main problem follows this language. There is a very rich body of mathematical work in this area (refer to Ron&#x0027;s survey&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]). Practical applications of graph property testing are quite rare, and we are only aware of one previous work on applications for finding dense cores in router networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. The specific problem of estimating the average degree (or the total number of edges) was studied by Feige&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>] and Goldreich-Ron&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>]. Gonen et al. and Eden et al. focus on the problem of estimating higher moments of the degree distribution&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. One of the main techniques we use of simulating edge queries was developed in sublinear algorithms results of Eden et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] in the context of triangle counting and degree moment estimation. We stress that all these results are purely theoretical, and their practicality is by no means obvious.</p>    <p>On the practical side, Dasgupta, Kumar, and Sarlos study average degree estimation in real graphs, and develop alternate algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>]. They require the graph to have low mixing time and demonstrate that the algorithm has excellent behavior in practice (compared to implementations of Feige&#x0027;s and the Goldreich-Ron algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>]). Dasgupta et al. note that sampling uniform random vertices is not possible in many settings, and thus they consider a significantly weaker setting than SM or HDM. Chierichetti et al. focus on sampling uniform random vertices, using only a small set of seed vertices and neighbor queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>].</p>    <p>We note that there is a large body of work on sampling graphs from a stream&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. This is quite different from our setting, since a streaming algorithm observes every edge at least once. The specific problem of estimating the degree distribution at all scales was considered by Simpson et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>]. They observe many of the challenges we mentioned earlier: the difficulty of estimating the tail accurately, finding vertices at all degree scales, and combining estimates from the head and the tail.</p>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>    </div>    </header>    <p>We say that the input graph <em>G</em> has <em>n</em> vertices and <em>m</em> edges and <em>m</em> &#x2265; <em>n</em> (since isolated vertices are not relevant here). For any vertex <em>v</em>, let <em>&#x0393;</em>(<em>v</em>) be the neighborhood of <em>v</em>, and <em>d<sub>v</sub>    </em> be the degree. As mentioned earlier, <em>n</em>(<em>d</em>) is the number of vertices of degree <em>d</em> and <em>N</em>(<em>d</em>) = &#x2211;<sub>     <em>r</em> &#x2265; <em>d</em>    </sub>    <em>n</em>(<em>r</em>) is the ccdh at <em>d</em>. We use &#x201C;u.a.r.&#x201D; as a shorthand for &#x201C;uniform at random&#x201D;. We stress that the all mention of probability and error is with respect to the randomness of the sampling algorithm. There is no stochastic assumption on the input graph <em>G</em>. We use the shorthand <em>A</em> &#x2208; (1 &#x00B1; <em>&#x03B1;</em>)<em>B</em> for <em>A</em> &#x2208; [(1 &#x2212; <em>&#x03B1;</em>)<em>B</em>, (1 + <em>&#x03B1;</em>)<em>B</em>]. We will apply the following (rescaled) Chernoff bound.</p>    <p>    <div class="theorem" id="enc7">     <Label>Theorem 2.1.</Label>     <p> [Theorem 1 in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>]] Let <em>X</em>      <sub>1</sub>, <em>X</em>      <sub>2</sub>, &#x2026;, <em>X<sub>k</sub>      </em> be a sequence of iid random variables with expectation <em>&#x03BC;</em>. Furthermore, <em>X<sub>i</sub>      </em> &#x2208; [0, <em>B</em>].</p>     <p>      <ul class="list-no-style">       <li id="list8" label="&#x2022;">For &#x025B; < 1, <span class="inline-equation"><span class="tex">$ \Pr [|\sum _{i=1}^k X_i - \mu k| \ge \varepsilon \mu k] \le 2\exp (-\varepsilon ^2 \mu k/3B)$</span>       </span>.<br/></li>       <li id="list9" label="&#x2022;">For <em>t</em> &#x2265; 2<em>e&#x03BC;</em>, <span class="inline-equation"><span class="tex">$\Pr [\sum _{i=1}^k X_i \ge tk] \le 2^{-tk/B}$</span>       </span>.<br/></li>      </ul>     </p>    </div>    </p>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> More on Fatness indices</h3>     </div>    </header>    <p>(All proofs in this section are fairly straightforward calculations, and are hence omitted in this version.) The following characterization of the <em>h</em>-index will be useful for analysis. Since (<em>d</em> + <em>N</em>(<em>d</em>))/2 &#x2264; max&#x2009;(<em>d</em>, <em>N</em>(<em>d</em>)) &#x2264; <em>d</em> + <em>N</em>(<em>d</em>), this proves that min&#x2009;<sub>      <em>d</em>     </sub>(<em>d</em> + <em>N</em>(<em>d</em>))/2 is a 2-factor approximation to the <em>h</em>-index.</p>    <div class="lemma" id="enc8">     <Label>Lemma 2.2.</Label>     <p> min&#x2009;<sub>       <em>d</em>      </sub>max&#x2009;(<em>d</em>, <em>N</em>(<em>d</em>)) &#x2208; {<em>h</em>, <em>h</em> + 1}</p>    </div>    <p>The <em>h</em>-index does not measure <em>d</em> vs <em>N</em>(<em>d</em>) at different scales, and a large <em>h</em>-index only ensures that there are &#x201C;enough&#x201D; high-degree vertices. For instance, the h-index does not distinguish between two different distributions whose ccdh <em>N</em>     <sub>1</sub> and <em>N</em>     <sub>2</sub> are such that <em>N</em>     <sub>1</sub>(100) = 100 and <em>N</em>     <sub>1</sub>(<em>d</em>) = 0 for <em>d</em> > 100, and <em>N</em>     <sub>2</sub>(100, 000) = 100 and <em>N</em>     <sub>2</sub>(<em>d</em>) = 100 for all other values of <em>d</em> &#x2265; 100. The <em>h</em>-index in both these cases is 100.</p>    <p>The <em>h</em> and <em>z</em>-indices are related to each other.</p>    <div class="claim" id="enc9">     <Label>Claim 2.3.</Label>     <p>      <span class="inline-equation"><span class="tex">$\sqrt {h} \le z \le h$</span>      </span>.</p>    </div>    <p>To give some intuition about these indices, we compute the <em>h</em> and <em>z</em> indices for power laws. The classic power law degree distribution sets <em>n</em>(<em>d</em>)&#x221D;1/<em>d<sup>&#x03B3;</sup>     </em>, where <em>&#x03B3;</em> is typically in [2, 3].</p>    <div class="claim" id="enc10">     <Label>Claim 2.4.</Label>     <p> If a degree distribution is bounded below by a power law with exponent <em>&#x03B3;</em>, then <span class="inline-equation"><span class="tex">$h = \Omega (n^\frac{1}{\gamma })$</span>      </span> and <span class="inline-equation"><span class="tex">$z = \Omega (n^\frac{1}{2(\gamma -1)})$</span>      </span>.</p>    </div>    <p>Plugging in values, for <em>&#x03B3;</em> = 2, both <em>h</em> and <em>z</em> are <span class="inline-equation"><span class="tex">$\Omega (\sqrt {n})$</span>     </span>. For <em>&#x03B3;</em> = 3, <em>h</em> = <em>&#x0398;</em>(<em>n</em>     <sup>1/3</sup>) and <em>z</em> = <em>&#x0398;</em>(<em>n</em>     <sup>1/4</sup>).</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Simulating degree queries for HDM</h3>     </div>    </header>    <p>The HDM does not allow for querying the degree <em>d<sub>v</sub>     </em> of a vertex <em>v</em>. Nonetheless, it is possible to get accurate estimates of <em>d<sub>v</sub>     </em> using the birthday paradox argument, as formalized by Ron and Tsur&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>], by sampling u.a.r. neighbors (with replacement) of <em>v</em> until the same vertex is seen twice. If this happens after <em>t</em> samples, <em>t</em>     <sup>2</sup> is a constant factor approximation for <em>d<sub>v</sub>     </em>. The following can be obtained directly from Theorem 3.1 of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>]. (Details omitted in this version.)</p>    <div class="corollary" id="enc11">     <Label>Corollary 2.5.</Label>     <p> There is an algorithm <SmallCap>DEG</SmallCap> that takes as input a vertex <em>v</em>, and has the following properties:</p>     <p>      <ul class="list-no-style">       <li id="list10" label="&#x2022;">For all <em>v</em>: with probability > 1 &#x2212; 1/<em>n</em>       <sup>3</sup>, the output <SmallCap>DEG</SmallCap>(<em>v</em>) is in (1 &#x00B1; &#x025B;/10)<em>d<sub>v</sub>       </em>.<br/></li>       <li id="list11" label="&#x2022;">The expected running time and query complexity of <SmallCap>DEG</SmallCap>(<em>v</em>) is <span class="inline-equation"><span class="tex">$O(\varepsilon ^{-2}\sqrt {d_v}\log n)$</span>       </span>.<br/></li>      </ul>     </p>    </div>    <p>We will assume that invocations of <SmallCap>DEG</SmallCap> with the same arguments use the same sequence of random bits. Alternately, imagine that a call to <SmallCap>DEG</SmallCap>(<em>v</em>) stores the output, so subsequent calls output the same value. For the sake of analysis, it is convenient to imagine that <SmallCap>DEG</SmallCap>(<em>v</em>) is called once for all vertices <em>v</em>, and these results are stored.</p>    <div class="definition" id="enc12">     <Label>Definition 2.6.</Label>     <p> The output <SmallCap>DEG</SmallCap>(<em>v</em>) is denoted by <span class="inline-equation"><span class="tex">$\hat{d_v}$</span>      </span>. The random bits used in all calls to <SmallCap>DEG</SmallCap> is collectively denoted <em>&#x039B;</em>. (Thus, <em>&#x039B;</em> completely specifies all the values <span class="inline-equation"><span class="tex">$\lbrace \hat{d_v}\rbrace$</span>      </span>.) We say <em>&#x039B;</em> is <em>good</em> if &#x2200;<em>v</em> &#x2208; <em>V</em>, <span class="inline-equation"><span class="tex">$\hat{d_v} \in (1\pm \varepsilon /10)d_v$</span>      </span>.</p>    </div>    <p>The following is a simple consequence of conditional probabilities (proof omitted in this version).</p>    <div class="claim" id="enc13">     <Label>Claim 2.7.</Label>     <p> Consider any event <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>      </span>, such that for any good <em>&#x039B;</em>, <span class="inline-equation"><span class="tex">$\Pr [\mathcal {A} | \Lambda ] \ge p$</span>      </span>. Then <span class="inline-equation"><span class="tex">$\Pr [\mathcal {A}] \ge p - 1/n^2$</span>      </span>.</p>    </div>    <p>For any fixed <em>&#x039B;</em>, we set <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d)$</span>     </span> to be <span class="inline-equation"><span class="tex">$|\lbrace v | \hat{d_v} \ge d\rbrace |$</span>     </span>. We will perform the analysis of <SmallCap>SADDLES</SmallCap> with respect to the <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }$</span>     </span>-values. (Proof is a straightforward calculation, and omitted in this version.)</p>    <div class="claim" id="enc14">     <Label>Claim 2.8.</Label>     <p> Suppose <em>&#x039B;</em> is good. For all <em>v</em>, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(v) \in [N((1+\varepsilon /9)d), N((1-\varepsilon /9)d)]$</span>      </span>.</p>    </div>    </section>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> The Main Result and <SmallCap>SADDLES</SmallCap>     </h2>    </div>    </header>    <p>We begin by stating the main result on the <SmallCap>SADDLES</SmallCap> procedure. Note that <em>D</em> refers to a set of degrees, for which we desire an approximation to <em>N</em>(<em>d</em>).</p>    <p>    <div class="theorem" id="enc15">     <Label>Theorem 3.1.</Label>     <p> There exists an algorithm <SmallCap>SADDLES</SmallCap> with the following properties. Let <em>c</em> be a sufficiently large constant. Fix any &#x025B; > 0, <em>&#x03B4;</em> > 0. Suppose that the parameters of <SmallCap>SADDLES</SmallCap> satisfy the following conditions: <em>r</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>      <em>n</em>/<em>h</em>, <em>q</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>      <em>m</em>/<em>z</em>      <sup>2</sup>, &#x2113; &#x2265; <em>c</em>log&#x2009;(<em>n</em>/<em>&#x03B4;</em>), <em>&#x03C4;</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>.</p>     <p>Then with probability at least 1 &#x2212; <em>&#x03B4;</em>, for all <em>d</em> &#x2208; <em>D</em>, <SmallCap>SADDLES</SmallCap> outputs an (&#x025B;, &#x025B;)-approximation of <em>N</em>(<em>d</em>).</p>     <p>The expected number of queries made depends on the model, <em>and is independent of the size of <em>D</em>      </em>.</p>     <p>      <ul class="list-no-style">       <li id="list12" label="&#x2022;">SM: <em>O</em>((<em>n</em>/<em>h</em> + <em>m</em>/<em>z</em>       <sup>2</sup>)(&#x025B;<sup>&#x2212; 2</sup>log&#x2009;(<em>n</em>/<em>&#x03B4;</em>))).<br/></li>       <li id="list13" label="&#x2022;">HDM: <em>O</em>((<em>m</em>/<em>z</em>)(&#x025B;<sup>&#x2212; 4</sup>log&#x2009;<sup>2</sup>(<em>n</em>/<em>&#x03B4;</em>))).<br/></li>      </ul>     </p>    </div>    </p>    <p>Ignoring constant factors and assuming <em>m</em> = <em>O</em>(<em>n</em>), asymptotically increasing <em>h</em> and <em>z</em>-indices lead to an algorithm with sublinear query complexity.</p>    <p>The same algorithmic structure is used for the SM and the HDM. The only difference is the use the algorithm of Corollary&#x00A0; 2.5 to estimate degrees in the HDM, while the degrees are directly available in SM.</p>    <p>    <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186111/images/www2018-120-img1.jpg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>    <em>The core theoretical bound:</em>. The central technical bound deals with the properties of each individual estimate <span class="inline-equation"><span class="tex">$\widetilde{N}(d)[t]$</span>    </span>.</p>    <p>    <div class="theorem" id="enc16">     <Label>Theorem 3.2.</Label>     <p> Suppose <em>r</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>      <em>n</em>/<em>h</em>, <em>q</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>      <em>m</em>/<em>z</em>      <sup>2</sup>, <em>&#x03C4;</em> = <em>c</em>&#x025B;<sup>&#x2212; 2</sup>. Then, for all <em>d</em> &#x2208; <em>D</em>, with probability &#x2265; 5/6, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)[t] \in [(1-\varepsilon /2)N((1+\varepsilon /2)d), (1+\varepsilon /2)N((1-\varepsilon /2)d]$</span>      </span>.</p>    </div>    </p>    <p>The proof of this theorem is the main part of our analysis, which appears in the next section. The error/accuracy bound of Theorem <a class="theorem" id="enc15">3.1</a> can be proved through a straightforward application of &#x201C;boosting through medians&#x201D;. For space constraints, we leave the proof of error bound and query complexity for a longer version of this paper available on arxiv&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>].</p>   </section>   <section id="sec-21">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Analysis of <SmallCap>SADDLES</SmallCap>     </h2>    </div>    </header>    <p>We now prove Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc16">3.2</a>. There are a number of intermediate claims towards that. We will fix <em>d</em> &#x2208; <em>D</em> and a choice of <em>t</em>. Abusing notation, we use <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>    </span> to refer to <span class="inline-equation"><span class="tex">$\widetilde{N}(d)[t]$</span>    </span>. The estimate of Step&#x00A0; 16 can be analyzed with a direct Chernoff bound.</p>    <p>    <div class="claim" id="enc17">     <Label>Claim 4.1.</Label>     <p> The following holds with probability > 9/10. If <SmallCap>SADDLES</SmallCap>(<em>r</em>, <em>q</em>) outputs an estimate in Step&#x00A0; 16 for a given <em>d</em>, then <span class="inline-equation"><span class="tex">$\widetilde{N}(d)\in (1 \pm \varepsilon /10) \widehat{N_\Lambda }(d)$</span>      </span>. If it does not output in Step&#x00A0; 16, then <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) {\lt} (2c/\varepsilon ^2)(n/r)$</span>      </span>.</p>    </div>    </p>    <div class="proof" id="proof1">    <Label>Proof.</Label>    <p> Each <em>X<sub>i</sub>     </em> is an iid Bernoulli random variable, with success probability precisely <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d)/n$</span>     </span>. We split into two cases.</p>    <p>Case 1: <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) \ge (c/10\varepsilon ^2)(n/r)$</span>     </span>. By the Chernoff bound of Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc7">2.1</a>, <span class="inline-equation"><span class="tex">$\Pr [|\sum _{i \le r} X_i - r \widehat{N_\Lambda }(d)/n| \ge (\varepsilon /10)(r \widehat{N_\Lambda }(d)/n)]$</span>     </span>     <span class="inline-equation"><span class="tex">$\le 2\exp (-(\varepsilon ^2/100) (r\widehat{N_\Lambda }(d)/n)$</span>     </span> &#x2264; 1/100.</p>    <p>Case 2: <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) \le (c/10\varepsilon ^2)(n/r)$</span>     </span>. Note that <strong>&#x2009;E</strong>[&#x2211;<sub>      <em>i</em> &#x2264; <em>r</em>     </sub>     <em>X<sub>i</sub>     </em>] &#x2264; <em>c</em>/10&#x025B;<sup>2</sup> &#x2264; (<em>c</em>/&#x025B;<sup>2</sup>)/2<em>e</em>. By the upper tail bound of Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc7">2.1</a>, <span class="inline-equation"><span class="tex">$\Pr [\sum _{i \le r} X_i \ge c/\varepsilon ^2] {\lt} 1/100$</span>     </span>.</p>    <p>Thus, with probability at least 99/100, if an estimate is output in Step&#x00A0; 16, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) {\gt} (c/10\varepsilon ^2)(n/r)$</span>     </span>. By the first case, with probability at least 99/100, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>     </span> is a (1 + &#x025B;/10)-estimate for <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d)$</span>     </span>. A union bound completes the first part.</p>    <p>Furthermore, if <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) \ge (2c/\varepsilon ^2)(n/r)$</span>     </span>, then with probability at least 99/100, <span class="inline-equation"><span class="tex">$\sum _{i \le r} X_i \ge (1-\varepsilon /10) r \widehat{N_\Lambda }(d)/n$</span>     </span> &#x2265; <em>c</em>/&#x025B;<sup>2</sup> = <em>&#x03C4;</em>. A union bound proves (the contrapositive of) the second part. &#x25A1;</p>    </div>    <p>We define weights of <em>ordered</em> edges. The weight only depends on the second member in the pair, but allows for a more convenient analysis. The weight of &#x27E8;<em>v</em>, <em>u</em>&#x27E9; is the random variable <em>Y<sub>i</sub>    </em> of Step&#x00A0; 13.</p>    <p>    <div class="definition" id="enc18">     <Label>Definition 4.2.</Label>     <p> The <em>d</em>-weight of an ordered edge &#x27E8;<em>v</em>, <em>u</em>&#x27E9; for a given <em>&#x039B;</em> (the randomness of <SmallCap>DEG</SmallCap>) is defined as follows. We set wt<sub>       <em>&#x039B;</em>, <em>d</em>      </sub>(&#x27E8;<em>v</em>, <em>u</em>&#x27E9;) to be <span class="inline-equation"><span class="tex">$1/\hat{d_u}$</span>      </span> if <span class="inline-equation"><span class="tex">$\hat{d_u} \ge d$</span>      </span>, and zero otherwise. For vertex <em>v</em>, wt<sub>       <em>&#x039B;</em>, <em>d</em>      </sub>(<em>v</em>) = &#x2211;<sub>       <em>u</em> &#x2208; <em>&#x0393;</em>(<em>v</em>)</sub>wt<sub>       <em>&#x039B;</em>, <em>d</em>      </sub>(&#x27E8;<em>v</em>, <em>u</em>&#x27E9;).</p>    </div>    </p>    <p>The utility of the weight definition is captured by the following claim. The total weight is an approximation of <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>    </span>, and thus, we can analyze how well <SmallCap>SADDLES</SmallCap> approximates the total weight.</p>    <p>    <div class="claim" id="enc19">     <Label>Claim 4.3.</Label>     <p> If <em>&#x039B;</em> is good, <span class="inline-equation"><span class="tex">$\sum _{v \in V} \mathrm{wt}_{\Lambda ,d}(v) \in (1\pm \varepsilon /9) \widehat{N_\Lambda }(d)$</span>      </span>.</p>    </div>    </p>    <div class="proof" id="proof2">    <Label>Proof.</Label>    <p>     <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \sum _{v \in V} \mathrm{wt}_{\Lambda ,d}(v) &#x0026; = &#x0026; \sum _{v \in V} \sum _{u \in \Gamma (v)} {\bf 1}_{\hat{d_u} \ge d}/\hat{d_u} \nonumber \\ &#x0026; = &#x0026; \sum _{u: \hat{d_u} \ge d} \sum _{v \in \Gamma (u)} 1/\hat{d_u} = \sum _{u: \hat{d_u} \ge d} d_u/\hat{d_u} \end{eqnarray} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div>    </p>    <p>Since <em>&#x039B;</em> is good, <span class="inline-equation"><span class="tex">$\forall u, \hat{d_u} \in (1\pm \varepsilon /10) d_u$</span>     </span>, and <span class="inline-equation"><span class="tex">$d_u/\hat{d_u} \in (1 \pm \varepsilon /9)$</span>     </span>. Applying in [eqn:wgt](1), &#x2211;<sub>      <em>v</em> &#x2208; <em>V</em>     </sub>wt<sub>      <em>&#x039B;</em>, <em>d</em>     </sub>(<em>v</em>)<span class="inline-equation"><span class="tex">$\in (1\pm \varepsilon /9) \widehat{N_\Lambda }(d)$</span>     </span>. &#x25A1;</p>    </div>    <p>We come to an important lemma, that shows that the weight of the random subset <em>R</em> (chosen in Step&#x00A0; 3) is well-concentrated. This is proven using a Chernoff bound, but we need to bound the maximum possible weight to get a good bound on <em>r</em> = |<em>R</em>|.</p>    <p>    <div class="lemma" id="enc20">     <Label>Lemma 4.4.</Label>     <p> Fix any good <em>&#x039B;</em> and <em>d</em>. Suppose <em>r</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>      <em>n</em>/<em>d</em>. With probability at least 9/10, <span class="inline-equation"><span class="tex">$\sum _{v \in R}\mathrm{wt}_{\Lambda ,d}(v) \in (1\pm \varepsilon /8) (r/n)\widehat{N_\Lambda }(d)$</span>      </span>.</p>    </div>    </p>    <div class="proof" id="proof3">    <Label>Proof.</Label>    <p> Let wt(<em>R</em>) denote &#x2211;<sub>      <em>v</em> &#x2208; <em>R</em>     </sub>wt<sub>      <em>&#x039B;</em>, <em>d</em>     </sub>(<em>v</em>). By linearity of expectation, <strong>&#x2009;E</strong>[wt(<em>R</em>)] = (<em>r</em>/<em>n</em>) &#x00B7; &#x2211;<sub>      <em>v</em> &#x2208; <em>V</em>     </sub>wt<sub>      <em>&#x039B;</em>, <em>d</em>     </sub>(<em>v</em>)<span class="inline-equation"><span class="tex">$ \ge (r/2n) \widehat{N_\Lambda }(d)$</span>     </span>. To apply the Chernoff bound, we need to bound the maximum weight of a vertex. For good <em>&#x039B;</em>, the weight wt<sub>      <em>&#x039B;</em>, <em>d</em>     </sub> of any ordered pair is at most 1/(1 &#x2212; &#x025B;/10)<em>d</em> &#x2264; 2/<em>d</em>. The number of neighbors of <em>v</em> such that <span class="inline-equation"><span class="tex">$\hat{d}_u \ge d$</span>     </span> is at most <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d)$</span>     </span>. Thus, <span class="inline-equation"><span class="tex">$\mathrm{wt}_{\Lambda ,d}(v) \le 2 \widehat{N_\Lambda }(d)/d$</span>     </span>.</p>    <p>By the Chernoff bound of Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc7">2.1</a> and setting <em>r</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>     <em>n</em>/<em>d</em>, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; Pr\left[ \left|\mathrm{wt}(R) - {\bf E}[\mathrm{wt}(R)] \right|{\gt} (\varepsilon /20){\bf E}[\mathrm{wt}(R)] \right] \\ &#x0026; {\lt} 2\exp \left(- \frac{\varepsilon ^2 \cdot (c \varepsilon ^{-2}n/d) \cdot (\widehat{N_\Lambda }(d)/2n) }{ 3 \cdot 20^2 \cdot 2\widehat{N_\Lambda }(d)/d}\right) \le 1/10\end{align*} </span>       <br/>      </div>     </div> With probability at least 9/10, wt(<em>R</em>) &#x2208; (1 &#x00B1; &#x025B;/20)<strong>&#x2009;E</strong>[wt(<em>R</em>)]. By the arguments given above, <span class="inline-equation"><span class="tex">${\bf E}[\mathrm{wt}(R)] \in (1 \pm \varepsilon /9) (r/n) \widehat{N_\Lambda }(d)$</span>     </span>. We combine to complete the proof. &#x25A1;</p>    </div>    <p>Now, we determine the number of edge samples required to estimate the weight wt<sub>     <em>&#x039B;</em>, <em>d</em>    </sub>(<em>R</em>).</p>    <p>    <div class="lemma" id="enc21">     <Label>Lemma 4.5.</Label>     <p> Let <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>      </span> be as defined in Step&#x00A0;17 of <SmallCap>SADDLES</SmallCap>. Assume <em>&#x039B;</em> is good, <em>r</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>      <em>n</em>/<em>d</em>, and <span class="inline-equation"><span class="tex">$q \ge c \varepsilon ^{-2}m/(d\widehat{N_\Lambda }(d))$</span>      </span>. Then, with probability > 7/8, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)\in (1\pm \varepsilon /4)\widehat{N_\Lambda }(d)$</span>      </span>.</p>    </div>    </p>    <div class="proof" id="proof4">    <Label>Proof.</Label>    <p> We define the random set <em>R</em> selected in Step&#x00A0; 3 to be <em>sound</em> if the following hold. (1) wt(<em>R</em>) = &#x2211;<sub>      <em>v</em> &#x2208; <em>R</em>     </sub>wt<sub>      <em>&#x039B;</em>, <em>d</em>     </sub>(<em>v</em>)<span class="inline-equation"><span class="tex">$\in (1 \pm \varepsilon /8) (r/n) \widehat{N_\Lambda }(d)$</span>     </span> and (2) &#x2211;<sub>      <em>v</em> &#x2208; <em>R</em>     </sub>     <em>d<sub>v</sub>     </em> &#x2264; 100<em>r</em>(2<em>m</em>/<em>n</em>). By Lemma <a class="lemma" href="http://delivery.acm.org/10.1145/3190000/3186111/enc20">4.4</a>, the first holds with probability > 9/10. Observe that <strong>&#x2009;E</strong>[&#x2211;<sub>      <em>v</em> &#x2208; <em>R</em>     </sub>     <em>d<sub>v</sub>     </em>] = <em>r</em>(2<em>m</em>/<em>n</em>), since 2<em>m</em>/<em>n</em> is the average degree. By the Markov bound, the second holds with probability > 99/100. By the union bound, <em>R</em> is sound with probability at least 1 &#x2212; (1/10 + 1/100) > 8/9.</p>    <p>Fix a sound <em>R</em>. Recall <em>Y<sub>i</sub>     </em> from Step&#x00A0; 13. The expectation of <em>Y<sub>i</sub>     </em>|<em>R</em> is <span class="inline-equation"><span class="tex">$\sum _{v \in R} \Pr [v\ \textrm{isselected}$</span></span>] &#x00B7; <span class="inline-equation"><span class="tex">$\sum _{u \in \Gamma (v)} \Pr [ u\ \textrm{isselected}]$</span></span>wt<sub>      <em>&#x039B;</em>, <em>d</em>     </sub>(&#x27E8;<em>v</em>, <em>u</em>&#x27E9;). We plug in the probability values, and observe that for good <em>&#x039B;</em>, for all <em>v</em>, <span class="inline-equation"><span class="tex">$\hat{d}_v/d_v \in (1\pm \varepsilon /10)$</span>     </span>. <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} {\bf E}[Y_i | R] &#x0026; = &#x0026; \sum _{v \in R} (\hat{d}_v/\hat{d}_R) \sum _{u \in \Gamma (v)} (1/d_v) \mathrm{wt}_{\Lambda ,d}(\langle v,u\rangle) \nonumber \\ &#x0026; = &#x0026; (1/\hat{d}_R) \sum _{v \in R} (\hat{d}_v/d_v)\sum _{u \in \Gamma (v)} \mathrm{wt}_{\Lambda , d}(\langle v,u\rangle) \nonumber \\ &#x0026; \in &#x0026; (1 \pm \varepsilon /10) (1/\hat{d}_R) \sum _{v \in R} \sum _{u \in \Gamma (v)} \mathrm{wt}_{\Lambda , d}(\langle v,u\rangle) \nonumber \\ &#x0026; \in &#x0026; (1 \pm \varepsilon /10) (\mathrm{wt}(R)/\hat{d}_R) \end{eqnarray} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> Note that <span class="inline-equation"><span class="tex">$\widetilde{N}(d)= (n/r) (\hat{d}_R/q) \sum _{i \le q} Y_i$</span>     </span> and <span class="inline-equation"><span class="tex">$(n/r) (\hat{d}_R/q) {\bf E}[\sum _{i \le q} Y_i|R]$</span>     </span> &#x2208; (1 &#x00B1; &#x025B;/10)(<em>n</em>/<em>r</em>)wt(<em>R</em>). Since <em>R</em> is sound, the latter is in <span class="inline-equation"><span class="tex">$(1\pm \varepsilon /4) \widehat{N_\Lambda }(d)$</span>     </span>. Also, note that <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\bf E}[Y_i | R] = {\bf E}[{Y_1} | R] \ge \frac{q\mathrm{wt}(R)}{2\hat{d}_R} \ge \frac{(r/n) \widehat{N_\Lambda }(d)}{4(100r(2m/n)} = \frac{\widehat{N_\Lambda }(d)}{800m} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> By linearity of expectation, <strong>&#x2009;E</strong>[&#x2211;<sub>      <em>i</em> &#x2264; <em>q</em>     </sub>     <em>Y<sub>i</sub>     </em>|<em>R</em>] = <em>q</em>     <strong>&#x2009;E</strong>[<em>Y</em>     <sub>1</sub>|<em>R</em>]. Observe that <em>Y<sub>i</sub>     </em> &#x2264; 1/<em>d</em>. We can apply the Chernoff bound of Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc7">2.1</a> to the iid random variables (<em>Y<sub>i</sub>     </em>|<em>R</em>). <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} &#x0026; &#x0026; \Pr [|\sum _i Y_i - {\bf E}[\sum _i Y_i]| {\gt} (\varepsilon /100){\bf E}[\sum _i Y_i] | R] \nonumber \\ &#x0026; \le &#x0026; 2\exp \Big (-\frac{\varepsilon ^2}{3\cdot 100^2} \cdot d \cdot q {\bf E}[{Y_1} | R] \Big) \end{eqnarray} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> We use (3) to bound the (positive) term in the exponent is at least <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \frac{\varepsilon ^2}{3\cdot 100^2} \cdot \frac{c\varepsilon ^{-2}m}{\widehat{N_\Lambda }(d)} \cdot \frac{\widehat{N_\Lambda }(d)}{800m} \ge 10. \] </span>       <br/>      </div>     </div> Thus, if <em>R</em> is sound, the following bound holds with probability at least 0.99. We also apply (2). <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray*} \widehat{N_\Lambda }(d) &#x0026; = &#x0026; (n/r)(\hat{d}_R/q) \sum _{i=1}^q Y_i \\ &#x0026;\in &#x0026; (1\pm \varepsilon /100) (n/r)(\hat{d}_R/q) q {\bf E}[Y_i | R] \\ &#x0026; \in &#x0026; (1\pm \varepsilon /100)(1\pm \varepsilon /10) (n/r) \mathrm{wt}(R) \in (1\pm \varepsilon /4) \widetilde{N}(d)\end{eqnarray*} </span>       <br/>      </div>     </div> The probability that <em>R</em> is sound is at least 8/9. A union bound completes the proof. &#x25A1;</p>    </div>    <p>The bounds on <em>r</em> and <em>q</em> in Lemma <a class="lemma" href="http://delivery.acm.org/10.1145/3190000/3186111/enc21">4.5</a> depend on the degree <em>d</em>. We now bring in the <em>h</em> and <em>z</em>-indices to derive bounds that hold for all <em>d</em>. We also remove the conditioning over a good <em>&#x039B;</em>.</p>    <div class="proof" id="proof5">    <Label>Proof.</Label>    <p> (of Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc7">3.2</a>) We will first assume that <em>&#x039B;</em> is good. By Claim&#x00A0; 2.8, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) \in [N((1+\varepsilon /9)d, N((1-\varepsilon /9)d)]$</span>     </span>.</p>    <p>Suppose <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) = 0$</span>     </span>, so there are no vertices with <span class="inline-equation"><span class="tex">$\hat{d}_v \ge d$</span>     </span>. By the bound above, <em>N</em>((1 + &#x025B;/9)<em>d</em>) = 0, implying that <em>N</em>((1 + &#x025B;/2)<em>d</em>) = 0. Furthermore <span class="inline-equation"><span class="tex">$\widetilde{N}(d)= 0$</span>     </span>, since the random variables <em>X<sub>i</sub>     </em> and <em>Y<sub>i</sub>     </em> in <SmallCap>SADDLES</SmallCap> can never be non-zero. Thus, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)= N((1+\varepsilon /2)d)$</span>     </span>, completing the proof.</p>    <p>We now assume that <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) {\gt} 0$</span>     </span>. We split into two cases, depending on whether Step&#x00A0; 16 outputs or not. By Claim&#x00A0; 4.1, with probability > 9/10, if Step&#x00A0; 16 outputs, then <span class="inline-equation"><span class="tex">$\widetilde{N}(d)\in (1\pm \varepsilon /9)\widehat{N_\Lambda }(d)$</span>     </span>. By combining these bounds, the desired bound on <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>     </span> holds with probability > 9/10, conditioned on a good <em>&#x039B;</em>.</p>    <p>Henceforth, we focus on the case that Step&#x00A0; 16 does not output. By Claim&#x00A0; 4.1, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) {\lt} 2c\varepsilon ^{-2}(n/r)$</span>     </span>. By the choice of <em>r</em> and Claim&#x00A0; 2.8, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }((1+\varepsilon /9)d) {\lt} h$</span>     </span>. By the characterization of <em>h</em> of Lemma <a class="lemma" href="http://delivery.acm.org/10.1145/3190000/3186111/enc8">2.2</a>, <span class="inline-equation"><span class="tex">$z^2 \le \max (\widehat{N_\Lambda }((1+\varepsilon /9)d), (1+\varepsilon /9)d) = (1+\varepsilon /9)d$</span>     </span>. This implies that <em>r</em> &#x2265; <em>c</em>&#x025B;<sup>&#x2212; 2</sup>     <em>n</em>/<em>d</em>. By the definition of <em>z</em>, <em>z</em>     <sup>2</sup> &#x2264; <em>N</em>(min&#x2009;(<em>d<sub>max</sub>     </em>, (1 + &#x025B;/9)<em>d</em>)) &#x00B7; min&#x2009;(<em>d<sub>max</sub>     </em>, (1 + &#x025B;/9)<em>d</em>). By the Claim&#x00A0; 2.8 bound in the first paragraph, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) \ge N((1+\varepsilon /9)d)$</span>     </span>. Since <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) {\gt} 0$</span>     </span>, <span class="inline-equation"><span class="tex">$\widehat{N_\Lambda }(d) \ge \widehat{N_\Lambda }(d_{max})$</span>     </span>. Thus, <span class="inline-equation"><span class="tex">$z^2 \le \widehat{N_\Lambda }(d)\cdot (1+\varepsilon /9)d$</span>     </span>. and hence, <span class="inline-equation"><span class="tex">$m \le c\varepsilon ^{-2}m/(d\widehat{N_\Lambda }(d))$</span>     </span>. The parameters satisfy the conditions in Lemma <a class="lemma" href="http://delivery.acm.org/10.1145/3190000/3186111/enc21">4.5</a>. With probability > 7/8, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)\in (1\pm \varepsilon /4)\widehat{N_\Lambda }(d)$</span>     </span>, and by Claim&#x00A0; 2.8, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>     </span> has the desired accuracy.</p>    <p>All in all, assuming <em>&#x039B;</em> is good, with probability at least 7/8, <span class="inline-equation"><span class="tex">$\widetilde{N}(d)$</span>     </span> has the desired accuracy. The conditioning on a good <em>&#x039B;</em> is removed by Claim&#x00A0; 2.7 to complete the proof. &#x25A1;</p>    </div>   </section>   <section id="sec-22">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experimental Results</h2>    </div>    </header>    <p>We implemented our algorithm in C++ and performed our experiments on a MacBook Pro laptop with 2.7 GHz Intel Core i5 with 8 GB RAM. We performed our experiments on a collection of graphs from SNAP&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], including social networks, web networks, and infrastructure networks. The graphs typically have millions of edges, with the largest having more than 100M edges. Basic properties of these graphs are presented in Table&#x00A0;<a class="tbl" href="#tab1">1</a>. We ignore direction and treat all edges as undirected edges.</p>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Implementation Details</h3>     </div>    </header>    <p>For the HDM, we explicitly describe the procedure <SmallCap>DEG</SmallCap>(<em>v</em>), which estimates the degree of a given vertex (<em>v</em>).</p>    <p>     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186111/images/www2018-120-img2.jpg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>In the algorithm <SmallCap>DEG</SmallCap>, a &#x201C;pair-wise collision&#x201D; refers to a pair of neighbor samples that yield the same vertex. The expected number of pair-wise collisions is <span class="inline-equation"><span class="tex">${|S|\choose 2}/d_v$</span>     </span>. We simply reverse engineer that inequality to get the estimate <span class="inline-equation"><span class="tex">$\hat{d}_v$</span>     </span>. Ron and Tsur essentially prove that this estimate has low variance&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>].</p>    <p>     <strong>Setting the parameter values.</strong> The boosting parameter &#x2113; is simply set to 1. (In some sense, we only introduced the median boosting for the theoretical union bound. In practice, convergence is much more rapid that predicted by the Chernoff bound.)</p>    <p>The threshold <em>&#x03C4;</em> is set to 100. The parameters <em>r</em> and <em>q</em> are chosen to be typically around 0.005<em>n</em>. These are not &#x201C;sublinear&#x201D; per se, but are an order of magnitude smaller than the queries made in existing graph sampling results (more discussion in next section).</p>    <p>We set <em>D</em> = {&#x230A;1.1<sup>      <em>i</em>     </sup>&#x230B;}, since that gives a sufficiently fine-grained approximation at all scales of the degree distribution.</p>    <p>Code for all experiments is available <a class="link-inline force-break" href="http://here">here</a><a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">       <strong>Graph properties</strong>: #vertices (n), #edges (m), maximum degree, <em>h</em>-index and <em>z</em>-index. The last column indicates the median number of samples over 100 runs (as a percentage of <em>m</em>) required by <SmallCap>SADDLES</SmallCap> under HDM, with <em>r</em> + <em>q</em> = 0.01<em>n</em>.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;"/>       <th style="text-align:center;"/>       <th style="text-align:center;"/>       <th style="text-align:center;">        <strong>max.</strong>       </th>       <th style="text-align:center;">        <strong>avg.</strong>       </th>       <th style="text-align:center;"/>       <th style="text-align:center;"/>       <th style="text-align:center;">        <strong>Perc. edge</strong>       </th>       </tr>       <tr>       <th style="text-align:left;">        <strong>graph</strong>       </th>       <th style="text-align:left;">        <strong>#vertices</strong>       </th>       <th style="text-align:left;">        <strong>#edges</strong>       </th>       <th style="text-align:right;">        <strong>degree</strong>       </th>       <th style="text-align:right;">        <strong>degree</strong>       </th>       <th style="text-align:right;">        <strong>H-index</strong>       </th>       <th style="text-align:right;">        <strong>Z-index</strong>       </th>       <th style="text-align:center;">        <strong>samples for HDM</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">loc-gowalla</td>       <td style="text-align:left;">1.97E+05</td>       <td style="text-align:left;">9.50E+05</td>       <td style="text-align:right;">14730</td>       <td style="text-align:right;">4.8</td>       <td style="text-align:right;">275</td>       <td style="text-align:right;">101</td>       <td style="text-align:center;">7.0</td>       </tr>       <tr>       <td style="text-align:left;">web-Stanford</td>       <td style="text-align:left;">2.82E+05</td>       <td style="text-align:left;">1.99E+06</td>       <td style="text-align:right;">38625</td>       <td style="text-align:right;">7.0</td>       <td style="text-align:right;">427</td>       <td style="text-align:right;">148</td>       <td style="text-align:center;">6.4</td>       </tr>       <tr>       <td style="text-align:left;">com-youtube</td>       <td style="text-align:left;">1.13E+06</td>       <td style="text-align:left;">2.99E+06</td>       <td style="text-align:right;">28754</td>       <td style="text-align:right;">2.6</td>       <td style="text-align:right;">547</td>       <td style="text-align:right;">121</td>       <td style="text-align:center;">11.7</td>       </tr>       <tr>       <td style="text-align:left;">web-Google</td>       <td style="text-align:left;">8.76E+05</td>       <td style="text-align:left;">4.32E+06</td>       <td style="text-align:right;">6332</td>       <td style="text-align:right;">4.9</td>       <td style="text-align:right;">419</td>       <td style="text-align:right;">73</td>       <td style="text-align:center;">6.2</td>       </tr>       <tr>       <td style="text-align:left;">web-BerkStan</td>       <td style="text-align:left;">6.85E+05</td>       <td style="text-align:left;">6.65E+06</td>       <td style="text-align:right;">84230</td>       <td style="text-align:right;">9.7</td>       <td style="text-align:right;">707</td>       <td style="text-align:right;">220</td>       <td style="text-align:center;">5.5</td>       </tr>       <tr>       <td style="text-align:left;">wiki-Talk</td>       <td style="text-align:left;">2.39E+06</td>       <td style="text-align:left;">9.32E+06</td>       <td style="text-align:right;">100029</td>       <td style="text-align:right;">3.9</td>       <td style="text-align:right;">1055</td>       <td style="text-align:right;">180</td>       <td style="text-align:center;">8.5</td>       </tr>       <tr>       <td style="text-align:left;">as-skitter</td>       <td style="text-align:left;">1.70E+06</td>       <td style="text-align:left;">1.11E+07</td>       <td style="text-align:right;">35455</td>       <td style="text-align:right;">6.5</td>       <td style="text-align:right;">982</td>       <td style="text-align:right;">184</td>       <td style="text-align:center;">6.7</td>       </tr>       <tr>       <td style="text-align:left;">cit-Patents</td>       <td style="text-align:left;">3.77E+06</td>       <td style="text-align:left;">1.65E+07</td>       <td style="text-align:right;">793</td>       <td style="text-align:right;">4.3</td>       <td style="text-align:right;">237</td>       <td style="text-align:right;">28</td>       <td style="text-align:center;">5.6</td>       </tr>       <tr>       <td style="text-align:left;">com-lj</td>       <td style="text-align:left;">4.00E+06</td>       <td style="text-align:left;">3.47E+07</td>       <td style="text-align:right;">14815</td>       <td style="text-align:right;">8.6</td>       <td style="text-align:right;">810</td>       <td style="text-align:right;">114</td>       <td style="text-align:center;">4.7</td>       </tr>       <tr>       <td style="text-align:left;">soc-LiveJournal1</td>       <td style="text-align:left;">4.85E+06</td>       <td style="text-align:left;">8.57E+07</td>       <td style="text-align:right;">20333</td>       <td style="text-align:right;">17.7</td>       <td style="text-align:right;">989</td>       <td style="text-align:right;">124</td>       <td style="text-align:center;">2.4</td>       </tr>       <tr>       <td style="text-align:left;">com-orkut</td>       <td style="text-align:left;">3.07E+06</td>       <td style="text-align:left;">1.17E+08</td>       <td style="text-align:right;">33313</td>       <td style="text-align:right;">38.1</td>       <td style="text-align:right;">1638</td>       <td style="text-align:right;">172</td>       <td style="text-align:center;">2.0</td>       </tr>      </tbody>      <tfoot>       <tr>       <td>max width=</td>       <td/>       <td/>       <td/>       <td/>       <td/>       <td/>       <td/>       </tr>      </tfoot>     </table>    </div>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Evaluation of <SmallCap>SADDLES</SmallCap>      </h3>     </div>    </header>    <p>     <strong>Accuracy over all graphs:</strong> We run <SmallCap>SADDLES</SmallCap> with the parameters discussed above for a variety of graphs. Because of space considerations, we do not show results for all graphs in this version. (We discovered the results to be consistent among all our experiments.) Fig. <a class="fig" href="#fig1">1</a> show the results for the SM for some graphs in Tab.<a class="tbl" href="#tab1">1</a>. For all these runs, we set <em>r</em> + <em>q</em> to be 1% of the number of vertices in the graph. Note that the sample size of <SmallCap>SADDLES</SmallCap> in the SM is exactly <em>r</em> + <em>q</em>. For the HDM, we show results in Fig. <a class="fig" href="#fig2">2</a>. Again, we set <em>r</em> + <em>q</em> to be 1%, though the number of edges sampled (due to invocations of <SmallCap>DEG</SmallCap>(<em>v</em>)) varies quite a bit. The required number of samples are provided in Tab. <a class="tbl" href="#tab1">1</a>. Note that the number of edges sampled is well within 10% of the total, except for the <tt>com-youtube</tt> graph.</p>    <p>Visually, we can see that the estimates are accurate for all degrees, in all graphs, for both models. This is despite there being sufficient irregular behavior in <em>N</em>(<em>d</em>). Note that the shape of the various ccdhs are different and none of them form an obvious straight line. Nonetheless, <SmallCap>SADDLES</SmallCap> captures the distribution almost perfectly in all cases by observing 1% of the vertices. <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186111/images/www2018-120-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">The result of runs of <SmallCap>SADDLES</SmallCap> on a variety of graphs, for the HDM. We set <em>r</em> + <em>q</em> to be 1% of the number of vertices, for all graphs. The actual number of edges sampled varies, and is given in Tab. <a class="tbl" href="#tab1">1</a>.</span>      </div>     </figure>    </p>    <p>     <strong>Convergence:</strong> To demonstrate convergence, we fix the graph <tt>com-orkut</tt>, and run <SmallCap>SADDLES</SmallCap> only for the degrees 10, 100, and 1000. For each choice of degree, we vary the total number of samples <em>r</em> + <em>q</em>. (We set <em>r</em> = <em>q</em> in all runs.) Finally, for each setting of <em>r</em> + <em>q</em>, we perform 100 independent runs of <SmallCap>SADDLES</SmallCap>.</p>    <p>For each such run, we compute an error parameter <em>&#x03B1;</em>. Suppose the output of a run is <em>M</em>, for degree <em>d</em>. The value of <em>&#x03B1;</em> is the smallest value of &#x03F5;, such that <em>M</em> &#x2208; [(1 &#x2212; &#x03F5;)<em>N</em>((1 + &#x03F5;)<em>d</em>), (1 + &#x03F5;)<em>N</em>((1 &#x2212; &#x03F5;)<em>d</em>)]. (It is the smallest &#x03F5; such that <em>M</em> is an (&#x03F5;, &#x03F5;)-approximation of <em>N</em>(<em>d</em>).)</p>    <p>Fig. <a class="fig" href="#fig3">3</a> shows the spread of <em>&#x03B1;</em>, for the 100 runs, for each choice of <em>r</em> + <em>q</em>. Observe how the spread decreases as <em>r</em> + <em>q</em> goes to 10%. In all cases, the values of <em>&#x03B1;</em> decay to less than 0.05. We notice that convergence is much faster for <em>d</em> = 10. This is because <em>N</em>(10) is quite large, and <SmallCap>SADDLES</SmallCap> is using vertex sampling to estimate the value. <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186111/images/www2018-120-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Convergence of <SmallCap>SADDLES</SmallCap>: We plot the values of the error parameter <em>&#x03B1;</em> (as defined in [sec:eval]&#x00A7; 5.2) for 100 runs at increasing values of <em>r</em> + <em>q</em>. We have a different plot for <em>d</em> = 10, 100, 1000, 10000 to show the convergence at varying portions of the ccdh.</span>      </div>     </figure>    </p>    <p>     <strong>Large value of <em>h</em> and <em>z</em>-index on real graphs:</strong> The <em>h</em> and <em>z</em>-index of all graphs is given in Tab. <a class="tbl" href="#tab1">1</a>. Observe how they are typically in the hundreds. Note that the average degree is typically an order of magnitude smaller than these indices. Thus, a sample size of <em>n</em>/<em>h</em> + <em>m</em>/<em>z</em>     <sup>2</sup> (as given by Theorem <a class="theorem" href="http://delivery.acm.org/10.1145/3190000/3186111/enc15">3.1</a>, ignoring constants) is significantly sublinear. This is consistent with our choice of <em>r</em> + <em>q</em> = <em>n</em>/100 leading to accurate estimates for the ccdh.</p>    </section>    <section id="sec-25">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Comparison with previous work</h3>     </div>    </header>    <p>There are several graph sampling algorithms that have been discussed in &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]. In all of these methods we collect the vertices and scale their counts appropriately to get the estimated ccdh. We describe these methods below in more detail, and discuss our implementation of the method.</p>    <ul class="list-no-style">     <li id="list14" label="&#x2022;">Vertex Sampling (VS, also called egocentric sampling) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>]: In this algorithm, we sample vertices u.a.r. and scale the ccdh obtained appropriately, to get an estimate for the ccdh of the entire graph.<br/></li>     <li id="list15" label="&#x2022;">Edge Sampling (ES) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>]: This algorithm samples edges u.a.r. and includes one or both end points in the sampled network. Note that this does <em>not</em> fall into the SM. In our implementation we pick a random end point.<br/></li>     <li id="list16" label="&#x2022;">Random walk with jump (RWJ) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>]: We start a random walk at a vertex selected u.a.r. and collect all vertices encountered on the path in our sampled network. At any point, with a constant probability (0.15, based on previous results) we jump to another u.a.r. vertex.<br/></li>     <li id="list17" label="&#x2022;">One Wave Snowball (OWS) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>]: Snowball sampling starts with some vertices selected u.a.r. and crawls the network until a network of the desired size is sampled. In our implementation, we usually stop at the first level since that accumulates enough vertices.<br/></li>     <li id="list18" label="&#x2022;">Forest fire (FF)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>]: This method generates random sub-crawls of the network. A vertex is picked u.a.r. and randomly selects a subset of its neighbors (according to a geometric distribution). The process is repeated from every selected vertex until it ends. It is then repeated from another u.a.r. vertex.<br/></li>    </ul>    <p>We run all these algorithms on the <tt>amazon0601</tt>, <tt>web-Google</tt>, <tt>cit-Patents</tt>, and <tt>com-orkut</tt> networks. To make fair comparisons, we run each method until it selects 1% of the vertices. The comparisons are shown in Fig. <a class="fig" href="#fig1">1</a>. Observe how none of the methods come close to accurately measuring the ccdh. (This is consistent with previous work, where typically 10-20% of the vertices are sampled for results.) Naive vertex sampling is accurate at the head of the distribution, but completely misses the tail. Except for vertex sampling, all other algorithms are biased towards the tail. Crawls find high degree vertices with disproportionately higher probability, and overestimate the tail.</p>    <p>Note that our implementations of FF, OWS, RWJ assume access to u.a.r. vertices. Variants of these algorithms can be used in situations where we only have access to seed vertices, however, one would typically have to sample many more edges to deal with larger correlation among the vertices obtained through the random walks. Despite this extra capability to sample u.a.r. vertices in our implementation of these algorithms, they show significant errors, particularly in the tail of the distribution.</p>    <p>     <strong>Inverse method of Zhang et al&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0046">46</a>]:</strong> An important result of estimating degree distributions is that of Zhang et al&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>], that explicitly points out the bias problems in various sampling methods. They propose a bias correction method by solving a constrained, penalized weighted least-squares problem on the sampled degree distribution. We apply this method for the sampling methods demonstrated in their paper, namely VS, OWS, and IN (sample vertices u.a.r. and only retain edges between sampled vertices). We show results in Fig. <a class="fig" href="#fig1">1</a>, again with a sample size of 1% of the vertices. Observe that no method gets even close to estimating the ccdh accurately, even after debiasing. Fundamentally, these methods require significantly more samples to generate accurate estimates.</p>    <p>The running time and memory requirements of this method grow superlinearly with the maximum degree in the graph. The largest graph processed by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>] has a few hundred thousand edges, which is on the smaller side of graphs in Tab. <a class="tbl" href="#tab1">1</a>. <SmallCap>SADDLES</SmallCap> processes a graph with more than 100M edges in less than a minute, while our attempts to run the&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>] algorithm on this graph did not terminate in hours.</p>    </section>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Acknowledgements</h2>    </div>    </header>    <p>Ali Pinar&#x0027;s work is supported by the Laboratory Directed Research and Development program at Sandia National Laboratories. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy&#x0027;s National Nuclear Security Administration under contract DE-NA-0003525.</p>    <p>Both Shweta Jain and C. Seshadhri are grateful to the support of the Sandia National Laboratories LDRD program for funding this research. C. Seshadhri also acknowledges the support of NSF TRIPODS grant, CCF-1740850.</p>    <p>This research was partially supported by the Israel Science Foundation grant No.&#x00A0;671/13 and by a grant from the Blavatnik fund. Talya Eden is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship.</p>    <p>Both Talya Eden and C. Seshadhri are grateful to the support of the Simons Institute, where this work was initiated during the Algorithms and Uncertainty Semester.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">D. Achlioptas, A. Clauset, D. Kempe, and C. Moore. 2009. On the bias of traceroute sampling: Or, power-law degree distributions in regular graphs. <em>      <em>J. ACM</em>     </em>56, 4 (2009).</li>    <li id="BibPLXBIB0002" label="[2]">N.K. Ahmed, J. Neville, and R. Kompella. 2010. Reconsidering the Foundations of Network Sampling. In <em>      <em>WIN 10.</em>     </em></li>    <li id="BibPLXBIB0003" label="[3]">N. Ahmed, J. Neville, and R. Kompella. 2012. Space-Efficient Sampling from Social Activity Streams. In <em>      <em>SIGKDD BigMine.</em></em> 1&#x2013;8.</li>    <li id="BibPLXBIB0004" label="[4]">Nesreen&#x00A0;K Ahmed, Nick Duffield, Jennifer Neville, and Ramana Kompella. 2014. Graph sample and hold: A framework for big-graph analytics. In <em>      <em>SIGKDD.</em></em> ACM, 1446&#x2013;1455.</li>    <li id="BibPLXBIB0005" label="[5]">Nesreen&#x00A0;K Ahmed, Jennifer Neville, and Ramana Kompella. 2014. Network sampling: From static to streaming graphs. <em>      <em>TKDD</em>     </em>8, 2 (2014), 7.</li>    <li id="BibPLXBIB0006" label="[6]">Sinan&#x00A0;G. Aksoy, Tamara&#x00A0;G. Kolda, and Ali Pinar. 2017. Measuring and modeling bipartite graphs with community structure. <em>      <em>Journal of Complex Networks</em>     </em>(2017). to appear.</li>    <li id="BibPLXBIB0007" label="[7]">Albert-L&#x00E1;szl&#x00F3; Barab&#x00E1;si and R&#x00E9;ka Albert. 1999. Emergence of Scaling in Random Networks. <em>      <em>Science</em>     </em>286 (Oct. 1999), 509&#x2013;512.</li>    <li id="BibPLXBIB0008" label="[8]">A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Wiener. 2000. Graph structure in the web. <em>      <em>Computer Networks</em>     </em>33(2000), 309&#x2013;320.</li>    <li id="BibPLXBIB0009" label="[9]">Deepayan Chakrabarti and Christos Faloutsos. 2006. Graph Mining: Laws, Generators, and Algorithms. <em>      <em>Comput. Surveys</em>     </em>38, 1 (2006). <a class="link-inline force-break" href="https://doi.org/10.1145/1132952.1132954"      target="_blank">https://doi.org/10.1145/1132952.1132954</a></li>    <li id="BibPLXBIB0010" label="[10]">F. Chierichetti, A. Dasgupta, R. Kumar, S. Lattanzi, and T. Sarlos. 2016. On Sampling Nodes in a Network. In <em>      <em>Conference on the World Wide Web (WWW).</em></em></li>    <li id="BibPLXBIB0011" label="[11]">A. Clauset and C. Moore. 2005. Accuracy and scaling phenomena in internet mapping. <em>      <em>Phys. Rev. Lett.</em>     </em>94(2005), 018701.</li>    <li id="BibPLXBIB0012" label="[12]">A. Clauset, C.&#x00A0;R. Shalizi, and M.&#x00A0;E.&#x00A0;J. Newman. 2009. Power-Law Distributions in Empirical Data. <em>      <em>SIAM Rev.</em>     </em>51, 4 (2009), 661&#x2013;703. <a class="link-inline force-break" href="https://doi.org/10.1137/070710111"      target="_blank">https://doi.org/10.1137/070710111</a></li>    <li id="BibPLXBIB0013" label="[13]">R. Cohen, K. Erez, D. ben Avraham, and S. Havlin. 2000. Resilience of the Internet to Random Breakdowns. <em>      <em>Phys. Rev. Lett.</em>     </em>85, 4626&#x2013;8 (2000).</li>    <li id="BibPLXBIB0014" label="[14]">A. Dasgupta, R. Kumar, and T. Sarlos. 2014. On estimating the average degree. In <em>      <em>Conference on the World Wide Web (WWW).</em></em> 795&#x2013;806.</li>    <li id="BibPLXBIB0015" label="[15]">D. Dubhashi and A. Panconesi. 2012. <em>      <em>Concentration of Measure for the Analysis of Randomised Algorithms.</em></em> Cambridge University Press.</li>    <li id="BibPLXBIB0016" label="[16]">N. Durak, T.G. Kolda, A. Pinar, and C. Seshadhri. 2013. A scalable null model for directed graphs matching all degree distributions: In, out, and reciprocal. In <em>      <em>Network Science Workshop (NSW), 2013 IEEE 2nd.</em></em> 23&#x2013;30. <a class="link-inline force-break" href="https://doi.org/10.1109/NSW.2013.6609190"      target="_blank">https://doi.org/10.1109/NSW.2013.6609190</a></li>    <li id="BibPLXBIB0017" label="[17]">Peter Ebbes, Zan Huang, Arvind Rangaswamy, Hari&#x00A0;P Thadakamalla, and ORGB Unit. 2008. Sampling large-scale social networks: Insights from simulated networks. In <em>      <em>18th Annual Workshop on Information Technologies and Systems, Paris, France.</em></em></li>    <li id="BibPLXBIB0018" label="[18]">Talya Eden, Shweta Jain, Ali Pinar, Dana Ron, and C. Seshadhri. 2017. Provable and practical approximations for the degree distribution using sublinear graph samples. <em>      <em>CoRR</em>     </em>abs/1710.08607(2017). arxiv:1710.08607 <a href="http://arxiv.org/abs/1710.08607" target="_blank">http://arxiv.org/abs/1710.08607</a></li>    <li id="BibPLXBIB0019" label="[19]">T. Eden, A. Levi, D. Ron, and C. Seshadhri. 2015. Approximately Counting Triangles in Sublinear Time. In <em>      <em>Foundations of Computer Science (FOCS)</em>     </em>, GRS11 (Ed.). 614&#x2013;633.</li>    <li id="BibPLXBIB0020" label="[20]">T. Eden, D. Ron, and C. Seshadhri. 2017. Sublinear Time Estimation of Degree Distribution Moments: The Degeneracy Connection. In <em>      <em>International Colloquium on Automata, Languages, and Programming (ICALP)</em>     </em>, GRS11 (Ed.). 614&#x2013;633.</li>    <li id="BibPLXBIB0021" label="[21]">M. Faloutsos, P. Faloutsos, and C. Faloutsos. 1999. On power-law relationships of the internet topology. In <em>      <em>SIGCOMM.</em></em> 251&#x2013;262.</li>    <li id="BibPLXBIB0022" label="[22]">U. Feige. 2006. On sums of independent random variables with unbounded variance and estimating the average degree in a graph. <em>      <em>SIAM J. Comput.</em>     </em>35, 4 (2006), 964&#x2013;984.</li>    <li id="BibPLXBIB0023" label="[23]">O. Goldreich and D. Ron. 2002. Property Testing in Bounded Degree Graphs. <em>      <em>Algorithmica</em>     </em> (2002), 302&#x2013;343.</li>    <li id="BibPLXBIB0024" label="[24]">O. Goldreich and D. Ron. 2008. Approximating average parameters of graphs. <em>      <em>Random Structures and Algorithms</em>     </em>32, 4 (2008), 473&#x2013;493.</li>    <li id="BibPLXBIB0025" label="[25]">M. Gonen, D. Ron, and Y. Shavitt. 2011. Counting stars and other small subgraphs in sublinear-time. <em>      <em>SIAM Journal on Discrete Math</em>     </em>25, 3 (2011), 1365&#x2013;1411.</li>    <li id="BibPLXBIB0026" label="[26]">Mira Gonen, Dana Ron, Udi Weinsberg, and Avishai Wool. 2008. Finding a dense-core in Jellyfish graphs. <em>      <em>Computer Networks</em>     </em>52, 15 (2008), 2831&#x2013;2841. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.comnet.2008.06.005"      target="_blank">https://doi.org/10.1016/j.comnet.2008.06.005</a></li>    <li id="BibPLXBIB0027" label="[27]">J.&#x00A0;E. Hirsch. 2005. An index to quantify an individual&#x0027;s scientific research output. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>102, 46(2005), 16569&#x2013;16572.</li>    <li id="BibPLXBIB0028" label="[28]">A. Lakhina, J. Byers, M. Crovella, and P. Xie.2003. Sampling biases in IP topology measurements. In <em>      <em>Proceedings of INFOCOMM</em>     </em>, Vol.&#x00A0;1. 332&#x2013;341.</li>    <li id="BibPLXBIB0029" label="[29]">Sang&#x00A0;Hoon Lee, Pan-Jun Kim, and Hawoong Jeong. 2006. Statistical properties of sampled networks. <em>      <em>Physical Review E</em>     </em>73, 1 (2006), 016102.</li>    <li id="BibPLXBIB0030" label="[30]">Jure Leskovec. 2015. SNAP Stanford Network Analysis Project. <a href="http://snap.standord.edu" target="_blank">http://snap.standord.edu</a>. (2015).</li>    <li id="BibPLXBIB0031" label="[31]">Jure Leskovec and Christos Faloutsos. 2006. Sampling from large graphs. In <em>      <em>Knowledge Data and Discovery (KDD).</em></em> ACM, 631&#x2013;636.</li>    <li id="BibPLXBIB0032" label="[32]">A.&#x00A0;S. Maiya and T.&#x00A0;Y. Berger-Wolf. 2011. Benefits of Bias: Towards Better Characterization of Network Sampling, In <em>Knowledge Data and Discovery (KDD)</em>. <em>      <em>ArXiv e-prints</em>     </em>, 105&#x2013;113. arxiv:1109.3911</li>    <li id="BibPLXBIB0033" label="[33]">Andrew McGregor. 2014. Graph stream algorithms: A survey. <em>      <em>SIGMOD</em>     </em>43, 1 (2014), 9&#x2013;20.</li>    <li id="BibPLXBIB0034" label="[34]">M. Mitzenmacher. 2003. A Brief History of Generative Models for Power Law and Lognormal Distributions. <em>      <em>Internet Mathematics</em>     </em>1, 2 (2003), 226&#x2013;251.</li>    <li id="BibPLXBIB0035" label="[35]">M.&#x00A0;E.&#x00A0;J. Newman. 2003. The Structure and Function of Complex Networks. <em>      <em>SIAM Rev.</em>     </em>45, 2 (2003), 167&#x2013;256. <a class="link-inline force-break" href="https://doi.org/10.1137/S003614450342480"      target="_blank">https://doi.org/10.1137/S003614450342480</a></li>    <li id="BibPLXBIB0036" label="[36]">M.&#x00A0;E.&#x00A0;J. Newman, S. Strogatz, and D. Watts. 2001. Random graphs with arbitrary degree distributions and their applications. <em>      <em>Physical Review E</em>     </em>64(2001), 026118.</li>    <li id="BibPLXBIB0037" label="[37]">D. Pennock, G. Flake, S. Lawrence, E. Glover, and C.&#x00A0;L. Giles. 2002. Winners don&#x0027;t take all: Characterizing the competition for links on the web. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>99, 8 (2002), 5207&#x2013;5211. <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.032085699"      target="_blank">https://doi.org/10.1073/pnas.032085699</a></li>    <li id="BibPLXBIB0038" label="[38]">T. Petermann and P. Rios. 2004. Exploration of scale-free networks. <em>      <em>European Physical Journal B</em>     </em>38 (2004), 201&#x2013;204.</li>    <li id="BibPLXBIB0039" label="[39]">Ali Pinar, Sucheta Soundarajan, Tina Eliassi-Rad, and Brian Gallagher. 2015. <em>      <em>MaxOutProbe: An Algorithm for Increasing the Size of Partially Observed Networks.</em>     </em>Technical Report. Sandia National Laboratories (SNL-CA), Livermore, CA (United States).</li>    <li id="BibPLXBIB0040" label="[40]">Bruno Ribeiro and Don Towsley. 2012. On the estimation accuracy of degree distributions from graph sampling. In <em>      <em>Annual Conference on Decision and Control (CDC).</em></em> IEEE, 5240&#x2013;5247.</li>    <li id="BibPLXBIB0041" label="[41]">Dana Ron. 2010. Algorithmic and Analysis Techniques in Property Testing. <em>      <em>Foundations and Trends in Theoretical Computer Science</em>     </em>5, 2(2010), 73&#x2013;205.</li>    <li id="BibPLXBIB0042" label="[42]">Dana Ron and Gilad Tsur. 2016. The Power of an Example: Hidden Set Size Approximation Using Group Queries and Conditional Sampling. <em>      <em>ACM Transactions on Computation Theory</em>     </em>8, 4 (2016), 15:1&#x2013;15:19.</li>    <li id="BibPLXBIB0043" label="[43]">C. Seshadhri, Tamara&#x00A0;G. Kolda, and Ali Pinar. 2012. Community structure and scale-free collections of Erd&#x00F6;s-R&#x00E9;nyi graphs. <em>      <em>Physical Review E</em>     </em>85, 5 (May 2012), 056109. <a class="link-inline force-break"      href="https://doi.org/10.1103/PhysRevE.85.056109"      target="_blank">https://doi.org/10.1103/PhysRevE.85.056109</a></li>    <li id="BibPLXBIB0044" label="[44]">Olivia Simpson, C Seshadhri, and Andrew McGregor. 2015. Catching the head, tail, and everything in between: a streaming algorithm for the degree distribution. In <em>      <em>International Conference on Data Mining (ICDM).</em></em> IEEE, 979&#x2013;984.</li>    <li id="BibPLXBIB0045" label="[45]">Michael&#x00A0;PH Stumpf and Carsten Wiuf. 2005. Sampling properties of random graphs: the degree distribution. <em>      <em>Physical Review E</em>     </em>72, 3 (2005), 036118.</li>    <li id="BibPLXBIB0046" label="[46]">Yaonan Zhang, Eric&#x00A0;D Kolaczyk, and Bruce&#x00A0;D Spencer. 2015. Estimating network degree distributions under sampling: An inverse problem, with applications to monitoring social media networks. <em>      <em>The Annals of Applied Statistics</em>     </em>9, 1 (2015), 166&#x2013;199.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn2"><a href="#foot-fn2"><sup>*</sup></a>Work funded by Sandia LDRD program, Israel Science Foundation grant No. 671/13, Azrieli Fellowship, and NSF TRIPODS grant CCF-1740850. Part of this work was initiated at the Simons Institute Semester on Algorithms and Uncertainty.</p>   <p id="fn3"><a href="#foot-fn3"><sup>&#x2020;</sup></a>Both Talya Eden and Shweta Jain contributed equally to this work, and are joint first authors of this work.</p>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a href="https://sjain12@bitbucket.org/sjain12/saddles.git" target="_blank">https://sjain12@bitbucket.org/sjain12/saddles.git</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186111">https://doi.org/10.1145/3178876.3186111</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
