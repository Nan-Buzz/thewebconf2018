<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Detection of Strength and Causal Agents of Stress and Relaxation for tweets</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Detection of Strength and Causal Agents of Stress and Relaxation for tweets</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Reshmi Gopalakrishna</span>     <span class="surName">Pillai</span>,     Supervised by Prof. Mike Thelwall and Dr. Constantin Orasan Research Institute of Information and Language Processing University of Wolverhampton, UK, <a href="mailto:reshmi.g85@gmail.com">reshmi.g85@gmail.com</a>        </div>        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3186572" target="_blank">https://doi.org/10.1145/3184558.3186572</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>The<a class="fn" href="#en1" id="foot-en1"><sup>1</sup></a> ability to detect human stress and relaxation is central for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of social media can be leveraged to identify stress and relaxation. In this PhD research, we introduce an improved method to detect expressions of stress and relaxation in social media content. It uses word sense vectors for word sense disambiguation to improve the performance of the first ever lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that TensiStrength with word sense disambiguation performs better than the original TensiStrength and state-of-the-art machine learning methods in terms of Pearson&#x0027;s correlation and accuracy. We also suggest a novel, word-vector based approach for detecting causes of stress and relaxation in social media content.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>KEYWORDS:</small>     </span>     <span class="keyword">      <small>Stress</small>, </span>     <span class="keyword">      <small>social media</small>, </span>     <span class="keyword">      <small>Twitter</small>, </span>     <span class="keyword">      <small>sentiment analysis</small>, </span>     <span class="keyword">      <small>word sense disambiguation</small>, </span>     <span class="keyword">      <small>word vectors</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>Reshmi Gopalakrishna Pillai. 2018. Detection of Strength and Causal Agents of Stress and Relaxation for tweets . In <em>Proceedings of The 2018 Web Conference Companion (WWW'2018 Companion). ACM, New York, NY, USA, 7 pages.</em> <a href="https://doi.org/10.1145/3184558.3186572" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3186572</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec1">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> PROBLEM</h2>    </div>    </header>    <p>The ability to identify stress in social media text is important in the health domain as a diagnostic tool for health-related disorders, for customer focused business to detect unpleasant experiences (e.g., tourism), and for systems that manage humans on a large scale (e.g., transportation, crowd management). The early detection of stress can help early action to be taken to avoid the situation escalating. Depending on the context, the actions might include recommending a medical consolation for a stress-related disorder (or providing evidence for such a consultation), changing hotel operating procedures to avoid customer stress hotspots, re-routing stressed customers to human agents rather than automatic phone agents, or triggering emergency traffic management measures. Similarly, it is important to know the measure and causes of relaxation as well, as an affective state opposite to stress and as an indicator of content in applications and services.</p>    <p>The detection of stress and relaxation from social media content has been a largely unexplored research field. This PhD research addresses two separate tasks related to stress and relaxation expressed in social media: The first task implements a word-vector based sense disambiguation method to improve the performance of the lexicon-based stress/relaxation detection TensiStrength [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib1">1</a>] on Twitter. In the second task, we propose a framework which makes use of word vectors to find the causal agents of stress and relaxation (called stressor and relaxer respectively, hereafter in this paper) in a tweet.</p>    <p>Contributions of the current work are as follows: <ol class="list-no-style">     <li label="1.">Incorporating WSD with stress/relaxation detection for the first time.<br/></li>     <li label="2.">An updated lexicon for stress/relaxation scores which accommodates polysemy in stress/relaxation word.<br/></li>     <li label="3.">We propose a novel approach to find out the reasons of stress and relaxation expressed in social media content. This is the first attempt to find out the means of stress and relaxation, to the best of our knowledge, in tweets belonging to different domains such as politics, sports etc.<br/></li>    </ol>    </p>   </section>   <section id="sec2">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> STATE OF THE ART</h2>    </div>    </header>    <p>Although automated stress/relaxation detection is relatively new, it is closely related to other affective tasks within sentiment analysis, such as for polarity or fine-grained emotions.</p>    <section id="sec2Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Identification of Stress and Relaxation from Social Media Content</h3>     </div>    </header>    <p>TensiStrength is the first published system to detect the strength of stress and relaxation expressed in tweets. It is primarily a lexical approach. The lexicon is partly derived from LIWC [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib2">2</a>], General Inquirer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib3">3</a>] and emotion terms from SentiStrength [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib4">4</a>] a similar lexicon-based sentiment analysis program. For the original evaluation, a corpus of 3066 English tweets were human-coded for stress and relaxation on five point scales: -1 denoting no stress and -5 denoting the highest stress; +1 for no relaxation and +5 for the highest relaxation. Its performance was similar to machine learning algorithms, including Support Vector Machines.</p>    <p>The hybrid system presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib5">5</a>] combines a factor graph model and convolutional neural network (CNN) and establishes the relation between users&#x2019; psychological stress levels and their social network interactions. This method improves stress detection by 6-9% in terms of F1-score.</p>    <p>The system described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib6">6</a>] identifies stress in social media texts. This uses stressor event categories and stressor subjects and a collection of words similar to each category and subject on the basis of word embeddings. The stressor event and subjects are identified using a novel hybrid model which combines multi-task learning with CNN. Tweets are assigned a stress value based on Social Readjustment Rating Scale [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib7">7</a>]. The stress scores obtained this way give results comparable to some state of the art machine learning models.</p>    <p>This work, while being a pioneer study to find the causes of stress and relaxation, limits its area of interest to personal events such as divorce, death and childbirth. Our study, on the other hand, analyzes tweets expressing stress and relaxation of both short-term and long-term nature, pertaining to a variety of contexts such as traffic, politics, personal events and academia.</p>    </section>    <section id="sec2Z2">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Word Sense Disambiguation in Sentiment Analysis</h3>     </div>    </header>    <p>An important challenge for detecting expressions of stress or any affective state is that social media text uses non-standard grammar and informal language elements. Affective words can be ambiguous, with their sense changing according to the context. For example, in &#x201C;The lady gave him a cool stare&#x201D;, the affect word &#x201C;cool&#x201D; indicates stress whereas in &#x201C;I am pretty cool about things&#x201D;, it indicates relaxation. A system to resolve the meaning of this word is essential to correctly identify stress. The natural language processing task of WSD identifying the meaning of words in context in a computational manner [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib8">8</a>], is the traditional response to this issue.</p>    <p>A WSD system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib9">9</a>] using Babelfy and SentiWordNet illustrates how it can improve the accuracy metrics of sentiment detection in Twitter and SMS test data. Babelfy is a multilingual graph-based method for disambiguating word senses. This system is shown to have better accuracy (58.55%) compared to the baseline method(45.26%).A graph-based method of WSD for sentiment analysis of figurative language [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib10">10</a>] is shown to have significantly better recall and precision compared to a polarity detection method without WSD and a baseline WSD (which assigns the first sense entry in WordNet to all senses).</p>     <p>Yet another study [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib11">11</a>] integrates a WSD algorithm using extended gloss overlap described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib12">12</a>] with VoxPop[13], a classifier of text polarity based on SentiWordNet and improved accuracy from 50.5% to 60.0%. While varying in methodology and lexical resources, all of these studies illustrate that sentiment analysis systems with WSD substantially outperforms those without WSD. Our study treats WSD as a potential way to improve accuracy of an existing lexical method.</p>    </section>    <section id="sec2Z3">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Word Vector Representation and its application in Word Sense Disambiguation</h3>     </div>    </header>    <p>Standard natural language processing methods for WSD are predicated on standard grammar and spelling, which is uncommon in the social web. A promising method for WSD for informal text is to represent a word as a real-valued vector registering the frequency of its contextual words (e.g., in the same sentence). This can be compared to similar aggregate vectors for the different senses of the word and the closest matching vector used to select the most likely work sense. GloVe, an unsupervised learning algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib14">14</a>] and Word2Vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib15">15</a>,<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib16">16</a>] are the two most common architectures for finding vector representations for words. Word2Vec uses continuous vector representations of words in two models &#x2013; Bag of words and Skip-gram. In bag of words, the target is predicting a central word in focus, given the input context words. The Skip-gram model, on the other hand, has the focus word as the single input vector and the target is to predict the context words as outcome. GloVe model aims to combine the benefits of skip-gram model with global statistical information about word co-occurrences using matrix factorization methods.</p>    <p>Sense2Vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib17">17</a>] is a supervised sense embedding system which can distinguish between different senses of the same word based on the context. Similarly, a sense vector scheme obtained from skip-gram based word vectors and WordNet glosses has been proposed for WSD [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib18">18</a>]. The algorithm presented in this paper works as two steps : initialization of word and sense vectors and WSD. In the first step, the word vectors are learned through skip-gram model and from this, sense vectors for each sense is constructed. For each word in the gloss, the cosine similarity with the original word is calculated. Those words with cosine similarity higher than a threshold are added to a candidate set. The average of the vectors of the candidate words is taken as the sense vector.</p>     <p>In the second step of word disambiguation, given a sentence, an initial context vector is calculated by finding the average of content words&#x2019; vectors. The sense which has the highest cosine similarity to the context vector is taken as the disambiguation result. We implemented this method of sense disambiguation with word vectors in our current experiments.</p>    </section>   </section>   <section id="sec3">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> PROPOSED APPROACH</h2>    </div>    </header>    <p>The proposed research work attempts to find out the answers to two research questions: <ol class="list-no-style">     <li label="1.">Can the accuracy of psychological stress or relaxation strength detection from social media content be improved using WSD?<br/></li>     <li label="2.">Can we detect causal agents of stress and relaxation from social media content?<br/></li>    </ol>    </p>    <section id="sec3Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Improving the accuracy of TensiStrength</h3>     </div>    </header>    <p>This task improves the performance of existing TensiStrength method by incorporating a word-vector based WSD solution (originally proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib18">18</a>]). The accuracy of the modified TensiStrength is compared to and found to be significantly better that that of TensiStrength without WSD and state-of-the-art machine learning algorithms.</p>    </section>    <section id="sec3Z2">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> A novel method to find out the reasons of stress and relaxation in tweets</h3>     </div>    </header>    <p>The proposed method divides the task of finding stressors and relaxers into two: finding the category/domain of tweets and word vector processing to find the exact stressor/relaxer.</p>    </section>   </section>   <section id="sec4">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> METHODOLOGY</h2>    </div>    </header>    <section id="sec4Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Improving the accuracy of TensiStrength</h3>     </div>    </header>    <p>The methodology consisted of the following steps:</p>    <p>     <em>4.1.1 Literature review.</em> The existing literature about the following aspects was studied: WSD and how it improves the accuracy of sentiment detection in social media content, Usage of word vectors in the task of WSD, stress/relaxation detection from social media content.</p>     <p>     <em>4.1.2 Workflow with a sample tweet.</em> A sample tweet to be processed was &#x201C;The lady gave me a cool stare&#x201D;. The classic version of TensiStrength will identify the affect word &#x2018;cool&#x2019; and will assign a relaxation strength of +2 to this tweet. Whereas, a human coder will be able to disambiguate &#x2018;cool&#x2019; and correctly assign a stress strength of -2 to it. In the modified version of TensiStrength, (depicted in fig. <a class="fig" href="#fig1">1</a>) the pre-processing step, which implements WSD first finds out in which sense out of a list of possible ones from WordNet, the word has been used in this context. This pre-processed tweet with the correctly disambiguated affect word is further given to the lexicon based stress/relaxation magnitude detection method which, now, correctly identifies the tweet as denoting stress, not relaxation.</p>    <p>     <em>4.1.3 Extending lexicon of TensiStrength to incorporate ambiguity of affect words</em>. We manually updated the TensiStrength lexicon to accommodate the stress/relaxation values for different senses of 40 ambiguous words. For example, the lexicon for</p>    <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186572/images/www18companion-126-fig1.jpeg" class="img-responsive" alt="Figure 1:" longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Workflow of tweets&#x2019; processing by the modified TensiStrength algorithm (with WSD).</span>     </div>    </figure>    <p>TensiStrength without WSD had only one entry for the affect word &#x2018;cool&#x2019;, as +2 indicating moderate relaxation. But the modified lexicon, acknowledges the various senses of the same word, obtained from the resource WordNet. For example, the ninth sense of the word &#x2018;cool&#x2019; in WordNet is &#x2018;unfriendly or showing dislike&#x2019; which has an indication for stress, hence it is assigned the value of -2. Similarly, other senses are assigned appropriate stress/relaxation values too. The ordinal number of the sense in WordNet is represented by a suffix to the original word. A few example additions to the lexicon, are given as below. The gloss of the sense is given in parentheses.cool_2 +3 (aplomb, assuredness, cool, poise, sangfroid)cool_9 -2 (unfriendly or unresponsive or showing dislike)</p>    <p>4.1.4 Dataset. A set of one thousand tweets with ambiguous affect words (e.g. cool) was collected over the period of one month (from 1<sup>st</sup> February 2017 to 1<sup>st</sup> March 2017) as the dataset for the experiments. It was annotated individually and independently by a set of three human coders on a five point scale for stress and relaxation. The coders had participated in an identical annotation procedure as part of the TensiStrength experiment. Hence they had prior experience with the stress/relaxation strength annotation task. The tweets were annotated for stress and relaxation strengths with scores ranging from +1 to +5 (+1 denotes no relaxation and +5 very high relaxation) and -1 to -5 (-1 denotes no stress and -5 very high stress). The arithmetic mean of the three coders&#x2019; annotation scores were calculated for each tweet and rounded off to the nearest integer value. This was taken as the gold standard of stress and relaxation values. The inter-coder agreement between each pair of coders was calculated using Krippendorff&#x0027;s &#x03B1; . Krippendorff suggests lowest conceivable limit of &#x03B1; &gt; 0.667. The overall agreement between the three coders for stress (0.778) and relaxation (0.781) was found to be high enough.</p>     <div class="table-responsive" id="tb1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Inter-coder agreement for stress value annotation.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Method</th>       <th style="text-align:left;">A and B</th>       <th style="text-align:left;">B and C</th>       <th style="text-align:left;">A and C</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Krippendorff&#x0027;s &#x03B1;</td>       <td style="text-align:left;">0.774</td>       <td style="text-align:left;">0.781</td>       <td style="text-align:left;">0.771</td>       </tr>       <tr>       <td style="text-align:left;">Pearson Correlation</td>       <td style="text-align:left;">0.796</td>       <td style="text-align:left;">0.814</td>       <td style="text-align:left;">0.792</td>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tb2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Inter-coder agreement for relaxation annotation.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Method</th>       <th style="text-align:left;">A and B</th>       <th style="text-align:left;">B and C</th>       <th style="text-align:left;">A and C</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Krippendorff&#x0027;s &#x03B1;</td>       <td style="text-align:left;">0.751</td>       <td style="text-align:left;">0.802</td>       <td style="text-align:left;">0.781</td>       </tr>       <tr>       <td style="text-align:left;">Pearson Correlation</td>       <td style="text-align:left;">0.796</td>       <td style="text-align:left;">0.820</td>       <td style="text-align:left;">0.797</td>       </tr>      </tbody>     </table>    </div>    <p>Higher agreement values in the subsequent experiments denote that the coders are consistently getting better with the annotation task. It also indicates that the problem statement and annotation instructions are very well-defined.</p>     <p>     <em>4.1.5 Experiments.</em> A twitter Word2Vec model trained on 400 million tweets, released as part of an ACL W-NUT task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib19">19</a>] is used for training the word and sense vectors for this experiment. The WSD module based on the method presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib11">11</a>] was implemented using Anaconda Python packages, as part of our research work to evaluate the improvement it would bring about in stress and relaxation strength detection.</p>    </section>    <section id="sec4Z2">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Finding the causal agents of stress and relaxation in tweets</h3>     </div>    </header>    <p>     <em>4.2.1 Literature Review.</em> The existing literature about the following aspects was studied: Stress/relaxation detection from social media content, domain classification of tweets.</p>    <p>     <em>4.2.2. Formulation of the proposed framework</em>    </p>    <figure id="fig2">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186572/images/www18companion-126-fig2.jpg" class="img-responsive" alt="Figure 2:" longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Framework for finding stressors and relaxers.</span>     </div>    </figure>    <p>We formulated a framework for finding the stressors and relaxers in tweets. This framework will primarily consist of two modules. 1) Categoriser and 2) Cause Finder. The first module uses a machine learning classifier to tag the tweets with a broad category of topic such as politics, sports, entertainment, personal events, health, climate and traffic. Each category has a list of associated potential stressors and relaxers. Thus, the reason for expressed stress and relaxation is narrowed down to one among this list. The Cause Finder module finds the actual cause from this list using word vector representations. It eliminates redundant words such as prepositions, conjunctions, interjections and articles from the tweet and constructs a set of keywords from the remaining words. The cosine similarity of vectors representing each word in this set with each potential stressor/relaxer in the relevant category. The stressor/relaxer word represented by the vector having the highest cosine similarity to any of the keywords in the tweet is chosen as the cause of stress/relaxation in the tweet.</p>    <p>     <em>4.2.3. Implementation.</em> To illustrate this methodology, we analyse the sample tweet, &#x201C;&#x201C;Make no mistakes about it, this is no usual social media noise. We shall shout. We shall march.&#x201D; which was found to have a stress/relaxation score of -3(high stress) and +1 (no relaxation) by TensiStrength.</p>     <p>We use a machine learning classifier to categorize this tweet into the broad category, &#x201C;politics&#x201D;. A potential stressor list belonging to the category of politics is S= {&#x2018;election&#x2019;, &#x2019;protest&#x2019;, &#x2019;scam&#x2019;, &#x2019;speech&#x2019;}. Next, the keyword set is created after eliminating all redundant words. The keyword set, K ={&#x2018;make&#x2019;, &#x2019;mistakes&#x2019;, &#x2019;usual&#x2019;, &#x2019;social&#x2019;, &#x2019;media&#x2019;, &#x2019;noise&#x2019;, &#x2019;shout&#x2019;, &#x2019;march&#x2019;} For each pair of words in the cross-product of these two sets, the cosine similarity is calculated. The first element in the pair with the highest cosine similarity is chosen as the stressor. Using a Twitter Word2Vec model trained on 400 million tweets, released as part of an ACL W-NUT task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#bib19">19</a>], the cosine similarity between the potential stressors and keywords is found to be as follows:</p>    <div class="table-responsive" id="tb3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Cosine similarity between stressors and keywords.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;"/>       <th style="text-align:left;">Election</th>       <th style="text-align:left;">Protest</th>       <th style="text-align:left;">Scam</th>       <th style="text-align:left;">speech</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Make</td>       <td style="text-align:left;">0.1045</td>       <td style="text-align:left;">0.1585</td>       <td style="text-align:left;">0.1629</td>       <td style="text-align:left;">0.0912</td>       </tr>       <tr>       <td style="text-align:left;">Mistake</td>       <td style="text-align:left;">0.1439</td>       <td style="text-align:left;">0.0835</td>       <td style="text-align:left;">0.1752</td>       <td style="text-align:left;">0.1510</td>       </tr>       <tr>       <td style="text-align:left;">Usual</td>       <td style="text-align:left;">0.1601</td>       <td style="text-align:left;">0.1592</td>       <td style="text-align:left;">0.0871</td>       <td style="text-align:left;">0.1411</td>       </tr>       <tr>       <td style="text-align:left;">Social</td>       <td style="text-align:left;">0.1793</td>       <td style="text-align:left;">0.2431</td>       <td style="text-align:left;">0.1946</td>       <td style="text-align:left;">0.1969</td>       </tr>       <tr>       <td style="text-align:left;">Media</td>       <td style="text-align:left;">0.3904</td>       <td style="text-align:left;">0.3073</td>       <td style="text-align:left;">0.2310</td>       <td style="text-align:left;">0.2451</td>       </tr>       <tr>       <td style="text-align:left;">Noise</td>       <td style="text-align:left;">0.1442</td>       <td style="text-align:left;">0.2111</td>       <td style="text-align:left;">0.1730</td>       <td style="text-align:left;">0.2070</td>       </tr>       <tr>       <td style="text-align:left;">Shout</td>       <td style="text-align:left;">0.0123</td>       <td style="text-align:left;">0.2305</td>       <td style="text-align:left;">0.1528</td>       <td style="text-align:left;">0.2612</td>       </tr>       <tr>       <td style="text-align:left;">March</td>       <td style="text-align:left;">0.3034</td>       <td style="text-align:left;">        <strong>0.4502</strong>       </td>       <td style="text-align:left;">0.1719</td>       <td style="text-align:left;">0.2474</td>       </tr>      </tbody>     </table>    </div>    <p>Here, the stressor-keyword pair with highest cosine similarity(0.4502) is (Protest, March). Thus, &#x2018;Protest&#x2019; in &#x2018;Politics&#x2019; category is chosen as the stressor for the tweet, &#x201C;Make no mistakes about it, this is no usual social media noise. We shall shout. We shall march.&#x201D;</p>    <p>This pilot experiment illustrates the feasibility of this approach. We are in the process of constructing the dataset which is a collection of tweets, annotated with categories and stress/relaxation causes, to evaluate the performance of this approach. The future study will consist of evaluating the proposed framework on this dataset and compiling the results.</p>    </section>   </section>   <section id="sec5">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> RESULTS</h2>    </div>    </header>    <section id="sec5Z1">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Improving the accuracy of stress and relaxation detection in tweets</h3>     </div>    </header>    <p>The accuracy of stress and relaxation detection in tweets using TensiStrength with WSD is compared with TensiStrength without the WSD preprocessing phase and a range of standard machine learning algorithms: Adaptive Boosting algorithm(AdaBoost), Na&#x00EF;ve Bayes classifier(Bayes), A classification tree(J48 Tree), Logistic Regression(Logistic) and Support Vector Machines(SVM). Unigrams, Bigrams and Trigrams were used as features. Each classifier was implemented using its configuration in Weka 3.6. Based on Pearson&#x0027;s correlations and exact match percentages with the human-annotated data, it is found TensiStrength with a preprocessing phase to disambiguate the word senses performs considerably better than the state-of-the-art machine learning algorithms and TensiStrength without the preprocessing. The results are summarized in the tables below:</p>    <div class="table-responsive" id="tb4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Stress detection results.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Method</th>       <th style="text-align:left;">Pearson&#x0027;s Correlation</th>       <th style="text-align:left;">Exact match %</th>       <th style="text-align:left;">Match %(within 1)</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">AdaBoost</td>       <td style="text-align:left;">0.3543</td>       <td style="text-align:left;">30.1671</td>       <td style="text-align:left;">83.1562</td>       </tr>       <tr>       <td style="text-align:left;">Bayes</td>       <td style="text-align:left;">0.3238</td>       <td style="text-align:left;">35.3464</td>       <td style="text-align:left;">90.1824</td>       </tr>       <tr>       <td style="text-align:left;">J48 Tree</td>       <td style="text-align:left;">0.4649</td>       <td style="text-align:left;">47.4562</td>       <td style="text-align:left;">88.3543</td>       </tr>       <tr>       <td style="text-align:left;">Logistic</td>       <td style="text-align:left;">0.4948</td>       <td style="text-align:left;">49.2679</td>       <td style="text-align:left;">89.2167</td>       </tr>       <tr>       <td style="text-align:left;">SVM</td>       <td style="text-align:left;">0.5124</td>       <td style="text-align:left;">51.2543</td>       <td style="text-align:left;">91.5626</td>       </tr>       <tr>       <td style="text-align:left;">TensiStrength<br/>Without WSD</td>       <td style="text-align:left;">0.4735</td>       <td style="text-align:left;">48.8112</td>       <td style="text-align:left;">83.2367</td>       </tr>       <tr>       <td style="text-align:left;">TensiStrength with WSD</td>       <td style="text-align:left;">0.5443</td>       <td style="text-align:left;">53.1091</td>       <td style="text-align:left;">92.4137</td>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tb5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Relaxation detection results.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Method</th>       <th style="text-align:left;">Pearson&#x0027;s Correlation</th>       <th style="text-align:left;">Exact Match %</th>       <th style="text-align:left;">Match % (within 1)</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">AdaBoost</td>       <td style="text-align:left;">0.3254</td>       <td style="text-align:left;">34.1335</td>       <td style="text-align:left;">83.5451</td>       </tr>       <tr>       <td style="text-align:left;">Bayes</td>       <td style="text-align:left;">0.3577</td>       <td style="text-align:left;">38.9221</td>       <td style="text-align:left;">84.6735</td>       </tr>       <tr>       <td style="text-align:left;">J48 Tree</td>       <td style="text-align:left;">0.5224</td>       <td style="text-align:left;">51.5634</td>       <td style="text-align:left;">86.1734</td>       </tr>       <tr>       <td style="text-align:left;">Logistic</td>       <td style="text-align:left;">0.4987</td>       <td style="text-align:left;">54.3267</td>       <td style="text-align:left;">89.1798</td>       </tr>       <tr>       <td style="text-align:left;">SVM</td>       <td style="text-align:left;">0.5546</td>       <td style="text-align:left;">58.7324</td>       <td style="text-align:left;">91.6598</td>       </tr>       <tr>       <td style="text-align:left;">TensiStrength<br/>Without WSD</td>       <td style="text-align:left;">0.5304</td>       <td style="text-align:left;">56.3878</td>       <td style="text-align:left;">85.8364</td>       </tr>       <tr>       <td style="text-align:left;">TensiStrength with WSD</td>       <td style="text-align:left;">0.56441</td>       <td style="text-align:left;">60.6981</td>       <td style="text-align:left;">93.1227</td>       </tr>      </tbody>     </table>    </div>    <p>TensiStrength with WSD phase performs substantially better than TensiStrength without WSD and the machine learning methods. However, the small size of the annotated dataset (and the training data) could have been a major cause of the relatively poor performance of the machine learning methods. Further results with higher number of tweets could better establish the superior effectiveness of TensiStrength with WSD over standard machine learning methods as a stress and relaxation detection method. Also, tweets in the current dataset contain the ambiguous words added to the TensiStrength lexicon. The performance on a randomly chosen set of tweets could be different.</p>    </section>   </section>   <section id="sec6">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> CONCLUSIONS AND FUTURE WORK</h2>    </div>    </header>    <p>We address two research problems as part of the PhD study. Firstly, we implemented a WSD solution as a pre-processing stage to an existing lexicon-based stress/relaxation method. Incorporating a WSD was found to significantly improve the performance of TensiStrength in terms of Pearson&#x0027;s correlation and exact match percentage, for both stress and relaxation. TensiStrength with WSD outperforms machine learning methods as well, however this has to be further studied using bigger datasets annotated with stress and relaxation strengths. The second research question is to find the reasons of stress and relaxation in social media. We propose a novel framework for finding the stressors and relaxers in a variety of domains such as politics, sports, traffic and personal events. We successfully ran pilot experiments to verify the feasibility of this approach. Implementation of this approach is the planned future step in this research work. In future, such an automatic method as developed in this research work can potentially replace or complement traditional stress/relaxation detection methods and can contribute to improved applications and services such as healthcare, traffic management or customer care. It is a promising step in towards the greater goal of leveraging the Web and specifically social media data.</p>   </section>  </section>  <section class="back-matter">   <section id="bib-sec-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="bib1" label="[1]">Thelwall, M.&#x00A0;TensiStrength: stress and relaxation magnitude detection for social media texts. J Inf Process Manag&#x00A0;2017; 53:&#x00A0;106&#x2013;121</li>    <li id="bib2" label="[2]">Tausczik, Y. R. and Pennebaker, J. W. 2010. The psychological meaning of words: LIWC and computerized text analysis methods<em>. Journal of language and social psychology</em>, 29(1), 24-54.</li>    <li id="bib3" label="[3]">Stone, P. J., Dunphy, D. C., Smith, M. S., and Ogilvie, D. M. 1966. The general inquirer: A computer approach to content analysis. Cambridge, MA: The MIT Press</li>    <li id="bib4" label="[4]">Thelwall, M., Buckley, K., Paltoglou, G. Cai, D., and Kappas, A. 2010. Sentiment strength detection in short informal text. <em>Journal of the American Society for Information Science and Technology</em>, 61(12), 2544&#x2013;2558.</li>    <li id="bib5" label="[5]">Lin, H., Jia, J., Qiu, J., <em>et al.</em> Detecting stress based on social interactions in social networks. 2017<em>. IEEE Transactions on Knowledge and Data Engineering,</em> 2017.</li>    <li id="bib6" label="[6]">Lin, H., Jia, J., Nie, L., Shen, G., and Chua, T.S. 2016. What does social media say about your stress? In <em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,</em>3775&#x2013;3781.</li>    <li id="bib7" label="[7]">Holmes, T.H., and Rahe, R.H. 1967. The social readjustment rating scale<em>. Journal of psychosomatic research</em>, 11(2):213&#x2013;218.</li>    <li id="bib8" label="[8]">Navigli, R. 2009. Word Sense Disambiguation: A survey. <em>ACM Computing Surveys, 41(2009), 1-69.</em></li>    <li id="bib9" label="[9]">Sumanth, C., and Inkpen, D. 2015. How much does word sense disambiguation help in sentiment analysis of micropost data? In <em>Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</em>, 115&#x2013;121, Lisbon.</li>    <li id="bib10" label="[10]">Rentoumi, V., Giannakopoulos, G., Karkaletsis, V., and Vouros, G. 2009. Sentiment analysis of figurative language using a word sense disambiguation approach. In&#x00A0;<em>Proceedings of the International Conference RANLP'09</em>, pages 370&#x2013;375, Borovets, Bulgaria.</li>    <li id="bib11" label="[11]">Razon, B., and Cheng, C., Word Sense Disambiguation of Opinionated Words Using Extended Gloss Overlap. 2011 In <em>Proceedings of the 8<sup>th</sup> National Natural Language Processing Research Symposium,</em> 1-5, Manilla</li>    <li id="bib12" label="[12]">Banerjee, S., and Peterson, T. 2003. Extended Gloss Overlaps as a Measure of Semantic Relatedness. In <em>Proceedings of the 18<sup>th</sup> International Joint Conference on Atrificial Intelligence</em>, 802-810, Mexico</li>    <li id="bib13" label="[13]">Bautista, G.Z., Garcia, M.A., and Tan, R.J. 2010. VoxPop: Automated Opinion Detection and Classification with Data Clustering. Manila.</li>    <li id="bib14" label="[14]">Pennington, J., Socher, R., and Manning, C.D. 2014. Glove: Global Vectors for Word Representation. In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> (October 2014), 1532-1543</li>    <li id="bib15" label="[15]">Mikolov, T., Chen, K., Corrado, G., and Dean, J. 2013. Efficient Estimation of Word Representation in Vector Space. <em>arXiv preprint arXiv:1301.3781</em>,1-12</li>    <li id="bib16" label="[16]">Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. 2013. Distributed Representations of Words, Advances in neural information processing systems, 3111-3119.</li>    <li id="bib17" label="[17]">Trask, A., P. Michalak, and J. Liu. sense2vec-a fast and accurate method for word sense disambiguation in neural word embeddings<em>. arXiv preprint arXiv:</em>1511.06388, 2015</li>    <li id="bib18" label="[18]">Chen, X., Liu, Z., and Sun, M. 2014. A unified model for word sense representation and disambiguation. In&#x00A0;<em>EMNLP</em>.1025&#x2013;1035.</li>    <li id="bib19" label="[19]">Godin, F., Vandersmissen, B., De Neve, W., and Van de Walle R. 2015. Named entity recognition for twitter microposts using distributed word representations. In&#x00A0;<em>Proceedings of the Workshop on Noisy User-generated Text</em>, pages 146&#x2013;153, Beijing, China, July 2015. Association for Computational Linguistics</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="en1"><a href="#foot-en1"><sup>1</sup></a>This PhD research is supervised by Prof. Mike Thelwall and Dr. Constantin Orasan of Research Institute of Information and Language Processing, University of Wolverhampton, Wolverhampton, UK.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186572">https://doi.org/10.1145/3184558.3186572</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
