<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Spectral Algorithms for Temporal Graph Cuts</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Spectral Algorithms for Temporal Graph Cuts</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Arlei</span>      <span class="surName">Silva</span>,     University of California, Santa Barbara, CA, <a href="mailto:arlei@cs.ucsb.edu">arlei@cs.ucsb.edu</a>     </div>     <div class="author">     <span class="givenName">Ambuj</span>      <span class="surName">Singh</span>,     University of California, Santa Barbara, CA, <a href="mailto:ambuj@cs.ucsb.edu">ambuj@cs.ucsb.edu</a>     </div>     <div class="author">     <span class="givenName">Ananthram</span>      <span class="surName">Swami</span>,     Army Research Laboratory, Adelphi, MD, <a href="mailto:ananthram.swami.civ@mail.mil">ananthram.swami.civ@mail.mil</a>     </div>    </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186118" target="_blank">https://doi.org/10.1145/3178876.3186118</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>The sparsest cut problem consists of identifying a small set of edges that breaks the graph into balanced sets of vertices. The normalized cut problem balances the total degree, instead of the size, of the resulting sets. Applications of graph cuts include community detection and computer vision. However, cut problems were originally proposed for static graphs, an assumption that does not hold in many modern applications where graphs are highly dynamic. In this paper, we introduce sparsest and normalized cuts in temporal graphs, which generalize their standard definitions by enforcing the smoothness of cuts over time. We propose novel formulations and algorithms for computing temporal cuts using spectral graph theory, divide-and-conquer and low-rank matrix approximation. Furthermore, we extend temporal cuts to dynamic graph signals, where vertices have attributes. Experiments show that our solutions are accurate and scalable, enabling the discovery of dynamic communities and the analysis of dynamic graph processes.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Data mining;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Graph mining; Spectral Theory</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Arlei Silva, Ambuj Singh, and Ananthram Swami. 2018. Spectral Algorithms for Temporal Graph Cuts. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186118" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186118</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Temporal graphs represent how a graph changes over time, being ubiquitous in data mining and Web applications. Users in social networks present dynamic behavior, leading to the evolution of communities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. In hyperlinked environments, such as blogs, new topics drive modifications in content and link structure [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. Communication, epidemics and mobility are other scenarios where temporal graphs can enable the understanding of complex processes. However, several key concepts and algorithms for static graphs have not been generalized to temporal graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>].</p>    <p>This paper focuses on cut problems in temporal graphs, which consist of finding a small sets of edges (or cuts) that break the graph into balanced sets of vertices. Two traditional graph cut problems are the <em>sparsest cut</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and the <em>normalized cut</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. In sparsest cuts, the resulting partitions are balanced in terms of size, while in normalized cuts, the balance is in terms of total degree (or volume) of the resulting sets. Graph cuts have applications in community detection, image segmentation, clustering, and VLSI design. Moreover, the computation of graph cuts based on eigenvectors of graph-related matrices is one of the earliest results in spectral graph theory [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>], a subject with great impact in information retrieval [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>], graph sparsification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>], and machine learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>].</p>    <p>One of our motivations to study graph cuts in this new setting is the emerging field of <em>Signal Processing on Graphs (SPG)</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. SPG is a framework for the analysis of data residing on vertices of a graph, as a generalization of traditional signal processing. Temporal cuts can be applied as bases for signal processing on dynamic graphs.</p>    <p>     <strong>Our Contributions.</strong> We propose formulations of sparsest and normalized cuts in a sequence of graph snapshots. The idea is to extend classical definitions of these problems while enforcing the smoothness (or stability) of cuts over time. Our formulations can be understood using a multiplex view of the temporal graph, where additional edges connect the same vertex in different snapshots.</p>    <p>Figure <a class="fig" href="#fig1">1</a> shows a sparse temporal graph cut for a school network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>], where children are connected based on proximity. Vertices are naturally organized into communities resulting from classes. However, there is a significant amount of interaction across classes (e.g. during lunch). Major changes in the contact network can be noticed during the experiment, causing several vertices to move across partitions&#x2014;identified with vertex shapes/colors. The temporal cut is able to capture such trends while keeping the remaining vertex assignments mostly unchanged.</p>    <p>Traditional spectral solutions, which compute approximated cuts as rounded eigenvectors of the Laplacian matrix, do not generalize to our setting. Thus, we propose new algorithms, still within the framework of spectral graph theory, for the computation of temporal cuts. We further exploit important properties of our formulation to design efficient approximation algorithms for temporal cuts combining <em>divide-and-conquer</em> and low-rank matrix approximation.</p>    <p>In order to also model dynamic data embedded on the vertices of a graph, we apply temporal cuts as data-driven wavelet bases. Our approach exploits smoothness in both space and time, illustrating how the techniques presented in this paper provide a powerful and general framework for the analysis of dynamic graphs.</p>    <p>     <strong>Summary of contributions.</strong>    </p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">We generalize sparsest and normalized cuts to temporal graphs; we further extend temporal cuts to graph signals.<br/></li>     <li id="list2" label="&#x2022;">We propose efficient approximate algorithms for temporal cuts via spectral graph theory and divide-and-conquer.<br/></li>     <li id="list3" label="&#x2022;">We evaluate our methods extensively, applying them for community detection and signal processing on graphs.<br/></li>    </ul>    <figure id="fig1">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Temporal graph cut capturing major changes in the network interactions. Figure is better seen in color.</span>     </div>    </figure>    <p>     <strong>Related Work.</strong> Computing graph cuts is a traditional problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] with a diverse set of applications, ranging from image segmentation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>] to community detection [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. This paper is focused on the sparsest and normalized cut problems, which are of particular interest due to their connections with the spectrum of the Laplacian matrix, mixing time of random walks, geometric embeddings, effective resistance, and graph expanders [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>].</p>    <p>Community detection in temporal graphs has attracted great interest in recent years. An evolutionary spectral clustering technique was proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. The idea is to minimize a cost function <em>&#x03B1;</em>.<em>CS</em> + <em>&#x03B2;</em>.<em>CT</em>, where <em>CS</em> is a snapshot cost and <em>CT</em> is a temporal cost. FacetNet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] and estrangement [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] apply a similar approach under different clustering models. An important limitation of these solutions is that they perform community assignments in a step-wise manner, being highly subject to local optima. In incremental clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], the main goal is to avoid recomputation, and not to capture long-term structural changes. Multi-view clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>] combines different subsets of features (or views) from a given dataset but does not model how objects navigate across clusters over time. Different from spatio-temporal data clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>], we do not assume that our data is embedded in an Euclidean space.</p>    <p>A formulation for temporal modularity that simultaneously partitions snapshots using a multiplex graph [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] was proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]. A similar idea was applied in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>] to generalize eigenvector centrality. In this paper, we propose generalizations for temporal cut problems by studying spectral properties of multiplex graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>]. As one of our contributions, we exploit the link between multiplex graphs and block tridiagonal matrices to efficiently approximate temporal cuts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>]. While extending spectral graph theory to tensors seems to be a more natural approach to our problems, eigenvectors are well-studied only for symmetric tensors [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], which is not our case due to the time dimension.</p>    <p>Our definition of temporal cuts is a special case of non-uniform cuts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0049">49</a>]&#x2014;the second graph is a sequence of disconnected cliques to enforce cuts over time. Different from the general case, which requires more sophisticated (and computationally intensive) schemes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], a relaxation for temporal cuts can be computed as an eigenvector of a linear combination of two matrices (see Theorem <a class="enc" href="#enc5">1</a>).</p>    <p>Signal processing on graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] is an interesting application of temporal cuts. Traditional signal processing operations are also relevant when the signal is embedded into sparse irregular spaces. For instance, in machine learning, object similarity can be represented as a graph and labels as signals to solve semi-supervised learning tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. In this paper, we show how temporal cuts can be applied as bases for representing dynamic graph signals, even in the case where the graph structure also changes over time.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Temporal Graph Cuts</h2>     </div>    </header>    <p>This section introduces temporal cuts (Section <a class="sec" href="#sec-6">2.1</a>) and spectral algorithms for these problems (Section <a class="sec" href="#sec-9">2.2</a>). Faster algorithms, using divide-and-conquer and low-rank matrix approximation, are presented in Section <a class="sec" href="#sec-12">2.3</a>. We also discuss theoretical guarantees (Section <a class="sec" href="#sec-13">2.4</a>) and generalizations for temporal cuts (Section <a class="sec" href="#sec-14">2.5</a>).</p>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Definitions</h3>     </div>     </header>     <p>A temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span> is a sequence of snapshots &#x27E8;<em>G</em>     <sub>1</sub>, <em>G</em>     <sub>2</sub>, &#x2026;<em>G<sub>m</sub>     </em>&#x27E9; where <em>G<sub>t</sub>     </em> is the snapshot at timestamp <em>t</em>. <em>G<sub>t</sub>     </em> is a tuple (<em>V</em>, <em>E<sub>t</sub>     </em>, <em>W<sub>t</sub>     </em>) where <em>V</em> is a fixed set with <em>n</em> vertices, <em>E<sub>t</sub>     </em> is a dynamic set of undirected edges and <span class="inline-equation"><span class="tex">$W_t:E_t\rightarrow \mathbb {R}$</span>     </span> is an edge weighting function.</p>     <p>We model temporal graphs as <em>multiplex graphs</em>, which connect vertices from different graph layers. We denote as <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})=(\mathcal {V,E,W})$</span>     </span> the multiplex view of <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span>, where <span class="inline-equation"><span class="tex">$\mathcal {V}=\lbrace v_t|v\in V \wedge t \in [1,m]\rbrace$</span>     </span> (<span class="inline-equation"><span class="tex">$|\mathcal {V}|=nm$</span>     </span>) and <span class="inline-equation"><span class="tex">$\mathcal {E}=E_1 \cup \ldots E_t \cup \lbrace (v_t,v_{t+1})|v\in V \wedge t \in [1,m-1]\rbrace$</span>     </span>. Thus, <span class="inline-equation"><span class="tex">$\mathcal {E}$</span>     </span> also includes &#x2019;vertical&#x2019; edges between nodes <em>v<sub>t</sub>     </em> and <em>v</em>     <sub>      <em>t</em> + 1</sub>. The edge weighting function <span class="inline-equation"><span class="tex">$\mathcal {W}:\mathcal {E}\rightarrow \mathbb {R}$</span>     </span> is defined as: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathcal {W}(u_r,v_s)= {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}W_t(u,v), &#x0026; \text{if}\ (u,v) \in E_t \wedge r=t \wedge s=t\\ \beta , &#x0026; \text{if}\ u=v \wedge |r-s|=1\\ 0,&#x0026; \text{otherwise} \end{array}\right.} \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div>     </p>     <p>As a result, each vertex <em>v</em> &#x2208; <em>V</em> has <em>m</em> representatives {<em>v</em>     <sub>1</sub>, &#x2026;<em>v<sub>m</sub>     </em>} in <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>     </span>. Besides the intra-layer edges corresponding to the connectivity of each snapshot <em>E<sub>t</sub>     </em>, temporal edges (<em>v<sub>t</sub>     </em>, <em>v</em>     <sub>      <em>t</em> + 1</sub>) connect consecutive versions of a vertex <em>v</em> at different layers, which is known as <em>diagonal coupling</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. Intra-layer edge weights are the same as in <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span> while inter-layer weights are set to <em>&#x03B2;</em>.</p>     <section id="sec-7">     <p><em>2.1.1 Sparsest Cut.</em> A graph cut <span class="inline-equation"><span class="tex">$(X,\overline{X})$</span>      </span> divides <em>V</em> into two disjoint sets: <em>X</em>&#x2286;<em>V</em> and <span class="inline-equation"><span class="tex">$\overline{X}=V-X$</span>      </span>. We denote the weight of a cut <span class="inline-equation"><span class="tex">$ |(X,\overline{X})|=\sum _{u\in X, v\in \overline{X}}W(u,v)$</span>      </span>. The <em>cut sparsity &#x03C3;</em> is the ratio of the cut weight and the product of the sizes of the sets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>]: <div class="table-responsive" id="eq1">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \sigma (X) = \frac{|(X,\overline{X})|}{|X||\overline{X}|} \end{equation} </span>        <br/>        <span class="equation-number">(2)</span>       </div>      </div>     </p>     <p>Here, we extend the notion of cut sparsity to temporal graphs. A temporal cut <span class="inline-equation"><span class="tex">$\langle (X_1,\overline{X}_1), \ldots (X_m,\overline{X}_m)\rangle$</span>      </span> is a sequence of graph cuts where <span class="inline-equation"><span class="tex">$(X_t,\overline{X}_t)$</span>      </span> is a cut of the graph snapshot <em>G<sub>t</sub>      </em>. The idea is that in temporal graphs, besides the cut weights and partition sizes, we also care about the smoothness (i.e. stability) of the cuts over time. We formalize the temporal cut sparsity <em>&#x03C3;</em> as follows: <figure id="fig2">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig2.jpg" class="img-responsive" alt="Figure 2"        longdesc=""/>       <div class="figure-caption">        <span class="figure-number">Figure 2:</span>        <span class="figure-title">Two temporal cuts for the same graph and multiplex view of cut II. For <em>&#x03B2;</em> = 1, cut I has a sparsity of (2 + 3 + 0)/(4 &#x00D7; 4 + 4 &#x00D7; 4) = 0.16 while cut II has sparsity of (2 + 1 + 1)/(4 &#x00D7; 4 + 5 &#x00D7; 3) = 0.13. Thus, II is a sparser cut.</span>       </div>      </figure>      <div class="table-responsive" id="eq2">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \sigma (X_1, \ldots X_m;\beta) = \frac{\sum _{t=1}^m |(X_t,\overline{X}_t)|+\sum _{t=1}^{m-1}\Delta (X_t,\overline{X}_{t+1})}{\sum _{t=1}^m |X_t||\overline{X}_t|} \end{equation} </span>        <br/>        <span class="equation-number">(3)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$\Delta (X_t,\overline{X}_{t+1}) = \beta |(X_t,\overline{X}_{t+1})|$</span>      </span> is the number of vertices that move from <em>X<sub>t</sub>      </em> to <span class="inline-equation"><span class="tex">$\overline{X}_{t+1}$</span>      </span> (or <span class="inline-equation"><span class="tex">$|X_t \cap \overline{X}_{t+1}|$</span>      </span>) times the constant <em>&#x03B2;</em>, which allows different weights to be given to the cut smoothness.</p>     <p>Figure <a class="fig" href="#fig2">2</a> shows two alternative cuts for a temporal graph (<em>&#x03B2;</em> = 1). Cut I (Figure 2a) is smooth, since no vertex changes partitions, and it has weight 5. Cut II (Figure 2b) is a sparser temporal cut, with weight 3 and only one vertex changing partitions. Notice that cut I becomes sparser than cut II if <em>&#x03B2;</em> is set to 2 instead of 1. We formalize the sparsest cut problem in temporal graphs as follows.</p>     <div class="definition" id="enc1">      <Label><strong>Definition 1.</strong></Label>      <p>       <strong>Sparsest temporal cut</strong>. The sparsest cut of a temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>       </span>, for a constant <em>&#x03B2;</em>, is defined as: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \smash{\arg \min }_{X_1 \ldots X_m}\sigma (X_1,\ldots X_m; \beta) \nonumber\end{equation*} </span>        <br/>        </div>       </div>      </p>     </div>     <p>The sparsest temporal cut is a generalization of the sparsest cut problem and thus also NP-hard [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0018">18</a>].</p>     <p>An interesting property of the multiplex model is that temporal cuts in <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>      </span> become standard&#x2014;single graph&#x2014;cuts in the multiplex view <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>      </span>. We can evaluate the sparsity of a cut in <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>      </span> by applying the original formulation (Expression <a class="eqn" href="#eq1">2</a>) to <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>      </span>, since both edges cut and partition changes in <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>      </span> become edges cut in <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>      </span>. As an example, we show the multiplex view of cut II (Figure 2b) in Figure 2c . However, notice that not every standard cut in <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>      </span> is a valid temporal cut. For instance, cutting all the temporal edges (i.e. separating the two snapshots in our example) would be allowed in the standard formulation, but would lead to an undefined value of sparsity as the denominator in Expression <a class="eqn" href="#eq2">3</a> will be 0. Therefore, we cannot directly apply existing sparsest cut algorithms to <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>      </span> and expect to achieve a sparse temporal cut for <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>      </span>.</p>     <p>The connection between temporal cuts and multiplex networks is one of the main motivations for our formulation. Moreover, Equation <a class="eqn" href="#eq2">3</a> is general enough to capture different dynamic behaviors depending on the constant <em>&#x03B2;</em>. More specifically, if <em>&#x03B2;</em> &#x2192; &#x221E;, the sparsity <em>&#x03C3;</em> will be minimized for a constant cut over the snapshots. On the other hand, if <em>&#x03B2;</em> &#x2192; 0, <em>&#x03C3;</em> approximates the sparsity of single snapshot cuts. Interestingly, these two extreme regimes have been studied in the context of random-walks on dynamic graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>].</p>     </section>     <section id="sec-8">     <p><em>2.1.2 Normalized Cut.</em> A limitation of Equation <a class="eqn" href="#eq1">2</a>, is that it favors sparsity over partition size balance. In community detection, this often leads to <em>&#x201C;whisker communities&#x201D;</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0032">32</a>]. Normalized cuts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>] take into account the <em>volume</em> (i.e. sum of the degrees of vertices) of the partitions, being less prone to this effect. The normalized version of the cut sparsity is defined as: <div class="table-responsive" id="eq3">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \phi (X) = \frac{|(X,\overline{X})|}{vol(X).vol(\overline{X})} \end{equation} </span>        <br/>        <span class="equation-number">(4)</span>       </div>      </div> where <em>vol</em>(<em>Y</em>) = &#x2211;<sub>       <em>v</em> &#x2208; <em>Y</em>      </sub>      <em>deg</em>(<em>v</em>) and <em>deg</em>(<em>v</em>) is the degree of <em>v</em>.</p>     <p>We also generalize the normalized sparsity <em>&#x03D5;</em> to temporal graphs: <div class="table-responsive" id="eq4">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \phi (X_1, \ldots X_m;\beta) = \frac{\sum _{t=1}^m |(X_t,\overline{X}_t)|+\sum _{t=1}^{m-1}\Delta (X_t,\overline{X}_{t+1})}{\sum _{t=1}^m vol(\overline{X}_t)vol(X_t)} \end{equation} </span>        <br/>        <span class="equation-number">(5)</span>       </div>      </div>     </p>     <p>Next, we define the normalized cut problem for temporal graphs.</p>     <div class="definition" id="enc2">      <Label><strong>Definition 2.</strong></Label>      <p>       <strong>Normalized temporal cut</strong>. The normalized temporal cut of <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>       </span>, for a constant <em>&#x03B2;</em>, is defined as: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \smash{\arg \min }_{X_1 \ldots X_m}\phi (X_1, \ldots X_m;\beta) \nonumber\end{equation*} </span>        <br/>        </div>       </div>      </p>     </div>     <p>Computing optimal normalized temporal cuts is also NP-hard. The next section introduces spectral approaches for temporal cuts.</p>     </section>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Spectral Approaches</h3>     </div>     </header>     <p>Similar to the single graph case, we also exploit spectral graph theory in order to compute good temporal graph cuts. Let <em>A</em> be an <em>n</em> &#x00D7; <em>n</em> weighted adjacency matrix of a graph <em>G</em>(<em>V</em>, <em>E</em>), where <em>A</em>     <sub>      <em>u</em>, <em>v</em>     </sub> = <em>W</em>(<em>u</em>, <em>v</em>) if (<em>u</em>, <em>v</em>) &#x2208; <em>E</em> or 0, otherwise. The degree matrix <em>D</em> is a an <em>n</em> &#x00D7; <em>n</em> diagonal matrix, with <em>D</em>     <sub>      <em>v</em>, <em>v</em>     </sub> = <em>deg</em>(<em>v</em>) and <em>D</em>     <sub>      <em>u</em>, <em>v</em>     </sub> = 0 for <em>u</em> &#x2260; <em>v</em>. The Laplacian matrix of <em>G</em> is defined as <em>L</em> = <em>D</em> &#x2212; <em>A</em>. Let <em>L<sub>t</sub>     </em> be the Laplacian matrix of the graph snapshot <em>G<sub>t</sub>     </em> and <em>I<sub>z</sub>     </em> be an <em>z</em> &#x00D7; <em>z</em> identity matrix. We define the Laplacian of the temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span> as the Laplacian of its multiplex view <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>     </span>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \mathcal {L} = {\left(\begin{array}{*10c}L_1+\beta I_{n}&#x0026; -\beta I_{n}&#x0026; 0 &#x0026; \ldots &#x0026; 0 \\ -\beta I_{n}&#x0026; L_2+2\beta I_{n}&#x0026; -\beta I_{n}&#x0026; \ldots &#x0026; 0\\ \vdots &#x0026; &#x0026; \ddots &#x0026; \ldots &#x0026; -\beta I_{n}\\ 0 &#x0026; 0 &#x0026; \ldots &#x0026; -\beta I_{n}&#x0026; L_m+\beta I_{n}\end{array}\right)} \nonumber\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>The matrix <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>     </span> can be also written in a more compact form using the Kronecker product as <strong>diag</strong>(<em>L</em>     <sub>1</sub>, &#x2026;<em>L<sub>m</sub>     </em>) + <em>&#x03B2;</em>(<em>L</em>     <sup>&#x2113;</sup>&#x2297;<em>I<sub>n</sub>     </em>), where <em>L</em>     <sup>&#x2113;</sup> is the Laplacian of a line graph. Similarly, we define the degree matrix <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span> of <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span> as <strong>diag</strong>(<em>D</em>     <sub>1</sub>, <em>D</em>     <sub>2</sub>&#x2026;<em>D<sub>m</sub>     </em>), where <em>D<sub>t</sub>     </em> is the degree matrix of <em>G<sub>t</sub>     </em>. Let <em>C</em> = <em>nI<sub>n</sub>     </em> &#x2212; <strong>1</strong>     <sub>      <em>n</em> &#x00D7; <em>n</em>     </sub> be the Laplacian of a clique with <em>n</em> vertices. We define another <em>nm</em> &#x00D7; <em>nm</em> Laplacian matrix <span class="inline-equation"><span class="tex">$\mathcal {C}=I_m\otimes C$</span>     </span>, which is the Laplacian of a graph composed of <em>m</em> isolated cliques. This matrix will be applied to enforce valid temporal cuts over the snapshots of <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span>. Further, we define a size-<em>nm</em> indicator vector <strong>x</strong> where each vertex <em>v</em> &#x2208; <em>V</em> is represented <em>m</em> times, one for each snapshot. The value <strong>x</strong>[<em>v<sub>t</sub>     </em>] = 1 if <em>v<sub>t</sub>     </em> &#x2208; <em>X<sub>t</sub>     </em> and <strong>x</strong>[<em>v<sub>t</sub>     </em>] = &#x2212;1 if <span class="inline-equation"><span class="tex">$v_t \in \overline{X}_t$</span>     </span>.</p>     <section id="sec-10">     <p><em>2.2.1 Sparsest Cut.</em> The next lemma shows how the matrices <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>      </span> can be applied to compute the sparsity of temporal cuts.</p>     <div class="lemma" id="enc3">      <Label><strong>Lemma 2.1.</strong></Label>      <p> The sparsity <em>&#x03C3;</em> of a temporal cut is equal to <span class="inline-equation"><span class="tex">$\frac{{\bf x}^{\intercal }\mathcal {L}{\bf x}}{{\bf x}^{\intercal }\mathcal {C}{\bf x}}$</span>       </span>.</p>     </div>     <div class="proof" id="proof1">      <Label><strong>Proof.</strong></Label>      <p> Since <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>       </span> is the Laplacian of <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>       </span>: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{split} {\bf x}^{\intercal }\mathcal {L}{\bf x} &#x0026;= \sum _{(u,v) \in \mathcal {E}}({\bf x}[u]-{\bf x}[v])^2W(u,v) \\ &#x0026; = 4\sum _{t=1}^m |(X_t,\overline{X}_t)|+4 \sum _{t=1}^{m-1}\Delta (X_t,\overline{X}_{t+1})\\ \end{split} \nonumber\end{equation*} </span>        <br/>        </div>       </div>      </p>      <p>Regarding the denominator: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{split} {\bf x}^{\intercal }\mathcal {C}{\bf x} &#x0026;= \sum _{t=1}^m \sum _{u \ne v} ({\bf x}[v_t]-{\bf x}[u_t])^2\\ &#x0026; = 4\sum _{t=1}^m |X_t||\overline{X}_t| \end{split} \nonumber\end{equation*} </span>        <br/>        </div>       </div>      </p>     </div>     <p>Based on Lemma <a class="enc" href="#enc3">2.1</a>, we can obtain a relaxed solution, <strong>x</strong> &#x2208; [ &#x2212; 1, 1]<sup>       <em>nm</em>      </sup>, for the sparsest temporal cut problem (Definition <a class="enc" href="#enc1">1</a>) in <em>O</em>(<em>n</em>      <sup>3</sup>      <em>m</em>      <sup>3</sup>) time via generalized eigenvalue computation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>], or an approximate solution in <em>O</em>(<em>n</em>      <sup>2</sup>      <em>m</em>log&#x2009;<sup>2</sup>(<em>n</em>      <sup>2</sup>      <em>m</em>)) time using a fast Laplacian linear system solver [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0045">45</a>]. Solutions are later rounded to adhere to the original (discrete) constraint <strong>x</strong> &#x2208; { &#x2212; 1, 1}<sup>       <em>nm</em>      </sup>. The next lemma supports a more efficient solution for our relaxation.</p>     <div class="lemma" id="enc4">      <Label><strong>Lemma 2.2.</strong></Label>      <p> The matrices <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>       </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>       </span> commute.</p>     </div>     <div class="proof" id="proof2">      <Label><strong>Proof.</strong></Label>      <p> First, we show that <em>C</em> commutes with any Laplacian <em>L<sub>t</sub>       </em>: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} CL_t=(nI_{n}-{\bf 1}_{n\times n})L_t=nL_t=L_tC\\\end{equation*} </span>        <br/>        </div>       </div> Using the Kronecker product notation (see Section <a class="sec" href="#sec-9">2.2</a>): <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{split} \mathcal {C}\mathcal {L}&#x0026;=(I_{m}\otimes C)[{\bf diag}(L_1, \ldots L_m)-\beta (L^{\ell }\otimes I_{n})]\\ &#x0026;={\bf diag}(CL_1, \ldots CL_m)-\beta (I_{m}L^{\ell })\otimes (CI_{n})\\ &#x0026;={\bf diag}(L_1C, \ldots L_mC)-\beta (L^{\ell }I_{m})\otimes (I_{n}C)\\ &#x0026;=\mathcal {L}\mathcal {C}\\ \end{split}\end{equation*} </span>        <br/>        </div>       </div>      </p>     </div>     <p>A relaxation of the sparsest temporal cut can be computed based on a linear combination of matrices <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>      </span>.</p>     <div class="theorem" id="enc5">      <Label><strong>Theorem 1.</strong></Label>      <p> A relaxed solution for the sparsest temporal cut problem can, alternatively, be computed as: <div class="table-responsive" id="eq5">        <div class="display-equation">        <span class="tex mytex">\begin{equation} {\bf y}* = \mathop{arg\,max}_{{\bf y} \in [-1,1]^{nm}} \frac{{\bf y}^{\intercal }[(3(n+2\beta)\mathcal {C}-\mathcal {L}]{\bf y}}{{\bf y}^{\intercal }{\bf y}} \end{equation} </span>        <br/>        <span class="equation-number">(6)</span>        </div>       </div> which is the largest eigenvector of <span class="inline-equation"><span class="tex">$3(n+2\beta)\mathcal {C}-\mathcal {L}$</span>       </span>.</p>     </div>     <div class="proof" id="proof3">      <Label><strong>Proof.</strong></Label>      <p>The spectrum of <em>C</em> is in the form (<strong>e</strong>       <sub>1</sub>, <em>&#x03BB;</em>       <sub>1</sub>) = (<strong>1</strong>       <sub>        <em>n</em>       </sub>, 0) and <em>&#x03BB;</em>       <sub>2</sub> = &#x2026; = <em>&#x03BB;<sub>n</sub>       </em> = <em>n</em> for any vector <strong>e</strong>       <sub>        <em>i</em>       </sub>&#x22A5;<strong>1</strong>       <sub>        <em>n</em>       </sub>. As a consequence, the spectrum of <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>       </span> is in the form <em>&#x03BB;</em>       <sub>1</sub> = &#x2026; = <em>&#x03BB;<sub>m</sub>       </em> = 0 and <em>&#x03BB;</em>       <sub>        <em>m</em> + 1</sub> = &#x2026;<em>&#x03BB;<sub>nm</sub>       </em> = <em>n</em> for any <strong>e</strong>       <sub>        <em>i</em>       </sub>&#x22A5;<strong>span</strong>{<em>e</em>       <sub>1</sub>, &#x2026;<em>e<sub>m</sub>       </em>}. Lemma <a class="enc" href="#enc4">2.2</a> implies that any linear combination of <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>       </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>       </span> has the same eigenvectors as <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>       </span> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0020">20</a>]. Upper bounding the eigenvalues of a Laplacian matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0001">1</a>]: <div class="table-responsive" id="eq6">        <div class="display-equation">        <span class="tex mytex">\begin{equation} 0 \le \frac{{\bf y}^{\intercal }\mathcal {L}{\bf y}}{{\bf y}^{\intercal }{\bf y}} \le 2(n+2\beta) \end{equation} </span>        <br/>        <span class="equation-number">(7)</span>        </div>       </div>      </p>      <p>This implies that: <div class="table-responsive" id="eq7">        <div class="display-equation">        <span class="tex mytex">\begin{equation} \frac{({\bf y}*)^{\intercal }\mathcal {C}({\bf y}*)}{({\bf y}*)^{\intercal }({\bf y}*)} = n \end{equation} </span>        <br/>        <span class="equation-number">(8)</span>        </div>       </div>      </p>      <p>as it guarantees a strictly positive ratio (i.e. eigenvalue) in Equation <a class="eqn" href="#eq5">6</a>. Based on Equation <a class="eqn" href="#eq7">8</a>, we can re-write <strong>y</strong>* as: <div class="table-responsive" id="eq8">        <div class="display-equation">        <span class="tex mytex">\begin{equation} {\bf y}* = \mathop{arg\,min}_{{\bf y} \in [-1,1]^{nm}, {\bf y}^{\intercal }\mathcal {C}{\bf y} {\gt} 0} \frac{{\bf y}^{\intercal }\mathcal {L}{\bf x}}{{\bf y}^{\intercal }{\bf y}} \end{equation} </span>        <br/>        <span class="equation-number">(9)</span>        </div>       </div>      </p>      <p>Moreover, from Lemma <a class="enc" href="#enc3">2.1</a>: <div class="table-responsive" id="eq9">        <div class="display-equation">        <span class="tex mytex">\begin{equation} {\bf x}* = \mathop{arg\,min}_{{\bf x} \in [-1,1]^{nm}, {\bf x}^{\intercal }\mathcal {C}{\bf x} {\gt} 0} \frac{{\bf x}^{\intercal }\mathcal {L}{\bf x}}{{\bf x}^{\intercal }\mathcal {C}{\bf x}} \end{equation} </span>        <br/>        <span class="equation-number">(10)</span>        </div>       </div>      </p>      <p>Equations <a class="eqn" href="#eq8">9</a> and <a class="eqn" href="#eq9">10</a> are related to an <em>eigenvalue</em> and a <em>generalized eigenvalue problem</em>, respectively, and can be written as follows: <div class="table-responsive" id="Xeq2">        <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathcal {L}{\bf y} = \lambda {\bf y}, \qquad \mathcal {L}{\bf x} = \lambda ^{\prime } \mathcal {C}{\bf x} \end{equation} </span>        <br/>        <span class="equation-number">(11)</span>        </div>       </div> where <em>&#x03BB;</em> and <em>&#x03BB;</em>&#x2032; are minimized and <span class="inline-equation"><span class="tex">${\bf y}^{\intercal }\mathcal {C}{\bf y}, {\bf x}^{\intercal }\mathcal {C}{\bf x}{\gt} 0$</span>       </span>. From Equation <a class="eqn" href="#eq7">8</a>, we know that <span class="inline-equation"><span class="tex">$\mathcal {C}{\bf y}=n{\bf y}$</span>       </span>. Thus, <span class="inline-equation"><span class="tex">$\mathcal {L}{\bf y} = (\lambda /n)\mathcal {C}{\bf y}$</span>       </span> is a corresponding solution (same eigenvector) to the generalized problem.</p>     </div>     <p>The matrix <span class="inline-equation"><span class="tex">$3(n+2\beta)\mathcal {C}-\mathcal {L}$</span>      </span> is a Laplacian of a multiplex graph in which temporal edges have weight <em>w</em>&#x2032;(<em>v<sub>t</sub>      </em>, <em>v</em>      <sub>       <em>t</em> + 1</sub>) = &#x2212;<em>&#x03B2;</em> and intra-layer edges have weight <em>w</em>&#x2032;(<em>u</em>, <em>v</em>) = 3(<em>n</em> + 2<em>&#x03B2;</em>) &#x2212; <em>w</em>(<em>u</em>, <em>v</em>). This leads to a reordering of the spectrum of <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>      </span> where cuts containing only temporal edges have negative associated eigenvalues and sparse cuts for each Laplacian <em>L<sub>t</sub>      </em> become dense cuts for a new Laplacian [3(<em>n</em> + 2<em>&#x03B2;</em>))<em>C</em> &#x2212; <em>L<sub>t</sub>      </em>]. In terms of complexity, computing the difference <span class="inline-equation"><span class="tex">$3(n+2\beta)\mathcal {C}-\mathcal {L}$</span>      </span> takes <em>O</em>(<em>n</em>      <sup>2</sup>      <em>m</em>) time and the largest eigenvector of <span class="inline-equation"><span class="tex">$[3(n+2\beta)\mathcal {C}-\mathcal {L}]$</span>      </span> can be calculated in <em>O</em>(<em>n</em>      <sup>2</sup>      <em>m</em>). The resulting complexity is a significant improvement over the <em>O</em>(<em>n</em>      <sup>3</sup>      <em>m</em>      <sup>3</sup>) standard alternative, specially if the number of snapshots is large.</p>     </section>     <section id="sec-11">     <p><em>2.2.2 Normalized Cut.</em> We follow the steps of the previous section to compute normalized temporal cuts. <em>See the extended version of this paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0043">43</a>] for proofs of Lemma <a class="enc" href="#enc6">2.3</a> and Theorem <a class="enc" href="#enc7">2</a>      </em>.</p>     <div class="lemma" id="enc6">      <Label><strong>Lemma 2.3.</strong></Label>      <p> The normalized sparsity <em>&#x03D5;</em> of a temporal cut is equal to <span class="inline-equation"><span class="tex">$\frac{{\bf x}^{\intercal }\mathcal {L}{\bf x}}{{\bf x}^{\intercal }\mathcal {D}^{\frac{1}{2}}\mathcal {C}\mathcal {D}^{\frac{1}{2}}{\bf x}}$</span>       </span>.</p>     </div>     <p>We also define an equivalent of Theorem <a class="enc" href="#enc5">1</a> for normalized cuts.</p>     <div class="theorem" id="enc7">      <Label><strong>Theorem 2.</strong></Label>      <p> A relaxed solution for the sparsest temporal cut problem can, alternatively, be computed as: <div class="table-responsive" id="eq10">        <div class="display-equation">        <span class="tex mytex">\begin{equation} {\bf y}* = \mathop{arg\,max}_{{\bf y} \in [-1,1]^{nm}} \frac{{\bf y}^{\intercal }[(3(n+2\beta)\mathcal {C}-(\mathcal {D}^+)^{\frac{1}{2}}\mathcal {L}(\mathcal {D}^+)^{\frac{1}{2}}]{\bf y}}{{\bf y}^{\intercal }{\bf y}} \end{equation} </span>        <br/>        <span class="equation-number">(12)</span>        </div>       </div> the largest eigenvector of <span class="inline-equation"><span class="tex">$3(n+2\beta)\mathcal {C}-(\mathcal {D}^+)^{\frac{1}{2}}\mathcal {L}(\mathcal {D}^+)^{\frac{1}{2}}$</span>       </span>.</p>     </div>     <p>The interpretation of matrix <span class="inline-equation"><span class="tex">$3(n+2\beta)\mathcal {C}-(\mathcal {D}^+)^{\frac{1}{2}}\mathcal {L}(\mathcal {D}^+)^{\frac{1}{2}}$</span>      </span> is similar to the one for sparsest cuts, with temporal edges having negative weights. Moreover, the complexity of computing the largest eigenvector of such matrix is also <em>O</em>(<em>n</em>      <sup>2</sup>      <em>m</em>). This quadratic cost on the size of the graph, for both sparsest and normalized cut problems, becomes prohibitive even for reasonably small graphs. The next section is focused on faster algorithms for temporal graph cuts.</p>     </section>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Fast Approximations</h3>     </div>     </header>     <p>By definition, sparse temporal cuts are sparse in each snapshot and smooth across snapshots. Similarly, normalized temporal cuts are composed of a sequence of good normalized snapshot cuts that are stable over time. This motivates <em>divide-and-conquer</em> approaches for computing temporal cuts that first find a good cut on each snapshot (<em>divide</em>) and then combine them (<em>conquer</em>). These solutions have the potential to be much more efficient than the ones based on Theorems <a class="enc" href="#enc5">1</a> and <a class="enc" href="#enc7">2</a> if the conquer step is fast. However, they could lead to sub-optimal results, as optimal temporal cuts might not be composed of each snapshot&#x0027;s best cuts. Instead, better <em>divide-and-conquer</em> schemes can explore multiple snapshot cuts in the conquer step to avoid local optima. Since we are working in the spectral domain, it is natural to take eigenvectors of blocks of <span class="inline-equation"><span class="tex">$\mathcal {M}_S=3(n+2\beta)\mathcal {C}-\mathcal {L}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {M}_N=3(n+2\beta)\mathcal {C}-(\mathcal {D}^+)^{\frac{1}{2}}\mathcal {L}(\mathcal {D}^+)^{\frac{1}{2}}$</span>     </span>, as continuous notions of snapshot cuts. This strategy is supported by the well-known connections between higher-order eigenvectors of the Laplacian matrix and the sparsity of multiway cuts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>].</p>     <p>This section will describe our general <em>divide-and-conquer</em> approach. We will focus our discussion on the sparsest cut problem and then briefly show how it can be generalized to normalized cuts. The following theorem is the basis of our algorithm.</p>     <div class="theorem" id="enc8">     <Label><strong>Theorem 3.</strong></Label>     <p> The eigenvalues of the matrix <span class="inline-equation"><span class="tex">$\mathcal {M}_S=3(n+2\beta)\mathcal {C}-\mathcal {L}$</span>      </span> are the same as the ones for the matrix <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>      </span>: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \mathcal {Q} = \mathbf {\Lambda } - \beta {\left(\begin{array}{*10c}I_n &#x0026; -U_1^{\intercal }U_2 &#x0026; 0 &#x0026; \ldots &#x0026; 0 \\ -U_2^{\intercal }U_1&#x0026; 2I_n &#x0026; -U_2^{\intercal }U_3&#x0026; \ldots &#x0026; 0\\ \vdots &#x0026; &#x0026; \ddots &#x0026; \ldots &#x0026; U_{m-1}^{\intercal }U_m\\ 0 &#x0026; 0 &#x0026; \ldots &#x0026; U_m^{\intercal }U_{m-1}&#x0026; I_n \end{array}\right)}\nonumber\end{equation*} </span>        <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">$U_t\Lambda _t U_t^{\intercal }$</span>      </span> is the eigendecomposition of <em>M<sub>t</sub>      </em> = (3(<em>n</em> + 2<em>&#x03B2;</em>)<em>C</em> &#x2212; <em>L<sub>t</sub>      </em>) and <strong>       <em>&#x039B;</em>      </strong> = <strong>diag</strong>(<em>&#x039B;</em>      <sub>1</sub>&#x2026;<em>&#x039B;<sub>m</sub>      </em>). An eigenvector <strong>e</strong>      <sub>       <em>j</em>      </sub> of <span class="inline-equation"><span class="tex">$\mathcal {M}_S$</span>      </span> is computed as <span class="inline-equation"><span class="tex">$\mathcal {U}.{\bf e}_j^{\mathcal {Q}}$</span>      </span>, where <span class="inline-equation"><span class="tex">$\mathcal {U} = {\bf diag}(U_1 \ldots$</span>      </span>      <em>U<sub>m</sub>      </em>) and <span class="inline-equation"><span class="tex">${\bf e}_j^{\mathcal {Q}}$</span>      </span> is an eigenvector of <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>      </span>.</p>     </div>     <div class="proof" id="proof4">     <Label><strong>Proof.</strong></Label>     <p> We show that <span class="inline-equation"><span class="tex">$\mathcal {M}_S$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>      </span> are similar matrices under the change of basis <span class="inline-equation"><span class="tex">$\mathcal {U}^{\intercal }$</span>      </span> and thus <span class="inline-equation"><span class="tex">$\mathcal {M} = (\mathcal {U}^{\intercal })^{-1}\mathcal {Q}\mathcal {U}^{\intercal }$</span>      </span>. Let&#x0027;s define matrices <span class="inline-equation"><span class="tex">$\mathcal {B} = \beta (L^{\ell }\otimes I_n)$</span>      </span> and <em>M<sub>t</sub>      </em> = 3(<em>n</em> &#x2212; 2<em>&#x03B2;</em>)<em>C</em> &#x2212; <em>L<sub>t</sub>      </em>. Because <em>L<sub>t</sub>      </em> is symmetric, <span class="inline-equation"><span class="tex">$\mathcal {U}^{-1}=\mathcal {U}^{\intercal }$</span>      </span>. For an eigenvector matrix <span class="inline-equation"><span class="tex">$\mathcal {U}$</span>      </span>, <span class="inline-equation"><span class="tex">$\mathcal {U}\mathcal {U}^{\intercal }$</span>      </span> is an <em>nm</em> &#x00D7; <em>nm</em> identity matrix <span class="inline-equation"><span class="tex">$\mathcal {I}$</span>      </span>. We rewrite <span class="inline-equation"><span class="tex">$\mathcal {M}_S$</span>      </span> as: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{split} \mathcal {M}_S&#x0026;= {\bf diag}(M_1, M_2 \ldots M_m) - \mathcal {B}\\ &#x0026;=\mathcal {U}\mathbf {\Lambda }\mathcal {U}^{\intercal } - \mathcal {I}\mathcal {B}\mathcal {I}\\ &#x0026;=\mathcal {U}\mathbf {\Lambda }\mathcal {U}^{\intercal } - \mathcal {U}\mathcal {U}^{\intercal }\mathcal {B}\mathcal {U}\mathcal {U}^{\intercal }\\ &#x0026;=\mathcal {U}(\mathbf {\Lambda }- \mathcal {U}^{\intercal }\mathcal {B}\mathcal {U})\mathcal {U}^{\intercal }\\ &#x0026;=(\mathcal {U}^{\intercal })^{-1}\mathcal {Q}\mathcal {U}^{\intercal } \end{split} \nonumber\end{equation*} </span>        <br/>       </div>      </div>     </p>     </div>     <p>     <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span> has <em>O</em>(<em>n</em>     <sup>2</sup>     <em>m</em>) non-zeros, being asymptotically as sparse as <span class="inline-equation"><span class="tex">$\mathcal {M}_S$</span>     </span>. However, <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span> can be block-wise sparsified using low-rank approximations of the matrices <em>M<sub>t</sub>     </em>. Given a constant <em>r</em> &#x2264; <em>n</em>, we approximate each <em>M<sub>t</sub>     </em> as <span class="inline-equation"><span class="tex">$U_t\Lambda _t^{\prime }U_t^{\intercal }$</span>     </span>, where <span class="inline-equation"><span class="tex">$\Lambda _i^{\prime }$</span>     </span> contains only the top-<em>r</em> eigenvalues of <em>M<sub>t</sub>     </em>. The benefits of such a strategy are the following: (1) The cost of computing the eigendecomposition of <em>M<sub>t</sub>     </em> changes from <em>O</em>(<em>n</em>     <sup>3</sup>) to <em>O</em>(<em>rn</em>     <sup>2</sup>); (2) the cost of multiplying eigenvector matrices decreases from <em>O</em>(<em>n</em>     <sup>3</sup>) to <em>O</em>(<em>r</em>     <sup>3</sup>); and (3) the number of non-zeros in <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span> is reduced from <em>O</em>(<em>n</em>     <sup>2</sup>     <em>m</em>) to <em>O</em>(<em>r</em>     <sup>2</sup>     <em>m</em>). Similar to the case of general block tridiagonal matrices [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>], we can show that the error associated with such approximation is bounded by <span class="inline-equation"><span class="tex">$2\lambda _{r+1}^{max}$</span>     </span>, where <span class="inline-equation"><span class="tex">$\lambda _{r+1}^{max}$</span>     </span> is the largest (<em>r</em> + 1)-nth eigenvalue of the approximated matrices <em>M<sub>t</sub>     </em>.</p>     <p>We improve our approach by speeding-up the eigendecomposition of the matrices <em>M<sub>t</sub>     </em>. The idea is to operate over the original Laplacians <em>L<sub>t</sub>     </em>, which are expected to be sparse. The eigendecomposition of a matrix with |<em>E</em>| non-zeros can be performed in time <em>O</em>(<em>n</em>|<em>E</em>|) and for real-world graphs |<em>E</em>| < < <em>n</em>     <sup>2</sup>. The following Lemma shows how the spectrum of <em>M<sub>t</sub>     </em> can be computed based on <em>L<sub>t</sub>     </em>.</p>     <p>     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-img1.svg" class="img-responsive" alt="" longdesc=""/>     </p>     <div class="lemma" id="enc9">     <Label><strong>Lemma 2.4.</strong></Label>     <p> Let <span class="inline-equation"><span class="tex">$\lambda _1^L, \lambda _2^L \ldots \lambda _n^L$</span>      </span> be the eigenvalues of a Laplacian matrix <em>L</em> in increasing order with associated eigenvectors <span class="inline-equation"><span class="tex">$e_1^L, e_2^L \ldots e_n^L$</span>      </span>. The eigenvectors <span class="inline-equation"><span class="tex">$e_i^L$</span>      </span> are also eigenvectors of 3(<em>n</em> + 2<em>&#x03B2;</em>)<em>C</em> &#x2212; <em>L</em> with associated eigenvalues <em>&#x03BB;</em>      <sub>1</sub> = 0 and <em>&#x03BB;<sub>i</sub>      </em> = 3(<em>n</em> + 2<em>&#x03B2;</em>)<em>n</em> &#x2212; <em>&#x03BB;</em>      <sub>       <em>n</em> &#x2212; <em>i</em> + 1</sub> for <em>i</em> > 0.</p>     </div>     <div class="proof" id="proof5">     <Label><strong>Proof.</strong></Label>     <p> The spectrum of <em>C</em> is (<strong>e</strong>      <sub>1</sub>, <em>&#x03BB;</em>      <sub>1</sub>) = (<strong>1</strong>      <sub>       <em>n</em>      </sub>, 0) and <em>&#x03BB;</em>      <sub>2</sub> = &#x2026; = <em>&#x03BB;<sub>n</sub>      </em> = <em>n</em> for any vector <strong>e</strong>      <sub>       <em>i</em>      </sub>&#x22A5;<strong>1</strong>      <sub>       <em>n</em>      </sub>. As <em>M<sub>i</sub>      </em> is also a Laplacian, it follows that <em>&#x03BB;</em>      <sub>1</sub> = 0 and <span class="inline-equation"><span class="tex">$e_1=e_1^L$</span>      </span>. Also, by definition <span class="inline-equation"><span class="tex">$L.e_i^L=\lambda _i^L.e_i^L$</span>      </span>, and thus <span class="inline-equation"><span class="tex">$(3(n+2\beta)C-L)e_i^L=(3(n+2\beta)n-\lambda _i)e_i^L$</span>      </span> for <em>i</em> > 0.</p>     </div>     <p>Algorithm 1 describes our <em>divide-and-conquer</em> approach for approximating the sparsest temporal cut. Its inputs are the temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span>, the rank <em>r</em> that controls the accuracy of the algorithm, and a constant <em>&#x03B2;</em>. It returns a cut <span class="inline-equation"><span class="tex">$\langle (X_1,\overline{X}_1) \ldots$</span>     </span>      <span class="inline-equation"><span class="tex">$(X_m,\overline{X}_m)\rangle$</span>     </span> that (approximately) minimizes the sparsity ratio defined in Equation <a class="eqn" href="#eq2">3</a>. In the <em>divide</em> phase, the top-<em>r</em> eigenvalues/eigenvectors of each matrix <em>M<sub>t</sub>     </em>&#x2014;related to the bottom-<em>r</em> eigenvalues/eigenvectors of <em>L<sub>t</sub>     </em>&#x2014;are computed using Lemma <a class="enc" href="#enc9">2.4</a> (steps 1-4). The <em>conquer</em> phase (steps 5-9) consists of building the matrix <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span>&#x2014;based on the blocks <span class="inline-equation"><span class="tex">$\mathcal {Q}_{t,t+1}$</span>     </span> adjacent to the diagonal&#x2014;and then computing its largest eigenvector as a relaxed version of a temporal cut. The resulting eigenvector is discretized using a standard sweep algorithm (<strong>sweep</strong>) over the vertices sorted by their corresponding value of <strong>x*</strong>. The selection criteria for the sweep algorithm is the sparsity ratio (Equation <a class="eqn" href="#eq2">3</a>).</p>     <p>The time complexity of our algorithm is <span class="inline-equation"><span class="tex">$O(mr\sum _{t=1}^m |E_t|$</span>     </span> + <em>mr</em>     <sup>3</sup>). The <em>divide</em> step has cost <span class="inline-equation"><span class="tex">$O(mr\sum _{t=1}^m|E_t|)$</span>     </span>, which corresponds to the computation of <em>r</em> eigenvectors/eigenvalues of matrices <em>L<sub>t</sub>     </em> with <em>O</em>(|<em>E<sub>t</sub>     </em>|) non-zeros each. As snapshots are processed independently, this part of the algorithm can be easily parallelized. In the <em>conquer</em> step, the most time consuming operation is computing <em>m</em> &#x2212; 1 <em>r</em> &#x00D7; <em>r</em> matrix products in the construction of <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span>, which takes <em>O</em>(<em>r</em>     <sup>3</sup>     <em>m</em>) time. Our algorithm has space complexity of <em>O</em>(<em>r</em>     <sup>2</sup>     <em>m</em>) due to the number of non-zeros in the sparse representation of <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span>.</p>     <p>We follow the same general approach discussed in this section to efficiently compute normalized temporal cuts. As in Theorem <a class="enc" href="#enc8">3</a>, we can compute the eigenvectors of <span class="inline-equation"><span class="tex">$\mathcal {M}_N$</span>     </span> using divide-and-conquer. However, each block <em>M<sub>t</sub>     </em> will be in the form <span class="inline-equation"><span class="tex">$3(n-2\beta)C-(D_t^+)^{\frac{1}{2}}L_t(D_t^+)^{\frac{1}{2}}$</span>     </span>. Moreover, similar to Lemma <a class="enc" href="#enc9">2.4</a>, we can also compute the eigendecomposition of <em>M<sub>t</sub>     </em> based on <span class="inline-equation"><span class="tex">$(D_t^+)^{\frac{1}{2}}L_t(D_t^+)^{\frac{1}{2}}$</span>     </span>.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Approximation Guarantees</h3>     </div>     </header>     <p>The algorithms presented in Sections <a class="sec" href="#sec-9">2.2</a> and <a class="sec" href="#sec-12">2.3</a> are based on eigenvector computations that are relaxations of temporal cut problems. A natural question to ask is: <em>Do they provide any approximation guarantees with respect to the optimal solution for the problems?</em> Notice that, for the fast solutions discussed in the previous section, the number of top eigenvalues (<em>r</em>) considered by Algorithm 1 gives some control over the quality of the approximations. Therefore, we focus on bounding the error induced by the relaxations described in Lemmas <a class="enc" href="#enc3">2.1</a> and <a class="enc" href="#enc6">2.3</a> and by Theorems <a class="enc" href="#enc5">1</a> and <a class="enc" href="#enc7">2</a>.</p>     <p>Our analysis is heavily based on a recent generalization of the Cheeger inequality [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>], which relates the ratio between the weights of the same cut on two graphs, <em>G</em> and <em>H</em>, with the same set of vertices, and the generalized eigenvalue involving their Laplacian matrices <em>L<sub>G</sub>     </em> and <em>L<sub>H</sub>     </em>. This generalization is the main ingredient to proof the following Lemma regarding our spectral algorithm for the sparsest temporal cut problem (Definition <a class="enc" href="#enc1">1</a>):</p>     <div class="lemma" id="enc10">     <Label><strong>Lemma 2.5.</strong></Label>     <p> The temporal sparsity ratio <span class="inline-equation"><span class="tex">$\lambda (\mathcal {G})$</span>      </span> achieved by our relaxation is such that: <div class="table-responsive" id="Xeq3">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \lambda (\mathcal {G}) \ge \frac{\varphi (\chi (\mathcal {G}))\min (\sigma _{X_1, \ldots X_m}(X_1, \ldots X_m;\beta))}{8} \end{equation} </span>        <br/>        <span class="equation-number">(13)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$\lambda (\mathcal {G}) = \min _{{\bf x}^{\intercal }\mathcal {C}{\bf x} {\gt} 0} \frac{{\bf x}^{\intercal }\mathcal {L}{\bf x}}{{\bf x}^{\intercal }\mathcal {C}{\bf x}}$</span>      </span> and <span class="inline-equation"><span class="tex">$\varphi (\chi (\mathcal {G}))$</span>      </span> is the (standard) conductance of the multiplex view of <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>      </span>.</p>     </div>     <p>The lemma follows directly from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, Theorem 1], by setting <em>G</em> to our temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span> and <em>H</em> to the sequence of cliques with Laplacian <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, and thus the proof is omitted. Similar to the classical Cheeger inequality [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>], the proof of its generalized version is also constructive, and the rounding algorithm applied by our solutions achieves this bound. We can interpret Lemmma <a class="enc" href="#enc10">2.5</a> as follows. If the temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span> has a low-sparsity cut compared to the conductance of its multiplex view <span class="inline-equation"><span class="tex">$\chi (\mathcal {G})$</span>     </span>, then our relaxations will find a good approximate (possibly different) temporal cut. A similar bound also holds for normalized temporal cuts (Definition <a class="enc" href="#enc2">2</a>).</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.5</span> Generalizations</h3>     </div>     </header>     <p>Here, we briefly address several generalizations of temporal cuts that aim to increase the applicability of this work.</p>     <p>     <strong>Arbitrary swap costs:</strong> While we have assumed uniform swap costs <em>&#x03B2;</em>, generalizing our formulation to arbitrary (non-negative) swap costs for pairs (<em>v<sub>t</sub>     </em>, <em>v</em>     <sub>      <em>t</em> + 1</sub>) is straightforward.</p>     <p>     <strong>Multiple cuts:</strong> Multi-cuts can be computed based on the top eigenvectors of our temporal cut matrices, as proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>]. We use <em>k</em>-means to obtain a <em>k</em>-way partition. <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Sparsity ratios for sparsest (a-d) and normalized (e-h) cuts. <em>STC</em> achieves the smallest ratio in most of the settings. <em>FSTC</em> also achieves good results, specially for <em>r</em> = 64, being able to adapt to different swap costs.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Signal Processing on Graphs</h2>     </div>    </header>    <p>We apply graph cuts as data-driven wavelet bases for dynamic signals. Given a sequence of signals &#x27E8;<strong>f</strong>     <sub>(1)</sub>, &#x2026;<strong>f</strong>     <sub>(<em>m</em>)</sub>&#x27E9;, <span class="inline-equation"><span class="tex">${\bf f}_{(i)} \in \mathbb {R}^n$</span>     </span>, on a temporal graph <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>     </span>, our goal is to discover a temporal cut that is sparse, smooth, and separates vertices with dissimilar signal values. A previous work [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] has shown that a relaxation of the <em>L</em>     <sub>2</sub> energy (or importance) ||<em>a</em>||<sub>2</sub> of a wavelet coefficient <em>a</em> for a single graph snapshot with signal <strong>f</strong> can be computed as: <div class="table-responsive" id="eq11">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \frac{(|X|\sum _{v\in \overline{X}} {\bf f}[v]-|\overline{X}|\sum _{u\in X} {\bf f}[u])^2}{|X||\overline{X}|} \propto -\frac{{\bf x}^{\intercal }CSC{\bf x}}{{\bf x}^{\intercal }C{\bf x}} \end{equation} </span>      <br/>      <span class="equation-number">(14)</span>     </div>     </div> where <strong>x</strong> is an indicator vector and <em>S</em>     <sub>     <em>u</em>, <em>v</em>     </sub> = (<strong>f</strong>[<em>v</em>] &#x2212; <strong>f</strong>[<em>u</em>])<sup>2</sup>. Sparsity is enforced by adding a Laplacian regularization factor <em>&#x03B1;</em>     <strong>x</strong>     <sup>&#x22BA;</sup>     <em>L</em>     <strong>x</strong>, where <em>&#x03B1;</em> is a user-defined constant, to the denominator of Equation <a class="eqn" href="#eq11">14</a>. This formulation supports an algorithm for computing graph wavelets, which we extend to dynamic signals. Following the same approach as in Section <a class="sec" href="#sec-6">2.1</a>, we apply the multiplex graph representation to compute the energy of a dynamic wavelet coefficient: <div class="table-responsive" id="eq12">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \frac{\sum _{t=1}^m \Theta _t^2 + \sum _{t=1}^{m-1} \Theta _t\Theta _{t+1}}{\sum _{t=1}^m|X_t||\overline{X}_t|} \end{equation} </span>      <br/>      <span class="equation-number">(15)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$\Theta _t = |X_t|\sum _{v\in \overline{X}_t} {\bf f}[v]-|\overline{X}_t|\sum _{u\in X_t} {\bf f}[u]$</span>     </span>. The first term in the numerator of Equation <a class="eqn" href="#eq12">15</a> is the sum of the numerator of Equation <a class="eqn" href="#eq11">14</a> over all snapshots. The second term acts on sequential snapshots and enforces the partitions to be consistent over time &#x2014;i.e. <em>X<sub>t</sub>     </em>&#x2019;s to be jointly associated with either large or small values. Intuitively, the energy is maximized for partitions that separate different values and are also balanced in size. The next theorem provides a spectral formulation for the energy of dynamic wavelets.</p>    <p>     <div class="theorem" id="enc11">     <Label><strong>Theorem 4.</strong></Label>     <p> The energy of a dynamic wavelet is proportional to <span class="inline-equation"><span class="tex">$-\frac{{\bf x}^{\intercal }\mathcal {C}\mathcal {S}\mathcal {C}{\bf x}}{{\bf x}^{\intercal }\mathcal {C}{\bf x}}$</span>      </span>, where <span class="inline-equation"><span class="tex">$\mathcal {S}_{u,v} = ({\bf f}[u]-{\bf f}[v])^2$</span>      </span> for values within one snapshot from each other.</p>     </div>    </p>    <p>We apply Theorem <a class="enc" href="#enc11">4</a> (see proof in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>]) to compute a relaxation of the optimal dynamic wavelet as a regularized eigenvalue problem: <div class="table-responsive" id="eq13">     <div class="display-equation">      <span class="tex mytex">\begin{equation} {\bf x}* = \smash{\mathop{arg\,min}_{{\bf x} \in [-1,1]^{nm}}} \frac{{\bf x}^{\intercal }\mathcal {CSC}{\bf x}}{{\bf x}^{\intercal }\mathcal {C}{\bf x} + \alpha {\bf x}^{\intercal }\mathcal {L}{\bf x}} \end{equation} </span>      <br/>      <span class="equation-number">(16)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>     </span> are matrices defined in Section <a class="sec" href="#sec-6">2.1</a>. Optimizations discussed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] can be applied to efficiently approximate Equation <a class="eqn" href="#eq13">16</a>. The resulting algorithm has complexity <em>O</em>(<em>pn</em>&#x2211;<sub>     <em>t</em>     </sub>|<em>E<sub>t</sub>     </em>| + <em>qn</em>     <sup>2</sup>     <em>m</em>     <sup>2</sup>), where <em>p</em> and <em>q</em> are small constants. Similar to Algorithm 1, we apply a sweep procedure to obtain a cut from <strong>x</strong>*.</p>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Eigenvector Computation</h3>     </div>     </header>     <p>We applied the Lanczos method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>] to implement the algorithms discussed in this paper<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. However, our algorithms can also be implemented using other eigensolvers from the literature.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Datasets</h3>     </div>     </header>     <p>     <em>School</em> is a contact network where vertices represent children from a primary school and edges are created based on proximity detected by sensors [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>], with 242 vertices, 17<em>K</em> edges and 3 snapshots. <em>Stock</em> is a correlation network of US stocks&#x2019; end of the day prices<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> with 500 vertices, 27<em>K</em> edges, and 26 snapshots (one for each year in the interval 1989-2015). <em>DBLP</em> is a sample from the DBLP collaboration network. Vertices corresponding to two authors are connected in a given snapshot if they co-authored a paper in the corresponding year. We selected authors who published at least 5 papers one of the following conferences: KDD, CVPR, and FOCS. The resulting temporal network has 3.4<em>K</em> vertices, 16.4<em>K</em> edges, and 4 snapshots.</p>     <p>We also use a synthetic data generator. Its parameters are a graph size <em>n</em>, partition size <em>k</em> < <em>n</em>, number of hops <em>h</em>, and noise level 0 &#x2264; &#x03F5; &#x2264; 1. Edges are created based on a <span class="inline-equation"><span class="tex">$\lceil \sqrt {n}\rceil \times \lceil \sqrt {n}\rceil$</span>     </span> grid, where each vertex is connected to its <em>h</em>-hop neighbors. A partition is a sub-grid initialized with <span class="inline-equation"><span class="tex">$\lceil \sqrt {k}\rceil \times \lceil \sqrt {k}\rceil$</span>     </span> dimensions (<em>k</em> = <em>n</em>/2). A value <em>&#x03C0;</em>(<em>v</em>) = 1. + <em>N</em>(0, &#x03F5;) is assigned to vertices inside the partition and the remaining vertices receive iid realizations of a Gaussian <em>N</em>(0, &#x03F5;). Given the node values, the weight <em>w</em> of an edge (<em>u</em>, <em>v</em>) is set as exp&#x2009;(|<em>&#x03C0;</em>(<em>v</em>) &#x2212; <em>&#x03C0;</em>(<em>u</em>)|). To produce the dynamics, we move the partition along the main diagonal of the grid.</p>     <p>To evaluate our wavelets for dynamic signals, we apply our approach to <em>Traffic</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>], a road network from California with 100 vertices, 200 edges, and 12 snapshots. Average vehicle speeds measured at the vertices were taken as a dynamic signal for the timespan of a Friday in April, 2011. Moreover, we apply the <em>heat equation</em> to generate synthetic signals over the <em>School</em> network. Different from <em>Traffic</em>, which has a static structure, the resulting dataset (<em>School-heat</em>) is dynamic in structure and signal (see details in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>]).</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Approximation and Performance</h3>     </div>     </header>     <p>Two general approaches for computing temporal cuts, for both sparsest and normalized cuts, are evaluated in this section. The first approach, <em>STC</em>, combines Theorems <a class="enc" href="#enc5">1</a> and <a class="enc" href="#enc7">2</a>, for sparsest and normalized cuts, respectively, and the same rounding scheme applied by Algorithm 1 . The second approach, <em>FSTC-r</em>, for a rank <em>r</em>, applies the fast approximation described in Section <a class="sec" href="#sec-12">2.3</a>.</p>     <p>We consider three baselines in this evaluation. <em>SINGLE</em> discovers the best cut on each snapshot and then binds them into one temporal cut. <em>UNION</em> computes the best average cut over all the snapshots. <em>LAP</em> is similar to our approach, but operates directly on the Laplacian matrix <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>     </span>. Notice that each of these baselines can be applied to either sparsest and normalized cuts as long as the appropriate (standard or normalized) Laplacian matrix is used. Each experiment was repeated 10 times and we report the average results.</p>     <p>Figure <a class="fig" href="#fig3">3</a> shows quality results (sparsity ratios) of the methods. We vary the swap cost (<em>&#x03B2;</em>) within a range that enforces local and global (or stable) patterns. The values of <em>&#x03B2;</em> shown are normalized to integers for ease of comparison. <em>STC</em> and <em>LAP</em> took too long to finish for the <em>Stock</em> and <em>DBLP</em> datasets, and thus their results are omitted. Our approach, <em>STC</em>, achieves the best results (smallest ratios) in most of the settings. For the <em>School</em> dataset, <em>LAP</em> also achieves good results, which is due to the small number of snapshots in the graph. As expected, <em>UNION</em> performs well for large swap costs, while <em>SINGLE</em> achieves good results when swap costs are close to 0. Our fast approximation (<em>FSTC</em>) is able to identify low-sparsity cuts in most of the settings, outperforming <em>SINGLE</em> and <em>UNION</em>. Notice that even though a larger value of <em>r</em> generates a better approximation for the temporal matrix, as discussed in Section <a class="sec" href="#sec-12">2.3</a>, the quality of the temporal cut is not guaranteed to increase monotonically with the value of <em>r</em> (see Figure 3g). However, a larger <em>r</em> often leads to a better approximation (i.e. lower sparsity ratio).</p>     <p>Figure <a class="fig" href="#fig4">4</a> shows the performance results (running time) using synthetic data for sparsest (Figure 4a -4d) and normalized (Figures 4e -4h) cuts. We vary the number of vertices, density, number of snapshots, and also the rank of FSTC. Similar conclusions can be drawn for both problems. <em>UNION</em> is the fastest method, as it operates over an <em>n</em> &#x00D7; <em>n</em> matrix. <em>STC</em> and <em>LAP</em>, which process <em>nm</em> &#x00D7; <em>nm</em> matrices, are the most time consuming methods. <em>STC</em> is even slower than <em>LAP</em>, due to its denser matrix. <em>SINGLE</em> and <em>FSTC</em> achieve similar performance, with running times close to <em>UNION</em>&#x2019;s. Figures 4d and 4h illustrate how the rank <em>r</em> of the matrix approximation performed by <em>FSTC</em> enables significant performance gains compared to <em>STC</em>. <figure id="fig4">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Running time results for sparsest (a-d) and normalized (e-h) cuts on synthetic data.</span>      </div>     </figure>     <figure id="fig5">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Dynamic communities discovered using <em>sparsest cuts</em> for the <em>DBLP</em> dataset (4 snapshots). <em>Better seen in color</em>.</span>      </div>     </figure>     </p>    </section>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Community Detection</h3>     </div>     </header>     <p>Dynamic community detection is an interesting application for temporal cuts. Two approaches from the literature, <em>FacetNet</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] and <em>GenLovain</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>], are used as the baselines. We focus our evaluation on <em>School</em> and <em>DBLP</em>, which have most meaningful communities. The following metrics are considered for comparison:</p>     <p>     <strong>Cut:</strong> Total weight of the edges across partitions computed as <span class="inline-equation"><span class="tex">$\sum _{t=1}^{m}\sum _{v,u \in G_t} w(u,v)(1-\delta (c_v,c_u))$</span>     </span>, where <em>c<sub>v</sub>     </em> and <em>c<sub>u</sub>     </em> are the partitions to which <em>v</em> and <em>u</em> are assigned, respectively.</p>     <p>     <strong>Sparsity:</strong> Sparsity ratio (Equation <a class="eqn" href="#eq2">3</a>) for <em>k</em>-cuts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>]: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \sum _{i=1}^k\frac{\sum _{t=1}^m |(X_{i,t},\overline{X}_{i,t}|+\beta \sum _{t=1}^{m-1}|(X_{i,t},\overline{X}_{i,t+1})|}{\sum _{t=1}^m |X_{i,t}||\overline{X}_{i,t}|}\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>     <strong>N-sparsity:</strong> Normalized <em>k</em>-cut ratio (similar to sparsity).</p>     <p>     <strong>Modularity:</strong> Temporal modularity, as defined in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>].</p>     <p>Baseline parameters were varied within a range of values and the best results were chosen. For <em>GenLovain</em>, we fixed the number of partitions by agglomerating pairs that maximize modularity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] and for <em>FacetNet</em>, we assign each vertex to its highest weight partition.</p>     <p>Community detection results, for 2 and 5 communities, are shown in Table . For <em>School</em>, both <em>GenLovain</em> and our methods found the same communities (<em>&#x03B2;</em> = 0.25) when <em>k</em> = 2, outperforming <em>FacetNet</em> in all the metrics. However, for <em>k</em> = 5, different communities were discovered by the methods, with <em>Sparsest</em> and <em>Normalized Cuts</em> achieving the best results in terms of sparsity and n-sparsity, respectively. Our methods also achieve competitive results in terms of modularity. Similar results were found using <em>DBLP</em> (<em>&#x03B2;</em> = 0.5), although <em>Sparsest</em> and <em>Normalized Cuts</em> switch as the best method for each other&#x0027;s metric in some settings. This is possible because our algorithms are approximations (i.e. not optimal). We illustrate the communities found in the <em>School</em> (<em>k</em> = 2) and <em>DBLP</em> (<em>k</em> = 5) datasets in Figures <a class="fig" href="#fig1">1</a> and <a class="fig" href="#fig5">5</a>, respectively.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Community detection results for <em>Sparsest and Normalized Cuts</em> (and two baselines) using <em>School</em> and <em>DBLP</em> datasets. Our methods achieve the best results for most of the metrics and are competitive in terms of modularity.</span>     </div>     <table class="table">      <thead>       <tr>        <th colspan="6" style="text-align:center;">School<hr/>        </th>       </tr>       <tr>        <th style="text-align:left;">k</th>        <th style="text-align:left;">Method</th>        <th style="text-align:left;">        <em>Cut</em>        </th>        <th style="text-align:left;">        <em>Sparsity</em>        </th>        <th style="text-align:left;">        <em>N-sparsity</em>        </th>        <th style="text-align:left;">        <em>Modularity</em>        </th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>GenLovain</em>        </td>        <td style="text-align:left;">        <strong>2.6</strong>        </td>        <td style="text-align:left;">        <strong>1.0e-4</strong>        </td>        <td style="text-align:left;">        <strong>5.0e-3</strong>        </td>        <td style="text-align:left;">        <strong>102.0</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Facetnet</em>        </td>        <td style="text-align:left;">6.0</td>        <td style="text-align:left;">3.8e-4</td>        <td style="text-align:left;">.012</td>        <td style="text-align:left;">95.7</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Sparsest</em>        </td>        <td style="text-align:left;">        <strong>2.6</strong>        </td>        <td style="text-align:left;">        <strong>1.0e-4</strong>        </td>        <td style="text-align:left;">        <strong>5.0e-3</strong>        </td>        <td style="text-align:left;">        <strong>102.0</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Norm.</em>        </td>        <td style="text-align:left;">        <strong>2.6</strong>        </td>        <td style="text-align:left;">        <strong>1.0e-4</strong>        </td>        <td style="text-align:left;">        <strong>5.0e-3</strong>        </td>        <td style="text-align:left;">        <strong>102.0</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>GenLovain</em>        </td>        <td style="text-align:left;">8.0</td>        <td style="text-align:left;">6.8e-4</td>        <td style="text-align:left;">2.7e-2</td>        <td style="text-align:left;">        <strong>110.0</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Facetnet</em>        </td>        <td style="text-align:left;">10.0</td>        <td style="text-align:left;">8.4e-4</td>        <td style="text-align:left;">3.0e-2</td>        <td style="text-align:left;">106.0</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Sparsest</em>        </td>        <td style="text-align:left;">8.3</td>        <td style="text-align:left;">        <strong>6.4e-4</strong>        </td>        <td style="text-align:left;">2.6e-2</td>        <td style="text-align:left;">109.0</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Norm.</em>        </td>        <td style="text-align:left;">        <strong>6.1</strong>        </td>        <td style="text-align:left;">9.9e-4</td>        <td style="text-align:left;">        <strong>1.8e-2</strong>        </td>        <td style="text-align:left;">        <strong>110.0</strong>        </td>       </tr>      </tbody>      <tbody>       <tr>        <td colspan="6" style="text-align:center;">DBLP<hr/>        </td>       </tr>       <tr>        <td style="text-align:left;">k</td>        <td style="text-align:left;">Method</td>        <td style="text-align:left;">        <em>Cut</em>        </td>        <td style="text-align:left;">        <em>Sparsity</em>        </td>        <td style="text-align:left;">        <em>N-sparsity</em>        </td>        <td style="text-align:left;">        <em>Modularity</em>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>GenLovain</em>        </td>        <td style="text-align:left;">80.</td>        <td style="text-align:left;">3.9e-4</td>        <td style="text-align:left;">1.3e-5</td>        <td style="text-align:left;">        <strong>38,612</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Facetnet</em>        </td>        <td style="text-align:left;">267.0</td>        <td style="text-align:left;">2.6e-3</td>        <td style="text-align:left;">8.9e-5</td>        <td style="text-align:left;">33,091</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Sparsest</em>        </td>        <td style="text-align:left;">        <strong>9.0</strong>        </td>        <td style="text-align:left;">        <strong>7.6e-5</strong>        </td>        <td style="text-align:left;">        <strong>3.6e-6</strong>        </td>        <td style="text-align:left;">38,450</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Norm.</em>        </td>        <td style="text-align:left;">19.0</td>        <td style="text-align:left;">1.2e-4</td>        <td style="text-align:left;">3.8e-6</td>        <td style="text-align:left;">38,516</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>GenLovain</em>        </td>        <td style="text-align:left;">174.</td>        <td style="text-align:left;">1.3e-3</td>        <td style="text-align:left;">4.1e-5</td>        <td style="text-align:left;">        <strong>39,342</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Facetnet</em>        </td>        <td style="text-align:left;">501.0</td>        <td style="text-align:left;">7.2e-3</td>        <td style="text-align:left;">2.8e-4</td>        <td style="text-align:left;">30,116</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Sparsest</em>        </td>        <td style="text-align:left;">40.0</td>        <td style="text-align:left;">5.2e-4</td>        <td style="text-align:left;">6.2e-5</td>        <td style="text-align:left;">38,498</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>Norm.</em>        </td>        <td style="text-align:left;">        <strong>31.0</strong>        </td>        <td style="text-align:left;">        <strong>4.0e-4</strong>        </td>        <td style="text-align:left;">        <strong>1.0e-5</strong>        </td>        <td style="text-align:left;">39,015</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Signal Processing on Graphs</h3>     </div>     </header>     <p>We finish our evaluation with the analysis of dynamic signals on graphs. In Figure <a class="fig" href="#fig6">6</a>, we illustrate three dynamic wavelets for <em>Traffic</em> discovered using our approach under different settings. First, in Figures 6a -6d, we consider cuts that take only the graph signal into account by setting both the regularization parameter <em>&#x03B1;</em> and the smoothness parameter <em>&#x03B2;</em> to 0, which leads to a cut that follows the traffic speeds but has many edges and is not smooth. Next (Figures 6e -6h), we increase <em>&#x03B1;</em> to 200, producing a much sparser cut that is still not smooth. Finally, in Figures 6i -6l, we increase the smoothness <em>&#x03B2;</em> to 10, which forces most of the vertices to remain in the same partition despite of speed variations.</p>     <p>We also evaluate our approach in signal compression, which consists of computing a compact representation for a dynamic signal. As a baseline, we consider the <em>Graph Fourier</em> scheme [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] applied to the temporal graph (i.e. the multiplex view of the graph). The size of the representation (<em>k</em>) is the number of partitions and the number of top eigenvectors for our method and <em>Graph Fourier</em>, respectively. Figures 7a and 7b show the compression results in terms of <em>L</em>     <sub>2</sub> error using a fixed representation size <em>k</em> for the <em>Traffic</em> and <em>School-heat</em> datasets, respectively. We vary the value of the regularization parameter <em>&#x03B1;</em>, which controls the impact of the network structure over the wavelets computed, for our method. As expected, a larger value of <em>&#x03B1;</em> leads to a higher <em>L</em>     <sub>2</sub> error. However, even for a high regularization, our approach is still able to compute wavelets that accurately compress the signal, outperforming the baseline. <figure id="fig6">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Wavelet cut of a 4-snapshot dynamic traffic network with vehicle speeds as a signal. Vertex colors correspond to speed values (red for high and blue for low) and shapes indicate the partitions for 3 different settings: <em>&#x03B1;</em> = 0. and <em>&#x03B2;</em> = 1 (a-d, no network effect), <em>&#x03B1;</em> = 200. and <em>&#x03B2;</em> = 1. (e-h, large network effect with low smoothness), and <em>&#x03B1;</em> = 200. and <em>&#x03B2;</em> = 10. (i-l, large network effect and high smoothness) .</span>      </div>     </figure>     <figure id="fig7">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186118/images/www2018-127-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">        <em>L</em>        <sub>2</sub> error with different representation sizes <em>k</em> for Graph Fourier and our approach while setting the regularization parameter <em>&#x03B1;</em> to 200, 100, and 0.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusion</h2>     </div>    </header>    <p>This paper studied cut problems in temporal graphs. Extensions of two existing graph cut problems, sparsest and normalized cuts, by enforcing the smoothness of cuts over time, were introduced. To solve these problems, we have proposed spectral approaches based on multiplex graphs by computing relaxed temporal cuts as eigenvectors. Scalable versions of our solutions using divide-and-conquer and low-rank matrix approximation were also presented. In order to compute cuts that take into account also graph signals, we have extended graph wavelets to the dynamic setting. Experiments have shown that our temporal cut algorithms outperform the baseline methods in terms of quality and are competitive in running time. Moreover, temporal cuts enable the discovery of dynamic communities and the analysis of dynamic graph processes.</p>    <p>This work opens several lines for investigation: (i) temporal cuts can be applied to many scenarios other than the ones considered in this paper (e.g., computer vision); (ii) Perturbation Theory can support fast updates for temporal cuts in graph streams [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>]; finally, (iii) we want to investigate the relationship between cuts and random-walks on temporal graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>].</p>    <p>     <strong>Acknowledgment</strong>. Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-09-2-0053 (the ARL Network Science CTA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">William Anderson&#x00A0;Jr and Thomas Morley. 1985. Eigenvalues of the Laplacian of a graph. <em>      <em>Linear and multilinear algebra</em>     </em>18, 2 (1985), 141&#x2013;145.</li>     <li id="BibPLXBIB0002" label="[2]">Sanjeev Arora, Satish Rao, and Umesh Vazirani. 2009. Expander flows, geometric embeddings and graph partitioning. <em>      <em>J. ACM</em>     </em>56, 2 (2009), 5.</li>     <li id="BibPLXBIB0003" label="[3]">Lars Backstrom, Dan Huttenlocher, Jon Kleinberg, and Xiangyang Lan. 2006. Group formation in large social networks: membership, growth, and evolution. In <em>      <em>KDD</em>     </em>. ACM, New York, NY, USA, 44&#x2013;54.</li>     <li id="BibPLXBIB0004" label="[4]">Marya Bazzi, Mason&#x00A0;A Porter, Stacy Williams, Mark McDonald, Daniel&#x00A0;J Fenn, and Sam&#x00A0;D Howison. 2016. Community detection in temporal multilayer networks, with an application to correlation networks. <em>      <em>Multiscale Modeling &#x0026; Simulation</em>     </em>14, 1 (2016), 1&#x2013;41.</li>     <li id="BibPLXBIB0005" label="[5]">Steffen Bickel and Tobias Scheffer. 2004. Multi-view clustering.. In <em>      <em>ICDM</em>     </em>. IEEE, Washington, DC, USA, 19&#x2013;26.</li>     <li id="BibPLXBIB0006" label="[6]">Michael&#x00A0;M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. Geometric deep learning: going beyond euclidean data. <em>      <em>IEEE Signal Processing Magazine</em>     </em>34, 4 (2017), 18&#x2013;42.</li>     <li id="BibPLXBIB0007" label="[7]">Moses Charikar, Chandra Chekuri, Tom&#x00E1;s Feder, and Rajeev Motwani. 2004. Incremental clustering and dynamic information retrieval. <em>      <em>SIAM J. Comput.</em>     </em>33, 6 (2004), 1417&#x2013;1440.</li>     <li id="BibPLXBIB0008" label="[8]">Yun Chi, Xiaodan Song, Dengyong Zhou, Koji Hino, and Belle&#x00A0;L Tseng. 2007. Evolutionary spectral clustering by incorporating temporal smoothness. In <em>      <em>KDD</em>     </em>. ACM, New York, NY, USA, 153&#x2013;162.</li>     <li id="BibPLXBIB0009" label="[9]">Fan&#x00A0;RK Chung. 1997. <em>      <em>Spectral graph theory</em>     </em>. American Mathematical Society, Providence, RI, USA.</li>     <li id="BibPLXBIB0010" label="[10]">Mihai Cucuringu, Ioannis Koutis, Sanjay Chawla, Gary Miller, and Richard Peng. 2016. Simple and Scalable Constrained Clustering: a Generalized Spectral Method. In <em>      <em>AISTATS</em>     </em>. PMLR, Cadiz, Spain, 445&#x2013;454.</li>     <li id="BibPLXBIB0011" label="[11]">JJM Cuppen. 1980. A divide and conquer method for the symmetric tridiagonal eigenproblem. <em>      <em>Numer. Math.</em>     </em>36, 2 (1980), 177&#x2013;195.</li>     <li id="BibPLXBIB0012" label="[12]">Thomas Erlebach, Michael Hoffmann, and Frank Kammer. 2015. On temporal graph exploration. In <em>      <em>ICALP</em>     </em>. Springer, Berlin, Heidelberg, 444&#x2013;455.</li>     <li id="BibPLXBIB0013" label="[13]">Daniel Figueiredo, Philippe Nain, Bruno Ribeiro, Edmundo de&#x00A0;Souza&#x00A0;e Silva, and Don Towsley. 2012. Characterizing continuous time random walks on time varying graphs. In <em>      <em>SIGMETRICS</em>     </em>. ACM, New York, NY, USA, 307&#x2013;318.</li>     <li id="BibPLXBIB0014" label="[14]">Wilfried&#x00A0;N Gansterer, Robert&#x00A0;C Ward, Richard&#x00A0;P Muller, and William&#x00A0;A Goddard&#x00A0;III. 2003. Computing approximate eigenpairs of symmetric block tridiagonal matrices. <em>      <em>SIAM Journal on Scientific Computing</em>     </em>25, 1 (2003), 65&#x2013;85.</li>     <li id="BibPLXBIB0015" label="[15]">Matan Gavish, Boaz Nadler, and Ronald Coifman. 2010. Multiscale Wavelets on Trees, Graphs and High Dimensional Data: Theory and Applications to Semi Supervised Learning.. In <em>      <em>ICML</em>     </em>. Omnipress, USA, 367&#x2013;374.</li>     <li id="BibPLXBIB0016" label="[16]">Gene&#x00A0;H Golub and Charles&#x00A0;F Van&#x00A0;Loan. 2012. <em>      <em>Matrix computations</em>     </em>. Vol.&#x00A0;3. JHU Press, Baltimore, MD, USA.</li>     <li id="BibPLXBIB0017" label="[17]">Sergio Gomez, Albert Diaz-Guilera, Jesus Gomez-Gardenes, Conrad&#x00A0;J Perez-Vicente, Yamir Moreno, and Alex Arenas. 2013. Diffusion dynamics on multiplex networks. <em>      <em>Physical review letters</em>     </em>110, 2 (2013), 028701.</li>     <li id="BibPLXBIB0018" label="[18]">Lars Hagen and Andrew&#x00A0;B Kahng. 1992. New spectral methods for ratio cut partitioning and clustering. <em>      <em>IEEE Transactions on Computer-aided Design of Integrated Circuits and Systems</em>     </em>11, 9(1992), 1074&#x2013;1085.</li>     <li id="BibPLXBIB0019" label="[19]">Christopher&#x00A0;J Hillar and Lek-Heng Lim. 2013. Most tensor problems are NP-hard. <em>      <em>Journal of the ACM (JACM)</em>     </em>60, 6 (2013), 45.</li>     <li id="BibPLXBIB0020" label="[20]">Roger&#x00A0;A Horn and Charles&#x00A0;R Johnson. 1990. <em>      <em>Matrix analysis</em>     </em>. Cambridge University Press, New York, NY, USA.</li>     <li id="BibPLXBIB0021" label="[21]">Vikas Kawadia and Sameet Sreenivasan. 2012. Sequential detection of temporal communities by estrangement confinement. <em>      <em>Scientific reports</em>     </em>2(2012), 794&#x2013;794.</li>     <li id="BibPLXBIB0022" label="[22]">Mikko Kivel&#x00E4;, Alex Arenas, Marc Barthelemy, James&#x00A0;P Gleeson, Yamir Moreno, and Mason&#x00A0;A Porter. 2014. Multilayer networks. <em>      <em>Journal of complex networks</em>     </em>2, 3 (2014), 203&#x2013;271.</li>     <li id="BibPLXBIB0023" label="[23]">Ioannis Koutis, Gary Miller, and Richard Peng. 2014. A Generalized Cheeger Inequality. <a href="https://arxiv.org/abs/1412.6075" target="_blank">https://arxiv.org/abs/1412.6075</a>. (2014).</li>     <li id="BibPLXBIB0024" label="[24]">Ioannis Koutis, Gary&#x00A0;L. Miller, and Richard Peng. 2011. A Nearly-m Log N Time Solver for SDD Linear Systems. In <em>      <em>FOCS</em>     </em>. IEEE, Washington, DC, USA, 590&#x2013;598.</li>     <li id="BibPLXBIB0025" label="[25]">J Kuczy&#x0144;ski and H Wo&#x017A;niakowski. 1992. Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start. <em>      <em>SIAM journal on matrix analysis and applications</em>     </em>13, 4(1992), 1094&#x2013;1122.</li>     <li id="BibPLXBIB0026" label="[26]">Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and Andrew Tomkins. 2005. On the bursty evolution of blogspace. <em>      <em>World Wide Web</em>     </em>8, 2 (2005), 159&#x2013;178.</li>     <li id="BibPLXBIB0027" label="[27]">John Lafferty and Guy Lebanon. 2005. Diffusion Kernels on Statistical Manifolds. <em>      <em>JMLR</em>     </em>6(2005), 129&#x2013;163.</li>     <li id="BibPLXBIB0028" label="[28]">Kevin Lang. 2006. Fixing two weaknesses of the spectral method. In <em>      <em>NIPS</em>     </em>. MIT Press, Cambridge, MA, USA, 715&#x2013;722.</li>     <li id="BibPLXBIB0029" label="[29]">Amy&#x00A0;N Langville and Carl&#x00A0;D Meyer. 2011. <em>      <em>Google&#x0027;s PageRank and beyond: The science of search engine rankings</em>     </em>. Princeton University Press, Princeton, New Jersey.</li>     <li id="BibPLXBIB0030" label="[30]">James&#x00A0;R Lee, Shayan&#x00A0;Oveis Gharan, and Luca Trevisan. 2014. Multiway spectral partitioning and higher-order cheeger inequalities. <em>      <em>J. ACM</em>     </em>61, 6 (2014), 1&#x2013;30.</li>     <li id="BibPLXBIB0031" label="[31]">Tom Leighton and Satish Rao. 1988. An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms. In <em>      <em>FOCS</em>     </em>. IEEE, White Plains, NY, 422&#x2013;431.</li>     <li id="BibPLXBIB0032" label="[32]">Jure Leskovec, Kevin&#x00A0;J Lang, Anirban Dasgupta, and Michael&#x00A0;W Mahoney. 2009. Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. <em>      <em>Internet Mathematics</em>     </em>6, 1 (2009), 29&#x2013;123.</li>     <li id="BibPLXBIB0033" label="[33]">Yifan Li, Jiawei Han, and Jiong Yang. 2004. Clustering Moving Objects. In <em>      <em>KDD</em>     </em>. ACM, New York, NY, USA, 617&#x2013;622.</li>     <li id="BibPLXBIB0034" label="[34]">Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, and Belle&#x00A0;L. Tseng. 2008. Facetnet: A Framework for Analyzing Communities and Their Evolutions in Dynamic Networks. In <em>      <em>WWW</em>     </em>. ACM, New York, NY, USA, 685&#x2013;694.</li>     <li id="BibPLXBIB0035" label="[35]">Anand Louis, Prasad Raghavendra, Prasad Tetali, and Santosh Vempala. 2012. Many Sparse Cuts via Higher Eigenvalues. In <em>      <em>STOC</em>     </em>. ACM, New York, NY, USA, 1131&#x2013;1140.</li>     <li id="BibPLXBIB0036" label="[36]">Othon Michail. 2016. An introduction to temporal graphs: An algorithmic perspective. <em>      <em>Internet Mathematics</em>     </em>12, 4 (2016), 239&#x2013;280.</li>     <li id="BibPLXBIB0037" label="[37]">Peter&#x00A0;J Mucha, Thomas Richardson, Kevin Macon, Mason&#x00A0;A Porter, and Jukka-Pekka Onnela. 2010. Community structure in time-dependent, multiscale, and multiplex networks. <em>      <em>Science</em>     </em>328, 5980 (2010), 876&#x2013;878.</li>     <li id="BibPLXBIB0038" label="[38]">Huazhong Ning, Wei Xu, Yun Chi, Yihong Gong, and Thomas&#x00A0;S Huang. 2010. Incremental spectral clustering by efficiently updating the eigen-system. <em>      <em>Pattern Recognition</em>     </em>43, 1 (2010), 113&#x2013;127.</li>     <li id="BibPLXBIB0039" label="[39]">James Rosswog and Kanad Ghose. 2008. Detecting and Tracking Spatio-temporal Clusters with Adaptive History Filtering. In <em>      <em>ICDMW</em>     </em>. IEEE, Washington, DC, USA, 448&#x2013;457.</li>     <li id="BibPLXBIB0040" label="[40]">Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>22 (2000), 888&#x2013;905.</li>     <li id="BibPLXBIB0041" label="[41]">David&#x00A0;I Shuman, Sunil&#x00A0;K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. 2013. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. <em>      <em>IEEE Signal Processing Magazine</em>     </em>30, 3 (2013), 83&#x2013;98.</li>     <li id="BibPLXBIB0042" label="[42]">Arlei Silva, Xuan&#x00A0;Hong Dang, Prithwish Basu, Ambuj Singh, and Ananthram Swami. 2016. Graph Wavelets via Sparse Cuts. In <em>      <em>KDD</em>     </em>. ACM, New York, NY, USA, 1175&#x2013;1184.</li>     <li id="BibPLXBIB0043" label="[43]">Arlei Silva, Ambuj Singh, and Ananthram Swami. 2017. Spectral Algorithms for Temporal Graph Cuts. <a href="http://arxiv.org/abs/1702.04746.pdf" target="_blank">http://arxiv.org/abs/1702.04746.pdf</a>. (2017).</li>     <li id="BibPLXBIB0044" label="[44]">Albert Sole-Ribalta, Manlio De&#x00A0;Domenico, Nikos&#x00A0;E Kouvaris, Albert Diaz-Guilera, Sergio Gomez, and Alex Arenas. 2013. Spectral properties of the Laplacian of multiplex networks. <em>      <em>Physical Review E</em>     </em>88, 3 (2013), 032807.</li>     <li id="BibPLXBIB0045" label="[45]">Daniel&#x00A0;A. Spielman and Shang-Hua Teng. 2004. Nearly-linear Time Algorithms for Graph Partitioning, Graph Sparsification, and Solving Linear Systems. In <em>      <em>STOC</em>     </em>. ACM, New York, NY, USA, 81&#x2013;90.</li>     <li id="BibPLXBIB0046" label="[46]">Daniel&#x00A0;A Spielman and Shang-Hua Teng. 2011. Spectral sparsification of graphs. <em>      <em>SIAM J. Comput.</em>     </em>40, 4 (2011), 981&#x2013;1025.</li>     <li id="BibPLXBIB0047" label="[47]">Juliette Stehl&#x00E9;, Nicolas Voirin, Alain Barrat, Ciro Cattuto, Lorenzo Isella, Jean-Fran&#x00E7;ois Pinton, Marco Quaggiotto, Wouter Van&#x00A0;den Broeck, Corinne R&#x00E9;gis, Bruno Lina, and others. 2011. High-resolution measurements of face-to-face contact patterns in a primary school. <em>      <em>PloS one</em>     </em>6, 8 (2011), e23176.</li>     <li id="BibPLXBIB0048" label="[48]">Dane Taylor, Sean&#x00A0;A Myers, Aaron Clauset, Mason&#x00A0;A Porter, and Peter&#x00A0;J Mucha. 2017. Eigenvector-based centrality measures for temporal networks. <em>      <em>Multiscale Modeling &#x0026; Simulation</em>     </em>15, 1 (2017), 537&#x2013;574.</li>     <li id="BibPLXBIB0049" label="[49]">Luca Trevisan. 2013. Is Cheeger-type Approximation Possible for Nonuniform Sparsest Cut?<a href="https://arxiv.org/pdf/1303.2730.pdf" target="_blank">https://arxiv.org/pdf/1303.2730.pdf</a>. (2013).</li>     <li id="BibPLXBIB0050" label="[50]">Chang Xu, Dacheng Tao, and Chao Xu. 2013. A survey on multi-view learning. <a href="https://arxiv.org/pdf/1304.5634.pdf" target="_blank">https://arxiv.org/pdf/1304.5634.pdf</a>. (2013).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>Data and code: <a class="link-inline force-break" href="https://github.com/arleilps/time-cuts">https://github.com/arleilps/time-cuts</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Qualdl data: <a class="link-inline force-break" href="https://www.quandl.com/data/">https://www.quandl.com/data/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186118">https://doi.org/10.1145/3178876.3186118</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
