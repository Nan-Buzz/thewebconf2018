<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Higher-order Network Representation Learning</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Higher-order Network Representation Learning</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Ryan A.</span>      <span class="surName">Rossi</span>     Adobe Research, <a href="mailto:rrossi@adobe.com">rrossi@adobe.com</a>     </div>     <div class="author">     <span class="givenName">Nesreen K.</span>      <span class="surName">Ahmed</span>     Intel Labs, <a href="mailto:nesreen.k.ahmed@intel.com">nesreen.k.ahmed@intel.com</a>     </div>     <div class="author">     <span class="givenName">Eunyee</span>      <span class="surName">Koh</span>     Adobe Research, <a href="mailto:eunyee@adobe.com">eunyee@adobe.com</a><a href="mailto:"/></div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186900" target="_blank">https://doi.org/10.1145/3184558.3186900</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>This paper describes a general framework for learning Higher-Order Network Embeddings (HONE) from graph data based on network motifs. The HONE framework is highly expressive and flexible with many interchangeable components. The experimental results demonstrate the effectiveness of learning higher-order network representations. In all cases, HONE outperforms recent embedding methods that are unable to capture higher-order structures with a mean relative gain in AUC of 19% (and up to 75% gain) across a wide variety of networks and embedding methods.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Artificial intelligence;</strong> <strong>Machine learning;</strong> &#x2022;<strong> Mathematics of computing </strong>&#x2192; <strong>Graph algorithms;</strong> <em>Combinatorics;</em> <em>Graph theory;</em> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Data mining;</strong> &#x2022;<strong> Theory of computation </strong>&#x2192; <strong>Graph algorithms analysis;</strong> <strong>Logical and relational learning;</strong></small> </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Ryan A. Rossi, Nesreen K. Ahmed, and Eunyee Koh. 2018. Higher-order Network Representation Learning. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 3 Pages. <a href="https://doi.org/10.1145/3184558.3186900" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186900</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Roles represent node (or edge&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]) connectivity patterns such as hubs, star-centers, star-edge nodes, near-cliques or nodes that act as bridges to different regions of the graph. Intuitively, two nodes belong to the same role if they are structurally similar&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. Many network representation learning methods (including random-walk based methods such as node2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]) seek to capture the notion of structural similarity (roles)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] by defining node similarity locally based on neighborhood properties and/or proximity (<em>e.g.</em>, near one another in the graph). However, such methods are insufficient for roles&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] as they fail to capture the higher-order connectivity patterns of a node. For instance, instead of representing hub nodes in a similar fashion, methods using random-walks (proximity/distance-based) would represent a hub node and its neighbors similarly despite them having fundamentally different connectivity patterns.</p>    <p>In this work, we propose <em>higher-order network representation learning</em> and describe a general framework called Higher-Order Network Embeddings (HONE) for learning such higher-order embeddings based on network motifs. The term motif is used generally and may refer to graphlets or orbits (graphlet automorphisms)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. The HONE framework expresses a general family of embedding methods based on a set of motif-based matrices and their powers. In this work, we investigate HONE variants based on the weighted motif graph, motif transition matrix, motif Laplacian matrix, as well as other motif-based matrix formulations. The experiments demonstrate the effectiveness of <em>higher-order network embeddings</em>.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Higher-order Network Embeddings</h2>     </div>    </header>    <p>This section describes the <em>Higher-Order Network Embedding</em> (HONE) framework. Given a network <em>G</em> = (<em>V</em>, <em>E</em>) with <em>N</em> = |<em>V</em>| nodes and a set <span class="inline-equation"><span class="tex">$\mathcal {H} = \lbrace H_1, \ldots , H_T\rbrace$</span>     </span> of <em>T</em> network motifs, form the motif (weighted) adjacency matrices: <span class="inline-equation"><span class="tex">$\mathcal {W} = \big \lbrace _1, _2, \ldots , _T \big \rbrace$</span>     </span> where (<sub>     <em>t</em>     </sub>)<sub>     <em>ij</em>     </sub> = number of instances of motif <span class="inline-equation"><span class="tex">$H_t \in \mathcal {H}$</span>     </span> that contain nodes <em>i</em> and <em>j</em>. To generalize HONE for any motif-based matrix formulation, we define <em>&#x03A8;</em> as a function <em>&#x03A8;</em>: <sup>     <em>N</em> &#x00D7; <em>N</em>     </sup> &#x2192; <sup>     <em>N</em> &#x00D7; <em>N</em>     </sup> over a (k-step) weighted motif adjacency matrix <span class="inline-equation"><span class="tex">$_t^{k}$</span>     </span>. For convenience, we use below to denote a weighted adjacency matrix for an arbitrary motif. We summarize the motif matrix functions <em>&#x03A8;</em> investigated below.</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>Motif Weighted Graph</strong>: In the case of using HONE directly with a weighted motif adjacency matrix , then <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \Psi : \rightarrow \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> The number of paths weighted by motif counts from node <em>i</em> to node <em>j</em> in <em>k</em>-steps is given by <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} (^k)_{ij} \,\, = \; \big (\,\underbrace{\, \cdots \, }_{k}\,\big)_{ij} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>     <br/></li>     <li id="list2" label="&#x2022;"><strong>Motif Transition Matrix</strong>: The random walk on a graph weighted by motif counts has transition probabilities <span class="inline-equation"><span class="tex">$P_{ij} = \frac{W_{ij}}{w_i}$</span>     </span> where <em>w<sub>i</sub>     </em> = &#x2211;<sub>      <em>j</em>     </sub>     <em>W<sub>ij</sub>     </em> is the motif degree of node <em>i</em>. The random walk motif transition matrix for an arbitrary weighted motif graph is defined as: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} = ^{-1}\end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$= \mathtt {diag}()$</span>     </span> is a <em>N</em> &#x00D7; <em>N diagonal motif degree matrix</em> with the motif degree <em>w<sub>i</sub>     </em> = &#x2211;<sub>      <em>j</em>     </sub>     <em>W<sub>ij</sub>     </em> of each node on the diagonal and = [&#x2009;1&#x2009;1&#x2009;&#x22C5;&#x22C5;&#x22C5;&#x2009;1&#x2009;]<sup>      <em>T</em>     </sup> &#x2208; <sup>      <em>N</em>     </sup> is the vector of all ones. The motif transition matrix represents the transition probabilities of a non-uniform random walk on the graph that selects subsequent nodes with probability proportional to the connecting edge&#x0027;s motif count. Therefore, the probability of transitioning from node <em>i</em> to node <em>j</em> depends on the motif degree of <em>j</em> relative to the total sum of motif degrees of all neighbors of <em>i</em>. The probability of transitioning from node <em>i</em> to node <em>j</em> in <em>k</em>-steps is given by (<sup>      <em>k</em>     </sup>)<sub>      <em>ij</em>     </sub>.<br/></li>     <li id="list3" label="&#x2022;"><strong>Motif Laplacian</strong>: The motif Laplacian for a weighted <em>motif</em> graph is defined as: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \, = \, - \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$= \mathtt {diag}()$</span>     </span> is the diagonal matrix of motif degrees.<br/></li>     <li id="list4" label="&#x2022;"><strong>Normalized</strong>      <em>      <strong>Motif</strong>     </em>     <strong>Laplacian</strong>: Given a graph weighted by the counts of an arbitrary network motif <span class="inline-equation"><span class="tex">$H_t \in \mathcal {H}$</span>     </span>, the <em>normalized motif Laplacian</em> is defined as <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \widehat{} \,\, = \,\, \, - \, ^{-1/2} ^{-1/2} \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where is the identity matrix and <span class="inline-equation"><span class="tex">$= \mathtt {diag}()$</span>     </span>.<br/></li>    </ul>    <p>Notice that all variants are easily formulated as functions <em>&#x03A8;</em> in terms of an arbitrary motif weighted graph . Next, we derive all <em>k</em>-step motif-based matrices for all <em>T</em> motifs and <em>K</em> steps: <div class="table-responsive" id="eq6">     <div class="display-equation">      <span class="tex mytex">\begin{equation} _t^{(k)} = \Psi (^k_t),\; \text{ for } k=1,2,\ldots ,K \,\text{ and }\, t=1,\ldots ,T \end{equation} </span>      <br/>      <span class="equation-number">(6)</span>     </div>     </div> These k-step motif-based matrices can densify quickly and therefore we recommend using <em>K</em> &#x2264; 4. Given a k-step motif-based matrix <span class="inline-equation"><span class="tex">$_t^{(k)}$</span>     </span> for an arbitrary network motif <span class="inline-equation"><span class="tex">$H_t \in \mathcal {H}$</span>     </span>, we learn node embeddings by solving the following objective function: <div class="table-responsive" id="eq7">     <div class="display-equation">      <span class="tex mytex">\begin{align} _{_t^{(k)}\!,_t^{(k)} \!\in \mathcal {C}} \mathbb {D}\,\! \big (\, ^{(k)}_t \; \Vert \; \Phi \langle _t^{(k)} \! _t^{(k)} \rangle \big) \end{align} </span>      <br/>      <span class="equation-number">(7)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$\mathbb {D}$</span>     </span> is a generalized Bregman divergence with matching linear or non-linear function <em>&#x03A6;</em> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span> denotes constraints (<em>e.g.</em>, <sup>     <em>T</em>     </sup> = , <sup>     <em>T</em>     </sup> =). We use Eq.&#x00A0;<a class="eqn" href="#eq7">7</a> to learn a <em>N</em> &#x00D7; <em>D</em>     <sub>&#x2113;</sub> local embedding <span class="inline-equation"><span class="tex">$_t^{(k)}$</span>     </span> from <span class="inline-equation"><span class="tex">$_t^{(k)}$</span>     </span> for all <em>t</em> = 1, &#x2026;, <em>T</em> and <em>k</em> = 1, &#x2026;, <em>K</em>.<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>Afterwards, we scale each column of <span class="inline-equation"><span class="tex">$_t^{(k)}$</span>     </span> using the Euclidean norm. Next, we concatenate the k-step embedding matrices for all <em>T</em> motifs and all <em>K</em> steps: <div class="table-responsive" id="eq8">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \; = \; \Big [ \, \underbrace{_1^{(1)} \;\, \cdots \,\; _T^{(1)}}_{1\text{-step}} \;\; \cdots \;\;\, \underbrace{_1^{(K)} \;\, \cdots \,\; _T^{(K)}}_{K\text{-steps}} \, \Big ] \end{equation} </span>      <br/>      <span class="equation-number">(8)</span>     </div>     </div> where is a <em>N</em> &#x00D7; <em>TKD</em>     <sub>&#x2113;</sub> matrix. Given , we learn a <em>global</em> higher-order network embedding by solving the following: <div class="table-responsive" id="eq9">     <div class="display-equation">      <span class="tex mytex">\begin{align} _{,\in \mathcal {C}} \;\, \mathbb {D} \big (\,\; \Vert \; \Phi \langle \rangle \big) \end{align} </span>      <br/>      <span class="equation-number">(9)</span>     </div>     </div> where is a <em>N</em> &#x00D7; <em>D</em> matrix of node embeddings. In Eq.&#x00A0;<a class="eqn" href="#eq9">9</a> we use Frobenius norm which leads to the following minimization problem: <div class="table-responsive" id="eq10">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \min _{, } \; \frac{1}{2} \, \big \Vert \, - \big \Vert _{F}^{2} \;=\; \frac{1}{2} \sum _{ij} \big (_{ij} - ()_{ij}\big)^2 \end{equation} </span>      <br/>      <span class="equation-number">(10)</span>     </div>     </div> A similar minimization problem is solved for Eq.&#x00A0;<a class="eqn" href="#eq7">7</a>.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experiments</h2>     </div>    </header>    <p>We compare the proposed HONE variants to five recent state-of-the-art methods (see Table&#x00A0;<a class="tbl" href="#tab1">1</a>). All methods output (<em>D</em> = 128)-dimensional node embeddings = [&#x2009;<sub>1</sub>&#x22C5;&#x22C5;&#x22C5;<sub>     <em>N</em>     </sub>&#x2009;]<sup>     <em>T</em>     </sup> where <sub>     <em>i</em>     </sub> &#x2208; <sup>     <em>D</em>     </sup>. For node2vec, we perform a grid search over <em>p</em>, <em>q</em> &#x2208; {0.25, 0.5, 1, 2, 4} as mentioned in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. All other hyperparameters for node2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], DeepWalk&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], and LINE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] correspond to those mentioned in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. In contrast, the HONE variants have only one hyperparameter, namely, the number of steps <em>K</em> which is selected automatically via a grid search over <em>K</em> &#x2208; {1, 2, 3, 4} using 10% of the labeled data. We use all 2-4 node connected orbits&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] and set <em>D</em>     <sub>&#x2113;</sub> = 16 for the local motif embeddings. All methods use logistic regression (LR) with an L2 penalty. The model is selected using 10-fold cross-validation on 10% of the labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>].</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">AUC results comparing HONE to recent embedding methods. See text for discussion.</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:center;"/>       <th style="text-align:left;">soc-hamster</th>       <th style="text-align:left;">rt-twitter-cop</th>       <th style="text-align:left;">soc-wiki-Vote</th>       <th style="text-align:left;">tech-routers-rf</th>       <th style="text-align:left;">facebook-PU</th>       <th style="text-align:left;">inf-openflights</th>       <th style="text-align:left;">soc-bitcoinA</th>       <th style="text-align:left;">        <SmallCap>Rank</SmallCap>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:center;">        <strong>HONE</strong>- (Eq.&#x00A0;<a class="eqn" href="#eq1">1</a>)</td>       <td style="text-align:center;">0.841</td>       <td style="text-align:center;">0.843</td>       <td style="text-align:center;">0.811</td>       <td style="text-align:center;">0.862</td>       <td style="text-align:center;">0.726</td>       <td style="text-align:center;">0.910</td>       <td style="text-align:center;">0.979</td>       <td style="text-align:center;">        <strong>1</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>HONE</strong>- (Eq.&#x00A0;<a class="eqn" href="#eq3">3</a>)</td>       <td style="text-align:center;">0.840</td>       <td style="text-align:center;">0.840</td>       <td style="text-align:center;">0.812</td>       <td style="text-align:center;">0.863</td>       <td style="text-align:center;">0.724</td>       <td style="text-align:center;">0.913</td>       <td style="text-align:center;">0.980</td>       <td style="text-align:center;">        <strong>2</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>HONE</strong>- (Eq.&#x00A0;<a class="eqn" href="#eq4">4</a>)</td>       <td style="text-align:center;">0.829</td>       <td style="text-align:center;">0.841</td>       <td style="text-align:center;">0.808</td>       <td style="text-align:center;">0.858</td>       <td style="text-align:center;">0.722</td>       <td style="text-align:center;">0.906</td>       <td style="text-align:center;">0.975</td>       <td style="text-align:center;">        <strong>3</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>HONE</strong>-<span class="inline-equation"><span class="tex">$\widehat{}$</span>        </span> (Eq.&#x00A0;<a class="eqn" href="#eq5">5</a>)</td>       <td style="text-align:center;">0.829</td>       <td style="text-align:center;">0.836</td>       <td style="text-align:center;">0.803</td>       <td style="text-align:center;">0.862</td>       <td style="text-align:center;">0.722</td>       <td style="text-align:center;">0.908</td>       <td style="text-align:center;">0.976</td>       <td style="text-align:center;">        <strong>4</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>Node2Vec</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0004">4</a>]</td>       <td style="text-align:center;">0.810</td>       <td style="text-align:center;">0.635</td>       <td style="text-align:center;">0.721</td>       <td style="text-align:center;">0.804</td>       <td style="text-align:center;">0.701</td>       <td style="text-align:center;">0.844</td>       <td style="text-align:center;">0.894</td>       <td style="text-align:center;">        <strong>5</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>DeepWalk</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0005">5</a>]</td>       <td style="text-align:center;">0.796</td>       <td style="text-align:center;">0.621</td>       <td style="text-align:center;">0.710</td>       <td style="text-align:center;">0.796</td>       <td style="text-align:center;">0.696</td>       <td style="text-align:center;">0.837</td>       <td style="text-align:center;">0.863</td>       <td style="text-align:center;">        <strong>6</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>LINE</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0009">9</a>]</td>       <td style="text-align:center;">0.752</td>       <td style="text-align:center;">0.706</td>       <td style="text-align:center;">0.734</td>       <td style="text-align:center;">0.800</td>       <td style="text-align:center;">0.630</td>       <td style="text-align:center;">0.837</td>       <td style="text-align:center;">0.780</td>       <td style="text-align:center;">        <strong>7</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>GraRep</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0003">3</a>]</td>       <td style="text-align:center;">0.805</td>       <td style="text-align:center;">0.672</td>       <td style="text-align:center;">0.743</td>       <td style="text-align:center;">0.829</td>       <td style="text-align:center;">0.702</td>       <td style="text-align:center;">0.898</td>       <td style="text-align:center;">0.559</td>       <td style="text-align:center;">        <strong>8</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">        <strong>Spectral</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0010">10</a>]</td>       <td style="text-align:center;">0.561</td>       <td style="text-align:center;">0.699</td>       <td style="text-align:center;">0.593</td>       <td style="text-align:center;">0.602</td>       <td style="text-align:center;">0.516</td>       <td style="text-align:center;">0.606</td>       <td style="text-align:center;">0.629</td>       <td style="text-align:center;">        <strong>9</strong>       </td>      </tr>     </tbody>     </table>    </div>    <p>We evaluate the HONE variants for link prediction. Given a partially observed graph <em>G</em> with a fraction of missing edges, the link prediction task is to predict these missing edges. We generate a labeled dataset of edges. Positive examples are obtained by removing 50% of edges randomly, whereas <em>negative examples</em> are generated by randomly sampling an equal number of node pairs <span class="inline-equation"><span class="tex">$(i,j) \not\in E$</span>     </span>. For each method, we learn embeddings using the remaining graph. Using the embeddings from each method, we then learn a model to predict whether a given edge in the test set exists in <em>E</em> or not.</p>    <p>To construct edge features from the node embeddings, we use the mean operator defined as (<sub>     <em>i</em>     </sub> + <sub>     <em>j</em>     </sub>)/2. The AUC results are provided in Table&#x00A0;<a class="tbl" href="#tab1">1</a>. In all cases, the HONE methods outperform the other embedding methods with an overall mean gain of 19.24% (and up to 75.21% gain) across a wide variety of graphs with different characteristics. Overall, the HONE variants achieve an average gain of 10.68% over node2vec, 12.56% over DeepWalk, 13.79% over LINE, 17.17% over GraRep, and 41.99% over Spectral clustering across all networks. We also derive a total ranking of the embedding methods over all graph problems based on mean relative gain (1-vs-all). Results are provided in the last column of Table&#x00A0;<a class="tbl" href="#tab1">1</a>.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Conclusion</h2>     </div>    </header>    <p>In this work, we introduced higher-order network representation learning and proposed a general framework called <em>higher-order network embedding</em> (HONE) for learning such embeddings based on higher-order connectivity patterns. The experimental results demonstrate the effectiveness of learning higher-order network representations. Future work will investigate the framework using other useful motif-based matrices.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Nesreen&#x00A0;K. Ahmed, Jennifer Neville, Ryan&#x00A0;A. Rossi, and Nick Duffield. 2015. Efficient Graphlet Counting for Large Networks. In <em>      <em>ICDM</em>     </em>. 10.</li>     <li id="BibPLXBIB0002" label="[2]">Nesreen&#x00A0;K. Ahmed, Ryan&#x00A0;A. Rossi, Theodore&#x00A0;L. Willke, and Rong Zhou. 2017. Edge Role Discovery via Higher-Order Structures. In <em>      <em>PAKDD</em>     </em>. 291&#x2013;303.</li>     <li id="BibPLXBIB0003" label="[3]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning graph representations with global structural information. In <em>      <em>CIKM</em>     </em>. ACM, 891&#x2013;900.</li>     <li id="BibPLXBIB0004" label="[4]">Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In <em>      <em>SIGKDD</em>     </em>. 855&#x2013;864.</li>     <li id="BibPLXBIB0005" label="[5]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In <em>      <em>SIGKDD</em>     </em>. 701&#x2013;710.</li>     <li id="BibPLXBIB0006" label="[6]">Nata&#x0161;a Pr&#x017E;ulj. 2007. Biological network comparison using graphlet degree distribution. <em>      <em>Bioinfo.</em>     </em>23, 2 (2007), e177&#x2013;e183.</li>     <li id="BibPLXBIB0007" label="[7]">Ryan&#x00A0;A. Rossi and Nesreen&#x00A0;K. Ahmed. 2015. The Network Data Repository with Interactive Graph Analytics and Visualization. In <em>      <em>AAAI</em>     </em>. 4292&#x2013;4293. <a class="link-inline force-break" href="http://networkrepository.com"      target="_blank">http://networkrepository.com</a></li>     <li id="BibPLXBIB0008" label="[8]">Ryan&#x00A0;A. Rossi and Nesreen&#x00A0;K. Ahmed. 2015. Role Discovery in Networks. <em>      <em>Transactions on Knowledge and Data Engineering</em>     </em>27, 4 (April 2015), 1112&#x2013;1131.</li>     <li id="BibPLXBIB0009" label="[9]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. In <em>      <em>WWW</em>     </em>. 1067&#x2013;1077.</li>     <li id="BibPLXBIB0010" label="[10]">Lei Tang and Huan Liu. 2011. Leveraging social media networks for classification. <em>      <em>Data Mining and Knowledge Discovery</em>     </em>23, 3 (2011), 447&#x2013;478.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>For the motif Laplacian matrix formulations proposed above, we also investigated using the eigenvectors of the <em>D</em>    <sub>&#x2113;</sub> smallest eigenvalues of <span class="inline-equation"><span class="tex">$\Psi (^k_t)$</span>    </span> as node embeddings.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186900">https://doi.org/10.1145/3184558.3186900</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
