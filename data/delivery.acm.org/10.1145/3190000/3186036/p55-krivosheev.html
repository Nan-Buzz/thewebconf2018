<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Crowd-based Multi-Predicate Screening of Papers in Literature Reviews</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Crowd-based Multi-Predicate Screening of Papers in Literature Reviews</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Evgeny</span>     <span class="surName">Krivosheev</span>,     University of Trento, Italy, <a href="mailto:evgeny.krivosheev@unitn.it">evgeny.krivosheev@unitn.it</a>    </div>    <div class="author">     <span class="givenName">Fabio</span>     <span class="surName">Casati</span>,     University of Trento, Italy, Tomsk Polytechnic University, Russia, <a href="mailto:fabio.casati@unitn.it">fabio.casati@unitn.it</a>    </div>    <div class="author">     <span class="givenName">Boualem</span>     <span class="surName">Benatallah</span>,     University of New South Wales, Australia, <a href="mailto:boualem@cse.unsw.edu.au">boualem@cse.unsw.edu.au</a>    </div>                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186036" target="_blank">https://doi.org/10.1145/3178876.3186036</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Systematic literature reviews (SLRs) are one of the most common and useful form of scientific research and publication. Tens of thousands of SLRs are published each year, and this rate is growing across all fields of science. Performing an accurate, complete and unbiased SLR is however a difficult and expensive endeavor. This is true in general for all phases of a literature review, and in particular for the paper screening phase, where authors filter a set of potentially in-scope papers based on a number of <em>exclusion criteria</em>. To address the problem, in recent years the research community has began to explore the use of the crowd to allow for a faster, accurate, cheaper and unbiased screening of papers. Initial results show that crowdsourcing can be effective, even for relatively complex reviews.</small>    </p>    <p>     <small>In this paper we derive and analyze a set of strategies for crowd-based screening, and show that an adaptive strategy, that continuously re-assesses the statistical properties of the problem to minimize the number of votes needed to take decisions for each paper, significantly outperforms a number of non-adaptive approaches in terms of cost and accuracy. We validate both applicability and results of the approach through a set of crowdsourcing experiments, and discuss properties of the problem and algorithms that we believe to be generally of interest for classification problems where items are classified via a series of successive tests (as it often happens in medicine).</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>human computation</small>, </span>     <span class="keyword">      <small> classification</small>, </span>     <span class="keyword">      <small> literature reviews</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Evgeny Krivosheev, Fabio Casati, and Boualem Benatallah. 2018. Crowd-based Multi-Predicate Screening of Papers in Literature Reviews. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186036" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186036</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Systematic literature reviews (SLR) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0019">19</a>] are reviews that follow a predefined process aimed at achieving transparency and impartiality with respect to the sources analyzed, minimizing distortions, biases, and conflicts of interest [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0035">35</a>]. They are one of the most important form of publications in science [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0036">36</a>], and are the basis for evidence-based practices and even government policies, from education to healthcare, as they pool results independently obtained from a number of research groups [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0011">11</a>]. Recognizing their importance, the number of systematic reviews is growing steadily, with tens of thousands of publications per year in all fields.</p>    <p>The cornerstone of transparency and impartiality in SLRs lies in a formalized paper selection process. This is typically formed by a stated scope and goal of the review (e.g., &#x201D;study the effect of regular physical exercises on progress of dementia in older adults, focusing only on papers describing randomized controlled trials&#x201D;), translated by the authors into a corresponding query (a boolean expression that includes relevant keywords) that retrieves candidate papers from a database such as Scopus. To avoid missing papers, the query tends to be inclusive, which means that it returns hundreds or thousands of results [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0030">30</a>] that are later screened by researches based on predefined <em>exclusion criteria</em> (e.g., &#x201D;filter out papers that do not measure cognitive decline&#x201D;), typically down to a few dozens.</p>    <p>While extremely useful, SLRs are very time consuming in terms of both effort and elapsed time, and this is true also for the paper screening phase [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0033">33</a>]. Furthermore, with hundreds of thousands of papers written every year, SLRs rapidly become outdated&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0004">4</a>], and although they should be updated periodically, the effort for doing so often represents a barrier [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0037">37</a>], so that it is not uncommon for reviews to miss 30% or 40% of relevant papers&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0004">4</a>].</p>    <p>In this paper we explore the use of crowdsourcing in systematic reviews, and specifically in the filtering phase, where we screen candidate papers resulting from the initial literature search to identify papers to be included in the analysis. This is an instance of finite pool classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0030">30</a>] and crowd screening problems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0032">32</a>] where we need to classify a finite set of objects while minimizing cost. The potential benefits of crowdsourcing here are in terms of a faster and cheaper screening (compared to screening by professionals) as well as increased transparency (process and votes can be made public if desired) and reduced risk of author bias. The crowd also brings diversity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0040">40</a>] and, as we experienced first hand, disagreement in the crowd may signal errors or ambiguities in the definition of exclusion criteria. Research in this area is still in its infancy, although a set of recent initial efforts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0039">39</a>] present very encouraging results in terms of both quality and cost reduction with respect to expert screening costs, and show feasibility of crowd-based screening in various domains, including healthcare.</p>    <p>In the following we present a probabilistic model suitable for the criteria-based screening of papers typical of SLRs and propose a set of strategies for crowd-based screening. Our main contribution consists in an adaptive crowdsourcing algorithm that significantly outperforms baselines. The algorithm polls the crowd in small batches and estimates, at each iteration and <em>for each item</em>, i) the criterion for which getting one more crowd vote on the paper can more efficiently lead us to a classification decision, and ii) whether we should give up trying to classify this item, recognizing that the crowd cannot efficiently reach a decision and therefore it should be left to the authors for expert screening. This also means that the algorithm is robust to papers and criteria that are overly difficult for the crowd to classify, in that it does not needlessly spend money on them.</p>    <p>The model is the result of many iterations and variations of experiments on commercial crowdsourcing platforms (Amazon Mechanical Turk (AMT) and <em>CrowdFlower</em><a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn1" id="foot-fn1"><sup>1</sup></a>). We then performed additional experiments to validate the effectiveness of the strategies. While we present the results in the context of SLRs because we validated the model and findings for this case, we believe that results can be generally of interest for classification problems where items are classified via series of successive tests, as it often happens in medicine, as well as for finite pool classification problems and crowd-based query optimization, where the crowd evaluates predicates (analogously to our exclusion criteria) that filter a set of tuples to compute the query results.</p>   </section>   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related work</h2>    </div>    </header>    <p>Our work builds on approaches in crowdsourcing in SLR but also more generally on works on crowd-based classification.</p>    <p>    <strong>Crowdsourcing in Systematic Reviews.</strong> Recently, Brown and Allison&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0003">3</a>] used crowdsourcing to, among other tasks, classify 689 abstracts based on a set of criteria using AMT. Authors report agreement in 75% of the abstract, based on two raters, and a third rater is used to break the tie in case of disagreement. The paper does not discuss optimal crowdsourcing strategies or algorithms to minimize errors, but points to the potential of crowdsourcing in analyzing literature.</p>    <p>Mortensen and colleagues crowdsourced paper screening&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0028">28</a>] in four literature reviews, each with several criteria. Their aim was to explore feasibility and costs of crowdsourcing and they address the problem by measuring workers agreement in a set of tasks run on AMT for papers in the medical field. Their work differs from ours in that it does not propose algorithms to identify optimal crowdsourcing strategies. However, it contains interesting observations related to the importance of task design, to the cost-effectiveness of crowdsourcing even when the task is not optimized, and to the high degree of variability in workers&#x0027;s agreement from paper to paper and criteria to criteria (Fleiss&#x2019; Kappa ranging from 0.5 to -0.03). This is consistent with our own studies (our papers are in a different scientific area) and we exploit this variability to optimize the cost/error tradeoff.</p>    <p>Krivosheev and colleagues&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>] also present a model and strategies for crowdsourcing SLR. An interesting aspect of the model and approach here is that the authors model cost and loss (error) resulting from crowdsourcing task, attempt to estimate them at the start, and provide authors with a price/error trade-off that can be used to decide how much to invest in the task. We borrow several concepts from this work, such as the ability to provide an estimate and a set of alternatives to SLR author, although the model of this paper is limited to screening based on one criterion.</p>    <p>Nguyen et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0030">30</a>] adopt a mixed crowd+expert+machine learning approach with an active learning classifier, where papers to be labeled are iteratively chosen to minimize overall cost and loss, by comparing estimated loss of crowd classification versus expert classification. This paper is part of a trend trying to leverage AI in literature reviews, which we do not discuss further as it is not the focus of this paper.</p>    <p>In general all papers reports positive results and complement them with insights and guidelines for task design of even for the design of a dedicated crowdsourcing platform for SLR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0040">40</a>] as well as investigate the use of crowd for other phases of interest for SLR such as information extraction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0036">36</a>]. Interestingly, the only exception is represented by a study performed with medical students as screeners rather than online crowd workers, which reports rather poor accuracy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0029">29</a>].</p>    <p>From these studies we also learn that workers&#x2019; accuracy vary across criteria, which points to the need of adapting to the characteristics of each SLR, criterion, and crowd. Indeed, one of the main differences of our approach lies in the ability to focus the crowd on &#x201D;low hanging fruits&#x201D;, that is, items and criteria that are statistically more efficient from the perspective of correctly excluding papers.</p>    <p>Although not focused on paper screening, we also mention a fascinating analysis by Law and colleagues trying to understand under which conditions do researchers resort to crowdsourcing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0022">22</a>]. Among the many interesting considerations lies the observation that crowdsourcing is viable only if both authors <em>and reviewers</em> find it acceptable. Paper screening in SLRs seem to fit the requirements for being acceptable by authors but it is equally important for the scientific community to provide solid evidence of the quality of crowdsourced screening if we want it to be accepted by reviewers - especially in fields where SLRs may form the base of policies and practices.</p>    <p>    <strong>Crowdsourced Classification</strong> The problem discussed here is an instance of a finite pool classification problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0030">30</a>] and specifically of crowdsourcing-based classification. This problem has been studied for hundreds of years now, dating back at least to the end of the 18<sup>     <em>th</em>    </sup> century, when the Marquis de Condorcet presented his <em>Jury Theorem</em><a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn2" id="foot-fn2"><sup>2</sup></a>, stating that if each juror in a jury has an error rate lower than 0.5 and if <em>guilty</em> vs <em>innocent</em> votes are independent, larger juries reach more accurate results, and approach perfection as the jury grows.</p>    <p>From there, researchers from the AI, database, and human computation communities have proposed many of classification algorithms, mostly based on variations of majority voting where votes are counted differently based on estimated worker&#x0027;s accuracy. The seminal work of Dawid and Skene [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0005">5</a>] and refinement by, among others, Whitehill [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0041">41</a>], Dong et al [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0007">7</a>], Li et al [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0023">23</a>], and Liu et al [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0025">25</a>] model workers&#x2019; accuracy - often with a confusion matrix - and then adopt variants of Expectation Maximization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0006">6</a>] to iteratively refine prior estimates of workers&#x2019; accuracy and of labels. Approaches based on spectral methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0018">18</a>] and maximum entropy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0043">43</a>] have also been proposed, and belief propagation has been recently shown [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0031">31</a>] to be optimal under certain assumptions.</p>    <p>Prior work also addresses the issue of optimizations in terms of costs for obtaining labels and techniques to reduce cheating [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0034">34</a>]. For example, Hirth and colleagues [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0015">15</a>] recommend specific cheating detection and task validation algorithms based on the cost structure of the task.</p>    <p>We build over many of these approaches, and in fact we adopt prior art algorithms for estimating workers&#x2019; accuracy and for assigning labels. Although classification algorithms are central to our overall problem, to a large extent they are for us a swappable component: Our goal is to, given a task design and a classification algorithm, identify how to efficiently query the crowd to minimize the number of labels needed to achieve the desired precision and recall in screening problems.</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Model and Objective</h2>    </div>    </header>    <p>We model the SLR screening problem as a set of papers (items) <em>I</em> to be classified by the screening phase as <em>included</em> (in scope) or excluded based on a set of exclusion criteria (predicates) <em>C</em> = {<em>c</em>    <sub>1</sub>, <em>c</em>    <sub>2</sub>, ...<em>c<sub>m</sub>    </em>}. A paper is excluded if at least one exclusion criterion applies, otherwise it is included. A typical SLR screens hundreds or thousands of papers with a handful of exclusion criteria. We focus on screening based on title and abstract, which is a classical first step screening, consistent with SLR guidelines&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0027">27</a>].</p>    <p>In a crowdsourcing approach, we ask each crowd worker to look at one or more pairs (<em>i</em>, <em>c</em>) and state if exclusion criteria <em>c</em> applies to paper <em>i</em>. Following the mentioned literature, we model a worker&#x0027;s accuracy with a confusion matrix <em>A</em>    <sub>     <em>c</em>, <em>w</em>    </sub> defining the probability of making correct and wrong classifications for each criterion <em>c</em>, thereby allowing us to model different accuracies when the true label is inclusion vs exclusion. Criteria can differ in <em>difficulty</em>. Some are easier to assess than others. Following Whitehill [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0041">41</a>], we model difficulty as a positive real number <em>d<sub>c</sub>    </em> that, given an expected accuracy <em>&#x03B1;<sub>w</sub>    </em> of a worker <em>w</em>, skews the accuracy as follows: <span class="inline-equation"><span class="tex">$\alpha _{c,w} = 0.5+ (\alpha _w-0.5)*e^{-d_c}$</span>    </span>. As the difficulty <em>d<sub>c</sub>    </em> grows, <em>&#x03B1;</em>    <sub>     <em>c</em>, <em>w</em>    </sub> goes to 0.5, corresponding to random selection, which we consider to be the lowest accuracy level<a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn3" id="foot-fn3"><sup>3</sup></a>. Each criterion also has a <em>power</em> (also called selectivity) <em>&#x03B8;<sub>c</sub>    </em>, defined as the percentage of papers to which the criterion applies (and hence need to be excluded). For each SLR and criterion, both accuracy and power are unknown a priori.</p>    <p>We assume the adoption of a general purpose crowdsourcing system with limited control on the kind of crowd we attract but with a near infinite pool of workers. We can however test workers by providing a number <em>N<sub>t</sub>    </em> of test questions (with gold answers provided by SLR authors), and count as valid only votes of workers who pass the test, thereby exercising some control over worker&#x0027;s accuracy (at a cost, as we specify later).</p>    <p>A crowdsourcing <em>strategy</em> is a set <em>K</em> of runs, where each run <em>R<sup>k</sup>    </em> collects <span class="inline-equation"><span class="tex">$J^k_{i,c}$</span>    </span> votes for criterion <em>c</em> on item <em>i</em>. A run may seek votes on all criteria and all papers, or focus on a subset (that is, <span class="inline-equation"><span class="tex">$J^k_{i,c}$</span>    </span> might be 0 for some items).</p>    <p>Tasks also have a cost, which is the unit cost <em>UC</em> for a (non-test) vote multiplied by the number of votes obtained. Although many systems allow not to pay for test answers, consistently with [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>], we believe it is fair and ethical to also pay for test questions for workers who pass them. Furthermore, placing unreasonably many test questions is likely to result in low reputation scores for us and hence in our ability to crowdsource. Concretely, this translates into considering a price per label <em>PPL</em> as follows (<em>N<sub>l</sub>    </em> is the number of valid judgments that a worker gives on non-test papers) <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} PPL = UC \cdot \frac{N_{l}+N_t}{N_{l}} \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div>    </p>    <p>The correction factor approaches 1 when <em>N<sub>l</sub>    </em> is large compared to <em>N<sub>t</sub>    </em>. In practice our control on <em>N<sub>l</sub>    </em> can be limited by many factors (also depending on the crowdsourcing platform policies), such as dropouts, the presence of many concurrent workers that exhaust the available tasks, and more. We observe that tests are for us &#x201D;simply&#x201D; a knob to turn when trading costs for accuracy. Any other knob that accomplishes the same effect can be equivalently used in what follows.</p>    <p>In terms of outcome, key measures are the precision and recall of papers to exclude. We also borrow the concept of loss function from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0030">30</a>] because it summarizes well the subjective perspective of the SLR authors. The <em>loss</em> = <em>lr</em>*<em>FE</em> + <em>FI</em> is represented by the sum of false inclusions <em>FI</em> (papers that survived the screening phase but that should have been excluded instead) and false exclusions <em>FE</em> (filtered-out papers that should have instead been left in), where <em>FE</em> are weighed by a loss ratio <em>lr</em> denoting that false exclusion are <em>lr</em> times more &#x201D;harmful&#x201D; than false inclusion (filtering out a paper is often considered a much more serious error than a false inclusion which &#x201D;simply&#x201D; requires extra work by the authors). The loss ratio is the only parameter we ask the authors to set.</p>    <p>Many variations of the model and of loss function are possible, but these suffice for our purposes. Given the model, our objective is to identify and evaluate a set of efficient crowdsourcing strategies for each SLR that correspond to estimated pareto-optimal price/loss curves. With infinite money we can always arrive at a perfect classification (if workers&#x2019; accuracy is above random and votes are independent), but the challenge is to classify efficiently and at a price/loss point that is acceptable to authors, who decide what price they are willing to pay and which loss they can tolerate. Based on this preference, the algorithm should set the relevant parameters of the crowdsourcing tasks and classification function. We next discuss how this can be done.</p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Algorithms</h2>    </div>    </header>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Baseline single-run algorithms</h3>     </div>    </header>    <p>Our set of baseline algorithms follows the methods applied in recent literature for crowdsourced classification in finite pool contexts and SLRs in particular. Specifically, as we are in the presence of incomplete information (we know neither the classification of the papers nor the accuracy of the workers), we leverage approaches such as TruthFinder [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0007">7</a>] and Expectation Maximization (EM, &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0005">5</a>]) to iteratively refine estimates of accuracy and class until convergence. In addition, simple majority voting is also commonly used as its performances are actually reasonable in finite pool classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0030">30</a>].</p>    <p>Applying them to our problem, we proceed in a single run where we ask each worker to vote on all criteria <em>C</em> for a set of papers. Each worker provides at most <em>N<sub>l</sub>     </em> labels, and we collect <em>J</em> votes per criteria and per paper. Classification proceeds by evaluating each criterion <em>c</em> &#x2208; <em>C</em> on each paper <em>i</em> and, based on the responses received, estimating with one of the mentioned algorithms the probability <em>P</em>(<em>i</em> &#x2208; <em>OUT<sub>c</sub>     </em>) that paper <em>i</em> is classified as out by criterion <em>c</em>.</p>    <p>Once we have probabilities for each criterion, we compute the probability <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>) that a paper <em>i</em> should be excluded as the probability that at least one criterion applies (we assume criteria application is independent): <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} P(i \in OUT) = 1 - \prod _{c \in C} P(i \in IN_c) \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>    </p>    <p>The loss ratio skews our classification decision to err on the side of inclusion (for <em>lr</em>>1). The expected loss per paper we suffer for an erroneous inclusion of a paper <em>i</em> is <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>), while for an erroneous exclusion it is <em>lr</em> &#x00B7; (1 &#x2212; <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>)). This means that our threshold for classifying a paper as OUT is when these quantities are the same, that is, <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>) = <em>lr</em>/(<em>lr</em> + 1).</p>    <p>Altering the number of votes per worker <em>N<sub>l</sub>     </em>, votes per item <em>J</em>, and number of tests <em>N<sub>t</sub>     </em> will modify the expected price and loss. More tests ideally lead to more accurate workers, more labels mean more accurate classification, and more votes per person enable a more accurate estimation of a worker&#x0027;s accuracy. To analyze price vs loss, we simulate the behavior of the model with various values of <em>N<sub>l</sub>     </em>, <em>J</em>, <em>N<sub>t</sub>     </em>, and apply EM, TruthFider (TF) or Majority Voting (MV) to classify papers, and compute the estimated loss. Since values of <em>N<sub>t</sub>     </em> and <em>J</em> correspond to a cost, we can also get the price tag corresponding to this loss. Out of this set of price/loss points, we can take the pareto-optimal ones and plot them so that authors can decide which one best fits their needs. As discussed there are cost penalties and practical constraints that do not allow us to set these parameters to arbitrarily high values, and values of <em>N<sub>t</sub>     </em> and <em>J</em> above 10 do not generate significant improvements [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>], so the number of reasonable alternatives is fairly small. To simulate the data we need either to make assumptions on the crowd accuracy as well as on criteria power and difficulty, possibly based on prior knowledge, or to estimate these parameters [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0021">21</a>] by crowdsourcing labels for a few papers (fifty papers already enable a good estimate as shown later). Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig1">1</a> shows the results of applying the three mentioned algorithms for 3 and 5 labels per item and criterion (the caption describes simulation parameters). The impact of choosing a specific algorithm is relatively small with the exception of MV performing better when labels per paper and per worker are few, which is a known behavior [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0016">16</a>]. The dots represent different number of tests (from 1 to 10) and the arrows shows the direction of growth, from top-left to bottom-right. Some points are Pareto-optimal, so in an interaction with SLR authors we would only show those points and ask for the preferred loss/price point. <figure id="fig1">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186036/images/www2018-45-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Performance of classification algorithms. Simulation with 1000 papers, four criteria of power = [<em>c</em>1 = 0.14, <em>c</em>2 = 0.14, <em>c</em>3 = 0.28, <em>c</em>4 = 0.42], <em>Nt</em> = [2, 3, .., 10], <em>lr</em> = 5. Workers are assumed to be cheaters with probability 0.3, and the rest has uniform accuracy in (0.5-1). Accuracy on OUT papers are 10% higher, as seen in experiments.</span>      </div>     </figure>    </p>    <p>The results vary slightly if the parameters of the problem and algorithms are different (such as different power, difficulty distributions across criteria, proportion of papers to be excluded, number of papers per worker). We discuss how quality and cost vary later in the paper when we compare and discuss algorithms.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Multi-Run Strategy by Criteria</h3>     </div>    </header>    <p>     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186036/images/www2018-45-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>The multi-run strategy follows the footsteps of the above-mentioned approaches for query optimization in crowd databases that identify the most selective criteria and query based on those first. The difference here is that we also estimate and consider accuracy (we do not want to query the crowd if this brings high disagreement, as it is less cost-effective), and that we work with a specified loss function and a price vs loss trade-offs that are based on the authors&#x2019; choice. The algorithms proceeds as follows.</p>    <p>     <strong>Baseline iteration.</strong> We first estimate power and difficulty via a <em>baseline iteration</em> (run <em>k</em> = 0) on a randomly selected subset <em>I</em>     <sup>0</sup> of the set of candidate papers <em>I</em>, as shown in Algorithm&#x00A0;1 (We will get back later in the paper about identifying how large should <em>I</em>     <sup>0</sup> be).</p>    <p>In step 4 we classify items and estimate accuracy of each worker with a classification algorithm that also provides accuracy estimates such as TruthFinder (TF) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0007">7</a>]. As TF estimates class probabilities, we estimate the power of a criterion <em>c</em> as the expected value of the probability that the criterion applies, as shown in step 7. We refer instead to the difficulty of a criteria <em>c</em> through the average workers&#x2019; accuracy on the given criteria, i.e., the average probability that a user, who passed the test questions, gives correct votes on that criteria.</p>    <p>     <strong>Criteria ranking.</strong> Finding the best ordering is trivial if one criterion is more powerful <em>and</em> easier than another. Otherwise, different ordering may lead to price/loss points that are on the Pareto frontier and need to be shown to authors for decision. The number of criteria is often low so that considering permutations of all cases where the ordering is not trivial is tractable. We do so in step 8, by computing for each ordering the expected price and loss for different values of <em>N<sub>t</sub>     </em> and <em>J</em>. The computation of price and loss can be done as for the previous algorithm. Notice that the ordering is very important: given an ordering of criteria <em>OC</em> = <em>c</em>     <sub>0</sub>, <em>c</em>     <sub>1</sub>, ..<em>c<sub>n</sub>     </em>, the probability of erroneously excluding an item (probability of false exclusion, or PFE) is the probability of erroneously excluding it in the first round (on <em>c</em>     <sub>0</sub>), plus the probability of correctly including it after <em>c</em>     <sub>0</sub> but erroneously excluding it after <em>c</em>     <sub>1</sub>, and so on. More formally, denoting with <em>PFE<sub>c</sub>     </em> the probability of erroneous exclusion when processing criteria <em>c</em> and with <em>PIN<sub>c</sub>     </em> the probability of classifying a paper as IN on criteria <em>c</em>: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} PFE = PFE_0+\sum \limits _{m \in 1,2..n} PFE_{m} \prod _{j=0}^{m-1} PIN_{j} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>    </p>    <p>PFE therefore decreases with PIN, and in practice it decreases sharply if we screen high power criteria first, given that criteria powers over 30% are quite common.</p>    <p>     <strong>Crowdsourcing iteration.</strong> The algorithm iterates through the criteria, excluding items (classified again based on TF in step 12).</p>    <p>The results of M-runs (orange) compared with the baseline single run algorithm (blue) are shown in Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> b and &#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> c, showing loss and precision vs price for different values of <em>N<sub>t</sub>     </em> and <em>J</em>. The simulation parameters are the same as previously described. The savings are of approximately 20%, and are in general higher if the criteria diversity in terms of power and accuracy is higher.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Short Adaptive Multi-Run</h3>     </div>    </header>    <p>The previous algorithms apply the same strategy to all papers left to classify. The Short Adaptive Multi-Run algorithm (SM for short)(Algorithm&#x00A0;2) defines instead an <em>individual</em> strategy for each item to be labeled, aimed at identifying the shortest path to decision. The idea is that as we collect votes we understand more about the statistical properties of the overall SLR task (such as criteria power and difficulty) and also of each specific paper, based on the votes obtained for that paper so far. Therefore, we can estimate which is the criterion to test next for each paper by maximizing the probability of (correctly) classifying it as out in the next run, and we can even decide to give up on a paper (leaving it in) because we realize it is too hard (or too expensive) for the crowd to reach consensus or because the probability that we will classify it as out are low. In other words, we aim at excluding the papers for which we can do so cheaply and confidently, and leave the rest to the experts (authors).</p>    <p>At an extreme we would like each run to be composed of one vote on one paper for one criterion (hence the name &#x201D;short run&#x201D;). Every time we get a vote we learn something new, and we can use this knowledge to optimize the next vote we ask. In practice a run cannot ask for one vote if we use the basic setup of typical crowdsourcing engines (it would not make sense to take time out of a person to explain a task and a criterion and stop after one vote)<a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn4" id="foot-fn4"><sup>4</sup></a>.</p>    <p>In the following we introduce SM (see Algorithm&#x00A0;2) by first presenting the intuition behind each step and then showing the related math.</p>    <p>We begin at iteration 0 with an empty set of classified items, both in and out: <span class="inline-equation"><span class="tex">$CI^0_{in} \cup CI^0_{out} = \emptyset$</span>     </span>. We assume that authors set thresholds for false inclusion and exclusions, that is, values <span class="inline-equation"><span class="tex">$\overline{P_{out}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\overline{P_{in}}$</span>     </span> so that we classify a paper <em>i</em> as out if <span class="inline-equation"><span class="tex">$P(i \in OUT) \ge \overline{P_{out}}$</span>     </span>, and analogously for <em>P</em>(<em>i</em> &#x2208; <em>IN</em>). Notice therefore that in SM the authors set the desired precision (as we will see, possibly at the expense of price and recall, but precision is typically non negotiable in SLR as false exclusions are costly).</p>    <p>     <strong>Baseline estimation</strong>. We perform a small baseline run as in the previous approach, to estimate power <span class="inline-equation"><span class="tex">$\hat{\theta }^0_c$</span>     </span> and difficulty (accuracy) <span class="inline-equation"><span class="tex">$\hat{\alpha }^0_{c}$</span>     </span> for each criterion (Algorithm 2, step 2). Experiments have shown us that a baseline of 50 items is often sufficient as an initial estimate (as discussed in the following section), considering also that we revise the estimates as we proceed.</p>    <p>     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186036/images/www2018-45-img2.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>     <strong>Exclusion probability estimation</strong>. Here we begin the iterations. Before each run of crowdsourcing we try to identify, for each item, and given the votes <em>V<sub>i</sub>     </em> obtained so far for each paper <em>i</em>, which criterion is more likely to <em>efficiently</em> filter a paper. In other words, we identify for each criterion <em>c</em> the minimal number <span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span> of successive out votes we need so that <em>if</em> we add <span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span> to <em>V<sub>i</sub>     </em> (resulting in a &#x201D;imaginary&#x201D; set of votes <span class="inline-equation"><span class="tex">$V^{\prime }_{i}$</span>     </span>) <em>then</em>     <span class="inline-equation"><span class="tex">$P(i \in OUT | V^{\prime }_{i}) {\gt} \overline{P_{out}}$</span>     </span>, and therefore we exclude the paper and stop working on it. Intuitively, for each item we want to select criteria that have a low <span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span> (low number of votes and therefore low cost) and a high probability <span class="inline-equation"><span class="tex">$P(N^{min}_{i,c})$</span>     </span> of getting those out votes.</p>    <p>Notice that every vote on (paper <em>i</em>, criterion <em>c</em>) we get will move <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>) closer or further away from the threshold <span class="inline-equation"><span class="tex">$\overline{P_{out}}$</span>     </span>. This will change our <em>N<sup>min</sup>     </em> and possibly the selected criterion for the next round. The probability of getting an out vote for (<em>i</em>, <em>c</em>) also changes, and it does so more strongly when the accuracy for that criterion is higher.</p>    <p>More formally we proceed as follows. If we denote with <em>k</em> the number of iterations run thus far, and with <span class="inline-equation"><span class="tex">$V^k_{i,c}$</span>     </span> the votes obtained in the first k runs, then by applying Bayesian rule we have: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P^k(i \in IN_c | V^k_{i,c})= \frac{P^k(V^k_{i,c} | i \in IN_c) * (1-\hat{\theta }^{k-1}_c)}{P^k(V^k_{i,c})} \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>    </p>    <p>In the formula, after the first run (k=1), the term <span class="inline-equation"><span class="tex">$\hat{\theta }^{k-1}_c$</span>     </span> is the proportion of papers to which criteria <em>c</em> applies, as computed after the baseline. <span class="inline-equation"><span class="tex">$\hat{\theta }_c$</span>     </span> is then updated after each run.</p>    <p>The two <em>P<sup>k</sup>     </em> factors on the right side of Equation <a class="eqn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#eq4">4</a> can be determined as follows, where <span class="inline-equation"><span class="tex">$J^c_{i,in}$</span>     </span> denotes the number of items <em>i</em> labeled as <em>in</em> for criterion <em>c</em>: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P^k(V^k_{i,c} | i\in IN_c) =\binom{J^c_i}{J^c_{i,in}} (\overline{\alpha }_c)^{J^c_{i,in}}*(1-\overline{\alpha }_c)^{J^c_{i,out}} \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>    </p>    <p>and <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P^k(V^k_{i,c}) =P^k(V^k_{i,c}|i \in IN_c) * (1-\hat{\theta }^{k-1}_c) + \\P^k(V^k_{i,c}|i \in OUT_c) * \hat{\theta }^{k-1}_c \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div>    </p>    <p>Now that we know how to compute <span class="inline-equation"><span class="tex">$P^k(i\in IN_c | V^k_{i,c})$</span>     </span> and therefore <span class="inline-equation"><span class="tex">$P(i \in OUT | V^k_{i,c})$</span>     </span> from Equation <a class="eqn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#eq2">2</a>, we can compute how the exclusion probability changes as we add n=1,2,.. out votes to <span class="inline-equation"><span class="tex">$V^k_{i,c}$</span>     </span> obtaining a set we denote as <span class="inline-equation"><span class="tex">$V^{k \leftarrow n}_{i,c}$</span>     </span> and stop when <em>n</em> is such that <span class="inline-equation"><span class="tex">$P(i \in OUT |V^{k \leftarrow n}_{i,c}) {\gt} \overline{P_{out}}$</span>     </span>.</p>    <p>To assess the probability of getting <span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span> out votes for criteria <em>c</em> on item <em>i</em> we proceed by first computing the probability that the next vote is out, as follows<a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn5" id="foot-fn5"><sup>5</sup></a> (all probabilities are conditional to the votes obtained thus far <span class="inline-equation"><span class="tex">$V^k_{i,c}$</span>     </span>):</p>    <p>     <span class="inline-equation"><span class="tex">$P(v^{k+1}_{i,c}=OUT)$</span>     </span> = <span class="inline-equation"><span class="tex">$\alpha _c * (1-P^k_i (I \in IN_c)) + (1- \alpha _c)P^k_i (I \in IN_c))$</span>     </span>.</p>    <p>We then iterate over this formula for getting the probability for the next out votes, remembering that <span class="inline-equation"><span class="tex">$P^k_i (I \in IN_c)$</span>     </span> will have changed due to the additional out vote. <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186036/images/www2018-45-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Behavior of algorithms. Charts are simulated with 1000 papers, four criteria of power = [<em>c</em>1 = 0.14, <em>c</em>2 = 0.14, <em>c</em>3 = 0.28, <em>c</em>4 = 0.42], <em>Nt</em> = [2, 3, .., 10], <em>lr</em> = 5. Workers are assumed to be cheaters with probability 0.3, and the rest has uniform accuracy in (0.5-1). Accuracy on OUT papers are 10% higher, as seen in experiments. See text for description.</span>      </div>     </figure>    </p>    <p>     <strong>Ranking.</strong> We rank criteria for each item by weighing cost (<span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span>) and probability of success (probability <span class="inline-equation"><span class="tex">$P(V^{k+1,k+n}_{i,c}=OUT)$</span>     </span> of getting <span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span> consecutive out votes). We define the value of applying a criterion as the price we have to pay for unit of probability of classifying the item as out in the next <span class="inline-equation"><span class="tex">$N^{min}_{i,c}$</span>     </span> votes, that is: <span class="inline-equation"><span class="tex">$Value_{i,c} = P(V^{k+1,k+n}_{i,c}=OUT) / N^{min}_{i,c}$</span>     </span> We then borrow ideas from predicate ranking optimization in query processing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0012">12</a>] that essentially ranks based on selectivity/cost (although here we do so per item and assess it at each iteration). Applying the same logic we look for each paper for the criterion with maximum value: <span class="inline-equation"><span class="tex">$Value_{i} = \max \limits _{c \in C} P(V^{k+1,k+n}_{i,c}=OUT) / N^{min}_{i,c}$</span>     </span>    </p>    <p>In developing SM we explored alternative approaches: a main one we explored involves estimating how <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>) is likely to change if we ask for one vote on <em>c</em>, as an attempt to drive our choice for which vote to ask next. With relatively simple math, we can estimate the probability of the next vote being in or out, and the impact that this has on <em>P</em>(<em>i</em> &#x2208; <em>OUT</em>), and we can select the criteria that leads us closer to the threshold. This initial choice however has an undesired behavior: if there is a low accuracy, high power criterion, it leads us to choosing this criterion. However, the low accuracy means we only take little steps towards our threshold, making the walk long and expensive. Instead, we choose criteria that can provide large variations towards the out threshold.</p>    <p>     <strong>Stopping.</strong> As we iterate, we can see that <em>Value<sub>i</sub>     </em> may be so low (for example, if we get conflicting votes) that it becomes ineffective to poll the crowd for that item. We can therefore stop working on papers for which <em>Value<sub>i</sub>     </em> is lower than a threshold based on authors&#x2019; preferences (Notice that we disregard the money already spent on a paper, as that is a <em>sunk cost</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0001">1</a>]). The reasonable threshold here depends on the cost ratio <em>cr</em> of the crowd cost for a single vote on one paper and criterion (PPL from Formula <a class="eqn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#eq1">1</a>) divided by the author classification cost. The lower the cost ratio, the more convenient it is to insist with the crowd. For typical cost ratios, considering classification costs as estimated in the literature (see, e.g., [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0028">28</a>]) of around 2<font style="normal">&#x0024;</font> per abstract (for the US, in the medical field and including overhead), a good empirically set threshold is 100. we do not discuss this threshold further here but refer the interested reader to <em><a class="link-inline href="http://jointreserch.net" target="_blank">http://jointreserch.net</a></em> for details.</p>    <p>     <strong>Crowdsourcing iteration.</strong> Ranking determines the priority for the next batch of votes. The batch size is the minimal size that can practically be achieved while ensuring each worker gets value for the time they spend learning and doing the task. In practice, it rarely makes sense to offer batches of less than 10 items as they are less attractive. We return to the crowd to ask one more vote for each paper in the batch, determine the probability of exclusion as discussed above and classify paper as out if <span class="inline-equation"><span class="tex">$P(i \in OUT) {\gt} \overline{P_{out}}$</span>     </span>. If there are no more paper left to classify we stop, else we iterate.</p>    <p>We next analyze the results of the algorithm and discuss its properties, also in light of crowdsourcing experiments.</p>    </section>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Analysis and Experiments</h2>    </div>    </header>    <p>    <strong>Simulations.</strong> We first show the behaviors of algorithms via simulations. The strategies presented here have a number of parameters and the behavior varies in interesting ways as we change these parameters. Here we limit to point out some aspects we found particularly interesting and provide an in-depth analysis online for the interested reader, along with the code to replicate both simulation and analysis of experiments<a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn6" id="foot-fn6"><sup>6</sup></a>. We also remind that authors do not set or estimate any parameter: they simply need to state their loss function and the preference for given loss vs price points when there is no Pareto-optimal value.</p>    <p>Figure <a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> shows the result of a simulation run with 1000 papers and parameters as described in the caption. It plots the loss vs price curve for the SM strategy for the same scenario discussed for the other algorithms (The SM variant adopted here has a 1000 papers run, assumes a stopping threshold of 100, and shows an average of 50 simulations). <span class="inline-equation"><span class="tex">$\overline{P_{out}}$</span>    </span> is 0.99. Figure <a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a>(a) and (b) show that SM can achieve the same loss and precision for a fraction of the cost (both could improve by changing <span class="inline-equation"><span class="tex">$\overline{P_{out}}$</span>    </span>, though increasing the price)<a class="fn" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fn7" id="foot-fn7"><sup>7</sup></a>. Notice that price and loss both decrease (at least initially) as we increase the number of tests <em>N<sub>t</sub>    </em>, which is our &#x201D;knob&#x201D; to increase accuracy (and cost) of workers. This is because SM detects the increased accuracy and adapts to it by asking for less votes for the same loss and precision. Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> c shows the ROC curve where we can see that SM has a greater area for a much smaller price. Charts are analogous in terms of shapes and trends for other values of <em>&#x03B8;</em>, <em>J</em> (and for <em>N<sub>t</sub>    </em> as well for the ROC curve) so we do not show them. Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> d shows again loss vs price but this time assuming the presence of a very difficult criterion (accuracy of 0.55), and shows the robustness of SM on the loss (even with the very conservative loss ratio of 5 we used here). <figure id="fig3">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186036/images/www2018-45-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Classification task for SLR.</span>     </div>    </figure>    </p>    <p>Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> e and &#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig2">2</a> f show the impact of estimation error for accuracy on precision and recall respectively. Notice that if we underestimate (orange line) we achieve higher precision (we are more conservative). For recall, if we underestimate accuracy and accuracy is low (<em>N<sub>t</sub>    </em> is low) then we get very low recall: we give up rather early, leaving papers to authors to classify. As accuracy increases, the differences smooth out and are within the variance. The charts for power estimation error have essentially the same shape and are not shown.</p>    <p>Baseline runs and numbers of labels per paper affect the estimation errors. The issue is not so much the number of papers in the baseline: 40-50 papers suffice to estimate power within a 5-7% margin of error (consider the problem similar to estimating the fairness of a coin modeled as a Beta distribution, 50 tosses would give a reasonable estimate). Furthermore, estimates are re-assessed as we go. The key here is rather to enable a good accuracy estimation, and experiments have shown that with less than 3 votes per paper the estimation error grows above 10% and with low accuracy criteria this can generate very low recall (as we may believe accuracy to be at 0.5). Experiments with variations of the stopping threshold also produced limited effect. When going from 100 to 150, the recall increased by approximately 0.04%, always keeping the precision threshold at 0.99. The price difference is also negligible. <figure id="fig4">     <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186036/images/www2018-45-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Behavior of SM with data from experiments. Runs of 1000 papers, accuracy and power as described in the text, the other parameters are unchanged.</span>     </div>    </figure>    </p>    <p>    <strong>Experiments.</strong> Between January and September 2017 we performed a set of studies and experiments on two commercial crowdsourcing engines (CrowdFlower and Mechanical Turk). We ran a total of 20 experiments with different settings, asking workers to label a total of 174 papers with two to four exclusion criteria (a total of 514 classification decisions) taken from two systematic reviews, one done by us in an interdisciplinary area (computer science and social sciences) reviewing technology for fighting loneliness (reference omitted for double blind), and the other in medicine [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0038">38</a>] having more complex exclusion criteria. We collected votes by over 3200 respondents. These initial studies helped us to understand the nature of the problem, estimate crowd accuracies, get a feeling for latencies and costs, and also refine task design which, although orthogonal to our goals here, is important for getting good results [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0042">42</a>]. In the following we focus on the experiments to assess the validity of SM with respect to other algorithms and baselines.</p>    <p>    <em>Setup.</em> To this end we classified 374 papers on AMT by posting tasks that were asking crowd workers to classify many papers based on one criteria. We requested workers with an HIT approval rate of 70% or more. The task starts by explaining the criteria to workers, providing a positive and a negative example, and then asking to label the papers as in, out, or unclear from the text (Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig3">3</a>). Adding the latter option was a result of previous experiments where many workers complained that this option was missing and were unsure about what to answer to qualify for payment. We also informed workers that they would need to get 75% of the answers as correct in order to be paid. The examples are taken from the &#x201D;gold&#x201D; dataset, which are the papers classified by researchers in our team. Each worker saw the same example papers in the instructions for the same criteria. The three criteria we tested involved assessing whether the paper included an <em>intervention</em> (as shown in Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig3">3</a>), whether it described studies on adults <em>65 and older</em>, and whether it involved the use of <em>technology</em>.</p>    <p>The task proceeded by criteria, not by paper: we showed instruction for a criterion, and then asked for classification on that criterion. We chose this option as we noticed in initial experiments that explaining and understanding criteria takes effort, and teaching workers the subtleties of several criteria at the same time may lead to increased effort and reduced accuracy. Workers could classify the papers until they wished to do so. We repeated the process for the three criteria getting at least 5 votes per paper (we collected up to 15 votes per paper for the intervention criterion show in the figure as it had low accuracy and we wanted to analyze it more deeply). Before running the task we did some pre-runs to assess the proper pay. In terms of costs, we experimented different payments, always making sure that we stay well over 8USD per hour based on estimated completion times, which results in approximately 10cents per vote. We did not screen workers with test questions, though the experiments gave us a dataset over which we can now use to &#x201D;simulate&#x201D; the effect of filtering out workers that did not get 100% accuracy on the first <em>N<sub>t</sub>    </em> questions. The dataset is publicly available online at <em>jointresearch.net</em>.</p>    <p>    <em>Results.</em> All tasks were completed in a few hours, and we assume that if we had more papers they would have been classified with sublinear increment in time. As expected (see table below), and consistently with the literature &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0028">28</a>], power and workers&#x2019; accuracy vary significantly by criteria. Use of technology has high power as words related to technology are very common and it is hard for keyword-based queries to filter for the specific use of technology we look for.</p>    <div class="table-responsive" id="inltbl1">    <table class="table">     <thead>      <tr>       <th style="text-align:left;">Criteria:</th>       <th style="text-align:center;">       <em>intervention</em>       </th>       <th style="text-align:right;">       <em>use of tech</em>       </th>       <th style="text-align:right;">       <em>65 and older</em></th>      </tr>     <thead>     <tbody>       <tr><td>Power</td>       <td>0.24</td>       <td>0.61</td>       <td>0.05</td>       </tr>       <tr>       <td>Accuracy</td>       <td>0.60</td>       <td>0.77</td>       <td>0.75</td>      </tr>     </tbody>    </table>    </div>    <p>The presence of a criteria (intervention) with rather low accuracy underscores the importance of an adaptive approach where we focus on high-accuracy criteria and leave the leftover papers to the authors.</p>    <p>Another interesting finding was that nearly half of the papers erroneously classified by the crowd were either errors in the gold data (our error) or they were cases where after reading in detail the abstract we were unsure ourselves. This prompted us to study a bit deeper average agreement among expert raters. Mateen et al report on an experiment that measured agreement on around 96% of the papers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0026">26</a>]. An analysis of SLRs conducted by our team reported 92% agreement among two raters and, in addition, 2% of cases where one rater was unsure. This indicates that the <span class="inline-equation"><span class="tex">$\overline{P_{out}}$</span>    </span> precision threshold we picked of 0.99 is in line or even exceeding current standards. We also observed that the mere act of having the need to explain criteria to others (that also demand fairness in job acceptance) forced us to be very precise and indeed, looking back at our own classification, we found errors also due to a certain imprecision in the initial definitions.</p>    <p>Using the experimental data to fuel simulations did not bring significant changes to the charts already discussed, although there are interesting differences and we focus on these in the following, especially to underline the limits of SM. One interesting aspect is that actual data do not precisely and consistently fit the model: workers accuracy cannot always be modeled as i.i.d. variables, and the margin of error in predicting future accuracy from past is rather high, even if we vary the testing patterns. This is not entirely surprising as some workers may improve as they proceed with the task while others may get sloppy or tired, and indeed optimal testing to cater for these issues is an active area of research &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0002">2</a>].</p>    <p>Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig4">4</a> shows the results of such experiments-fueled simulations, assuming no additional tests filtering. Figure&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig4">4</a> a shows the usual loss vs price chart, and the results are fairly consistent with the simulations. Figures&#x00A0;<a class="fig" href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#fig4">4</a> b and c break this down by precision and recall. The latter is particularly interesting as the recall for baseline is somewhat comparable to M-runs, or at least it makes for a non obvious choice for lower level of <em>J</em>. This is probably due to the relatively low accuracy we have in these (relatively untested) set of workers.</p>    <p>Finally we observe that in terms of overall cost, even at 10cents per vote we remain well below author cost for the paper we screen out (from 20 to 40%).</p>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>The SM algorithm seems to have the potential to outperform baselines for finite pool classification problems, and especially for SLR. We also confirm initial findings that crowdsourcing is feasible for paper screening in SLR. We have also explored extensions of this approach to general classification problems, including problems combining crowd and machine classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#BibPLXBIB0020">20</a>]. The work still has many limitations, especially that of improving the estimation of accuracies and extending the model to cover the case where workers&#x2019; accuracy is very &#x201D;noisy&#x201D;.</p>    <p>    <strong>Acknowledgements.</strong> This project has received funding from the EU Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No 690962.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">H. Arkes and C. Blumer. 1985. The psychology of sunk cost. <em>      <em>Organizational Behavior and Human Decision Processes</em>     </em>35, 1(1985).</li>    <li id="BibPLXBIB0002" label="[2]">Jonathan Bragg, Mausam, and Daniel&#x00A0;S. Weld. 2016. Optimal Testing for Crowd Workers. In <em>      <em>Proceedings of the 2016 International Conference on Autonomous Agents &#x0026;#38; Multiagent Systems</em>     </em>(AAMAS &#x2019;16). International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 966&#x2013;974. <a class="link-inline force-break"      href="http://dl.acm.org/citation.cfm?id=2937029.2937066"      target="_blank">http://dl.acm.org/citation.cfm?id=2937029.2937066</a></li>    <li id="BibPLXBIB0003" label="[3]">Andrew&#x00A0;W. Brown and David&#x00A0;B. Allison. 2014. Using Crowdsourcing to Evaluate Published Scientific Literature: Methods and Example. <em>      <em>Plos One</em>     </em>9, 7 (2014).</li>    <li id="BibPLXBIB0004" label="[4]">Perrine Cr&#x00E9;quit, Ludovic Trinquart, Am&#x00E9;lie Yavchitz, and Philippe Ravaud. 2016. Wasted research when systematic reviews fail to provide a complete and up-to-date evidence synthesis: the example of lung cancer. <em>      <em>BMC Medicine</em>     </em>14, 1 (2016), 8. <a class="link-inline force-break"      href="https://doi.org/10.1186/s12916-016-0555-0"      target="_blank">https://doi.org/10.1186/s12916-016-0555-0</a></li>    <li id="BibPLXBIB0005" label="[5]">A.&#x00A0;P. Dawid and A.&#x00A0;M. Skene. 1979. Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm. <em>      <em>Journal of the Royal Statistical Society. Series C Applied Statistics</em>     </em>28, 1(1979).</li>    <li id="BibPLXBIB0006" label="[6]">A.&#x00A0;P. Dempster, N.&#x00A0;M. Laird, and D.&#x00A0;B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. <em>      <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>     </em>39, 1(1977).</li>    <li id="BibPLXBIB0007" label="[7]">Xin&#x00A0;Luna Dong, Laure Berti-Equille, and Divesh Srivastava. 2013. Data Fusion : Resolving Conflicts from Multiple Sources. In <em>      <em>Procs of WAIM2013</em>     </em>. Springer. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-642-36257-6"      target="_blank">https://doi.org/10.1007/978-3-642-36257-6</a></li>    <li id="BibPLXBIB0008" label="[8]">Carsten Eickhoff and Arjen&#x00A0;P de Vries. 2013. Increasing cheat robustness of crowdsourcing tasks. <em>      <em>Information retrieval</em>     </em>16, 2 (2013), 121&#x2013;137.</li>    <li id="BibPLXBIB0009" label="[9]">M.J. Grant and A. Booth. 2009. A typology of reviews: an analysis of 14 review types and associated methodologies. <em>      <em>Health Info Libr J</em>     </em>26, 2 (2009), 91&#x2013;108.</li>    <li id="BibPLXBIB0010" label="[10]">JPT Higginsand&#x00A0;S. Green. 2011. <em>      <em>Cochrane Handbook for Systematic Reviews of Interventions Version 5.1.0</em>     </em>. The Cochrane Collaboration. Available from <a class="link-inline force-break" href="http://www.handbook.cochrane.org">www.handbook.cochrane.org</a>.</li>    <li id="BibPLXBIB0011" label="[11]">A. Haidich. 2010. Meta-analysis in medical research. <em>      <em>Hippokratia</em>     </em>14, Suppl 1 (2010), 29&#x2013;37.</li>    <li id="BibPLXBIB0012" label="[12]">Joseph&#x00A0;M. Hellerstein and Michael Stonebraker. 1993. Predicate Migration: Optimizing Queries with Expensive Predicates. In <em>      <em>Proceedings of ACM SIGMOD</em>     </em>. ACM.</li>    <li id="BibPLXBIB0013" label="[13]">Lorna&#x00A0;K Henderson, Jonathan&#x00A0;C Craig, Narelle&#x00A0;S Willis, David Tovey, and Angela&#x00A0;C Webster. 2010. How to write a Cochrane systematic review. <em>      <em>Nephrology</em>     </em>15, 6 (2010), 617&#x2013;624.</li>    <li id="BibPLXBIB0014" label="[14]">Matthias Hirth, Tobias Ho&#x00DF;feld, and Phuoc Tran-Gia. 2011. Cost-optimal validation mechanisms and cheat-detection for crowdsourcing platforms. In <em>      <em>Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2011 Fifth International Conference on</em>     </em>. IEEE, 316&#x2013;321.</li>    <li id="BibPLXBIB0015" label="[15]">Matthias Hirth, Tobias Ho&#x00DF;feld, and Phuoc Tran-Gia. 2013. Analyzing costs and accuracy of validation mechanisms for crowdsourcing platforms. <em>      <em>Mathematical and Computer Modelling</em>     </em>57, 11 (2013), 2918&#x2013;2932.</li>    <li id="BibPLXBIB0016" label="[16]">Yuan Jin, Mark Carman, Dongwoo Kim, and Lexing Xie. 2017. Leveraging Side Information to Improve Label Quality Control in Crowd-Sourcing. In <em>      <em>Procs of Hcomp2017</em>     </em>. AAAI.</li>    <li id="BibPLXBIB0017" label="[17]">David&#x00A0;R Karger, Sewoong Oh, and Devavrat Shah. 2011. Budget-optimal crowdsourcing using low-rank matrix approximations. In <em>      <em>Communication, Control, and Computing (Allerton), 2011 49th Annual Allerton Conference on</em>     </em>. IEEE, 284&#x2013;291.</li>    <li id="BibPLXBIB0018" label="[18]">David&#x00A0;R Karger, Sewoong Oh, and Devavrat Shah. 2011. Iterative learning for reliable crowdsourcing systems. In <em>      <em>Advances in neural information processing systems</em>     </em>. 1953&#x2013;1961.</li>    <li id="BibPLXBIB0019" label="[19]">Khalid&#x00A0;S Khan, Regina Kunz, Jos Kleijnen, and Gerd Antes. 2003. Five steps to conducting a systematic review. <em>      <em>Journal of the Royal Society of Medicine</em>     </em>96, 3 (2003), 118&#x2013;121.</li>    <li id="BibPLXBIB0020" label="[20]">Evgeny Krivosheev, Boualem&#x00A0;Benatallah Bahareh&#x00A0;Harandizadeh, and Fabio Casati. 2018. Crowd-based Multi-predicate Screening of Papers in Literature Reviews (poster). In <em>      <em>Proceedings of WWW2018</em>     </em>. International World Wide Web Conferences Steering Committee.</li>    <li id="BibPLXBIB0021" label="[21]">Evgeny Krivosheev, Valentina Caforio, Boualem Benatallah, and Fabio Casati. 2017. Crowdsourcing Paper Screening in Systematic Literature Reviews. In <em>      <em>Procs of Hcomp2017</em>     </em>. AAAI.</li>    <li id="BibPLXBIB0022" label="[22]">Edith Law, Krzysztof&#x00A0;Z. Gajos, Andrea Wiggins, Mary&#x00A0;L. Gray, and Alex Williams. 2017. Crowdsourcing As a Tool for Research: Implications of Uncertainty. In <em>      <em>Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</em>     </em>(CSCW &#x2019;17). ACM, New York, NY, USA, 1544&#x2013;1561. <a class="link-inline force-break" href="https://doi.org/10.1145/2998181.2998197"      target="_blank">https://doi.org/10.1145/2998181.2998197</a></li>    <li id="BibPLXBIB0023" label="[23]">Hongwei Li, Bin Yu, and Dengyong Zhou. 2013. Error Rate Analysis of Labeling by Crowdsourcing. In <em>      <em>Procs of ICML2013</em>     </em>.</li>    <li id="BibPLXBIB0024" label="[24]">Chao Liu and Yi&#x00A0;Min Wang. 2012. TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings. In <em>      <em>Procs of ICML2012</em>     </em>. ICML.</li>    <li id="BibPLXBIB0025" label="[25]">Qiang Liu, Alexander&#x00A0;T Ihler, and Mark Steyvers. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 1914&#x2013;1922.</li>    <li id="BibPLXBIB0026" label="[26]">FJ Mateen, J Oh, AI Tergas, NH Bhayani, and BB Kamdar. 2013. Titles versus titles and abstracts for initial screening of articles for systematic reviews. <em>      <em>Clin Epidemiol.</em>     </em>5, 1 (2013).</li>    <li id="BibPLXBIB0027" label="[27]">David Moher, Alessandro Liberati, Jennifer Tetzlaff, Douglas&#x00A0;G Altman, Prisma Group, <em>et al.</em> 2009. Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement. <em>      <em>PLoS med</em>     </em>6, 7 (2009), e1000097.</li>    <li id="BibPLXBIB0028" label="[28]">Michael&#x00A0;L. Mortensen, Gaelen&#x00A0;P. Adam, Thomas&#x00A0;A. Trikalinos, Tim Kraska, and Byron&#x00A0;C. Wallace. 2016. An exploration of crowdsourcing citation screening for systematic reviews. <em>      <em>Research Synthesis Methods</em>     </em>(2016). 1759-2887RSM-02-2016-0006.R4.</li>    <li id="BibPLXBIB0029" label="[29]">Lauren Ng, Veronica Pitt, Kit Huckvale, Ornella Clavisi, Tari Turner, Russell Gruen, and Julian&#x00A0;H Elliott. 2014. Title and Abstract Screening and Evaluation in Systematic Reviews (TASER): a pilot randomised controlled trial of title and abstract screening by medical students.<em>      <em>Systematic reviews</em>     </em>3, 1 (2014), 121. <a class="link-inline force-break" href="https://doi.org/10.1186/2046-4053-3-121"      target="_blank">https://doi.org/10.1186/2046-4053-3-121</a></li>    <li id="BibPLXBIB0030" label="[30]">An&#x00A0;T Nguyen, Byron&#x00A0;C Wallace, and Matthew Lease. 2015. Combining Crowd and Expert Labels using Decision Theoretic Active Learning. <em>      <em>Proceedings of the 3rd AAAI Conference on Human Computation (HCOMP)</em>     </em> (2015), 120&#x2013;129.</li>    <li id="BibPLXBIB0031" label="[31]">Jungseul Ok, Sewoong Oh, Jinwoo Shin, and Yung Yi. 2016. Optimality of Belief Propagation for Crowdsourced Classification. In <em>      <em>Procs of ICML2016</em>     </em>.</li>    <li id="BibPLXBIB0032" label="[32]">Aditya Parameswaran, Hector Garcia-Molina, Hyunjung Park, Neoklis Polyzotis, Aditya Ramesh, and Jennifer Widom. 2012. CrowdScreen: Algorithms for Filtering Data with Humans. In <em>      <em>Proceedings of ACM SIGMOD</em>     </em>. ACM.</li>    <li id="BibPLXBIB0033" label="[33]">Margaret Sampson, Kaveh&#x00A0;G Shojania, Chantelle Garritty, Tanya Horsley, Mary Ocampo, and David Moher. 2008. Systematic reviews can be produced and published faster. <em>      <em>Journal of clinical epidemiology</em>     </em>61, 6 (2008), 531&#x2013;536.</li>    <li id="BibPLXBIB0034" label="[34]">Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjective labelling of venus images. <em>      <em>Advances in neural information processing systems</em>     </em>7 (1995), 1085&#x2013;1092.</li>    <li id="BibPLXBIB0035" label="[35]">Earl Steinberg, Sheldon Greenfield, Dianne&#x00A0;Miller Wolman, Michelle Mancher, Robin Graham, <em>et al.</em> 2011. <em>      <em>Clinical practice guidelines we can trust</em>     </em>. National Academies Press.</li>    <li id="BibPLXBIB0036" label="[36]">Yalin Sun, Pengxiang Cheng, Shengwei Wang, Hao Lyu, Matthew Lease, Iain Marshall, and Byron&#x00A0;C. Wallace. 2016. Crowdsourcing Information Extraction for Biomedical Systematic Reviews. In <em>      <em>4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP): Works-in-Progress Track</em>     </em>. <a class="link-inline force-break" href="http://arxiv.org/abs/1609.010173 pages. arXiv:1609.01017"      target="_blank">http://arxiv.org/abs/1609.010173 pages. arXiv:1609.01017</a>.</li>    <li id="BibPLXBIB0037" label="[37]">Yemisi Takwoingi, Sally Hopewell, David Tovey, and Alex&#x00A0;J Sutton. 2013. A multicomponent decision tool for prioritising the updating of systematic reviews. <em>      <em>Bmj</em>     </em>7191, December (2013), 1&#x2013;8. <a class="link-inline force-break" href="https://doi.org/10.1136/bmj.f7191"      target="_blank">https://doi.org/10.1136/bmj.f7191</a></li>    <li id="BibPLXBIB0038" label="[38]">Nicola Veronese, Silvia Facchini, Brendon Stubbs, Claudio Luchini, Marco Solmi, Enzo Manzato, Giuseppe Sergi, Stefania Maggi, Theodore Cosco, and Luigi Fontana. 2017. Weight loss is associated with improvements in cognitive function among overweight and obese people: A systematic review and meta-analysis. <em>      <em>Neuroscience &#x0026; Biobehavioral Reviews</em>     </em>72 (2017), 87&#x2013;94.</li>    <li id="BibPLXBIB0039" label="[39]">Byron&#x00A0;C Wallace, Anna Noel-Storr, Iain&#x00A0;J Marshall, Aaron&#x00A0;M Cohen, Neil&#x00A0;R Smalheiser, and James Thomas. 2017. Identifying reports of randomized controlled trials (RCTs) via a hybrid machine learning and crowdsourcing approach. <em>      <em>J Am Med Inform Assoc</em>     </em>(2017).</li>    <li id="BibPLXBIB0040" label="[40]">Michael Weiss. 2016. Crowdsourcing literature reviews in new domains. <em>      <em>Technology Innovation Management Review</em>     </em>6, 2 (2016), 5&#x2013;14.</li>    <li id="BibPLXBIB0041" label="[41]">Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier&#x00A0;R Movellan, and Paul&#x00A0;L Ruvolo. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. (2009), 2035&#x2013;2043.</li>    <li id="BibPLXBIB0042" label="[42]">Jie Yang, Judith Redi, Gianluca DeMartini, and Alessandro Bozzon. 2016. Modeling Task Complexity in Crowdsourcing. In <em>      <em>Proceedings of The Fourth AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2016)</em>     </em>. AAAI, 249&#x2013;258.</li>    <li id="BibPLXBIB0043" label="[43]">D. Zhou, J. Platt, S. Basu, and Y. Mao. 2012. Learning from the wisdom of crowds by minimax entropy. In <em>      <em>Procs of Nips 2012</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://www.mturk.com">www.mturk.com</a> and <a class="link-inline force-break" href="http://www.crowdflower.com">www.crowdflower.com</a>   </p>   <p id="fn2"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://www.stat.berkeley.edu/mossel/teach/ SocialChoiceNetworks10/ScribeAug31.pdf">http://www.stat.berkeley.edu/mossel/teach/ SocialChoiceNetworks10/ScribeAug31.pdf</a></p>   <p id="fn3"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn3"><sup>3</sup></a>In this paper we do not consider the problem of accuracies below random, but we stress that they can occur in rare cases, for example if criteria are erroneously specified.</p>   <p id="fn4"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn4"><sup>4</sup></a>With ad hoc implementations, either stand-alone or on top of commercial engines, and with fast estimation it might be possible to achieve one-vote runs though the key optimization here lies in the personalized strategy: the most important aspect is not so much asking at most one vote in each run, but asking one vote per paper.</p>   <p id="fn5"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn5"><sup>5</sup></a>To simplify the presentation here we take a single value for criteria accuracy as opposed to a confusion matrix.</p>   <p id="fn6"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn6"><sup>6</sup></a><a class="link-inline href="http://jointresearch.net" target="_blank">http://jointresearch.net</a></p>   <p id="fn7"><a href="p55-krivosheev.html?ip=84.92.48.26&amp;id=3186036&amp;acc=OPENTOC&amp;key=4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4D4702B0C3E38B35%252E4DD68#foot-fn7"><sup>7</sup></a>We omit plotting the std bars as they would make the chart unreadable.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186036">https://doi.org/10.1145/3178876.3186036</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
