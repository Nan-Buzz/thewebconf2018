<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Content Attention Model for Aspect Based Sentiment Analysis</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Content Attention Model for Aspect Based Sentiment Analysis</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Qiao</span>     <span class="surName">Liu</span>,     University of Electronic Science and Technology of China, No. 4, North Jianshe Road, Chengdu, China 610054, <a href="mailto:qliu@uestc.edu.cn">qliu@uestc.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Haibin</span>     <span class="surName">Zhang</span>,     University of Electronic Science and Technology of China, No. 4, North Jianshe Road, Chengdu, China 610054, <a href="mailto:herb.zhang@std.uestc.edu.cn">herb.zhang@std.uestc.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Yifu</span>     <span class="surName">Zeng</span>,     University of Electronic Science and Technology of China, No. 4, North Jianshe Road, Chengdu, China 610054, <a href="mailto:ifz@std.uestc.edu.cn">ifz@std.uestc.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Ziqi</span>     <span class="surName">Huang</span>,     University of Electronic Science and Technology of China, No. 4, North Jianshe Road, Chengdu, China 610054, <a href="mailto:2016220401037@std.uestc.edu.cn">2016220401037@std.uestc.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Zufeng</span>     <span class="surName">Wu</span>,     University of Electronic Science and Technology of China, No. 4, North Jianshe Road, Chengdu, China 610054, <a href="mailto:wuzufeng@uestc.edu.cn">wuzufeng@uestc.edu.cn</a>    </div>                        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186001" target="_blank">https://doi.org/10.1145/3178876.3186001</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Aspect based sentiment classification is a crucial task for sentiment analysis. Recent advances in neural attention models demonstrate that they can be helpful in aspect based sentiment classification tasks, which can help identify the focus words in human. However, according to our empirical study, prevalent content attention mechanisms proposed for aspect based sentiment classification mostly focus on identifying the sentiment words or shifters, without considering the relevance of such words with respect to the given aspects in the sentence. Therefore, they are usually insufficient for dealing with multi-aspect sentences and the syntactically complex sentence structures. To solve this problem, we propose a novel content attention based aspect based sentiment classification model, with two attention enhancing mechanisms: <em>sentence-level content attention mechanism</em> is capable of capturing the important information about given aspects from a global perspective, whiles the <em>context attention mechanism</em> is responsible for simultaneously taking the order of the words and their correlations into account, by embedding them into a series of <em>customized memories</em>. Experimental results demonstrate that our model outperforms the state-of-the-art, in which the proposed mechanisms play a key role.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Sentiment analysis;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <em>Information extraction;</em> <em>Neural networks;</em></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Sentiment Analysis</small>, </span>     <span class="keyword">      <small> Aspect Based</small>, </span>     <span class="keyword">      <small> Attention Mechanism</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Qiao Liu, Haibin Zhang, Yifu Zeng, Ziqi Huang, and Zufeng Wu. 2018. Content Attention Model for Aspect Based Sentiment Analysis. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018 (WWW 2018),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186001" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186001</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Aspect based sentiment analysis is an important subtask of sentiment analysis (SA), which is also a central concern of the semantic web and the computational linguistics community in recent years [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. The goal of aspect based SA is to identify the aspects of given entities (aspect extraction), and determine the sentiment expressed for each aspect (a.k.a. aspect based sentiment classification,). In this paper, we focus on the problem of aspect based sentiment classification (ABSC), the aim is to determine whether user opinions conveyed in comments/tweets on specific aspects are positive, negative, or neutral [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. As an example, consider the following sample sentence taken from the SemEval 2014 restaurant dataset:</p>    <Quote>    <p>     <em>Looking around, I saw a room full of New Yorkers enjoying a real meal in a real restaurant, not a clubhouse of the fabulous trying to be seen.</em>    </p>    </Quote>    <p>The aspects for this sentence are &#x201C;room&#x201D;, &#x201C;meal&#x201D;, and &#x201C;clubhouse&#x201D;, the expected outputs of the aspect based sentiment classifier are intend to be neutral, positive and negative respectively.</p>    <p>Recent advances in neural network based ABSC models have deeply reshaped the research because of their capability of learning to predict in an end-to-end manner [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>]. In these studies, context words are usually regarded with equal importance across the mentioned aspects. However, in many cases only a subset of the context words would be relevant to the sentiment polarity of a given aspect [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]. As one could see from the above example, the word &#x201C;enjoying&#x201D; is important for determining the sentiment polarity w.r.t the aspect &#x201C;meal&#x201D;, but the word &#x201C;fabulous&#x201D; seems to be irrelevant to it. If the classification model can not differentiate aspect words, it would be problematic for practical use.</p>    <p>To solve this problem, some neural attention models were introduced to this area [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. However, according to our empirical study, there are some common problems shared by these existing neural attention models. <strong>Firstly</strong>, most of the attention modeling strategies in this area only consider a <em>partial</em> of the context information in a sentence without considering the relevance or contribution of each context word to the given aspect, which we consider could be a serious problem since most of the sentiment words and shifters will be focused but not all of them are relative to the given aspect. Take the following example,</p>    <Quote>    <p>     <em>The mini&#x0027;s body hasn&#x0027;t changed since late 2010- and for a good reason.</em>    </p>    </Quote>    <p>In this statement, the words &#x201C;n&#x0027;t&#x201D;, &#x201C;good&#x201D; and &#x201C;late&#x201D; would be captured by a typical attention model as <em>focused words</em>, which indicate an overall negative sentiment on aspect word &#x201C;body&#x201D;. However, one could tell that among these words, only the word &#x201C;good&#x201D; is <em>really</em> associated with the sentimental polarity of the &#x201C;body&#x201D;, therefore the correct sentiment conveyed should be positive. In this study, we argue that such a <em>short-sighted</em> behavior will cause a significant loss in the predictive accuracy of the classifier.</p>    <p>    <strong>Secondly</strong>, most of the existing attention models only consider the &#x201C;words-level&#x201D; attention calculation without taking into account the overall meaning conveyed by the sentence. However, for complex sentences such as ironical or sarcastic statements which are commonly seen in practical user comments, the classifier may need more precise information (about the entire sentence) to predict the correct results. For example, in the following sentence:</p>    <Quote>    <p>     <em>Maybe the mac os improvement were not the product they want to offer.</em>    </p>    </Quote>    <p>a typical attention model would allocate a high attention weight to the word &#x201C;improvement&#x201D;, which is an obvious <em>polar word</em> with positive sentiment tendency closely related to the given aspect &#x201C;mac os&#x201D; in that statement. However, this is apparently an ironic statement, which expresses a negative sentiment on &#x201C;mac os&#x201D;.</p>    <p>    <strong>Thirdly</strong>, a sentence might contain multiple aspects of a given topic. Therefore, each word may have a different importance in a sentence depending on the given aspect. Previous works have taken into consideration this problem with prevalent solutions including the hidden state based model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] and the memory based model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]. All these solutions have their cons. For the hidden state based model, the sentiment feature contained in a word representation is an <em>inexplicit</em> word sequence mixed feature. The memory based model is usually based on position attention mechanism which assumes that a context word closer to the aspect should be more important, this assumption is not true in some cases.</p>    <p>In this paper, we consider all these problems systematically. In order to solve the first and second problems, we propose a <em>sentence-level content attention mechanism</em>. When calculating the attention weights, our model does not only consider the information conveyed by each word and aspect in the sentence, but also considers the whole meaning of the full sentence. Based on this mechanism, our sentence-level content attention module (SAM) can capture the important information about a given aspect from a global perspective and embeds the full sentence into the output embedding vector. The output vector of the SAM can be treated as an aspect-specific sentence representation, we argue that this will improve the ability of the ABSC model to handle complex sentences.</p>    <p>In order to tackle the third problem, we propose a <em>context attention mechanism</em>, which does not only considers the order of the word sequence, but also takes into account the correlations between the words and the aspect. Based on this, our context attention based memory module (CAM) provides a <em>customized memory</em> for each aspect, which will be updated in a sequential manner.</p>    <p>The major contributions of this paper are: (1) We develop a neural <strong>C</strong>ontent <strong>a</strong>ttention <strong>b</strong>ased <strong>a</strong>spect based <strong>s</strong>entiment <strong>c</strong>lassification model called <strong>Cabasc</strong>. The framework is illustrated in Fig. <a class="fig" href="#fig1">1</a>. (2) We propose two novel attention modeling mechanisms to tackle the semantic-mismatch problem discussed above, and we also carry out comparison studies with respect to the proposed model to verify the validity of these mechanisms. (3) We evaluate our model on three datasets, two of which are the laptop and restaurant datasets from SemEval 2014 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], the other one is the twitter dataset introduced by Dong et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. Experimental results show that our model can help improve aspect based sentiment classification accuracy. <figure id="fig1">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">The framework of content attention based aspect based sentiment classification model.</span>     </div>    </figure>    </p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>Aspect based sentiment analysis is a fundamental task in sentiment analysis research field [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>], which includes several core subtasks: aspect extraction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], opinion identification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] and aspect based sentiment classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>].</p>    <p>Some previous studies have try to solve these subtasks jointly [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>], dedicating most of the research work in solving an individual subtask. This is attributed to the fact that the remain research tasks are still challenging. In this study, we focus on improving the attention modeling mechanism for solving the aspect based sentiment classification problem. Some related works are briefly introduced in the following section.</p>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Neural Network Models for ABSC Task</h3>     </div>    </header>    <p>Aspect based sentiment classification aims at determining the sentiment polarity of a given aspect [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. Conventional methods usually come from the computational linguistic community, which are mostly machine learning models based on hand-crafted lexicons and syntactic features [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]. The performance of such models is highly dependent on the quality of the artifact features which is labor intensive. Therefore, recent research has turned its focus on developing end-to-end neural network models.</p>    <p>Recursive neural networks (RecNNs) were firstly introduced into this field by Dong et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>], they propose an adaptive recursive neural network which can adaptively propagate the sentiments of context words to the target. It has been demonstrated that the RecNNs are effective in obtaining sentence representations from the recursive structure of the text [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>], but they may suffer from syntax parsing errors which are common in practice [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>]. In contrast, the recurrent neural networks (RNNs) have been proven to be effective in many (language) sequence learning tasks, hence, most of state-of-the-art solutions are based on RNNs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>].</p>    <p>Tang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>] propose a target-dependent long short-term memory network model (TD-LSTM), which learns representations directly from the left and right context w.r.t. the given aspect by making use of two LSTM networks respectively. Zhang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] used gated neural network structures to model the syntax and semantics in tweets and interaction between the surrounding contexts and the target. These RNNs based models have achieved promising results, but they are computationally expensive. Some other researchers have try to improve the computational efficiency with other deep learning architectures. Vo and Zhang [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] used neural pooling functions to automatically extract features from word embeddings. Tang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] developed a deep memory network based on a multi-hop attention mechanism, which is effective and computationally inexpensive. It is worth mentioning that this model is also a neural content attention model that is closely related to our model, which is discussed in the section below.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Neural Attention Models for ABSC Task</h3>     </div>    </header>    <p>As mentioned in the previous section, most of the neural network models proposed for solving the ABSC problem do not take into account the correlations between the context words and the given aspect. Therefore they easily suffer from the semantic mismatching problem. To solve this problem a variety of attention models have recently been introduced to this area. Attention mechanism has been successfully applied to many natural language processing tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>], such as neural machine translation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>], question answering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>], textual entailment recognizing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>], sentence summarization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] and machine comprehension [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]. A significant advantage of the neural attention model is that it can <em>automatically</em> identify the relevant information w.r.t. a specific target in a source sentence, which can be directly used for improving the quality of the feature extraction results of the neural leaning models [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>].</p>    <p>Some of the representative neural attention models proposed for ABSC task are discussed below. Wang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] proposed an LSTM based single-hop attention model, which takes the concatenations of the aspect and the word embeddings as input and uses the LSTM hidden states for attention computation. Ma et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] propose an interactive attention mechanism, which interactively learns attentions from the context and the aspect. We call these models <em>hidden state based</em> attention models. The neural attention models that are most closely related to this work are probably [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>], we call them <em>memory based position-aware</em> attention models.</p>    <p>Tang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] compute the attentions based on the relative distance of the words w.r.t the given aspect in a sentence. But the authors implicitly assume that a word located closer to the aspect should be given more credits, which we consider is arguable in practice. For instance, in &#x201C;<em>The two waitress&#x0027;s looked like they had been sucking on lemons.</em>&#x201D;, the phrase &#x201C;sucking on lemons&#x201D; is more important than other words such as the closer mention &#x201C;like&#x201D; for determining the sentiment polarity of the aspect &#x201C;waitress&#x201D;. In our model, the proposed context attention mechanism can help alleviate this contradiction by explicitly considering correlation between each context word and the given aspect. Chen et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] propose a recurrent attention mechanism based on <em>customized memory</em> for each aspect to capture sentiment features separated by a long distance, which is similar in part to our memory mechanism. However, due to the nature of the LSTM adopted in their model, it can only consider partial sentence information when calculating attention weights. In our model, the proposed sentence-level content attention mechanism can provide general and effective remedy for the <em>short-sight</em> problem of these <em>memory</em> models.</p>    </section>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Model</h2>    </div>    </header>    <p>The proposed model is introduced in this section. Before dive into the details of our proposed Cabasc model, we first introduce three baseline models used to verify the validity of the above mentioned mechanisms. The training process is also covered in this section.</p>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Baseline Model A (BaseA)</h3>     </div>    </header>    <figure id="fig2">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">The content attention module of BaseA model.</span>     </div>    </figure>    <p>The baseline model A is a basic model, and the subsequent models are derived from the model. Let <em>S</em> = {<em>s</em>     <sub>1</sub>, <em>s</em>     <sub>2</sub>, &#x2026;, <em>s<sub>i</sub>     </em>, &#x2026;, <em>s</em>     <sub>      <em>i</em> + <em>L</em>     </sub>, &#x2026;, <em>s<sub>N</sub>     </em>} denote an input sentence which consists of <em>N</em> words and <em>S<sub>a</sub>     </em> = {<em>s<sub>i</sub>     </em>, &#x2026;, <em>s</em>     <sub>      <em>i</em> + <em>L</em>     </sub>} denote the given aspect that appears in the input sentence, which consists of <em>L</em> words. The goal of our models is to predict the polarity of sentence <em>S</em> towards the aspect <em>S<sub>a</sub>     </em>.</p>    <p>     <strong>Inputs.</strong> &#x00A0;&#x00A0;&#x00A0;We use <span class="inline-equation"><span class="tex">${\mathbb {L}} \in {\mathbb {R}}^{d \times \left| V \right|}$</span>     </span> to be a word embedding matrix for a vocabulary <em>V</em>, where <em>d</em> is the dimension of a word vector. The matrix can be generated by an unsupervised method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>], a distant-supervised method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>], or a random initialization method. The word <em>s<sub>i</sub>     </em> in the sentence <em>S</em> is mapped into a low-dimensional, real-valued embedding <span class="inline-equation"><span class="tex">${\mathbf {e}}_i \in {\mathbb {R}}^{d}$</span>     </span> formally defined as, <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\mathbf {e}}_i = {\mathbb {L}} {\mathbf {o}}_i \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <strong>o</strong>     <sub>      <em>i</em>     </sub> is the one-hot vector of <em>s<sub>i</sub>     </em>, the length is |<em>V</em>|. After that, we get a list of vectors <strong>E</strong> = {<strong>e</strong>     <sub>1</sub>, <strong>e</strong>     <sub>2</sub>, &#x2026;, <strong>e</strong>     <sub>      <em>i</em>     </sub>, &#x2026;, <strong>e</strong>     <sub>      <em>i</em> + <em>L</em>     </sub>, &#x2026;, <strong>e</strong>     <sub>      <em>N</em>     </sub>} corresponding to sentence <em>S</em>. If aspect <em>S<sub>a</sub>     </em> is a single word the aspect representation <strong>v</strong>     <sub>      <em>a</em>     </sub> is the embedding of aspect word. If <em>S<sub>a</sub>     </em> is a phrase, <strong>v</strong>     <sub>      <em>a</em>     </sub> takes the mean of aspect word embeddings, this simple representation has proven to be effective in a number of tasks, including aspect based sentiment classification task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>].</p>    <p>     <strong>Memory Module.</strong> &#x00A0;&#x00A0;&#x00A0;The vectors in <strong>E</strong> are stacked one after another to construct a long-term memory <strong>M</strong> = {<strong>m</strong>     <sub>1</sub>, <strong>m</strong>     <sub>2</sub>, &#x2026;, <strong>m</strong>     <sub>      <em>N</em>     </sub>}, where <span class="inline-equation"><span class="tex">${\mathbf {M} \in {\mathbb {R}}^{d \times N}}$</span>     </span>. <strong>M</strong> stores information of the input sentence, where the contents of memory are treated as learnable variables and the model learns how to use the memory for prediction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>].</p>    <p>     <strong>Content Attention Module.</strong> &#x00A0;&#x00A0;&#x00A0;Intuitively, words in a sentence have different contributions to the sentiment polarity of a given aspect occurring in the sentence. And the contribution of a word to the sentiment polarity of different aspects in the sentence should be different [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]. Bahdanau et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] use an alignment model (attention model) to search for a set of important positions in a sentence to capture the most relevant information. Inspired by the effectiveness of this method, we use a content attention module (Fig. <a class="fig" href="#fig2">2</a>) to retrieve information that is most relevant to the sentiment polarity of a given aspect <em>S<sub>a</sub>     </em> from the memory <strong>M</strong> constructed above.</p>    <p>For calculating the attention weight of the memory slice <strong>m</strong>     <sub>      <em>i</em>     </sub> of <strong>M</strong>, we use feed forward neural networks with two inputs (FwNN2) to score how important the word <em>s<sub>i</sub>     </em> is to the sentiment polarity of aspect <em>S<sub>a</sub>     </em>. All of the feed forward neural networks share parameters with each other. The score is calculated based on <strong>m</strong>     <sub>      <em>i</em>     </sub> and aspect representation <strong>v</strong>     <sub>      <em>a</em>     </sub>, formally defined as, <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {c_i}={{\mathbf {W}}_{1}}\tanh ({{\mathbf {W}}_{2}}{{\mathbf {m}}_{i}}+{{\mathbf {W}}_{3}}{{\mathbf {v}}_{a}} + {\mathbf {b}}_1) \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where matrices <span class="inline-equation"><span class="tex">${{\mathbf {W}}_{1}}\in {{\mathbb {R}}^{1\times d}}$</span>     </span>, <span class="inline-equation"><span class="tex">${{\mathbf {W}}_{2}}\in {{\mathbb {R}}^{d\times d}}$</span>     </span> and <span class="inline-equation"><span class="tex">${{\mathbf {W}}_{3}}\in {{\mathbb {R}}^{d\times d}}$</span>     </span> and vector <span class="inline-equation"><span class="tex">${\mathbf {b}}_1 \in {{\mathbb {R}}^{d}}$</span>     </span> are model parameters. After obtaining {<em>c</em>     <sub>1</sub>, <em>c</em>     <sub>2</sub>, &#x2026;, <em>c<sub>N</sub>     </em>}, the attention weight <em>&#x03B1;<sub>i</sub>     </em> of <em>s<sub>i</sub>     </em> is computed by <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \alpha _i = \frac{{\rm exp}(c_i)}{\sum _{j=1}^N{\rm exp}(c_j)} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>    </p>    <p>All the calculated attention weights form an attention weight vector &#x03B1; = (<em>&#x03B1;</em>     <sub>1</sub>, <em>&#x03B1;</em>     <sub>2</sub>, &#x2026;, <em>&#x03B1;<sub>N</sub>     </em>) of the memory <strong>M</strong> with given aspect <em>S<sub>a</sub>     </em>. Finally, the aspect-specific sentence representation <strong>v</strong>     <sub>      <em>ns</em>     </sub> is calculated as follows, where <span class="inline-equation"><span class="tex">${\mathbf {v}}_{ns} \in {\mathbb {R}}^{d}$</span>     </span>. <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\mathbf {v}}_{ns} = {\mathbf {M}} { \alpha } \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>     <strong>MLP.</strong> &#x00A0;&#x00A0;&#x00A0;Depth is an important part of deep learning methods the multiple non-linear layers help to yield more abstract and useful representations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. We use an MLP with one hidden layer which is a simple and effective way to increase the depth of the model. The purpose is to use the hidden layer to represent the inputs in a more predictable way [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. The MLP takes the aspect-specific sentence representation <strong>v</strong>     <sub>      <em>ns</em>     </sub> as its input, the module is defined as, <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\mathbf {v}}_{ms} = g({\mathbf {W}}_4 {\mathbf {v}}_{ns} + {\mathbf {b}}_2) \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {v}}_{ms} \in {\mathbb {R}}^{d}$</span>     </span> is the output of the MLP module, <span class="inline-equation"><span class="tex">${\mathbf {W}}_4 \in {\mathbb {R}}^{d \times d}$</span>     </span> is a weight matrix and <span class="inline-equation"><span class="tex">${\mathbf {b}}_2 \in {\mathbb {R}}^{d}$</span>     </span> is a bias vector, <em>g</em>(&#x00B7;) is a non-linear activation function and we use tanh&#x2009; here.</p>    <p>     <strong>Softmax.</strong> &#x00A0;&#x00A0;&#x00A0;The output <strong>v</strong>     <sub>      <em>ms</em>     </sub> of MLP is converted by a linear layer to a real-valued vector of length |<em>C</em>|, where <em>C</em> is the collection of possible sentiment categories. Then the vector is fed into a softmax layer to predict the sentiment polarity of the aspect <em>S<sub>a</sub>     </em>, formally, <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} pred = {\rm softmax}({\mathbf {W}}_5 {\mathbf {v}}_{ms} + {\mathbf {b}}_3) \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$pred \in {\mathbb {R}}^{\left| C \right|}$</span>     </span> is a conditional probability distribution over <em>C</em>, weight matrix <span class="inline-equation"><span class="tex">${\mathbf {W}}_5 \in {{\mathbb {R}}^{\left| C \right| \times d}}$</span>     </span> and bias vector <span class="inline-equation"><span class="tex">${\mathbf {b}}_3 \in {{\mathbb {R}}^{\left| C \right|}}$</span>     </span> are parameters for the linear layer.</p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Baseline Model B (BaseB)</h3>     </div>    </header>    <figure id="fig3">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">The sentence-level content attention module of BaseB model.</span>     </div>    </figure>    <p>Content attention module in BaseA calculates an attention weight without considering the whole meaning of the entire sentence. However, considering only part of the sentence information may lead to some focused words not being related to the given aspect. This may affect the model&#x0027;s sentiment polarity prediction. In addition, for complex sentences, the aspect-specific sentence representation produced by content attention module may not have the overall meaning conveyed by the sentence will not be enough for the correct sentiment classification. In order to address these issues, we propose the use of the sentence information to enhance BaseA. The new model is BaseB, which adds two extensions to the content attention module of BaseA by using the sentence-level content attention mechanism, shown in Fig. <a class="fig" href="#fig3">3</a>.</p>    <p>     <strong>Sentence-level Content Attention Module (SAM).</strong> The first extension adds the sentence representation to the calculation of the attention weight. The sentence representation smooths the score of the importance of a word for the sentiment polarity of an aspect from a global perspective, so as to improve the ability to accurately capture important sentiment features. A feed forward neural network with three inputs (FwNN3) is used to calculate score <em>c<sub>i</sub>     </em> of word <em>s<sub>i</sub>     </em>, that is, <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {c_i}={{\mathbf {W}}_{6}}\tanh ({{\mathbf {W}}_{7}}{{\mathbf {m}}_{i}}+{{\mathbf {W}}_{8}}{{\mathbf {v}}_{a}} + {\mathbf {W}}_9{\mathbf {v}}_s + {\mathbf {b}}_4) \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {m}}_i \in {\mathbb {R}}^{d}$</span>     </span> is memory slice of <em>s<sub>i</sub>     </em>, <span class="inline-equation"><span class="tex">${\mathbf {v}}_a \in {\mathbb {R}}^{d}$</span>     </span> is the aspect representation, <span class="inline-equation"><span class="tex">${\mathbf {v}}_s \in {\mathbb {R}}^{d}$</span>     </span> is the sentence representation, <span class="inline-equation"><span class="tex">${\mathbf {W}}_6 \in {\mathbb {R}}^{1 \times d}$</span>     </span>, <span class="inline-equation"><span class="tex">${\mathbf {W}}_7 \in {\mathbb {R}}^{d \times d}$</span>     </span>, <span class="inline-equation"><span class="tex">${\mathbf {W}}_8 \in {\mathbb {R}}^{d \times d}$</span>     </span> and <span class="inline-equation"><span class="tex">${\mathbf {W}}_9 \in {\mathbb {R}}^{d \times d}$</span>     </span> are weight matrices and <span class="inline-equation"><span class="tex">${\mathbf {b}}_4 \in {{\mathbb {R}}^{d}}$</span>     </span> is a bias vector. We take the average of the embeddings of words participating in the sentence as sentence representation to preserve the sentence information, which is proved to be surprisingly effective [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. All of the calculated scores of the sentence are denoted as {<em>c</em>     <sub>1</sub>, <em>c</em>     <sub>2</sub>, &#x2026;, <em>c<sub>N</sub>     </em>}.</p>    <p>After that, the Eq. (<a class="eqn" href="#eq3">3</a>) is used to compute the elements in the attention weight vector &#x03B1; = (<em>&#x03B1;</em>     <sub>1</sub>, <em>&#x03B1;</em>     <sub>2</sub>, &#x2026;, <em>&#x03B1;<sub>N</sub>     </em>) of the sentence <em>S</em>. The output embedding vector <strong>v</strong>     <sub>      <em>ts</em>     </sub> is calculated by, <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\mathbf {v}}_{ts} = {\mathbf {M}}{ \alpha } \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {v}}_{ts} \in {\mathbb {R}}^{d}$</span>     </span>, <strong>M</strong> is the memory built in the memory module.</p>    <p>The second extension is to embed the entire sentence into the output vector which could improve the ability of the model to handle complex sentences. We achieve this by adding the sentence representation <strong>v</strong>     <sub>      <em>s</em>     </sub> to the output vector <strong>v</strong>     <sub>      <em>ts</em>     </sub>, computed as follows, <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\mathbf {v}}_{ns} = {\mathbf {v}}_{ts} + {\mathbf {v}}_s \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> where <strong>v</strong>     <sub>      <em>ns</em>     </sub> is the aspect-specific sentence representation as the output of the sentence-level content attention module in which the noticed sentiment features are highlighted and the sentence information is embedded. <figure id="fig4">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">The position attention based memory module of BaseC model.</span>      </div>     </figure>    </p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Baseline Model C (BaseC)</h3>     </div>    </header>    <p>The same word has same memory slice as generated in the above memory module. However, considering that due to the diversity of the meanings of words, a word may indicate different relative polarity in different contexts and a sentence might contain multiple aspects of a given topic. Therefore, each word may have a different importance depending on the aspect discussed in that sentence. To ease these problems, we use the position attention mechanism [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] to extend the memory module of BaseB to be position attention based memory module, constructing a customized memory for a given aspect of a sentence, as shown in Fig. <a class="fig" href="#fig4">4</a>. The intuition behind the position attention mechanism is that the words around the aspect have a greater impact on the sentiment polarity of the aspect. This model is called BaseC.</p>    <p>     <strong>Position Attention Based Memory Module.</strong> &#x00A0;&#x00A0;&#x00A0;The position of a context word is defined as its absolute distance with the aspect in the sentence and the position of an aspect word is regarded as 0. If aspect is a phrase, then the positions of left context words are calculated with the first word of aspect, whiles the positions of right context words are calculated with the last word. Let <span class="inline-equation"><span class="tex">${ {\lambda }} \in {{\mathbb {R}}^{N}}$</span>     </span> be the position weight vector of sentence <em>S</em>, the <em>i</em>-th element of the vector is calculated as: <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \lambda _i=1-{{p}_{i}}/N \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> where <em>p<sub>i</sub>     </em> is the position of word <em>s<sub>i</sub>     </em> &#x2208; <em>S</em> and <em>N</em> is the sentence length.</p>    <p>The memory <strong>M</strong> is weighted by the position attention weights to produce weighted memory <strong>M</strong>     <sub>      <em>w</em>     </sub> = (<strong>m</strong>     <sub>      <em>w</em>1</sub>, <strong>m</strong>     <sub>      <em>w</em>2</sub>, &#x2026;, <strong>m</strong>     <sub>      <em>wN</em>     </sub>). Memory slice <span class="inline-equation"><span class="tex">$\mathbf {m}_{wi} \in {{\mathbb {R}}^{d}}$</span>     </span> is calculated as: <div class="table-responsive" id="eq11">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {m}_{wi}={{\mathbf {q}}_{i}}\odot {{\mathbf {m}}_{i}} \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {m}}_i \in {\mathbb {R}} ^{d}$</span>     </span> is memory slice of <em>s<sub>i</sub>     </em> in <strong>M</strong>, <span class="inline-equation"><span class="tex">${\mathbf {q}}_i \in {\mathbb {R}}^{d}$</span>     </span> is a vector obtained by tiling <em>&#x03BB;<sub>i</sub>     </em>, that is copying <em>&#x03BB;<sub>i</sub>     </em> to <em>d</em> times to form the <strong>q</strong>     <sub>      <em>i</em>     </sub> vector. &#x2299; means element-wise multiplication. Then <strong>M</strong>     <sub>      <em>w</em>     </sub> is fed into sentence-level content attention module.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Content Attention Based Aspect Based Sentiment Classification Model (Cabasc)</h3>     </div>    </header>    <figure id="fig5">     <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig5.jpg" class="img-responsive" alt="Figure 5"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">The context attention based memory module of Cabasc model.</span>     </div>    </figure>    <p>We introduce the Cabasc model in this subsection. The position attention mechanism has been described in previous subsection, however, the mechanism only calculates the position attention weight based on the relative position of a context word to the aspect, ignoring the correlation between the word and the aspect which is more important than the relative position. In this way, the calculated attention weight is rough and is not flexible enough for this task. To address this deficiency, we propose a context attention mechanism in which the word order information, the aspect information and the correlation between the word and the aspect are modeled into the calculated attention weight. Cabasc is the same as BaseC except the memory module. The memory module of Cabasc is context attention based memory module as shown in Fig. <a class="fig" href="#fig5">5</a>, which differs from position attention based memory module which is used in the BaseC in that the Cabasc module uses a context attention mechanism rather than a position attention mechanism. <figure id="fig6">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">The context attention mechanism.</span>      </div>     </figure>     <strong>Context Attention Based Memory Module (CAM).</strong> &#x00A0;&#x00A0;&#x00A0;The details of this module are visualized in Fig. <a class="fig" href="#fig6">6</a>. We first divide the input sentence <em>S</em> into the left context with aspect part <em>S<sub>ls</sub>     </em> = {<em>s</em>     <sub>1</sub>, &#x2026;, <em>s</em>     <sub>      <em>i</em> &#x2212; 1</sub>, <em>s<sub>i</sub>     </em>, &#x2026;, <em>s</em>     <sub>      <em>i</em> + <em>L</em>     </sub>} and the right context with aspect part <em>S<sub>rs</sub>     </em> = {<em>s<sub>i</sub>     </em>, &#x2026;, <em>s</em>     <sub>      <em>i</em> + <em>L</em>     </sub>, <em>s</em>     <sub>      <em>i</em> + <em>L</em> + 1</sub>, &#x2026;, <em>s<sub>N</sub>     </em>}. The embeddings of the words in these two parts are retrieved from matrix <span class="inline-equation"><span class="tex">${\mathbb {L}}$</span>     </span>, and two lists of vectors <strong>E</strong>     <sub>      <em>ls</em>     </sub> = {<strong>e</strong>     <sub>1</sub>, &#x2026;, <strong>e</strong>     <sub>      <em>i</em> &#x2212; 1</sub>, <strong>e</strong>     <sub>      <em>i</em>     </sub>, &#x2026;, <strong>e</strong>     <sub>      <em>i</em> + <em>L</em>     </sub>} and <strong>E</strong>     <sub>      <em>rs</em>     </sub> = {<strong>e</strong>     <sub>      <em>i</em>     </sub>, &#x2026;, <strong>e</strong>     <sub>      <em>i</em> + <em>L</em>     </sub>, <strong>e</strong>     <sub>      <em>i</em> + <em>L</em> + 1</sub>, &#x2026;, <strong>e</strong>     <sub>      <em>N</em>     </sub>} corresponding to <em>S<sub>ls</sub>     </em> and <em>S<sub>rs</sub>     </em> are obtained respectively.</p>    <p>In order to model the context information between the context words and aspect, as well as the aspect, we use two GRU neural networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], a left one GRU<sub>L</sub> and a right one GRU<sub>R</sub>, to model <strong>E</strong>     <sub>      <em>ls</em>     </sub> and <strong>E</strong>     <sub>      <em>rs</em>     </sub> respectively. The input of GRU<sub>L</sub> is <strong>E</strong>     <sub>      <em>ls</em>     </sub>, we run GRU<sub>L</sub> from right to left. At time step t, the GRU<sub>L</sub> observes an element <strong>e</strong>     <sub>      <em>t</em>     </sub> of <strong>E</strong>     <sub>      <em>ls</em>     </sub> and updates its internal hidden state <strong>h</strong>     <sub>      <em>t</em>     </sub> as follows: <div class="table-responsive" id="eq12">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\bf r}_t = \sigma ({\bf W}_r{\bf e}_t + {\bf U}_r{\bf h}_{t-1}) \end{equation} </span>       <br/>       <span class="equation-number">(12)</span>      </div>     </div>     <div class="table-responsive" id="eq13">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\bf z}_t=\sigma ({\bf W}_z{\bf e}_t + {\bf U}_z{\bf h}_{t-1}) \end{equation} </span>       <br/>       <span class="equation-number">(13)</span>      </div>     </div>     <div class="table-responsive" id="eq14">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\bf \tilde{h}}_t={\rm tanh}({\bf W}_h{\bf e}_t + {\bf U}_h({\bf r}_t\odot {\bf h}_{t-1})) \end{equation} </span>       <br/>       <span class="equation-number">(14)</span>      </div>     </div>     <div class="table-responsive" id="eq15">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\bf h}_t = {\bf z}_t\odot {\bf h}_{t-1} + ({\bf 1}-{\bf z}_t)\odot {\tilde{\bf h}}_t \end{equation} </span>       <br/>       <span class="equation-number">(15)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {W}}_r \in {\mathbb {R}}^{d \times d}$</span>     </span>, <span class="inline-equation"><span class="tex">${\mathbf {U}}_r \in {\mathbb {R}}^{d \times d}$</span>     </span>, <span class="inline-equation"><span class="tex">${\mathbf {W}}_z \in {\mathbb {R}}^{d \times d}$</span>     </span>, <span class="inline-equation"><span class="tex">${\mathbf {U}}_z \in {\mathbb {R}}^{d \times d},{\mathbf {W}}_h \in {\mathbb {R}}^{d \times d}$</span>     </span>, <span class="inline-equation"><span class="tex">${\mathbf {U}}_h \in {\mathbb {R}}^{d \times d}$</span>     </span> are weight matrices and <em>&#x03C3;</em> denotes the logistic sigmoid function. The update gate <strong>r</strong>     <sub>      <em>t</em>     </sub> controls the update extent of the output from a new hidden state <span class="inline-equation"><span class="tex">${\bf \tilde{h}}_t$</span>     </span> and the reset gate <strong>z</strong>     <sub>      <em>t</em>     </sub> controls how much information from the previous hidden state is allowed. After reading <strong>E</strong>     <sub>      <em>ls</em>     </sub>, GRU<sub>L</sub> produces a hidden state list <span class="inline-equation"><span class="tex">${\mathbf {H}}_{ls} = \lbrace {\mathbf {h}}_{{i+L}_l},\dots ,{\mathbf {h}}_{i_l},{\mathbf {h}}_{i-1},\dots ,{\mathbf {h}}_1\rbrace$</span>     </span>. GRU<sub>R</sub> does the same thing, except that it takes <strong>E</strong>     <sub>      <em>rs</em>     </sub> as input and its run from left to right. The hidden state list of GRU<sub>R</sub> is <span class="inline-equation"><span class="tex">${\mathbf {H}}_{rs} = \lbrace {\mathbf {h}}_{i_r},\dots ,{\mathbf {h}}_{{i+L}_r},{\mathbf {h}}_{i+L+1},\dots ,{\mathbf {h}}_N\rbrace$</span>     </span>.</p>    <p>An MLP is used to calculate the attention weight <em>&#x03B2;<sub>l</sub>     </em> of <strong>h</strong>     <sub>      <em>l</em>     </sub>, where <strong>h</strong>     <sub>      <em>l</em>     </sub> is an element of <strong>H</strong>     <sub>      <em>ls</em>     </sub>, formally, <div class="table-responsive" id="eq16">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \beta _l = {\sigma }({\mathbf {W}}_{10} {\mathbf {h}}_l + b_5) + b_l \end{equation} </span>       <br/>       <span class="equation-number">(16)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {W}}_{10} \in {{\mathbb {R}}^{1 \times d}}$</span>     </span> is a weight matrix, <span class="inline-equation"><span class="tex">$b_5 \in {\mathbb {R}}$</span>     </span> is a bias, and <span class="inline-equation"><span class="tex">$b_l \in {\mathbb {R}}$</span>     </span> is a basic attention weight. An MLP that takes an element in <strong>H</strong>     <sub>      <em>ls</em>     </sub> as input is called MLP<sub>L</sub> and all MLP<sub>L</sub>s share parameters. So the reverse attention weight list for <strong>H</strong>     <sub>      <em>ls</em>     </sub> is <span class="inline-equation"><span class="tex">${ \beta }_{ls} = \lbrace \beta _1,\dots ,\beta _{i-1},\beta _{i_l},\dots ,\beta _{{i+L}_l}\rbrace$</span>     </span>. Then the attention weight list <span class="inline-equation"><span class="tex">${ \beta }_{rs} = \lbrace \beta _{i_r},\dots ,\beta _{{i+L}_r},\beta _{i+L+1},\dots ,\beta _N\rbrace$</span>     </span> for <strong>H</strong>     <sub>      <em>rs</em>     </sub> is computed by a set of MLP<sub>R</sub>s. An attention weight <em>&#x03B2;<sub>r</sub>     </em> in &#x03B2;<sub>      <em>rs</em>     </sub> is calculated as follows: <div class="table-responsive" id="eq17">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \beta _r = {\sigma }({\mathbf {W}}_{11} {\mathbf {h}}_r + b_6) + b_r \end{equation} </span>       <br/>       <span class="equation-number">(17)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\mathbf {W}}_{11} \in {{\mathbb {R}}^{1 \times d}}$</span>     </span> is a weight matrix, <span class="inline-equation"><span class="tex">$b_6 \in {\mathbb {R}}$</span>     </span> is a bias, <strong>h</strong>     <sub>      <em>r</em>     </sub> &#x2208; <strong>H</strong>     <sub>      <em>rs</em>     </sub> , and <span class="inline-equation"><span class="tex">$b_r \in {\mathbb {R}}$</span>     </span> is a basic attention weight. Each MLP<sub>R</sub> takes an element of <strong>H</strong>     <sub>      <em>rs</em>     </sub> as input and these MLP<sub>R</sub>s share parameters.</p>    <p>The attention weights corresponding to the left context are extracted from &#x03B2;<sub>      <em>ls</em>     </sub> to construct &#x03B2;<sub>      <em>lc</em>     </sub> = {<em>&#x03B2;</em>     <sub>1</sub>, &#x2026;, <em>&#x03B2;</em>     <sub>      <em>i</em> &#x2212; 1</sub>}, the attention weights corresponding to the right context are extracted from &#x03B2;<sub>      <em>rs</em>     </sub> to construct &#x03B2;<sub>      <em>rc</em>     </sub> = {<em>&#x03B2;</em>     <sub>      <em>i</em> + <em>L</em> + 1</sub>, &#x2026;, <em>&#x03B2;<sub>N</sub>     </em>}. &#x03B2;<sub>      <em>a</em>     </sub> = {<em>&#x03B2;<sub>i</sub>     </em>, &#x2026;, <em>&#x03B2;</em>     <sub>      <em>i</em> + <em>L</em>     </sub>} are the attention weights corresponding to the aspect, one of the elements <em>&#x03B2;<sub>k</sub>     </em> is computed by, <div class="table-responsive" id="eq18">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \beta _k = (\beta _{k_l} + \beta _{k_r}) \times 0.5 \end{equation} </span>       <br/>       <span class="equation-number">(18)</span>      </div>     </div> where <em>i</em> &#x2264; <em>k</em> &#x2264; <em>i</em> + <em>L</em>, <span class="inline-equation"><span class="tex">${\beta _{k_l}} \in { \beta }_{ls}$</span>     </span> and <span class="inline-equation"><span class="tex">${\beta _{k_r}} \in { \beta }_{rs}$</span>     </span>.</p>    <p>We concatenate &#x03B2;<sub>      <em>lc</em>     </sub>, &#x03B2;<sub>      <em>a</em>     </sub> and &#x03B2;<sub>      <em>rc</em>     </sub> as the context attention weight vector &#x03B2; = {<em>&#x03B2;</em>     <sub>1</sub>, <em>&#x03B2;</em>     <sub>2</sub>, &#x2026;, <em>&#x03B2;<sub>N</sub>     </em>}.</p>    <p>The weighted memory <strong>M</strong>     <sub>      <em>w</em>     </sub> = (<strong>m</strong>     <sub>      <em>w</em>1</sub>, <strong>m</strong>     <sub>      <em>w</em>2</sub>, &#x2026;, <strong>m</strong>     <sub>      <em>wN</em>     </sub>) is computed based on &#x03B2;, formally, <div class="table-responsive" id="eq19">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\mathbf {m}}_{wi} = {\mathbf {y}}_i \odot {\mathbf {m}}_i \end{equation} </span>       <br/>       <span class="equation-number">(19)</span>      </div>     </div> where <strong>m</strong>     <sub>      <em>i</em>     </sub> is a memory slice of memory <strong>M</strong> and <span class="inline-equation"><span class="tex">${\mathbf {y}}_i \in {\mathbb {R}}^d$</span>     </span> is a vector obtained by tiling <em>&#x03B2;<sub>i</sub>     </em>, <em>&#x03B2;<sub>i</sub>     </em> &#x2208; &#x03B2;. Accordingly, we take the average of <strong>M</strong>     <sub>      <em>w</em>     </sub> as sentence representation here.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Model Training</h3>     </div>    </header>    <p>Our model is trained to minimize a cross-entropy loss objective in a supervised manner, the loss function is defined by, <div class="table-responsive" id="eq20">      <div class="display-equation">       <span class="tex mytex">\begin{equation} loss=-\sum \limits _{i}{\log }{{p}_{{{t}_{i}}}} \end{equation} </span>       <br/>       <span class="equation-number">(20)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${{p}_{{{t}_{i}}}}$</span>     </span> is the probability of the <em>i</em>-th training example as given by the model. We use back propagation to calculate the gradients of the parameters, and update them with stochastic gradient descent. The dropout technique is used to ease overfitting. We clamp the word embeddings with 300-dimensio-nal Glove<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> vectors [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] for our experiments, which the vocabulary size is 1.9M. We divide the development set from the training set and use it to select the hyper-parameters. The learning rate is set as 0.001. All the parameters in the model are initialized randomly with a normal distribution <em>N</em>(0, 0.05<sup>2</sup>). And the basic attention weights <em>b<sub>l</sub>     </em> and <em>b<sub>r</sub>     </em> are 0.5.</p>    </section>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments and Discussions</h2>    </div>    </header>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Experimental Setting</h3>     </div>    </header>    <p>     <strong>Dataset.</strong> &#x00A0;&#x00A0;&#x00A0;We experiment the proposed models on three publicly available datasets, with two from SemEval 2014 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>], which contain reviews of restaurant and laptop domains. The third one is a twitter dataset collected by Dong et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. The statistics of these datasets are presented in Table <a class="tbl" href="#tab1">1</a>. Following Tang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], we remove conflict category in SemEval 2014 datasets to avoid datasets getting unbalanced. The evaluation metric is classification accuracy.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Statistics of the datasets.</span>     </div>     <table class="table"> 				 <thead>       <tr>       <th style="text-align:center;">Dataset</th>       <th style="text-align:center;"/>       <th style="text-align:center;">Pos.</th>       <th style="text-align:center;">Neg.</th>       <th style="text-align:center;">Neu.</th>       <th>Total</th>       </tr> 				 </thead>      <tbody>       <tr>       <td style="text-align:center;">Laptop</td>       <td style="text-align:center;">train</td>       <td style="text-align:center;">994</td>       <td style="text-align:center;">870</td>       <td style="text-align:center;">464</td>       <td>2328</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">test</td>       <td style="text-align:center;">341</td>       <td style="text-align:center;">128</td>       <td style="text-align:center;">169</td>       <td>638</td>       </tr>       <tr>       <td style="text-align:center;">Restaurant</td>       <td style="text-align:center;">train</td>       <td style="text-align:center;">2164</td>       <td style="text-align:center;">807</td>       <td style="text-align:center;">637</td>       <td>3608</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">test</td>       <td style="text-align:center;">728</td>       <td style="text-align:center;">196</td>       <td style="text-align:center;">196</td>       <td>1120</td>       </tr>       <tr>       <td style="text-align:center;">Twitter</td>       <td style="text-align:center;">train</td>       <td style="text-align:center;">1561</td>       <td style="text-align:center;">1560</td>       <td style="text-align:center;">3127</td>       <td>6248</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">test</td>       <td style="text-align:center;">173</td>       <td style="text-align:center;">173</td>       <td style="text-align:center;">346</td>       <td>692</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Baseline.</strong> &#x00A0;&#x00A0;&#x00A0;We compare our models with the following baseline methods on three datasets:</p>    <p>     <strong>SVM</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]: The classic SVM model using a series of manual features has the best results in SemEval-2014 Task 4.</p>    <p>     <strong>TD-LSTM</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]: The model uses a forward LSTM and a backward LSTM to model the left context with an aspect and right context with an aspect, and concatenates the last hidden states from both directions as sentiment features for sentiment classification.</p>    <p>     <strong>ATAE-LSTM</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]: An LSTM based model which appends each word input vector with the aspect embedding to strengthen the effects of aspects in hidden states, it then uses attention mechanism to generate the final representation from the hidden states.</p>    <p>     <strong>MemNet</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]: A deep memory model which employs multi-hop attention on the memory stacked by input word embeddings. It uses the attention at a previous hop to help calculate more accurate attention distribution at a later hop.</p>    <p>     <strong>IAN</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>]: An LSTM based model which uses attention mechanism to capture important information to generate representations of aspect and context separately by interactive learning.</p>    <p>     <strong>RAN</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]: A state-of-the-art model which adopts recurrent attention to capture sentiment features separated by a long distance on position-weighted memory. The memory is built on the hidden states of a deep bidirectional LSTM.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Main Results</h3>     </div>    </header>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Main results. The results with &#x201C;*&#x201D; are retrieved from the papers of compared methods. &#x201C;N/A&#x201D; means this result is not available.</span>     </div>     <table class="table"> 					 <thead>       <tr>       <th style="text-align:center;">Model</th>       <th style="text-align:center;">Laptops</th>       <th style="text-align:center;">Restaurants</th>       <th>Twitter</th>       </tr> 					</thead>      <tbody>       <tr>       <td style="text-align:center;">SVM</td>       <td style="text-align:center;">70.49<sup>*</sup>       </td>       <td style="text-align:center;">80.16<sup>*</sup>       </td>       <td>N/A</td>       </tr>       <tr>       <td style="text-align:center;">TD-LSTM</td>       <td style="text-align:center;">67.55</td>       <td style="text-align:center;">77.58</td>       <td>70.80</td>       </tr>       <tr>       <td style="text-align:center;">ATAE-LSTM</td>       <td style="text-align:center;">69.27</td>       <td style="text-align:center;">78.50</td>       <td>69.88</td>       </tr>       <tr>       <td style="text-align:center;">IAN</td>       <td style="text-align:center;">72.10<sup>*</sup>       </td>       <td style="text-align:center;">78.60<sup>*</sup>       </td>       <td>N/A</td>       </tr>       <tr>       <td style="text-align:center;">MemNet</td>       <td style="text-align:center;">71.89</td>       <td style="text-align:center;">79.69</td>       <td>69.65</td>       </tr>       <tr>       <td style="text-align:center;">RAN</td>       <td style="text-align:center;">74.49<sup>*</sup>       </td>       <td style="text-align:center;">80.23<sup>*</sup>       </td>       <td>69.36<sup>*</sup>       </td>       </tr>       <tr>       <td style="text-align:center;">BaseA</td>       <td style="text-align:center;">70.84</td>       <td style="text-align:center;">78.83</td>       <td>68.93</td>       </tr>       <tr>       <td style="text-align:center;">BaseB</td>       <td style="text-align:center;">72.25</td>       <td style="text-align:center;">79.46</td>       <td>69.36</td>       </tr>       <tr>       <td style="text-align:center;">BaseC</td>       <td style="text-align:center;">72.72</td>       <td style="text-align:center;">79.73</td>       <td>69.79</td>       </tr>       <tr>       <td style="text-align:center;">Cabasc</td>       <td style="text-align:center;">        <strong>75.07</strong>       </td>       <td style="text-align:center;">        <strong>80.89</strong>       </td>       <td>        <strong>71.53</strong>       </td>       </tr>      </tbody>     </table>    </div>    <p>Table <a class="tbl" href="#tab2">2</a> illustrates the experimental results. Our proposed model (Cabasc) achieves state-of-the-art performances on three datasets. From Table <a class="tbl" href="#tab2">2</a> we can make the following observations.</p>    <p>TD-LSTM performs poorly, implying that the method used by the author to model the preceding and following contexts as aspect-dependent features maybe incapable of capturing the interactions between aspects and contexts. In addition, the hidden state in the last time step contains information about the sequence with a strong focus on the parts close to the aspect word [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>], so the sentiment features of words with a long distance may be forgotten.</p>    <p>Further, the LSTM based model ATAE-LSTM and IAN perform better than TD-LSTM on laptop and restaurant datasets. One main reason maybe the introduction of an attention mechanism that can make the models notice important parts of a sentence for a given aspect. Compared to ATAE-LSTM, IAN has better results because does not only models the context representation, but also models the representation of an aspect by using attention mechanism, which makes better use of aspect information than ATAE-LSTM. MemNet which does not apply the classical recurrent neural network, performs comparably with IAN. It proves the effectiveness of multi-hop attention mechanism. Moreover, MemNet is computationally efficient because its network structure does not have the complex operations as in LSTM. RAN achieves the best performances among the baselines. RAN does not only uses the multi-hop attention mechanism and deep bidirectional LSTM, but also uses position attention mechanism to provide tailor-made memories for different aspects in a sentence and non-linearly combines the results from each computational hops.</p>    <p>Among our proposed models, the Cabasc model obtains the highest classification accuracies on three datasets. The basic BaseA performs the poorly on the three datasets. This is not surprising because it does not consider the correlation between each context word and the given aspect in a sentence, consequently not all of the focused words are relative to the given aspect and may hide the characteristics of the keywords. Compared with BaseA, BaseB achieves 1.41%, 0.63%, 0.43% improvements on three datasets respectively. The improvements demonstrate the effects of sentence-level content attention mechanism which calculates the attention weights from a global perspective by considering the information of the full sentence, and embeds the entire sentence information into the output embedding vector. Now on the basis of BaseB, its extension BaseC outperforms it on the three datasets. As we expect, with a customized memory which considers the position information between aspect and context words, BaseC is able to better predict the sentiment polarity for a given aspect in sentence. It is worth noting that the performance of BaseB is not much worse than BaseC, which shows that the ability of position attention is limited. We argue that it is because the importance of a context word is not only dependent on the word order, but also on the information of context and aspect. Cabasc uses context attention mechanism and outperforms BaseC. The results confirm that the context attention mechanism is more effective than position attention mechanism. The context attention mechanism simultaneously takes into account the order of the words and their correlations to the given aspect in a sentence and benefits the model in generating customized memory in response to each given aspect. This helps to improve the classification accuracy of the model.</p>    <p>Cabasc outperforms RAN which is the recently state-of-the-art model on three datasets, thus showing the effectiveness of our model. In terms of model complexity, Cabasc is simpler and is easier to train than RAN because of there is no multi-hop structure.</p>    <p>Moreover, SVM outperforms TD-LSTM and has an advanced result on restaurant dataset, which demonstrates the importance of high quality features for aspect based sentiment classification.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Comparison of Cabasc and RAN in an approximate production environment</h3>     </div>    </header>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Experiment 1 ans 2 show the results of the dataset divided into 10 or 5 subsets respectively. Ave acc and std dev present the average accuracy and standard deviations of 10 or 5 rounds prediction results. Time is the average time cost for one training iteration. There are three datasets used, laptop (Lap.), restaurant (Res.) and twitter (Twi.).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;"/>       <th style="text-align:center;"/>       <th colspan="3" style="text-align:center;">Experiment 1<hr/>       </th>       <th colspan="3" style="text-align:center;">Experiment 2<hr/>       </th>       </tr>       <tr>       <th style="text-align:left;">Model</th>       <th style="text-align:center;">Metrics</th>       <th style="text-align:center;">Lap.</th>       <th style="text-align:center;">Res.</th>       <th style="text-align:center;">Twi.</th>       <th style="text-align:center;">Lap.</th>       <th>Res.</th>       <th>Twi.</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">RAN</td>       <td style="text-align:center;">ave acc</td>       <td style="text-align:center;">65.20</td>       <td style="text-align:center;">70.06</td>       <td style="text-align:center;">60.92</td>       <td style="text-align:center;">67.67</td>       <td>72.37</td>       <td>63.69</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">std dev</td>       <td style="text-align:center;">1.09e-2</td>       <td style="text-align:center;">6.38e-3</td>       <td style="text-align:center;">8.82e-3</td>       <td style="text-align:center;">4.25e-3</td>       <td>9.27e-3</td>       <td>7.04e-3</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">time(s)</td>       <td style="text-align:center;">4.20</td>       <td style="text-align:center;">3.91</td>       <td style="text-align:center;">2.48</td>       <td style="text-align:center;">5.41</td>       <td>4.93</td>       <td>3.13</td>       </tr>       <tr>       <td style="text-align:left;">Cabasc</td>       <td style="text-align:center;">ave acc</td>       <td style="text-align:center;">67.41</td>       <td style="text-align:center;">71.84</td>       <td style="text-align:center;">62.47</td>       <td style="text-align:center;">69.42</td>       <td>73.49</td>       <td>64.27</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">std dev</td>       <td style="text-align:center;">8.76e-3</td>       <td style="text-align:center;">4.58e-3</td>       <td style="text-align:center;">5.07e-3</td>       <td style="text-align:center;">5.46e-3</td>       <td>4.93e-3</td>       <td>5.98e-3</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">time(s)</td>       <td style="text-align:center;">2.29</td>       <td style="text-align:center;">2.04</td>       <td style="text-align:center;">1.31</td>       <td style="text-align:center;">2.69</td>       <td>2.51</td>       <td>1.74</td>       </tr>      </tbody>     </table>    </div>    <p>Aspect based sentiment analysis has been heavily studied recently because it is widely used in various domains, such as investigating the sentiment of consumer towards products from product reviews [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. As we know, the labeled dataset used in the field of sentiment analysis in academic literatures usually has a larger training set than test set. But labeling the data is quite expensive and labor-intensive, and in production environment the amount of labeled training data is usually smaller than test data.</p>    <p>In order to verify the performance of our proposed Cabasc model and the recently state-of-the-art RAN model in real production environment, we design two experiments to simulate a practical situation. We argue that the experimental results may to some extent reflect their performances in the real production environment. The first one combines the training set and test set as a new dataset, then divides the dataset into 10 subsets where each round successively uses one of the subsets as training set and others as test set. The second one combines the training set and test set and divides the dataset into 5 subsets. The results are shown in Table <a class="tbl" href="#tab3">3</a>. Since the source code of the RAN model is still not publicly available, we used our implementation to test.</p>    <p>Table <a class="tbl" href="#tab3">3</a> Experiment 1 gives the average accuracies, standard deviations of the prediction results of Cabasc and RAN in the first experiment, Table <a class="tbl" href="#tab3">3</a> Experiment 2 gives the results of the second experiment. We find that in both experiments our model has better average accuracies than the RAN model on all the datasets, and lower standard deviations on almost all the datasets. Meanwhile, the time cost of each training iteration about Cabasc is almost half of RAN, and the two experimental results also show that more training data could lead to better performance. Based on the above experimental results and the main results in subsection 4.2, , we advocate that the proposed model is more effective than RAN in aspect based sentiment classification. We argue that it may result from the higher complexity of RAN which makes it is hard to train with less training data, because it adopts multiple computational hops which increase the complexity. The results imply that Cabasc may be more suitable for practical application than RAN.</p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Effects of embedding the entire sentence into the output embedding vector</h3>     </div>    </header>    <p>In this section, we design a series of contrast models to verify the validity of embedding the entire sentence into the output vector:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>BaseA+CAM (un_add)</strong>: Using the CAM in BaseA the same as Cabasc.<br/></li>     <li id="list2" label="&#x2022;"><strong>BaseA+CAM (is_add)</strong>: Based on BaseA+CAM (un_add), the entire sentence is embedded into the output vector.<br/></li>     <li id="list3" label="&#x2022;"><strong>Cabasc (un_add)</strong>: On the basis of Cabasc, but not embedding the entire sentence into the output embedding vector.<br/></li>     <li id="list4" label="&#x2022;"><strong>Cabasc (is_add)</strong>: The Cabasc model proposed in the paper.<br/></li>    </ul>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Test accuracies of whether embedding the entire sentence into the output embedding vector. The &#x201C;is_add&#x201D; and &#x201C;un_add&#x201D; in parentheses of the results indicate whether the entire sentence is embedded.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Method</th>       <th style="text-align:center;">Laptops</th>       <th>Restaurants</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">BaseA+CAM (un_add)</td>       <td style="text-align:center;">72.57</td>       <td>80.17</td>       </tr>       <tr>       <td style="text-align:center;">BaseA+CAM (is_add)</td>       <td style="text-align:center;">73.51</td>       <td>80.44</td>       </tr>       <tr>       <td style="text-align:center;">Cabasc (un_add)</td>       <td style="text-align:center;">72.72</td>       <td>80.53</td>       </tr>       <tr>       <td style="text-align:center;">Cabasc (is_add)</td>       <td style="text-align:center;">75.07</td>       <td>80.89</td>       </tr>      </tbody>     </table>    </div>    <p>The results are shown in Table <a class="tbl" href="#tab4">4</a>. We can see that models in which the entire sentence is embedded into the output embedding vector, have better performances than those without it. This proves that embedding the entire sentence into the output embedding vector can help judge the sentiment polarity of an aspect.</p>    <p>In order to verify the effect of embedding the entire sentence into the output embedding vector to handle complex sentence, we study contrary sentences as representatives of complex sentences. In order to facilitate research, we treat the sentences which have &#x201C;but&#x201D; or &#x201C;however&#x201D;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] as contrary sentences. We count the contrary sentences in laptop test set, and the statistical result as 66, accounting for 10.34% in the test set. By analyzing the classification results of the above experiments, we find that the error rate of contrary sentences in BaseA+CAM (is_add) decreases from 54.54% to 40.90%, and in Cabasc (is_add) decreases from 53.03% to 37.87% compared with each corresponding un_add model, respectively. This indicates that embedding entire sentence into the output vector can help the model to better understand the complex sentence.</p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Effects of different Context Attention Mechanisms</h3>     </div>    </header>    <p>We design and investigate the effects of different context attention mechanisms by replacing the used mechanism of CAM in Cabasc. Fig. <a class="fig" href="#fig7">7</a> (a) shows the performances of the following mechanisms:</p>    <ul class="list-no-style">     <li id="list5" label="&#x2022;"><strong>RCAM</strong>: Making a modifier to CAM in Cabasc which runs the GRU<sub>L</sub> from left to right and GRU<sub>R</sub> from right to left to model the context information.<br/></li>     <li id="list6" label="&#x2022;"><strong>GCAM</strong>: Making a modifier to CAM in Cabasc without using two GRUs to model the left and right context respectively. The module only uses one GRU over the entire sentence and feeds the hidden states into the MLP layer.<br/></li>     <li id="list7" label="&#x2022;"><strong>CAM</strong>: Using the CAM described in subsection 3.4.<br/></li>    </ul>    <p>From Fig. <a class="fig" href="#fig7">7</a> (a), we can see that RCAM outperforms GCAM. It proves the effect of considering the left and right context information respectively. RCAM performs worse than CAM because there is no interaction between aspect and context words in RCAM. CAM achieves the best results,we argue that it may be due to the fact that the CAM simultaneously takes into account the aspect information and the context information between aspect and context words, resulting in aspect-dependent context attention weights which contribute to the aspect based sentiment classification. <figure id="fig7">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">(a) Test accuracies of using different context attention mechanisms in Cabasc. (b) Test accuracies of using different content attention computational methods.</span>      </div>     </figure>    </p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.6</span> Comparison to other content attention computational methods in content attention module</h3>     </div>    </header>    <p>Luong et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] propose three kinds of attention computational methods in the field of machine translation and showed that the General method performs the best. For investigating the effect of these methods in aspect based sentiment classification, we adopt the above three computational methods in the sentence-level content attention module from BaseA+CAM (is_add) which is designed in subsection 4.4 and compare the results with our proposed FwNN2: <div class="table-responsive" id="eq21">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \left\lbrace \begin{array}{lr}\text{s}({{\mathbf {v}}_{a}},{{\mathbf {m}}_{wi}})=\mathbf {v}_{a}^{T}{\mathbf {m}_{wi}} &#x0026; \text{Dot}\\ \text{s}({{\mathbf {v}}_{a}},{{\mathbf {m}}_{wi}})=\mathbf {v}_{a}^{T}{{\mathbf {W}}_{12}}{\mathbf {m}_{wi}} &#x0026; \text{General}\\ \text{s}({{\mathbf {v}}_{a}},{{\mathbf {m}}_{wi}})=\mathbf {v}_{1}^{T}\tanh ({{\mathbf {W}}_{13}}[{{\mathbf {v}}_{a}};{{\mathbf {m}}_{wi}}]) &#x0026; \text{Concat} \end{array} \right. \end{equation} </span>       <br/>       <span class="equation-number">(21)</span>      </div>     </div>    </p>    <p>Fig. <a class="fig" href="#fig7">7</a> (b) shows the results. Dot and General compute the attention weight based on the similarity between the vectors of word and aspect. Concat and FwNN2 are similar because both use the feed forward neural network to compute the potential relatedness [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] between the word and the sentiment polarity of the given aspect. From Fig. <a class="fig" href="#fig7">7</a> (b), we see that Dot and General perform worse than Concat and FwNN2. It may because the potential correlation between words is more suitable than vector similarity in aspect based sentiment classification tasks. Fwnn2 outperforms Concat which may result from the increased parameters in FwNN2, which makes the method more powerful.</p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.7</span> Case Study</h3>     </div>    </header>    <p>We pick some examples and visualize the attention results to show what are noticed by the different attention mechanisms. <figure id="fig8">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">Content attention visualization. The color depth indicates the importance degree of a word. Attention weight in an attention vector is used as the color-coding.</span>      </div>     </figure>    </p>    <p>The FwNN2 and FwNN3 in Fig. <a class="fig" href="#fig8">8</a> present the visualization results of the content attention distributions from BaseA and BaseB (un_add), where (un_add) means not embedding the entire sentence into the output embedding vector. The sentence in Fig. <a class="fig" href="#fig8">8</a> is &#x201C;The mini&#x0027;s body hasn&#x0027;t changed since late 2010- and for a good reason.&#x201D;, in which the corresponding aspect is &#x201C;body&#x201D;. From Fig. <a class="fig" href="#fig8">8</a>, we find that multiple words &#x201C;late&#x201D;, &#x201C;good&#x201D; and &#x201C;n&#x0027;t&#x201D; are focused by content attention module. However, the words &#x201C;n&#x0027;t&#x201D; and &#x201C;late&#x201D; are not related to the polarity of the given aspect of the sentence. As a result, the model makes incorrect prediction &#x201C;negative&#x201D;, which may be because the wrongly focused words hide the sentiment features of &#x201C;good&#x201D;. The sentence-level content attention mechanism is used in BaseB (un_add). The word &#x201C;good&#x201D; which is most relevant to sentiment polarity of &#x201C;body&#x201D; in this sentence makes a significant improvement on attention weight compared with that from BaseA, and the weight of the unrelated word &#x201C;n&#x0027;t&#x201D; is reduced. Accordingly, the model predicts the correct sentiment label &#x201C;positive&#x201D;. This shows that the sentence-level attention mechanism can capture important parts of sentence more accurately.</p>    <p>Fig. <a class="fig" href="#fig9">9</a> visualizes the attention distributions of position attention in BaseC and context attention in Cabasc. The example is &#x201C;The two waitress&#x0027;s looked like they had been sucking on lemons&#x201D; and the aspect is &#x201C;waitress&#x0027;s&#x201D;. Obviously in the position attention mechanism, larger distance between word and aspect, smaller position attention weight of the word. In this case, the keywords &#x201C;sucking&#x201D; and &#x201C;lemon&#x201D; which are important to the sentiment polarity of aspect but each has a so small weight that shows the position attention mechanism maybe rough and not flexible enough. Using context attention mechanism, the context attention weights of the two keywords are not monotonically decreasing by the increasing of relative distance from aspect, and become more reasonable. The results show that the context attention could be better than position attention to construct customized memory for a given aspect. <figure id="fig9">      <img src="http://deliveryimages.acm.org/10.1145/3190000/3186001/images/www2018-10-fig9.jpg" class="img-responsive" alt="Figure 9"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 9:</span>       <span class="figure-title">Position and context attention visualization.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-24">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> CONCLUSION</h2>    </div>    </header>    <p>In this paper, we develop a content attention based aspect based sentiment classification model for the ABSC task. Two novel attention mechanisms, namely sentence-level content attention mechanism and context attention mechanism have been introduced to tackle the semantic-mismatch problem. The sentence-level content attention mechanism captures the important information about the given aspect from a global perspective by considering the information of entire sentence when calculating attention weights, and generates an output vector which embeds the entire sentence to help the model improve its ability of handling complex sentences. Context attention mechanism models the information of word order and the correlations between the words and the aspect into attention weights, the weights are used to generate a customized memory for each aspect. The proposed model is evaluated on three datasets, experimental results demonstrate the validity of the proposed attention mechanisms, and show that the proposed model achieves the state-of-the-art performance.</p>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Acknowledgments</h2>    </div>    </header>    <p>We thank the anonymous reviewers for taking time to read and make valuable comments on this paper. This work was supported by NSFC under grant 61133016 and 61772117, the General Equipment Department Foundation (61403120102), and the Sichuan Hi-Tech industrialization program (2017GZ0308).</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In <em>      <em>Proc. of the 5th ICLR</em>     </em>. CoRR, Toulon, France.</li>    <li id="BibPLXBIB0002" label="[2]">Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In <em>      <em>Proc. of the 3rd International Conference on Learning Representations</em>     </em>. CoRR, Scottsdale, USA.</li>    <li id="BibPLXBIB0003" label="[3]">Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>35, 8(2013), 1798&#x2013;1828.</li>    <li id="BibPLXBIB0004" label="[4]">Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on memory for aspect sentiment analysis. In <em>      <em>Proc. of the 2017 Conference on Empirical Methods in Natural Language Processing</em>     </em>. Association for Computational Linguistics, Copenhagen, Denmark, 463&#x2013;472.</li>    <li id="BibPLXBIB0005" label="[5]">Zhiyuan Chen, Arjun Mukherjee, and Bing Liu. 2014. Aspect extraction with automated prior knowledge learning. In <em>      <em>Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics</em>     </em>. ACL, Baltimore, Maryland, 347&#x2013;358.</li>    <li id="BibPLXBIB0006" label="[6]">Kyunghyun Cho, Bart van Merri&#x00EB;nboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: encoder-decoder approaches. In <em>      <em>Proc. of 8th Workshop on Syntax, Semantics and Structure in Statistical Translation</em>     </em>. Association for Computational Linguistics, Doha, Qatar, 103&#x2013;111.</li>    <li id="BibPLXBIB0007" label="[7]">Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment classification. In <em>      <em>Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</em>     </em>. ACL, Baltimore, Maryland, 49&#x2013;54.</li>    <li id="BibPLXBIB0008" label="[8]">Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi&#x0144;ska, Sergio&#x00A0;G&#x00F3;mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adri&#x00E0;&#x00A0;Puigdom&#x00E8;nech Badia, Karl&#x00A0;Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. 2016. Hybrid computing using a neural network with dynamic external memory. <em>      <em>Nature</em>     </em>538, 7626 (2016), 471&#x2013;476.</li>    <li id="BibPLXBIB0009" label="[9]">Karl&#x00A0;Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In <em>      <em>Advances in Neural Information Processing Systems 28</em>     </em>. Curran Associates, Inc., Montr&#x00E9;al, Canada, 1693&#x2013;1701.</li>    <li id="BibPLXBIB0010" label="[10]">Ozan Irsoy and Claire Cardie. 2014. Opinion mining with deep recurrent neural networks. In <em>      <em>Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Doha, Qatar, 720&#x2013;728.</li>    <li id="BibPLXBIB0011" label="[11]">Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In <em>      <em>Proc. of the 49th Annual Meeting of the Association for Computational Linguistics</em>     </em>. ACL, Portland, Oregon, USA, 151&#x2013;160.</li>    <li id="BibPLXBIB0012" label="[12]">Yoon Kim, Carl Denton, Luong Hoang, and Alexander&#x00A0;M Rush. 2017. Structured attention networks. In <em>      <em>Proc. of the 5th International Conference on Learning Representations</em>     </em>. CoRR, Toulon, France.</li>    <li id="BibPLXBIB0013" label="[13]">Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M.&#x00A0;Mohammad. 2014. NRC-Canada-2014: Detecting aspects and sentiment in customer reviews. In <em>      <em>Proc. of the 8th International Workshop on Semantic Evaluation</em>     </em>. Dublin, Ireland, 437&#x2013;442.</li>    <li id="BibPLXBIB0014" label="[14]">Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In <em>      <em>Proc. of The 33rd International Conference on Machine Learning</em>     </em>, Vol.&#x00A0;48. PMLR, New York, NY, USA, 1378&#x2013;1387.</li>    <li id="BibPLXBIB0015" label="[15]">Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. <em>      <em>Nature</em>     </em>521, 7553 (2015), 436&#x2013;444.</li>    <li id="BibPLXBIB0016" label="[16]">Bing Liu. 2012. Sentiment analysis and opinion mining. <em>      <em>Synthesis Lectures on Human Language Technologies</em>     </em>5, 1(2012), 1&#x2013;167.</li>    <li id="BibPLXBIB0017" label="[17]">Yue Lu and Chengxiang Zhai. 2008. Opinion integration through semi-supervised topic modeling. In <em>      <em>Proc. of the 17th International Conference on World Wide Web</em>     </em>. ACM, Beijing, China, 121&#x2013;130.</li>    <li id="BibPLXBIB0018" label="[18]">Minh-Thang Luong, Hieu Pham, and Christopher&#x00A0;D Manning. 2015. Effective approaches to attention-based neural machine translation. In <em>      <em>Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Lisbon, Portugal, 1412&#x2013;1421.</li>    <li id="BibPLXBIB0019" label="[19]">Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classification. In <em>      <em>Proc. of the Twenty-Sixth IJCAI</em>     </em>. Melbourne, Australia, 4068&#x2013;4074.</li>    <li id="BibPLXBIB0020" label="[20]">Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In <em>      <em>Proc. of the 16th international conference on World Wide Web</em>     </em>. ACM, Banff, Alberta, Canada, 171&#x2013;180.</li>    <li id="BibPLXBIB0021" label="[21]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em>      <em>Advances in Neural Information Processing Systems 26</em>     </em>. Curran Associates, Inc., Lake Tahoe, USA, 3111&#x2013;3119.</li>    <li id="BibPLXBIB0022" label="[22]">Samaneh Moghaddam and Martin Ester. 2013. The FLDA model for aspect-based opinion mining: addressing the cold start problem. In <em>      <em>Proc. of the 22nd International Conference on WWW</em>     </em>. ACM, Rio de Janeiro, Brazil, 909&#x2013;918.</li>    <li id="BibPLXBIB0023" label="[23]">Thien&#x00A0;Hai Nguyen and Kiyoaki Shirai. 2015. PhraseRNN: Phrase recursive neural network for aspect-based sentiment analysis. In <em>      <em>Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Lisbon, Portugal, 2509&#x2013;2514.</li>    <li id="BibPLXBIB0024" label="[24]">Jeffrey Pennington, Richard Socher, and Christopher&#x00A0;D Manning. 2014. Glove: Global vectors for word representation. In <em>      <em>Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Doha, Qatar, 1532&#x2013;1543.</li>    <li id="BibPLXBIB0025" label="[25]">Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In <em>      <em>Proc. of the 8th International Workshop on Semantic Evaluation</em>     </em>. Dublin, Ireland, 27&#x2013;35.</li>    <li id="BibPLXBIB0026" label="[26]">Soujanya Poria, Erik Cambria, and Alexander Gelbukh. 2016. Aspect extraction for opinion mining with a deep convolutional neural network. <em>      <em>Knowledge-Based Systems</em>     </em>108, Supplement C (2016), 42&#x2013;49.</li>    <li id="BibPLXBIB0027" label="[27]">Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. <em>      <em>Computational Linguistics</em>     </em>37, 1 (2011), 9&#x2013;27.</li>    <li id="BibPLXBIB0028" label="[28]">Tim Rockt&#x00E4;schel, Edward Grefenstette, Karl&#x00A0;Moritz Hermann, Tom&#x00E1;&#x0161; Ko&#x010D;isk&#x00FD;, and Phil Blunsom. 2016. Reasoning about entailment with neural attention. In <em>      <em>Proc. of the 4th ICLR</em>     </em>. San Juan, Puerto Rico.</li>    <li id="BibPLXBIB0029" label="[29]">Sebastian Ruder, Parsa Ghaffari, and John&#x00A0;G Breslin. 2016. A hierarchical model of reviews for aspect-based sentiment analysis. In <em>      <em>Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Austin, Texas, USA, 999&#x2013;1005.</li>    <li id="BibPLXBIB0030" label="[30]">Alexander&#x00A0;M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for sentence summarization. In <em>      <em>Proc. of the 2015 Conference on EMNLP</em>     </em>. ACL, Lisbon, Portugal, 379&#x2013;389.</li>    <li id="BibPLXBIB0031" label="[31]">Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bi-directional attention flow for machine comprehension. In <em>      <em>Proc. of the 5th International Conference on Learning Representations</em>     </em>. CoRR, Toulon, France.</li>    <li id="BibPLXBIB0032" label="[32]">Richard Socher, Danqi Chen, Christopher&#x00A0;D Manning, and Andrew&#x00A0;Y. Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In <em>      <em>NIPS 2013</em>     </em>. Curran Associates, Inc., Lake Tahoe, USA, 926&#x2013;934.</li>    <li id="BibPLXBIB0033" label="[33]">Kaisong Song, Ling Chen, Wei Gao, Shi Feng, Daling Wang, and Chengqi Zhang. 2016. PerSentiment: A personalized sentiment classification system for microblog users. In <em>      <em>Proc. of the 25th International Conference Companion on World Wide Web</em>     </em>. International World Wide Web Conferences Steering Committee, Montr&#x00E9;al, Canada, 255&#x2013;258.</li>    <li id="BibPLXBIB0034" label="[34]">Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In <em>      <em>Advances in Neural Information Processing Systems 28</em>     </em>. Curran Associates, Inc., Montr&#x00E9;al, Canada, 2440&#x2013;2448.</li>    <li id="BibPLXBIB0035" label="[35]">Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2015. Modeling mention, context and entity with neural networks for entity disambiguation. In <em>      <em>Proc. of the Twenty-Fourth International Joint Conference on Artificial Intelligence</em>     </em>. Buenos Aires, Argentina, 1333&#x2013;1339.</li>    <li id="BibPLXBIB0036" label="[36]">Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2016. Effective LSTMs for target-dependent sentiment classification. In <em>      <em>Proc. of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em>     </em>. Osaka, Japan, 3298&#x2013;3307.</li>    <li id="BibPLXBIB0037" label="[37]">Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classification with deep memory network. In <em>      <em>Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Austin, Texas, USA, 214&#x2013;224.</li>    <li id="BibPLXBIB0038" label="[38]">Duyu Tang, Furu Wei, Nan Yang, Zhou Ming, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In <em>      <em>Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics</em>     </em>. ACL, Baltimore, Maryland, 1555&#x2013;1565.</li>    <li id="BibPLXBIB0039" label="[39]">Duy-Tin Vo and Yue Zhang. 2015. Target-dependent twitter sentiment classification with rich automatic features. In <em>      <em>Proc. of the 24th International Joint Conference on Artificial Intelligence</em>     </em>. Buenos Aires, Argentina, 1347&#x2013;1353.</li>    <li id="BibPLXBIB0040" label="[40]">Shuai Wang, Zhiyuan Chen, and Bing Liu. 2016. Mining aspect-specific opinion using a holistic lifelong topic model. In <em>      <em>Proc. of the 25th International Conference on World Wide Web</em>     </em>. International World Wide Web Conferences Steering Committee, Montr&#x00E9;al, Canada, 167&#x2013;176.</li>    <li id="BibPLXBIB0041" label="[41]">Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based LSTM for aspect-level sentiment classification. In <em>      <em>Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing</em>     </em>. ACL, Austin, Texas, USA, 606&#x2013;615.</li>    <li id="BibPLXBIB0042" label="[42]">Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory networks. In <em>      <em>Proc. of the 3rd International Conference on Learning Representations</em>     </em>. CoRR, Scottsdale, Arizona, USA.</li>    <li id="BibPLXBIB0043" label="[43]">Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016. Gated neural networks for targeted sentiment analysis. In <em>      <em>Proc. of the Thirtieth AAAI</em>     </em>. AAAI Press, Phoenix, Arizona, USA, 3087&#x2013;3093.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>Available at: <a class="link-inline force-break" href="http://nlp.stanford.edu/projects/glove/">http://nlp.stanford.edu/projects/glove/</a>. </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186001">https://doi.org/10.1145/3178876.3186001</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
