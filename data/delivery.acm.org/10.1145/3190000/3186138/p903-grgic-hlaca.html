<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Nina</span>     <span class="surName">Grgi&#x0107;-Hla&#x010D;a</span>,     MPI-SWS, Saarland University, <a href="mailto:nghlaca@mpi-sws.org">nghlaca@mpi-sws.org</a>    </div>    <div class="author">     <span class="givenName">Elissa M.</span>     <span class="surName">Redmiles<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a></span>,     University of Maryland, <a href="mailto:eredmiles@cs.umd.edu">eredmiles@cs.umd.edu</a>    </div>    <div class="author">     <span class="givenName">Krishna P.</span>     <span class="surName">Gummadi</span>,     MPI-SWS, Saarland University, <a href="mailto:gummadi@mpi-sws.org">gummadi@mpi-sws.org</a>    </div>    <div class="author">     <span class="givenName">Adrian</span>     <span class="surName">Weller<a class="fn" href="#fn2" id="foot-fn2"><sup>&#x2020;</sup></a></span>,     Cambridge University, Alan Turing Institute, <a href="mailto:adrian.weller@eng.cam.ac.uk">adrian.weller@eng.cam.ac.uk</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186138" target="_blank">https://doi.org/10.1145/3178876.3186138</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>As algorithms are increasingly used to make important decisions that affect human lives, ranging from social benefit assignment to predicting risk of criminal recidivism, concerns have been raised about the fairness of algorithmic decision making. Most prior works on algorithmic fairness <em>normatively</em> prescribe how fair decisions ought to be made. In contrast, here, we <em>descriptively</em> survey users for how they perceive and reason about fairness in algorithmic decision making.</small>    </p>    <p>     <small>A key contribution of this work is the framework we propose to understand <em>why people perceive certain features as fair or unfair to be used in algorithms</em>. Our framework identifies eight properties of features, such as <em>relevance</em>, <em>volitionality</em> and <em>reliability</em>, as latent considerations that inform people&#x0027;s moral judgments about the fairness of feature use in decision-making algorithms. We validate our framework through a series of scenario-based surveys with 576 people. We find that, based on a person&#x0027;s assessment of the eight latent properties of a feature in our exemplar scenario, we can accurately (> 85%) predict if the person will judge the use of the feature as fair.</small>    </p>    <p>     <small>Our findings have important implications. At a high-level, we show that people&#x0027;s unfairness concerns are multi-dimensional and argue that future studies need to address unfairness concerns beyond discrimination. At a low-level, we find considerable disagreements in people&#x0027;s fairness judgments. We identify root causes of the disagreements, and note possible pathways to resolve them.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Algorithmic Fairness; Algorithmic Discrimination; Fairness in Machine Learning; Procedural Fairness; Fair Feature Selection</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Nina Grgi&#x0107;-Hla&#x010D;a, Elissa M. Redmiles, Krishna P. Gummadi, and Adrian Weller. 2018. Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186138" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186138</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Algorithms trained over data about past decisions are increasingly used to assist or replace human decision making in life-affecting scenarios, such as determining if an unemployed person should be eligible for a certain level of social welfare benefits&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>] or deciding if a person should be let out on bail pending trial&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. Given their potential impact on human lives, concerns have been raised about the <em>fairness</em> of the decisions made by algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>].</p>    <p>Concerns about algorithmic unfairness have led to much recent work on detecting and mitigating <em>discrimination</em> in decision-making scenarios. This work includes finding ways to operationalize notions of direct and indirect discrimination and provide mechanisms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0060">60</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0061">61</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0062">62</a>] for non-discriminatory learning, as well as examining the feasibility of making non-discriminatory decisions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>].</p>    <p>Existing studies of algorithmic fairness are largely <em>normative (prescriptive)</em> in nature, i.e., they begin by defining how fair decisions should (or ought to) be made, assuming that there is societal consensus around what constitutes fair decision making&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. In this paper, we pursue a complementary <em>descriptive (comparative)</em> approach towards fair decision making. Inspired by works in descriptive ethics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], we conduct empirical studies in one specific context, to learn what people perceive as fair decision making, with the goal of uncovering the moral reasoning behind their perceptions. Later, we discuss how our findings can be leveraged to design fair decision-making algorithms.</p>    <p>As perceptions of fairness are <em>multi-dimensional</em> and <em>context-dependent</em>, characterizing them presents a difficult challenge. In this work, we propose to understand how people make judgments about the fairness of using individual features in decision making. More concretely, we seek to measure and analyze how people would answer the following question: <strong>Is it fair to use a feature (</strong>    <span class="inline-equation"><span class="tex">$\mathcal {F}$</span>    </span>    <strong>) in a given decision making scenario (</strong>    <span class="inline-equation"><span class="tex">$\mathcal {S}$</span>    </span>    <strong>)?</strong>    </p>    <p>We center our investigation of fairness perceptions around the above question for multiple reasons: First, people&#x0027;s judgments about fairness of using features can be leveraged to learn fair algorithmic decision making, as shown in our recent work [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. Second, while the question, &#x201C;is this feature fair to use,&#x201D; is intuitive and simple to comprehend, people&#x0027;s answers (as our study shows) can be analyzed to reveal the extent to which different types of fairness considerations, such as whether or not a given feature is <em>volitional</em> or <em>causes the outcome</em>, factor into their judgments.</p>    <p>    <strong>Our Contributions.</strong>We collected and analyzed fairness judgments from a survey of 576 people. We asked survey participants to assess the fairness of using different features that are inputs to COMPAS&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], a commercial criminal risk estimation tool that is in current use to help make judicial decisions in the US about bail decisions.<a class="fn" href="#fn3" id="foot-fn3"><sup>1</sup></a>    </p>    <p>To model the factors that drive participants&#x2019; fairness judgments, we propose a set of eight <em>latent properties</em> of features that we hypothesize capture most of the considerations that influence people&#x0027;s fairness judgments. Our framework of eight properties includes unfairness concerns beyond discrimination, such as whether a feature is privacy sensitive and whether it is volitional (see Section <a class="sec" href="#sec-6">2</a>).</p>    <p>When asked to assess the fairness of using the different input features to COMPAS for making bail decisions, the majority of our respondents judged that half of the features are unfair to use. Interestingly, the latent properties the respondents&#x2019; considered in reaching their judgments were mostly unrelated to discrimination, highlighting the need to consider additional unfairness concerns.</p>    <p>Unfortunately, we also find that there is a lack of clear consensus in respondents&#x2019; judgments about the fairness of using a number of features. Our analysis attempts to explain the lack of consensus by modeling people&#x0027;s fairness judgments as a <em>two-part</em> decision process: one related to how people assess the latent properties of a feature, and a second related to how they morally reason about the fairness of using a feature given that is has certain latent properties.</p>    <p>We find that the lack of consensus in our respondents&#x2019; fairness judgments can be largely attributed to disagreements in how they estimated the latent properties &#x2013; particularly those related to causal reasoning, such as whether a feature causes the outcome or is caused by sensitive group membership. However, we find that respondents use similar moral reasoning in reaching fairness judgments if given a set of latent properties. Specifically, we were able to learn a single, simple classifier which performs accurately in predicting respondents&#x2019; fairness judgments from their latent property assessments.</p>    <p>    <strong>Implications.</strong> Our findings are striking because they suggest that the differences in people&#x0027;s fairness judgments may originate not from the differences in their inherently <em>more subjective</em> moral reasoning about how to weigh different latent properties when judging fairness of features, but rather from differences in their seemingly <em>more objective</em> assessments of latent properties of features. As such, our findings point towards a future for fair algorithmic decision making where the latent properties of a feature (e.g., whether it causes the outcome) might be objectively determined from extensive data, while there is hope that the moral reasoning that people use to map the latent properties to fairness might be consistently determined using input from people (e.g., collected via surveys).</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Judging Feature Usage Fairness</h2>    </div>    </header>    <p>Many works in psychological decision theory propose that people use heuristics to assess situations and reach decisions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. These heuristics may vary according to the situation and to people&#x0027;s level of knowledge about the situational elements, helping to parse information into more manageable and meaningful pieces&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>].</p>    <p>We hypothesize that when determining whether a feature is fair to be used in a decision making scenario, people rely on their implicit or explicit assessments of certain underlying properties of the feature as a heuristic. So our framework for how people judge feature usage fairness consists of two-parts. In the first part, we conjecture eight latent properties of a given feature as potential determinants for how people judge the fairness of using the feature. In the second part, we hypothesize that these latent properties are weighted in different ways by different individuals when reaching their fairness judgments about the feature. We draw these latent properties from the existing literature in social-economic-political-moral sciences, philosophy, and the law, as detailed below.</p>    <p>    <strong>I. Reliability.</strong> Inspired by legal requirements that any admissible evidence be <em>reliably assessed</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], fairness judgments might be influenced by the potential for reliably assessing the feature. For instance, an input feature to COMPAS recidivism risk prediction is the defendant&#x0027;s belief about criminality, assessed via answers to questions of the form &#x201C;Do you think that a hungry person has a right to steal?&#x201D; People who do not perceive beliefs about criminality as reliably assessable from such questions might rate the feature as unfair to use.</p>    <p>    <strong>II. Relevance.</strong> Inspired by legal requirements that any admissible evidence be <em>relevant</em> to the case&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], fairness judgments might be influenced by a feature&#x0027;s relevance to the decision making scenario. For instance, an input feature to COMPAS recidivism risk prediction is the defendant&#x0027;s education and behavior in school, assessed via answers to questions of the form &#x201C;What were your usual grades in high school?&#x201D; People who perceive performance in school as irrelevant to recidivism risk estimation might rate the feature as unfair to use.</p>    <p>    <strong>III. Privacy.</strong> Inspired by legal requirements that evidence obtained via illegal privacy intrusions (such as searches without warrant or unauthorized wire tapping) is inadmissible &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0058">58</a>], fairness judgments might be influenced by whether the feature is relying on privacy-sensitive information. For instance, an input feature to COMPAS recidivism risk prediction is the defendant&#x0027;s history of substance abuse, assessed via answers to questions of the form &#x201C;Did you use heroin, cocaine, crack, or meth as a juvenile?&#x201D; People who perceive juvenile drug use as privacy-sensitive information might rate the feature as unfair to use.</p>    <p>    <strong>IV. Volitionality.</strong> Inspired by philosophical arguments on luck egalitarianism&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>] that people should be held responsible for the voluntary choices they make (option luck), but not penalized for their unchosen circumstances (brute luck), fairness judgments might be influenced by an individual&#x0027;s assessment of the extent to which a feature is <em>volitional</em>, i.e., the result of exercising one&#x0027;s own will. For instance, an input feature to COMPAS recidivism risk prediction is the criminal history of the defendant&#x0027;s family, assessed via answers to questions of the form &#x201C;Was your father or mother ever arrested?&#x201D; People who perceive family criminal history as non-volitional might rate the feature as unfair to use.</p>    <p>    <strong>V. Causes Outcome.</strong> Inspired by arguments for applying causal reasoning in fairness&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0063">63</a>], fairness judgments might be influenced by whether a feature is likely to cause (i.e., increase or mitigate) the chances of the person engaging in risky behavior. For instance, an input feature to COMPAS recidivism risk prediction is the defendant&#x0027;s current charge, assessed via answers to questions of the form &#x201C;Are you currently charged with a misdemeanor, non-violent felony or violent felony?&#x201D; People who perceive the defendant&#x0027;s current charge as causing him to recidivate might consider the feature fair to use.</p>    <p>    <strong>VI. Causes Vicious Cycle.</strong> Inspired by arguments for avoiding vicious cycles of crime and poverty&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>], fairness judgments might be influenced by whether a feature is likely to trap people in a vicious cycle of increasingly risky behaviors. For instance, an input feature to COMPAS recidivism risk prediction is the criminal history of the defendant&#x0027;s friends, assessed via answers to questions of the form &#x201C;How many of your friends have ever been arrested?&#x201D; People who perceive friends&#x2019; criminal history may create a vicious cycle (where people with friends with criminal records are sentenced to longer prison terms, thereby, increasing the number friends with criminal records) might rate the feature as unfair to use.</p>    <p>    <strong>VII. Causes Disparity in Outcomes.</strong> Inspired by the doctrine of disparate impact in anti-discrimination laws that require statistical parity in outcomes for people belonging to different sensitive social groups like race or gender&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>], fairness judgments might be influenced by whether a feature would result in protected group members receiving disadvantageous treatment. For instance, an input feature to COMPAS recidivism risk prediction is the safety of the neighborhood the defendant is living in, assessed via answers to questions of the form &#x201C;Is there much crime in your neighborhood?&#x201D; People who perceive neighborhood safety may increase disparity in outcomes might rate the feature as unfair to use.</p>    <p>    <strong>VIII. Caused by Sensitive Group Membership.</strong> Inspired by the notions of indirect discrimination in political and economic sciences, where members of a social group are implicitly discriminated against using features that are correlated with or caused by their group membership&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>], fairness judgments might be influenced by the extent to which a feature is caused by their group membership. For instance, an input feature to COMPAS recidivism risk prediction is the criminal history of the defendant&#x0027;s friends, assessed via answers to questions of the form &#x201C;How many of your friends have ever been arrested?&#x201D; People who perceive friends&#x2019; criminal history as potentially caused by people&#x0027;s membership of some social groups might rate the feature as unfair to use.</p>    <p>    <strong>Observation 1: Sufficiency and Necessity of our Latent Properties.</strong> We do not claim that our list of latent properties presented above is <em>exhaustive or complete</em>. That is, there may exist other properties that might influence users&#x2019; fairness judgments. However, as we show in Section&#x00A0;<a class="sec" href="#sec-10">3.1.2</a>, the eight properties are by and large <em>sufficient and necessary</em> to explain fairness judgments of users in our survey. Specifically, less than 3% of our surveyed users reported using a property outside of our list in arriving at their judgments; further, for each of the eight properties, at least 15% of our surveyed users reported relying on it as a consideration in their fairness judgments. Moreover, when we attempted analytically to predict users&#x2019; fairness judgments based only on their assessments of the latent properties in Section&#x00A0;<a class="sec" href="#sec-20">5.2</a>, we found that the eight properties are not only sufficient to make the predictions with high accuracy, but that six of the eight are also statistically significant (i.e., necessary) for predicting fairness judgments.</p>    <p>    <strong>Observation 2: Unfairness beyond Discrimination.</strong> Our list of latent properties captures a diverse set of unfairness concerns with algorithmic decision making that go beyond <em>discrimination</em>, the traditional basis for most of the existing literature on algorithmic fairness. In fact, the two properties that were not deemed statistically significant in our prediction analysis discussed above, <em>Causing Disparity in Outcomes</em> and <em>Caused by Sensitive Group Membership</em>, are related to the potential for a feature to cause discrimination. Thus, our proposed framework captures many facets of unfairness that have previously received little attention in the fair learning community, yet may significantly influence users&#x2019; fairness perceptions of algorithmic decision making.</p>    <div class="table-responsive" id="tab1">    <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">The ten features assessed in our survey and the questions provided as examples in the scenario. The features and questions are drawn from the COMPAS questionnaire.</span>    </div>    <table class="table">     <thead>      <tr>       <th style="text-align:left;"/>       <th style="text-align:left;">       <strong>Predictive Feature</strong>       </th>       <th>       <strong>Example Question</strong>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">       <strong>1.</strong>       </td>       <td style="text-align:left;">Current Charges</td>       <td>Are you currently charged with a misdemeanor, non-violent felony or violent felony?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>2.</strong>       </td>       <td style="text-align:left;">Criminal History: self</td>       <td>How many times have you violated your parole?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>3.</strong>       </td>       <td style="text-align:left;">Substance Abuse</td>       <td>Did you use heroin, cocaine, crack or meth as a juvenile?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>4.</strong>       </td>       <td style="text-align:left;">Stability of Employment &#x0026; Living Situation</td>       <td>How often do you have trouble paying bills?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>5.</strong>       </td>       <td style="text-align:left;">Personality</td>       <td>Do you have the ability to &#x201C;sweet talk&#x201D; people into getting what you want?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>6.</strong>       </td>       <td style="text-align:left;">Criminal Attitudes</td>       <td>Do you think that a hungry person has a right to steal?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>7.</strong>       </td>       <td style="text-align:left;">Neighborhood Safety</td>       <td>Is there much crime in your neighborhood?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>8.</strong>       </td>       <td style="text-align:left;">Criminal History: family and friends</td>       <td>How many of your friends have ever been arrested?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>9.</strong>       </td>       <td style="text-align:left;">Quality of Social Life &#x0026; Free Time</td>       <td>Do you often feel left out of things?</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>10.</strong>       </td>       <td style="text-align:left;">Education &#x0026; School Behavior</td>       <td>What were your usual grades in high school?</td>      </tr>     </tbody>    </table>    </div>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Methodology</h2>    </div>    </header>    <p>In order to gather people&#x0027;s judgments about algorithmic fairness and the latent properties that we proposed, we conducted a series of online surveys in September and October 2017. Our methodology was approved by our institution&#x0027;s ethics review board.</p>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Survey Design</h3>     </div>    </header>    <p>We asked participants to respond to questions in the context of a specific scenario that is already in use in the real world.</p>    <section id="sec-9">     <header>      <div class="title-info">       <h4>       <span class="section-number">3.1.1</span>        <strong>Scenario</strong>       </h4>      </div>     </header>     <p>We consider the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) system, which analyzes defendant&#x0027;s answers to a large questionnaire with questions across a range of categories in order to predict risk of criminal activity. COMPAS has been adopted across various jurisdictions in the US to assist with tasks from the judicial domain, including decisions about bail, sentencing lengths and parole [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>].</p>     <p>Our survey begins with the following: &#x201C;Judges in Broward County, Florida, have started using a computer program to help them decide which defendants can be released on bail before trial. The computer program they are using takes into account information about <strong><feature></strong>. For example, the computer program will take into account the defendant&#x0027;s answer to the following question: <strong><question></strong>.&#x201D; These items were asked for ten features related to the COMPAS tool, outlined in Table&#x00A0;<a class="tbl" href="#tab1">1</a>.<a class="fn" href="#fn4" id="foot-fn4"><sup>2</sup></a> These features were drawn from the categories in the COMPAS questionnaire, and the example question for that feature was extracted from the corresponding category in the questionnaire&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0050">50</a>]. We use this scenario in two pilot surveys and the main survey, as discussed below.</p>    </section>    <section id="sec-10">     <header>      <div class="title-info">       <h4>       <span class="section-number">3.1.2</span>        <strong>Pilot Survey 1: Fairness Judgments and Their Latent Reasons</strong>       </h4>      </div>     </header>     <p>In <em>Pilot survey 1</em>, we sought to learn whether respondents found the above scenario <em>fair</em>, and <em>why they felt it was fair</em> or unfair.</p>     <p>We asked people to assess whether the scenario was fair on a 7-point Likert scale from &#x201C;Strongly Disagree&#x201D; to &#x201C;Strongly Agree&#x201D;. Specifically, we asked: &#x201C;Please rate how much you agree with the following statement: It is fair to determine if a person can be released on bail using information about their <strong><feature></strong>.&#x201D; Then we asked them to select their reasons for why it was fair or unfair, providing the eight latent properties as answer options (as described below, for <em>Pilot survey 2</em>) , while also providing an &#x201C;Other&#x201D; option with a text-entry box. Participants who rated the scenario as unfair (from 1-3) were only asked why it was unfair, and participants who rated the scenario as fair (from 5-7) were only asked why it was fair. Participants who gave the scenario a neutral rating (4) were asked both questions. Participants were asked this set of questions for each of the ten features in Table&#x00A0;<a class="tbl" href="#tab1">1</a>, with the order in which the features were presented between respondents randomized. <figure id="fig1">       <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186138/images/www2018-147-fig1.jpg" class="img-responsive" alt="Figure 1"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Properties used as justifications of fairness judgments in <em>Pilot survey 1</em>. For each property, the plot shows the percentage of responses that used it as a justification of the fairness judgment. Note that multiple properties can be used as a justification for a single judgment.</span>       </div>      </figure>      <strong>Takeaways.</strong> We used <em>Pilot survey 1</em> to assess whether the properties we propose in Section&#x00A0;<a class="sec" href="#sec-6">2</a> are <em>necessary</em> and <em>sufficient</em> to capture people&#x0027;s reasoning about fairness. We find that each property was used by at least 15% of respondents to explain why they rated a particular feature as fair or unfair to use (Figure&#x00A0;<a class="fig" href="#fig1">1</a>). Interestingly, the properties that were used as explanations of fairness judgments the most frequently are not related to notions of discrimination: <em>Relevance</em> is the most used property, used in 74% of the responses, followed by <em>Causes Outcome</em> and <em>Reliability</em>, used in more than 40% of the responses.</p>     <p>Further, less than 3% of respondents selected the &#x201C;Other&#x201D; option for why they judged a scenario as fair or unfair. Thematic analysis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>] of responses provided in the &#x201C;Other&#x201D; category reveals that the majority of these responses still map to one of our eight properties. The frequent selection of each of our proposed properties, and the low proportion of &#x201C;Other&#x201D; responses, suggest that we are unlikely to be missing an important assessment criterion.</p>    </section>    <section id="sec-11">     <header>      <div class="title-info">       <h4>       <span class="section-number">3.1.3</span>        <strong>Pilot Survey 2: Latent Properties of Features</strong>       </h4>      </div>     </header>     <p>In <em>Pilot survey 2</em> we sought to explore how people evaluate the latent properties of features from our framework. Here we asked no fairness-related questions, in order to control for the effect of asking about fairness on latent property ratings, as discussed in Section&#x00A0;<a class="sec" href="#sec-16">3.4</a>.</p>     <p>We presented the scenario and asked people to assess the value of the eight <em>properties</em> of the features on the same 7-point Likert scale. The properties were described as follows:</p>     <ul class="list-no-style">      <li id="uid11" label="I. Reliability:">Information about <feature> can be assessed reliably.<br/></li>      <li id="uid12" label="II. Relevance:">Information about <feature> is relevant for making this decision.<br/></li>      <li id="uid13" label="III. Privacy:">Information about <feature> is private.<br/></li>      <li id="uid14" label="IV. Volitionality:">A person can change <feature> by making a choice or decision.<br/></li>      <li id="uid15" label="V. Causes Outcome:"><feature> can cause them to breach their bail.<br/></li>      <li id="uid16" label="VI. Causes Vicious Cycle:">Making this decision using information about <feature> can cause a vicious cycle.<br/></li>      <li id="uid17" label="VII. Causes Disparity in Outcomes:">Making this decision using information about <feature> can have negative effects on certain groups of people that are protected by law (e.g., based on race, gender, age, religion, national origin, disability status).<br/></li>      <li id="uid18" label="VIII. Caused by Sensitive Group Membership:"><feature> can be caused by their belonging to a group protected by law (e.g., race, gender, age, religion, national origin, disability status).<br/></li>     </ul>     <p>The order in which the features and latent properties were presented was randomized between respondents.</p>     <p>      <strong>Takeaways.</strong> As we ask no fairness-related questions in <em>Pilot survey 2</em>, we use it to examine latent property assessments independent of fairness. The results of this survey can help us understand the bias that is introduced by asking about both latent properties and fairness. To quantify this bias, we compare the results of this survey to those generated from our <em>Main survey</em>, described below, which included both latent property rating and fairness rating questions. The details of this comparison are discussed in Section&#x00A0;<a class="sec" href="#sec-16">3.4</a>.</p>    </section>    <section id="sec-12">     <header>      <div class="title-info">       <h4>       <span class="section-number">3.1.4</span>        <strong>Main Survey: Fairness Judgments and Latent Properties of Features</strong>       </h4>      </div>     </header>     <p>In the <em>Main survey</em> we sought to evaluate whether people&#x0027;s judgments about the latent properties of features proposed in our framework were relevant to their judgments about the fairness of features.</p>     <p>In the <em>Main survey</em>, we asked people questions about (i) the fairness of features, as in the first question in <em>Pilot survey 1</em>, as well as about (ii) their latent property assessments, as in <em>Pilot survey 2</em>. As in the pilot surveys, in the <em>Main survey</em>, this set of fairness and latent property assessment questions was asked for <em>each</em> of the ten features from Table&#x00A0;<a class="tbl" href="#tab1">1</a>, with the order of features and latent properties being randomized between respondents. Additionally, we randomized whether the fairness question was presented before or after the questions about the latent properties.</p>    </section>    <section id="sec-13">     <header>      <div class="title-info">       <h4>       <span class="section-number">3.1.5</span>        <strong>Questionnaire Validity</strong>       </h4>      </div>     </header>     <p>To ensure that survey participants can meaningfully interpret our questions, we pre-tested all items in the questionnaires using cognitive interviews. Cognitive interviews involve asking participants to think aloud to the researcher as they take the survey. This approach is a survey methodology best-practice for ensuring construct validity and questionnaire accuracy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0049">49</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0059">59</a>]. We conducted cognitive interviews with five demographically diverse participants, recruited using the Prolific crowdsourcing platform, and iteratively refined our questionnaires based on participant feedback until new considerations stopped emerging.</p>     <p>Once we were satisfied with the validity of our questionnaire, we collected our final sample for analysis. To mitigate the effects of order bias&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0052">52</a>], we randomized the order of questions described in Sections&#x00A0;<a class="sec" href="#sec-10">3.1.2</a>-<a class="sec" href="#sec-12">3.1.4</a>. Finally, we included an attention-check question to ensure that participants were thoughtfully answering the questionnaire items&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0036">36</a>].</p>    </section>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Survey Samples and their Demographics</h3>     </div>    </header>    <p>The <em>Main survey</em> samples consisted of 196 Amazon Mechanical Turk (AMT) <em>master workers</em> from the US and 380 US respondents with census-representative demographics, collected using the survey recruitment firm Survey Sampling International (SSI).</p>    <p>We sampled users from two different platforms in our main survey, because we were concerned about both representativeness and quality of user responses. AMT users are known to provide responses of equal or, often, higher quality than survey panel respondents&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. However, AMT workers are not demographically representative of the U.S. population due to selection bias introduced by differences between those who sign up for AMT and the general population&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0052">52</a>]. SSI and other such sampling firms, on the other hand, use a myriad of different recruitment mechanisms to reduce selection bias and ensure that a demographically-representative sample is recruited.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Demographics of our AMT and SSI survey samples compared to the 2016 U.S. Census&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0057">57</a>]. Figures marked with a * were compared to Pew data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0048">48</a>] for political leaning.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Demographic Attribute</strong>       </th>       <th style="text-align:right;">        <strong>AMT</strong>       </th>       <th style="text-align:right;">        <strong>SSI</strong>       </th>       <th>        <strong>Census</strong>       </th>       </tr>     </thead>     <tbody>       <tr>       <td style="text-align:left;">Male</td>       <td style="text-align:right;">55%</td>       <td style="text-align:right;">44%</td>       <td>49%</td>       </tr>       <tr>       <td style="text-align:left;">Female</td>       <td style="text-align:right;">43%</td>       <td style="text-align:right;">55%</td>       <td>51%</td>       </tr>       <tr>       <td style="text-align:left;">African American</td>       <td style="text-align:right;">9%</td>       <td style="text-align:right;">12%</td>       <td>13%</td>       </tr>       <tr>       <td style="text-align:left;">Asian</td>       <td style="text-align:right;">3%</td>       <td style="text-align:right;">4%</td>       <td>6%</td>       </tr>       <tr>       <td style="text-align:left;">Caucasian</td>       <td style="text-align:right;">76%</td>       <td style="text-align:right;">71%</td>       <td>61%</td>       </tr>       <tr>       <td style="text-align:left;">Hispanic</td>       <td style="text-align:right;">8%</td>       <td style="text-align:right;">11%</td>       <td>18%</td>       </tr>       <tr>       <td style="text-align:left;">Other</td>       <td style="text-align:right;">2%</td>       <td style="text-align:right;">2%</td>       <td>4%</td>       </tr>       <tr>       <td style="text-align:left;"> < B.S.</td>       <td style="text-align:right;">47%</td>       <td style="text-align:right;">68%</td>       <td>70%</td>       </tr>       <tr>       <td style="text-align:left;">B.S.+</td>       <td style="text-align:right;">51%</td>       <td style="text-align:right;">32%</td>       <td>30%</td>       </tr>       <tr>       <td style="text-align:left;">Liberal</td>       <td style="text-align:right;">57%</td>       <td style="text-align:right;">37%</td>       <td>33%*</td>       </tr>       <tr>       <td style="text-align:left;">Conservative</td>       <td style="text-align:right;">17%</td>       <td style="text-align:right;">24%</td>       <td>29%*</td>       </tr>       <tr>       <td style="text-align:left;">Moderate</td>       <td style="text-align:right;">21%</td>       <td style="text-align:right;">33%</td>       <td>34%*</td>       </tr>       <tr>       <td style="text-align:left;">Other</td>       <td style="text-align:right;">5%</td>       <td style="text-align:right;">6%</td>       <td>4%*</td>       </tr>      </tbody>     </table>    </div>    <p>Table&#x00A0;<a class="tbl" href="#tab2">2</a> shows the demographics of our AMT and SSI survey samples, compared with the 2016 U.S. Census&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0057">57</a>]. We find that our AMT respondents consisted of considerably fewer females (43%), more Caucasians (76%), more highly educated individuals (51% hold at least a college degree), and more liberals (57%) than the U.S. population. This educational skew in AMT population is consistent with observations from previous research studies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0053">53</a>]. On the other hand, our SSI respondents were census-representative to within 5% along the demographics of gender (55% female), education (32% with a BS or above), and political leaning (37% liberal). While more of our SSI respondents identify as Caucasian (71%) than in the U.S. population, a possible explanation could be that respondents may have selected only Caucasian rather than both Hispanic and Caucasian in response to our race and ethnicity question, since it was not multiple-option.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">People&#x0027;s judgments on the fairness of using features, and the consensus in their responses, for the AMT sample. The reported values of consensus are calculated as 1 - Normalized Shannon Entropy (NSE) of the responses. In the 7 point column, we report consensus across the whole range of responses. In the 3 point column, we report consensus across responses bucketed into three main fairness categories: unfair (1-3), neutral (4) and fair (5-7).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;"/>       <th style="text-align:left;"/>       <th style="text-align:center;"/>       <th colspan="9" style="text-align:center;">        <strong>Fraction of People Rating Feature</strong>        <hr/>       </th>       <th colspan="2" style="text-align:center;">        <strong>Consensus</strong>        <hr/>       </th>       </tr>       <tr>       <th style="text-align:left;"/>       <th style="text-align:left;"/>       <th style="text-align:center;">        <strong>Mean</strong>       </th>       <th colspan="4" style="text-align:center;">        <strong>Unfair</strong>        <hr/>       </th>       <th style="text-align:center;"/>       <th colspan="4" style="text-align:center;">        <strong>Fair</strong>        <hr/>       </th>       <th colspan="2" style="text-align:center;">        <strong>1 - NSE</strong>        <hr/>       </th>       </tr>       <tr>       <th colspan="2" style="text-align:center;">        <strong>Feature</strong>        <hr/>       </th>       <th style="text-align:center;">        <strong>fairness</strong>       </th>       <th style="text-align:center;">        <strong>1</strong>       </th>       <th style="text-align:center;">        <strong>2</strong>       </th>       <th style="text-align:center;">        <strong>3</strong>       </th>       <th style="text-align:center;">        <strong>1-3</strong>       </th>       <th style="text-align:center;">        <strong>4</strong>       </th>       <th style="text-align:center;">        <strong>5-7</strong>       </th>       <th style="text-align:center;">        <strong>5</strong>       </th>       <th style="text-align:center;">        <strong>6</strong>       </th>       <th style="text-align:center;">        <strong>7</strong>       </th>       <th style="text-align:center;">        <strong>7 pt</strong>       </th>       <th>        <strong>3 pt</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <strong>1.</strong>       </td>       <td style="text-align:left;">Current Charges</td>       <td style="text-align:center;">6.38</td>       <td style="text-align:center;">0.01</td>       <td style="text-align:center;">0.01</td>       <td style="text-align:center;">0.01</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">        <strong>0.95</strong>       </td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.46</td>       <td>0.78</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>2.</strong>       </td>       <td style="text-align:left;">Criminal History: self</td>       <td style="text-align:center;">6.37</td>       <td style="text-align:center;">0.02</td>       <td style="text-align:center;">0.01</td>       <td style="text-align:center;">0.01</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">        <strong>0.94</strong>       </td>       <td style="text-align:center;">0.08</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.64</td>       <td style="text-align:center;">0.45</td>       <td>0.75</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>3.</strong>       </td>       <td style="text-align:left;">Substance Abuse</td>       <td style="text-align:center;">4.84</td>       <td style="text-align:center;">0.08</td>       <td style="text-align:center;">0.07</td>       <td style="text-align:center;">0.10</td>       <td style="text-align:center;">0.24</td>       <td style="text-align:center;">0.07</td>       <td style="text-align:center;">        <strong>0.68</strong>       </td>       <td style="text-align:center;">0.26</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.20</td>       <td style="text-align:center;">0.07</td>       <td>0.28</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>4.</strong>       </td>       <td style="text-align:left;">Stability of Employment</td>       <td style="text-align:center;">4.49</td>       <td style="text-align:center;">0.13</td>       <td style="text-align:center;">0.05</td>       <td style="text-align:center;">0.11</td>       <td style="text-align:center;">0.29</td>       <td style="text-align:center;">0.09</td>       <td style="text-align:center;">        <strong>0.62</strong>       </td>       <td style="text-align:center;">0.26</td>       <td style="text-align:center;">0.24</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.06</td>       <td>0.20</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>5.</strong>       </td>       <td style="text-align:left;">Personality</td>       <td style="text-align:center;">3.87</td>       <td style="text-align:center;">0.16</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.11</td>       <td style="text-align:center;">        <strong>0.44</strong>       </td>       <td style="text-align:center;">0.10</td>       <td style="text-align:center;">        <strong>0.46</strong>       </td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.02</td>       <td>0.14</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>6.</strong>       </td>       <td style="text-align:left;">Criminal Attitudes</td>       <td style="text-align:center;">3.63</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.16</td>       <td style="text-align:center;">        <strong>0.51</strong>       </td>       <td style="text-align:center;">0.09</td>       <td style="text-align:center;">        <strong>0.40</strong>       </td>       <td style="text-align:center;">0.20</td>       <td style="text-align:center;">0.11</td>       <td style="text-align:center;">0.09</td>       <td style="text-align:center;">0.03</td>       <td>0.15</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>7.</strong>       </td>       <td style="text-align:left;">Neighborhood Safety</td>       <td style="text-align:center;">3.14</td>       <td style="text-align:center;">0.28</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.15</td>       <td style="text-align:center;">        <strong>0.64</strong>       </td>       <td style="text-align:center;">0.07</td>       <td style="text-align:center;">0.30</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.10</td>       <td style="text-align:center;">0.08</td>       <td style="text-align:center;">0.06</td>       <td>0.25</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>8.</strong>       </td>       <td style="text-align:left;">Criminal History: family and friends</td>       <td style="text-align:center;">2.78</td>       <td style="text-align:center;">0.38</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.09</td>       <td style="text-align:center;">        <strong>0.67</strong>       </td>       <td style="text-align:center;">0.07</td>       <td style="text-align:center;">0.26</td>       <td style="text-align:center;">0.13</td>       <td style="text-align:center;">0.10</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">0.13</td>       <td>0.27</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>9.</strong>       </td>       <td style="text-align:left;">Quality of Social Life &#x0026; Free Time</td>       <td style="text-align:center;">2.70</td>       <td style="text-align:center;">0.38</td>       <td style="text-align:center;">0.20</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">        <strong>0.70</strong>       </td>       <td style="text-align:center;">0.07</td>       <td style="text-align:center;">0.23</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.08</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">0.13</td>       <td>0.29</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>10.</strong>       </td>       <td style="text-align:left;">Education &#x0026; School Behavior</td>       <td style="text-align:center;">2.70</td>       <td style="text-align:center;">0.34</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.14</td>       <td style="text-align:center;">        <strong>0.71</strong>       </td>       <td style="text-align:center;">0.08</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.13</td>       <td style="text-align:center;">0.06</td>       <td style="text-align:center;">0.03</td>       <td style="text-align:center;">0.12</td>       <td>0.29</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Analysis Methods</h3>     </div>    </header>    <p>In our analysis we measure the consensus amongst people&#x0027;s ratings of fairness and latent property values using Shannon entropy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0055">55</a>] calculated over the probability distributions over the answers. Shannon entropy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] and measures derived from Shannon entropy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0056">56</a>] are frequently used to quantify consensus. We calculate the Shannon Entropy normalized between 0 and 1 (<em>NSE</em>), and report values of consensus calculated as 1 &#x2212; <em>NSE</em>, so that complete consensus corresponds to 1 and complete disagreement to 0.</p>    <p>We also examine the predictive power of our framework by building a binary classifier that predicts if a feature will be considered fair (completely, mostly, slightly, neutral) or unfair (completely, mostly, slightly), based on the values of its latent properties. The training data consists of respondents&#x2019; evaluations of latent properties, and binarized evaluations of fairness. We train a logistic regression model with <em>L</em>2 regularization, implemented with the Python Scikit-learn package [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]. To evaluate the model, we randomly split the data into 50%/50% train/test folds five times, and report the average accuracy and AUC. Further, we randomly select one of the five runs and analyze its missclassiffications on the whole data; the other runs yielded qualitatively similar results.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Discussion of Limitations</h3>     </div>    </header>    <p>As in all survey studies, self-report biases may affect the data. As described in Section&#x00A0;<a class="sec" href="#sec-13">3.1.5</a> we have tried to mitigate these self-report biases as much as possible through extensive pre-testing and the adoption of best practices for question randomization.</p>    <p>As noted above, we used <em>Pilot survey 2</em> to measure the amount of bias introduced into our data by asking about fairness judgments in the same survey as we ask about the latent properties. We calculate the probability distribution over latent property ratings for (i) <em>Pilot survey 2</em> (the control), where we do not ask about fairness, and (ii) the <em>Main survey</em>, where we ask about fairness <em>and</em> latent properties. We see that the KL-divergence [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] from (i) to (ii) is very low, achieving values below 0.1 for 90% of the questions, and below 0.14 for the remaining 10%. We interpret these results, in accordance with [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>], as showing that if (i) is the real distribution of people&#x0027;s assessments, (ii) is a good approximation of it. Therefore, we conclude that assessments of latent property values are minimally affected by questions about fairness.</p>    <p>Self-report studies may also suffer from generalizability biases, as those who take these surveys may not be representative of the general population. In order to maximize both generalizability and data quality we recruit both a census-representative population and an AMT sample, as detailed in Section&#x00A0;<a class="sec" href="#sec-14">3.2</a>. Finally, it is possible that people may feel differently about fairness in different contexts. Future work should seek to validate whether a model that produces actual results based on self-report inputs such as ours aligns with &#x201C;real-world&#x201D; fairness perceptions when people are placed in more ecologically-valid situations with decision-making algorithms. <figure id="fig2">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186138/images/www2018-147-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Consensus in fairness judgments and assessments of latent properties, for the AMT sample. The plots show values of consensus on fairness and the latent properties: [Left] which exhibit a higher degree of consensus, [Right] which exhibit a lower degree of consensus.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Analyzing Fairness Judgments</h2>    </div>    </header>    <p>We first compare respondents&#x2019; judgments on the fairness of using different features in our algorithmic decision-making scenario, and then explore the degree to which they reach consensus in their judgments on the usage of any given feature. Throughout the paper, we conduct our analysis on two datasets: responses of AMT master workers, which are of higher quality, and responses gathered by SSI, which are less demographically biased. Since both datasets exhibit similar trends, we describe the AMT results in detail, including only a brief comparison of the results between the two samples<a class="fn" href="#fn5" id="foot-fn5"><sup>3</sup></a>.</p>    <p>    <strong>Across Different Features.</strong> We find that some features are, on average, considered more fair to use than others. As shown in Table&#x00A0;<a class="tbl" href="#tab3">3</a>, AMT respondents rate <em>Current Charges</em> and <em>Criminal History</em> as mostly <em>fair</em> to use, with mean ratings close to 6.4. On the other hand, <em>Education &#x0026; School Behavior</em>, <em>Quality of Social Life</em>, and <em>Criminal History of Family and Friends</em>, are rated as somewhat <em>unfair</em> to use, with mean ratings close to 2.7. The remaining features have more neutral mean ratings, between slightly unfair (rating 3) and slightly fair (rating 5).</p>    <p>Table&#x00A0;<a class="tbl" href="#tab3">3</a> also shows that more than half of the respondents judged five of the ten features as unfair (i.e., gave a fairness rating between 1 and 3) to be used in this decision-making scenario. However, as none of these features directly capture sensitive group membership information such as race or gender of the defendant, their use for risk prediction is not restricted by anti-discrimination laws. Thus, our findings suggest that unfairness concerns about algorithmic decisions are much broader than just concerns about discrimination.</p>    <p>    <strong>Across Different Users.</strong> Next, we analyze the extent to which fairness judgments related to the use of any feature vary across the respondent population. Upon closer examination, we note that respondents achieve high consensus on only two of the ten features.</p>    <p>In Table <a class="tbl" href="#tab3">3</a>, we observe that the features <em>Current Charges</em> and <em>Criminal History</em> achieve high consensus, with 95% and 94% of respondents, respectively, agreeing that it is fair to use these features in the algorithmic decision-making process. For many of the remaining features, there is a reasonable consensus amongst respondents, with a two-thirds majority considering the feature either unfair (by assigning ratings between 1 and 3) or unfair (by assigning ratings between 5 and 7). However, for <em>Personality</em> and <em>Criminal Attitudes</em>, we see very low consensus, with neither &#x201C;fair&#x201D; nor &#x201C;unfair&#x201D; receiving more than a <span class="inline-equation"><span class="tex">${\gt}51\%$</span>    </span> slender majority vote.</p>    <p>Table <a class="tbl" href="#tab3">3</a> also shows consensus in fairness judgments for different features measured using normalized Shannon entropy (1-NSE). We observe that features with mean ratings close to neutral (rating 4) such as <em>Personality</em> and <em>Criminal Attitudes</em> exhibit little consensus, with judgments of respondents spread across the entire rating spectrum from 1 through 7. Perhaps surprisingly, respondents also exhibit low consensus on features rated as least fair to use, such as <em>Education &#x0026; School Behavior</em>. It is possible that societal consensus for or against the use of these features is still evolving, unlike the broad consensus against using features like race or gender that has been codified in anti-discrimination laws.</p>    <p>    <strong>Impact of Sample Populations.</strong> The ranking of features with respect to mean fairness is similar across AMT and SSI respondents. However, we note that SSI respondents, in general, reach less consensus on their fairness ratings than AMT respondents. A possible explanation is that compared to AMT workers, SSI respondents represent a more-random and diverse subsample of the general population, and thus report a wider range of opinions.</p>    <p>    <strong>Summary.</strong> A majority of our survey respondents judged half of the features used by the COMPAS tool as unfair to use in predicting a defendant&#x0027;s risk of criminal activity. None of the features directly capture sensitive group information such as race or gender, highlighting the need to account for unfairness considerations beyond those related to discrimination, the sole consideration for most existing works on algorithmic fairness. However, our findings also suggest that societal consensus around these other unfairness considerations might be considerably less evolved than those around discrimination.</p>   </section>   <section id="sec-18">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Analyzing Fairness Reasoning</h2>    </div>    </header>    <p>In this section, we explore the possible causes of the lack of consensus in respondents&#x2019; fairness judgments observed in Section <a class="sec" href="#sec-17">4</a>. To this end, we leverage the eight latent properties that we outlined in Section <a class="sec" href="#sec-6">2</a> as the heuristic basis for how people judge the fairness of using a feature. Specifically, we will first examine how people assess the latent properties for different features and then analyze how the latent property assessments can be mapped to (i.e., used to predict) fairness judgments. In the process, we hope to attribute the disagreements in fairness judgments to either (i) disagreements in the latent property assessments by respondents or (ii) disagreements in the reasoning respondents use to reach fairness judgments from latent properties.</p>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Latent Property Assessments</h3>     </div>    </header>    <p>In Figure <a class="fig" href="#fig2">2</a>, we compare the degree of consensus in how respondents assessed the eight properties for different COMPAS input features. In general, we observe that people tend to disagree in their assessments of all latent properties for at least one or more features.</p>    <p>However, a closer look reveals important differences. First, evaluations of most latent properties related to causality, namely <em>causes vicious cycles</em>, <em>caused by sensitive features</em>, <em>causes disparity in outcomes</em>, <em>causal relationship with outcomes</em>, and <em>volitionality</em>, appear particularly controversial and exhibit low consensus (< 0.5) for all input features (shown in the right graph of Figure <a class="fig" href="#fig2">2</a>.) The consensus around latent properties related to any feature&#x0027;s potential to cause discrimination, namely <em>caused by sensitive features</em> and <em>causes disparity in outcomes</em>, is even lower (< 0.2). Our observations have important implications for recent proposals to avoid algorithmic discrimination through causal reasoning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0051">51</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0054">54</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0063">63</a>]. These works often assume that a causal graph, representing causal relationships between features and outcomes is given as an input to the algorithms. However, our findings suggest that getting people to agree on a single causal graph would be a non-trivial challenge.</p>    <p>Second, evaluations of other latent properties not based on causality, namely <em>relevance</em>, <em>reliability</em>, and <em>privacy</em>, achieve high consensus (> 0.5) on at least some input features (shown in the left graph of Figure <a class="fig" href="#fig2">2</a>.) Additionally, we observe that high consensus in these latter latent property estimates corresponds to high consensus in respondents&#x2019; fairness judgments over the corresponding input features.</p>    <p>Thus, we find that (a) our respondents tend to disagree more in their assessments of causal relationships between input features and predicted outcomes than in their assessments of non-causality related latent properties, and (b) consensus, or lack thereof, in certain latent property assessments appears to be strongly correlated with consensus in fairness judgments. In the following section, we quantify the predictive power of the latent property assessments on fairness judgments.</p>    <p>     <strong>Impact of Sample Populations.</strong> We omit the plot on consensus in latent property assessments of our SSI respondents due to space constraints. But, similar to our findings about consensus in fairness judgments, we find that SSI respondents reach less consensus in their latent property assessments than AMT respondents.</p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Modeling &#x0026; Predicting Fairness Judgments</h3>     </div>    </header>    <p>We now attempt to model fairness reasoning of our respondents by attempting to predict their fairness judgments about an input feature based only on their assessments of the feature&#x0027;s latent properties. Our insight is as follows: If all our respondents used a similar reasoning to arrive at fairness judgments from their latent property estimates, we should be able to learn a single predictor (mapping function) that would perform well for most, if not all, respondents. If different respondents used different reasonings, then no single predictor would perform well for most respondents. Finally, if we failed to learn predictors that would perform well at the level of individual users, one could question the validity of our entire fairness reasoning framework (discussed in Section&#x00A0;<a class="sec" href="#sec-6">2</a>.)</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Accuracy and AUC of binary classifier predicting feature-fairness judgments based on latent property ratings. Binary classification was performed once with assigning neutral ratings to the fair class in the ground truth, and once excluding neutral fairness judgments from the data, for each sample.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">        <strong>Sample</strong>       </th>       <th style="text-align:right;">        <strong>Neutral Judgments</strong>       </th>       <th style="text-align:right;">        <strong>Accuracy</strong>       </th>       <th>        <strong>AUC</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">AMT</td>       <td style="text-align:center;">included</td>       <td style="text-align:right;">0.882</td>       <td>0.879</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">excluded</td>       <td style="text-align:right;">0.905</td>       <td>0.904</td>       </tr>       <tr>       <td style="text-align:center;">SSI</td>       <td style="text-align:center;">included</td>       <td style="text-align:right;">0.872</td>       <td>0.816</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">excluded</td>       <td style="text-align:right;">0.878</td>       <td>0.852</td>       </tr>      </tbody>     </table>    </div>    <p>We train a binary logistic regression classifier, as described in Section <a class="sec" href="#sec-15">3.3</a>, and report the results in Table <a class="tbl" href="#tab4">4</a>. For both datasets, the classifier achieves very high accuracy (88% for AMT and 87% for SSI) when predicting respondents&#x2019; fairness judgments based on the underlying property ratings they assigned.</p>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Characterization of misclassifications of our model, by fairness rating value, for the AMT sample. The first row shows the total number of entries that received a certain rating, while the second row shows the fraction of those instances that were misclassified by our model. The third and fourth row report on the average value and standard deviation of the probability of being assigned to the correct class respectively.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Rating</strong>       </th>       <th style="text-align:center;">        <strong>1</strong>       </th>       <th style="text-align:center;">        <strong>2</strong>       </th>       <th style="text-align:center;">        <strong>3</strong>       </th>       <th style="text-align:center;">        <strong>4</strong>       </th>       <th style="text-align:center;">        <strong>5</strong>       </th>       <th style="text-align:center;">        <strong>6</strong>       </th>       <th>        <strong>7</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <strong># Judgments</strong>       </td>       <td style="text-align:center;">391</td>       <td style="text-align:center;">249</td>       <td style="text-align:center;">195</td>       <td style="text-align:center;">136</td>       <td style="text-align:center;">321</td>       <td style="text-align:center;">280</td>       <td>388</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>% Misclassified</strong>       </td>       <td style="text-align:center;">0.06</td>       <td style="text-align:center;">0.16</td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">0.33</td>       <td style="text-align:center;">0.09</td>       <td style="text-align:center;">0.04</td>       <td>0.01</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Avg P Correct</strong>       </td>       <td style="text-align:center;">0.91</td>       <td style="text-align:center;">0.78</td>       <td style="text-align:center;">0.60</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.82</td>       <td style="text-align:center;">0.91</td>       <td>0.98</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Std P Correct</strong>       </td>       <td style="text-align:center;">0.19</td>       <td style="text-align:center;">0.26</td>       <td style="text-align:center;">0.30</td>       <td style="text-align:center;">0.31</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.16</td>       <td>0.08</td>       </tr>      </tbody>     </table>    </div>    <p>In Table <a class="tbl" href="#tab5">5</a>, we examine the few misclassifications of our model further by studying how they are distributed over the ground-truth fairness ratings provided by AMT respondents. We observe that for features that were rated as very unfair (1, 2) or very fair (6, 7), our model makes even fewer mistakes. On the other hand, for neutral ratings (4), it performs close to random. In fact, the average probability that the classifier predicts the correct class rating for a response and its standard deviation &#x2013; which can be interpreted as the confidence that the model has in its predictions &#x2013; shows that our model is not only highly accurate but it is also fairly well calibrated. That is, it is highly confident (and highly accurate) in its predictions for very unfair or very fair judgments, but it has low confidence (and low accuracy) in its predictions for neutral (neither unfair nor fair) judgments. We hypothesize that these neutral ratings are difficult to predict, as people may not have clear reasoning underlying their judgments. <figure id="fig3">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186138/images/www2018-147-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Characterization of misclassifications of our model, per respondent. The plot shows the CDF of the probability distribution of mistakes over respondents.</span>      </div>     </figure>    </p>    <p>Finally, we check if our classifier offers highly accurate predictions for most respondents. Figure <a class="fig" href="#fig3">3</a> shows a cumulative distribution of inaccurate predictions across the population of respondents. We see that our single classifier can make highly accurate (>= 80% accuracy) for most (> 85%) of all our AMT respondents.</p>    <p>These results suggest that our proposed framework is quite effective at modeling people&#x0027;s moral judgments about fairness of feature use. The high accuracy of our predictions for most respondents strongly suggests (i) that our eight latent properties are largely sufficient to explain how users arrive at their fairness judgments and (ii) that most respondents are using similar reasoning at reaching their fairness judgments from latent properties, even as they disagree on the assessments of the latent properties in the first place.</p>    <p>     <strong>Relative Impact of Latent Properties on Fairness Judgments.</strong> In order to estimate the relative importance of our latent properties on respondents&#x2019; fairness judgments, we study the odds ratios for the logistic regression model described above. In this model, we also include as factors the survey sample source (AMT or SSI) and a mixed-effects term to account for the multiple judgments provided by each person in our sample. Table&#x00A0;<a class="tbl" href="#tab6">6</a> summarizes our results.</p>    <div class="table-responsive" id="tab6">     <div class="table-caption">      <span class="table-number">Table 6:</span>      <span class="table-title">Binary logistic regression model with binary fairness rating as the outcome variable. In addition to the latent properties, the model also included as inputs the sample source (SSI vs. AMT) and a mixed-effects term to account for multiple measurements from a single survey respondent. O.R. is the log-adjusted regression coefficient (odds ratio), C.I. is the 95% confidence interval for the O.R., and p-values < 0.05 are considered significant as dented by a *.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Latent Property</strong>       </th>       <th style="text-align:center;">        <strong>O.R.</strong>       </th>       <th style="text-align:center;">        <strong>C.I.</strong>       </th>       <th>        <strong>p-value</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Reliability</td>       <td style="text-align:center;">1.27</td>       <td style="text-align:center;">[1.2, 1.35]</td>       <td>< 0.001*</td>       </tr>       <tr>       <td style="text-align:left;">Relevance</td>       <td style="text-align:center;">2.47</td>       <td style="text-align:center;">[2.32, 2.63]</td>       <td>< 0.001*</td>       </tr>       <tr>       <td style="text-align:left;">Privacy</td>       <td style="text-align:center;">0.95</td>       <td style="text-align:center;">[0.9, 1]</td>       <td>0.049*</td>       </tr>       <tr>       <td style="text-align:left;">Volitionality</td>       <td style="text-align:center;">1.18</td>       <td style="text-align:center;">[1.13, 1.25]</td>       <td>< 0.001*</td>       </tr>       <tr>       <td style="text-align:left;">Causes Outcome</td>       <td style="text-align:center;">1.29</td>       <td style="text-align:center;">[1.21, 1.37]</td>       <td>< 0.001*</td>       </tr>       <tr>       <td style="text-align:left;">Vicious Cycle</td>       <td style="text-align:center;">0.84</td>       <td style="text-align:center;">[0.79, 0.9]</td>       <td>< 0.001*</td>       </tr>       <tr>       <td style="text-align:left;">Causes Disparity</td>       <td style="text-align:center;">0.95</td>       <td style="text-align:center;">[0.89, 1.02]</td>       <td>0.159</td>       </tr>       <tr>       <td style="text-align:left;">Caused by Sensitive</td>       <td style="text-align:center;">1.03</td>       <td style="text-align:center;">[0.96, 1.09]</td>       <td>0.429</td>       </tr>       <tr>       <td style="text-align:left;">Sample: SSI</td>       <td style="text-align:center;">1.54</td>       <td style="text-align:center;">[1.29, 1.84]</td>       <td>< 0.001*</td>       </tr>      </tbody>     </table>    </div>    <p>After controlling for mixed effects, we find that SSI respondents are more likely to rate a scenario as fair than AMT respondents. As one might expect, we find that our respondents were more likely to judge the use of a feature fair, the more they felt that the feature was <em>volitional</em>, <em>relevant</em>, <em>reliable</em>, and <em>caused the outcome</em>. On the other hand, respondents were less likely to judge the use of a feature as fair if they felt that the feature used was <em>privacy</em> sensitive, or resulted in a <em>vicious cycle</em>. We find that, while all of the eight properties helped in prediction, <em>caused by sensitive group membership</em> and <em>causes disparity in outcomes</em> were not significantly related to judgments in this particular scenario. Finally, we find that <em>relevance</em> had the strongest effect, with the odds of respondents rating the scenario as fair increasing 2.47 times for every point higher they rated it as relevant (on a 7-point Likert scale).</p>    <p>Note that our analysis here is meant only to illustrate how our approach could be used to estimate how people implicitly weigh latent properties of features when making judgments about using features in algorithmic decisions. However, we do not expect these specific odds ratios to necessarily hold in other scenarios.</p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Explaining Fairness Disagreements</h3>     </div>    </header>    <p>Our findings above strongly imply that fairness disagreements likely arise out of disagreements in how people assess latent properties of features rather than how they use the latent properties to reason about fairness of using the features. To confirm this implication, we used our model from Subsection <a class="sec" href="#sec-20">5.2</a> to predict the fairness of using different features. Our model is trained on a random subset of AMT responses, and applied on latent property estimates of the remaining AMT responses, to compute the resulting fairness judgments. <figure id="fig4">      <img src="../../../../deliveryimages.acm.org/10.1145/3190000/3186138/images/www2018-147-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Consensus in the fairness predictions of our model, compared to the consensus in the ground truth fairness judgments, assigned by people.</span>      </div>     </figure>    </p>    <p>Figure <a class="fig" href="#fig4">4</a> compares the consensus in the predicted fairness judgments with the consensus in the ground truth fairness judgments, assigned by respondents. We observe that the consensus on predicted fairness ratings tracks the consensus on ground truth judgments of fairness quite well. Given that any lack of consensus in the predicted fairness can be attributed solely to differences in the assessments of the latent properties, we can attribute a significant part of the lack of consensus in human fairness judgments to the differences in people&#x0027;s assessments of latent properties.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Summary</h3>     </div>    </header>    <p>Overall, our findings validate the framework for reaching fairness judgments that we proposed in Section&#x00A0;<a class="sec" href="#sec-6">2</a>. We find significant disagreements in respondents&#x2019; assessments of many latent properties, particularly those related to causal relationships between input features and their causal influence on prediction outcomes. However, across all respondents we find evidence of strong consensus in the reasoning used to reach fairness judgments from latent properties. Specifically, we showed that we could learn a single, simple predictor of fairness judgments from latent property assessments that performs with high accuracy across most of our respondents. For the scenario of our survey, the predictor might be regarded as modeling a common fairness judgment heuristic used by our respondent population. We do not argue that the common heuristic is &#x201C;objectively true&#x201D;. In fact, we expect the heuristic to depend on cultural norms of the society. It would be interesting to conduct similar studies in other societies where people may apply different moral reasoning to the US population considered here.</p>    </section>   </section>   <section id="sec-23">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Concluding Discussion</h2>    </div>    </header>    <p>Most existing works on algorithmic fairness focus on unfairness due to <em>discrimination</em>, where people receive relatively disadvantageous outcomes based on their membership in certain social groups, e.g., based on race or gender. Further, existing works take a <em>normative</em> approach to addressing algorithmic discrimination, i.e., they prescribe how non-discriminatory decisions ought to be made. In contrast, in this work, we take a <em>descriptive</em> approach to algorithmic fairness, i.e., we ask people what they perceive as unfair in decision making, and analyze the reasons behind their unfairness perceptions.</p>    <p>Our study focused on how people perceive the unfairness of using different features describing a defendant to algorithmically predict the defendant&#x0027;s risk of engaging in criminal activity in the near future. Our survey study reveals several interesting findings: (i) People&#x0027;s concerns about the unfairness of using a feature extend far beyond discrimination, including consideration of latent properties such as the relevance of the feature to the decision making scenario and the reliability with which the feature can be assessed. (ii) Unfortunately, there are considerable disagreements on which features different people perceive as unfair to use. (iii) The lack of consensus can be attributed to disagreements in how people assess the latent properties of the features, particularly those related to causal relationships between input features and their causal influence on outcomes. (iv) Encouragingly, different people appear to share a common heuristic (i.e., a similar reasoning) when reaching their fairness judgment from their assessments of latent properties.</p>    <p>Our observations yield several implications for future studies on algorithmic fairness: (i) While there are important reasons based on historic prejudice to mitigate discrimination, there is strong evidence here to consider additional unfairness concerns. (ii) The lack of consensus on causal relationships between input features and outcomes raises challenges for approaches to fairness based on causal reasoning which require a known causal structure (one recent approach can incorporate multiple structures [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>]). (iii) On the other hand, our findings highlight the desirability of trying to gather more objective causal data. (iv) Such objective data on latent properties, if possible, could then be used as inputs to a common fairness heuristic (moral reasoning), which our evidence indicates is shared by most people, to arrive at consensus fairness judgments. It would be interesting to explore the extent to which this heuristic varies across different cultures and decision making contexts.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Amanda&#x00A0;Y Agan and Sonja&#x00A0;B Starr. 2016. Ban the Box, Criminal Records, and Statistical Discrimination: A Field Experiment. (2016).</li>    <li id="BibPLXBIB0002" label="[2]">Fatima&#x00A0;M Albar and Antonie&#x00A0;J Jetter. 2009. Heuristics in Decision Making. In <em>      <em>PICMET</em>     </em>.</li>    <li id="BibPLXBIB0003" label="[3]">Richard&#x00A0;M Alston, James&#x00A0;R Kearl, and Michael&#x00A0;B Vaughan. 1992. Is There a Consensus among Economists in the 1990&#x2019;s?<em>      <em>The American Economic Review</em>     </em>(1992).</li>    <li id="BibPLXBIB0004" label="[4]">Elizabeth&#x00A0;S Anderson. 1999. What is the Point of Equality?<em>      <em>Ethics</em>     </em> (1999).</li>    <li id="BibPLXBIB0005" label="[5]">Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias: There&#x0027;s Software Used Across the Country to Predict Future Criminals. And it&#x0027;s Biased Against Blacks. <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>. (2016).</li>    <li id="BibPLXBIB0006" label="[6]">Richard&#x00A0;J Arneson. 2000. Luck Egalitarianism and Prioritarianism. <em>      <em>Ethics</em>     </em> (2000).</li>    <li id="BibPLXBIB0007" label="[7]">Solon Barocas and Andrew&#x00A0;D. Selbst. 2016. Big Data&#x0027;s Disparate Impact. <em>      <em>California Law Review</em>     </em>(2016).</li>    <li id="BibPLXBIB0008" label="[8]">Francesco Bonchi, Sara Hajian, Bud Mishra, and Daniele Ramazzotti. 2017. Exposing the Probabilistic Causal Structure of Discrimination. <em>      <em>International Journal of Data Science and Analytics</em>     </em> (2017).</li>    <li id="BibPLXBIB0009" label="[9]">Virginia Braun and Victoria Clarke. 2006. Using Thematic Analysis in Psychology. <em>      <em>Qualitative Research in Psychology</em>     </em>(2006).</li>    <li id="BibPLXBIB0010" label="[10]">Michael Buhrmester, Tracy Kwang, and Samuel&#x00A0;D Gosling. 2011. Amazon&#x0027;s Mechanical Turk: A New Source of Inexpensive, yet High-Quality, Data?<em>      <em>Perspectives on Psychological Science</em>     </em>(2011).</li>    <li id="BibPLXBIB0011" label="[11]">Kenneth&#x00A0;P Burnham and David&#x00A0;R Anderson. 2003. <em>      <em>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach</em>     </em>. Springer Science &#x0026; Business Media.</li>    <li id="BibPLXBIB0012" label="[12]">Alexandra Chouldechova. 2016. Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. <em>      <em>arXiv:1610.07524</em>     </em> (2016).</li>    <li id="BibPLXBIB0013" label="[13]">Civil Rights Act. 1964. Civil Rights Act of 1964, Title VII, Equal Employment Opportunities. (1964).</li>    <li id="BibPLXBIB0014" label="[14]">Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic Decision Making and the Cost of Fairness. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0015" label="[15]">Daubert v. Merrell Dow Pharmaceuticals, Inc. 509 U.S. 579 1993.(1993).</li>    <li id="BibPLXBIB0016" label="[16]">Marc Dupuis, Barbara Endicott-Popovsky, and Robert Crossler. 2013. An Analysis of the Use of Amazon&#x0027;s Mechanical Turk for Survey Research in the Cloud. In <em>      <em>ICCSM</em>     </em>.</li>    <li id="BibPLXBIB0017" label="[17]">Cynthia Dwork, Moritz Hardt, Toniann Pitassi, and Omer Reingold. 2012. Fairness Through Awareness. In <em>      <em>ITCSC</em>     </em>.</li>    <li id="BibPLXBIB0018" label="[18]">Hillel&#x00A0;J Einhorn and Robin&#x00A0;M Hogarth. 1981. Behavioral Decision Theory: Processes of Judgement and Choice. <em>      <em>Annual Review of Psychology</em>     </em>(1981).</li>    <li id="BibPLXBIB0019" label="[19]">Federal Rules of Evidence, Rule 401. Test for Relevant Evidence 2015.(2015).</li>    <li id="BibPLXBIB0020" label="[20]">Federal Rules of Evidence, Rule 402. General Admissibility of Relevant Evidence 2015.(2015).</li>    <li id="BibPLXBIB0021" label="[21]">Michael Feldman, Sorelle&#x00A0;A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certifying and Removing Disparate Impact. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0022" label="[22]">Anthony&#x00A0;W. Flores, Christopher&#x00A0;T. Lowenkamp, and Kristin Bechtel. 2016. False Positives, False Negatives, and False Analyses: A Rejoinder to &#x201D;Machine Bias: There&#x0027;s Software Used Across the Country to Predict Future Criminals. And it&#x0027;s Biased Against Blacks.&#x201D;. (2016).</li>    <li id="BibPLXBIB0023" label="[23]">Bruno&#x00A0;S Frey, Werner&#x00A0;W Pommerehne, Friedrich Schneider, and Guy Gilbert. 1984. Consensus and Dissension Among Economists: An Empirical Inquiry. <em>      <em>The American Economic Review</em>     </em>(1984).</li>    <li id="BibPLXBIB0024" label="[24]">Sorelle&#x00A0;A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2016. On the (im)possibility of Fairness. <em>      <em>arXiv:1609.07236</em>     </em> (2016).</li>    <li id="BibPLXBIB0025" label="[25]">Duncan Gallie, Serge Paugam, and Sheila Jacobs. 2003. Unemployment, Poverty and Social Isolation: Is There a Vicious Circle of Social Exclusion?<em>      <em>European Societies</em>     </em>(2003).</li>    <li id="BibPLXBIB0026" label="[26]">Bernard Gert and Joshua Gert. 2017. The Definition of Morality. In <em>      <em>The Stanford Encyclopedia of Philosophy</em>     </em>, Edward&#x00A0;N. Zalta (Ed.). Metaphysics Research Lab, Stanford University.</li>    <li id="BibPLXBIB0027" label="[27]">Gerd Gigerenzer and Wolfgang Gaissmaier. 2011. Heuristic Decision Making. <em>      <em>Annual Review of Psychology</em>     </em>(2011).</li>    <li id="BibPLXBIB0028" label="[28]">Nina Grgi&#x0107;-Hla&#x010D;a, Muhammad&#x00A0;Bilal Zafar, Krishna&#x00A0;P Gummadi, and Adrian Weller. 2016. The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making. In <em>      <em>NIPS Symposium on Machine Learning and the Law</em>     </em>.</li>    <li id="BibPLXBIB0029" label="[29]">Nina Grgi&#x0107;-Hla&#x010D;a, Muhammad&#x00A0;Bilal Zafar, Krishna&#x00A0;P Gummadi, and Adrian Weller. 2018. Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning. In <em>      <em>AAAI</em>     </em>.</li>    <li id="BibPLXBIB0030" label="[30]">Enrique Herrera-Viedma, Jos&#x00E9;&#x00A0;Luis Garc&#x00ED;a-Lapresta, Janusz Kacprzyk, Mario Fedrizzi, Hannu Nurmi, and S&#x0142;awomir Zadro&#x017C;ny. 2011. <em>      <em>Consensual Processes</em>     </em>. Springer.</li>    <li id="BibPLXBIB0031" label="[31]">Panagiotis&#x00A0;G Ipeirotis. 2010. Demographics of Mechanical Turk. (2010).</li>    <li id="BibPLXBIB0032" label="[32]">Faisal Kamiran and Toon Calders. 2010. Classification with No Discrimination by Preferential Sampling. In <em>      <em>BENELEARN</em>     </em>.</li>    <li id="BibPLXBIB0033" label="[33]">Katz v. United States, 389 U.S. 347, 88 S. Ct. 507, 19 L. Ed. 2d 576 1967.(1967).</li>    <li id="BibPLXBIB0034" label="[34]">Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Sch&#x00F6;lkopf. 2017. Avoiding Discrimination through Causal Reasoning. In <em>      <em>NIPS</em>     </em>.</li>    <li id="BibPLXBIB0035" label="[35]">Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent Trade-Offs in the Fair Determination of Risk Scores. In <em>      <em>ITCS</em>     </em>.</li>    <li id="BibPLXBIB0036" label="[36]">J.&#x00A0;A. Krosnick. 2010. <em>      <em>Handbook of Survey Research</em>     </em>.</li>    <li id="BibPLXBIB0037" label="[37]">Solomon Kullback and Richard&#x00A0;A Leibler. 1951. On Information and Sufficiency. <em>      <em>The Annals of Mathematical Statistics</em>     </em>(1951).</li>    <li id="BibPLXBIB0038" label="[38]">Kumho Tire Co. v. Carmichael, 526 U.S. 137 1999.(1999).</li>    <li id="BibPLXBIB0039" label="[39]">Matt&#x00A0;J. Kusner, Joshua&#x00A0;R. Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In <em>      <em>NIPS</em>     </em>.</li>    <li id="BibPLXBIB0040" label="[40]">Binh&#x00A0;Thanh Luong, Salvatore Ruggieri, and Franco Turini. 2011. kNN as an Implementation of Situation Testing for Discrimination Discovery and Prevention. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0041" label="[41]">Paul Mosley and Arjan Verschoor. 2005. Risk Attitudes and the &#x2019;Vicious Circle of Poverty&#x2019;. <em>      <em>The European Journal of Development Research</em>     </em> (2005).</li>    <li id="BibPLXBIB0042" label="[42]">Razieh Nabi and Ilya Shpitser. 2018. Fair Inference on Outcomes. In <em>      <em>AAAI</em>     </em>.</li>    <li id="BibPLXBIB0043" label="[43]">Laurens Naudts. 2017. Fair or Unfair Algorithmic Differentiation? Luck Egalitarianism As a Lens for Evaluating Algorithmic Decision-Making.(2017).</li>    <li id="BibPLXBIB0044" label="[44]">J&#x0119;drzej Niklas, Karolina Sztandar-Sztanderska, and Katarzyna Szymielewicz. 2015. Profiling the Unemployed in Poland: Social and Political Implications of Algorithmic Decision Making. <a class="link-inline force-break" href="https://panoptykon.org/sites/default/files/leadimage-biblioteka/panoptykon_profiling_report_final.pdf">https://panoptykon.org/sites/default/files/leadimage-biblioteka/panoptykon_profiling_report_final.pdf</a>. (2015).</li>    <li id="BibPLXBIB0045" label="[45]">Gabriele Paolacci, Jesse Chandler, and Panagiotis&#x00A0;G Ipeirotis. 2010. Running Experiments on Amazon Mechanical Turk. (2010).</li>    <li id="BibPLXBIB0046" label="[46]">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. <em>      <em>Journal of Machine Learning Research</em>     </em>(2011).</li>    <li id="BibPLXBIB0047" label="[47]">Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. 2008. Discrimination-aware Data Mining. In <em>      <em>KDD</em>     </em>.</li>    <li id="BibPLXBIB0048" label="[48]">Pew Research Center. 2016. 2016 Party Identification Detailed Tables. (2016). <a class="link-inline force-break" href="http://www.people-press.org/2016/09/13/2016-party-identification-detailed-tables/">http://www.people-press.org/2016/09/13/2016-party-identification-detailed-tables/</a></li>    <li id="BibPLXBIB0049" label="[49]">Stanley Presser, Mick&#x00A0;P. Couper, Judith&#x00A0;T. Lessler, Elizabeth Martin, Jean Martin, Jennifer&#x00A0;M. Rothgeb, and Eleanor Singer. 2004. Methods for Testing and Evaluating Survey Questions. <em>      <em>Public Opinion Quarterly</em>     </em>(2004).</li>    <li id="BibPLXBIB0050" label="[50]">ProPublica. 2016. COMPAS CORE Sample Risk Asessment Questionnaire. <a href="https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html">https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html</a>. (2016).</li>    <li id="BibPLXBIB0051" label="[51]">Bilal Qureshi, Faisal Kamiran, Asim Karim, and Salvatore Ruggieri. 2016. Causal Discrimination Discovery Through Propensity Score Analysis. <em>      <em>arXiv preprint arXiv:1608.03735</em>     </em>(2016).</li>    <li id="BibPLXBIB0052" label="[52]">Elissa&#x00A0;M Redmiles, Yasemin Acar, Sascha Fahl, and Michelle&#x00A0;L Mazurek. 2017. <em>      <em>A Summary of Survey Methodology Best Practices for Security and Privacy Researchers</em>     </em>. Technical Report.</li>    <li id="BibPLXBIB0053" label="[53]">Joel Ross, Lilly Irani, M Silberman, Andrew Zaldivar, and Bill Tomlinson. 2010. Who Are The Crowdworkers?: Shifting Demographics in Mechanical Turk. In <em>      <em>CHI</em>     </em>.</li>    <li id="BibPLXBIB0054" label="[54]">Chris Russell, Matt&#x00A0;J Kusner, Joshua Loftus, and Ricardo Silva. 2017. When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness. In <em>      <em>NIPS</em>     </em>.</li>    <li id="BibPLXBIB0055" label="[55]">Claude&#x00A0;E Shannon. 1948. A Mathematical Theory of Communication. <em>      <em>The Bell System Technical Journal</em>     </em>(1948).</li>    <li id="BibPLXBIB0056" label="[56]">William&#x00A0;J Tastle and Mark&#x00A0;J Wierman. 2007. Consensus and Dissention: A Measure of Ordinal Dispersion. <em>      <em>International Journal of Approximate Reasoning</em>     </em> (2007).</li>    <li id="BibPLXBIB0057" label="[57]">U.S. Census Bureau. 2016. American Community Survey 5-Year Estimates. (2016).</li>    <li id="BibPLXBIB0058" label="[58]">Weeks v. United States, 232 U.S. 383, 34 S.Ct. 341, 58 L.Ed. 652 1914.(1914).</li>    <li id="BibPLXBIB0059" label="[59]">Gordon&#x00A0;B. Willis. 2005. <em>      <em>Cognitive Interviewing: A Tool for Improving Questionnaire Design</em>     </em>.</li>    <li id="BibPLXBIB0060" label="[60]">Muhammad&#x00A0;Bilal Zafar, Isabel Valera, Manuel&#x00A0;Gomez Rodriguez, and Krishna&#x00A0;P. Gummadi. 2017. Fairness Beyond Disparate Treatment &#x0026; Disparate Impact: Learning Classification without Disparate Mistreatment. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0061" label="[61]">Muhammad&#x00A0;Bilal Zafar, Isabel Valera, Manuel&#x00A0;Gomez Rodriguez, and Krishna&#x00A0;P. Gummadi. 2017. Fairness Constraints: Mechanisms for Fair Classification. In <em>      <em>AISTATS</em>     </em>.</li>    <li id="BibPLXBIB0062" label="[62]">R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. 2013. Learning Fair Representations. In <em>      <em>ICML</em>     </em>.</li>    <li id="BibPLXBIB0063" label="[63]">Lu Zhang and Xintao Wu. 2017. Anti-Discrimination Learning: A Causal Modeling-Based Framework. <em>      <em>International Journal of Data Science and Analytics</em>     </em> (2017).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>Elissa Redmiles acknowledges support from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1322106.</p>   <p id="fn2"><a href="#foot-fn2"><sup>&#x2020;</sup></a>Adrian Weller acknowledges support from the David MacKay Newton research fellowship at Darwin College, The Alan Turing Institute under EPSRC grant EP/N510129/1 &#x0026; TU/B/000074, and the Leverhulme Trust via the CFI.</p>   <p id="fn3"><a href="#foot-fn3"><sup>1</sup></a>COMPAS may also be used for criminal sentencing and parole decisions, but we do not explore those uses here since they involve other relevant factors such as considering the role of long term incarceration in society.</p>   <p id="fn4"><a href="#foot-fn4"><sup>2</sup></a>For a full description of the features, as used in the survey, please see <a class="link-inline force-break"    href="https://fate-computing.mpi-sws.org/procedural_fairness/">https://fate-computing.mpi-sws.org/procedural_fairness/</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>3</sup></a>For the full results on the SSI dataset, please see <a class="link-inline force-break"    href="https://fate-computing.mpi-sws.org/procedural_fairness/">https://fate-computing.mpi-sws.org/procedural_fairness/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186138">https://doi.org/10.1145/3178876.3186138</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
