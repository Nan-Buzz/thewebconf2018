<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Training 100,000 Classes on a Single Titan X in 7 Hours or 15 Minutes with 25 Titan Xs&#x204E;&#x204E;This talk is based on https://openreview.net/pdf?id=r1RQdCg0W </title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../../dl.acm.org/pubs/lib/css/main.css"/><script src="../../../../dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../../dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../../dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Training 100,000 Classes on a Single Titan X in 7 Hours or 15 Minutes with 25 Titan Xs<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>      </span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Anshumali</span>      <span class="surName">Shrivastava</span>     Rice University, 6100 Main StreetHouston, TX 77005, <a href="mailto:anshumali@rice.edu">anshumali@rice.edu</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3193135" target="_blank">https://doi.org/10.1145/3184558.3193135</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>In this talk, I will present Merged-Averaged Classifiers via Hashing (MACH) for <em>K</em>-classification with ultra-large values of <em>K</em>. Compared to traditional one-vs-all classifiers that require <em>O</em>(<em>Kd</em>) memory and inference cost, MACH only need <em>O</em>(<em>d</em>log&#x2009;<em>K</em>) (<em>d</em> is dimensionality) memory while only requiring <em>O</em>(<em>K</em>log&#x2009;<em>K</em> + <em>d</em>log&#x2009;<em>K</em>) operation for inference. MACH is a generic <em>K</em>-classification algorithm, with provably theoretical guarantees, without any assumption on the relationship between classes. MACH uses universal hashing to reduce classification with a large number of classes to few (logarithmic many) independent classification tasks with small (constant) number of classes. I will show the first quantification of discriminability-memory tradeoff in multi-class classification. Using the simple idea of hashing, we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU, with the classification accuracy of 19.28%, which is the best-reported accuracy on this dataset. Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (160 GB model size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy with 480x reduction in the model size (of mere 0.3GB). With MACH, we also demonstrate complete training of feature extracted fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU. To the best of our knowledge, this is the first work to demonstrate complete training of these extreme-class datasets on a single Titan X. Furthermore, the algorithm is trivially parallelizable. Our experiments show that we can train ODP datasets in 7 hours on a single GPU or in 15 minutes with 25 GPUs. Similarly, we can train classifiers over the fine-grained imagenet dataset in 24 hours on a single GPU which can be reduced to little over 1 hour with 20 GPUs.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Mathematics of computing </strong>&#x2192; <strong>Probabilistic algorithms;</strong> <strong>Information theory;</strong> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Clustering and classification;</strong> &#x2022;<strong> Theory of computation </strong>&#x2192; <strong>Distributed algorithms;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Parallel computing methodologies;</strong> <strong>Machine learning;</strong> <strong>Supervised learning by classification;</strong> <strong>Neural networks;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Large-scale Learning</small>, </span>     <span class="keyword">      <small> Extreme Classification</small>, </span>     <span class="keyword">      <small> Hashing</small>, </span>     <span class="keyword">      <small> Sketching</small>, </span>     <span class="keyword">      <small> Distributed Machine Learning</small>, </span>     <span class="keyword">      <small> GPU</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Anshumali Shrivastava. 2018. Training 100,000 Classes on a Single Titan X in 7 Hours or 15 Minutes with 25 Titan Xs. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 1 Pages. <a href="https://doi.org/10.1145/3184558.3193135" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3193135</a></small>     </p>     </div>    </div>   </section>  </section>  </body> </html> 
