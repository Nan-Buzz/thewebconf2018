<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>The missing science of knowledge curation</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">The missing science of knowledge curation</span>      <br/>      <span class="subTitle">       <SubTitle>(Improving incentives for large-scale knowledge curation)</SubTitle>      </span>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Praveen</span>      <span class="surName">Paritosh</span>     Google, <a href="mailto:pkp@google.com">pkp@google.com</a>         </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191551" target="_blank">https://doi.org/10.1145/3184558.3191551</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Dictionaries, encyclopedias, knowledge graphs, annotated corpora, library classification systems and world maps are all examples of human-curated knowledge resources that have been highly valuable to science as well as amortized across multiple large-scale systems in practice. Many of these were started and built even before a crowdsourcing research community existed. While the last decade has seen unprecedented growth in research and practice in building crowdsourcing systems to do increasingly complex tasks at scale, many of these resources are still woefully incomplete&#x2014;lacking coverage in languages and subject matter domains. Moreover, many knowledge resources needed to fill other semantic gaps for artificial intelligence systems simply don&#x0027;t exist or aren&#x0027;t being built. Why? I argue that we don&#x0027;t have the right incentives, and that in order to improve the incentives, we have some fundamental scientific questions to answer. While building a large knowledge resource, we have little more than intuitions when it comes to estimating the reusability, maintainability, and long-term value of the effort. These make it difficult to fund or manage such projects, often requiring herculean personalities or fortunate businesses. Building or expanding a resource is often not seen as &#x201C;sexy,&#x201D; which results in lack of resources to answer those questions in any principled manner. These problems begin to outline a new science of curation, making progress on which could help improve the discussion around and funding for building sorely needed knowledge resources.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>Praveen Paritosh. 2018. The missing science of knowledge curation:(Improving incentives for large-scale knowledge curation). In <em>Proceedings of The 2018 Web Conference Companion (WWW'18 Companion). ACM, New York, NY, USA, 3 pages.</em> <a href="https://doi.org/10.1145/3184558.3191551" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191551</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-001">    <header>     <div class="title-info">     <h2>The unreasonable effectiveness of human curation</h2>     </div>    </header>    <p>Large-scale, human-curated knowledge resources such as dictionaries, encyclopedias, knowledge graphs, annotated corpora, world maps, and library classification systems are substantial investments of resources. The cost of building and maintaining these resources has been amortized across multiple large-scale systems, where they have been shown to be disproportionately more valuable than their cost. For example, linguistic knowledge resources such as treebanks and inventories such as wordnet are used by most information and question answering systems. Factual knowledge resources such as knowledge graphs and structured world maps have been key for question answering systems such as Watson, search engines, and for entity and relation extraction body of research. Despite these successes, there are some widely held myths about human curation.</p>   </section>   <section id="sec-002">    <header>     <div class="title-info">     <h2>Myths shaping the incentives for human curation</h2>     </div>    </header>    <p>I believe that we have failed to create proper incentives to foster investments in building and scaling these knowledge resources. Here are the narratives that have shaped these broken incentives, which should be revisited given the recent successes in large-scale curation and crowdsourcing: <ul class="list-no-style">     <li>Expanding a knowledge resource is not new science.<br/></li>     <li>Machine-learned resources are more scalable.<br/></li>     <li>Long-term value of knowledge resources is impossible to quantify.<br/></li>     </ul>    </p>    <p>Let us examine each one of these myths more closely.</p>    <p>     <strong>&#x201C;Expanding a knowledge resource is not new science&#x201D;</strong>    </p>    <p>Enumerating by hand is considered a weak, if not the weakest, scientific theory. However, expanding it in a reusable and extensible way is! Just as no cartographer starts from scratch, it is the scientific obligation of a knowledge resource to be easy for future users and developers to extend it, as well as to be interoperable with other resources. Can others reuse and maintain it? Does it share identifiers and vocabulary with other resources? What other best practices can help make the content of knowledge resources more enduring?</p>    <p>The Pareto principle might be good news to those who care about the content, suggesting that a small catalog can capture a large number of cases in the world. But current incentives focus disproportionate research on the long tail of edge cases, while never cataloging the head with its high explanatory power per unit cost. There is a lot of science in modeling and in building elegant formalisms to house the data that will forever remain empty if we don&#x0027;t value collecting the data.</p>    <p>     <strong>&#x201C;Machine-learned resources are more scalable&#x201D;</strong>    </p>    <p>Just to take a case study, the research community (both academic and industrial) has spent far more resources in automatically expanding resources, such as Freebase, than in building them in the first place. There are early results in expanding the easy parts, using the human-curated core as a bootstrap. Even after substantial investment in automatically curating and expanding Freebase, most successful knowledge resources are almost entirely human curated. In addition, machine-learned resources critically depend on human-curated resources such as Wikipedia.</p>    <p>     <strong>&#x201C;Long-term value of knowledge resources is impossible to quantify&#x201D;</strong>    </p>    <p>One-off solutions always fare better in this calculus of immediate utility-based funding than something that will be more expensive up front but amortize later. This is a very tricky one, as currently seen, most knowledge resources are in service of some systems. Thus their evaluation and success is intrinsically tied to the system&#x0027;s success. This makes the evaluation both myopic to a single utility versus amortization across new forms of usage that come in the wake of the existence of the resource. The ways in which dictionaries and wordnets are being put to by IR/QA systems could not be easily imagined when it was being built.</p>    <p>However, we have learned to counteract this in fields, such as software development, that benefit from amortization. We have learned to fund and incentivize infrastructure building and capacity planning of assets such as factories and data centers. To the extent that there are more uncertainties in characterizing the specs and risks of knowledge resource projects than other long-term investments, we need best practices and supporting science for estimating current and future value of knowledge resources.</p>   </section>   <section id="sec-003">    <header>     <div class="title-info">     <h2>We need a new science of knowledge curation</h2>     </div>    </header>    <p>The issue with incentives is knowing when things are &#x201C;better.&#x201D; Understanding and scaling curation is about characterizing the content, not the form. Having strong empirical or theoretical understanding of these issues would help reduce the risk in making larger investments.</p>    <p>While there is a rigorous foundation and understanding of formalisms used to write knowledge&#x2014;whether it be triples, frames, logics, or controlled vocabularies&#x2014;we don&#x0027;t know much about the best practices of how to write a billion assertions in a reusable and maintainable manner in any of those formalisms. Currently, most of this knowledge of practice is tucked away in curation guidelines and the minds of the curators.</p>    <p>Unlike the foundations of the formalisms, which can be done in the artificial universe of mathematics, knowledge curation is a human task and thus any understanding of this requires venturing into fields as diverse as survey design, linguistics, psychology, sociology, the so-called soft sciences. In other words, there are hard scientific problems about characterizing the curation and the content of large-scale knowledge resource building efforts.</p>    <p>In order to improve the incentives, we need a science of knowledge curation that addresses these misconceptions and answers questions about how to measure and compare reliability, coverage, explanatory power, progress, utility, and the long-term value of knowledge resource building efforts.</p>   </section>  </section>  <section class="back-matter">   <section id="bib-sec-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">References</h2>     </div>    </header>    <ul class="bibUl">     <li id="bib1" label="[1]">Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and Taylor, J. (2008). Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. In Proceedings of the 28th ACM SIGMOD/PODS International Conference on Management of Data (SIGMOD 2008), Vancouver, Canada.</li>     <li id="bib2" label="[2]">Chang, N., Paritosh, P., Huynh, D., &amp; Baker, C. (2015, June). Scaling Semantic Frame Annotation. In LAW@ NAACL-HLT (pp. 1-10).</li>     <li id="bib3" label="[3]">Ferrucci, David, <em>et al.</em> "Building Watson: An overview of the DeepQA project." AI magazine 31.3 (2010): 59-79.</li>     <li id="bib4" label="[4]">Han, S., Dai, P., Paritosh, P.&#x200B;, and Huynh, D. (2016). Crowdsourcing Human Annotation on Web Page Structure: Infrastructure Design and Behavior-Based Quality Control. ACM Transactions on Intelligent Systems and Technology (TIST), 7(4), 56.</li>     <li id="bib5" label="[5]">Ipeirotis, P., and Paritosh, P. (2011). Managing Crowdsourced Human Computation. Tutorial presented at WWW 2011. In Proceedings of the 20th International World Wide Web conference, Hyderabad, India.</li>     <li id="bib6" label="[6]">Josephy, T., Lease, M., Paritosh, P., (2014). Crowdsourcing at Scale workshop report. AI Magazine, vol 35, No. 1.</li>     <li id="bib7" label="[7]">Kochhar, S., Mazzocchi, S., and Paritosh, P. (2010). The Anatomy of a Large-Scale Human Computation Engine, In Proceedings of Human Computation Workshop at the 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2010, Washington D.C.</li>     <li id="bib8" label="[8]">Mintz, M., Bills, S., Snow, R., &amp; Jurafsky, D. (2009). Distant supervision for relation extraction without labeled data, ACL, Association for Computational Linguistics.</li>     <li id="bib9" label="[9]">Paritosh, P. (2012). Human Computation Must Be Reproducible. In Proceedings of CrowdSearch: Crowdsourcing Web Search at the 21st International World Wide Web Conference (WWW), Lyon, France.</li>     <li id="bib10" label="[10]">Riezler, S. (2014). On the problem of theoretical terms in empirical computational linguistics. Computational Linguistics, 40(1), 235-245.</li>     <li id="bib11" label="[11]">Sameki, M., Barua, A., and Paritosh, P. (2016). Rigorously Collecting Commonsense Judgments for Complex Question-Answer Content. In Third AAAI Conference on Human Computation and Crowdsourcing.</li>     <li id="bib12" label="[12]">Welty, C. A., &amp; Jenkins, J. (1999). Formal ontology for subject. Data &amp; Knowledge Engineering, 31(2), 155-181.</li>     <li id="bib13" label="[13]">Zhang, A. X., Culbertson, B., and Paritosh, P. (2017). Characterizing Online Discussion Using Coarse Discourse Sequences, In 11th AAAI International Conference on Web and Social Media (ICWSM).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191551">https://doi.org/10.1145/3184558.3191551</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
