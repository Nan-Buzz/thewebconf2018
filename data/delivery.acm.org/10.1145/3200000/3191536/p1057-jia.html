<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>TempQuestions: A Benchmark for Temporal Question Answering</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">TempQuestions: A Benchmark for Temporal Question Answering</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Zhen</span>     <span class="surName">Jia</span><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>,     School of Information Science, Southwest Jiaotong University China, <a href="mailto:zjia@swjtu.edu.cn">zjia@swjtu.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Abdalghani</span>     <span class="surName">Abujabal</span>,     Max Planck Institute for Informatics, Saarland Informatics Campus, Germany, <a href="mailto:abujabal@mpi-inf.mpg.de">abujabal@mpi-inf.mpg.de</a>    </div>    <div class="author">     <span class="givenName">Rishiraj Saha</span>     <span class="surName">Roy</span>,     Max Planck Institute for Informatics, Saarland Informatics Campus, Germany, <a href="mailto:rishiraj@mpi-inf.mpg.de">rishiraj@mpi-inf.mpg.de</a>    </div>    <div class="author">     <span class="givenName">Jannik</span>     <span class="surName">Str&#x00F6;tgen</span>,     Max Planck Institute for Informatics, Saarland Informatics Campus, Germany, <a href="mailto:jannik.stroetgen@mpi-inf.mpg.de">jannik.stroetgen@mpi-inf.mpg.de</a>    </div>    <div class="author">     <span class="givenName">Gerhard</span>     <span class="surName">Weikum</span>,     Max Planck Institute for Informatics, Saarland Informatics Campus, Germany, <a href="mailto:weikum@mpi-inf.mpg.de">weikum@mpi-inf.mpg.de</a>    </div>                        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3191536" target="_blank">https://doi.org/10.1145/3184558.3191536</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Answering complex questions is one of the challenges that question-answering (QA) systems face today. While complexity has several facets, question dimensions like temporal and spatial intents necessitate specialized treatment. Methods geared towards such questions need benchmarks that reflect the desired aspects and challenges. Here, we take a key step in this direction, and release a new benchmark, <em>TempQuestions</em>, containing 1,271 questions, that are all <em>temporal</em> in nature, paired with their answers. As a key contribution that enabled the creation of this resource, we provide a crisp <em>definition</em> for temporal questions. Most questions require decomposing them into sub-questions, and the questions are of a kind that they would be best evaluated on a combination of structured data and unstructured text sources. Experiments with two QA systems demonstrate the need for further research on complex questions.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Test collections;</strong></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Question answering; Temporal questions; Benchmarks</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Str&#x00F6;tgen, and Gerhard Weikum. 2018. TempQuestions: A Benchmark for Temporal Question Answering. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 7 Pages. <a href="https://doi.org/10.1145/3184558.3191536" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3191536</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>    <strong>Motivation.</strong> Answering natural-language questions (QA) has been intensively researched over the last few decades. Earlier approaches, up to when IBM Watson won the Jeopardy! quiz show, have mostly tapped into textual sources (including Wikipedia articles) using passage retrieval and other techniques &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. In the last few years, the paradigm of translating questions into formal queries over structured knowledge bases (KBs) and data bases (DBs, including Linked Open Data) has become prevalent &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>].</p>    <p>QA over structured data (KB-QA) translates the terms in a question into the vocabulary of the underlying KB or DB: entity names, semantic types, and predicate names for attributes and relations. State-of-the-art systems (e.g.,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>]) perform well for simple questions that involve a few predicates around a single target entity (or a qualifying entity list). A typical question is:</p>    <p>    <img src="http://deliveryimages.acm.org/10.1145/3200000/3191536/images/www18companion-275-img1.svg" class="img-responsive" alt=""      longdesc=""/> which can be translated into a SPARQL query, like:</p>    <p>    <img src="http://deliveryimages.acm.org/10.1145/3200000/3191536/images/www18companion-275-img2.svg" class="img-responsive" alt=""      longdesc=""/>    </p>    <p>with the answer: <em>&#x2018;The Fifth Element&#x2019;</em>.</p>    <p>However, KB-QA has limitations regarding <em>complex questions</em> that require decomposing the input into sub-questions. A typical example is (with the answer being <em>&#x2018;Milla Jovovich&#x2019;</em>):</p>    <p>    <img src="http://deliveryimages.acm.org/10.1145/3200000/3191536/images/www18companion-275-img3.svg" class="img-responsive" alt=""      longdesc=""/> Here, a SPARQL query would require multiple query variables, and a three-way join between actresses, movies and directors. Such complex questions are too difficult for today&#x0027;s KB-QA systems. Decomposing the question into <em>&#x201C;actress in a Besson movie&#x201D;</em> and <em>&#x201C;actress married Besson&#x201D;</em>, and subsequently intersecting their results would be a viable execution plan, though.</p>    <p>The need for this kind of decomposition arises for all kinds of complex questions. In this paper, we focus on a specific kind of user input, namely, <em>temporal questions</em>. A substantial fraction of online information needs are time-dependent &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. Even when a search request does not explicitly refer to dates or events, computing answers may require testing temporal conditions. Consider the example (the answer again being <em>&#x2018;Milla Jovovich&#x2019;</em>):</p>    <p>    <img src="http://deliveryimages.acm.org/10.1145/3200000/3191536/images/www18companion-275-img4.svg" class="img-responsive" alt=""      longdesc=""/> A QA system could decompose this into sub-questions like SQ1: <em>&#x201C;science fiction movies directed by Luc Besson&#x201D;</em>, SQ2: <em>&#x201C;actresses starring in Luc Besson movies&#x201D;</em>, and SQ3: <em>&#x201C;actresses married to Luc Besson&#x201D;</em>. Additionally, we need to filter the results of SQ1 to identify the first (i.e., earliest in time) answer, and we need to test the year of the movie against the date of the marriages for the results of SQ3 to rule out spouses who pre-dated that movie.</p>    <p>An ideal execution plan for this complex question needs to compute this <em>decomposition</em>, and also needs to generate the post-processing in terms of <em>reasoning</em> about time points and intervals. The latter is a new aspect that KB-QA has not considered so far. Prior work on text-oriented QA discussed this point &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] but did not aim for general solutions.</p>    <p>    <strong>Contribution.</strong>The quality of QA is usually evaluated by benchmarks. As a first step towards addressing the challenge of handling complex questions, we offer a new benchmark set of temporal questions. The questions are chosen such that many of them require a combination of evaluating sub-questions and reasoning over sub-results (results of the sub-questions).</p>    <p>There already exists a variety of QA benchmarks. For KB-QA, the Free917&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] and WebQuestions &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] collections are the most popular. Both are vastly dominated by simple questions and do not exercise a system&#x0027;s capability to decompose and process complex questions. The QALD series of evaluation tasks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] includes both simple and complex questions. However, the number of questions per year is relatively small (50 &#x2212; 250 questions). The ComplexQuestions collection of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] contains various types of complex questions: however, temporal questions present only a small fraction. For text-oriented QA, the TREC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] and CLEF&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] conference series offer a wealth of benchmark questions, but there is no design consideration on harnessing structured data at all.</p>    <p>The benchmark proposed in this paper, called <em>TempQuestions</em>, consists of 1,271 temporal questions with gold-standard answers. This collection is derived by judiciously selecting time-related questions from the Free917, WebQuestions and ComplexQuestions sets, with additional curation and tagging of temporal cues.</p>    <p>Our benchmark supports systematic testing and evaluation of how well QA systems can handle temporal questions that require decomposition and reasoning on sub-results. We ran the benchmark with two state-of-the-art QA systems, AQQU&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] and QUINT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>], where source code is available, and found that both performed marginally. This shows that there is ample room for improvement, and emphasizes the need for research on complex questions. TempQuestions is publicly available at the following link: <a class="link-inline force-break"     href="http://qa.mpi-inf.mpg.de/TempQuestions.zip">http://qa.mpi-inf.mpg.de/TempQuestions.zip</a>. </p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Defining Temporal Questions</h2>    </div>    </header>    <p>There are diverse types of questions with a temporal aspect. Questions can contain temporal expressions or signals to express temporal relations. Furthermore, questions may ask for some kind of temporal information, e.g., a date. However, to concisely define <em>temporal questions</em>, these concepts, i.e., temporal expressions and temporal signals, need to be precisely specified as well. In this section, we first explain these concepts, which are typically used for temporal information annotation in the context of natural language processing (NLP). Then, we define <em>temporal questions</em> based on these existing concepts, which we extend according to the requirements for temporal QA, as explained below.</p>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Temporal Expressions</h3>     </div>    </header>    <p>In NLP, the temporal markup language TimeML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>] is frequently used for annotating temporal information in text documents. It is also the annotation standard used by most tools, which perform temporal annotation automatically, e.g., temporal taggers for temporal expressions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>].</p>    <p>Besides tags for events and temporal relations between two TimeML entities, TimeML contains <tt>TIMEX3</tt> tags for temporal expressions and <tt>SIGNAL</tt> tags for temporal signals (cf.&#x00A0;Sec.&#x00A0;<a class="sec" href="#sec-9">2.2</a>). The <tt>TIMEX3</tt> tag is used to annotate temporal expressions of four types: date, time, duration, and set expressions. The semantics of all temporal expressions can be normalized to some value in a standard format, which allows the comparison between temporal expressions &#x2013; a characteristic of temporal information, which can also be exploited for temporal QA. TimeML&#x0027;s most important attribute to capture the temporal information of temporal expressions is the <tt>value</tt> attribute. In the case of duration and set expressions, the <tt>value</tt> attribute captures the length of the interval, and the <tt>value</tt> attribute of date and time expressions contains information how to anchor the point in time on a timeline of the respective granularity.</p>    <p>According to TimeML&#x0027;s specifications, set expressions refer to the re-occurring nature of an event. Examples are <em>&#x2018;once a week&#x2019;</em> and <em>&#x2018;daily&#x2019;</em>. Duration expressions are used to specify the length of an interval. For instance, <em>&#x2018;three weeks&#x2019;</em> and <em>&#x2018;several years&#x2019;</em> are two duration expressions. Note that the temporal information might be concrete as in <em>&#x2018;three weeks&#x2019;</em> or vague as in <em>&#x2018;several years&#x2019;</em>. Date and time expressions both refer to points in time &#x2013; though the points in time are of different granularities: all granularities smaller than <em>&#x2018;day&#x2019;</em> are considered as time expressions, for instance, expressions referring to parts of a day (e.g., <em>&#x2018;Monday morning&#x2019;</em> and <em>&#x2018;yesterday night&#x2019;</em>) and expressions referring to a specified time (e.g., <em>&#x2018;9 pm&#x2019;</em>, <em>&#x2018;three o&#x0027;clock&#x2019;</em> and <em>&#x2018;February 5, 2018 23:59:59 CET&#x2019;</em>). In contrast, date expressions may refer to a particular day (e.g., <em>&#x2018;last Thursday&#x2019;</em> and <em>&#x2018;23rd of November&#x2019;</em>) or to any point in time of a coarser granularity (e.g., <em>&#x2018;the 21st century&#x2019;</em>, <em>&#x2018;last year&#x2019;</em> and <em>&#x2018;September 2016&#x2019;</em>).</p>    <p>Note that these examples directly show that date and time expressions can be realized in different ways: fully-specified, relatively specified, underspecified, or implicitly specified&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. Fully-specified expressions can be normalized without any further context information (e.g., <em>&#x2018;September 2016&#x2019;</em> as <tt>2016-09</tt>). In contrast, relative expressions require a reference time (e.g., <em>&#x2018;last Thursday&#x2019;</em>) and underspecified expressions need a reference time and a relation to the reference time (e.g., <em>&#x2018;(on) Thursday&#x2019;</em>). In both cases, the reference time might be the time of the sentence or a date mentioned in the textual context. If relative and underspecified date and time expressions occur in NL questions, it is thus important that the information about when the question was formulated is also available. Otherwise, questions such as <em>&#x201C;Who was the US president two years ago?&#x201D;</em> cannot be answered as it is impossible to determine to which year <em>&#x2018;two years ago&#x2019;</em> refers.</p>    <p>Finally, non-standard temporal knowledge is required for normalizing implicit expressions such as holidays (e.g., <em>&#x2018;Columbus Day 2018&#x2019;</em> &#x2013; which is, in the US, the second Monday in October). In some works, the definition of implicit temporal expressions has been extended to further include all types of free-text temporal expressions, such as event names or other textual phrases with temporal scopes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] (e.g., &#x00A0;<em>&#x2018;Obama&#x0027;s presidency&#x2019;</em>, which can be normalized to an interval with a particular start and end date).</p>    <p>In the creation and analysis of our benchmark (Sec.&#x00A0;<a class="sec" href="#sec-11">3</a> and&#x00A0;<a class="sec" href="#sec-12">4</a>), we will consider questions with fully-specified, underspecified, and relative temporal expressions as <em>explicit temporal questions</em>, in contrast to <em>implicit temporal questions</em>, which contain implicit temporal expressions including free-text temporal expressions.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Temporal Signals</h3>     </div>    </header>    <p>TimeML defines <em>temporal signals</em> as textual elements that make explicit the <em>temporal relation</em> between two TimeML entities (events or temporal expressions), such as <em>&#x2018;before&#x2019;</em> or&#x00A0;<em>&#x2018;during&#x2019;</em>. In natural language (NL) questions, signals occur, for instance, to explicitly specify a valid time interval for the searched information, as in: <em>&#x201C;Which movies did Besson work on before his marriage to Jovovich?&#x201D;</em>. Note that we relax the TimeML definition to consider all trigger terms as temporal signals, even if one of the entities is not mentioned explicitly, but is the answer of a question, e.g., in when-questions. <figure id="fig1">      <img src="http://deliveryimages.acm.org/10.1145/3200000/3191536/images/www18companion-275-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">The 13 temporal relations (nos. 2 through 7 have inverses)between two intervals X and Y, as in Allen&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0003">3</a>].</span>      </div>     </figure>    </p>    <p>In general, any of the 13 temporal relations defined in Allen&#x0027;s interval algebra for temporal reasoning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] can be the described relation, that is, the <tt>equal</tt> relation as well as the six relations <tt>before</tt>, <tt>meets</tt>, <tt>overlaps</tt>, <tt>during</tt>, <tt>starts</tt>, and <tt>finishes</tt> with respective inverses (see Fig.&#x00A0;<a class="fig" href="#fig1">1</a> for visualizations of the relations). However, due to ambiguities, it is often not possible to select a unique temporal relation for a temporal question. For example, the question <em>&#x201C;What did Besson work on before his marriage to Jovovich?&#x201D;</em> could be interpreted as asking for either the movie he was working on directly before his marriage or all movies which he was working on any time before his marriage.</p>    <p>It is crucial to point out that NL questions are often formulated with even further ambiguities. While the question <em>&#x201C;Which movies did Besson work on before his marriage to Jovovich?&#x201D;</em> as well as <em>&#x201C;Which movie did Besson work on before his marriage to Jovovich?&#x201D;</em> concisely describe the required number of answer movies (several due to plural and one due to singular, respectively), the latter requires the movie which Besson worked on directly before his marriage, i.e., the <em>temporal constraint</em> cannot be simply validated, but valid answers have to be sorted and the closest one has to be chosen. In addition, the slightly reformulated question <em>&#x201C;What did Besson work on before his marriage to Jovovich?&#x201D;</em> could be interpreted one way or the other (singular or plural) &#x2013; a fact that also makes it sometimes difficult, even for humans, to determine the correct answer of a question.</p>    <p>Due to such ambiguities, in the context of temporal QA, temporal relations could be simplified as the following three types:</p>    <ul class="list-no-style">     <li id="uid5" label="(i)"><tt>before</tt> and <tt>meet</tt> are treated as the relation <tt>BEFORE</tt>      <br/></li>     <li id="uid6" label="(ii)"><tt>before_inverse</tt> and <tt>meet_inverse</tt> are treated as <tt>AFTER</tt>      <br/></li>     <li id="uid7" label="(iii)">all other relations are treated as <tt>OVERLAP</tt>      <br/></li>    </ul>    <p>Typical <em>trigger words</em> suggesting the three temporal relations above, respectively, are the <em>temporal signals</em>:</p>    <ul class="list-no-style">     <li id="uid8" label="(i)"><em>&#x2018;before&#x2019;</em>, <em>&#x2018;prior to&#x2019;</em>      <br/></li>     <li id="uid9" label="(ii)"><em>&#x2018;after&#x2019;</em>, <em>&#x2018;following&#x2019;</em>      <br/></li>     <li id="uid10" label="(iii)"><em>&#x2018;during&#x2019;</em>, <em>&#x2018;while&#x2019;</em>, <em>&#x2018;when&#x2019;</em>, <em>&#x2018;until&#x2019;</em>, <em>&#x2018;in&#x2019;</em>, <em>&#x2018;at the same time&#x2019;</em>      <br/></li>    </ul>    <p>In addition to the trigger terms defined in TimeML, we add ordinals to the class of temporal signals, as they are often used in NL questions to specify particular instances of items which can be sorted chronologically. An example is <em>&#x2018;last&#x2019;</em> in <em>&#x201C;What was Besson&#x0027;s last movie before his marriage to Jovovich?&#x201D;</em>.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Temporal Questions</h3>     </div>    </header>    <p>Based on the extended concepts of temporal expressions and temporal signals, we can now concisely define a temporal question:</p>    <div class="definition" id="enc1">     <Label>Definition 2.1.</Label>     <p> A <em>temporal question</em> is any question, which contains a temporal expression, a temporal signal, or whose answer is of temporal nature.</p>    </div>    <p>Note that this definition is purely <em>semantic</em>. In practice, these categories are detected by matching against patterns and lexicons (Sec.&#x00A0;<a class="sec" href="#sec-11">3</a>), accompanied by subsequent reasoning to remove false positives. Thus, various detection techniques (say, for temporal expressions with varying levels of implicitness considerations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]), may have different recall in the retrieval of temporal questions from a given corpus. Also, note that a temporal question may contain multiple temporal signals and temporal expressions. In addition, any question containing any type of temporal expression is covered under the umbrella of temporal questions. In this work, we consider all temporal expressions independent of their occurrence type as long as they can be anchored on a timeline, either as points in time or as (possibly open) time intervals.</p>    <p>In our analysis of the benchmark in the next section, we distinguish four types of temporal questions: <em>explicit</em> and <em>implicit</em> with respective temporal expressions (Sec.&#x00A0;<a class="sec" href="#sec-8">2.1</a>), <em>ordinal</em> (containing an ordinal), and <em>temporal answer</em> which covers all questions asking for some kind of temporal information (e.g., when-questions).</p>    </section>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> TempQuestions: Creation</h2>    </div>    </header>    <p>Existing KB-QA datasets&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] are mixed bags with several types of questions: simple, compositional, ordinal, temporal, and spatial, among others. While we have reasonable evidence of the presence of temporal questions across these benchmarks, the fraction in each dataset individually is small: as a result, systems that ignore temporal questions can still achieve acceptable performance on these benchmarks. This motivated us to collate temporal questions from existing resources to create our temporal-questions-only benchmark. This was enabled by formulating unambiguous definitions and conventions for temporal questions (Sec.&#x00A0;<a class="sec" href="#sec-7">2</a>). We refer to our new benchmark as <em>TempQuestions</em>, and it contains 1,271 questions with various temporal facets: explicit and implicit temporal expressions, temporal answers, and ordinal constraints. <em>TempQuestions</em> is available at: <a class="link-inline force-break"     href="http://qa.mpi-inf.mpg.de/TempQuestions.zip">http://qa.mpi-inf.mpg.de/TempQuestions.zip</a>. </p>    <p>    <strong>Source datasets.</strong> Specifically, we extracted temporal questions from the following three KB-QA datasets whose answer sets are based on Freebase:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;"><strong>Free917&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0010">10</a>]:</strong> It consists of 917 questions (641 training and 276 test), manually annotated by experts with their SPARQL queries. These factoid questions were provided by two native English speakers.<br/></li>    <li id="list2" label="&#x2022;"><strong>WebQuestions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0007">7</a>] (WQ)</strong>: This has been one of the most popular benchmarks in KB-QA, and contains 5,810 question-answer pairs split into 3,778 training and 2,032 test instances. This dataset was constructed using a combination of Google Suggest API and crowdsourcing.<br/></li>    <li id="list3" label="&#x2022;"><strong>ComplexQuestions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>] (CQ):</strong> It contains 2,100 questions paired with their answers; 1,300 training and 800 test. The questions are samples from query logs of a commercial search engine, together with extractions from previous benchmarks (WebQuestions and Yin et al.&#x2019;s data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>]). Questions in this dataset are syntactically more complex than questions in previous datasets.<br/></li>    </ul>    <p>    <strong>Method overview.</strong> We follow a two-stage approach to construct <em>TempQuestions</em>: (i) an automated temporal question detection on the above datasets, and (ii) a manual inspection to rule out mistakes in the first step. Additionally, all answers to the final questions were manually verified and mistakes and redundancies in the previous gold standards were corrected.</p>    <p>    <strong>Automated detection.</strong> To identify temporal questions in accordance with the conceptual definitions proposed in Sec.&#x00A0;<a class="sec" href="#sec-7">2</a>, we use a combination of existing taggers, dictionaries, and lexico-syntactic patterns. First, we ran temporal expression taggers SUTime&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] and HeidelTime&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] over all questions. These taggers annotate explicit TIMEX3 tags, and we were thus able to identify questions with explicit temporal expressions (like <em>&#x201C;who won the state of texas in [2008]?&#x201D;</em>). HeidelTime&#x0027;s temponym tagging extension and an event dictionary created using Freebase were used to identify questions with implicit temporal expressions. SIGNAL words are tagged using a dictionary constructed as per suggestions from Setzer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], and the list of temporal prepositions (Sec.&#x00A0;<a class="sec" href="#sec-9">2.2</a>) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] (like <em>&#x201C;who lived in america [before] europeans arrived?&#x201D;</em>). We tag ordinal words like <em>first, second,</em> and <em>last</em> using the Stanford CoreNLP&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] recognizer and a dictionary. After this step, we can identify temporal questions like <em>&#x201C;who was the [first] coach of the buccaneers?&#x201D;</em>. Finally, questions whose answers are temporal are identified using simple start patterns like <em>when</em>, <em>since when</em>, <em>what date</em>, <em>in what year</em>, <em>which century</em>, etc. We now had 1,541 potential temporal questions. Since we were focused on recall and wanted to collect as many temporal questions as possible, there were quite a few false positives.</p>    <p>    <strong>Manual inspection.</strong> Next, a human expert went over each question to remove non-temporal questions. Some instances that were removed were: <em>&#x201C;what is president nixon&#x0027;s first name?&#x201D;</em> (wrong interpretation of the ordinal tag), and <em>&#x201C;who does nicolas cage play in a christmas carol?&#x201D;</em> (Christmas was wrongly tagged as an event). Moreover, the same human expert also verified whether existing gold answers were incorrect or noisy. Redundant answers were normalized to the names of the corresponding Freebase entities. As an example, for the question <em>&#x201C;who did libya gain independence from in 1951?&#x201D;</em>, the answer <em>&#x2018;its independence from Italy&#x2019;</em> was removed, and only <em>&#x2018;Italy&#x2019;</em> was retained. Finally, we had a total of 1,271 cleaned and verified temporal questions in our benchmark.</p>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> TempQuestions: Analysis</h2>    </div>    </header>    <p>We now present detailed qualitative and quantitative analyses of our benchmark, giving the reader glimpses into the content. We also highlight scope for research in this direction, by showing below-par performance of state-of-the-art systems on TempQuestions.</p>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Measurement</h3>     </div>    </header>    <figure id="fig2">     <img src="http://deliveryimages.acm.org/10.1145/3200000/3191536/images/www18companion-275-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Length distribution in TempQuestions.</span>     </div>    </figure>    <p>First, in Fig.&#x00A0;<a class="fig" href="#fig2">2</a>, we show how questions in our benchmark are distributed by length (in words), and contrast this with Free917, WQ, and CQ. Questions in our benchmark are between 4 and 15 words long, and the average question length is 8.28 words. The figure shows that a good proportion of questions in TempQuestions are relatively verbose, implying increased parsing difficulty for QA systems. Next, to give readers a feel of the questions in our resource upfront, we present sample questions in Table&#x00A0;<a class="tbl" href="#tab1">1</a>, segmented by the following three dimensions: temporal category, numbers of entities and relations, and question source.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Representative examples from TempQuestions.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Property</strong>       </th>       <th style="text-align:center;">        <strong>Question</strong>       </th>       </tr>       <tr>       <th style="text-align:left;"><strong>Segmentation by question type</strong>       </th>       <th/>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Explicit</td>       <td style="text-align:center;">        <em>&#x201C;who won the state of texas in 2008?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">temporal</td>       <td style="text-align:center;">        <em>&#x201C;what kind of government does iran have after 1979?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Implicit</td>       <td style="text-align:center;">        <em>&#x201C;who was the president after jfk died?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">temporal</td>       <td style="text-align:center;">        <em>&#x201C;what team did michael jordan play for after the bulls?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Temporal</td>       <td style="text-align:center;">        <em>&#x201C;what years did the knicks win the championship?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">answer</td>       <td style="text-align:center;">        <em>&#x201C;when was the united nations founded?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Ordinal</td>       <td style="text-align:center;">        <em>&#x201C;who was the first coach of the bucaneers?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;">contraint</td>       <td style="text-align:center;">        <em>&#x201C;who was andy williams second wife?&#x201D;</em>       </td>       </tr> 						<tr style="border-top: solid 2px"><td colspan="2"><strong>Segmentation by question concepts</strong></td></tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Multi-entity</td>       <td style="text-align:center;">        <em>&#x201C;what did france lose to the british in the treaty of paris in 1763?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">        <em>&#x201C;when was the last time the oakland raiders won the super bowl?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Multi-relation</td>       <td style="text-align:center;">        <em>&#x201C;who won best supporting actor when alfred junge won best art direction?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">        <em>&#x201C;what book was written by george orwell and published in 1945?&#x201D;</em>       </td>       </tr> 						<tr style="border-top: solid 2px"><td colspan="2"><strong>Segmentation by question source</strong></td></tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">Free917&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0010">10</a>]</td>       <td style="text-align:center;">        <em>&#x201C;when was the airspeed oxford first flown?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">        <em>&#x201C;in 1981 what award did danny devito win?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">WQ&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0007">7</a>]</td>       <td style="text-align:center;">        <em>&#x201C;what was the currency in france before euro?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">        <em>&#x201C;who is julia roberts married to 2012?&#x201D;</em>       </td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">CQ&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0005">5</a>]</td>       <td style="text-align:center;">        <em>&#x201C;who was us president when vietnam war started?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:center;">        <em>&#x201C;who did michael jordan play for after the bulls?&#x201D;</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td/>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Distribution of question types by source. The total is greater than 1,271 as some questions have multiple tags.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Question Tag</strong>       </th>       <th style="text-align:center;">        <strong>Free917</strong>       </th>       <th style="text-align:center;">        <strong>WQ</strong>       </th>       <th style="text-align:center;">        <strong>CQ</strong>       </th>       <th style="text-align:center;">        <strong>Total</strong>       </th>       </tr></thead> 						<tbody>       <tr>       <td style="text-align:left;">        <strong>Explicit temporal</strong>       </td>       <td style="text-align:center;">41</td>       <td style="text-align:center;">344</td>       <td style="text-align:center;">222</td>       <td style="text-align:center;">607</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Implicit temporal</strong>       </td>       <td style="text-align:center;">3</td>       <td style="text-align:center;">81</td>       <td style="text-align:center;">125</td>       <td style="text-align:center;">209</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Temporal answer</strong>       </td>       <td style="text-align:center;">88</td>       <td style="text-align:center;">254</td>       <td style="text-align:center;">51</td>       <td style="text-align:center;">393</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>Ordinal constraint</strong>       </td>       <td style="text-align:center;">18</td>       <td style="text-align:center;">111</td>       <td style="text-align:center;">26</td>       <td style="text-align:center;">155</td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">        <strong>Total</strong>       </td>       <td style="text-align:center;">150</td>       <td style="text-align:center;">790</td>       <td style="text-align:center;">424</td>       <td style="text-align:center;">1,364</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Distribution of question types.</strong> We provide a simultaneous breakdown into the four classes of temporal questions, along with the input source, in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. The two key points are: (a) TempQuestions has a good number of questions with implicit temporal expressions (209) and ordinals (155) &#x2013; both these classes require <em>additional reasoning</em> and ranking on part of the QA-system, and thus add a level of difficulty; (b) the total 1,364 is higher than 1,271, showing that there are several questions that belong to more than one category, and are thus quite challenging for current QA systems (like <em>&#x201C;who was elected the first governor of virginia in 1776?&#x201D;</em>, with both explicit and ordinal tags).</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Distribution of entities and relations in questions.</span>     </div>     <table class="table"> 				 <thead>       <tr>       <th style="text-align:left;">        <strong>Property</strong>       </th>       <th style="text-align:center;">        <strong>0</strong>       </th>       <th style="text-align:center;">        <strong>1</strong>       </th>       <th style="text-align:center;">        <strong>2</strong>       </th>       <th style="text-align:center;">        <strong>3</strong>       </th>       <th style="text-align:center;">        <strong>Total</strong>       </th>       </tr></thead>      <tbody>       <tr>       <td style="text-align:left;">        <strong>#Question entities</strong>       </td>       <td style="text-align:center;">5</td>       <td style="text-align:center;">1,061</td>       <td style="text-align:center;">201</td>       <td style="text-align:center;">4</td>       <td style="text-align:center;">1,271</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>#Question relations</strong>       </td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">1,126</td>       <td style="text-align:center;">145</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">1,271</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Multiple entities and relations.</strong> Table&#x00A0;<a class="tbl" href="#tab3">3</a> shows the way entities and relations appear in TempQuestions. Stanford NER &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] was used to tag entities, followed by a round of manual inspection (among detected entities, 36% were of type <em>person</em>, 30% of type <em>location</em>, 17% of type <em>organization</em>, and 17% were miscellaneous). Relation tagging was done manually by an expert, as current systems like Saha et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] performing automated relation (fact) extraction are far from perfect. What is noteworthy here is that there are several questions with multiple entities (205) and relations (145) in TempQuestions (examples in Table&#x00A0;<a class="tbl" href="#tab1">1</a>). Multi-relation and multi-entity questions are more difficult for <em>semantic parsing</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>], and reflect <em>semantic compositionality</em>. Most current KB-QA systems are designed for single-entity single-relation questions and would require new techniques to address questions in our resource. An example of a question with no named entity is <em>&#x201C;who is the richest person 2015?&#x201D;</em>.</p>    <p>     <strong>Presence of temporal signals.</strong> Finally, we show how temporal signals are distributed: before (49 questions), after (28), overlap (435), and ordinal (156). Signal words may indicate the necessity of question decomposition, rewriting, and separate processing of individual subquestions. As discussed earlier (Sec.&#x00A0;<a class="sec" href="#sec-6">1</a>), this is yet another key challenge that needs to be overcome if QA systems are to answer complex temporal questions. Higher numbers of questions with the <em>overlap signal</em> (signifying temporal durations or intervals) point to increased difficulty levels.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Performance</h3>     </div>    </header>    <p>We now evaluate how two state-of-the-art KB-QA systems AQQU&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] and QUINT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>] perform on TempQuestions, with Freebase as the backend KB. AQQU uses distant supervision and learning-to-rank techniques on several generated SPARQL candidates to find the best query to be executed over the KB, and relies on a set of hand-coded query templates for semantic parsing. QUINT removes this dependence on hand-coded templates for KB-QA, and automatically learns question-query templates solely from user questions paired with their answers. Results are shown in Table&#x00A0;<a class="tbl" href="#tab4">4</a>, where numbers are shown for TempQuestions, and contrasted with WebQuestions (WQ). These systems are designed for standard KB-QA, and thus perform significantly worse on our new benchmark. This is evident from F1-scores of around <span class="inline-equation"><span class="tex">$27-30\%$</span>     </span>, which are <span class="inline-equation"><span class="tex">$\simeq 50.0\%$</span>     </span> for WQ. This raises the call for better systems tailored for handling temporal intent, while also addressing challenges raised by compositionality and reasoning constraints. Detailed results by question category are shown in Table&#x00A0;<a class="tbl" href="#tab5">5</a>. The sweeping observation is that while all categories reflect poor performance, questions with implicit temporal expressions are particularly challenging.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Performance of state-of-the-art KB-QA systems AQQU and QUINT on TempQuestions and WebQuestions.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Benchmark</strong>       </th>       <th style="text-align:left;">        <strong>Method</strong>       </th>       <th style="text-align:center;">        <strong>Precision</strong>       </th>       <th style="text-align:center;">        <strong>Recall</strong>       </th>       <th style="text-align:center;">        <strong>F-Score</strong>       </th>       </tr></thead> 						<tbody>       <tr>       <td style="text-align:left;">TempQuestions</td>       <td style="text-align:left;">AQQU</td>       <td style="text-align:center;">24.6</td>       <td style="text-align:center;">48.0</td>       <td style="text-align:center;">27.2</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">QUINT</td>       <td style="text-align:center;">27.3</td>       <td style="text-align:center;">52.8</td>       <td style="text-align:center;">30.0</td>       </tr>       <tr style="border-top: solid 2px">       <td style="text-align:left;">WebQuestions</td>       <td style="text-align:left;">AQQU</td>       <td style="text-align:center;">49.8</td>       <td style="text-align:center;">60.4</td>       <td style="text-align:center;">49.4</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">QUINT</td>       <td style="text-align:center;">52.1</td>       <td style="text-align:center;">60.3</td>       <td style="text-align:center;">51.0</td>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Detailed performance of AQQU and QUINT on TempQuestions, segmented by question type.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Type</strong>       </th>       <th style="text-align:center;" colspan="3"><strong>Explicit temporal</strong>        <hr/>       </th>       <th style="text-align:center;" colspan="3">        <strong>Implicit temporal</strong>        <hr/>       </th>       <th style="text-align:center;" colspan="3"><strong>Temporal answer</strong>        <hr/>       </th>       <th style="text-align:center;" colspan="3">        <strong>Ordinal constraint</strong>        <hr/>       </th>       </tr>       <tr>       <th style="text-align:left;">        <strong>Method</strong>       </th>       <th style="text-align:center;">        <strong>Precision</strong>       </th>       <th style="text-align:center;">        <strong>Recall</strong>       </th>       <th style="text-align:center;">        <strong>F-Score</strong>       </th>       <th style="text-align:center;">        <strong>Precision</strong>       </th>       <th style="text-align:center;">        <strong>Recall</strong>       </th>       <th style="text-align:center;">        <strong>F-Score</strong>       </th>       <th style="text-align:center;">        <strong>Precision</strong>       </th>       <th style="text-align:center;">        <strong>Recall</strong>       </th>       <th style="text-align:center;">        <strong>F-Score</strong>       </th>       <th style="text-align:center;">        <strong>Precision</strong>       </th>       <th style="text-align:center;">        <strong>Recall</strong>       </th>       <th style="text-align:center;">        <strong>F-Score</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <strong>AQQU</strong>       </td>       <td style="text-align:center;">27.6</td>       <td style="text-align:center;">60.7</td>       <td style="text-align:center;">31.1</td>       <td style="text-align:center;">12.9</td>       <td style="text-align:center;">34.9</td>       <td style="text-align:center;">14.5</td>       <td style="text-align:center;">26.1</td>       <td style="text-align:center;">33.5</td>       <td style="text-align:center;">27.4</td>       <td style="text-align:center;">28.4</td>       <td style="text-align:center;">57.4</td>       <td style="text-align:center;">32.7</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>QUINT</strong>       </td>       <td style="text-align:center;">29.3</td>       <td style="text-align:center;">60.9</td>       <td style="text-align:center;">32.6</td>       <td style="text-align:center;">25.6</td>       <td style="text-align:center;">54.4</td>       <td style="text-align:center;">27.0</td>       <td style="text-align:center;">25.2</td>       <td style="text-align:center;">38.2</td>       <td style="text-align:center;">27.3</td>       <td style="text-align:center;">21.3</td>       <td style="text-align:center;">54.9</td>       <td style="text-align:center;">26.1</td>       </tr>      </tbody>     </table>    </div>    </section>   </section>   <section id="sec-15">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Related Resources</h2>    </div>    </header>    <p>Multiple datasets have been proposed for KB-QA, which differ in the underlying KB (DBpedia or Freebase), size (a couple of hundreds to a few thousands), and question phenomena they involve (simple, compositional, and/or questions with conditions, among others)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. We refer the reader to Diefenbach et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] for further details.</p>    <p>Benchmarks with complex questions are still ad hoc, and in their infancy. QALD&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] is a series of evaluation campaigns on QA over linked data, and releases datasets every year to evaluate KB-QA systems. Thus far, seven challenges have been presented. Questions in QALD cover many interesting phenomena such as aggregation, count, and additional conditions, (for example, <em>&#x201C;Which German cities have more than 250000 inhabitants?&#x201D;</em>). However, the main shortcoming is the very small size (50 &#x2212; 250 questions). Recently, Abujabal et al.&#x00A0;<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a> released 150 questions paired with their answers over Freebase. While all questions in this dataset contain more than one entity/relation, the underlying SPARQL query would still require joining over a single variable only. Questions were collected using a public crawl of WikiAnswers, a large, community-authored corpus of NL questions. The WebQuestions (WQ)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] and SimpleQuestions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] datasets contain a majority of simple factoid questions, e.g., <em>&#x201C;what language does cuba speak?&#x201D;</em>, with a few exceptions. While questions in WQ&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] are only paired with answers, they are improved with SPARQL queries in Bordes et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. Bao et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] released a new dataset with complex questions paired with their answers over Freebase (2,100 question-answer pairs). The LC-QuAD dataset&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] contains 5,000 questions and their corresponding SPARQL queries over DBpedia. Questions in LC-QuAD exhibit high syntactic and structural variation. These were generated using a set of hand-written templates that verbalize SPARQL queries, which are then corrected and paraphrased by humans.</p>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions and future work</h2>    </div>    </header>    <p>We released <em>TempQuestions</em>, a new benchmark for temporal question answering, with 1,271 question-answer pairs. With textual answers, the resource is suitable for answering over KBs, free text, or hybrid sources. The questions are accompanied by useful markup tags like question types and signals to allow for detailed system analysis. To facilitate follow-up research, we make results of two state-of-the-art systems on TempQuestions available with our release, and with thorough scrutiny, show that this benchmark is particularly challenging for current KB-QA. As an additional contribution, we provide a concrete definition of a temporal question. Finally, through this benchmark, we call upon the community to build QA-systems that can handle open challenges like temporal intent, compositionality, and constraint-based reasoning.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Abdalghani Abujabal, Mohamed Yahya, Mirek Riedewald, and Gerhard Weikum. 2017. Automated Template Generation for Question Answering over Knowledge Graphs. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0002" label="[2]">Eugene Agichtein, David Carmel, Dan Pelleg, Yuval Pinter, and Donna Harman. 2015. Overview of the TREC 2015 LiveQA Track. In <em>      <em>TREC</em>     </em>.</li>    <li id="BibPLXBIB0003" label="[3]">James&#x00A0;F. Allen. 1983. Maintaining Knowledge About Temporal Intervals. <em>      <em>Comm. ACM</em>     </em> (1983).</li>    <li id="BibPLXBIB0004" label="[4]">Omar Alonso, Michael Gertz, and Ricardo Baeza-Yates. 2007. On the value of temporal information in information retrieval. In <em>      <em>ACM SIGIR Forum</em>     </em>.</li>    <li id="BibPLXBIB0005" label="[5]">Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-Based Question Answering with Knowledge Graph. In <em>      <em>COLING</em>     </em>.</li>    <li id="BibPLXBIB0006" label="[6]">Hannah Bast and Elmar Haussmann. 2015. More Accurate Question Answering on Freebase. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0007" label="[7]">Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0008" label="[8]">Branimir Boguraev, Siddharth Patwardhan, Aditya Kalyanpur, Jennifer Chu-Carroll, and Adam Lally. 2014. Parallel and nested decomposition for factoid questions. <em>      <em>Natural Language Engineering</em>     </em>(2014).</li>    <li id="BibPLXBIB0009" label="[9]">Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. <em>      <em>arXiv</em>     </em> (2015).</li>    <li id="BibPLXBIB0010" label="[10]">Qingqing Cai and Alexander Yates. 2013. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0011" label="[11]">Angel&#x00A0;X. Chang and Christopher&#x00A0;D. Manning. 2012. SUTime: A library for recognizing and normalizing time expressions. In <em>      <em>LREC</em>     </em>.</li>    <li id="BibPLXBIB0012" label="[12]">Dennis Diefenbach, Vanessa Lopez, Kamal Singh, and Pierre Maret. 2017. Core techniques of question answering systems over knowledge bases: A survey. In <em>      <em>Knowledge and Information systems</em>     </em>.</li>    <li id="BibPLXBIB0013" label="[13]">Aditya&#x00A0;Kalyanpur et al.2012. Structured data and inference in DeepQA. <em>      <em>IBM Journal of Research and Development</em>     </em>(2012).</li>    <li id="BibPLXBIB0014" label="[14]">David A.&#x00A0;Ferrucci et al.2012. This is Watson. <em>      <em>IBM Journal of Research and Development</em>     </em>56 (2012). Issue 3/4.</li>    <li id="BibPLXBIB0015" label="[15]">Aditya Kalyanpur, Siddharth Patwardhan, BK Boguraev, Adam Lally, and Jennifer Chu-Carroll. 2012. Fact-based question decomposition in DeepQA. <em>      <em>IBM Journal of Research and Development</em>     </em>(2012).</li>    <li id="BibPLXBIB0016" label="[16]">Erdal Kuzey, Vinay Setty, Jannik Str&#x00F6;tgen, and Gerhard Weikum. 2016. As Time Goes By: Comprehensive Tagging of Textual Phrases with Temporal Scopes. In <em>      <em>WWW</em>     </em>.</li>    <li id="BibPLXBIB0017" label="[17]">Ken Litkowski. 2014. Pattern Dictionary of English Prepositions. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0018" label="[18]">Ken Litkowski and Orin Hargraves. 2006. Coverage and inheritance in the preposition project. In <em>      <em>SIGSEM</em>     </em>.</li>    <li id="BibPLXBIB0019" label="[19]">Christopher&#x00A0;D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven&#x00A0;J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0020" label="[20]">Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang Zhang. 2009. Improving Search Relevance for Implicitly Temporal Queries. In <em>      <em>SIGIR</em>     </em>.</li>    <li id="BibPLXBIB0021" label="[21]">Anselmo Pe&#x00F1;as, Christina Unger, Georgios Paliouras, and Ioannis Kakadiaris. 2015. Overview of the CLEF Question Answering Track 2015. In <em>      <em>CLEF</em>     </em>.</li>    <li id="BibPLXBIB0022" label="[22]">James Pustejovsky, Robert Knippen, Jessica Littman, and Roser Saur&#x00ED;. 2005. Temporal and Event Information in Natural Language Text. In <em>      <em>LREC</em>     </em>.</li>    <li id="BibPLXBIB0023" label="[23]">Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0024" label="[24]">Swarnadeep Saha, Harinder Pal, and Mausam. 2017. Bootstrapping for Numerical Open IE. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0025" label="[25]">Andrea Setzer. 2002. <em>Temporal information in newswire articles: An annotation scheme and corpus study</em>. Ph.D. Dissertation. University of Sheffield.</li>    <li id="BibPLXBIB0026" label="[26]">Jannik Str&#x00F6;tgen and Michael Gertz. 2015. A Baseline Temporal Tagger for all Languages. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0027" label="[27]">Jannik Str&#x00F6;tgen and Michael Gertz. 2016. <em>      <em>Domain-sensitive Temporal Tagging</em>     </em>. Morgan &#x0026; Claypool Publishers.</li>    <li id="BibPLXBIB0028" label="[28]">Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017. LC-QuAD: A Corpus for Complex Question Answering over Knowledge Graphs. In <em>      <em>ISWC</em>     </em>.</li>    <li id="BibPLXBIB0029" label="[29]">Christina Unger, Corina Forascu, Vanessa L&#x00F3;pez, Axel-Cyrille&#x00A0;Ngonga Ngomo, Elena Cabrio, Philipp Cimiano, and Sebastian Walter. 2015. Question Answering over Linked Data (QALD-5). In <em>      <em>CLEF</em>     </em>.</li>    <li id="BibPLXBIB0030" label="[30]">Christina Unger, Andr&#x00E9; Freitas, and Philipp Cimiano. 2014. An introduction to question answering over linked data. In <em>      <em>Reasoning Web</em>     </em>.</li>    <li id="BibPLXBIB0031" label="[31]">Ricardo Usbeck, Axel-Cyrille&#x00A0;Ngonga Ngomo, Bastian Haarmann, Anastasia Krithara, Michael R&#x00F6;der, and Giulio Napolitano. 2017. 7th Open Challenge on Question Answering over Linked Data (QALD-7). In <em>      <em>SemWebEval</em>     </em>.</li>    <li id="BibPLXBIB0032" label="[32]">Ellen&#x00A0;M. Voorhees. 2010. Reflections on TREC QA. In <em>      <em>CLEF</em>     </em>.</li>    <li id="BibPLXBIB0033" label="[33]">Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural language questions for the Web of data. In <em>      <em>EMNLP</em>     </em>.</li>    <li id="BibPLXBIB0034" label="[34]">Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base. In <em>      <em>ACL</em>     </em>.</li>    <li id="BibPLXBIB0035" label="[35]">Pengcheng Yin, Nan Duan, Ben Kao, Jun-Wei Bao, and Ming Zhou. 2015. Answering Questions with Complex Semantic Constraints on Open Knowledge Bases. In <em>      <em>CIKM</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>The work was done when the author was at the MPI for Informatics.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191536">https://doi.org/10.1145/3184558.3191536</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
