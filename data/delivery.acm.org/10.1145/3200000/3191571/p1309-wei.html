<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Residual Convolutional LSTM for Tweet Count Prediction</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="https://dl.acm.org/pubs/lib/css/main.css"/><script src="https://dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="https://dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Residual Convolutional LSTM for Tweet Count Prediction</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Hong</span>      <span class="surName">Wei</span>     Department of Computer Science, University of Maryland, College Park, Maryland 20742, <a href="mailto:hyw@cs.umd.edu">hyw@cs.umd.edu</a>     </div>     <div class="author">     <span class="givenName">Hao</span>      <span class="surName">Zhou</span>     Department of Computer Science, University of Maryland, College Park, Maryland 20742, <a href="mailto:hzhou@cs.umd.edu">hzhou@cs.umd.edu</a>     </div>     <div class="author">     <span class="givenName">Jagan</span>      <span class="surName">Sankaranarayanan</span>     UMIACS, University of Maryland, College Park, Maryland 20742, <a href="mailto:jagan@umiacs.umd.edu">jagan@umiacs.umd.edu</a>     </div>     <div class="author">     <span class="givenName">Sudipta</span>      <span class="surName">Sengupta</span>     Amazon Web Services (AWS), Seattle, Washington 98101, <a href="mailto:sudipta@amazon.com">sudipta@amazon.com</a>     </div>     <div class="author">     <span class="givenName">Hanan</span>      <span class="surName">Samet</span>     Department of Computer Science, University of Maryland, College Park, Maryland 20742, <a href="mailto:hjs@cs.umd.edu">hjs@cs.umd.edu</a>     </div>                         </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191571" target="_blank">https://doi.org/10.1145/3184558.3191571</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>The tweet count prediction of a local spatial region is to forecast the number of tweets that are likely to be posted from that area over a relatively short period of time. It has many applications such as human mobility analysis, traffic planning, and abnormal event detection. In this paper, we formulate tweet count prediction as a spatiotemporal sequence forecasting problem and design an end-to-end convolutional LSTM based network with skip connection for this problem. Such a model enables us to exploit the unique properties of spatiotemporal data, consisting of not only the temporal characteristics such as temporal closeness, period and trend properties, but also spatial dependencies. Our experiments on the city of Seattle, WA as well as a larger city of New York City show that the proposed method consistently outperforms the competitive baseline approaches.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Networks </strong>&#x2192; <strong>Social media networks;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Neural networks;</strong> <em>Applied computing;</em> </small> </p>    </div>    <div class="CCSconcepts">     <ccs2012>     <small>      <span style="font-weight:bold;">CCS Concepts:</span>       <concept_desc>       <strong>Networks~Social media networks;</strong>      </concept_desc>        <concept_desc>       <em>Applied computing;</em>      </concept_desc>        <concept_desc>       <strong>Networks~Social media networks;</strong>      </concept_desc>        <concept_desc>       <strong>Computing methodologies~Neural networks;</strong>      </concept_desc>        <concept_desc>       <em>Applied computing;</em>      </concept_desc></small>     </ccs2012>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Social Network</small>, </span>     <span class="keyword">      <small> Twitter</small>, </span>     <span class="keyword">      <small> Tweet Count Prediction</small>, </span>     <span class="keyword">      <small> LSTM</small>, </span>     <span class="keyword">      <small> Convolution</small>, </span>     <span class="keyword">      <small> Convolutional LSTM</small>, </span>     <span class="keyword">      <small> Residual Neural Network</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Hong Wei, Hao Zhou, Jagan Sankaranarayanan, Sudipta Sengupta, and Hanan Samet. 2018. Residual Convolutional LSTM for Tweet Count Prediction. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 9 Pages. <a href="https://doi.org/10.1145/3184558.3191571" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191571</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Given a geographical region (e.g., New York City), the goal of tweet count prediction is to forecast the spatial distribution of number of tweets that are likely to appear in the next time frame based on the previously observed data. Such a problem has many applications such as human mobility modeling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and abnormal event detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. Taking abnormal event detection as an example, one may compare the predicted tweet count with the actual number of tweets in a geospatial local region. A significant difference is considered as a strong indicator of the occurrence of an abnormal event. <figure id="fig1">     <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">(a) Tweet Count Distribution around the Seattle city center at 17:00-17:30 on 2016-07-16. (b) Tweet Count Distribution around around the Seattle city center at 17:30-18:00 on 2016-07-16. (A number in a grid cell refers to the value of tweet count at that time interval while an empty grid cell means no tweets.)</span>     </div>     </figure>    </p>    <p>It is, however, challenging to make high-quality predictions of tweet count in a region due to both the spatial and temporal dependences. For example, Figure&#x00A0;<a class="fig" href="#fig1">1</a> gives the tweet count in two consecutive time interval around the Seattle city center area. The number in each grid cell refers to the value of tweet count at that time interval while an empty grid cell means no tweets. We notice that: (1) The number of tweets in a grid cell is positively correlated with that of nearby cells, i.e., a grid cell tends to have larger (smaller) number of tweets if nearby cells also have larger (smaller) number of tweets, indicating the spatial dependences between cells. (2) The difference of the number of tweets between two temporal adjacent data is small, indicating the existence of temporal dependence. In fact, there are studies pointing out that spatiotemporal data also has a certain periodic pattern&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], which indicates that we should also capture the periodic time-varying changes in tweet volume.</p>    <p>In this paper, we design an end-to-end model to predict the spatiotemporal tweet count sequence. Convolutional neural networks (CNNs) are designed to account for the spatial dependences of data. &#x00A0;Zhang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] extend CNNs to account for temporal dependences by stacking spatial data of several consecutive time frames as input to CNNs, i.e., they simply treat spatial data at different time intervals as different channels of the input data. As a result, the way of encoding the temporal dependences is the same as that of spatial dependences, which may not be optimal. In this paper, we propose to apply the convolution LSTM (ConvLSTM)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] layer as the basic stack unit which has convolutional structures in both the input-to-state and state-to-state transitions. In such a way, the spatial dependences are encoded by convolutional filters and temporal dependences are encoded by LSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]. Both convolutional filters and LSTM play the role they are designed for. However, we notice that only using convolution LSTM cannot give us the best results. One reason may be that both convolutional neural network and LSTM are notorious for being highly non-convex and difficulty to converge to a good local minimum. Recent studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] have shown that using skip connections [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] can prevent the loss function from being chaotic, leading to a more convex loss function. Inspired by this and its effectiveness in many applications [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>], we propose to add skip connections to our convolution LSTM. To further account for the temporal properties, we follow the idea of ST-ResNet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] and partition sequences into 3 subsets: <em>closeness</em>, <em>period</em> and <em>trend</em> corresponding to recent, near and distant history, respectively. Each of these subsets of sequences is then separately fed into our method to generate an individual prediction which is then combined together to achieve the final prediction as discussed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. We test the proposed method using two sets of geotagged tweets collected for Seattle, WA and New York City. Our experimental results demonstrate that the proposed method consistently outperforms the competitive baseline approaches.</p>    <p>To reiterate, the contributions of this paper are threefold. First, we are the first to apply ConvLSTM to tweet count problem, in which both convolutional filters and LSTM play the role they are designed for. Second, we add skip connections to ConvLSTM, which leads to a more convex loss function. It eases for the training procedure to find a better local minimum. Third, the proposed method achieved state-of-the-art results on two sets of geotagged tweets collected for Seattle, WA and New York city, showing the effectiveness of the proposed method.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>As time goes by, the tweet counts in a region may be formulated as time series data, which enables the exploitation of the techniques like historical average and autoregressive integrated moving average (ARIMA)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. For example, TwitInfo&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] uses the weighted average of historical tweet counts to compute the expected frequency of tweets.&#x00A0;Lin et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] proposed a space-time autoregressive integrated moving average (STARIMA) model to predict urban traffic flow volume. Moreover,&#x00A0;Chae et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] adopt a similar model to seasonal ARIMA and decomposes the time series into the sum of a seasonal part, a trend part, and a remainder part, to check whether there exists an unusual volume of tweets.</p>    <p>Time series analysis based techniques, however, often neglect the effects exerted by nearby geographical regions when making predictions on a specific local region. Therefore, in their work on finding anomalies,&#x00A0;Krumm and Horvitz [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] build a gradient boosting regression function that estimates the number of tweets on a region based on a list of features including the time of the day, the day of the week, and the tweet counts from neighboring regions.</p>    <p>With the recent advances in deep learning, a few recent studies have focused on introducing deep neural networks into modeling spatiotemporal data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. For example,&#x00A0;Shi et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] propose a novel <em>convolutional LSTM</em> (<em>ConvLSTM</em>) network for precipitation nowcasting on radar echo spatiotemporal data, which enables the capture of both spatial and temporal correlation simultaneously by combining a convolution network and a recurrent LSTM network. Such a combination is done by innovatively replacing the matrix multiplication operations used in LSTM with convolution operations. This is different from Spatiotemporal Recurrent Convolutional Networks (SRCN) proposed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] which simply stack additional LSTM layers after convolutional layers.</p>    <p>Focusing on citywide crowd prediction,&#x00A0;Zhang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] first partition historical spatiotemporal sequences into three subsets <em>closeness</em>, <em>period</em> and <em>trend</em>, which correspond to recent, near and distant history. Each subset is then fed into a Deep Convolution Neural Network to yield a prediction, and such predictions are then fused together along with external features such as week-of-day to produce the final forecast. Moreover, their subsequent work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] further introduces the residual network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] to capture citywide spatial dependence and gives better accuracy. Our method is different from them in the sense that we utilize ConvLSTM layers instead of regular convolution layers to build up our model, which shows effectiveness in our dataset.</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Method</h2>     </div>    </header>    <p>In this section, we first define the tweet count prediction problem in Section&#x00A0;<a class="sec" href="#sec-9">3.1</a>. Next, we briefly review a few key technologies used in our model such as Convolutional LSTM (ConvLSTM)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] (Section&#x00A0;<a class="sec" href="#sec-10">3.2</a>), Deep Residual Network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] (Section&#x00A0;<a class="sec" href="#sec-10">3.2</a>), and temporal property fusion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] (Section&#x00A0;<a class="sec" href="#sec-12">3.4</a>). Finally, we present the design of our model in Section&#x00A0;<a class="sec" href="#sec-13">3.5</a>.</p>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Tweet Count Prediction Problem</h3>     </div>     </header>     <p>The goal of tweet count prediction is to use previously observed historical tweet count data in a local region to forecast the number of tweets in the next time step. In practice, a region can be represented by an <em>M</em> &#x00D7; <em>N</em> grid map based on the longitude and latitude. Thus, the observation at time step <em>t</em> can be represented by a tensor <strong>X<sub>t</sub>     </strong> &#x2208; <em>R</em>     <sup>      <em>M</em> &#x00D7; <em>N</em>     </sup> where <strong>X<sub>t</sub>     </strong>(<em>m</em>, <em>n</em>) is the tweet count in the grid cell (<em>m</em>, <em>n</em>) at time step <em>t</em>. Therefore, the tweet count prediction problem is formulated as follows:</p>     <div class="definition" id="enc1">     <Label>Definition 3.1.</Label>     <p> The tweet count prediction problem <span class="inline-equation"><span class="tex">$\mathcal {P}$</span>      </span> is to generate a prediction <strong>Y</strong>      <sub>       <em>T</em>      </sub>, which is an estimation of <strong>X</strong>      <sub>       <em>T</em>      </sub>, given a list of historical observations {<strong>X</strong>      <sub>       <em>t</em>      </sub>|<em>t</em> = 0, &#x22C5;&#x22C5;&#x22C5;, <em>T</em> &#x2212; 1}.</p>     </div>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Convolutional LSTM</h3>     </div>     </header>     <figure id="fig2">     <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">The Inner Structure of ConvLSTM. The LSTM matrix multiplication is replaced with convolution.</span>     </div>     </figure>     <p>The Long Short-Term Memory (LSTM) network, one of the well-known recurrent neural networks, has achieved great success in many applications such as sequence modeling and especially sequence prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. Despite its strong ability in modeling temporal dependences of sequences, LSTM ignores spatial information when the sequence data is multi-dimensional. To overcome this drawback,&#x00A0;Shi et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] proposed the Convolutional LSTM (ConvLSTM) which innovatively uses a convolution operator in the state-to-state and input-to-state transitions (see Figure&#x00A0;<a class="fig" href="#fig2">2</a>). The key equations in ConvLSTM are shown as follows: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} \mathbf {i_t} &#x0026;= \sigma (\mathbf {W}_{xi}\ast \mathbf {X}_t + \mathbf {W}_{hi}\ast \mathbf {h}_{t-1} + \mathbf {W}_{ci}\circ \mathbf {c}_{t-1} + \mathbf {b}_i) \\ \mathbf {f_t} &#x0026;= \sigma (\mathbf {W}_{xf}\ast \mathbf {X}_t + \mathbf {W}_{hf}\ast \mathbf {h}_{t-1} + \mathbf {W}_{cf}\circ \mathbf {c}_{t-1} + \mathbf {b}_f) \\ \mathbf {c_t} &#x0026;= \mathbf {f}_t\circ \mathbf {c}_{t-1} + \mathbf {i}_t\circ tanh(\mathbf {W}_{xc}\ast \mathbf {X}_{t} + \mathbf {W}_{hc}\ast \mathbf {h}_{t-1} + \mathbf {b}_c) \\ \mathbf {o_t} &#x0026;= \sigma (\mathbf {W}_{xo}\ast \mathbf {X}_t + \mathbf {W}_{ho}\ast \mathbf {h}_{t-1} + \mathbf {W}_{co}\circ \mathbf {c}_{t} + \mathbf {b}_o) \\ \mathbf {h_t} &#x0026;= \mathbf {o}_t\circ tanh(\mathbf {c}_t) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <em>t</em> iterates from 1 to <em>T</em> &#x2212; 1. The variables <strong>X</strong>     <sub>      <em>t</em>     </sub> , <strong>c</strong>     <sub>      <em>t</em>     </sub>, <strong>h</strong>     <sub>      <em>t</em>     </sub>, <strong>i</strong>     <sub>      <em>t</em>     </sub>, <strong>f</strong>     <sub>      <em>t</em>     </sub>, and <strong>o</strong>     <sub>      <em>t</em>     </sub> are tensors to represent values of the inputs, cell outputs, hidden states, input gates, forget gates and output gates. <em>&#x03C3;</em> is a logistic sigmoid function. The operator &#x25CB; denotes the Hadamard product, i.e., element-wise product of matrix. And * denotes the convolution operator instead of matrix multiplication, which is a key difference from FC-LSTM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. At last, <strong>W</strong>     <sub>*</sub> and <strong>b</strong>     <sub>*</sub> are weight and bias matrices parameters which need to be learned during training.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Residual Network</h3>     </div>     </header>     <figure id="fig3">     <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">(a) Residual ConvLSTM block. (b) Residual block in ST-ResNet. BN: Batch Normalization</span>     </div>     </figure>     <p>It is well known that deeper networks can model more complex functions and thus are more expressive. However, networks that work well in practice usually cannot be very deep. This is due to the vanishing gradient problem. To avoid this vanishing gradient problem and make the design of a deeper network possible, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] proposed skip connections which directly link the output of lower layers to the input of higher layers. This shortcut has proven to be effective to alleviate the vanishing gradient problem in the training process and achieved significantly better performance in many applications. Recently, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] has shown that skip connections can also help to prevent the loss function from being chaotic, leading to a more convex loss function, and thus, making it easy to find a good local minimum. Essentially, a residual building block can be defined as: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {Y} = \mathcal {F}(\mathbf {X}) + \mathbf {X}, \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <strong>X</strong> and <strong>Y</strong> are the input and output tensors of the residual block. The function <span class="inline-equation"><span class="tex">$\mathcal {F}$</span>     </span> represents several convolutional or ConvLSTM layers&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>]. In this study, we always use the ConvLSTM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] to assemble the residual block, which is illustrated in Figure&#x00A0;<a class="fig" href="#fig3">3</a>. This is a key difference from ST-ResNet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>] which uses a regular convolutional layer instead as shown in Figure&#x00A0;<a class="fig" href="#fig3">3</a>.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Temporal Properties Fusion</h3>     </div>     </header>     <p>&#x00A0;Zhang et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], ,<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] pointed out that in spatiotemporal data sequences, making predictions on the future observations does not only rely on the observations of <em>recent time</em> but also depends on those in <em>near history</em> and <em>distant history</em>. Such temporal dependencies are modeled as temporal <em>closeness</em>, <em>period</em> and <em>trend</em>. More specifically, the temporal <em>closeness</em> dependence sequence is a <em>l<sub>c</sub>     </em>-long list of consecutive observations before the current time step and can be denoted by <span class="inline-equation"><span class="tex">$X^c_t={\left[\begin{array}{*10c}X_{t-l_c} &#x0026; X_{t-(l_c-1)} &#x0026; \cdots &#x0026; X_{t-1}\end{array}\right]}$</span>     </span>. The temporal <em>period</em> dependence sequence is a <em>l<sub>p</sub>     </em>-long list of historical observations which are periodically chosen with a time interval <em>p</em>: <span class="inline-equation"><span class="tex">$X^p_t={\left[\begin{array}{*10c}X_{t-p\cdot l_p} &#x0026; X_{t-p\cdot (l_p-1)} &#x0026; \cdots &#x0026; X_{t-p\cdot 1}\end{array}\right]}$</span>     </span>. Similarly, the temporal <em>trend</em> dependence sequence is a <em>l<sub>q</sub>     </em>-long list of historical observations which are also periodically chosen but with time interval <em>q</em>: <span class="inline-equation"><span class="tex">$X^q_t={\left[\begin{array}{*10c}X_{t-q\cdot l_q} &#x0026; X_{t-q\cdot (l_q-1)} &#x0026; \cdots &#x0026; X_{t-1\cdot q}\end{array}\right]}$</span>     </span>. In practice, <em>p</em> is set to a period of one-day to capture daily periodicity and <em>q</em> is set to one-week to reveal weekly trend.</p>     <p>Each of <span class="inline-equation"><span class="tex">$X^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$X^p_t$</span>     </span> and <span class="inline-equation"><span class="tex">$X^q_t$</span>     </span> are separately fed into three designated neural networks, which have the same structure but different weights, to generate observation predictions <span class="inline-equation"><span class="tex">$Y^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$Y^p_t$</span>     </span> and <span class="inline-equation"><span class="tex">$Y^q_t$</span>     </span>, respectively. At last, a parametric-matrix-based fusion is adopted to combine the three outputs <span class="inline-equation"><span class="tex">$Y^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$Y^p_t$</span>     </span> and <span class="inline-equation"><span class="tex">$Y^q_t$</span>     </span> to yield the final prediction <strong>Y<sub>t</sub>     </strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>] using the following equation: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {Y_t}=\mathbf {W^c}\circ \mathbf {Y^c_t} + \mathbf {W^p}\circ \mathbf {Y^p_t} + \mathbf {W^q}\circ \mathbf {Y^q_t} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <strong>W<sup>*</sup>     </strong> are weight matrices that balance different components. Additionally, features such as the time of the day and the day of the week can also be incorporated into <strong>Y<sub>t</sub>     </strong> using fully-connected layers.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Building Our Model</h3>     </div>     </header>     <p>In this section, we present our model used for tweet count prediction. The structure of our model is illustrated in Figure&#x00A0;<a class="fig" href="#fig4">4</a>. <figure id="fig4">      <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Our Model. ResConvLSTM: Residual ConvLSTM block; FCs: Fully-Connected Layers, i.e. Dense layers.</span>      </div>     </figure>     </p>     <p>Similar to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], we define our model to have three branches: <em>closeness</em>, <em>period</em> and <em>trend</em>, to incorporate periodic information in our data. This is because our data reveals positive correlation between adjacent time steps as well as periodic ones such as daily and weekly patterns. For example, Figure&#x00A0;<a class="fig" href="#fig5">5</a> draws the tweet counts in a region for 500 time steps in the city of Seattle and NYC, respectively. The two regions are the bold grid cells marked in Figure&#x00A0;<a class="fig" href="#fig7">7</a>. The results show that our data indeed have certain temporal periodical pattern. As a result, in order to predict an expected tweet count <em>Y<sub>t</sub>     </em> at time step <em>t</em>, we break the historical observations to extract the <em>closeness</em>, <em>period</em> and <em>trend</em> dependence sequences <span class="inline-equation"><span class="tex">$X^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$X^p_t$</span>     </span> and <span class="inline-equation"><span class="tex">$X^q_t$</span>     </span> which are defined in Section&#x00A0;<a class="sec" href="#sec-12">3.4</a>. Each of the three dependence sequences is then fed into a designated network with the same structure but different weights to get the three predictions <span class="inline-equation"><span class="tex">$Y^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$Y^p_t$</span>     </span> and <span class="inline-equation"><span class="tex">$Y^q_t$</span>     </span>, respectively. These three predictions, together with meta data prediction, are combined using the parametric-matrix-based fusion to generate our final prediction as discussed in section&#x00A0;<a class="sec" href="#sec-12">3.4</a>. Please note that we can also define our model to have only one branch which takes a very long-range time series data so as to capture temporally periodical properties. However, this will introduce a huge amount of parameters, which is not only memory demanding, but also makes the networks much harder to train and slower to converge. <figure id="fig5">      <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Temporal Pattern. (a) Seattle City; (b) NYC. Time step is in the unit of 30 minutes, starting from 18:30 on 2016-06-15.</span>      </div>     </figure>     </p>     <p>As shown in Figure&#x00A0;<a class="fig" href="#fig4">4</a>, each branch of our model has the same network structure, comprising of an input ConvLSTM layer, a ResConvLSTM block as described in Figure&#x00A0;<a class="fig" href="#fig3">3</a>, and an output ConvLSTM layer. As a result of using ConvLSTM instead of convolutional layers as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], our model naturally takes a list of sequences as input and does not have to concatenate the long sequences e.g. <span class="inline-equation"><span class="tex">$X^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$X^p_t$</span>     </span> and <span class="inline-equation"><span class="tex">$X^q_t$</span>     </span> into one image-format-like tensor. Moreover, the outputs of the input ConvLSTM layer and ResConvLSTM block are in the form of a list of sequences which has the same length with the input such as <span class="inline-equation"><span class="tex">$X^c_t$</span>     </span>, <span class="inline-equation"><span class="tex">$X^p_t$</span>     </span> or <span class="inline-equation"><span class="tex">$X^q_t$</span>     </span>.</p>     <p>Except for the output ConvLSTM layer which has only 1 hidden state, all ConvLSTM layers are configured to have 32 hidden states. Since we only focus on predicting the expected spatiotemporal tweet count for the next time step, we set the output ConvLSTM layer to return one prediction sequence.</p>     <p>We define the size of the filter in our ConvLSTM to be 3 &#x00D7; 3. This is because the spatial correlation of tweet count data is quite local, i.e., the number of tweets in a grid is correlated with the ones in the nearby grids instead of grids farther away. <figure id="fig6">      <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Histogram of Moving Distance of Twitter Users. We only consider Twitter users who have 2 or more geotagged tweets in the 3-hour time period starting from 18:30 on 2016-06-15. The moving distance of a user is calculated as the largest distance between the GPS coordinates in his geotagged tweets.</span>      </div>     </figure>     </p>     <p>For example, Figure&#x00A0;<a class="fig" href="#fig6">6</a> shows the histogram of moving distance of Twitter users during a time period of 3 hours in the city of Seattle and NYC, respectively. We notice that the majority of Twitter users travel less than 500 meters, i.e. less than the size of a grid cell.</p>     <p>Comparing with ST-ResNet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], we replace its regular convolutional layers with ConvLSTM, as the latter is more powerful in capturing temporal dependence. Moreover, we stack only one residual block, instead of multiple blocks, because we empirically notice that adding more layers to our model cannot improve the performance of the model and sometimes results in over fitting. This also corresponds to the fact that Twitter users in our dataset usually have shorter moving distances.</p>     <p>Meta-data features such as time-of-day, day-of-week are also incorporated in the model to capture the regular time-varying changes. To achieve this, we stack two fully-connected layers. The first is an embedding layer for features and the second maps from low to high dimensions to make the output have the same shape as the target&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>].</p>    </section>   </section>   <section id="sec-14">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>All the experiments in this study are completed on an Nvidia GPU Quadro P6000 and the models are built using Keras&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] libraries with TensorFlow&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] as the backend.</p>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>     </header>     <p>We use two sets of geotagged tweets collected from 2015-07-09 to 2017-09-30 in two cities: Seattle, WA (SEA) and New York City (NYC) to carry out all our experiments. The total number of tweets in each dataset is 1,025,181 and 10,084,839 , respectively. Geotagged tweets are those that contain a pair of longitude and latitude coordinates values which indicate their location. These geotagged tweets are then aggregated into grid cells, which are 500<em>m</em> &#x00D7; 500<em>m</em> squares spanning from [47.579784, -122.373135] to [47.633604, -122.293062] in SEA, and from [40.647984, -74.111093] to [40.853945, -73.837472] in NYC, which correspond to their metropolitan area, respectively. The two grid maps are shown in Figure&#x00A0;<a class="fig" href="#fig7">7</a>, respectively. Note that the examples in Figure&#x00A0;<a class="fig" href="#fig1">1</a> and Figure&#x00A0;<a class="fig" href="#fig8">8</a> are illustrated on the inner 8 &#x00D7; 8 grid cells of Figure&#x00A0;<a class="fig" href="#fig7">7</a>(a) as the boundary cells have few tweets to show. In this study, we define the interval of a time step to be 30 minutes, an empirical trade-off between the prediction promptness and accuracy. For example, the task of prediction prefers shorter temporal intervals as it gives more timely results. Shorter temporal intervals, however, might be too small to aggregate enough tweets for making high-quality prediction due to the sparsity of tweets. <figure id="fig7">      <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">(a) 12 &#x00D7; 12 grid map in the Seattle. (b) 46 &#x00D7; 46 grid map in the NYC. The bold cells in each grid map are the chosen regions to draw Figure&#x00A0;<a class="fig" href="#fig5">5</a>, respectively.</span>      </div>     </figure>     </p>     <p>     <strong>Removing Spam Tweets</strong> We identify two types of tweets as spam: (1) The tweets whose geographical coordinate values are the same as one of the city centers. Because such tweets are likely posted by accounts who simply give out a nominal location address (e.g., &#x201C;Seattle, WA&#x201D; and &#x201C;New York City&#x201D;) which are then automatically geodecoded by the Twitter location service to city centers. Such accounts send out geo-targeted tweets spams such as &#x201C;@tmj_sea_legal1&#x201D; and they are very unlikely to be present exactly at the city centers. We removed 224,335 and 0 tweets for Seattle and NYC in this step. (2) The tweets that are posted by suspicious Twitter users who behave more like bots, e.g., publishing more than 5 tweets at exactly the same location and 3 or more of such tweets are sent out only in 1 minute. We removed 204,800 and 44,389 tweets for Seattle and NYC datasets in this step. After filtering out spam tweets, we now have 756,457 and 9,880,039 tweets in the Seattle and NYC datasets, respectively.</p>     <p>     <strong>Normalization</strong> The values of the tweet count are scaled to [ &#x2212; 1, 1] using Min-Max normalization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. Consequently, a tanh activation function is applied to the output for a faster convergence&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. To compare with the groundtruth, the predicted values are scaled back to normal ranges.</p>     <p>     <strong>Training</strong> We split the data in each of the two cities into the training and the testing dataset, where the testing dataset contains the last 28 days of the observation sequences and the rest of the data belong to the training dataset. In so doing, we have 18,624 training samples and 1,344 testing samples for the city of Seattle, and 26,304 training and 1,344 testing samples for New York City. The discrepancy between the numbers of training samples are due to occasional missing data on some days for each of the two cities. Following&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], our training procedure contains two steps. (1) To find a good initialization of our model, We first train our model using 90% of the training data and reserve the rest 10% as validation data. During this step, we apply early-stopping based on the validation loss. (2) After that, we continue to train our model on all the training data for another fixed number of epochs (e.g. 100 epochs). The loss function used in the training process is the Mean Squared Error.</p>     <p>By default, the periodicity and trend interval <em>p</em> and <em>q</em> are set to one day and one week, respectively. The lengths of the dependence sequences are set to <em>l<sub>c</sub>     </em> = 3, <em>l<sub>p</sub>     </em> = 1 and <em>l<sub>q</sub>     </em> = 1.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Baseline Approaches</h3>     </div>     </header>     <p>We choose the following seven methods as the baseline approaches:</p>     <ul class="list-no-style">     <li id="list1" label="&#x2022;"><em>ZERO</em>: a naive baseline approach which simply yields predictions of 0s for all tweet count.<br/></li>     <li id="list2" label="&#x2022;"><em>ARIMA</em>: Auto Regressive Integrated Moving Average (ARIMA) model is a time series analysis model for understanding the time series data or predicting future points in the series&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>].<br/></li>     <li id="list3" label="&#x2022;"><em>SARIMA</em>: Seasonal ARIMA, which additionally considers possible seasonal effects.<br/></li>     <li id="list4" label="&#x2022;"><em>Eyewitness</em>: Eyewitness&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>] uses gradient boosting regressors to train a regression function by considering features such as the time of the day, the day of the week and tweet counts from neighboring regions.<br/></li>     <li id="list5" label="&#x2022;"><em>ST-ResNet</em>: ST-ResNet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0032">32</a>] is the currently state-of-the-art method used in spatiotemporal data prediction which is a strong baseline. Different from the proposed method, it uses regular convolution layers instead of convolutional LSTM layers. By default, ST-ResNet uses one residual block, which achieves the best results on our dataset. The effects of stacking multiple residual blocks will be further explored in Section&#x00A0;<a class="sec" href="#sec-22">4.4.4</a>.<br/></li>     <li id="list6" label="&#x2022;"><em>ConvLSTM &#x00D7; &#x2009;3</em>: a baseline approach that simply stacks three layers of ConvLSTM in order to contrast the effectiveness of a residual block over a ConvLSTM layer. It replaces the Residual ConvLSTM block with a ConvLSTM layer in Figure&#x00A0;<a class="fig" href="#fig4">4</a>.<br/></li>     <li id="list7" label="&#x2022;"><em>ConvLSTM &#x00D7; &#x2009;4</em>: a baseline approach that stacks four layers of ConvLSTM in order to contrast the effectiveness of the skip connection in the residual block. We define this model by simply removing the skip connections in our proposed model.<br/></li>     </ul>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Evaluation Metric</h3>     </div>     </header>     <p>The results are measured by the Root Mean Square Error (RMSE): <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \sqrt []{\frac{1}{n}\sum ^n\limits _{i=1}{(Y_i-X_i)^2}} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <em>n</em> is number of testing cases, and <em>Y<sub>i</sub>     </em> and <em>X<sub>i</sub>     </em> are the prediction and groundtruth values, respectively.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Experimental Results</h3>     </div>     </header>     <p>We start with an illustration of two predication examples, followed by a comparison between our proposed method and the six baselines mentioned in section&#x00A0;<a class="sec" href="#sec-16">4.2</a>. Then we study the effectiveness of temporal dependence sequences and the effect of deeper neural networks.</p>     <p>Figure&#x00A0;<a class="fig" href="#fig8">8</a> presents the prediction results using our model for the two tweet count distribution examples in Figure&#x00A0;<a class="fig" href="#fig1">1</a>. The denotation in each grid cell is in the form of &#x201C;prediction|groundtruth&#x201D;, referring to the prediction <em>vs.</em> groundtruth number of tweet count. The numbers in red are predictions. No denotation in a cell means a correct match with the groundtruth. The results show that both of the predications are generally good matches to the groundtruth by being able to capture the overall distribution of tweets as well as yielding only a slight difference for grid cells that have larger values of the tweet count. The error is mostly caused from predicting empty tweets for grid cells which have only one tweet. Such a situation is relatively arbitrary in the sense that the occurrence of such a tweet can be sporadic, which makes it hard to predict. <figure id="fig8">      <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">(a) Prediction Example of Tweet Count Distribution around the Seattle city center at 17:00-17:30 on 2016-07-16. (b) Prediction Example of Tweet Count Distribution around around the Seattle city center at 17:30-18:00 on 2016-07-16. (The denotation in each grid cell is in the form of &#x201C;prediction|groundtruth&#x201D;, referring to the prediction <em>vs.</em> groundtruth number of tweets. The numbers in red are predictions. No denotation in a cell means a correct match with the groundtruth.)</span>      </div>     </figure>     </p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Comparison Results (RMSE) on city of Seattle and NYC</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">Method</td>        <td style="text-align:right;">Seattle</td>        <td style="text-align:right;">NYC</td>       </tr>       <tr>        <td style="text-align:left;">ZERO</td>        <td style="text-align:right;">0.6353</td>        <td style="text-align:right;">1.2054</td>       </tr>       <tr>        <td style="text-align:left;">ARIMA</td>        <td style="text-align:right;">0.5117</td>        <td style="text-align:right;">0.5301</td>       </tr>       <tr>        <td style="text-align:left;">SARIMA</td>        <td style="text-align:right;">0.5242</td>        <td style="text-align:right;">0.5340</td>       </tr>       <tr>        <td style="text-align:left;">Eyewitness</td>        <td style="text-align:right;">0.4580</td>        <td style="text-align:right;">0.5332</td>       </tr>       <tr>        <td style="text-align:left;">ST-ResNet</td>        <td style="text-align:right;">0.4344</td>        <td style="text-align:right;">0.5166</td>       </tr>       <tr>        <td style="text-align:left;">ConvLSTM &#x00D7;&#x2009;3</td>        <td style="text-align:right;">0.4659</td>        <td style="text-align:right;">0.5232</td>       </tr>       <tr>        <td style="text-align:left;">ConvLSTM &#x00D7;&#x2009;4</td>        <td style="text-align:right;">0.4557</td>        <td style="text-align:right;">0.5278</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>Our Model</strong>        </td>        <td style="text-align:right;">        <strong>0.4164</strong>        </td>        <td style="text-align:right;">        <strong>0.4879</strong>        </td>       </tr>      </tbody>     </table>     </div>     <section id="sec-19">     <p><em>4.4.1 Compare with Baselines.</em> Table&#x00A0;<a class="tbl" href="#tab1">1</a> shows the results of seven baselines and the proposed method on two cities: Seattle and New York City. Simply generating prediction of 0s (ZERO) for every grid cell performs much worse than all other methods. We notice that ST-ResNet outperforms all the other methods except the proposed one, showing its effectiveness. Using ConvLSTM achieves comparative results to ST-ResNet. We believe that this is because of the ability of ConvLSTM to model the spatial and temporal information well. The proposed method outperforms all the baselines and achieves state-of-the-art results. It achieves significantly better accuracies than both ConvLSTM &#x00D7;&#x2009;3 and ConvLSTM &#x00D7;&#x2009;4, which illustrates the effectiveness of the skip connections. As mentioned in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>], the loss functions of deeper networks are more likely to be chaotic, while adding skip connections can prevent this leading to a more convex loss function which is easier to train.</p>     </section>     <section id="sec-20">     <header>      <div class="title-info">       <h4>        <span class="section-number">4.4.2</span> Effects of <em>period</em> and <em>trend</em> Dependence</h4>      </div>     </header>     <p>We now investigate the performance of our model with and without utilizing <em>period</em> and <em>trend</em> information. We set the corresponding length variables <em>l<sub>q</sub>      </em> (<em>l<sub>q</sub>      </em>) to 0 or 1 to indicate whether the model is configured to use such information. The results are presented in Figure&#x00A0;<a class="fig" href="#fig9">9</a>(a). It shows that only using <em>closeness</em> information may perform even worse than the baselines and justifies the exploitation of <em>period</em> and <em>trend</em> dependence sequences. Nevertheless, in this study, we found that longer (> 2) <em>period</em> and <em>trend</em> dependence sequences do not always yield better accuracy. <figure id="fig9">       <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig9.jpg" class="img-responsive" alt="Figure 9"        longdesc=""/>       <div class="figure-caption">        <span class="figure-number">Figure 9:</span>        <span class="figure-title">(a) Effects of using <em>period</em> and <em>trend</em> dependence or not. (b) Effects of length of <em>closeness</em> sequences. Note that the higher the curve, the smaller the RMSE value.</span>       </div>      </figure>     </p>     </section>     <section id="sec-21">     <header>      <div class="title-info">       <h4>        <span class="section-number">4.4.3</span> Effects of Length of <em>closeness</em> Dependence Sequences</h4>      </div>     </header>     <p>In this subsection, we study whether a longer <em>closeness</em> dependence sequence can help achieve better performance in method ST-ResNet and in our model. The results are illustrated in Figure&#x00A0;<a class="fig" href="#fig9">9</a>(b). It can be seen that both models are able to achieve slightly better accuracy when the length begins to increase, but the performance saturates or becomes worse after <em>l<sub>c</sub>      </em> reaches 4. One possible reason is that the tweets that happened a longer time ago may not provide much information for predicting the tweet at the current time. Meanwhile, our model has higher gains than ST-ResNet because recurrent structure is more powerful in capturing temporal information. Moreover, we notice that ST-ResNet is more sensitive to tweets posted a longer time ago as the performance drops dramatically when <em>l<sub>c</sub>      </em> = 4 for Seattle and <em>l<sub>c</sub>      </em> = 5 for New York City.</p>     </section>     <section id="sec-22">     <p><em>4.4.4 Effects of Building Deeper Networks.</em> In general, we found no significant gains by stacking more residual ConvLSTM blocks in our method ResConvLSTM. Take the city of Seattle for example, Figure&#x00A0;<a class="fig" href="#fig10">10</a> illustrates the results of stacking {0, 1, 2, 4} residual blocks using RMSE metrics. It shows that two or more layers can not guarantee to achieve better results, although the performance deteriorates if no residual block is used at all. The situation is similar when it comes to stacking more residual convolutional blocks in baseline approach ST-ResNet. We believe this is due to the following two reasons: (1) As discussed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>], deeper networks usually have a more chaotic loss function, making them difficult to train. (2) Deeper networks are more likely to suffer from over fitting. <figure id="fig10">       <img src="http://deliveryimages.acm.org/10.1145/3200000/3191571/images/www18companion-310-fig10.jpg" class="img-responsive"        alt="Figure 10"        longdesc=""/>       <div class="figure-caption">        <span class="figure-number">Figure 10:</span>        <span class="figure-title">Results of stacking more residual blocks in the city of Seattle.</span>       </div>      </figure>     </p>     </section>    </section>   </section>   <section id="sec-23">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusions</h2>     </div>    </header>    <p>In this paper, we proposed a novel residual convolutional LSTM model for predicting tweet count. In essence, we utilize the framework of ST-ResNet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] to model the temporal properties in spatiotemporal tweet count data such as <em>closeness</em>, <em>period</em> and <em>trend</em> dependence. To better capture the temporal correlation between sequences, we use ConvLSTM layers instead of regular convolution layers in ST-ResNet as the building block of the network. To make the network easier to train, we added skip connections. We evaluated the proposed method on the geotagged tweets collected for two cities: Seattle, WA and New York City. Our experiments show that the proposed method outperforms the baseline approaches and achieves state-of-the-art results. We carried out ablation studies and confirmed the necessity of utilizing the temporal properties <em>period</em> and <em>trend</em>. Finally, due to the fact that Twitter users have less intensive spatial moving activity, together with the data sparsity in some spatial area, we found that stacking more residual blocks to build deeper networks does not always yield better accuracy.</p>    <p>Predicting tweet count at a local place have many potential applications such as anomaly and event detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. In the future, we will exploit our method on local news detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. The intuition is that if there is suddenly an abnormal change in the number of tweets at a location (like a significant increase), it probably means something is happening there. Specifically, one can first make a prediction on the number of tweets at a location to appear in the next time step. If the prediction is significantly less than the actual number of tweets, it might be considered as an anomaly, which likely corresponds to a local event.</p>    <p>In addition, instead of predicting the number of tweets at a location, we may also investigate the possibility of predicting the number of Twitter users at a location. This has many applications as well such as population estimation and human mobility monitoring at city-wide scale.</p>    <p>Moreover, it is also interesting to extend the current model on tweets that don&#x0027;t have embedded GPS coordinates. We plan to approach this by applying geotagging procedures&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>].</p>   </section>   <section id="sec-24">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Acknowledgement</h2>     </div>    </header>    <p>We would like to thank Dr. John Krumm and Dr. Jin Li from Microsoft Research for providing supporting funding and the access to tweets of Twitter. This work was also supported in part by the National Science Foundation under Grant IIS-13-20791 and Grant CNS-1405688.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Mart&#x00ED;n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg&#x00A0;S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man&#x00E9;, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi&#x00E9;gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. (2015). <a class="link-inline force-break" href="https://www.tensorflow.org/"      target="_blank">https://www.tensorflow.org/</a>Software available from tensorflow.org.</li>     <li id="BibPLXBIB0002" label="[2]">M. Ceci, R. Corizzo, F. Fumarola, D. Malerba, and A. Rashkovska. 2017. Predictive Modeling of PV Energy Production: How to Set Up the Learning Task for a Better Prediction?<em>      <em>IEEE Transactions on Industrial Informatics</em>     </em>13, 3 (June 2017), 956&#x2013;966. <a class="link-inline force-break" href="https://doi.org/10.1109/TII.2016.2604758"      target="_blank">https://doi.org/10.1109/TII.2016.2604758</a></li>     <li id="BibPLXBIB0003" label="[3]">J. Chae, D. Thom, H. Bosch, Y. Jang, R. Maciejewski, D.&#x00A0;S. Ebert, and T. Ertl. 2012. Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition. In <em>      <em>2012 IEEE Conference on Visual Analytics Science and Technology (VAST)</em>     </em>(<em>VAST &#x2019;12</em>). 143&#x2013;152. <a class="link-inline force-break"      href="https://doi.org/10.1109/VAST.2012.6400557"      target="_blank">https://doi.org/10.1109/VAST.2012.6400557</a></li>     <li id="BibPLXBIB0004" label="[4]">Francois Chollet <em>et al.</em> 2015. Keras. https://github.com/fchollet/keras. (2015).</li>     <li id="BibPLXBIB0005" label="[5]">Alex Graves. 2013. Generating Sequences With Recurrent Neural Networks. abs/1308.0850 (2013). arxiv:1308.0850<a class="link-inline force-break" href="http://arxiv.org/abs/1308.0850"      target="_blank">http://arxiv.org/abs/1308.0850</a></li>     <li id="BibPLXBIB0006" label="[6]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. <em>      <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>     </em> (2016), 770&#x2013;778.</li>     <li id="BibPLXBIB0007" label="[7]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In <em>      <em>2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>     </em>(<em>CVPR &#x2019;16</em>). 770&#x2013;778. <a class="link-inline force-break" href="https://doi.org/10.1109/CVPR.2016.90"      target="_blank">https://doi.org/10.1109/CVPR.2016.90</a></li>     <li id="BibPLXBIB0008" label="[8]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <em>      <em>Identity Mappings in Deep Residual Networks</em>     </em>. Springer International Publishing, Cham, 630&#x2013;645. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-46493-0_38"      target="_blank">https://doi.org/10.1007/978-3-319-46493-0_38</a></li>     <li id="BibPLXBIB0009" label="[9]">S.L. Ho and M. Xie. 1998. The use of ARIMA models for reliability forecasting and analysis. <em>      <em>Computers &#x0026; Industrial Engineering</em>     </em>35, 1 (1998), 213&#x2013;216.</li>     <li id="BibPLXBIB0010" label="[10]">Sepp Hochreiter and J&#x00FC;rgen Schmidhuber. 1997. Long Short-Term Memory. <em>      <em>Neural Computation.</em>     </em>9, 8 (1997).</li>     <li id="BibPLXBIB0011" label="[11]">Sepp Hochreiter and J&#x00FC;rgen Schmidhuber. 1997. Long Short-Term Memory. <em>      <em>Neural Comput.</em>     </em>9, 8 (Nov. 1997), 1735&#x2013;1780. <a class="link-inline force-break"      href="https://doi.org/10.1162/neco.1997.9.8.1735"      target="_blank">https://doi.org/10.1162/neco.1997.9.8.1735</a></li>     <li id="BibPLXBIB0012" label="[12]">Alan Jackoway, Hanan Samet, and Jagan Sankaranarayanan. 2011. Identification of Live News Events Using Twitter. In <em>      <em>Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Social Networks</em>     </em>(<em>LBSN &#x2019;11</em>). ACM, New York, NY, USA, 25&#x2013;32. <a class="link-inline force-break" href="https://doi.org/10.1145/2063212.2063224"      target="_blank">https://doi.org/10.1145/2063212.2063224</a></li>     <li id="BibPLXBIB0013" label="[13]">John Krumm and Eric Horvitz. 2015. Eyewitness: Identifying Local Events via Space-time Signals in Twitter Feeds. In <em>      <em>Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</em>     </em>(<em>SIGSPATIAL &#x2019;15</em>). ACM, New York, NY, USA, Article 20, 10&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2820783.2820801"      target="_blank">https://doi.org/10.1145/2820783.2820801</a></li>     <li id="BibPLXBIB0014" label="[14]">Yann LeCun, L&#x00E9;on Bottou, Genevieve&#x00A0;B. Orr, and Klaus-Robert M&#x00FC;ller. 1998. Efficient BackProp. In <em>      <em>Neural Networks: Tricks of the Trade</em>     </em>. Springer-Verlag, London, UK, UK, 9&#x2013;50. <a class="link-inline force-break"      href="http://dl.acm.org/citation.cfm?id=645754.668382"      target="_blank">http://dl.acm.org/citation.cfm?id=645754.668382</a></li>     <li id="BibPLXBIB0015" label="[15]">Ryong Lee and Kazutoshi Sumiya. 2010. Measuring Geographical Regularities of Crowd Behaviors for Twitter-based Geo-social Event Detection. In <em>      <em>Proceedings of the 2Nd ACM SIGSPATIAL International Workshop on Location Based Social Networks</em>     </em>(<em>LBSN &#x2019;10</em>). ACM, New York, NY, USA, 1&#x2013;10. <a class="link-inline force-break" href="https://doi.org/10.1145/1867699.1867701"      target="_blank">https://doi.org/10.1145/1867699.1867701</a></li>     <li id="BibPLXBIB0016" label="[16]">H. Li, Z. Xu, G. Taylor, and T. Goldstein. 2017. Visualizing the Loss Landscape of Neural Nets. <em>      <em>ArXiv e-prints</em>     </em> (Dec. 2017).</li>     <li id="BibPLXBIB0017" label="[17]">Michael&#x00A0;D. Lieberman and Hanan Samet. 2011. Multifaceted Toponym Recognition for Streaming News. In <em>      <em>Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;11</em>). ACM, New York, NY, USA, 843&#x2013;852. <a class="link-inline force-break" href="https://doi.org/10.1145/2009916.2010029"      target="_blank">https://doi.org/10.1145/2009916.2010029</a></li>     <li id="BibPLXBIB0018" label="[18]">Michael&#x00A0;D. Lieberman and Hanan Samet. 2012. Adaptive Context Features for Toponym Resolution in Streaming News. In <em>      <em>Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(<em>SIGIR &#x2019;12</em>). ACM, New York, NY, USA, 731&#x2013;740. <a class="link-inline force-break" href="https://doi.org/10.1145/2348283.2348381"      target="_blank">https://doi.org/10.1145/2348283.2348381</a></li>     <li id="BibPLXBIB0019" label="[19]">Michael&#x00A0;D. Lieberman, Hanan Samet, and Jagan Sankaranayananan. 2010. Geotagging: Using Proximity, Sibling, and Prominence Clues to Understand Comma Groups. In <em>      <em>Proceedings of the 6th Workshop on Geographic Information Retrieval</em>     </em>(<em>GIR &#x2019;10</em>). ACM, New York, NY, USA, Article 6, 8&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/1722080.1722088"      target="_blank">https://doi.org/10.1145/1722080.1722088</a></li>     <li id="BibPLXBIB0020" label="[20]">Shu-Lan Lin, Hong-Qiong Huang, Da-Qi Zhu, and Tian-Zhen Wang. 2009. The application of space-time ARIMA model on traffic flow forecasting. In <em>      <em>2009 International Conference on Machine Learning and Cybernetics</em>     </em>(<em>ICMLC &#x2019;09</em>), Vol.&#x00A0;6. 3408&#x2013;3412. <a class="link-inline force-break"      href="https://doi.org/10.1109/ICMLC.2009.5212785"      target="_blank">https://doi.org/10.1109/ICMLC.2009.5212785</a></li>     <li id="BibPLXBIB0021" label="[21]">Adam Marcus, Michael&#x00A0;S. Bernstein, Osama Badar, David&#x00A0;R. Karger, Samuel Madden, and Robert&#x00A0;C. Miller. 2011. Twitinfo: Aggregating and Visualizing Microblogs for Event Exploration. In <em>      <em>CHI &#x2019;11</em>     </em>. 227&#x2013;236.</li>     <li id="BibPLXBIB0022" label="[22]">Hanan Samet. 2014. Using Minimaps to Enable Toponym Resolution with an Effective 100% Rate of Recall. In <em>      <em>Proceedings of the 8th Workshop on Geographic Information Retrieval</em>     </em>(<em>GIR &#x2019;14</em>). ACM, New York, NY, USA, Article 9, 8&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2675354.2675698"      target="_blank">https://doi.org/10.1145/2675354.2675698</a></li>     <li id="BibPLXBIB0023" label="[23]">Jagan Sankaranarayanan, Hanan Samet, Benjamin&#x00A0;E. Teitler, Michael&#x00A0;D. Lieberman, and Jon Sperling. 2009. TwitterStand: News in Tweets. In <em>      <em>Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</em>     </em>(<em>SIGSPATIAL &#x2019;09</em>). ACM, New York, NY, USA, 42&#x2013;51. <a class="link-inline force-break" href="https://doi.org/10.1145/1653771.1653781"      target="_blank">https://doi.org/10.1145/1653771.1653781</a></li>     <li id="BibPLXBIB0024" label="[24]">Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. 2015. Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. In <em>      <em>Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada</em>     </em>(<em>NIPS &#x2019;15</em>). 802&#x2013;810. <a class="link-inline force-break"      href="http://papers.nips.cc/paper/5955-convolutional-lstm-"      target="_blank">http://papers.nips.cc/paper/5955-convolutional-lstm-</a></li>     <li id="BibPLXBIB0025" label="[25]">Ilya Sutskever, Oriol Vinyals, and Quoc&#x00A0;V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In <em>      <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>     </em>(<em>NIPS &#x2019;14</em>). MIT Press, Cambridge, MA, USA, 3104&#x2013;3112. <a class="link-inline force-break"      href="http://dl.acm.org/citation.cfm?id=2969033.2969173"      target="_blank">http://dl.acm.org/citation.cfm?id=2969033.2969173</a></li>     <li id="BibPLXBIB0026" label="[26]">Akin Tascikaraoglu. 2018. Evaluation of spatio-temporal forecasting methods in various smart city applications. <em>      <em>Renewable and Sustainable Energy Reviews</em>     </em>82 (2018), 424 &#x2013; 435. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.rser.2017.09.078"      target="_blank">https://doi.org/10.1016/j.rser.2017.09.078</a></li>     <li id="BibPLXBIB0027" label="[27]">Faizan Wajid, Hong Wei, and Hanan Samet. 2017. Identifying Short-Names for Place Entities from Social Networks. In <em>      <em>Proceedings of the 1st ACM SIGSPATIAL Workshop on Recommendations for Location-based Services and Social Networks</em>     </em>(<em>LocalRec&#x2019;17</em>). ACM, New York, NY, USA, Article 4, 4&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/3148150.3148157"      target="_blank">https://doi.org/10.1145/3148150.3148157</a></li>     <li id="BibPLXBIB0028" label="[28]">Hong Wei, Jagan Sankaranarayanan, and Hanan Samet. 2017. Finding and Tracking Local Twitter Users for News Detection. In <em>      <em>Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</em>     </em>(<em>SIGSPATIAL&#x2019;17</em>). ACM, New York, NY, USA, Article 64, 4&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/3139958.3141797"      target="_blank">https://doi.org/10.1145/3139958.3141797</a></li>     <li id="BibPLXBIB0029" label="[29]">Hong Wei, Jagan Sankaranarayanan, and Hanan Samet. 2017. Measuring Spatial Influence of Twitter Users by Interactions. In <em>      <em>Proceedings of the 1st ACM SIGSPATIAL Workshop on Analytics for Local Events and News</em>     </em>(<em>LENS&#x2019;17</em>). ACM, New York, NY, USA, Article 2, 10&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/3148044.3148046"      target="_blank">https://doi.org/10.1145/3148044.3148046</a></li>     <li id="BibPLXBIB0030" label="[30]">Haiyang Yu, Zhihai Wu, Shuqin Wang, Yunpeng Wang, and Xiaolei Ma. 2017. Spatiotemporal Recurrent Convolutional Networks for Traffic Prediction in Transportation Networks. In <em>      <em>Sensors</em>     </em>.</li>     <li id="BibPLXBIB0031" label="[31]">Quan Yuan, Wei Zhang, Chao Zhang, Xinhe Geng, Gao Cong, and Jiawei Han. 2017. PRED: Periodic Region Detection for Mobility Modeling of Social Media Users. In <em>      <em>Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</em>     </em>(<em>WSDM &#x2019;17</em>). ACM, New York, NY, USA, 263&#x2013;272. <a class="link-inline force-break" href="https://doi.org/10.1145/3018661.3018680"      target="_blank">https://doi.org/10.1145/3018661.3018680</a></li>     <li id="BibPLXBIB0032" label="[32]">Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction. In <em>      <em>AAAI</em>     </em>(<em>AAAI &#x2019;17</em>).</li>     <li id="BibPLXBIB0033" label="[33]">Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen Yi. 2016. DNN-based Prediction Model for Spatio-temporal Data. In <em>      <em>Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</em>     </em>(<em>SIGSPATIAL &#x2019;16</em>). ACM, New York, NY, USA, Article 92, 4&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2996913.2997016"      target="_blank">https://doi.org/10.1145/2996913.2997016</a></li>     <li id="BibPLXBIB0034" label="[34]">Yu Zhang, William Chan, and Navdeep Jaitly. 2017. Very deep convolutional networks for end-to-end speech recognition. <em>      <em>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>     </em> (2017), 4845&#x2013;4849.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International (CC-BY-NC-ND&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY-NC-ND&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191571">https://doi.org/10.1145/3184558.3191571</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
