<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Gnirut: The Trouble With Being Born Human In An Autonomous
  World</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191612'>https://doi.org/10.1145/3184558.3191612</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191612'>https://w3id.org/oa/10.1145/3184558.3191612</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Gnirut: The Trouble With Being
          Born Human In An Autonomous World</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Luca</span> <span class=
          "surName">Viganò</span> King's College London, Bush
          House, 30 AldwychLondon, UKWC2B 4BG, <a href=
          "mailto:luca.vigano@kcl.ac.uk">luca.vigano@kcl.ac.uk</a>
        </div>
        <div class="author">
          <span class="givenName">Diego</span> <span class=
          "surName">Sempreboni</span> King's College London, Bush
          House, 30 AldwychLondon, UKWC2B 4BG, <a href=
          "mailto:diego.sempreboni@kcl.ac.uk">diego.sempreboni@kcl.ac.uk</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191612"
        target=
        "_blank">https://doi.org/10.1145/3184558.3191612</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>What if we delegated so much to autonomous AI and
        intelligent machines that They passed a law that forbids
        humans to carry out a number of professions? We conceive
        the plot of a new episode of Black Mirror to reflect on
        what might await us and how we can deal with such a
        future.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Computing methodologies</strong>
        → <strong>Philosophical/theoretical foundations of
        artificial intelligence;</strong> <strong>Theory of
        mind;</strong> • <strong>Security and privacy</strong> →
        <em>Social aspects of security and privacy;</em> •
        <strong>Human-centered computing</strong> → <em>HCI theory,
        concepts and models;</em></small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Luca Viganò and Diego Sempreboni. 2018. Gnirut: The
          Trouble With Being Born Human In An Autonomous World. In
          <em>WWW '18 Companion: The 2018 Web Conference
          Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 5 Pages. <a href=
          "https://doi.org/10.1145/3184558.3191612" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3191612</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <p>Autonomous AI; Reverse Turing Test; Theory of Mind;
    Deception</p>
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Prologue: “What
          do you want to be when you grow up?”</h2>
        </div>
      </header>
      <p>EXT. MAIN STREET — DAY.</p>
      <p>An eight-year old child (C), one of the child's parents
      (P) and a woman they just met (W).</p>
      <ol class="list-no-style">
        <li id="list1" label="W:">And what do you want to be when
        you grow up?<br /></li>
        <li id="list2" label="C:">Well, I...<br /></li>
        <li id="list3" label="W:">A philosopher like your
        father?<br /></li>
        <li id="list4" label="C:">I... I...<br /></li>
        <li id="list5" label="P:">Come on, don't be shy. Answer the
        nice lady.<br /></li>
        <li id="list6" label="W:">Or perhaps a painter like your
        mother?<br /></li>
        <li id="list7" label="C:">I...<br /></li>
        <li id="list8" label="P:">Come on, speak up! Please,
        forgive him. He's so shy. Come on!<br /></li>
        <li id="list9" label="C:">I...<br /></li>
        <li id="list10" label="P:">This is not ok. Not ok, you
        understand? The nice lady asked you a question and you
        should answer.<br /></li>
        <li id="list11" label="W:">No worries, no
        worries.<br /></li>
        <li id="list12" label="P:">Wait till we get
        home...<br /></li>
        <li id="list13" label="W:">You'll tell me another time,
        ok?<br /></li>
        <li id="list14" label="C:">... a pilot! I want be a
        pilot!<br /></li>
        <li id="list15" label="W:">Oh, but that's impossible,
        dear.<br /></li>
        <li id="list16" label="C:">Why?<br /></li>
        <li id="list17" label="W:">Because you can't.<br /></li>
        <li id="list18" label="C:">I can't? Why?<br /></li>
        <li id="list19" label="W:">It's not just you. We. We
        can't.<br /></li>
        <li id="list20" label="C:">Why?<br /></li>
        <li id="list21" label="P:">That's enough. Don't be a
        nuisance.<br /></li>
        <li id="list22" label="W:">Don't be silly, no nuisance at
        all.<br /></li>
        <li id="list23" label="C:">Why can't I be a
        pilot?<br /></li>
        <li id="list24" label="P:">That's enough, I
        said!<br /></li>
        <li id="list25" label="W:">Oh, no, no, please allow me to
        explain. There is a law.<br /></li>
        <li id="list26" label="C:">A law?<br /></li>
        <li id="list27" label="W:">A law that says we are not
        allowed to be pilots.<br /></li>
        <li id="list28" label="C:">Why?<br /></li>
        <li id="list29" label="W:">Because people were
        afraid.<br /></li>
        <li id="list30" label="P:">People were afraid of flying, so
        They made a law.<br /></li>
        <li id="list31" label="C:">How come people were afraid of
        flying? If at all, people should be afraid of
        falling.<br /></li>
        <li id="list32" label="W:">Ha! That's clever.<br /></li>
        <li id="list33" label="P:">People were too afraid that
        their time had come, you understand?<br /></li>
        <li id="list34" label="C:">But if it had come, why worry?
        It had come anyway. On a plane or on the ground.<br /></li>
        <li id="list35" label="W:">A-ha, and what if it's the
        pilot's time to go?<br /></li>
        <li id="list36" label="P:">What if the pilot got sick, or
        decided to commit suicide?<br /></li>
        <li id="list37" label="W:">What if the pilot made a
        mistake?<br /></li>
        <li id="list38" label="C:">I wouldn't make any
        mistakes.<br /></li>
        <li id="list39" label="W:">But of course you would. Of
        course. We all do.<br /></li>
        <li id="list40" label="C:">I wouldn't get sick or kill
        myself. They could test me.<br /></li>
        <li id="list41" label="W:">Oh, They did. They did test us.
        That's why They passed the law.<br /></li>
        <li id="list42" label="P:">That's why They passed all those
        laws.<br /></li>
        <li id="list43" label="W:">For our good.<br /></li>
        <li id="list44" label="P:">Only for our good. That's why we
        have to obey.<br /></li>
        <li id="list45" label="C:">But...<br /></li>
        <li id="list46" label="P:">No but! You must learn to know
        your place.<br /></li>
        <li id="list47" label="C:">But I want to be a
        pilot.<br /></li>
        <li id="list48" label="W:">But you can't dear.<br /></li>
        <li id="list49" label="P:">That's why They made a law. To
        protect us.<br /></li>
        <li id="list50" label="W:">Pilots, doctors, surgeons,
        judges, construction builders, ... that's not for us
        anymore.<br /></li>
        <li id="list51" label="P:">To protect us.<br /></li>
        <li id="list52" label="C:">Yes, but why? Why can't I be a
        pilot?<br /></li>
      </ol>
      <p>We wrote this scene as the set-up of a future Black Mirror
      episode, in which “They” have passed laws that forbid humans
      to carry out a number of professions. How could this have
      happened?</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> From the
          Human-to-AI Ratio ...</h2>
        </div>
      </header>
      <p>The current recommendations (and, in some cases,
      regulations) require a <em>many-to-1 human-to-robot
      ratio</em> (typically between <em>2-to-1</em> and
      <em>5-to-1</em>) for tasks that are particularly
      security-sensitive and/or safety-sensitive as they involve
      the protection of diverse critical assets such as data,
      systems, infrastructures and, ultimately, human life.
      Examples of such tasks are those carried out in military
      operations, such as drone strikes or rescue, demining or
      bomb-disposal operations — studies carried out in the early
      2000’s in the context of military operations indicated the
      then current state of practice for aerial systems to be
      many-to-1 (as witnessed by the Global Hawk and Predator
      crews) and that ground robots are more effective with a
      2-to-1 ratio&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>]. Other, more “civilian”, examples
      are tasks to be carried out in critical infrastructures or in
      specific automation environments and situations where human
      life or expensive assets might be in danger, such as remote
      surgeries or tasks carried out in power plants, in autonomous
      and assisted transportation systems,
      underwater&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>], in space&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>], or in harsh conditions,
      say due to excessive cold, heat or radiation&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0012">12</a>].</p>
      <p>As stated by Murphy and Burke in 2010, the
      human-robot-interaction literature has (often) ignored
      safety, making the assumption that low human-to-robot ratios
      are desirable per se, and</p>
      <p><em>often pursuing an arbitrary goal of 1-to-many based on
      expected advances in vehicle autonomy</em>. <em>[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>]</em></p>
      <p>Nowadays, this 1-to-many goal is not so arbitrary anymore:
      thanks to advances in robot autonomy, we appear to be not too
      far removed from a ratio of 1 human to 5 robots
      (<em>1-to-5</em>) for all kinds of tasks, while ensuring
      safety and security (and privacy) of team, bystanders, and
      robots, as well as logical efficiency at the same time.</p>
      <p>But why stop here? Solutions are currently being sought
      for full automation of systems by removing the human and
      transforming the operating space into a highly instrumented
      and controlled area for robots, as has been common in car
      manufacturing or even in places like Amazon warehouses (with
      carefully segregated spaces). The present reality is that in
      many dynamic and challenging environments we still cannot
      remove the human as there will be times an automated system
      is operating at the boundaries of its competence and might
      require human intervention. However, it is not utopian to
      imagine a near future reality in which the human-to-robot
      ratio has been reduced to <em>0-to-5</em>, fulfilling the
      vision of a fully autonomous world in which robots carry out
      unmanned tasks. Actually, many tasks don't necessarily need a
      physical entity such as an arm or wheels to be carried out,
      but could be accomplished directly by some form of artificial
      intelligence, so we can speak more generally of a 0-to-5
      <em>human-to-AI ratio</em>.</p>
      <p>But why stop here? Why not invert the ratio, and consider
      the case in which 1 human is supervised by one or more AIs?
      In 2017, Rachel Botsman carried out a small but enlightening
      experiment that involved her 3-year old daughter Grace and
      Amazon's Alexa, which she summarized in a New York Times
      Sunday Review Opinion by observing that in the last few
      years, we have been (often passive) witnesses to</p>
      <p><em>a profound shift in our relationship with technology.
      For generations, our trust in it has gone no further than
      feeling confident the machine or mechanism will do what it's
      supposed or expected to do, nothing more, nothing less. We
      trust a washing machine to clean our clothes or an A.T.M. to
      dispense money, but we don't expect to form a relationship
      with them or call them by name</em>.&nbsp;<em>[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>]</em></p>
      <p>Now we can call them by name: Amazon's Alexa, Apple's
      Siri, Google's Google Assistant and Microsoft's Cortana are
      all intelligent personal assistants that recognize natural
      voice (in many different languages) without the requirement
      for keyboard input and support a wide range of user commands,
      ranging from answering questions to providing real-time
      information (such as news, weather forecast or traffic
      information), from making phone calls to compiling to-do
      lists, from setting alarms to playing music or audiobooks or
      streaming podcasts. Alexa can also control several smart
      devices using itself as a home automation system. It's the
      two-faced, sirenlike, beauty of the <em>Internet of
      Things</em>, in which we silently enter an agreement to hand
      over the keys to our houses, to our appliances, to our data,
      to large parts of our lives:</p>
      <p><em>Today, we're no longer trusting machines just to do
      something, but to decide what to do and when to do it. The
      next generation will grow up in an age where it's normal to
      be surrounded by autonomous agents, with or without cute
      names. The Alexas of the world will make a raft of decisions
      for my kids and others like them as they proceed through life
      — everything from whether to have mac and cheese or a green
      bowl for dinner to the perfect gift for a friend's birthday
      to what to do to improve their mood or energy and even advice
      on whom they should date. In time, the question for them
      won't be, “Should we trust robots?” but “Do we trust them too
      much?”</em>&nbsp;<em>[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>]</em></p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> ... to the
          AI-to-Human Ratio</h2>
        </div>
      </header>
      <p>Our thesis in this paper is that we will not stop here. We
      will silently but willingly go for the full inversion of the
      ratio, from human-to-AI to AI-to-human, and hand over the
      vast majority of our choices and decisions, of our data and
      personal information, of our privacy, the vast majority of
      the different facets and aspects of our lives. We won't stop
      at the 1-to-1 AI-to-human ratio exemplified by little Grace's
      naive, and somewhat cute, trust in Alexa:</p>
      <p><em>With some trepidation, I watched my daughter gaily
      hand her decisions over. “Alexa, what should I do today?”
      Grace asked in her singsong voice on Day 3. It wasn't long
      before she was trusting her with the big choices. “Alexa,
      what should I wear today? My pink or my sparkly
      dress?”</em>&nbsp;<em>[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>]</em></p>
      <p>We will go “all in”. The AI-to-human ratio will soon be
      <em>1-to-some</em> (for instance, what if all of Botsman's
      family members, including the adults, had developed such a
      close relationship with Alexa?) and, ultimately,
      <em>some-to-many</em>.</p>
      <p>This will happen gradually, with the change differentials
      initially too small to be noticed, or without us really
      realizing that the sequence of differentials over a longer
      period of time have caused a momentous change. This is
      somewhat reminiscent of the legendary social experiment
      involving 5 monkeys, a ladder and a banana in which, after a
      number of cold showers whenever one of the monkeys tried to
      reach for the banana, and after all 5 original monkeys have
      been stepwise replaced with 5 new monkeys, what is left is a
      group of 5 monkeys that, without ever having received a cold
      shower, continue to beat up any monkey who attempts to climb
      the ladder. The change has occurred gradually, one monkey at
      a time, but in the end a new status quo has been reached
      whose justification has long been forgotten: if it was
      possible to ask the monkeys why they beat up on all those who
      attempted to climb the ladder, their most likely answer would
      be “because that's the way it's always been done around
      here.”<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a> Will we end up being the monkeys
      in our own experiment towards a fully autonomous world?</p>
      <p>There are already plenty of articles, news feeds and books
      explaining the future of employment and how intelligent
      machines will replace humans in many jobs in the context of
      the “industrial revolution 4.0”&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0003">3</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0016">16</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0029">29</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0030">30</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0033">33</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0034">34</a>]. It is not just that many
      boring or strenuous jobs (such as taxi driver&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>], factory
      worker&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>],
      etc.) are under threat of automation and will result in
      technological unemployment — this is often referred to as the
      <em>economic singularity</em>&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0007">7</a>] that will bring us one
      step closer to the <em>technological singularity</em> in
      which ordinary humans will someday be overtaken by
      artificially intelligent machines or cognitively enhanced
      biological intelligence&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>]. In a laudable attempt to safeguard
      human safety, we will also likely soon legislate that some
      jobs should be carried out only by intelligent machines. This
      won't be limited to what is already happening (as we remarked
      above, robots have, at least partially, taken over dangerous
      operations such as demining, bomb-disposal, or nuclear
      meltdown inspections). With the advances in the automation of
      decision making, planning and autonomy, this will also
      encompass professions in which human nature and subjectivity
      might slow down reaction time or adversely affect the
      end-result.</p>
      <p>Think, for example, about the story of the “Miracle on the
      Hudson”: the US Airways Flight 1549, in the climbout after
      takeoff from New York City's LaGuardia Airport on January 15,
      2009, struck a flock of Canada geese and consequently lost
      all engine power. Unable to reach any airport, the pilot
      Chesley “Sully” Sullenberger and his co-pilot Jeffrey Skiles
      glided the plane to a ditching in the Hudson River off
      Midtown Manhattan. All 155 people aboard were rescued by
      nearby boats and there were only few serious injuries. As
      masterfully portrayed in the movie <em>Sully</em> directed by
      Clint Eastwood from a screenplay by Todd Komarnicki (based on
      the book <em>Highest Duty</em> by Sullenberger and Jeffrey
      Zaslow), Sullenberger and Skiles were subject to an
      investigation by the National Transportation Safety Board,
      which initially seemed to conclude that they could have
      safely landed in one of the nearby airports instead of
      attempting a one-in-a-million landing in the river. A number
      of pilots were asked to carry out human-piloted simulations
      of the accident which all showed that it was actually
      possible to land safely in one of the nearby airports. This
      is Tom Hank's/Sully's reply</p>
      <p><em>...you're still not taking into account the human
      factor. These pilots are not reacting like human beings. Like
      people who are experiencing this for the first time.</em>
      ...</p>
      <p><em>You have allowed no time for analysis and decision
      making. And with these sims, you have taken all the humanity
      out of the cockpit.</em>&nbsp;<em>[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>]</em></p>
      <p>Reaction-decision time is then set at thirty-five seconds
      and the simulations are run again... and all result in a
      crash. With that delay, attempting to land in the Hudson was
      indeed the only option. An intelligent machine, on the other
      hand, might have required a much lower reaction-decision time
      and safely made it to a runway.</p>
      <p>To overcome, and prevent, similar problems, the rise in AI
      will alter also legal frameworks. As observed by Bowcott in
      his analysis of a report by the International Bar
      Association&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>],</p>
      <p><em>Among the professions deemed most likely to disappear
      are accountants, court clerks and ‘desk officers at fiscal
      authorities’. ... Even some lawyers risk becoming unemployed.
      “An intelligent algorithm went through the European Court of
      Human Rights’ decisions and found patterns in the text,” the
      report records. “Having learned from these cases, the
      algorithm was able to predict the outcome of other cases with
      79% accuracy ... According to a study conducted by [the
      auditing firm] Deloitte, 100,000 jobs in the English legal
      sector will be automated in the next 20 years.”</em></p>
      <p>But why stop here? Why not hand over the actual
      legislative power to intelligent machines, which, after all,
      have the ability to take better and more consistent
      decisions? “They” might then legislate that the list of
      professions that should not be accessible to humans ought be
      extended to include also all kinds of drivers and pilots (no
      more Sully, then), doctors, surgeons, accountants, lawyers,
      judges, lawmakers, soldiers, ... you get the drift.</p>
      <p>What will be left for us humans to do? Well, first of all,
      we will hope that the system doesn't go <em>Skynet</em> on
      us<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>,
      but then we will also be faced with the problem of what to do
      with our lives. Will we end up fat and lazy, although
      potentially happy, like the humans in <em>Wall ·
      E</em>&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0027">27</a>]?
      Will we all be rich and bored, and have lives of leisure
      while the machines are taking the sweat&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0034">34</a>]? Will we succumb to
      pessimism like Cioran, whose best-known work&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>] inspired
      the subtitle of this paper?</p>
      <ol class="list-no-style">
        <li id="list53" label="—"><em>What do you do from morning
        to night?</em><br /></li>
        <li id="list54" label="—"><em>I endure
        myself</em>.<br /></li>
      </ol>
      <p>So, what will be left for us to do? Actually, Cioran
      himself (unknowingly) suggests an answer:</p>
      <p><em>A zoologist who observed gorillas in their native
      habitat was amazed by the uniformity of their life and their
      vast idleness. Hours and hours without doing anything. Was
      boredom unknown to them? This is indeed a question raised by
      a human, a busy ape. ... Man alone, in nature, is incapable
      of enduring monotony, man alone wants something to happen at
      all costs — something, anything...</em>&nbsp;<em>[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>]</em></p>
      <p>A common distinguishing trait of us human beings with
      respect to animals, monkeys, apes and primates, and possibly
      with respect to present and future AI, is our desire for
      more, our insatiable curiosity<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>3</sup></a>. We desire what we don't have,
      and in some cases what we can't have, and we strategize and
      make long-term plans to achieve what we desire. In the scene
      at the beginning of this paper, the child C wants to become a
      pilot. C doesn't care that the laws forbid it, C wants to
      fly. C asks “why?”, C is the monkey that challenges the
      status quo, the way things are done around here.</p>
      <p>What can C do then? If it really wishes to become a pilot,
      C will muster up a plan. C will be prepared to lie to achieve
      what it wants. Perhaps inspired by Yentl, the Jewish girl who
      disguises herself as a boy to enter religious training in
      Isaac Bashevis Singer's short story and play <em>Yentl, the
      Yeshiva Boy</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>] as well as in Barbra Streisand's
      eponymous movie&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>] (but see also similar plotlines in
      <em>Tootsie</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>], <em>Albert
      Nobbs</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>] and many more movies, books and
      plays going back to Greek mythology and actually even earlier
      than that).</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Can life
          imitate AI?</h2>
        </div>
      </header>
      <p>C could attempt to disguise itself as an AI to enter the
      “pilot academy” of this futuristic, but not utopian, fully
      autonomous world. Will C be able to pass the
      (to-be-developed) <em>Gnirut Test</em>, a fully reverse
      Turing test that will involve an AI judge and a human subject
      which attempts to appear artificial? The underlying
      presumption here is that an AI subject will always be judged
      artificial, and a human is said to “pass the Gnirut test” if
      it is also judged artificial.</p>
      <p>The idea of a deceptive machine is fundamental to AI
      research and has been present in the AI literature since
      Turing introduced the <em>Imitation Game</em>, so why not
      consider a deceptive human in our plot? Will C be able to
      deceive the AI judge? Difficult, probably impossible, but we
      wish to believe that at least in our Black Mirror episode C
      will stand a chance<a class="fn" href="#fn4" id=
      "foot-fn4"><sup>4</sup></a>.</p>
      <p>Advances in <em>Deception Theory</em> might provide C with
      a range of techniques to fool the judge, especially when
      coupled with advances in the <em>Theory of Mind</em>, which
      investigates the creation of intelligent machines that are
      able to model other agents’ minds. Maybe then C will be able
      not only to pass as an AI but also to lower its
      reaction-decision time to that of an intelligent machine in
      case of an accident to the plane it is piloting. C will
      hopefully be able to leverage on the (to-be-developed)
      <em>Theory of Artificial Mind</em> to plot its disguise in
      what could be an entertaining and, as usual,
      thought-provoking new episode of Black Mirror aptly titled
      <em>Gnirut</em>.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Epilogue</h2>
        </div>
      </header>
      <p>Like most episodes of Black Mirror, our own Gnirut would
      also be discomforting and scary. It would ask a lot of “What
      ifs?” and act as a cautionary tale, putting us in front of
      that black mirror that we can find</p>
      <p><em>on every wall, on every desk, in the palm of every
      hand: the cold, shiny screen of a TV, a monitor, a
      smartphone.</em>&nbsp;<em>[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>]</em></p>
      <p>Should we be scared by the reflection that we see? Of
      course, we should, at least a little. Should we retire Alexa
      to the closet like Botsman tells us that she did after the
      experiment with her daughter&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>]? Our answer is no, we
      should not.</p>
      <p>Rather than halting technological progress, we should make
      sure to accompany it, bringing together, in a
      multi-disciplinary supervision effort, teams of experts
      including AI experts, informaticians, mathematical and social
      scientists, psychologists, lawmakers and more. We should be
      careful who we trust&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>] and, above all, we should not lose
      our faith in expertise&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>]. Only then will the “magnificent and
      progressive fate of the human race”&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0017">17</a>] stand a chance to
      materialize in its full glory.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>We thank Daniele Magazzeni for many interesting
      discussions (and for allowing us to paraphrase a couple of
      his still unpublished sentences).</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Rachel Botsman. 2017.
        Co-Parenting with Alexa. <em><em>The New York
        Times</em></em> Oct. 10 (2017).</li>
        <li id="BibPLXBIB0002" label="[2]">Rachel Botsman. 2017.
        <em><em>Who Can You Trust? (How Techology Brought Us
        Together — and Why It Could Drive Us Aparte)</em></em> .
        Penguin Random House.</li>
        <li id="BibPLXBIB0003" label="[3]">Owen Bowcott. 2017. Rise
        of robotics will upend laws and lead to human job quotas,
        study says. <em><em>The Guardian</em></em> Apr. 4(2017).
        <a class="link-inline force-break" href=
        "https://www.theguardian.com/technology/2017/apr/04/innovation-in-ai-could-see-governments-introduce-human-quotas-study-says."
          target=
          "_blank">https://www.theguardian.com/technology/2017/apr/04/innovation-in-ai-could-see-governments-introduce-human-quotas-study-says.</a>
        </li>
        <li id="BibPLXBIB0004" label="[4]">Jennifer&nbsp;L. Burke,
        Robin&nbsp;R. Murphy, Michael&nbsp;D. Coovert, and
        Dawn&nbsp;L. Riddle. 2004. Moonlight in Miami: Field Study
        of Human-Robot Interaction in the Context of an Urban
        Search and Rescue Disaster Response Training Exercise.
        <em><em>Human-Computer Interaction</em></em> 19, 1-2
        (2004), 85–116.</li>
        <li id="BibPLXBIB0005" label="[5]">Jennifer&nbsp;L. Burke,
        Robin&nbsp;R. Murphy, Erika Rogers, Vladimir&nbsp;J.
        Lumelsky, and Jean Scholtz. 2004. Final report for the
        DARPA/NSF interdisciplinary study on human-robot
        interaction. <em><em>IEEE Trans. Systems, Man, and
        Cybernetics, Part C</em></em> 34, 2 (2004), 103–112.</li>
        <li id="BibPLXBIB0006" label="[6]">James
        Cameron&nbsp;(created by). 1984–. The Terminator Franchise.
        (1984–). <a class="link-inline force-break" href=
        "http://www.imdb.com/title/tt0088247/." target=
        "_blank">http://www.imdb.com/title/tt0088247/.</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Calum Chace. 2016.
        <em><em>The Economic Singularity: Artificial intelligence
        and the death of capitalism</em></em> . Three Cs.</li>
        <li id="BibPLXBIB0008" label="[8]">Emil&nbsp;M. Cioran.
        1973. <em><em>De l'inconvénient d’être né’ (“The Trouble
        With Being Born”)</em></em> . Gallimard.</li>
        <li id="BibPLXBIB0009" label="[9]">Steve Davis. 2018. Soft
        robots could be the factory workers of the future. <em><em>
          The Conversation</em></em> Jan. 10(2018). <a class=
          "link-inline force-break" href=
          "http://theconversation.com/soft-robots-could-be-the-factory-workers-of-the-future-89885."
          target=
          "_blank">http://theconversation.com/soft-robots-could-be-the-factory-workers-of-the-future-89885.</a>
        </li>
        <li id="BibPLXBIB0010" label="[10]">Philip&nbsp;K. Dick.
        1968. <em><em>Do Androids Dream of Electric
        Sheep?</em></em> Doubleday.</li>
        <li id="BibPLXBIB0011" label="[11]">Clint
        Eastwood&nbsp;(directed by). 2016. Sully. (2016).
          <a class="link-inline force-break" href=
          "http://www.imdb.com/title/tt3263904/." target=
          "_blank">http://www.imdb.com/title/tt3263904/.</a>
        </li>
        <li id="BibPLXBIB0012" label="[12]">Martin Fackler. 2017.
        Six Years After Fukushima, Robots Finally Find Reactors’
        Melted Uranium Fuel. <em><em>The New York Times</em></em>
        Nov. 19 (2017).</li>
        <li id="BibPLXBIB0013" label="[13]">Martin Ford. 2016.
        <em><em>The Rise of the Robots: Technology and the Threat
        of Mass Unemployment</em></em> . Oneworld
        Publications.</li>
        <li id="BibPLXBIB0014" label="[14]">Rodrigo
        García&nbsp;(directed by). 2011. Albert Nobbs. (2011).
        <a class="link-inline force-break" href=
        "http://www.imdb.com/title/tt1602098/?ref_=nm_flmg_act_21."
          target=
          "_blank">http://www.imdb.com/title/tt1602098/?ref_=nm_flmg_act_21.</a>
        </li>
        <li id="BibPLXBIB0015" label="[15]">Jerry Kaplan. 2015.
        <em><em>Humans Need Not Apply: A Guide to Wealth and Work
        in the Age of Artificial Intelligence</em></em> . Yale
        University Press.</li>
        <li id="BibPLXBIB0016" label="[16]">Gerd Leonhard. 2016.
        <em><em>Technology vs. Humanity: The coming clash between
        man and machine</em></em> . Fast Future Publishing.</li>
        <li id="BibPLXBIB0017" label="[17]">Giacomo Leopardi. 1845.
        Wild Broom (XXXIV). In <em><em>The Canti</em></em> . OUP
        Usa.</li>
        <li id="BibPLXBIB0018" label="[18]">Rod Lurie (written
        &amp;&nbsp;directed by). 2000. The Contender. (2000).
        <a class="link-inline force-break" href=
        "http://www.imdb.com/title/tt0208874/." target=
        "_blank">http://www.imdb.com/title/tt0208874/.</a>
        </li>
        <li id="BibPLXBIB0019" label="[19]">Robin&nbsp;R. Murphy
        and Jennifer&nbsp;L. Burke. 2010. The Safe Human-Robot
        Ratio. (2010), 31–49&nbsp;pages.</li>
        <li id="BibPLXBIB0020" label="[20]">Tom Nichols. 2017.
        <em><em>The Death of Expertise: The Campaign Against
        Established Knowledge and Why it Matters</em></em> . OUP
        Usa.</li>
        <li id="BibPLXBIB0021" label="[21]">Laura Niles. 2015.
        First Humanoid Robot In Space Receives NASA Government
        Invention of the Year.
        <em><em>https://www.nasa.gov/mission_pages/station/research/news/invention_of_the_year</em></em>
        Jun. 17 (2015).</li>
        <li id="BibPLXBIB0022" label="[22]">Narcís Palomeras, Arnau
        Carrera, Natàlia Hurtós, George&nbsp;C. Karras,
        Charalampos&nbsp;P. Bechlioulis, Michael Cashmore, Daniele
        Magazzeni, Derek Long, Maria Fox, Kostas&nbsp;J.
        Kyriakopoulos, Petar Kormushev, Joaquim Salvi, and Marc
        Carreras. 2016. Toward persistent autonomous intervention
        in a subsea panel. <em><em>Auton. Robots</em></em> 40, 7
        (2016), 1279–1306.</li>
        <li id="BibPLXBIB0023" label="[23]">Sydney
        Pollack&nbsp;(directed by). 1982. Tootsie. (1982).
          <a class="link-inline force-break" href=
          "http://www.imdb.com/title/tt0084805/?ref_=fn_al_tt_1."
          target=
          "_blank">http://www.imdb.com/title/tt0084805/?ref_=fn_al_tt_1.</a>
        </li>
        <li id="BibPLXBIB0024" label="[24]">Ridley
        Scott&nbsp;(directed by). 1982. Blade Runner. (1982).
        <a class="link-inline force-break" href=
        "http://www.imdb.com/title/tt0083658/." target=
        "_blank">http://www.imdb.com/title/tt0083658/.</a>
        </li>
        <li id="BibPLXBIB0025" label="[25]">Murray Shanahan. 2015.
        <em><em>The Technological Singularity</em></em> . MIT
        PRess.</li>
        <li id="BibPLXBIB0026" label="[26]">Isaac&nbsp;Bashevis
        Singer. 1983. <em><em>Yentl, the Yeshiva Boy</em></em>
        .</li>
        <li id="BibPLXBIB0027" label="[27]">Andrew
        Stanton&nbsp;(directed by). 2008. Wall · E. (2008).
        <a class="link-inline force-break" href=
        "http://www.imdb.com/title/tt0910970/." target=
        "_blank">http://www.imdb.com/title/tt0910970/.</a>
        </li>
        <li id="BibPLXBIB0028" label="[28]">Barbra
        Streisand&nbsp;(directed by). 1983. Yentl. (1983).
          <a class="link-inline force-break" href=
          "http://www.imdb.com/title/tt0086619/." target=
          "_blank">http://www.imdb.com/title/tt0086619/.</a>
        </li>
        <li id="BibPLXBIB0029" label="[29]">Richard Susskind and
        Daniel Susskind. 2017. <em><em>The Future of the
        Professions: How Technology Will Transform the Work of
        Human Experts</em></em> . Oxford University Press.</li>
        <li id="BibPLXBIB0030" label="[30]">Max Tegmark. 2017.
        <em><em>Life 3.0: Being Human in the Age of Artificial
        Intelligence</em></em> . Allen Lane.</li>
        <li id="BibPLXBIB0031" label="[31]">The Guardian — Gadgets.
        2011. Charlie Brooker: the dark side of our gadget
        addiction. <em><em>The Guardian</em></em> Dec. 1(2011).
        <a class="link-inline force-break" href=
        "https://www.theguardian.com/technology/2011/dec/01/charlie-brooker-dark-side-gadget-addiction-black-mirror."
          target=
          "_blank">https://www.theguardian.com/technology/2011/dec/01/charlie-brooker-dark-side-gadget-addiction-black-mirror.</a>
        </li>
        <li id="BibPLXBIB0032" label="[32]">
          <em></em>The UK Autodrive project 2018. (2018). <a class=
          "link-inline force-break" href=
          "http://www.ukautodrive.com." target=
          "_blank">http://www.ukautodrive.com.</a>
        </li>
        <li id="BibPLXBIB0033" label="[33]">Jane Wakefield. 2015.
        Intelligent Machines: The jobs robots will steal first.
        <em><em>BBC News — Technology</em></em> Sep. 14 (2015).
        <a class="link-inline force-break" href=
        "http://www.bbc.co.uk/news/technology-33327659." target=
        "_blank">http://www.bbc.co.uk/news/technology-33327659.</a>
        </li>
        <li id="BibPLXBIB0034" label="[34]">Toby Walsh. 2017. Will
        robots bring about the end of work? <em><em>The
        Guardian</em></em> Oct. 1(2017). <a class=
        "link-inline force-break" href=
        "https://www.theguardian.com/science/political-science/2017/oct/01/will-robots-bring-about-the-end-of-work."
          target=
          "_blank">https://www.theguardian.com/science/political-science/2017/oct/01/will-robots-bring-about-the-end-of-work.</a>
        </li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>We wrote
    “legendary” as, apparently, the experiment was never really
    carried out but was rather inspired in part by the experiments
    of G.R. Stephenson, found in “Cultural acquisition of a
    specific learned response among rhesus monkeys”, as well as
    experiments with chimpanzees conducted by Wolfgang Kohler in
    the 1920s. Over the years, it was pieced together to form the
    urban legend as it now stands, see <a class=
    "link-inline force-break" href=
    "http://www.wisdompills.com/2014/05/28/the-famous-social-experiment-5-monkeys-a-ladder/">
    http://www.wisdompills.com/2014/05/28/the-famous-social-experiment-5-monkeys-a-ladder/</a>
    as well as Jeff Bridges’/President Jackson Evans’ excellent
    rendition in the movie <em>The Contender</em>, written and
    directed by Rod Lurie&nbsp;[<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0018">18</a>].</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>
    <em>Skynet</em> is the fictional AI that serves as the main
    antagonist in the movies of the <em>Terminator</em>
    franchise&nbsp;[<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0006">6</a>]. Skynet came to the logical conclusion
    that all of humanity would attempt to destroy it. In order to
    continue fulfilling its programming mandates of “safeguarding
    the world” and to defend itself against humanity, Skynet
    launched a series of nuclear attacks killing over three billion
    people and gathered a slave labor force from surviving
    humans.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>As Albert
    Einstein said, “I have no special talents, I am only
    passionately curious.”</p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Or maybe the AI
    judge will fall in love with C and help it to go unnoticed like
    Rick Deckard does with Rachel in <em>Blade
    Runner</em>&nbsp;[<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0024">24</a>] (although the original novella by
    Philip K. Dick&nbsp;[<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0010">10</a>] actually followed a slightly different
    storyline).</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3191612">https://doi.org/10.1145/3184558.3191612</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
