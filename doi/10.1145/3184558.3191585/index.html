<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Incremental Matrix Co-factorization for Recommender
  Systems with Implicit Feedback</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191585'>https://doi.org/10.1145/3184558.3191585</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191585'>https://w3id.org/oa/10.1145/3184558.3191585</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Incremental Matrix
          Co-factorization for Recommender Systems with Implicit
          Feedback</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <a href="https://orcid.org/0000-0002-0797-5201" ref=
          "author"><span class="givenName">Susan C.</span>
          <span class="surName">Anyosa</span></a> LIAAD - INESC
          TEC, Porto, Portugal, <a href=
          "mailto:scanyosa@inesctec.pt">scanyosa@inesctec.pt</a>
        </div>
        <div class="author">
          <span class="givenName">João</span> <span class=
          "surName">Vinagre</span> LIAAD - INESC TEC FCUP,
          University of Porto, Porto, Portugal, <a href=
          "mailto:jnsilva@inesctec.pt">jnsilva@inesctec.pt</a>
        </div>
        <div class="author">
          <span class="givenName">Alípio M.</span> <span class=
          "surName">Jorge</span> LIAAD - INESC TEC FCUP, University
          of Porto, Porto, Portugal, <a href=
          "mailto:amjorge@fc.up.pt">amjorge@fc.up.pt</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191585"
        target=
        "_blank">https://doi.org/10.1145/3184558.3191585</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Recommender systems try to predict which items a
        user will prefer. Traditional models for recommendation
        only take into account the user-item interaction, usually
        expressed by explicit ratings. However, in these days, web
        services continuously generate auxiliary data from users
        and items that can be incorporated into the recommendation
        model to improve recommendations. In this work, we propose
        an incremental Matrix Co-factorization model with implicit
        user feedback, considering a real-world data-stream
        scenario. This model can be seen as an extension of the
        conventional Matrix Factorization that includes additional
        dimensions to be decomposed in the common latent factor
        space. We test our proposal against a baseline algorithm
        that relies exclusively on interaction data, using
        prequential evaluation. Our experimental results show a
        significant improvement in the accuracy of recommendations,
        after incorporating an additional dimension in three music
        domain datasets.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Recommender systems;</strong> • <strong>Computing
        methodologies</strong> → <strong>Factorization
        methods;</strong> • <strong>Theory of computation</strong>
        → <strong>Online algorithms;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Recommender Systems; Matrix
          Co-Factorization; Implicit feedback; Incremental
          Learning; Data Streams</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Susan C. Anyosa, João Vinagre, and Alípio M. Jorge. 2018.
          Incremental Matrix Co-factorization for Recommender
          Systems with Implicit Feedback. In <em>WWW '18 Companion:
          The 2018 Web Conference Companion,</em> <em>April 23–27,
          2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em>
          6 Pages. <a href=
          "https://doi.org/10.1145/3184558.3191585" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3191585</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Recommender systems (RS) are well-known for being used on
      e-commerce websites like Amazon<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a>, on video recommendations like
      Youtube<a class="fn" href="#fn2" id=
      "foot-fn2"><sup>2</sup></a> and streamming programs such
      Netflix<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>3</sup></a>. They attempt to predict
      customers responses in order to guide them through the wide
      variety of offered items.</p>
      <p>RS can be built using two fundamentally different
      strategies: content-based filtering and collaborative
      filtering (CF). The first one recommends items considering
      their intrinsic or descriptive information. The second one
      exploits past interactions between users and items to select
      the items that best match each user's preferences.</p>
      <p>Considering the CF approach, there are two main model
      classes: neighborhood-based and matrix factorization. The
      first class relies on relationships between either items
      (item-based) or users (user-based) using a similarity
      measure. K-nearest neighbors (kNN) algorithm is one of the
      most popular in CF and are usually used as a benchmark in
      comparisons. The second class describes both items and users
      in a common latent factor space using matrix decomposition
      methods. Currently, most state-of-the art algorithms adopt MF
      models.</p>
      <p>One popular strategy to make MF models more accurate is to
      use side information, typically available in real-world
      applications. Recent proposals adapt traditional MF to
      incorporate the information of additional dimensions, such as
      users or items features [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>]. Another approach is Matrix
      Co-factorization (MCF) [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>], in which the additional dimensions
      are also factorized onto the same shared latent feature space
      of users and items.</p>
      <p>Another important aspect of recommendation problems is the
      type of feedback, and the rate at which it is generated. In
      RS problems, the user-item interaction can be expressed by
      explicit ratings given by the users, known as explicit
      feedback. However, in real-world scenarios, explicit feedback
      is not always available. In this situation user preferences
      are inferred from implicit feedback, such as purchase
      history, browsing history, search patterns, clicks on items,
      etc. Moreover, this kind of feedback is continuously being
      generated, at potentially fast rates. In the era of big data,
      it is important to take into account that RS users generate
      user feedback continuously, as they interact with items. New
      users, items, and features are added constantly. In this
      setting, it is necessary to learn from data without
      recomputing the models again every time new data is
      available. Instead, we can achieve this considering an
      incremental learning approach and assess the incremental
      modeling with suitable stream-based methodologies [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>].</p>
      <p>To address the situations described before, in this paper,
      we propose the first incremental MCF formulation and
      algorithm for recommendation in presence of implicit
      feedback. Also, we present an empirical comparison of the
      online behavior of MCF with additional features against
      incremental MF without additional features. Using proper
      stream-based evaluation, when compared with MF, our
      incremental MCF implementation shows superior accuracy.</p>
      <p>This paper is structured as follows. In the following
      section we review the related work. In Sec. <a class="sec"
      href="#sec-6">3</a> we review useful preliminaries on MF
      model and notations. In Sec. <a class="sec" href=
      "#sec-7">4</a> we present our incremental MCF formulation and
      algorithm. The experiments and results are reported in Sec.
      <a class="sec" href="#sec-8">5</a>. Finally, we conclude in
      Sec. <a class="sec" href="#sec-15">6</a>.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <p>In this section, we categorize the related work into the
      following topics:</p>
      <p><strong>Additional dimensions.</strong> RS models that
      incorporate content (features) or context information to
      improve predictions are vast in the literature. In the group
      of matrix decomposition, Singh and Gordon [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>] proposed the MCF model
      (also known as Collective Matrix Factorization) using a solid
      theoretical support. Later, an application of the former
      work, was made by Fang and Si [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>]. Also, an extension applied to the
      analysis of social network data was made by Hong et&nbsp;al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0010">10</a>]. Using
      another approach, Li et&nbsp;al. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0016">16</a>] showed that feature data
      can also be incorporated inside a MF model, previously using
      an attribute-based similarity measure between items.
      Additionally, using a linear model approach, Chen et&nbsp;al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>] included
      feature data in a MF model. Domingues et&nbsp;al. [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>] treated new
      dimensions as virtual items which are processed as regular
      items by algorithms such as association rules or neighborhood
      based approaches. In the group of tensor models, a natural
      way to include additional dimensions is considering them as
      modes [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0013">13</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>].</p>
      <p><strong>Incremental learning.</strong> CF systems that
      provide a real-time response to the big data challenges have
      been developed recently [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>]; other proposals consider a
      probabilistic model, such as [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>]. Additionally, MF incremental
      algorithms have been developed by Gemulla et&nbsp;al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>], Pálovics
      et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0017">17</a>],
      Sarwar et&nbsp;al. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>], Takács et&nbsp;al. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>], Vinagre et&nbsp;al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0025">25</a>], Zheng and
      Xie [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0029">29</a>].
      Tensor dynamic implementations were also provided by Kasai
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0014">14</a>], Song
      et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0022">22</a>],
      Zhou et&nbsp;al. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0030">30</a>].</p>
      <p><strong>Implicit feedback.</strong> Some authors studied
      RS algorithms that address implicit user feedback, also known
      as one-class collaborative filtering [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>]. Pilászy et&nbsp;al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>] studied an
      improved MF model using Alternating Least Squares
      optimization, and as an extension, Hidasi and Tikk [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0009">9</a>] developed
      the tensor version. Also, Vinagre et&nbsp;al. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0025">25</a>], ,<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>] worked with one-class
      feedback.</p>
      <p>In this stage of our work we are interested in evaluating
      the recommendation improvement obtained after incorporating
      additional dimensions, for implicit user feedback in a
      stream-based learning setup. For this reason, we focus on
      comparisons with the RAISGD algorithm [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>].</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span>
          Preliminaries</h2>
        </div>
      </header>
      <p>In this section we briefly describe the MF procedure since
      a similar approach is followed for MCF. The notation along
      this work is presented in Table <a class="tbl" href=
      "#tab1">1</a>.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Table of symbols.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;">Notations</td>
              <td style="text-align:center;">Definitions</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>A</strong>, <strong>a</strong>, <em>a</em></td>
              <td style="text-align:center;">Matrix, vector,
              scalar</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              |<strong>a</strong>|</td>
              <td style="text-align:center;">Vector norm
              (ℓ<sup>2</sup>-norm) of <strong>a</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;">||<strong>A</strong>||
              <sub><em>F</em></sub></td>
              <td style="text-align:center;">Frobenius norm of
              <strong>A</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class=
              "tex">$\#(S)$</span></span></td>
              <td style="text-align:center;">Cardinality of set
              <em>S</em></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>MF and MCF models consider an entity-relation schema. For
      example, in a music domain problem, the entities may be
      users, songs and artists. These entities have relations
      between them that can be represented with matrices. In simple
      MF, only the information of the relation user-item is
      considered. This can be represented with the matrix
      <strong>R</strong>. <span class=
      "inline-equation"><span class="tex">$\mathbf {R}\in \mathbb
      {R}^{m\times n}$</span></span> is also known as <em>feedback
      matrix</em>, with <em>m</em> users and <em>n</em> items. Each
      value of <strong>R</strong> is defined by:</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ r_{ui}={\left\lbrace
          \begin{array}{@{}l@{\quad }l@{}}1 &amp; \textrm {if user
          u interacts with item i}, \cr 0 &amp; \textrm
          {otherwise}. \end{array}\right.} \]</span><br />
        </div>
      </div>The MF model decomposes <strong>R</strong> into two
      low-rank matrices <span class="inline-equation"><span class=
      "tex">$\mathbf {A}\in \mathbb {R}^{m\times k}$</span></span>
      and <span class="inline-equation"><span class="tex">$\mathbf
      {B}\in \mathbb {R}^{n\times k}$</span></span> that cover a
      common <em>k</em>-dimensional latent space:
      <strong>R</strong> ≈ <strong>A</strong> <strong>B</strong>
      <sup>T</sup>. Matrix <strong>A</strong> spans the user space,
      while <strong>B</strong> spans the item space. The predicted
      value of the interaction of user <em>u</em> with item
      <em>i</em> is given by the following product:
      <div class="table-responsive" id="Xeq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation}
          \hat{r}_{ui}=\mathbf {a}_{u}\mathbf {b}_{i}^{\textrm
          {T}}=\left[a_{u1},a_{u2},\ldots
          ,a_{uk}\right]\left[b_{i1},b_{i2},\ldots
          ,b_{ik}\right]^{\textrm {T}}. \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>In MF algorithms, we want to find the optimal
      <strong>A</strong> and <strong>B</strong> that minimize the
      <em>L</em> <sup>2</sup>-regularized squared error, given by:
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation}
          \underset{\mathbf {A},\mathbf {B}}{\min } \sum
          _{\left(u,i\right)\in D} \left(1-\hat{r}_{ui}\right)^2+
          \lambda \left(\left|\mathbf
          {a}_{u}\right|^{2}+\left|\mathbf
          {b}_{i}\right|^{2}\right), \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>where <em>D</em> is the set containing each
      (<em>u</em>, <em>i</em>) known pair – i.e. where
      <em>r<sub>ui</sub></em> = 1 –, the error term is <span class=
      "inline-equation"><span class=
      "tex">$1-\hat{r}_{ui}$</span></span> and <em>λ</em> ≥ 0 is
      the regularization parameter. The formula on (<a class="eqn"
      href="#eq1">2</a>) was initially popularized by Simon
      Funk<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>,
      and later clearly detailed by Takács et&nbsp;al. [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>]. The most
      popular learning methods used to solve this optimization
      problem are Alternating Least Squares (ALS) [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>] and Stochastic Gradient
      Descent (SGD). Koren et&nbsp;al. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>] gives an introductory
      description of both methods.
      <p></p>
      <p>In this paper we use SGD, which starts initializing
      <strong>A</strong> and <strong>B</strong> with random values
      from <em>N</em>(<em>μ</em>, <em>σ</em>), where <em>μ</em> = 0
      and small <em>σ</em>. In the batch mode, SGD performs several
      iterations over all the available tuples (<em>u</em>,
      <em>i</em>), until a stopping criterion is met. At each
      iteration, SGD updates <strong>a</strong>
      <sub><em>u</em></sub> and <strong>b</strong>
      <sub><em>i</em></sub> , correcting them in the opposite
      direction of the gradient of (<a class="eqn" href=
      "#eq1">2</a>) by a factor of the learn rate <em>η</em> &lt;
      1. For each (<em>u</em>, <em>i</em>) known pair, the
      following updates are performed:</p>
      <div class="table-responsive" id="Xeq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          \mathbf {a}_{u}\leftarrow \mathbf {a}_{u}+\eta
          \left[\left(1-\hat{r}_{ui}\right)\mathbf {b}_{i}-\lambda
          \mathbf {a}_{u}\right]\\ \mathbf {b}_{i}\leftarrow
          \mathbf {b}_{i}+\eta
          \left[\left(1-\hat{r}_{ui}\right)\mathbf {a}_{u}-\lambda
          \mathbf {b}_{i}\right] \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>
      <p></p>
      <p>An incremental version of SGD (ISGD) is proposed by
      Vinagre et&nbsp;al. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>].</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Incremental
          Matrix Co-Factorization</h2>
        </div>
      </header>
      <p>Following the example of music domain in Sec. <a class=
      "sec" href="#sec-6">3</a>, in MCF, we use extra information,
      considering also the song-artist relation. The idea is to
      jointly factorize users, items, user features and item
      features. For each additional relation user-/item-feature, a
      new matrix is added alongside the feedback matrix. For
      simplicity, in the following definitions, we considered two
      added matrices; however, the extension to more than two can
      be easily done.</p>
      <p>Let <span class="inline-equation"><span class=
      "tex">$\mathbf {S}\in \mathbb {R}^{m\times p}$</span></span>
      be a user-feature matrix, with <em>m</em> users and
      <em>p</em> possible values for the feature, where:</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ s_{ug}={\left\lbrace
          \begin{array}{@{}l@{\quad }l@{}}1 &amp; \textrm {if user
          u has the feature value g}, \cr 0 &amp; \textrm
          {otherwise}. \end{array}\right.} \]</span><br />
        </div>
      </div>Let <span class="inline-equation"><span class=
      "tex">$\mathbf {T}\in \mathbb {R}^{n\times q}$</span></span>
      be an item-feature matrix, with <em>n</em> items and
      <em>q</em> possible values for the feature, where:
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ t_{ih}={\left\lbrace
          \begin{array}{@{}l@{\quad }l@{}}1 &amp; \textrm {if item
          i has the feature value h}, \cr 0 &amp; \textrm
          {otherwise}. \end{array}\right.} \]</span><br />
        </div>
      </div>In Sec. <a class="sec" href="#sec-6">3</a>, we saw that
      <strong>R</strong> ≈ <strong>A</strong> <strong>B</strong>
      <sup>T</sup>. In MCF, we decompose <strong>S</strong> into
      two low-rank matrices <span class=
      "inline-equation"><span class="tex">$\mathbf {A}\in \mathbb
      {R}^{m\times k}$</span></span> and <span class=
      "inline-equation"><span class="tex">$\mathbf {X}\in \mathbb
      {R}^{p\times k}$</span></span> as well. <strong>T</strong> is
      decomposed into two low-rank matrices <span class=
      "inline-equation"><span class="tex">$\mathbf {B}\in \mathbb
      {R}^{n\times k}$</span></span> and <span class=
      "inline-equation"><span class="tex">$\mathbf {Y}\in \mathbb
      {R}^{q\times k}$</span></span> . Four matrices
      <strong>A</strong> (user matrix), <strong>B</strong> (item
      matrix), <strong>X</strong> (users’ feature matrix) and
      <strong>Y</strong> (items’ feature matrix) cover the common
      <em>k</em>-dimensional latent space. We have that
      <strong>S</strong> ≈ <strong>A</strong> <strong>X</strong>
      <sup>T</sup> and <strong>T</strong> ≈ <strong>B</strong>
      <strong>Y</strong> <sup>T</sup>. We want to find the optimal
      <strong>A</strong>, <strong>B</strong>, <strong>X</strong>
      and <strong>Y</strong> that minimize:
      <div class="table-responsive" id="eq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{aligned}
          \underset{\mathbf {A},\mathbf {B},\mathbf {X},\mathbf
          {Y}}{\min } \sum _{\left(u,i\right)\in D} &amp; \Biggl
          [\omega _{1}\left(1-\hat{r}_{ui}\right)^2+ \frac{\omega
          _{2}}{\#(G_{u})}\left|\mathbf {1}-\mathbf {a}_{u}\mathbf
          {X}_{u}^{\textrm {T}}\right|^2+ \frac{\omega
          _{3}}{\#(H_{i})}\left|\mathbf {1}-\mathbf {b}_{i}\mathbf
          {Y}_{i}^{\textrm {T}}\right|^2\\ &amp; + \lambda
          \left(\left|\mathbf {a}_{u}\right|^{2}+\left|\mathbf
          {b}_{i}\right|^{2}+
          \frac{1}{\#(G_{u})}\left|\left|\mathbf
          {X}_{u}\right|\right|^{2}_{F}+
          \frac{1}{\#(H_{i})}\left|\left|\mathbf
          {Y}_{i}\right|\right|^{2}_{F}\right)\Biggr ],
          \end{aligned} \end{equation}</span><br />
          <span class="equation-number">(4)</span>
        </div>
      </div>where <strong>1</strong> is the unity vector,
      <em>ω</em> <sub>1</sub>, <em>ω</em> <sub>2</sub>, <em>ω</em>
      <sub>3</sub> are weight parameters and the other values are
      the same as considered in Sec. <a class="sec" href=
      "#sec-6">3</a>. For a given user <em>u</em> and item
      <em>i</em>, <em>G<sub>u</sub></em> is the set of feature's
      values that user <em>u</em> has, and <em>H<sub>i</sub></em>
      is the set of feature's values that item <em>i</em> has.
      Also, let <strong>X</strong> <sub><em>u</em></sub> and
      <strong>Y</strong> <sub><em>i</em></sub> be sub-matrices of
      <strong>X</strong> and <strong>Y</strong>. <strong>X</strong>
      <sub><em>u</em></sub> is obtained by selecting the
      <strong>x</strong> <sub><em>g</em></sub> = [<em>x</em>
      <sub><em>g</em>1</sub>, <em>x</em> <sub><em>g</em>2</sub>, …,
      <em>x<sub>gk</sub></em> ] rows of <strong>X</strong>, where
      <em>g</em> ∈ <em>G<sub>u</sub></em> . <strong>Y</strong>
      <sub><em>i</em></sub> is obtained by selecting the
      <strong>y</strong> <sub><em>h</em></sub> = [<em>y</em>
      <sub><em>h</em>1</sub>, <em>y</em> <sub><em>h</em>2</sub>, …,
      <em>y<sub>hk</sub></em> ] rows of <strong>Y</strong>, where
      <em>h</em> ∈ <em>H<sub>i</sub></em> . Similarly to the SGD
      procedure in Sec. <a class="sec" href="#sec-6">3</a>, for
      each (<em>u</em>, <em>i</em>) we follow the gradient of
      (<a class="eqn" href="#eq2">4</a>), using the update
      expressions given below:
      <div class="table-responsive" id="eq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          \begin{aligned} &amp;\mathbf {a}_{u}\leftarrow \mathbf
          {a}_{u}+\eta \left[\omega
          _{1}\left(1-\hat{r}_{ui}\right)\mathbf
          {b}_{i}+\frac{\omega _{2}}{\#(G_{u})} \left(\mathbf
          {1}-\mathbf {a}_{u}\mathbf {X}_{u}^{\textrm
          {T}}\right)\mathbf {X}_{u}-\lambda \mathbf
          {a}_{u}\right]\\ &amp;\mathbf {b}_{i}\leftarrow \mathbf
          {b}_{i}+\eta \left[\omega
          _{1}\left(1-\hat{r}_{ui}\right)\mathbf
          {a}_{u}+\frac{\omega _{3}}{\#(H_{i})} \left(\mathbf
          {1}-\mathbf {b}_{i}\mathbf {Y}_{i}^{\textrm
          {T}}\right)\mathbf {Y}_{i}-\lambda \mathbf
          {b}_{i}\right]\\ &amp;\mathbf {X}_{u}\leftarrow \mathbf
          {X}_{u}+\eta \left[\frac{\omega
          _{2}}{\#(G_{u})}\left(\mathbf {1}-\mathbf {a}_{u}\mathbf
          {X}_{u}^{\textrm {T}}\right)^{\textrm {T}}\mathbf
          {a}_{u}-\frac{\lambda }{\#(G_{u})}\mathbf
          {X}_{u}\right]\\ &amp;\mathbf {Y}_{i}\leftarrow \mathbf
          {Y}_{i}+\eta \left[\frac{\omega
          _{3}}{\#(H_{i})}\left(\mathbf {1}-\mathbf {b}_{i}\mathbf
          {Y}_{i}^{\textrm {T}}\right)^{\textrm {T}}\mathbf
          {b}_{i}-\frac{\lambda }{\#(H_{i})}\mathbf {Y}_{i}\right].
          \end{aligned} \end{split} \end{equation}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>
      <p></p>
      <p>To add more dimensions to the MCF model, we need to
      include the correspondent low-rank matrices in (<a class=
      "eqn" href="#eq2">4</a>) and then obtain the update
      expressions as the ones presented in (<a class="eqn" href=
      "#eq3">5</a>). Details of a batch implementation of MCF under
      ALS optimization can be found in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0006">6</a>]. Also, details about the
      convergence properties of the MCF model are given by
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>].</p>
      <p>Our incremental MCF algorithm using SGD does a single pass
      over the dataset performing the updates given by (<a class=
      "eqn" href="#eq3">5</a>).</p>
      <p>The implicit feedback only provides positives examples,
      which causes global convergence to the positive class and
      loss of accuracy [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>]. To solve the lack of negative
      examples, we use recency-based negative feedback imputation
      proposed by Vinagre et&nbsp;al. [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>]. A queue <em>Q</em> containing all
      the items seen in the stream is included. For each
      (<em>u</em>, <em>i</em>) in the stream, <em>l</em> negative
      examples (<em>u</em>, <em>j</em> <sub>1</sub>), …,
      (<em>u</em>, <em>j<sub>l</sub></em> ) are included for that
      user <em>u</em>, based on the recency of occurrence of the
      items. The <em>j</em> <sub>1</sub>, …, <em>j<sub>l</sub></em>
      items are the <em>l</em> items that are in the tail of
      <em>Q</em>. Every time an item occurs, it is moved to the
      head of the queue. For more details on the method, please
      refer to [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>].</p>
      <p>The proposed algorithm (CORAISGD) is presented in Alg. 1,
      where the functions <span class=
      "inline-equation"><span class="tex">$\mathtt
      {initqueue},\mathtt {dequeue},\mathtt
      {enqueue}$</span></span> and <span class=
      "inline-equation"><span class="tex">$\mathtt
      {remove}$</span></span> perform queue initialization, tail
      removal, head insertion and index-based removal,
      respectively.</p>
      <p><img src=
      "../../../data/deliveryimages.acm.org/10.1145/3200000/3191585/images/www18companion-324-img1.svg"
      class="img-responsive" alt="" longdesc="" /></p>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experiments and
          evaluation</h2>
        </div>
      </header>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Prequential
            evaluation</h3>
          </div>
        </header>
        <p>Considering that CORAISGD performs incremental learning
        over data streams, it is necessary to use a suitable
        evaluation process [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>]. In our experiments, we use the
        prequential evaluation protocol studied by Vinagre
        et&nbsp;al. [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>].</p>
        <p>As each (<em>u</em>, <em>i</em>) pair arrives in the
        stream, we first use it to test the model, and then we use
        it to update the model. We perform the following steps for
        each new (<em>u</em>, <em>i</em>) pair:</p>
        <ol class="list-no-style">
          <li id="list1" label="(1)">If <em>u</em> is a known user,
          use the current model to recommend a list of items to u,
          otherwise go to step 3;<br /></li>
          <li id="list2" label="(2)">Score the recommendation list
          given the observed item <em>i</em>;<br /></li>
          <li id="list3" label="(3)">Update the model with
          (<em>u</em>, <em>i</em>);<br /></li>
          <li id="list4" label="(4)">Proceed to the next
          observation.<br /></li>
        </ol>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span>
            Datasets</h3>
          </div>
        </header>
        <p>All datasets in our experiments belong to the music
        domain and contain information about users, songs, artists
        and timestamp. We chose music domain datasets due to the
        great availability of resources. We sorted the records
        using the timestamp to simulate a real-world dynamic
        environment and feed them into our incremental algorithms.
        Besides the regular dimensions of users and items (songs),
        we took into account one additional dimension concerning
        items: artist. The datasets are:</p>
        <ul class="list-no-style">
          <li id="list5" label="•">Last.fm was obtained from a
          music service website of the same name. The dataset was
          collected by Celma [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0002">2</a>];<br />
          </li>
          <li id="list6" label="•">Palco Principal was obtained
          from a social network of the same name and collected by
          Vinagre et&nbsp;al. [<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0025">25</a>];<br />
          </li>
          <li id="list7" label="•">
            <span class="inline-equation"><span class=
            "tex">$\#$</span></span> nowplaying was obtained from
            the Twitter users’ posts of current listened songs and
            gathered by Zangerle et&nbsp;al. [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href=
            "#BibPLXBIB0028">28</a>].<br />
          </li>
        </ul>
        <p>We worked with a sample from the original datasets.
        Table <a class="tbl" href="#tab2">2</a> presents the number
        of values and the percentage of sparsity.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Dataset description.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">Dataset</td>
                <td style="text-align:center;">Events</td>
                <td style="text-align:center;">Users</td>
                <td style="text-align:center;">Songs</td>
                <td style="text-align:center;">Artists</td>
                <td style="text-align:center;">Sparsity</td>
              </tr>
              <tr>
                <td style="text-align:center;">Last.fm</td>
                <td style="text-align:center;">403,798</td>
                <td style="text-align:center;">280</td>
                <td style="text-align:center;">196,734</td>
                <td style="text-align:center;">21,566</td>
                <td style="text-align:center;">99.27%</td>
              </tr>
              <tr>
                <td style="text-align:center;">Palco Principal</td>
                <td style="text-align:center;">508,705</td>
                <td style="text-align:center;">20,875</td>
                <td style="text-align:center;">25,262</td>
                <td style="text-align:center;">5,163</td>
                <td style="text-align:center;">99.90%</td>
              </tr>
              <tr>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class=
                "tex">$\#$</span></span> nowplaying</td>
                <td style="text-align:center;">444,086</td>
                <td style="text-align:center;">4,131</td>
                <td style="text-align:center;">175,014</td>
                <td style="text-align:center;">36,519</td>
                <td style="text-align:center;">99.94%</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span>
            Experimental setup</h3>
          </div>
        </header>
        <p>As we mentioned in Sec. <a class="sec" href=
        "#sec-5">2</a>, in this work we compared CORAISGD with
        RAISGD [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>]. Both are incremental algorithms
        that use <em>recency-based</em> imputation and deal with
        implicit feedback from users. We tuned the hyper-parameters
        manually using the first 25% of the instances of each
        dataset, selecting those that gave us higher accuracy
        considering the prequential evaluation protocol.
        Afterwards, we used the chosen hyper-parameters to evaluate
        the algorithmic performance in the 100% of the datasets and
        report the results. The experiments were ran on a hp i7-16
        GB desktop using Python 2.7.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.4</span>
            Metrics</h3>
          </div>
        </header>
        <p>We report accuracy using the recall@N measure at
        cut-offs of N ∈ {1, 5, 10, 20}. The recall = 1 if the item
        <em>i</em> is in the first N recommended items, and 0
        otherwise. To obtain the top-N items, we previously score
        all the available items using <span class=
        "inline-equation"><span class=
        "tex">$\hat{r}_{ui}$</span></span> , order them, and select
        those with the highest values. Additionally, we present
        plots of the moving average of recall@20 that is computed
        considering a sliding window of <em>n</em> = 4000, that was
        set for illustrative purposes.</p>
        <p>We also report the update time per tuple processed by
        the algorithm.</p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.5</span> Statistical
            test</h3>
          </div>
        </header>
        <p>Our incremental implementations provide us with a
        sequence of the recall values that represent the learning
        process of the models. As we mentioned before, we can use
        sliding windows to report the mean of the recall
        continuously. Hence, in each window we are going to have a
        sequence of recall ∈ {0, 1} of size <em>n</em> that is
        compatible with the case of the 0-1 loss function described
        by Gama et&nbsp;al. [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>]. Consequently, we use the McNemar
        test in each window, to compare the incremental learning
        performance of RAISG and CORAISGD.</p>
        <p>The test defines that given two algorithms <em>A</em>
        and <em>B</em>, we count the number of times <em>n</em>
        <sub>10</sub> where the prediction of <em>A</em> is correct
        and the prediction of <em>B</em> is wrong and the number of
        times <em>n</em> <sub>01</sub> where we have the opposite
        situation. The statistic of the test is given by:</p>
        <div class="table-responsive" id="Xeq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            M=\frac{\left(n_{10}-n_{01}\right)^{2}}{n_{10}+n_{01}},
            \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$M\sim \chi ^{2}_{1}$</span></span> and the critical
        value for a significance level of <em>α</em> = 0.01 is
        <em>M</em> = 6.635 . As we can see, the computations are
        simple and can be easily implemented in a fully data-stream
        approach.
        <p></p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.6</span>
            Results</h3>
          </div>
        </header>
        <p>The values of global average of recall and update time
        are presented in Table <a class="tbl" href="#tab3">3</a>.
        We can see that CORAISGD has better accuracy than RAISGD in
        the three datasets. The update time is higher for CORAISGD
        for all the cases, but this was expected since it has more
        calculations involved. As we can see, analyzing only
        summary results give us a general idea of which algorithm
        is better, but not further details of the learning process
        as it occurs. However we can achieve it using stream-based
        prequential evaluation [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>].</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Results for RAISGD and CORAISGD.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">Dataset</td>
                <td colspan="2" style="text-align:center;">
                  Last.fm
                  <hr />
                </td>
                <td colspan="2" style="text-align:center;">
                  Palco Principal
                  <hr />
                </td>
                <td colspan="2" style="text-align:center;">
                  #nowplaying
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:center;">Algorithm</td>
                <td style="text-align:center;">RAISGD</td>
                <td style="text-align:center;">CORAISGD</td>
                <td style="text-align:center;">RAISGD</td>
                <td style="text-align:center;">CORAISGD</td>
                <td style="text-align:center;">RAISGD</td>
                <td style="text-align:center;">CORAISGD</td>
              </tr>
              <tr>
                <td style="text-align:center;">recall@1</td>
                <td style="text-align:center;">&lt;0.001</td>
                <td style="text-align:center;">&lt;0.001</td>
                <td style="text-align:center;">0.004</td>
                <td style="text-align:center;">0.010</td>
                <td style="text-align:center;">&lt;0.001</td>
                <td style="text-align:center;">&lt;0.001</td>
              </tr>
              <tr>
                <td style="text-align:center;">recall@5</td>
                <td style="text-align:center;">0.038</td>
                <td style="text-align:center;">0.042</td>
                <td style="text-align:center;">0.171</td>
                <td style="text-align:center;">0.345</td>
                <td style="text-align:center;">0.002</td>
                <td style="text-align:center;">0.008</td>
              </tr>
              <tr>
                <td style="text-align:center;">recall@10</td>
                <td style="text-align:center;">0.057</td>
                <td style="text-align:center;">0.060</td>
                <td style="text-align:center;">0.225</td>
                <td style="text-align:center;">0.463</td>
                <td style="text-align:center;">0.003</td>
                <td style="text-align:center;">0.012</td>
              </tr>
              <tr>
                <td style="text-align:center;">recall@20</td>
                <td style="text-align:center;">0.064</td>
                <td style="text-align:center;">0.075</td>
                <td style="text-align:center;">0.268</td>
                <td style="text-align:center;">0.522</td>
                <td style="text-align:center;">0.005</td>
                <td style="text-align:center;">0.016</td>
              </tr>
              <tr>
                <td style="text-align:center;">Update time
                (ms)</td>
                <td style="text-align:center;">6.481</td>
                <td style="text-align:center;">6.754</td>
                <td style="text-align:center;">2.807</td>
                <td style="text-align:center;">4.574</td>
                <td style="text-align:center;">7.540</td>
                <td style="text-align:center;">8.041</td>
              </tr>
            </tbody>
          </table>
        </div>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3200000/3191585/images/www18companion-324-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">Moving average of recall@20
            of RAISGD and CORAISGD with window size <em>n</em> =
            4000.</span>
          </div>
        </figure>
        <p>In Fig. <a class="fig" href="#fig1">1</a> we show the
        moving average of recall@20, each point is the average
        recall of a sequence of <em>n</em> = 4000. This plot
        confirms the superiority of the proposed algorithm and
        consequently, the usefulness of taking into account
        additional feature data in the modeling. Furthermore, we
        can see how the recall evolves over time. For (b) Palco
        Principal and (c) #nowplaying the improvement is visible;
        however for (a) Last.fm it is not that easy. In such cases,
        we test if, in fact, a statistically significance
        difference between the performance of both algorithms
        exists.</p>
        <p>Hence, we used the McNemar test in each sequence of
        <em>n</em> = 4000. In detail, we found that:</p>
        <ul class="list-no-style">
          <li id="list8" label="•">For Last.fm: In 70.26% of the
          prequential process CORAISGD was better, in 2.06% RAISGD
          was better, and in 27.68% the difference between them was
          not statistically significant.<br /></li>
          <li id="list9" label="•">For Palco Principal: In 100% of
          the prequential process CORAISGD was better.<br /></li>
          <li id="list10" label="•">For #nowplaying: In 79.25% of
          the prequential process CORAISGD was better, in 8.63%
          RAISGD was better, and in 12.12% the difference between
          them was not statistically significant.<br /></li>
        </ul>
        <p>Fig. <a class="fig" href="#fig2">2</a> presents
        illustrations of the outcome of the McNemar test, for each
        window, over time. We can see that CORAISGD was better in
        most of the windows.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3200000/3191585/images/www18companion-324-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">McNemar test's
            results.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusions and
          future work</h2>
        </div>
      </header>
      <p>In this work, we propose an incremental matrix
      co-factorization algorithm for implicit feedback. We assess
      the improvement in the recommendation obtained after
      considering additional feature dimensions in three different
      datasets of the music domain. For these datasets, we present
      the overall, as well as over time results, using prequential
      evaluation. We have found statistically significant
      improvements of our proposed algorithm over the baseline.</p>
      <p>For future work, we will research other comparable
      algorithms such as MAST [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>], that will need to be adapted to the
      recommendation task. Furthermore, we expect to study
      incremental ways of hyper-parameter calibration in order to
      decrease the effort of performing manual tuning.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>The work of Susan Anyosa and João Vinagre is supported by
      the European Regional Development Fund (ERDF) through the
      Operational Programme of Competitiveness and
      Internationalization – COMPETE 2020 – of Portugal 2020 within
      project POCI-01-0145-FEDER-006961, and by national funds
      through the Portuguese Foundation for Science and Technology
      (FCT) as part of project UID/EEA/50014/2013. The work of
      Alípio Jorge is supported by the ERDF through COMPETE 2020
      within project PushNews (POCI-01-0247-FEDER-0024257).</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Robert&nbsp;M. Bell and
        Yehuda Koren. 2007. Scalable collaborative filtering with
        jointly derived neighborhood interpolation weights. In
        <em><em>Data Mining, 2007. ICDM 2007. Seventh IEEE
        International Conference on</em></em> . IEEE, 43–52.</li>
        <li id="BibPLXBIB0002" label="[2]">Òscar Celma. 2010.
        <em><em>Music Recommendation and Discovery - The Long Tail,
        Long Fail, and Long Play in the Digital Music
        Space</em></em> . Springer. I–XVI, 1–194 pages.</li>
        <li id="BibPLXBIB0003" label="[3]">Badrish Chandramouli,
        Justin&nbsp;J Levandoski, Ahmed Eldawy, and Mohamed&nbsp;F
        Mokbel. 2011. StreamRec: a real-time recommender system. In
        <em><em>Proceedings of the 2011 ACM SIGMOD International
        Conference on Management of data</em></em> . ACM,
        1243–1246.</li>
        <li id="BibPLXBIB0004" label="[4]">Tianqi Chen, Zhao Zheng,
        Qiuxia Lu, Weinan Zhang, and Yong Yu. 2011.
        <em><em>Feature-based matrix factorization</em></em> .
        Technical Report. Apex Data &amp; Knowledge Managment Lab,
        Shanghai Jiao Tong University.</li>
        <li id="BibPLXBIB0005" label="[5]">Marcos&nbsp;Aurélio
        Domingues, Alípio&nbsp;Mário Jorge, and Carlos Soares.
        2013. Dimensions as virtual items: Improving the predictive
        ability of top-n recommender systems. <em><em>Information
        Processing &amp; Management</em></em> 49, 3 (2013),
        698–720.</li>
        <li id="BibPLXBIB0006" label="[6]">Yi Fang and Luo Si.
        2011. Matrix Co-factorization for Recommendation with Rich
        Side Information and Implicit Feedback. In
        <em><em>Proceedings of the 2nd International Workshop on
        Information Heterogeneity and Fusion in Recommender
        Systems</em></em> (<em>HetRec ’11</em>). ACM, New York, NY,
        USA, 65–69.</li>
        <li id="BibPLXBIB0007" label="[7]">João Gama, Raquel
        Sebastião, and Pedro&nbsp;Pereira Rodrigues. 2009. Issues
        in evaluation of stream learning algorithms. In
        <em><em>Proceedings of the 15th ACM SIGKDD international
        conference on Knowledge discovery and data mining</em></em>
        . ACM, 329–338.</li>
        <li id="BibPLXBIB0008" label="[8]">Rainer Gemulla, Erik
        Nijkamp, Peter&nbsp;J Haas, and Yannis Sismanis. 2011.
        Large-scale matrix factorization with distributed
        stochastic gradient descent. In <em><em>Proceedings of the
        17th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em> . ACM, 69–77.</li>
        <li id="BibPLXBIB0009" label="[9]">Balázs Hidasi and
        Domonkos Tikk. 2012. Fast ALS-based tensor factorization
        for context-aware recommendation from implicit feedback.
        <em><em>Machine Learning and Knowledge Discovery in
        Databases</em></em> (2012), 67–82.</li>
        <li id="BibPLXBIB0010" label="[10]">Liangjie Hong,
        Aziz&nbsp;S Doumith, and Brian&nbsp;D Davison. 2013.
        Co-factorization machines: modeling user interests and
        predicting individual decisions in twitter. In
        <em><em>Proceedings of the sixth ACM international
        conference on Web search and data mining</em></em> . ACM,
        557–566.</li>
        <li id="BibPLXBIB0011" label="[11]">Yifan Hu, Yehuda Koren,
        and Chris Volinsky. 2008. Collaborative filtering for
        implicit feedback datasets. In <em><em>Data Mining, 2008.
        ICDM’08. Eighth IEEE International Conference on</em></em>
        . Ieee, 263–272.</li>
        <li id="BibPLXBIB0012" label="[12]">Yanxiang Huang, Bin
        Cui, Wenyu Zhang, Jie Jiang, and Ying Xu. 2015. Tencentrec:
        Real-time stream recommendation in practice. In
        <em><em>Proceedings of the 2015 ACM SIGMOD International
        Conference on Management of Data</em></em> . ACM,
        227–238.</li>
        <li id="BibPLXBIB0013" label="[13]">Alexandros Karatzoglou,
        Xavier Amatriain, Linas Baltrunas, and Nuria Oliver. 2010.
        Multiverse recommendation: n-dimensional tensor
        factorization for context-aware collaborative filtering. In
        <em><em>Proceedings of the fourth ACM conference on
        Recommender systems</em></em> . ACM, 79–86.</li>
        <li id="BibPLXBIB0014" label="[14]">Hiroyuki Kasai. 2016.
        Online low-rank tensor subspace tracking from incomplete
        data by CP decomposition using recursive least squares. In
        <em><em>Acoustics, Speech and Signal Processing (ICASSP),
        2016 IEEE International Conference on</em></em> . IEEE,
        2519–2523.</li>
        <li id="BibPLXBIB0015" label="[15]">Yehuda Koren, Robert
        Bell, and Chris Volinsky. 2009. Matrix factorization
        techniques for recommender systems.
        <em><em>Computer</em></em> 42, 8 (2009).</li>
        <li id="BibPLXBIB0016" label="[16]">Fangfang Li, Guandong
        Xu, and Longbing Cao. 2014. Coupled item-based matrix
        factorization. In <em><em>International Conference on Web
        Information Systems Engineering</em></em> . Springer,
        1–14.</li>
        <li id="BibPLXBIB0017" label="[17]">Róbert Pálovics,
        András&nbsp;A. Benczúr, Levente Kocsis, Tamás Kiss, and
        Erzsébet Frigó. 2014. Exploiting temporal influence in
        online recommendation. In <em><em>Eighth ACM Conference on
        Recommender Systems, RecSys ’14, Foster City, Silicon
        Valley, CA, USA - October 06 - 10, 2014</em></em> .
        273–280. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/2645710.2645723" target="_blank">
          https://doi.org/10.1145/2645710.2645723</a>
        </li>
        <li id="BibPLXBIB0018" label="[18]">Rong Pan, Yunhong Zhou,
        Bin Cao, Nathan&nbsp;N Liu, Rajan Lukose, Martin Scholz,
        and Qiang Yang. 2008. One-class collaborative filtering. In
        <em><em>Data Mining, 2008. ICDM’08. Eighth IEEE
        International Conference on</em></em> . IEEE, 502–511.</li>
        <li id="BibPLXBIB0019" label="[19]">István Pilászy, Dávid
        Zibriczky, and Domonkos Tikk. 2010. Fast als-based matrix
        factorization for explicit and implicit feedback datasets.
        In <em><em>Proceedings of the fourth ACM conference on
        Recommender systems</em></em> . ACM, 71–78.</li>
        <li id="BibPLXBIB0020" label="[20]">B.&nbsp;M. Sarwar, G.
        Karypis, J. Konstan, and J. Riedl. 2002. Incremental
        SVD-Based Algorithms for Highly Scalable Recommender
        Systems. In <em><em>Fifth International Conference on
        Computer and Information Technology</em></em> . 27–28.</li>
        <li id="BibPLXBIB0021" label="[21]">Ajit&nbsp;P. Singh and
        Geoffrey&nbsp;J. Gordon. 2008. Relational learning via
        collective matrix factorization. In <em><em>Proceedings of
        the 14th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em> . ACM, 650–658.</li>
        <li id="BibPLXBIB0022" label="[22]">Qingquan Song, Xiao
        Huang, Hancheng Ge, James Caverlee, and Xia Hu. 2017.
        Multi-aspect streaming tensor completion. In
        <em><em>Proceedings of the 23rd ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining</em></em>
        . ACM, 435–443.</li>
        <li id="BibPLXBIB0023" label="[23]">Gábor Takács, István
        Pilászy, Bottyán Németh, and Domonkos Tikk. 2009. Scalable
        collaborative filtering approaches for large recommender
        systems. <em><em>Journal of machine learning
        research</em></em> 10, Mar (2009), 623–656.</li>
        <li id="BibPLXBIB0024" label="[24]">João Vinagre,
        Alípio&nbsp;Mário Jorge, and João Gama. 2014. Evaluation of
        recommender systems in streaming environments. In
        <em><em>Proceedings of the Workshop on Recommender Systems
        Evaluation: Dimensions and Design in conjunction with the
        8th ACM Conference on Recommender Systems (RecSys 2014),
        Foster City, CA, USA, October 10, 2014.</em></em></li>
        <li id="BibPLXBIB0025" label="[25]">João Vinagre,
        Alípio&nbsp;Mário Jorge, and João Gama. 2014. Fast
        incremental matrix factorization for recommendation with
        positive-only feedback. In <em><em>International Conference
        on User Modeling, Adaptation, and Personalization</em></em>
        . Springer, 459–470.</li>
        <li id="BibPLXBIB0026" label="[26]">João Vinagre,
        Alípio&nbsp;Mário Jorge, and João Gama. 2015. Collaborative
        Filtering with Recency-based Negative Feedback. In
        <em><em>Proceedings of the 30th Annual ACM Symposium on
        Applied Computing</em></em> (<em>SAC ’15</em>). ACM, New
        York, NY, USA, 963–965.</li>
        <li id="BibPLXBIB0027" label="[27]">Hongzhi Yin, Bin Cui,
        Xiaofang Zhou, Weiqing Wang, Zi Huang, and Shazia Sadiq.
        2016. Joint modeling of user check-in behaviors for
        real-time point-of-interest recommendation. <em><em>ACM
        Transactions on Information Systems (TOIS)</em></em> 35, 2
        (2016), 11.</li>
        <li id="BibPLXBIB0028" label="[28]">Eva Zangerle, Martin
        Pichl, Wolfgang Gassler, and Günther Specht. 2014. #
        nowplaying music dataset: Extracting listening behavior
        from twitter. In <em><em>Proceedings of the First
        International Workshop on Internet-Scale Multimedia
        Management</em></em> . ACM, 21–26.</li>
        <li id="BibPLXBIB0029" label="[29]">Yu Zheng and Xing Xie.
        2011. Learning travel recommendations from user-generated
        GPS traces. <em><em>ACM Transactions on Intelligent Systems
        and Technology (TIST)</em></em> 2, 1(2011), 2.</li>
        <li id="BibPLXBIB0030" label="[30]">Shuo Zhou,
        Nguyen&nbsp;Xuan Vinh, James Bailey, Yunzhe Jia, and Ian
        Davidson. 2016. Accelerating online cp decompositions for
        higher order tensors. In <em><em>Proceedings of the 22Nd
        ACM SIGKDD International Conference on Knowledge Discovery
        and Data Mining</em></em> . ACM, 1375–1384.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.amazon.com">https://www.amazon.com</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://www.youtube.com">https://www.youtube.com</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://www.netflix.com">https://www.netflix.com</a></p>
    <p id="fn4"><a href=
    "#foot-fn4"><sup>4</sup></a>http://sifter.org/&nbsp;simon/journal/20061211.html</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3191585">https://doi.org/10.1145/3184558.3191585</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
