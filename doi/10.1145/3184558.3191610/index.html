<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Is this the Era of Misinformation yet? Combining Social Bots and Fake News to Deceive the Masses</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Is this the Era of Misinformation yet? Combining Social Bots and Fake News to Deceive the Masses</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author"><a href="https://orcid.org/0000-0003-3117-8189" ref="author"><span class="givenName">Patrick</span>      <span class="surName">Wang</span></a>     Institut Sup&#x00E9;rieur d&#x2019;&#x00C9;lectronique de Paris, 28 rue Notre-Dame des ChampsParis, France 75006, <a href="mailto:patrick.wang@isep.fr">patrick.wang@isep.fr</a>     </div>     <div class="author"><a href="https://orcid.org/0000-0002-2025-2489" ref="author"><span class="givenName">Rafael</span>      <span class="surName">Angarita</span></a>     Institut Sup&#x00E9;rieur d&#x2019;&#x00C9;lectronique de Paris, 28 rue Notre-Dame des ChampsParis, France 75006, <a href="mailto:rafael.angarita@isep.fr">rafael.angarita@isep.fr</a>     </div>     <div class="author">     <span class="givenName">Ilaria</span>      <span class="surName">Renna</span>     Institut Sup&#x00E9;rieur d&#x2019;&#x00C9;lectronique de Paris, 28 rue Notre-Dame des ChampsParis, France 75006, <a href="mailto:ilaria.renna@isep.fr">ilaria.renna@isep.fr</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191610" target="_blank">https://doi.org/10.1145/3184558.3191610</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Social media is an amazing platform for enhancing public exposure. Anyone, even social bots, can reach out to a vast community and expose one&#x0027;s opinion. But what happens when <em>fake news</em> is (un)intentionally spread within a social media? This paper reviews techniques that can be used to fabricate fake news and depicts a scenario where social bots evolve in a fully semantic Web to infest social media with automatically generated deceptive information.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>World Wide Web;</strong> <strong>Social networks;</strong> <em>Internet communications tools;</em> &#x2022;<strong> Human-centered computing </strong>&#x2192; <em>Social content sharing;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Social bots</small>, </span>     <span class="keyword">      <small> Social media</small>, </span>     <span class="keyword">      <small> Deceptive information</small>, </span>     <span class="keyword">      <small> Semantic Web</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Patrick Wang, Rafael Angarita, and Ilaria Renna. 2018. Is this the Era of Misinformation yet? Combining Social Bots and Fake News to Deceive the Masses. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href="https://doi.org/10.1145/3184558.3191610" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191610</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Social media can be defined as a group of Internet-based services built on top of the ideological and technological foundations of allowing users to create and exchange content&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. There is no doubt about how social media impacts each and every aspect of our society. Some of these aspects include: social communication, the creation of friendships and communities, and even professional advertisement. Possibly one of the most disruptive technologies that have emerged in recent times, social media sometimes has a positive impact on society. For instance, Massive Open Online Courses (MOOCs) provide free first-rate education to anyone having an Internet connection. Social media also helps to increase job satisfaction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], foster social relationships&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>], and organize efforts to face the aftermath of disasters&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>].</p>    <p>However, social media is also the scene of three types of abuses: excessive use, malicious use, and unexpected consequences. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] signaled the risks of addiction to social media and &#x201C;Facebook depression&#x201D;; [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] showed the correlation between a user&#x0027;s time spent on social media and her anxiety; [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] warned about the dangers of cyberbullying, cyberstalking, or online harassment; and&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] recently revealed the case of a social activity tracker disclosing the locations of US military camps in war zones. Some scenarios can overlap multiple types of abuses. For example, <em>Tay</em>, a Twitter chatbot, became &#x201C;an evil Hitler-loving, incestuous sex-promoting, Bush did 9/11-proclaiming robot&#x201D; after a single day of existence&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. Malicious users took advantage of Tay&#x0027;s learning algorithm to promote offensive ideas, and the scientists behind Tay were not prepared for such a turn of event. This example is somewhat comical, but social media abuses can have otherwise more dramatic fallouts.</p>    <p>In 2016, the world witnessed the storming of social media by <em>social bots</em> spreading fake news during the US Presidential election&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. In a study presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], researchers collected Twitter data over four weeks preceding the final ballot to estimate the magnitude of this phenomenon<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. Their results showed that social bots were behind 15% of all accounts and produced roughly 19% of all tweets. These figures are worrisome and suggest the role these social bots had in twisting the public debate. Spreading like infectious diseases&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], deceptive information reaches us more and more often. Though techniques exist to prevent the spread of deceptive information, users&#x00A0;&#x2013;&#x00A0;and especially the <em>influential</em> ones&#x00A0;&#x2013;&#x00A0;must think critically when confronted with news on social media. But what would happen if social media were to get so contaminated by fake news that trustworthy information hardly reaches us anymore?</p>    <p>This paper envisions such a scenario. We first define deceptive information, then list existing technologies to fabricate it, and describe how information diffuses in social media. We then present social bots and how they can operate to fabricate and spread deceptive information. We conclude by proposing some actions which could refrain such a scenario from occurring.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Deceptive Information</h2>     </div>    </header>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Fabrications, satire, large-scale hoaxes</h3>     </div>     </header>     <p>The nature of social media allows any user to create and relay content with little third-party filtering or fact-checking. A consequence is that anyone can engage in amateur citizen journalism with the risk of (un)willingly participating in the dissemination of deceptive information. Indeed, a majority of adults get news on social media&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] and research has shown that people exposed to fake news find them to be accurate&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>].</p>     <p>Journalistic deception corresponds to either the act of withholding true information or providing false information&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. This phenomenon is not new: The Sun published in 1865 the <em>Great Moon Hoax</em>, a story about the discovery of life and civilization on the Moon. Serious fabrications, humorous fakes and large-scale hoaxes are the three categories of deceptive information listed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>].</p>     <p>Serious fabrications concern the creation of deceptive information characterized by a relatively small outreach. Examples of distribution channels are the yellow press, tabloids, or online websites&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. Their contents are centralized in a unique location and are still mostly produced by human means, reducing the capacity of serious fabrications to spread rapidly.</p>     <p>Humorous fakes are conveyed online and oftentimes presented in a format reminiscent of mainstream journalism. A popular example is <em>The Onion</em>, which presents itself as <em>America&#x0027;s finest news source</em>. Although they may seem unsettling at first<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, we consider that humorous fakes can sometimes be flagged because of their heavily satirical nature (and sometimes popularity).</p>     <p>Finally, large-scale hoaxes are characterized by coordinated and complex fabrications that disseminated on multiple platforms so as to get relayed by mainstream or social media. As suggested by its designation, this type of deceptive information distinguishes itself from serious fabrications by its operation scale. Indeed, the objective is to quickly reach a large audience and to effectively disseminate false information. A recent case involving multiple channels is the Columbian Chemicals plant explosion hoax that propagated using text messages, Twitter, Facebook, and YouTube.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Creation of deceptive information</h3>     </div>     </header>     <p>Digital information can take the shape of text, audio tracks, pictures, or videos. Lots of popular tools already exist to doctor pictures or videos (e.g., Photoshop or After Effects). In this section, we present state-of-the-art techniques that can be used to fabricate deceptive information and which are lesser known to the public.</p>     <p>Natural language generation is the research field interested in designing computer systems that produce meaningful texts in natural language on a specific topic. Such systems can generate texts using a variety of input sources such as numerical data, corpus, or taxonomies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. However, a difficulty still resides in generating longer texts, as they require more vocabulary and can present more grammatical or semantic flaws. As a result, generating fake news automatically could be feasible but the resulting text might present poorly constructed sentences, thus raising suspicion.</p>     <p>Voice transformation is a technique used to alter the voice of a person. In a survey&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>], the author lists three types of applications: voice conversion, voice modification, and voice morphing. Voice conversion concerns the transformation of the voice of a source speaker to imitate the one of a target speaker. Voice modification and voice morphing are focused on altering the voice of one or more source speakers without specifying any target. Voice conversion presents a risk of fabricating audio discourses, but several techniques can detect such frauds&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>].</p>     <p>Videos also can be fabricated. For example, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>], authors demonstrated the ability to learn lip sync from audio and video footages<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. Face2Face&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] is another technique which transposes in real-time facial expressions from an actor onto faces present in videos<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>. Recently, we also witnessed the emergence of <em>deepfakes</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] which uses deep learning techniques to swap faces in videos and sparked a vivid interest in the pornographic scene. These techniques can open the door to fake speeches, degrading videos of celebrities, or even the alteration of historical footages.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Information diffusion in social networks</h3>     </div>     </header>     <p>Understanding how information diffuses within a social network provides insights on how deceptive information spreads. Information diffusion analysis is concerned with three main objectives&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]: the modeling of information diffusion, the detection of popular topics, and the identification of influential users.</p>     <p>Out of the models presented, we focus on&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] which studied the diffusion of information in Twitter through the lens of internal and external influences. In particular, this study sought to determine if a tweet containing a URL resulted from network diffusion or factors external to the network. Three points in this article are worth mentioning. First, out of a month worth of Twitter data<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>, only 71% of URL mentions were attributed to internal exposure (e.g., commenting, citing, or retweeting a tweet) and 29% were driven by external exposure (e.g., sharing an external link on Twitter). Second, the analysis of information diffusion on Twitter showed that some topics, and in particular Politics, were more prone to external influence. Third, information diffusion usually follows these steps: (1), the network is <em>exposed</em> to external information and gets <em>externally infected</em> whenever a node shares this information; (2), by sharing this external information, external infection causes a burst of internal exposure; (3), internal exposure can lead nodes to relay the information resulting in an internal infection; and (4), external and internal exposures continue to feed the contagion after the initial burst of infection.</p>     <p>In the light of this model, we can better understand how popular topics emerge and how influential users can reach a massive audience. In the case of deceptive information, malicious software agents can be used to generate an initial burst of exposure by relaying deceptive information or participating in discussions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. However, these actions still depend on the fact that deceptive information is generated by humans. In the following sections, we first describe the current limitations to the creation of social bots. Then, we examine the consequences of having social bots generating and spreading credible deceptive information.</p>    </section>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Not so <em>autonomous</em> Social Bots</h2>     </div>    </header>    <p>Social bots are software agents that cohabit with humans in the social media ecosystem. They can interact with other users via social media functionalities; for example, they can tweet, send emails, or engage in conversations using natural language via instant messaging applications. With these functionalities, social bots (or human users) can disseminate deceptive information. However, a social bot can only do what its social media platform and human creator allow it to do. For example, a Twitter social bot can act just as any other human user (re)tweeting or sending direct messages. On the contrary, a Facebook social bot can be, by its creator&#x0027;s choice, prevented from interacting using instant messaging or group posts.</p>    <p>We have seen that social bots can diffuse deceptive information but they still rely on humans to produce meaningful fake news. Before becoming <em>fully</em> autonomous, social bots need to overcome three obstacles. The first issue concerns the automatic creation of social bots for current or new social media platforms. The second issue relates to automatic adaptation to comply with changes in the social media platforms. The third issue concerns the impossibility for social bots to detect and understand content that can be useful for their objectives. We argue that social bots will have a greater impact once these issues are resolved. We name such social bots <em>intelligent social bots</em>. Currently, <em>political</em> and <em>technical</em> barriers exist to prevent their emergence.</p>    <p>     <em>Personal accounts</em> are the sole propriety of human users. The creation of personal accounts is hard to automate. It usually requires a human user to interact with Web-based interfaces and to confirm being human indeed. To enforce this rule, social media implement CAPTCHAs and/or email and SMS verifications that add even more complexity to the process of creating accounts. A social bot could create and use a personal account by simulating the use of a Web browser programmatically. However, it is a time consuming and error-prone process and still must be implemented by a human.</p>    <p>Social media platforms occasionally propose <em>social bots accounts</em> to give social bots the possibility to interact with users in a controlled way. These accounts are found under different names<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a>: <em>apps</em>, <em>business pages</em>, <em>bots</em>, etc. In any case, the creation of such accounts also necessitates passing verification tests similar to the ones mentioned previously. Moreover, there always need to be a human responsible for a social bot account.</p>    <p>     <em>Political &#x0026; legal barriers</em> are imposed by social media platforms or governments to limit the reach of social bots. For example, the Terms of Service of a particular social media platform may forbid social bot accounts from interacting with personal accounts<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>. This is true with Facebook as it does not allow social bots to initiate a conversation with a human user. Another example is the 2016 BOTS Act<a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a> which prohibits the use of software agents to buy online tickets for concerts or sporting events. These political and legal barriers influence important aspects of the <em>technical barriers</em>. A first consequence of the enforcement such political and legal barriers is that social bots own a distinct type of account. A second consequence relates to how social bots can use social media platforms.</p>    <p>     <em>Using social media functionalities</em> requires understanding the semantics of the social media APIs. For instance, consider the development of a social bot for reading emails using Gmail<a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a>. An engineer will have to create a Gmail account, register as a developer to get an API key, and implement a social bot coordinating the following operations: get the list of new emails, extract their identifiers, get each email individually by its identifier, extract and process their contents, and mark each processed email as &#x201C;read&#x201D;. It can be a trivial task for an engineer, but automating the development of these steps can be difficult.</p>    <p>These technical barriers still exist for two reasons. First, human developers are the ones to specify how social media APIs must be used. Second, social bots cannot fully understand the semantics of these APIs yet. As a consequence, humans are still responsible for creating social bots and defining their behaviors. But in a world where the Web is fully semantic, we argue that intelligent social bots can appear, develop themselves, and adapt to changes in their social media platforms. In the following section, we attempt to portray this world and describe how such intelligent social bots can massively spread misinformation during critical political events.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> A possible political overthrow?</h2>     </div>    </header>    <p>In this section, we present the following hypothetical scenario: intelligent social bots massively spread deceptive information to destabilize a government in place and eventually overthrow it. We set the context to an era in which machines would be able to browse a fully semantic Web. We provide a step-by-step description of how this scenario could unfold. This description is also backed up by information diffusion theories and technological progress.</p>    <p>The first step relates to intelligent social bots scanning and searching the Web for information about the current political climate. In parallel, social bots can also learn to create deceptive information using techniques described in Section&#x00A0;<a class="sec" href="#sec-7">2.2</a> and publish it on the Web. This step has a crucial objective that is to select the contents that will be used to disseminate fake news. In this context, implementing opinion mining or sentiment analysis algorithms would allow social bots to recognize and share pieces of information that endorse their claims.</p>    <p>The second step consists in social bots spreading deceptive information on social media. The objective is to create initial bursts of internal exposure in social media platforms by relaying fake news. In Figure&#x00A0;<a class="fig" href="#fig1">1</a>, we illustrate the dynamics of the diffusion of deceptive information in Twitter according to the model presented in Section&#x00A0;<a class="sec" href="#sec-8">2.3</a>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. The choice of Twitter serves as an illustration, but the same development of events could occur in other social media as long as social bots can publicize deceptive information on their platforms. In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], we have seen that Twitter bots represented 14% of active users while generating approximately a fifth of all tweets during the 2016 US Presidential election. Considering the impacts social bots had in this event, these figures are already not negligible. Increasing either one of these two figures could give a boost to the initial burst of internal exposure, but not without side effects. On the one hand, increasing the number of malicious social bots could lead to social bots infecting each other. On the other hand, a suspiciously high publication frequency could give away the fact that the authors are not human. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191610/images/www18companion-349-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Hypothetical scenario of Twitter social bots generating, fetching, and spreading deceptive information based on the dynamics of information diffusion described in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0020">20</a>].</span>     </div>     </figure>    </p>    <p>The objective of the third step is to infect as many human users as possible, i.e., to have them share or comment on a deceptive information created by an intelligent and malicious social bot. This step corresponds to enabling the internal influence mentioned in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. Note that here, the use of natural language processing techniques can be leveraged to interact with human users in an attempt to plant even deeper the seed of deception. For example, social bots could implement opinion mining and sentiment analysis techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] to locate users whose views are in line with the deceptive information being scattered. Moreover, social bots could evaluate, thanks to the social network APIs, the size of the audience of each infected human users. This should allow social bots to specifically target influential human users in the future.</p>    <p>After the initial burst, these three steps could repeat several times more with a lesser and lesser impact. Ultimately, a well-timed strike comparable to the one described in this section could have devastating consequences if the deceptive information conveys hate messages, for example. In a period when society seems more and more divided despite being hyper-connected, when nationalism experiences a renewed interest, the scenario depicted in this section looks quite dreary.</p>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Discussion and Conclusion</h2>     </div>    </header>    <p>The scenario depicted in the previous section is not completely hypothetical. Nowadays, we witness more and more reports of social bots interfering in political or societal debates&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>]. This situation could get worse with breakthroughs in Semantic Web or Natural Language Processing. In a society where all computing devices will be able to communicate with each other, the proliferation of both intelligent social bots and fake news could influence the world in ways we still cannot fathom.</p>    <p>Methods exist to detect social bots, or at least to discriminate between them and humans. In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>], these methods are divided into techniques based on (1) social network information, (2) crowd-sourced information, and (3) behavioral information. Techniques based on social network information rely on community detection algorithms making the assumption that, within a community, social bots are more densely interconnected with each other than humans do. Such techniques were proved to perform poorly because of the assumption not being well grounded. Crowd-sourcing the detection of social bots can be both time-consuming and expensive, without even mentioning the lack of automation. Finally, techniques based on behavioral information attempt to apply supervised learning algorithms to distinguish humans from social bots. When performed on Twitter datasets, these algorithms showed that social bots possess recent accounts, tend to tweet a lot less and retweet a lot more than humans, or simply have longer usernames. This last set of techniques also presents a flaw as social bots can evolve to adopt a human behavior.</p>    <p>Alongside the detection of social bots, fact-checking is a key technique to assess the veracity and correctness of a claim. After all, fake news can be fabricated and spread by both humans and social bots. Given the scale of information diffusion in social media, manual fact-checking is a tedious task. The need for automated tools is imperative. ClaimBuster&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] is a fact-checking system that aims towards that goal. ClaimBuster comprises five elements: (1) a claim monitor that collects data from websites, social media, and broadcast media; (2) a claim spotter that identifies check-worthy factual sentences in a dataset; (3) a claim matcher that finds fact-checks of similar claims on other fact-checking websites such as Politifact; (4) a claim checker that gathers supporting or disproving evidence from the Web; and (5) a fact-check reporter that gathers all the information previously mentioned and displays it to the user. Despite showing promising results, there is still room for improvements in the design of ClaimBuster. Indeed, Hassan et al. noted some differences in the evaluations formulated by human checkers or the claim spotter. Reducing these differences would mean that there will be fewer errors when identifying check-worthy factual sentences.</p>    <p>To conclude, we insist on the importance of planning for both malicious use and unexpected consequences of social media. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>], authors insisted on gathering all stakeholders to think about how to prevent such abuses, and qualified the &#x201C;ignorance, willful or unintentional, [of an agent&#x0027;s eventual behavior to be] itself an ethical lapse, a lapse that is shared [amongst all stakeholders]&#x201D;.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">David Alandete and Daniel Verd&#x00FA;. 2018. Russian interference: How Russian networks worked to boost the far right in Italy | In English | EL PA&#x00CD;S. https://elpais.com/elpais/2018/03/01/inenglish/1519922107_909331.html?id_externo_rsoc=FB_CC. (2018). (Accessed on 03/02/2018).</li>     <li id="BibPLXBIB0002" label="[2]">Federico Alegre, Asmaa Amehraye, and Nicholas Evans. 2013. Spoofing countermeasures to protect automatic speaker verification from voice conversion. In <em>      <em>Acoustics, Speech and Signal Processing, 2013 IEEE International Conf. on</em>     </em>. IEEE, 3068&#x2013;3072.</li>     <li id="BibPLXBIB0003" label="[3]">Hunt Allcott and Matthew Gentzkow. 2017. Social Media and Fake News in the 2016 Election. <em>      <em>Journal of Economic Perspectives</em>     </em>(2017).</li>     <li id="BibPLXBIB0004" label="[4]">Laignee Barron. 2018. U.S. Soldiers are Accidentally Revealing Sensitive Locations by Mapping Their Exercise Routes. Time. http://time.com/5122495/strava-heatmap-military-bases/. (2018). (Accessed on 01/31/2018).</li>     <li id="BibPLXBIB0005" label="[5]">Alessandro Bessi and Emilio Ferrara. 2016. Social bots distort the 2016 US Presidential election online discussion. (2016).</li>     <li id="BibPLXBIB0006" label="[6]">Samantha Cole. 2017. AI-Assisted Fake Porn Is Here and We&#x0027;re All Fucked - Motherboard. https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn. (2017). (Accessed on 02/09/2018).</li>     <li id="BibPLXBIB0007" label="[7]">Deni Elliott and Charles Culver. 1992. Defining and analyzing journalistic deception. <em>      <em>Journal of Mass Media Ethics</em>     </em>7, 2 (1992), 69&#x2013;84.</li>     <li id="BibPLXBIB0008" label="[8]">Nicole&#x00A0;B Ellison, Charles Steinfield, and Cliff Lampe. 2007. The benefits of Facebook friends: Social capital and college students use of online social network sites. <em>      <em>Journal of Computer-Mediated Communication</em>     </em>12, 4 (2007), 1143&#x2013;1168.</li>     <li id="BibPLXBIB0009" label="[9]">Emilio Ferrara <em>et al.</em> 2016. The rise of social bots. <em>      <em>Commun. ACM</em>     </em>59, 7 (2016), 96&#x2013;104.</li>     <li id="BibPLXBIB0010" label="[10]">Jeffrey Gottfried and Elisa Shearer. 2016. <em>      <em>News Use Across Social Medial Platforms 2016</em>     </em>. Pew Research Center.</li>     <li id="BibPLXBIB0011" label="[11]">Erin Griffith. 2018. Pro-Gun Russian Bots Flood Twitter After Parkland Shooting | WIRED. https://www.wired.com/story/pro-gun-russian-bots-flood-twitter-after-parkland-shooting/. (2018). (Accessed on 03/02/2018).</li>     <li id="BibPLXBIB0012" label="[12]">Frances&#x00A0;S. Grodzinsky, Keith&#x00A0;W. Miller, and Marty&#x00A0;J. Wolf. 2008. The ethics of designing artificial agents. <em>      <em>Ethics and Information Technology</em>     </em>10, 2 (2008), 115&#x2013;121. <a class="link-inline force-break"      href="https://doi.org/10.1007/s10676-008-9163-9"      target="_blank">https://doi.org/10.1007/s10676-008-9163-9</a></li>     <li id="BibPLXBIB0013" label="[13]">Adrien Guille, Hakim Hacid, Cecile Favre, and Djamel&#x00A0;A Zighed. 2013. Information diffusion in online social networks: A survey. <em>      <em>ACM Sigmod Record</em>     </em>42, 2 (2013), 17&#x2013;28.</li>     <li id="BibPLXBIB0014" label="[14]">Brittany Hanna, Kerk&#x00A0;F Kee, and Brett&#x00A0;W Robertson. 2017. Positive impacts of social media at work: Job satisfaction, job calling, and Facebook use among co-workers. In <em>      <em>SHS Web of Conf.</em>     </em>, Vol.&#x00A0; 33 EDP Sciences..</li>     <li id="BibPLXBIB0015" label="[15]">Naeemul Hassan <em>et al.</em> 2017. Toward automated fact-checking: Detecting check-worthy factual claims by ClaimBuster. In <em>      <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 1803&#x2013;1812.</li>     <li id="BibPLXBIB0016" label="[16]">Helena Horton. 2015. Microsoft deletes &#x2019;teen girl&#x2019; AI after it became a Hitler-loving sex robot within 24 hours. The Telegraph. http://www.telegraph.co.uk/technology/2016/03/24/microsofts-teen-girl-ai-turns-into-a-hitler-loving-sex-robot-wit/. (2015). Accessed: 2018-01-31.</li>     <li id="BibPLXBIB0017" label="[17]">Ipsos. 2016. Ipsos/BuzzFeed Poll - Fake News. https://www.ipsos.com/en-us/news-polls/ipsosbuzzfeed-poll-fake-news. (2016). (Accessed on 02/06/2018).</li>     <li id="BibPLXBIB0018" label="[18]">Andreas&#x00A0;M Kaplan and Michael Haenlein. 2010. Users of the world, unite! The challenges and opportunities of Social Media. <em>      <em>Business horizons</em>     </em>53, 1 (2010), 59&#x2013;68.</li>     <li id="BibPLXBIB0019" label="[19]">Adam Kucharski. 2016. Post-truth: Study epidemiology of fake news. <em>      <em>Nature</em>     </em>540, 7634 (2016), 525.</li>     <li id="BibPLXBIB0020" label="[20]">Seth&#x00A0;A Myers, Chenguang Zhu, and Jure Leskovec. 2012. Information diffusion and external influence in networks. In <em>      <em>Proceedings of the 18th ACM SIGKDD international conf. on Knowledge discovery and data mining</em>     </em>. ACM, 33&#x2013;41.</li>     <li id="BibPLXBIB0021" label="[21]">Gwenn O&#x0027;Keeffe <em>et al.</em> 2011. The impact of social media on children, adolescents, and families. <em>      <em>Pediatrics</em>     </em>127, 4 (2011), 800&#x2013;804.</li>     <li id="BibPLXBIB0022" label="[22]">Leysia Palen and Amanda&#x00A0;L Hughes. 2018. Social Media in Disaster Communication. In <em>      <em>Handbook of Disaster Research</em>     </em>. Springer, 497&#x2013;518.</li>     <li id="BibPLXBIB0023" label="[23]">Bo Pang, Lillian Lee, <em>et al.</em> 2008. Opinion mining and sentiment analysis. <em>      <em>Foundations and Trends&#x00AE; in Information Retrieval</em>     </em>2, 1&#x2013;2(2008), 1&#x2013;135.</li>     <li id="BibPLXBIB0024" label="[24]">Patrick Perrot, Guido Aversano, and G&#x00E9;rard Chollet. 2007. Voice disguise and automatic detection: review and perspectives. In <em>      <em>Progress in nonlinear speech processing</em>     </em>. Springer, 101&#x2013;117.</li>     <li id="BibPLXBIB0025" label="[25]">Ehud Reiter and Robert Dale. 2000. <em>      <em>Building natural language generation systems</em>     </em>. Cambridge university press.</li>     <li id="BibPLXBIB0026" label="[26]">Victoria&#x00A0;L Rubin, Yimin Chen, and Niall&#x00A0;J Conroy. 2015. Deception detection for news: three types of fakes. <em>      <em>Proceedings of the Association for Information Science and Technology</em>     </em>52, 1 (2015), 1&#x2013;4.</li>     <li id="BibPLXBIB0027" label="[27]">Chengcheng Shao <em>et al.</em> 2017. The spread of fake news by social bots. <em>      <em>arXiv preprint arXiv:1707.07592</em>     </em>(2017).</li>     <li id="BibPLXBIB0028" label="[28]">Yannis Stylianou. 2009. Voice transformation: a survey. In <em>      <em>Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conf. on</em>     </em>. IEEE, 3585&#x2013;3588.</li>     <li id="BibPLXBIB0029" label="[29]">Supasorn Suwajanakorn, Steven&#x00A0;M Seitz, and Ira Kemelmacher-Shlizerman. 2017. Synthesizing Obama: learning lip sync from audio. <em>      <em>ACM Transactions on Graphics</em>     </em>36, 4 (2017), 95.</li>     <li id="BibPLXBIB0030" label="[30]">Justus Thies <em>et al.</em> 2016. Face2face: Real-time face capture and reenactment of rgb videos. In <em>      <em>Computer Vision and Pattern Recognition, 2016 IEEE Conf. on</em>     </em>. IEEE, 2387&#x2013;2395.</li>     <li id="BibPLXBIB0031" label="[31]">Anna Vannucci, Kaitlin&#x00A0;M. Flannery, and Christine&#x00A0;McCauley Ohannessian. 2017. Social media use and anxiety in emerging adults. <em>      <em>Journal of Affective Disorders</em>     </em>207 (2017), 163&#x2013;166.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>This represented approximately 2.8M accounts generating 20M tweets.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>We can also cite <em>Le Garofi</em>, a French satirical news website whose name is a pun to the mainstream journal <em>Le Figaro</em>. The graphical identity of the former seems strongly inspired by the latter.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>See this video clip made by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]: <a class="link-inline force-break" href="https://youtu.be/9Yq67CjDqvw?t=5m41s">https://youtu.be/9Yq67CjDqvw?t=5m41s</a>. </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>See this video clip made by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>]: <a class="link-inline force-break" href="https://youtu.be/ohmajJTcpNk">https://youtu.be/ohmajJTcpNk</a>. </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>After data cleaning, this represented 18,186 different URLs being diffused on Twitter.</p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>See, for example, <a class="link-inline force-break" href="https://developers.facebook.com">https://developers.facebook.com</a>. </p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a>See Facebook terms of service: <a class="link-inline force-break" href="https://www.facebook.com/terms.php">https://www.facebook.com/terms.php</a>. </p>   <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>BOTS Act: <a class="link-inline force-break"     href="https://www.congress.gov/bill/114th-congress/senate-bill/3183">https://www.congress.gov/bill/114th-congress/senate-bill/3183</a>   </p>   <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a><a class="link-inline force-break" href="https://developers.google.com/gmail/api/">https://developers.google.com/gmail/api/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191610">https://doi.org/10.1145/3184558.3191610</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
