<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Coevolutionary Recommendation Model: Mutual Learning
  between Ratings and Reviews</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Coevolutionary Recommendation
          Model: Mutual Learning between Ratings and
          Reviews</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Yichao</span> <span class=
          "surName">Lu</span>, University of Toronto, <a href=
          "mailto:yichao@cs.toronto.edu">yichao@cs.toronto.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Ruihai</span> <span class=
          "surName">Dong</span>, Insight Centre for Data Analytics,
          University College Dublin, <a href=
          "mailto:ruihai.dong@ucd.ie">ruihai.dong@ucd.ie</a>
        </div>
        <div class="author">
          <span class="givenName">Barry</span> <span class=
          "surName">Smyth</span>, Insight Centre for Data
          Analytics, University College Dublin, <a href=
          "mailto:barry.smyth@ucd.ie">barry.smyth@ucd.ie</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186158"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186158</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Collaborative filtering (CF) is a common
        recommendation approach that relies on user-item ratings.
        However, the natural sparsity of user-item rating data can
        be problematic in many domains and settings, limiting the
        ability to generate accurate predictions and effective
        recommendations. Moreover, in some CF approaches latent
        features are often used to represent users and items, which
        can lead to a lack of recommendation transparency and
        explainability. User-generated, customer reviews are now
        commonplace on many websites, providing users with an
        opportunity to convey their experiences and opinions of
        products and services. As such, these reviews have the
        potential to serve as a useful source of recommendation
        data, through capturing valuable sentiment information
        about particular product features. In this paper, we
        present a novel deep learning recommendation model, which
        co-learns user and item information from ratings and
        customer reviews, by optimizing matrix factorization and an
        attention-based GRU network. Using real-world datasets we
        show a significant improvement in recommendation
        performance, compared to a variety of alternatives.
        Furthermore, the approach is useful when it comes to
        assigning intuitive meanings to latent features to improve
        the transparency and explainability of recommender
        systems.</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Yichao Lu, Ruihai Dong, and Barry Smyth. 2018.
          Coevolutionary Recommendation Model: Mutual Learning
          between Ratings and Reviews. In <em>WWW 2018: The 2018
          Web Conference,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France</em>. ACM, New York, NY, USA 10 Pages.
          <a href="https://doi.org/10.1145/3178876.3186158" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186158</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Recommender systems are an essential part of e-commerce
      platforms. They help customers to find what they are looking
      for and have been proven to drive sales and customer loyalty
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>].
      Collaborative Filtering (CF) [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>] is a common recommendation approach
      that has been adopted by many e-commerce sites, from Netflix
      and Amazon to Digg and Zalando. Briefly, CF algorithms rely
      on user-item ratings, either directly [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0031">31</a>] or indirectly (using
      latent factor models) [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>], to make rating predictions and/or
      generate ranked recommendations. However, these approaches
      tend to suffer from the natural sparsity of the user-item
      ratings data; typically each user will only have “rated” a
      small fraction of the available products. Moreover, the
      latent features that are at the core of modern matrix
      factorization approaches [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>] can lead to other problems, such as a
      lack of transparency and explainability.</p>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.1</span> User
            Reviews for Recommendation</h3>
          </div>
        </header>
        <p>The rise of user-generated reviews has introduced a
        novel source of recommendation data. Such reviews are
        plentiful and informative, and contain valuable information
        including the opinion of users on products and product
        features. For example, “After walking and biking all up and
        down the coast of Ambergris Caye, I am still positive that
        Caye Casa has the best restaurants and activities”, tells
        us that the user likes the food in this restaurant, that
        walking and biking are personal interests, and that the
        restaurant is close to the coast of Ambergris Cayes.
        Recently, such user reviews have been ustilized as the
        basis for new types of recommender systems. For example,
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0013">13</a>] proposed
        a method to generate users and products profiles used in
        various recommendation tasks; see also [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0009">9</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0010">10</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0011">11</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0012">12</a>]. And the ubiquitous
        nature of customer reviews makes them an important data
        source used to address the sparsity and transparency issues
        of CF algorithms.</p>
        <p>Such techniques can be used to infer user ratings for
        products and services, and even combined with real ratings
        and more conventional ratings-based techniques, to generate
        improved recommendations; for example see [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0005">5</a>]. One limitation of this
        type of approach is that it treats inferred ratings and
        real ratings as independent types of ratings data,
        combining their associated predictions/recommendations to
        generate final recommendations. Topic modeling methods such
        as Latent Dirichlet Allocation (LDA) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0003">3</a>] provide another way to
        integrate customer reviews into CF algorithms. For example,
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0041">41</a>] proposed
        the method to combine latent feature based CF and
        probabilistic topic modeling to provide an interpretable
        latent structure for users and items; see also [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0008">8</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0026">26</a>]. The
        limitation of this approach is that it treats reviews as
        simple bags-of-words and, as such, ignores important
        sequential information that may help recommendation.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.2</span> Deep
            Learning for Recommendation</h3>
          </div>
        </header>
        <p>Recently, Deep Learning has been adopted by recommender
        systems research, in part because of its ability to handle
        sequential information. For example, [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0039">39</a>] proposed the method to
        treat a song as a set of 599 sequential frames, training a
        convolutional neural network (CNN) to learn its profile for
        the purpose of addressing the so-called cold-start problem
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0033">33</a>] in
        recommendation. A customer review can also be considered as
        a sequence of words. For example, [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>] proposed a method that
        integrates a product description summarized by a trained
        CNN network into probabilistic matrix factorization.
        Compared to topic models, it is capable of catching the
        contextual information of a document.</p>
        <p>In related work, Recurrent Neural Networks (RNN) have
        been applied to various natural language processing tasks
        with great success. For instance, Bidirectional RNN,
        including Bidirectional Long Short-Term Memory (Bi-LSTM)
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0015">15</a>], and
        Bidirectional Gated Recurrent Unit (Bi-GRU) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0046">46</a>], can encode a target
        word with contextual and sequential information (encoding
        not just the target word but also the surrounding words),
        when using sentences, documents as input sequences. RNNs
        are commonly used to capture or summarize the meaning of
        sentences or documents in various tasks such as machine
        translation[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>], sentence summarization [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0032">32</a>] and
        sentiment analysis [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0038">38</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0042">42</a>]. Meanwhile, attention-based
        methods, based on the visual attention mechanism found in
        humans [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>], by learning weight vectors for
        different sub-tasks, are becoming widely used in machine
        translation [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>] and image tracking [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0045">45</a>]. Similar ideas have
        also proven helpful for extracting textual features from
        customer reviews; see for example [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>].</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig1.png"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">High-level architecture of
            mutual learning between reviews and ratings. Textual
            features of user_i and item_j are extracted from their
            review documents by utilizing bidirectional recurrent
            neural networks with topical attention mechanism.
            Latent features are extracted from the matrix
            factorization model. Textural features and latent
            features approximate to each other during
            training.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.3</span> Main
            Contributions</h3>
          </div>
        </header>
        <p>Inspired by the recent success of attention-based models
        and RNNs, in this paper we propose an attention-based
        mechanism to learn representative features from
        user-generated reviews and combine this with a conventional
        matrix factorization recommendation model as shown in
        Figure <a class="fig" href="#fig1">1</a>.</p>
        <p>The contribution of this paper is threefold:</p>
        <ol class="list-no-style">
          <li id="list1" label="(i)">We introduce a novel
          recommendation model called <em>TARMF</em>, which
          utilizes attention-based recurrent neural networks to
          extract topical information from review
          documents.<br /></li>
          <li id="list2" label="(ii)">We demonstrate how textual
          features can be applied to enhance the performance of
          matrix factorization recommendation techniques, and
          propose an optimization algorithm for training our
          <em>TARMF</em> model.<br /></li>
          <li id="list3" label="(iii)">We demonstrate the ability
          of <em>TARMF</em> to achieve superior recommendation
          performance on five publicly available benchmark
          datasets, in comparison to a variety of state-of-the-art
          baseline alternatives.<br /></li>
        </ol>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Latent
            Factor Models in Recommender Systems</h3>
          </div>
        </header>
        <p>The latent factor models are a set of collaborative
        filtering approaches widely used in the literature of
        recommender systems. Through characterizing each user and
        item as a fixed dimension vector, latent factor models
        could learn user preferences and item features from
        observed ratings, and accordingly recommend new items to
        users.</p>
        <p>Among all the different alternatives of latent factor
        models, matrix factorization based methods [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0021">21</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0022">22</a>] are arguably the most
        prevalent ones. Essentially, matrix factorization turns the
        recommendation task into a matrix completion problem. To
        date much of the state-of-the-art recommendation models are
        built upon matrix factorization techniques. For example,
        the Probabilistic Matrix Factorization (PMF) model
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0029">29</a>] is a
        widely adopted framework with reliable performance.</p>
        <p>The problem with pure matrix factorization models is
        that, the number of unobserved ratings scales linearly with
        the product of the number of users and the number of items,
        while the number of known ratings typically scales linearly
        with the number of users. Therefore with the rapid growth
        of user numbers in modern e-commerce platforms, the
        increasing sparsity of the data becomes a critical
        concern.</p>
        <p>One way to solve the sparseness problem is to mine
        useful features from user-generated content, e.g., user
        reviews, movie plots, and item usage instructions. For
        example, [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0008">8</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0026">26</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0041">41</a>] proposed
        to use topic modeling to learn features from review
        documents, based on which matrix factorization techniques
        could gain useful prior knowledge of the distribution of
        parameters.</p>
        <p>In addition, utilizing user-generated content in latent
        factor models helps to improve the interpretability of
        recommendations. In [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>], the authors demonstrate that
        displaying the top-<em>k</em> words of an LDA model could
        yield meaningful word clusters related to distinct topics.
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0047">47</a>] explores
        the effectiveness of explicitly aligning the latent factors
        and review aspects, which results in an explainable model
        that could make reasoning about recommendation choices.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Deep Neural
            Networks in Natural Language Processing</h3>
          </div>
        </header>
        <p>The recent enthusiasm for applying deep learning
        techniques in natural language processing originates from
        the success of learning representative word vectors
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0028">28</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0030">30</a>]. With
        the utilization of meaningful word embedding vectors,
        almost all deep computational frameworks used in the
        literature of computer vision and speech recognition can be
        seamlessly applied to natural language processing.</p>
        <p>Most widely used neural network structures, including
        convolutional neural networks [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>], recurrent neural networks
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0027">27</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0037">37</a>], and
        neural turing machine [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0044">44</a>], have shown promising results in
        various natural language processing benchmarks.
        Specifically, the attention mechanism introduced by
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0002">2</a>] enabled
        neural language models to achieve state-of-the-art results
        in machine translation [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0040">40</a>], reading comprehension [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0016">16</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0034">34</a>], speech
        recognition [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>], etc.</p>
        <p>The success of deep neural networks in a variety of
        natural language processing tasks has raised the attention
        of the recommender systems community as well. For example,
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0019">19</a>] proposed
        to employ a convolutional neural network to facilitate the
        learning of matrix factorization. Similarly, [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0035">35</a>] utilized
        an attention-based convolutional neural network for
        modeling review documents, and achieved state-of-the-art
        results in the task of rating prediction.</p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Topical
          Attention Regularized Matrix Factorization</h2>
        </div>
      </header>
      <p>In this section, we present the detail of our proposed
      model, Topical Attention Regularized Matrix Factorization
      (<em>TARMF</em>). We begin by describing the attention-based
      recurrent neural network architecture we utilized for
      document modeling, followed by the approach of extracting
      textual features from user and item review documents. We then
      introduce an extension of the traditional probabilistic
      matrix factorization model by incorporating textual
      regularization. Finally, we present a computational framework
      for optimizing the parameters.</p>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span>
            Attention-Based Recurrent Neural Network for Document
            Modeling</h3>
          </div>
        </header>
        <p>We employ a bidirectional recurrent neural network with
        attention mechanism to learn representative features from
        review documents. The network architecture consists of four
        primary components: (i) a word embedding layer, (ii) a
        sequence encoding layer, (iii) a topical attention layer,
        and (iv) a feature projection layer; see Figure <a class=
        "fig" href="#fig2">2</a>.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig2.png"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">The attention-based
            bidirectional GRU network for document modeling.</span>
          </div>
        </figure>
        <p></p>
        <section id="sec-13">
          <p><em>3.1.1 Word Embedding Layer.</em> The word
          embedding layer takes as input a sequence of words
          (<em>w</em> <sub>1</sub>, <em>w</em> <sub>2</sub>,
          <em>w</em> <sub>3</sub>, ..., <em>w<sub>T</sub></em> ),
          and maps each word to its respective
          <em>k</em>-dimensional vector representation <span class=
          "inline-equation"><span class="tex">$x_{i} \in \mathcal
          {R}^{k}$</span></span> . The vector representations are
          expected to encode the semantic and syntactic information
          carried by each word, thus enabling the sequence encoding
          layer to effectively capture the contextual dependencies
          of the input sequence. We initialize the word embedding
          layer with pre-trained word vectors obtained from
          <em>word2vec</em> [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0028">28</a>], and then fine-tune it with
          back-propagation.</p>
        </section>
        <section id="sec-14">
          <p><em>3.1.2 Sequence Encoding Layer.</em> The sequence
          encoding layer provides contextual annotations for the
          input sequence. Specifically, we utilize the
          bidirectional GRU architecture proposed by [<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0006">6</a>] due to
          its computational efficiency and robust performance in
          our experiments.</p>
          <p>A Gated Recurrent Unit (GRU) is a popular variant of
          the vanilla recurrent hidden unit. Through utilizing
          gating units to modulate the flow of information, each
          recurrent unit is capable of encapsulating sequential
          dependencies across different time scales.</p>
          <p>Formally speaking, a GRU computes its activation at
          time step <em>t</em> as the linear interpolation between
          the previous activation <em>h</em> <sub><em>t</em> −
          1</sub> and the candidate activation <span class=
          "inline-equation"><span class=
          "tex">$\widetilde{h_{t}}$</span></span> :</p>
          <div class="table-responsive" id="Xeq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} h_{t} = (1 -
              z_{t}) \odot h_{t-1} + z_{t} \odot \widetilde{h_{t}},
              \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>
          <p></p>
          <p>where</p>
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \widetilde{h_{t}} = tanh(W_{h}x_{t} + r_{t} \odot
              (U_{h} h_{t-1}) + b_{h}), \end{equation}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>
          <div class="table-responsive" id="Xeq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} z_{t} =
              \sigma (W_{z}x_{t} + U_{z} h_{t-1} + b_{z}),
              \end{equation}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>
          <div class="table-responsive" id="Xeq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} r_{t} =
              \sigma (W_{r}x_{t} + U_{r} h_{t-1} + b_{z}).
              \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>
          <p></p>
          <p>The <em>update gate z<sub>t</sub></em> decides the
          extent to which past information is superseded by new
          information, while the <em>reset gate r<sub>t</sub></em>
          determines the degree that the previous activation
          contributes to the candidate activation.</p>
          <p>In order for the annotations to summarize the
          information from both the preceding words and the
          following words, we employ a bidirectional GRU consisting
          of forward and backward GRUs. The forward GRU reads the
          input sequence in the usual order, while the backward GRU
          reads the input sequence in the reversed order. The
          activations of the forward GRU and the backward GRU at
          time step <em>t</em> are denoted as <span class=
          "inline-equation"><span class=
          "tex">$\overrightarrow{h_{t}}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\overleftarrow{h_{t}}$</span></span> ,
          respectively. At each time step, we concatenate the
          forward activation and the backward activation to obtain
          the final annotation, i.e., <span class=
          "inline-equation"><span class="tex">$h_{t} = \left[
          \overrightarrow{h_{t}}, \overleftarrow{h_{t}}
          \right]$</span></span> .</p>
        </section>
        <section id="sec-15">
          <p><em>3.1.3 Topical Attention Layer.</em> The topical
          attention layer extracts topic-related information
          associated with the set of topics of interest to the
          recommendation task. We assume that not all parts of a
          document are equally relevant to a specific topic.
          Therefore we introduce the attention mechanism to capture
          the relative importance between different words.</p>
          <p>Suppose that each user and item can be characterized
          by the corresponding <em>K</em>-dimensional latent factor
          vector, each latent dimension is expected to represent a
          topic related to the specific user or item. Intuitively,
          the distribution of attention weights on each word should
          be different for each topic. Thus we employ <em>K</em>
          distinct attention modules corresponding to the
          <em>K</em> topics.</p>
          <p>Consider, for example, the <em>k</em>-th attention
          module. Given the sequence of word annotations
          (<em>h</em> <sub>1</sub>, <em>h</em> <sub>2</sub>,
          <em>h</em> <sub>3</sub>, ..., <em>h<sub>T</sub></em> ),
          the attention module first transforms each word
          annotation through a single layer perceptron with the
          <em>tanh</em> activation function:</p>
          <div class="table-responsive" id="Xeq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} s_{t}^{k} =
              tanh(W_{s}^{k}h_{t} + b_{s}^{k}).
              \end{equation}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>
          <p></p>
          <p>Then the attention module compares the similarities
          between a context vector <em>z<sub>k</sub></em> and the
          transformed annotations by computing the dot products,
          and assigns each annotation a weighting score with the
          <strong>softmax</strong> function:</p>
          <div class="table-responsive" id="Xeq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} a_{t}^{k} =
              \frac{z_{k} \cdot s_{t}^{k}}{\sum _{t=1}^{T}{z_{k}
              \cdot s_{t}^{k}}}. \end{equation}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>
          <p></p>
          <p>Finally, the attention module computes its output
          <span class="inline-equation"><span class=
          "tex">$\hat{h}_{k}$</span></span> as the weighted sum of
          the annotations:</p>
          <div class="table-responsive" id="Xeq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \hat{h}_{k}
              = \sum _{t=1}^{T}a_{t}^{k}h_{t}.
              \end{equation}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>
          <p></p>
          <p>The output of each individual attention module is
          passed together to the feature projection layer as the
          activation of the topical attention layer.</p>
        </section>
        <section id="sec-16">
          <p><em>3.1.4 Feature Projection Layer.</em> The feature
          projection layer performs non-linear transformations on
          the feature representations generated by the penultimate
          layer. We employ a single layer perceptron with
          <em>tanh</em> as its activation function. The activation
          for the <em>k</em>-th attention module is thus
          transformed as:</p>
          <div class="table-responsive" id="Xeq8">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} c_{k}=
              tanh(W_{c}^{k}\hat{h}_{k} + b_{c}^{k}).
              \end{equation}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>
          <p></p>
          <p>The feature projection layer concatenates the
          transformed activations, and outputs it as the latent
          document representation, i.e., <em>c</em> = [<em>c</em>
          <sub>1</sub>, <em>c</em> <sub>2</sub>, <em>c</em>
          <sub>3</sub>, ..., <em>c<sub>K</sub></em> ].</p>
        </section>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Extracting
            Textual Features from Review Documents</h3>
          </div>
        </header>
        <p>We assume that the textual features extracted from
        review documents can serve as a reasonable indicator of the
        user and item latent factor vectors. To begin with, we need
        to define the concept of a <em>review document</em>. We
        define the user review document <em>d</em> <sub><em>u</em>,
        <em>i</em></sub> as the set of all reviews written by user
        <em>i</em>. Similarly, the item review document <em>d</em>
        <sub><em>v</em>, <em>j</em></sub> is defined to be the
        collection of reviews written on item <em>j</em>.</p>
        <p>Note that the review written by user <em>i</em> on item
        <em>j</em> would be included both in the user review
        document <em>d</em> <sub><em>u</em>, <em>i</em></sub> and
        the item review document <em>d</em> <sub><em>v</em>,
        <em>j</em></sub> . However, the same review should be
        treated differently in these circumstances. For reviews in
        the user review document, we expect to learn user
        preferences revealed in the content. When it comes to the
        reviews in the item review document, our aim is to extract
        the features related to the specific item. Due to the
        inherent difference of the user review document and the
        item review document, they are modeled by two
        attention-based recurrent neural networks with the same
        architecture and different parameters. The attention-based
        recurrent neural networks for modeling user review
        documents and item review documents are named as the user
        attention network and the item attention network,
        respectively.</p>
        <p>Given a review document, we first generate the latent
        document representation for each individual review with the
        attention-based recurrent neural network, and then average
        them as the textual features extracted from the review
        document.</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Textual
            Regularized Matrix Factorization</h3>
          </div>
        </header>
        <p>We extend the Probabilistic Matrix Factorization (PMF)
        model [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0029">29</a>] by introducing textual
        regularization in the user and item latent factor
        vectors.</p>
        <p>Suppose we have <em>N</em> users and <em>M</em> items,
        matrix factorization finds a user coefficient matrix
        <span class="inline-equation"><span class="tex">$U \in
        \mathcal {R}^{D \times N}$</span></span> and an item factor
        matrix <span class="inline-equation"><span class="tex">$V
        \in \mathcal {R}^{D \times M}$</span></span> whose product
        <span class="inline-equation"><span class="tex">$\hat{R} =
        U^{T}V$</span></span> approximates the rating matrix
        <span class="inline-equation"><span class="tex">$R \in
        \mathcal {R}^{N \times M}$</span></span> . The column
        vectors <em>u<sub>i</sub></em> and <em>v<sub>j</sub></em>
        are the <em>D</em>-dimensional distributed representations
        for user <em>i</em> and item <em>j</em>, respectively. For
        a linear model with Gaussian observation noise, the
        conditional distribution over the observed ratings can be
        defined as in Equation <a class="eqn" href="#eq1">9</a>,
        where <span class="inline-equation"><span class=
        "tex">$\mathcal {N}(\mu , \sigma ^{2})$</span></span> is
        the probability density function of the Gaussian
        distribution with mean <em>μ</em> and variance <em>σ</em>
        <sup>2</sup>, and <em>I<sub>ij</sub></em> is an indicator
        function where <em>I<sub>ij</sub></em> = 1 if user
        <em>i</em> rated item <em>j</em> and
        <em>I<sub>ij</sub></em> = 0 otherwise.</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(R \mid U, V,
            \sigma ^{2}) = \prod \limits _{i=1}^{N}\prod \limits
            _{j=1}^{M} \bigg [\mathcal {N} (R_{ij} \mid u_{i}^{T}
            \cdot v_{j}, \sigma ^{2})\bigg ] ^{I_{ij}}
            \end{equation}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <p>Unlike the traditional probabilistic matrix
        factorization model, which places zero-mean isotropic
        Gaussian prior distributions on all the latent variables,
        the prior means of the user and item latent factors in the
        <em>TARMF</em> model are not fixed at zero. Instead, we
        assume that the user and item latent factors are highly
        correlated with the textual features extracted from the
        review documents. Therefore, we define the prior
        distributions of the user and item latent factor vectors
        as:</p>
        <div class="table-responsive" id="Xeq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(U \vert
            \widetilde{U}, \sigma _{U}^{2}) = \prod
            _{i}^{N}\mathcal {N}(U_{i} \vert \widetilde{U}_{i},
            \sigma _{U}^{2}I), \end{equation}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>
        <div class="table-responsive" id="Xeq10">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(V \vert
            \widetilde{V}, \sigma _{V}^{2}) = \prod
            _{j}^{M}\mathcal {N}(V_{j} \vert \widetilde{V}_{j},
            \sigma _{V}^{2}I), \end{equation}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>
        <p></p>
        <p>where <span class="inline-equation"><span class=
        "tex">$\widetilde{U}_{i}$</span></span> and <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{V}_{j}$</span></span> are the textual
        features extracted from the review documents of user
        <em>i</em> and item <em>j</em>, as described in Section
        <a class="sec" href="#sec-17">3.2</a>.</p>
        <p>The introduction of the textual features in the prior
        distributions essentially regularizes the matrix
        factorization model so that it could generalize well on the
        unseen test dataset.</p>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span>
            Optimization Methodology</h3>
          </div>
        </header>
        <p>Training the <em>TARMF</em> model involves optimizing
        the following unknown parameters: (i) the user coefficient
        matrix <em>U</em>, (ii) the item factor matrix <em>V</em>,
        (iii) the parameters <em>W<sub>U</sub></em> in the user
        attention network, and (iv) the parameters
        <em>W<sub>V</sub></em> in the item attention network.</p>
        <p>While the optimization objective of <em>U</em> and
        <em>V</em> is straightforward, i.e., to minimize the
        difference between the rating matrix <em>R</em> and the
        product of <em>U</em> and <em>V</em>, the optimization
        criterion of <em>W<sub>U</sub></em> and
        <em>W<sub>V</sub></em> is still unclear. Since we expect
        the textual features to serve as reliable indicators of the
        latent factor vectors, <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{U}_{i}$</span></span> and <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{V}_{j}$</span></span> should approximate
        <em>U<sub>i</sub></em> and <em>V<sub>j</sub></em> . We
        therefore optimize <em>W<sub>U</sub></em> and
        <em>W<sub>V</sub></em> through maximizing <span class=
        "inline-equation"><span class="tex">$sim(U,
        \widetilde{U})$</span></span> and <span class=
        "inline-equation"><span class="tex">$sim(V,
        \widetilde{V})$</span></span> , where the <em>sim</em>
        function measures the similarity between two matrices.</p>
        <p>An intuitive optimization strategy is to define an
        overall loss function, and to train all the parameters
        simultaneously with stochastic gradient descent.
        Nevertheless, due to the high correlation between the
        parameters, the stochastic gradient descent algorithm could
        easily get trapped in one of the undesired local minima.
        Consider, for example, the user coefficient matrix
        <em>U</em> and the parameters <em>W<sub>U</sub></em> of the
        user attention network. On one hand, <em>U</em> is
        dependent upon <em>W<sub>U</sub></em> as changes in
        <em>W<sub>U</sub></em> would affect the textual features
        <span class="inline-equation"><span class=
        "tex">$\widetilde{U}$</span></span> generated by the user
        attention network, and thus altering the posterior
        distribution of <em>U</em>. On the other hand,
        <em>W<sub>U</sub></em> is optimized through maximizing the
        similarity between <em>U</em> and <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{U}$</span></span> . Thus, when <em>U</em>
        and <em>W<sub>U</sub></em> are jointly optimized, they are
        likely to mislead each other which results in the
        deterioration of model performance. The same situation
        holds for the optimization of <em>V</em> and
        <em>W<sub>V</sub></em> as well.</p>
        <p>Instead of jointly optimizing all the unknown
        parameters, we adopt an alternative approach, which
        iteratively updates each of the four sets of parameters in
        a specific order. When the model is optimizing a particular
        set of parameters, we temporarily fix all the remaining
        parameters to be constant. Our rationale is that, such
        iterative training methodology can help to alleviate the
        dependency between the parameters, and accordingly
        facilitate the training process.</p>
        <p>Suppose that the optimal <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{U}$</span></span> and <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{V}$</span></span> are known and fixed,
        the posterior distribution over <em>U</em> and <em>V</em>
        is given by</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \begin{split}
            &amp; \max \limits _{U, V} p(U, V \vert R,
            \widetilde{U}, \widetilde{V}, \sigma ^{2}, \sigma
            _{U}^{2}, \sigma _{V}^{2}) \\ = &amp; \max \limits _{U,
            V} p(R \vert U, V, \sigma ^{2}) p(U, V \vert
            \widetilde{U}, \widetilde{V}, \sigma _{U}^{2}, \sigma
            _{V}^{2}), \end{split} \end{align}</span><br />
            <span class="equation-number">(12)</span>
          </div>
        </div>where
        <div class="table-responsive" id="Xeq11">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(U, V \vert
            \widetilde{U}, \widetilde{V}, \sigma _{U}^{2}, \sigma
            _{V}^{2}) = p(U \vert \widetilde{U}, \sigma _{U}^{2})
            p(V \vert \widetilde{V}, \sigma _{V}^{2})
            \end{equation}</span><br />
            <span class="equation-number">(13)</span>
          </div>
        </div>is the joint posterior distribution of <em>U</em> and
        <em>V</em> given <span class="inline-equation"><span class=
        "tex">$\widetilde{U}$</span></span> , <span class=
        "inline-equation"><span class=
        "tex">$\widetilde{V}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\sigma
        _{U}^{2}$</span></span> , and <span class=
        "inline-equation"><span class="tex">$\sigma
        _{V}^{2}$</span></span> .
        <p></p>
        <p>Maximizing the posterior probability is equivalent to
        minimizing its negative logarithm, which is given by</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \begin{split}
            \mathcal {L}(U, V \vert R, \widetilde{U},
            \widetilde{V}) &amp;= \frac{1}{2}\sum _{i}^{N}\sum
            _{j}^{M}{I_{ij}(R_{ij} - U_{i}^{T}V_{j})^{2}} \\ &amp;+
            \frac{\lambda _{U}}{2}\sum _{i}^{N}{\Vert U -
            \widetilde{U} \Vert _{F}^{2}} + \frac{\lambda
            _{V}}{2}\sum _{i}^{N}{\Vert V - \widetilde{V} \Vert
            _{F}^{2}}, \end{split} \end{align}</span><br />
            <span class="equation-number">(14)</span>
          </div>
        </div>
        <p></p>
        <p>where <span class="inline-equation"><span class=
        "tex">$\lambda _{U} = \sigma ^{2} / \sigma
        _{U}^{2}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\lambda _{V} = \sigma
        ^{2} / \sigma _{V}^{2}$</span></span> , and ‖ · ‖
        <sub><em>F</em></sub> denotes the Frobenius norm.</p>
        <p>Note that Equation <a class="eqn" href="#eq3">14</a>
        becomes a quadratic function with respect to <em>U</em> (or
        <em>V</em>) when <em>V</em> (or <em>U</em>) is treated as
        constant, which implies that the equation reaches its
        optimal solution when the gradient of <em>U</em> (or
        <em>V</em>) equals zero. Therefore we adopt the alternating
        least squares technique which repeatedly optimizes one of
        <em>U</em> and <em>V</em> while temporarily fixing the
        other to be constant:</p>
        <div class="table-responsive" id="Xeq12">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} U_{i} =
            (VI_{i}V^{T} + \lambda _{U}I_{K})^{-1}(VR_{i} + \lambda
            _{U}\widetilde{U}_{i}), \end{equation}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>
        <div class="table-responsive" id="Xeq13">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} V_{j} =
            (UI_{j}U^{T} + \lambda _{V}I_{K})^{-1}(UR_{j} + \lambda
            _{V}\widetilde{V}_{j}), \end{equation}</span><br />
            <span class="equation-number">(16)</span>
          </div>
        </div>
        <p></p>
        <p>where <span class="inline-equation"><span class=
        "tex">$I_{i} \in \mathcal {R}^{M \times M}$</span></span>
        is a diagonal matrix with <em>I<sub>ij</sub></em> as its
        diagonal elements, and <span class=
        "inline-equation"><span class="tex">$R_{i} \in \mathcal
        {R}^{M}$</span></span> is a vector of
        <em>R<sub>ij</sub></em> . Recall that
        <em>I<sub>ij</sub></em> = <em>R<sub>ij</sub></em> = 0 if
        user <em>i</em> has not yet rated item <em>j</em>.
        <em>I<sub>j</sub></em> and <em>R<sub>j</sub></em> are
        defined in an analogous manner.</p>
        <p>Conversely, consider the circumstance in which the
        optimal <em>U</em> and <em>V</em> are known a priori. The
        goal of the user and item attention network is then to
        adjust their internal weights <em>W<sub>U</sub></em> and
        <em>W<sub>V</sub></em> so that the textual features they
        extract could approximate the ideal <em>U</em> and
        <em>V</em>. For a given user <em>i</em> with user review
        document <em>X</em> <sub><em>u</em>, <em>i</em></sub> and
        user latent factor <em>U<sub>i</sub></em> , we can define
        the loss function for <em>W<sub>U</sub></em> as</p>
        <div class="table-responsive" id="Xeq14">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {L_{W_{U}}}(X_{u, i}, U_{i}) = \Vert UAN(W_{U}, X_{u,
            i}) - U_{i} \Vert _{F}^{2}, \end{equation}</span><br />
            <span class="equation-number">(17)</span>
          </div>
        </div>
        <p></p>
        <p>where <span class="inline-equation"><span class=
        "tex">$\widetilde{U_{i}} = UAN(W_{U}, X_{u,
        i})$</span></span> denotes the textual features for user
        <em>i</em> generated by feeding the user review document
        <em>X</em> <sub><em>u</em>, <em>i</em></sub> into the user
        attention network with parameters <em>W<sub>U</sub></em>
        .</p>
        <p>The loss function for <em>W<sub>V</sub></em> can be
        similarly defined as follows:</p>
        <div class="table-responsive" id="Xeq15">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {L_{W_{V}}}(X_{v, j}, V_{j}) = \Vert IAN(X_{v, j}) -
            V_{j} \Vert _{F}^{2}, \end{equation}</span><br />
            <span class="equation-number">(18)</span>
          </div>
        </div>
        <p></p>
        <p>where <em>IAN</em> refers to the item attention
        network.</p>
        <p>The full algorithm for optimizing the <em>TARMF</em>
        model is presented in Algorithm 1 . At each epoch, we
        alternate between the optimization of <em>U</em>,
        <em>V</em>, <em>W<sub>U</sub></em> , and
        <em>W<sub>V</sub></em> . While <em>U</em> and <em>V</em>
        are fitted with alternating least squares,
        <em>W<sub>U</sub></em> and <em>W<sub>V</sub></em> are
        optimized with mini-batch gradient descent. The parameters
        currently being optimized make the assumption that all the
        other parameters are optimal. Apparently, such assumption
        is far from the truth in the first few epochs, and the
        parameters might be falsely guided by other unoptimized
        parameters. However, as the model goes through the
        optimization procedure, each parameter is getting closer
        and closer to its optimal value, and the model would
        eventually converge.</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-img1.svg"
        class="img-responsive" alt="" longdesc="" /></p>
      </section>
    </section>
    <section id="sec-20">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Quantitative
          Evaluation</h2>
        </div>
      </header>
      <p>In this section, we evaluate the <em>TARMF</em> model on
      real-world datasets to compare its performance with a number
      of state-of-the-art recommendation techniques reported in the
      literature.</p>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Datasets
            and Evaluation Metrics</h3>
          </div>
        </header>
        <p>We use five publicly available datasets - including two
        datasets from the Yelp Dataset Challenge <a class="fn"
        href="#fn1" id="foot-fn1"><sup>1</sup></a> and three others
        from Amazon - for the purpose of this analysis; see Table
        <a class="tbl" href="#tab1">1</a>.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Statistics of the evaluation
            datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">#users</th>
                <th style="text-align:center;">#items</th>
                <th style="text-align:center;">#ratings</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Yelp 2013</td>
                <td style="text-align:center;">1,631</td>
                <td style="text-align:center;">1,633</td>
                <td style="text-align:center;">78,966</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yelp 2014</td>
                <td style="text-align:center;">4,818</td>
                <td style="text-align:center;">4,194</td>
                <td style="text-align:center;">231,163</td>
              </tr>
              <tr>
                <td style="text-align:center;">Amazon
                Electronics</td>
                <td style="text-align:center;">37,128</td>
                <td style="text-align:center;">25,783</td>
                <td style="text-align:center;">1,689,188</td>
              </tr>
              <tr>
                <td style="text-align:center;">Amazon Video
                Games</td>
                <td style="text-align:center;">24,303</td>
                <td style="text-align:center;">10,672</td>
                <td style="text-align:center;">231,780</td>
              </tr>
              <tr>
                <td style="text-align:center;">Amazon Gourmet
                Foods</td>
                <td style="text-align:center;">14,681</td>
                <td style="text-align:center;">8,713</td>
                <td style="text-align:center;">151,254</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>We first randomly split all the five datasets into
        training / validation / test sets with a 70 /10 / 20 split.
        We then tune the hyperparameters on the validation set, and
        evaluate the performance of different approaches by
        calculating the Mean Squared Error (MSE) on the test set,
        which compares the differences between the predicted
        ratings and the golden truth:</p>
        <div class="table-responsive" id="Xeq16">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} MSE =
            \frac{\sum _{i = 1}^{N}{(r_{i} - \hat{r_{i}})^{2}}}{N}
            \end{equation}</span><br />
            <span class="equation-number">(19)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Baseline
            Models</h3>
          </div>
        </header>
        <p>For the purpose of comparison, we examine the
        performance of the <em>TARMF</em> model together with the
        following baseline models:</p>
        <ol class="list-no-style">
          <li id="list4" label="(1)"><strong>Offset</strong>: The
          offset estimator takes the average across all the ratings
          in the training set, and utilizes it as the predictions
          of the ratings in the test set.<br /></li>
          <li id="list5" label="(2)">
            <strong>PMF</strong>: Probabilistic Matrix
            Factorization (PMF) [<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0029">29</a>] is a popular factor-based
            model from a probabilistic point of view that performs
            well on very sparse and imbalanced datasets.<br />
          </li>
          <li id="list6" label="(3)">
            <strong>HFT</strong>: Hidden Factor as Topics (HFT)
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0026">26</a>] is a novel recommendation
            technique that utilizes Latent Dirichlet Allocation
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0003">3</a>] to model review documents. The
            model is optimized through considering both the errors
            in the predicted ratings and the corpus likelihood of
            the learned latent factors.<br />
          </li>
          <li id="list7" label="(4)">
            <strong>CTR</strong>: Collaborative Topic Regression
            (CTR) [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0041">41</a>] learns interpretable latent
            structure from user generated content so that
            probabilistic topic modeling can be integrated into
            collaborative filtering.<br />
          </li>
          <li id="list8" label="(5)">
            <strong>JMARS</strong>: Jointly Modeling Aspects,
            Ratings, and Sentiments (JMARS) [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0008">8</a>] is
            another state-of-the-art probabilistic model that
            combines collaborative filtering and topic
            modeling.<br />
          </li>
          <li id="list9" label="(6)">
            <strong>ConvMF+</strong>: Convolutional Matrix
            Factorization (ConvMF) [<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0019">19</a>] is a newly proposed
            recommendation model that employs a convolutional
            neural network for learning item features from item
            review documents. ConvMF+ refers to the ConvMF model
            initialized with pre-trained word embeddings.<br />
          </li>
        </ol>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Tuning
            Hyperparameters</h3>
          </div>
        </header>
        <p>We explore how different settings of the hyperparameters
        would influence the performance of our proposed model. The
        examined hyperparameters include the dimension of the word
        embeddings <em>d<sub>W</sub></em> , the state dimension of
        the sequence encoder <em>d<sub>S</sub></em> , the dimension
        of the transformed annotations in the attention module
        <em>d<sub>A</sub></em> , and the regularization terms
        <em>λ<sub>U</sub></em> and <em>λ<sub>V</sub></em> .</p>
        <p>The validation MSE as a result of varying
        <em>d<sub>W</sub></em> , <em>d<sub>S</sub></em> ,
        <em>d<sub>A</sub></em> , <em>λ<sub>U</sub></em> , and
        <em>λ<sub>V</sub></em> are presented in Figure <a class=
        "fig" href="#fig3">3</a>, Figure <a class="fig" href=
        "#fig4">4</a>, Figure <a class="fig" href="#fig5">5</a>,
        and Figure <a class="fig" href="#fig6">6</a>, respectively.
        As we can see, the optimal value of each hyperparameter
        remains the same regardless of the evaluated dataset.
        Therefore we empirically set the word embedding dimension
        to be 256, the sequence encoder state dimension to be 128,
        the dimension of the transformed annotations in the
        attention module to be 128, and <em>λ<sub>U</sub></em> and
        <em>λ<sub>V</sub></em> to be 100.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig3.png"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Validation MSE as a result
            of varying <em>d<sub>W</sub></em> .</span>
          </div>
        </figure>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig4.png"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Validation MSE as a result
            of varying <em>d<sub>S</sub></em> .</span>
          </div>
        </figure>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig5.png"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Validation MSE as a result
            of varying <em>d<sub>A</sub></em> .</span>
          </div>
        </figure>
        <figure id="fig6">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig6.png"
          class="img-responsive" alt="Figure 6" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 6:</span>
            <span class="figure-title">Validation MSE as a result
            of varying <em>λ<sub>U</sub></em> and
            <em>λ<sub>V</sub></em> .</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Evaluation
            Results</h3>
          </div>
        </header>
        <p>The evaluation results of all the compared models are
        presented in Table <a class="tbl" href="#tab2">2</a>. We
        note that the <em>TARMF</em> model outperforms all the
        baseline models across all the five datasets. Furthermore,
        these differences between our proposed method and each of
        the baseline models are statistically significant for
        <em>p</em> &lt; 0.05.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Recommendation performance in terms of
            MSE.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:center;" colspan="5">
                  Dataset
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;">2-6 Model</th>
                <th style="text-align:left;">Yelp 2013</th>
                <th style="text-align:left;">Yelp 2014</th>
                <th style="text-align:left;">Amazon
                Electronics</th>
                <th style="text-align:left;">Amazon Video
                Games</th>
                <th style="text-align:left;">Amazon Gourmet
                Foods</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Offset</td>
                <td style="text-align:left;">1.017</td>
                <td style="text-align:left;">1.125</td>
                <td style="text-align:left;">1.476</td>
                <td style="text-align:left;">1.435</td>
                <td style="text-align:left;">1.397</td>
              </tr>
              <tr>
                <td style="text-align:left;">PMF</td>
                <td style="text-align:left;">0.985</td>
                <td style="text-align:left;">1.053</td>
                <td style="text-align:left;">1.411</td>
                <td style="text-align:left;">1.297</td>
                <td style="text-align:left;">1.251</td>
              </tr>
              <tr>
                <td style="text-align:left;">HFT</td>
                <td style="text-align:left;">0.977</td>
                <td style="text-align:left;">1.029</td>
                <td style="text-align:left;">1.259</td>
                <td style="text-align:left;">1.152</td>
                <td style="text-align:left;">1.121</td>
              </tr>
              <tr>
                <td style="text-align:left;">CTR</td>
                <td style="text-align:left;">0.975</td>
                <td style="text-align:left;">1.013</td>
                <td style="text-align:left;">1.284</td>
                <td style="text-align:left;">1.147</td>
                <td style="text-align:left;">1.139</td>
              </tr>
              <tr>
                <td style="text-align:left;">JMARS</td>
                <td style="text-align:left;">0.970</td>
                <td style="text-align:left;">0.998</td>
                <td style="text-align:left;">1.244</td>
                <td style="text-align:left;">1.133</td>
                <td style="text-align:left;">1.114</td>
              </tr>
              <tr>
                <td style="text-align:left;">ConvMF+</td>
                <td style="text-align:left;">0.917</td>
                <td style="text-align:left;">0.954</td>
                <td style="text-align:left;">1.241</td>
                <td style="text-align:left;">1.092</td>
                <td style="text-align:left;">1.084</td>
              </tr>
              <tr>
                <td style="text-align:left;">TARMF</td>
                <td style="text-align:left;">
                <strong>0.875</strong></td>
                <td style="text-align:left;">
                <strong>0.909</strong></td>
                <td style="text-align:left;">
                <strong>1.147</strong></td>
                <td style="text-align:left;">
                <strong>1.043</strong></td>
                <td style="text-align:left;">
                <strong>1.019</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The evaluation results actually meet our expectations.
        The offset estimator has the poorest performance as it
        makes constant predictions regardless of the difference of
        users and items. The PMF model, on the other hand,
        characterizes users and items with latent factors.
        Nevertheless, building such model completely from ratings
        can be quite hard, especially when the rating data is
        sparse. Therefore the PMF model still cannot yield
        satisfying results.</p>
        <p>The remaining five algorithms, i.e., HFT, CTR, JMARS,
        ConvMF+, and <em>TARMF</em>, all utilize user-generated
        content as an auxiliary source of information for
        recommendation. This has proven to be an effective approach
        to deal with the sparseness problem. In particular, HFT,
        CTR, and JMARS are based on topic modeling with the
        bag-of-words assumption. Due to the inherent limitation of
        the bag-of-words model, i.e., it completely ignores the
        context of each word, these model are not fully capable of
        capturing the textual features in the review document. The
        ConvMF+ model, which employs a convolutional neural network
        for document modeling, partially solved this problem by
        integrating a set of filters corresponding to
        <em>n</em>-gram features in the text. It therefore
        significantly outperforms the bag-of-words models.</p>
        <p>The <em>TARMF</em> model is the best performing
        algorithm among all the five compared models. Similar to
        the ConvMF+ model, the <em>TARMF</em> model relaxes the
        bag-of-words assumption as well. In addition, it further
        improves the ConvMF+ model in three approaches. Firstly,
        the <em>TARMF</em> model employs a bidirectional recurrent
        neural network with attention mechanism, which is capable
        of modeling long documents. Secondly, the <em>TARMF</em>
        model applies the topical attention approach, which is
        similar to the idea of topic modeling, so that each latent
        factor dimension can be aligned with a specific topic. And
        thirdly, the ConvMF+ model only considers the item review
        documents, while the <em>TARMF</em> model takes both the
        user and item review documents into account, which
        introduces more flexibility to the model.</p>
      </section>
    </section>
    <section id="sec-25">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Qualitative
          Evaluation</h2>
        </div>
      </header>
      <section id="sec-26">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Attention
            Visualization</h3>
          </div>
        </header>
        <p>In order to understand the mechanism behind the
        recommendation, we try to visualize the positions that each
        attention module attend to. Consider a particular word in a
        review text of length <em>T</em>, the expected attention
        score assigned by each attention module is 1/<em>T</em>. We
        assume that if a word <em>w<sub>t</sub></em> is assigned
        with an attention score <em>s<sub>t</sub></em> ≥
        5/<em>T</em> by a specific attention module, the word is
        then of interest to the particular attention module. In
        addition, if a word simultaneously reaches the attention
        threshold of different attention modules, we assume that it
        is only attended by the attention module that assigns it
        the maximum attention score.</p>
        <p>Figure <a class="fig" href="#fig7">7</a> and Figure
        <a class="fig" href="#fig8">8</a> visualize the attention
        distribution of a specific review in the amazon electronics
        dataset assigned by the user and item attention networks.
        Words attended by different attention modules are
        highlighted with distinct colors, and darker colors refer
        to higher attention scores. We can make several
        observations from these figures. Firstly, the attention
        modules can learn interpretable regions of interest in the
        review text. For example, the red highlights in Figure
        <a class="fig" href="#fig7">7</a> corresponding to the
        first attention module, extract information about children.
        Through analyzing these information, the model could learn
        that the user that wrote this review has interests in
        purchasing electronic devices for children. Similarly, the
        yellow highlights represent the responding speed of the
        device, while the blue highlights refer to the price.
        Secondly, the attention distributions assigned by the user
        and item attention network are nicely aligned. For example,
        the yellow highlights in both Figure <a class="fig" href=
        "#fig7">7</a> and Figure <a class="fig" href="#fig8">8</a>
        consider the responding speed of the device. Such attention
        alignment is crucial as the rating is predicted by the dot
        product between the user and item latent factor
        vectors.</p>
        <figure id="fig7">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig7.png"
          class="img-responsive" alt="Figure 7" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 7:</span>
            <span class="figure-title">Attention visualization of
            user attention network.</span>
          </div>
        </figure>
        <figure id="fig8">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186158/images/www2018-167-fig8.png"
          class="img-responsive" alt="Figure 8" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 8:</span>
            <span class="figure-title">Attention visualization of
            item attention network.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span>
            Discussion</h3>
          </div>
        </header>
        <p>It is straightforward to see that the attention-based
        GRU network has indeed learned to selectively attend to
        content of interest when modeling different topics in a
        review text. We can gain intuitive interpretation of the
        hidden topic related to each dimension of the latent factor
        by examining regions with high attention scores. One would
        expect that we could automatically generate such
        interpretations by gathering words with high attention
        scores. For example, in [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>], the authors demonstrate that
        extracting the top <em>k</em> words of the LDA model would
        yield explainable topics. However, this approach does not
        work for the purpose of this paper.</p>
        <p>The reason is that, instead of making the bag-of-words
        assumption as in LDA, the GRU network incorporates
        sequential information in its annotations. Therefore, the
        annotation of each word does not only contain its own
        semantic meaning, but also include information from its
        surrounding words. As a consequence, the attention scores
        no longer measure the importance of the words alone. In
        fact, the attention scores summarize the level of interest
        of the context. We can see in Figure <a class="fig" href=
        "#fig8">8</a> that, in “I am a fan of Amazon.com and meant
        to buy a kindle for my children. But the displayed sample
        on my local Bestbuy store showed me that the nook tablet
        responded much faster than kindle fire”, instead of
        attending to “children”, the item attention network has
        actually attended to the “for” in front of it. Therefore we
        cannot directly generate recommendation explanations by
        extracting words with high attention scores in each topic.
        We leave the automatic creation of recommendation
        interpretations as a future work.</p>
      </section>
    </section>
    <section id="sec-28">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion and
          Future Work</h2>
        </div>
      </header>
      <p>Incorporating textual features has proven to enhance the
      performance of collaborative filtering algorithms. In this
      paper, we present a novel idea that employs an
      attention-based GRU network to facilitate matrix
      factorization, and introduce a coevolutionary algorithm for
      optimizing the recommendation model. The proposed model,
      which we name as <em>TARMF</em>, achieves state-of-the-art
      results on all of the five benchmark datasets. In addition,
      we demonstrate how the attention weights assigned by each
      attention module can be utilized to interpret the meaning
      associated with each dimension of the latent factor vectors.
      However, simply identifying the representative words for each
      hidden topic is far from enough for providing personalized
      recommendation explanations. The desirable accompanying
      explanation for each recommendation should be written in
      informative and readable human languages. Meanwhile, the
      recent success of applying deep neural networks and deep
      reinforcement learning algorithms for natural language
      generation [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0023">23</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0036">36</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0043">43</a>] has made
      it feasible to create meaningful and relevant texts
      conditioned on specific topics. Accordingly, such natural
      language generation models can be employed for explaining the
      suggestions provided by recommender systems, through learning
      to transform the learned latent features into human readable
      texts. Therefore, in future work, we will explore the
      potential of utilizing a sequence-to-sequence (seq2seq)
      learning framework [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0037">37</a>] for the purpose of generating
      persuasive recommendation explanations that can help
      customers make better purchasing decisions.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-29">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This work is supported by Science Foundation Ireland under
      Grant Number SFI/12/RC/2289.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Dario Amodei, Sundaram
        Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric
        Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang
        Cheng, Guoliang Chen, <em>et al.</em> 2016. Deep speech 2:
        End-to-end speech recognition in english and mandarin. In
        <em><em>International Conference on Machine
        Learning</em></em>. 173–182.</li>
        <li id="BibPLXBIB0002" label="[2]">Dzmitry Bahdanau,
        Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine
        translation by jointly learning to align and translate.
        <em><em>arXiv preprint arXiv:1409.0473</em></em>
        (2014).</li>
        <li id="BibPLXBIB0003" label="[3]">David&nbsp;M Blei,
        Andrew&nbsp;Y Ng, and Michael&nbsp;I Jordan. 2003. Latent
        dirichlet allocation. <em><em>Journal of machine Learning
        research</em></em> 3, Jan (2003), 993–1022.</li>
        <li id="BibPLXBIB0004" label="[4]">Emmanuel&nbsp;J Candès
        and Benjamin Recht. 2009. Exact matrix completion via
        convex optimization. <em><em>Foundations of Computational
        mathematics</em></em> 9, 6 (2009), 717.</li>
        <li id="BibPLXBIB0005" label="[5]">Li Chen, Guanliang Chen,
        and Feng Wang. 2015. Recommender systems based on user
        reviews: the state of the art. <em><em>User Modeling and
        User-Adapted Interaction</em></em> 25, 2 (2015),
        99–154.</li>
        <li id="BibPLXBIB0006" label="[6]">Kyunghyun Cho, Bart
        Van&nbsp;Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
        Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014.
        Learning phrase representations using RNN encoder-decoder
        for statistical machine translation. <em><em>arXiv preprint
        arXiv:1406.1078</em></em> (2014).</li>
        <li id="BibPLXBIB0007" label="[7]">Robert Desimone and John
        Duncan. 1995. Neural mechanisms of selective visual
        attention. <em><em>Annual review of neuroscience</em></em>
        18, 1 (1995), 193–222.</li>
        <li id="BibPLXBIB0008" label="[8]">Qiming Diao, Minghui
        Qiu, Chao-Yuan Wu, Alexander&nbsp;J Smola, Jing Jiang, and
        Chong Wang. 2014. Jointly modeling aspects, ratings and
        sentiments for movie recommendation (jmars). In
        <em><em>Proceedings of the 20th ACM SIGKDD international
        conference on Knowledge discovery and data
        mining</em></em>. ACM, 193–202.</li>
        <li id="BibPLXBIB0009" label="[9]">Ruihai Dong,
        Michael&nbsp;P O'Mahony, Markus Schaal, Kevin McCarthy, and
        Barry Smyth. 2013. Sentimental product recommendation. In
        <em><em>Proceedings of the 7th ACM conference on
        Recommender systems</em></em>. ACM, 411–414.</li>
        <li id="BibPLXBIB0010" label="[10]">Ruihai Dong,
        Michael&nbsp;P O'Mahony, and Barry Smyth. 2014. Further
        experiments in opinionated product recommendation. In
        <em><em>International Conference on Case-Based
        Reasoning</em></em>. Springer, 110–124.</li>
        <li id="BibPLXBIB0011" label="[11]">Ruihai Dong, Markus
        Schaal, Michael&nbsp;P O'Mahony, Kevin McCarthy, and Barry
        Smyth. 2013. Opinionated product recommendation. In
        <em><em>International Conference on Case-Based
        Reasoning</em></em>. Springer, 44–58.</li>
        <li id="BibPLXBIB0012" label="[12]">Ruihai Dong, Markus
        Schaal, Michael&nbsp;P O'Mahony, and Barry Smyth. 2013.
        Topic Extraction from Online Reviews for Classification and
        Recommendation.. In <em><em>IJCAI</em></em>, Vol.&nbsp;13.
        1310–1316.</li>
        <li id="BibPLXBIB0013" label="[13]">Ruihai Dong and Barry
        Smyth. 2016. Personalized Opinion-Based Recommendation. In
        <em><em>International Conference on Case-Based
        Reasoning</em></em>. Springer, 93–107.</li>
        <li id="BibPLXBIB0014" label="[14]">Jonas Gehring, Michael
        Auli, David Grangier, Denis Yarats, and Yann&nbsp;N
        Dauphin. 2017. Convolutional sequence to sequence learning.
        <em><em>arXiv preprint arXiv:1705.03122</em></em>
        (2017).</li>
        <li id="BibPLXBIB0015" label="[15]">Alex Graves and Jürgen
        Schmidhuber. 2005. Framewise phoneme classification with
        bidirectional LSTM and other neural network architectures.
        <em><em>Neural Networks</em></em> 18, 5-6 (2005),
        602–610.</li>
        <li id="BibPLXBIB0016" label="[16]">Karl&nbsp;Moritz
        Hermann, Tomas Kocisky, Edward Grefenstette, Lasse
        Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
        2015. Teaching machines to read and comprehend. In
        <em><em>Advances in Neural Information Processing
        Systems</em></em>. 1693–1701.</li>
        <li id="BibPLXBIB0017" label="[17]">Nal Kalchbrenner,
        Edward Grefenstette, and Phil Blunsom. 2014. A
        convolutional neural network for modelling sentences.
        <em><em>arXiv preprint arXiv:1404.2188</em></em>
        (2014).</li>
        <li id="BibPLXBIB0018" label="[18]">Timothy&nbsp;L
        Keiningham, Bruce Cooil, Lerzan Aksoy, Tor&nbsp;W
        Andreassen, and Jay Weiner. 2007. The value of different
        customer satisfaction and loyalty metrics in predicting
        customer retention, recommendation, and share-of-wallet.
        <em><em>Managing Service Quality: An International
        Journal</em></em> 17, 4 (2007), 361–384.</li>
        <li id="BibPLXBIB0019" label="[19]">Donghyun Kim, Chanyoung
        Park, Jinoh Oh, Sungyoung Lee, and Hwanjo Yu. 2016.
        Convolutional matrix factorization for document
        context-aware recommendation. In <em><em>Proceedings of the
        10th ACM Conference on Recommender Systems</em></em>. ACM,
        233–240.</li>
        <li id="BibPLXBIB0020" label="[20]">Yoon Kim. 2014.
        Convolutional neural networks for sentence classification.
        <em><em>arXiv preprint arXiv:1408.5882</em></em>
        (2014).</li>
        <li id="BibPLXBIB0021" label="[21]">Yehuda Koren. 2008.
        Factorization meets the neighborhood: a multifaceted
        collaborative filtering model. In <em><em>Proceedings of
        the 14th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em>. ACM, 426–434.</li>
        <li id="BibPLXBIB0022" label="[22]">Yehuda Koren, Robert
        Bell, and Chris Volinsky. 2009. Matrix factorization
        techniques for recommender systems.
        <em><em>Computer</em></em> 42, 8 (2009).</li>
        <li id="BibPLXBIB0023" label="[23]">Jiwei Li, Will Monroe,
        Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky.
        2016. Deep reinforcement learning for dialogue generation.
        <em><em>arXiv preprint arXiv:1606.01541</em></em>
        (2016).</li>
        <li id="BibPLXBIB0024" label="[24]">Greg Linden, Brent
        Smith, and Jeremy York. 2003. Amazon. com recommendations:
        Item-to-item collaborative filtering. <em><em>IEEE Internet
        computing</em></em> 7, 1 (2003), 76–80.</li>
        <li id="BibPLXBIB0025" label="[25]">Minh-Thang Luong, Hieu
        Pham, and Christopher&nbsp;D Manning. 2015. Effective
        approaches to attention-based neural machine translation.
        <em><em>arXiv preprint arXiv:1508.04025</em></em>
        (2015).</li>
        <li id="BibPLXBIB0026" label="[26]">Julian McAuley and Jure
        Leskovec. 2013. Hidden factors and hidden topics:
        understanding rating dimensions with review text. In
        <em><em>Proceedings of the 7th ACM conference on
        Recommender systems</em></em>. ACM, 165–172.</li>
        <li id="BibPLXBIB0027" label="[27]">Tomáš Mikolov, Martin
        Karafiát, Lukáš Burget, Jan Černockỳ, and Sanjeev
        Khudanpur. 2010. Recurrent neural network based language
        model. In <em><em>Eleventh Annual Conference of the
        International Speech Communication
        Association</em></em>.</li>
        <li id="BibPLXBIB0028" label="[28]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Greg&nbsp;S Corrado, and Jeff Dean.
        2013. Distributed representations of words and phrases and
        their compositionality. In <em><em>Advances in neural
        information processing systems</em></em>. 3111–3119.</li>
        <li id="BibPLXBIB0029" label="[29]">Andriy Mnih and
        Ruslan&nbsp;R Salakhutdinov. 2008. Probabilistic matrix
        factorization. In <em><em>Advances in neural information
        processing systems</em></em>. 1257–1264.</li>
        <li id="BibPLXBIB0030" label="[30]">Jeffrey Pennington,
        Richard Socher, and Christopher Manning. 2014. Glove:
        Global vectors for word representation. In
        <em><em>Proceedings of the 2014 conference on empirical
        methods in natural language processing (EMNLP)</em></em>.
        1532–1543.</li>
        <li id="BibPLXBIB0031" label="[31]">Alexandrin Popescul,
        David&nbsp;M Pennock, and Steve Lawrence. 2001.
        Probabilistic models for unified collaborative and
        content-based recommendation in sparse-data environments.
        In <em><em>Proceedings of the Seventeenth conference on
        Uncertainty in artificial intelligence</em></em>. Morgan
        Kaufmann Publishers Inc., 437–444.</li>
        <li id="BibPLXBIB0032" label="[32]">Alexander&nbsp;M Rush,
        Sumit Chopra, and Jason Weston. 2015. A neural attention
        model for abstractive sentence summarization. <em><em>arXiv
        preprint arXiv:1509.00685</em></em> (2015).</li>
        <li id="BibPLXBIB0033" label="[33]">Andrew&nbsp;I Schein,
        Alexandrin Popescul, Lyle&nbsp;H Ungar, and David&nbsp;M
        Pennock. 2002. Methods and metrics for cold-start
        recommendations. In <em><em>Proceedings of the 25th annual
        international ACM SIGIR conference on Research and
        development in information retrieval</em></em>. ACM,
        253–260.</li>
        <li id="BibPLXBIB0034" label="[34]">Minjoon Seo, Aniruddha
        Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016.
        Bidirectional attention flow for machine comprehension.
        <em><em>arXiv preprint arXiv:1611.01603</em></em>
        (2016).</li>
        <li id="BibPLXBIB0035" label="[35]">Sungyong Seo, Jing
        Huang, Hao Yang, and Yan Liu. 2017. Interpretable
        Convolutional Neural Networks with Dual Local and Global
        Attention for Review Rating Prediction. In
        <em><em>Proceedings of the Eleventh ACM Conference on
        Recommender Systems</em></em>. ACM, 297–305.</li>
        <li id="BibPLXBIB0036" label="[36]">Alessandro Sordoni,
        Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji,
        Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill
        Dolan. 2015. A neural network approach to context-sensitive
        generation of conversational responses. <em><em>arXiv
        preprint arXiv:1506.06714</em></em> (2015).</li>
        <li id="BibPLXBIB0037" label="[37]">Ilya Sutskever, Oriol
        Vinyals, and Quoc&nbsp;V Le. 2014. Sequence to sequence
        learning with neural networks. In <em><em>Advances in
        neural information processing systems</em></em>.
        3104–3112.</li>
        <li id="BibPLXBIB0038" label="[38]">Duyu Tang, Bing Qin,
        and Ting Liu. 2015. Document Modeling with Gated Recurrent
        Neural Network for Sentiment Classification.. In
        <em><em>EMNLP</em></em>. 1422–1432.</li>
        <li id="BibPLXBIB0039" label="[39]">Aaron Van&nbsp;den
        Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
        content-based music recommendation. In <em><em>Advances in
        neural information processing systems</em></em>.
        2643–2651.</li>
        <li id="BibPLXBIB0040" label="[40]">Ashish Vaswani, Noam
        Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
        Aidan&nbsp;N Gomez, Lukasz Kaiser, and Illia Polosukhin.
        2017. Attention Is All You Need. <em><em>arXiv preprint
        arXiv:1706.03762</em></em> (2017).</li>
        <li id="BibPLXBIB0041" label="[41]">Chong Wang and
        David&nbsp;M Blei. 2011. Collaborative topic modeling for
        recommending scientific articles. In <em><em>Proceedings of
        the 17th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em>. ACM, 448–456.</li>
        <li id="BibPLXBIB0042" label="[42]">Yequan Wang, Minlie
        Huang, Li Zhao, <em>et al.</em> 2016. Attention-based lstm
        for aspect-level sentiment classification. In
        <em><em>Proceedings of the 2016 Conference on Empirical
        Methods in Natural Language Processing</em></em>.
        606–615.</li>
        <li id="BibPLXBIB0043" label="[43]">Tsung-Hsien Wen, Milica
        Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve
        Young. 2015. Semantically conditioned lstm-based natural
        language generation for spoken dialogue systems.
        <em><em>arXiv preprint arXiv:1508.01745</em></em>
        (2015).</li>
        <li id="BibPLXBIB0044" label="[44]">Jason Weston, Antoine
        Bordes, Sumit Chopra, Alexander&nbsp;M Rush, Bart van
        Merriënboer, Armand Joulin, and Tomas Mikolov. 2015.
        Towards ai-complete question answering: A set of
        prerequisite toy tasks. <em><em>arXiv preprint
        arXiv:1502.05698</em></em> (2015).</li>
        <li id="BibPLXBIB0045" label="[45]">Kelvin Xu, Jimmy Ba,
        Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
        Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show,
        attend and tell: Neural image caption generation with
        visual attention. In <em><em>International Conference on
        Machine Learning</em></em>. 2048–2057.</li>
        <li id="BibPLXBIB0046" label="[46]">Zichao Yang, Diyi Yang,
        Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016.
        Hierarchical attention networks for document
        classification. In <em><em>Proceedings of the 2016
        Conference of the North American Chapter of the Association
        for Computational Linguistics: Human Language
        Technologies</em></em>. 1480–1489.</li>
        <li id="BibPLXBIB0047" label="[47]">Yongfeng Zhang, Guokun
        Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma. 2014.
        Explicit factor models for explainable recommendation based
        on phrase-level sentiment analysis. In <em><em>Proceedings
        of the 37th international ACM SIGIR conference on Research
        &amp; development in information retrieval</em></em>. ACM,
        83–92.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.yelp.com/dataset/challenge">https://www.yelp.com/dataset/challenge</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186158">https://doi.org/10.1145/3178876.3186158</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
