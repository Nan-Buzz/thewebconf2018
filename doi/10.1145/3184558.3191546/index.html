<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Exploring the Efficiency of Batch Active Learning for Human-in-the-Loop Relation Extraction</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Exploring the Efficiency of Batch Active Learning for Human-in-the-Loop Relation Extraction</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Ismini</span>      <span class="surName">Lourentzou</span>     University of Illinois at Urbana-Champaign, <a href="mailto:lourent2@illinois.edu">lourent2@illinois.edu</a>     </div>     <div class="author">     <span class="givenName">Daniel</span>      <span class="surName">Gruhl</span>     IBM Watson Research Lab, NY, US, <a href="mailto:dgruhl@us.ibm.com">dgruhl@us.ibm.com</a>     </div>     <div class="author">     <span class="givenName">Steve</span>      <span class="surName">Welch</span>     IBM Watson Research Lab, NY, US, <a href="mailto:welchs@us.ibm.com">welchs@us.ibm.com</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191546" target="_blank">https://doi.org/10.1145/3184558.3191546</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Domain-specific relation extraction requires training data for supervised learning models, and thus, significant labeling effort. Distant supervision is often leveraged for creating large annotated corpora however these methods require handling the inherent noise. On the other hand, active learning approaches can reduce the annotation cost by selecting the most beneficial examples to label in order to learn a good model. The choice of examples can be performed sequentially, i.e. select one example in each iteration, or in batches, i.e. select a set of examples in each iteration. The optimization of the batch size is a practical problem faced in every real-world application of active learning, however it is often treated as a parameter decided in advance. In this work, we study the trade-off between model performance, the number of requested labels in a batch and the time spent in each round for real-time, domain specific relation extraction. Our results show that the use of an appropriate batch size produces competitive performance, even compared to a fully sequential strategy, while reducing the training time dramatically.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Information extraction;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Information extraction;</strong> <em>Active learning settings;</em> <em>Neural networks;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>relation extraction;deep learning;active learning;batch mode active learning;neural networks</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Ismini Lourentzou, Daniel Gruhl, and Steve Welch. 2018. Exploring the Efficiency of Batch Active Learning for Human-in-the-Loop Relation Extraction. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3191546" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191546</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Many important natural language processing tasks, such as knowledge graph completion and question answering require semantic relation classification, where the goal is to categorize relations between entities in unstructured text. Supervised methods for this task are either based on hand-engineered features or learned representations by deep neural networks. However both methods rely heavily on large quantities of high-quality annotated data. The requirement of large labeled corpora limits the application of neural models to many Information Extraction tasks, as it is often quite expensive and challenging to acquire large amounts of reliable gold standard validation data for training. To address this issue approaches such as active learning and distant supervision were proposed.</p>    <p>Distant supervision aims to classify sentences at a bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. To reduce the noise multi-instance learning is used, however these methods cannot handle sentence-level prediction or bags where all sentences do not describe a relation. Moreover the coverage of annotations is largely dependent on the type of entities/relations: while popular relations will have good coverage, tail ones may not be well represented. Thus, incorporating human annotation is crucial, especially for domains where we have many tail relations or many sentences where the entities are mentioned but the relation does not hold (e.g.,finding adverse drug events in medical forums).</p>    <p>Active learning tries to find the most efficient way to query the unlabeled data and learn a classifier with the minimal amount of human supervision. In classical active learning setting a single instance at each iteration is chosen. However, the sequential active learning methods have many drawbacks when combined with expensive complex models, such as neural networks: training deep networks usually takes a long time, and therefore updating the model after each label is costly in terms of both the human annotation time waiting for the next datum to tag as well as computational resources. Moreover, due to the local optimization methods used for training neural networks it is highly unlikely for a single point to result in significant impact on the performance. Therefore in practical applications it is often useful to perform batch active learning, as the cost of acquiring a batch of labels for training might be significantly less than the cost of acquiring the same number of sequential individual label requests. This holds true when the time to update the model and select the next example is prohibitively large. But under labeling budget constraints there is an inherent trade-off between efficiency and performance, as large batches will result in less frequent model updates and increased prediction error.</p>    <p>Decisions regarding the parameters such as the batch size or the total budget constraints are usually taken as arguments in batch model active learning related work. However, these decisions are likely to be suboptimal as they do not rely on information acquired from the data distribution or the learned model. Thus, optimizing these parameters automatically is an important problem for many tasks. Ideally, we would want a methodology that can inform us about the batch size rather fast irrespective of the number of unlabeled examples, i.e. low complexity.</p>    <p>In this paper we focus on applying neural models for extracting an arbitrary user-defined relation from a potentially infinite pool of unlabeled Web and social stream data. Despite the advantages of batch active learning, previous work in relation extraction has not explored the trade-offs between performance and batch size, or annotation costs versus training delay [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. To better understand the nature of annotation costs for relation extraction we present an empirical study of batch active learning in ten real-world relation extraction tasks involving human annotators. More specifically we try to optimize the batch size so as to keep the model performance at a satisfactory level but reduce the total training time.</p>    <p>The contribution of this work is a systematic analysis for optimizing the batch size in an end-to-end neural net framework for relation extraction method with Human-in-the-Loop on any domain and concept that the user is interested in extracting. We examine several popular strategies to select the next examples to present to the human annotator: uncertainty sampling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], QUIRE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] and a recently proposed method that computes uncertainty estimates by sampling from the same neural model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. We test our hypothesis on publicly available standard datasets for relation extraction and on a challenging task of extracting causal relations among drugs and adverse drug events from user generated text. Our experimental results show that increasing the batch size in active learning up to around five examples produces comparable results with a sequential active learning approach. Furthermore, we propose to always keep the human annotator busy, even during model updates by training and performing next batch selection on slightly out-of-date information. We show that this approach reduces the total training time by <span class="inline-equation"><span class="tex">$\approx 50\%$</span>     </span> without hurting the overall performance.</p>    <p>The rest of the paper is organized as follows. We give an overview of related work in Section <a class="sec" href="#sec-5">2</a>; we formally define the relation extraction problem and describe our experiments in Section <a class="sec" href="#sec-9">3</a>; and we present our analysis on both on publicly available standard datasets, as well as in the medical domain for the extraction of adverse drug events (Section <a class="sec" href="#sec-10">4</a>). Finally, in Section <a class="sec" href="#sec-17">5</a> we describe future directions of our work.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Relation Extraction</h3>     </div>     </header>     <p>Early works in relation extraction include classical machine learning approaches with SVMs and kernel-based methods as the ones most commonly used [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0060">60</a>], including specialized kernels designed for relation extraction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] and Tree Kernels [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0058">58</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0060">60</a>]. Their main drawback is that they rely on human-engineered features and linguistic knowledge in the form of various Natural Language Processing operations (POS tagging, morphology, dependency parsing) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0049">49</a>], which can make them difficult to extend to new entity-relation types, different prose styles, new domains and other languages.</p>     <p>Considerable attention has been given to deep learning models for relation classification. Convolutional Neural Networks (CNNs) have been extensively explored: with lexical features and synonym class embeddings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]; with the addition of POS tagging and WordNet hypernyms and pre-trained word embeddings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0059">59</a>]; including dependency patterns and dependency trees [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0057">57</a>]; exploiting pre-training on large general corpus and then fine-tuning on the target corpus [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]; relying on word-level attention mechanism to detect cues and learn which parts of a sentence are relevant to a given relation type [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>]; with the combination of word embeddings and clustering to improve the generalization of relation extractors across domains [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]. Some works also investigated replacing the common soft-max loss function with a ranking-based loss function [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>] and add a novel attention mechanism to capture the relevance of words with respect to the target entities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0053">53</a>]. Ensemble of CNNs and Recurrent Neural Networks (RNNs) have also been explored with a novel mechanism for sentence splitting and a simple voting scheme [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0052">52</a>] as well as hierarchical attention-based RNNs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>].</p>     <p>The main drawback of many related works is that models are built under the assumption that a (large) pool of manually annotated examples exists already and in many cases this assumption does not hold: the definition of a relation is highly dependent on the task at hand and on the view of the user, therefore having annotated data readily available for any specific case is unlikely. Several approaches have been proposed to reduce the annotation cost for relation classification. The most prominent methods exploit large knowledge bases to automatically label entities in text [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] and circumvent the annotation problem. Such methods rely on distant supervision and assume that when two entities co-occur a certain relation is expressed in the sentence, and then try to handle the noise [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0054">54</a>]. For many ambiguous relations mere co-occurrence does not guarantee the existence of the relation and these architectures can fail on the prediction task. For example, while annotating data on Adverse Drug Events (see Section <a class="sec" href="#sec-11">4.1</a>) we found that half of the sentences mentioning a drug and an Adverse Drug Event do not express causality between them<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>.</p>     <p>A machine learning system build solely on large corpora is unlikely to capture the subtle nuances of constantly evolving social language including new terms, phrases and deviation from normal usage. Human knowledge is therefore crucial, but human supervision can be expensive. Active learning methods limit this cost by selecting the most useful examples for human annotation. We briefly discuss how active learning has been leveraged in relation extraction related work.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Active Learning for relation extraction</h3>     </div>     </header>     <p>Angeli et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] leverage active learning for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples. They show that, for the 2013 KBP English Slot Filling task<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, 10,000 labeled examples and a large corpus for distantly labeled data can yield notable improvements in performance over distantly labeled data alone. Sterckx et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0048">48</a>] perform noise reduction by using semantic clustering and word embeddings: they perform hierarchical clustering of the candidate training samples to select the most reliable ones. Fu and Grishman [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] propose to interleave self-training with co-testing to reduce the annotation cost. The co-testing (the sampling method) leverages local and global data views [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0050">50</a>]: a global classifier that relies on similarity of relation phrases and a local classifier that uses a set of lexical and syntactic features. The effectiveness of instance-ranking criteria used in active learning, such as uncertainty [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], representativeness [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>] or information gain [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], is highly dependent on the underlying data and the relation to extract and it is very difficult to identify strong connections between any of the criteria and the task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]. Moreover, the methods leveraged in relation extraction related work assume a sequential active learning setting, where we query one example at a time. However single instance selection strategies are quite expensive when dealing with training neural models in terms of computational resources and waiting time for the human annotator, as they require tedious retraining with each instance labeled.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Batch Mode Active Learning</h3>     </div>     </header>     <p>Many batch mode active learning methods have extended single instance selection strategies or propose other heuristics based on the informativeness or diversity of the selected batch [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0051">51</a>]. Proposed frameworks that try to incorporate information overlap between the instances [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] treat this as an integer programming problem and utilize second-order Taylor approximation methods. Wei et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0056">56</a>] design submodular functions for specific classifiers such as Nearest Neighbors and Naive Bayes. Recent work [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] applies core-set theory to CNNs and compares with empirical risk minimization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0055">55</a>] and clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]. However, all of the aforementioned methods have two main drawbacks:</p>     <ol class="list-no-style">     <li id="list1" label="(1)">second-order methods have high complexity and do not scale well with larger datasets.<br/></li>     <li id="list2" label="(2)">the number of instances per batch is not optimized but rather pre-selected to a specific constant number.<br/></li>     </ol>     <p>Both components, i.e. the number of instances to be queried from a given pool of unlabeled set of examples and the selection of the specific instances to be labeled are critical for a system that can generalize to many tasks and minimize human labeling effort. Most existing work requires the number of instances in each round as input argument. In real-world applications prior knowledge is crucial for these choices. But in the case of starting a system from scratch, there is typically no knowledge of the data stream with respect to its quality, the complexity of the samples or the confidence of current models that will help in designing a classifier with good generalization accuracy. Thus we cannot decide in advance the batch size.</p>     <p>The question then is how to optimize the whole process taking into account annotation and training time, as well as model performance. To the best of our knowledge, the only work that optimizes both the selection of batch size and instances transforms the problem into a single optimization function that maximizes diversity, uncertainty and redundancy as well as adding a penalty term that depends on the batch size [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>]. They solve the optimization problem with gradient-based methods, however apart from the quadratic complexity with respect to the number of unlabeled data samples (and thus not scaling well for large datasets), their method tries to optimize a function that penalizes larger batch sizes, while in our case we try to find the largest possible batch size that keeps performance at a satisfactory level, subject to our total annotating budget. Nevertheless, we have experimented with their method but unfortunately, even without the expensive computation of the diversity scoring function, the time for one iteration to return a batch size and the corresponding instances was prohibitive for our real-world Human-in-the-Loop system.</p>     <p>The aim of this work is to investigate the influence of the batch size for different active learning strategies on different relation classification tasks and extract valuable knowledge in finding the optimal batch for Human-in-the-Loop systems while keeping a satisfactory level of performance. Additionally we propose an approach that will eliminate the waiting time for the human annotator without reducing the system performance. Our training time is reduced significantly while training with 200 examples, our accuracy is on average only 5% less that a model trained on full data, which achieves 90% accuracy.</p>    </section>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Relation classification</h2>     </div>    </header>    <p>In this work, we treat relation extraction as a binary classification task, where given user-generated text <em>s</em> containing one or more target entities <em>e<sub>i</sub>     </em>, our goal is to identify if <em>s</em> expresses a certain relation <em>r</em> among the entities <em>e<sub>i</sub>     </em>. We treat relation extraction as a cold-start problem, where no labeled data exist and query a human annotator for labels. Thus active learning is the most appropriate framework to tackle this problem.</p>    <p>We consider a pool-based active learning scenario [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>] in which there exists a small set of labeled data <span class="inline-equation"><span class="tex">$L={(x_1,y_1),\dots ,(x_{n_l},y_{n_l})}$</span>     </span>    </p>    <p>and a large pool of unlabeled instances <span class="inline-equation"><span class="tex">$U={x_1,\dots ,x_{n_u}}$</span>     </span>. The task for the learner is to draw examples to be labeled from <em>U</em>, so as to maximize the performance of the classifier while limiting the expected number of labels requested and thus the annotation cost. In our task an instance is a text snippet expressing the relation between the entities and annotation refers to manually assigning a &#x201C;true/false&#x201D; label to each instance, i.e. <em>y<sub>i</sub>     </em> &#x2208; {0, 1}, where <em>y<sub>i</sub>     </em> is the annotation of instance <em>x<sub>i</sub>     </em>.</p>    <p>To acquire a large pool of unlabeled text data from any web source such as online news articles or social media streams (Twitter, blogs etc.), one can create dictionaries using any off-the-shelf tool (e.g. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>]) and select sentences based on the co-occurrence of the entities of interest. There are several approaches available for identifying entities in unstructured text [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>], thus we treat this first step as a black-box component. We then segment the learning process into <em>B</em> training rounds of <em>k</em> instances at a time and interactively annotate the data as we train the models. In each round we train a neural model using the instances we have labeled so far and use the model to select the next <em>k</em> examples to annotate from <em>U</em>. Thus our training procedure resembles recent advances in Deep Learning showing that increasing the batch size during training produces comparable results with methods that decay the learning rate but often leads to shorter training times [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>].</p>    <p>We experiment with several active learning strategies to determine the next batch of examples, specifically:</p>    <ul class="list-no-style">     <li id="list3" label="&#x2022;"><strong>us</strong>: Uncertainty sampling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], which ranks the samples according to the model&#x0027;s belief it will mislabel them<br/></li>     <li id="list4" label="&#x2022;"><strong>quire</strong>: QUIRE measures each instance informativeness and representativeness by its prediction uncertainty [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]<br/></li>     <li id="list5" label="&#x2022;"><strong>bald</strong>: a recently proposed combination of Monte Carlo and Dropout to obtain uncertainty measures and Bayesian Active Learning by Disagreement as an acquisition function to select examples that are expected to maximize the information gained about the model parameters [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>].<br/></li>    </ul>    <p>Our goal is not to specifically improve a particular learning model per-se, but rather minimize the use of computational resources as well as the human annotation effort and waiting time by choosing an optimal batch size <em>k</em>. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191546/images/www18companion-285-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">CNNs for relation extraction</span>     </div>     </figure>    </p>    <p>We chose Convolutional Neural Networks (CNNs) as our classification models, as they are highly expressive leading to very low training error and faster in training than recurrent architectures. More importantly CNNs are known to perform well in the relation classification task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0059">59</a>]. To keep our classifier lightweight and robust our input representations rely solely on distributional semantics and not on lexical features or any other language-dependent prior knowledge, as shown in Fig. <a class="fig" href="#fig1">1</a>:</p>    <ul class="list-no-style">     <li id="list6" label="&#x2022;"><strong>CNNpos</strong>: Positional features [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0059">59</a>] along with word sequences, i.e. we generate three embedding matrices, one initialized with pre-trained word embeddings and two randomly initialized for the positional features<br/></li>     <li id="list7" label="&#x2022;"><strong>CNNcontext</strong>: context-wise splits of sentences [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], i.e. using pre-trained word embeddings and the two entities in the text as split points to generate three matrices - left, middle and right context.<br/></li>    </ul>    <p>Our models are using 100-dimensional pre-trained Glove word embeddings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], 100-dimensional positional embeddings, and contain 300 convolutional filters, kernels of width 3, and ReLU nonlinearities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>]. Training is performed with cross-entropy as cost function that is optimized with Adam [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] with 0.001 initial learning rate. Dropout is set to 0.25.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>As noted, the relation extraction task is a challenging one. Especially in the case of developing early prototype systems little can be done with a traditional neural network in the absence of a significant quantity of hand labeled data. While a task specific labeling system can help [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>], it makes sense to consider the &#x201C;best order&#x201D; to ask the user for input in the hopes of achieving a sufficiently performant system with minimal human effort.</p>    <p>Our goal in this work is to limit the human and computational resources without significantly impacting the performance of the models by optimizing the active learning batch size for an arbitrary relation extraction task. We simulate the Human-in-the-Loop by using existing benchmark datasets on relation extraction. More specifically, we treat all examples as unlabeled and &#x201C;request&#x201D; the annotations in small batches from the existing labels, as if they were annotated in real-time by a user. This setting allows us to run in parallel multiple experiments varying the batch size for all active learning strategies and all tasks. We also continue our analysis on our real case scenario of extracting Adverse Drug Reaction (details on the data in Section <a class="sec" href="#sec-11">4.1</a>).</p>    <p>Our experiments showcase a methodology that can be used to decide on the optimal batch size based on the <em>average</em> performance on datasets that solve the same task for disjoint domains, for example relation extraction where the relation is different across datasets. We present a set of directly useful recommendations that can guide the development of domain-specific relation extraction systems. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191546/images/www18companion-285-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">A look at the impact of batch size on training rate for one active learning strategy, one neural structure on one task. Note that the best strategy in this case is two at a time.</span>     </div>     </figure>    </p>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>     </header>     <p>For our analysis to produce robust results that generalize across relation classification tasks and models we utilize two different datasets containing 10 relations in total:</p>     <ol class="list-no-style">     <li id="list8" label="(1)">We perform our analysis on a real case experiment by extracting Adverse Drug Events (ADE) relations from a Web forum<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. Our Human-in-the-Loop is a medical doctor using our system to annotate the data. In this dataset posts are tagged based on mentions of certain drugs, adverse drug reactions, symptoms, findings etc. However, the mere co-occurrence of a drug and an ADE in a sentence does not necessarily imply a causal event/drug relation among the two. We name this dataset <em>causalADEs</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0032">32</a>].<br/></li>     <li id="list9" label="(2)">We also leverage existing corpora: the <em>Semeval 2010 - Task 8</em> dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0019">19</a>], which consists of 8,000 training and 2,717 test examples covering nine relation types: Cause-Effect, Component-Whole, Content-Container, Entity-Destination, Message-Topic, Entity-Origin, Instrument-Agency, Member-Collection, Product-Producer. Additionally, some sentences are labeled as &#x201C;Other&#x201D;, indicating that none of those relations are expressed.<br/></li>     </ol>     <p>We run a series of experiments to quantify best practices with respect to batch size and HumL systems. Ultimately, we will examine 10 tasks and apply 3 active learning strategies to them. Initially though, we will begin looking at a single plot, that of using uncertainty sampling to train a CNNcontext model on the SemEval Component task (see Figure&#x00A0;<a class="fig" href="#fig2">2</a>). In this experiment, we train the model with batches of various sizes without any pre-training. This allows us to observe how the model is affected by varying the batch size in cold-start scenarios when no annotated data are available and we wish to start the human annotation process as quickly as possible.</p>     <p>There are a few things to notice here. The first is that training with 100 or 200 examples per batch is substantially less efficacious than the smaller batches. Additionally note that by the time you&#x0027;ve scored 200 examples, batches of 5 or 10 do nearly as well as anything else. Lastly note that until there are around 20 examples scored the system does not really take off. The intuition here is that you need enough examples to &#x201C;span the space&#x201D; or you end up over fitting what little data you have. It&#x0027;s this last point we will examine first.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Initial Batch</h3>     </div>     </header>     <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191546/images/www18companion-285-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">An exploration of the impact of initial batch size. For our datasets an an initial batch of 30 seems like a good place to start. It gives enough examples to begin to span the space. This plot is the average of 10 datasets with CNNcontext as our classification model.</span>     </div>     </figure>     <p>As we noticed above, despite having the best performance in the end, active learning with just one or two examples in not appropriate when initializing the model due to high variance in small corpora, thus the model tends to &#x201C;overfit&#x201D; these first few concepts. An alternative is to order the data based on unsupervised text based criteria and select the highest ranked ones as initial training examples. Our experiments with several criteria, including random, showed that maximizing linguistic dissimilarity between sentences (by utilizing Glove embeddings) works well [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>].</p>     <p>The first question is how large this initial batch should be for good results. We explore this by fixing the learning batch size at 5 and vary the size of the initial batch (<em>B</em>     <sup>0</sup>) generated via linguistic dissimilarity to prime the run. We continue the process until we hit a fixed training size of 200 (our budget constraint) and plot the accuracy at 200. As you can see in Figure&#x00A0;<a class="fig" href="#fig3">3</a> <a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>, starting with a batch of about 20-40 examples results in better results. The intuition here is that less than 20 and the system overfits the initial training data, more than 40 and the active learning is unable to take over and focus on the regions of confusion.</p>     <p>     <strong>Recommendation:</strong> Use an initial batch (<em>B</em>     <sup>0</sup>) derived through linguistic dissimilarity of about 30 labeled examples to train the system before engaging active learning for more efficient human annotation.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Subsequent Batch Size</h3>     </div>     </header>     <p>After obtaining an initial linguistically diverse batch of 30 examples as a good starting point, we need to decide on a proper subsequent batch size. Since computing the next &#x201C;batch&#x201D; and loading it into the UI for the subject matter expert to score takes some time, there is a preference for larger batches. However, as Figure&#x00A0;<a class="fig" href="#fig4">4</a> shows, there is a negative impact of these larger batch sizes (here we compare the accuracy across different batch sizes, after 100 training examples). The best performance is when using batch size of 1, but the real drop seems to be after 5 (which only loses &#x00A0;5% compared to the batch size of 1). Thus, if your system has a finite cost associated with generating batches this may be good place to stop.</p>     <p>     <strong>Recommendation:</strong> A default batch size of 5 examples seems to be a good compromise between efficiency of example generation and speed of learning in the active system.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Interleaving</h3>     </div>     </header>     <p>While the prior section points out the advantage of smaller batches, these advantages do not come for free. Generating a batch of examples and loading it into the scoring framework for the user to look at takes time. We have observed that generating a training example for a single sentence (e.g., is there a causal relation between A and B) with a good UI and well defined task takes between three and ten seconds on average. Five seconds is a fairly good median. If it takes 25 seconds to compute a batch and load it into the UI, then the work flow for a single item batch will be:</p>     <ol class="list-no-style">     <li id="list10" label="(1)">User spends 5 seconds scoring a single example.<br/></li>     <li id="list11" label="(2)">System spends 25 seconds getting the next example ready.<br/></li>     <li id="list12" label="(3)">Repeat.<br/></li>     </ol>     <p>Or, in other words, over 80% of the time the user is sitting around waiting. Even with the recommended batch size of 5 the user will be spending half their time waiting. The single largest cost in a Human-in-the-Loop system is the human annotation time. In an ideal world they would be scoring constantly.</p>     <p>Interleaving provides us a potential for achieving this. Without interleaving the system trains batch <em>B<sup>n</sup>     </em> using all the information from batches <em>B</em>     <sup>0</sup>...<em>B</em>     <sup>      <em>n</em> &#x2212; 1</sup>. With interleaving it uses only <em>B</em>     <sup>0</sup>...<em>B</em>     <sup>      <em>n</em> &#x2212; 2</sup>. This means that the user can be scoring a batch <em>B</em>     <sup>      <em>n</em> &#x2212; 1</sup> while the computation and loading of the next batch <em>B<sup>n</sup>     </em> is occurring.</p>     <p>Obviously, with less training data the accuracy is likely to suffer; the question is by how much. We perform this experiment by comparing the two approaches, i.e. with or without interleaving, using a <em>B</em>     <sup>0</sup> of 30 and a batch size of 5 (see Figure&#x00A0;<a class="fig" href="#fig5">5</a>). As can be seen, these two are quite close in terms of accuracy. If we additionally plot the total time required for all iterations (Figure <a class="fig" href="#fig6">6</a>) the result is even more striking; we see that interleaving produces comparable performance in <span class="inline-equation"><span class="tex">$\approx 50\%$</span>     </span> less training time, irrespective of the active learning method chosen. Moreover, we showcase the inefficiency of training for one round with 200 examples (horizontal lines).</p>     <p>     <strong>Recommendation:</strong> Use interleaving with a batch size that is as small as possible while still allowing continuous human work. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191546/images/www18companion-285-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">A view of performance of the CNNcontext model trained under different active learning methods. This is a look at the performance after 100 examples have been scored. As can be seen, compared to the fully sequential approach of one example at a time, there is approximately only 5% decrease in the performance of using a slightly larger batch size of 5 examples.</span>      </div>     </figure>     <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191546/images/www18companion-285-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Comparison of interleaving and classic training sessions in terms of accuracy.</span>      </div>     </figure>     <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191546/images/www18companion-285-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Comparison of interleaving and classic training sessions in terms of total training and labeling time.</span>      </div>     </figure>     </p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Active Learning comparison</h3>     </div>     </header>     <p>To conclude, we also present a comparison of active learning methods. As expected, Uncertainty is much faster than the rest of the active learning strategies, and QUIRE is slower than all (Fig. <a class="fig" href="#fig6">6</a>). Since bald requires sampling from the model during testing time, it requires slightly more time than uncertainty to compute the final ranking of the samples, but also suffers from noise due to the monte carlo estimation of the ranking score. Uncertainty seems to be the winner in all dimensions, as it produces the best results faster (shown in Figures <a class="fig" href="#fig3">3</a>-<a class="fig" href="#fig5">5</a>). Despite the fact that using only uncertainty does not incorporate other information, such as representativeness or diversity, the method is extremely robust and appropriate for Human-in-the-Loop applications that require efficient switching between model updates and human querying.</p>     <p>     <strong>Recommendation:</strong> Start with active learning methods that are based on fast, less complex metrics. Compare with additional methods when sufficient data are gathered.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.6</span> Overall impact</h3>     </div>     </header>     <p>In figure <a class="fig" href="#fig5">5</a> we can also see the overall impact of leveraging active learning methods. The dotted line represents scoring 200 examples selected with linguistic dissimilarity; it indicates 61% accuracy, with random being slightly lower. For a fixed amount of work (200 examples) we see our prescription results in a 40% increase in performance (to an accuracy of 86%). For a fixed performance point we see an even more impressive result, as &#x00A0;25 scored examples achieve the same performance as the 200, an 72.5% reduction in human time.</p>     <p>Finally, we also plot the average accuracy of training with all data available as labeled. We see that the difference in terms of accuracy with our best performing model is only 4%. On average each relation task has a pool with more than 1,000 examples. Thus our system is trained on only 20% of the data, a result that proves the importance of incorporating human knowledge on relation extraction systems.</p>    </section>   </section>   <section id="sec-17">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusions and future work</h2>     </div>    </header>    <p>Relation extraction for any arbitrary domain of user interest is a challenging task. To leverage state-of-the-art neural network approaches in settings where large pre-annotated corpora are not available human annotation is necessary. In this work, we aim to reduce to the computational and annotation costs incurred from training a relation classifier under streamed annotations, while sustaining a reasonable level of performance. We provide an analysis of active learning methods that can be adapted to the setting in which labels are requested in equal-sized batches of <em>k</em> examples. We show that as <em>k</em> increases, the model performance is lower than that of the analogous results for fully-sequential active learning. Our experimental results show that we can achieve competitive performance for extracting relations with very little annotated data. Finally, we propose a method that trains on slightly outdated information but keeps the human annotator busy, and show that this results in a <span class="inline-equation"><span class="tex">$\approx 50\%$</span>     </span> decrease in total time without significantly impacting the accuracy of the resulting model. Our Human-in-the-Loop system can easily learn new arbitrary relations efficiently, fully leveraging the human annotator throughout the process.</p>    <p>Our work is directly applicable to Human-in-the-Loop relation extraction. However we have experimented only with RE systems, and therefore our work is still tentative for general applications. Although intuitively we believe that the recommendations should generalize well across many tasks, our results could be potentially sensitive to the data distribution. We leave the analysis of this sensitivity to future work.</p>    <p>Active learning might be widely explored but several components remain as open problems. We conclude with a description of potential future directions that we hope to explore:</p>    <ul class="list-no-style">     <li id="list13" label="&#x2022;">Adaptive batch size active learning methods, where the batch is changed dynamically between iterations, depending on additional features of specific instances.<br/></li>     <li id="list14" label="&#x2022;">Our work assumes perfect ground truth labels. However, in reality we often deal with non-perfect labelers and this introduces challenges in real-world applications of active learning. It would be useful to explore how the optimal batch size varies with respect to the labeling noise.<br/></li>     <li id="list15" label="&#x2022;">Blending semi-supervised with batch active learning, as this will help us explore distributional semantics for pre-training our models and potentially decrease the number of labels needed to reach a good performance.<br/></li>     <li id="list16" label="&#x2022;">Framing the relation extraction problem as a resource-bounded multi-objective optimization problem and try to reduce the complexity of batch mode active learning methods.<br/></li>     <li id="list17" label="&#x2022;">Meta-learning approaches, i.e. learning the best active learning strategy instead of relying on heuristics such as uncertainty, diversity etc. Current meta-learning approaches are limited to stream-based active learning or static one-step selection of a batch for labeling. Extending to pool-based adaptive scenarios can potentially leverage the representational similarity of unlabeled data points and lower the total number of examples that the system asks humans to annotate.</li>    </ul>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Heike Adel, Benjamin Roth, and Hinrich Sch&#x00FC;tze. Comparing convolutional neural networks to traditional models for slot filling. In <em>NAACL-HLT</em>, 2016.</li>     <li id="BibPLXBIB0002" label="[2]">Alfredo Alba, Anni Coden, Anna&#x00A0;Lisa Gentile, Daniel Gruhl, Petar Ristoski, and Steve Welch. Language agnostic dictionary extraction. In <em>ISWC (ISWC-PD-Industry)</em>, number 1963 in CEUR Workshop Proceedings, 2017.</li>     <li id="BibPLXBIB0003" label="[3]">Gabor Angeli, Julie Tibshirani, Jean Wu, and Christopher&#x00A0;D Manning. Combining distant and partial supervision for relation extraction. In <em>EMNLP</em>, pages 1556&#x2013;1567, 2014.</li>     <li id="BibPLXBIB0004" label="[4]">Isabelle Augenstein, Diana Maynard, and Fabio Ciravegna. Distantly supervised web relation extraction for knowledge base population. <em>Semantic Web</em>, 7(4):335&#x2013;349, 2016.</li>     <li id="BibPLXBIB0005" label="[5]">Nguyen Bach and Sameer Badaskar. A review of relation extraction. <em>Literature review for Language and Statistics II</em>, 2, 2007.</li>     <li id="BibPLXBIB0006" label="[6]">Klaus Brinker. Incorporating diversity in active learning with support vector machines. In <em>Proceedings of the 20th International Conference on Machine Learning (ICML-03)</em>, pages 59&#x2013;66, 2003.</li>     <li id="BibPLXBIB0007" label="[7]">Razvan Bunescu and Raymond Mooney. Learning to extract relations from the web using minimal supervision. In <em>ACL</em>, 2007.</li>     <li id="BibPLXBIB0008" label="[8]">Razvan&#x00A0;C Bunescu and Raymond&#x00A0;J Mooney. A shortest path dependency kernel for relation extraction. In <em>HLT/EMNLP</em>, pages 724&#x2013;731. ACL, 2005.</li>     <li id="BibPLXBIB0009" label="[9]">Rui Cai, Xiaodong Zhang, and Houfeng Wang. Bidirectional recurrent convolutional neural network for relation classification. In <em>ACL</em>, 2016.</li>     <li id="BibPLXBIB0010" label="[10]">Shayok Chakraborty, Vineeth Balasubramanian, and Sethuraman Panchanathan. Adaptive batch mode active learning. <em>IEEE transactions on neural networks and learning systems</em>, 26(8):1747&#x2013;1760, 2015.</li>     <li id="BibPLXBIB0011" label="[11]">Anni Coden, Daniel Gruhl, Neal Lewis, Michael Tanenblatt, and Joe Terdiman. SPOT the drug! An unsupervised pattern matching method to extract drug names from very large clinical corpora. <em>HISB&#x2019;12</em>, pages 33&#x2013;39, 2012.</li>     <li id="BibPLXBIB0012" label="[12]">Aron Culotta and Jeffrey Sorensen. Dependency tree kernels for relation extraction. In <em>ACL</em>, 2004.</li>     <li id="BibPLXBIB0013" label="[13]">Beg&#x00FC;m Demir, Claudio Persello, and Lorenzo Bruzzone. Batch-mode active-learning methods for the interactive classification of remote sensing images. <em>IEEE Transactions on Geoscience and Remote Sensing</em>, 49(3):1014&#x2013;1031, 2011.</li>     <li id="BibPLXBIB0014" label="[14]">Lisheng Fu and Ralph Grishman. An efficient active learning framework for new relation types. In <em>IJCNLP</em>, 2013.</li>     <li id="BibPLXBIB0015" label="[15]">Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian Active Learning with Image Data. In <em>ICML</em>, 2017.</li>     <li id="BibPLXBIB0016" label="[16]">Anna&#x00A0;Lisa Gentile, Ziqi Zhang, Isabelle Augenstein, and Fabio Ciravegna. Unsupervised wrapper induction using linked data. In <em>K-CAP</em>, pages 41&#x2013;48. ACM, 2013.</li>     <li id="BibPLXBIB0017" label="[17]">Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In <em>NIPS</em>, 2008.</li>     <li id="BibPLXBIB0018" label="[18]">Zhou GuoDong, Su&#x00A0;Jian, Zhang Jie, and Zhang Min. Exploring various knowledge in relation extraction. In <em>ACL</em>, 2005.</li>     <li id="BibPLXBIB0019" label="[19]">Iris Hendrickx, Su&#x00A0;Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid &#x00D3;&#x00A0;S&#x00E9;aghdha, Sebastian Pad&#x00F3;, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In <em>DEW Workshop</em>, pages 94&#x2013;99. ACL, 2009.</li>     <li id="BibPLXBIB0020" label="[20]">Steven&#x00A0;CH Hoi, Rong Jin, and Michael&#x00A0;R Lyu. Large-scale text categorization by batch mode active learning. In <em>Proceedings of the 15th international conference on World Wide Web</em>, pages 633&#x2013;642. ACM, 2006.</li>     <li id="BibPLXBIB0021" label="[21]">Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In <em>AAAI</em>, 2015.</li>     <li id="BibPLXBIB0022" label="[22]">Sheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. Active learning by querying informative and representative examples. In <em>NIPS</em>, pages 892&#x2013;900, 2010.</li>     <li id="BibPLXBIB0023" label="[23]">Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Distant supervision for relation extraction with sentence-level attention and entity descriptions. In <em>AAAI</em>, pages 3060&#x2013;3066, 2017.</li>     <li id="BibPLXBIB0024" label="[24]">Nanda Kambhatla. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In <em>Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</em>, page&#x00A0;22. ACL, 2004.</li>     <li id="BibPLXBIB0025" label="[25]">Diederik&#x00A0;P. Kingma and Jimmy Ba. Adam:a method for stochastic optimization. In <em>ICLR</em>, 2015.</li>     <li id="BibPLXBIB0026" label="[26]">David&#x00A0;D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In <em>ICML</em>, pages 148&#x2013;156, 1994.</li>     <li id="BibPLXBIB0027" label="[27]">Zhuang Li, Lizhen Qu, Qiongkai Xu, and Mark Johnson. Unsupervised pre-training with sequence reconstruction loss for deep relation extraction models. In <em>Australasian Language Technology Association Workshop 2016</em>.</li>     <li id="BibPLXBIB0028" label="[28]">Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation extraction with selective attention over instances. In <em>ACL</em>, 2016.</li>     <li id="BibPLXBIB0029" label="[29]">ChunYang Liu, WenBo Sun, WenHan Chao, and Wanxiang Che. Convolution neural network for relation extraction. In <em>Part II of the Proceedings of the 9th International Conference on Advanced Data Mining and Applications-Volume 8347</em>, 2013.</li>     <li id="BibPLXBIB0030" label="[30]">Minguang Xiao&#x00A0;Cong Liu. Semantic relation classification via hierarchical recurrent neural network with attention. In <em>COLING</em>, 2016.</li>     <li id="BibPLXBIB0031" label="[31]">Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and Houfeng Wang. A dependency-based neural network for relation classification. In <em>arXiv preprint arXiv:1507.04646</em>, 2015.</li>     <li id="BibPLXBIB0032" label="[32]">Ismini Lourentzou, Alfredo Alba, Anni Coden, Anna&#x00A0;Lisa Gentile, Daniel Gruhl, and Steve Welch. Mining relations from unstructured content. In <em>Advances in Knowledge Discovery and Data Mining - 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, Australia, June 2018</em>, page to appear, 2018.</li>     <li id="BibPLXBIB0033" label="[33]">Makoto Miwa and Mohit Bansal. End-to-end relation extraction using lstms on sequences and tree structures. In <em>arXiv preprint arXiv:1601.00770</em>, 2016.</li>     <li id="BibPLXBIB0034" label="[34]">Raymond&#x00A0;J Mooney and Razvan&#x00A0;C Bunescu. Subsequence kernels for relation extraction. In <em>NIPS</em>, pages 171&#x2013;178, 2006.</li>     <li id="BibPLXBIB0035" label="[35]">Vinod Nair and Geoffrey&#x00A0;E Hinton. Rectified linear units improve restricted boltzmann machines. In <em>ICML</em>, pages 807&#x2013;814, 2010.</li>     <li id="BibPLXBIB0036" label="[36]">Thien&#x00A0;Huu Nguyen and Ralph Grishman. Employing word representations and regularization for domain adaptation of relation extraction. In <em>ACL</em>, 2014.</li>     <li id="BibPLXBIB0037" label="[37]">Thien&#x00A0;Huu Nguyen and Ralph Grishman. Relation extraction: Perspective from convolutional neural networks. In <em>VS@ HLT-NAACL</em>, 2015.</li>     <li id="BibPLXBIB0038" label="[38]">Jeffrey Pennington, Richard Socher, and Christopher&#x00A0;D Manning. Glove: Global vectors for word representation. In <em>EMNLP</em>, volume&#x00A0;14, pages 1532&#x2013;1543, 2014.</li>     <li id="BibPLXBIB0039" label="[39]">Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In <em>Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1</em>, pages 697&#x2013;704. ACL, 2008.</li>     <li id="BibPLXBIB0040" label="[40]">Alexander&#x00A0;J. Ratner, Christopher&#x00A0;De Sa, Sen Wu, Daniel Selsam, and Christopher R&#x00E9;. Data programming: Creating large training sets, quickly. In <em>NIPS</em>, pages 3567&#x2013;3575, 2016.</li>     <li id="BibPLXBIB0041" label="[41]">Benjamin Roth, Tassilo Barth, Michael Wiegand, and Dietrich Klakow. A survey of noise reduction methods for distant supervision. In <em>AKBC</em>, pages 73&#x2013;78. ACM, 2013.</li>     <li id="BibPLXBIB0042" label="[42]">Cicero Nogueira&#x00A0;dos Santos, Bing Xiang, and Bowen Zhou. Classifying relations by ranking with convolutional neural networks. In <em>arXiv preprint arXiv:1504.06580</em>, 2015.</li>     <li id="BibPLXBIB0043" label="[43]">Ozan Sener and Silvio Savarese. A geometric approach to active learning for convolutional neural networks. <em>arXiv preprint arXiv:1708.00489</em>, 2017.</li>     <li id="BibPLXBIB0044" label="[44]">Burr Settles. Active learning literature survey. <em>University of Wisconsin, Madison</em>, 52(55-66):11, 2010.</li>     <li id="BibPLXBIB0045" label="[45]">Yatian Shen and Xuanjing Huang. Attention-based convolutional neural network for semantic relation extraction. In <em>COLING</em>, 2016.</li>     <li id="BibPLXBIB0046" label="[46]">Samuel&#x00A0;L Smith, Pieter-Jan Kindermans, and Quoc&#x00A0;V Le. Don&#x0027;t decay the learning rate, increase the batch size. <em>arXiv preprint arXiv:1711.00489</em>, 2017.</li>     <li id="BibPLXBIB0047" label="[47]">Gabriel Stanovsky, Daniel Gruhl, and Pablo Mendes. Recognizing mentions of adverse drug reaction in social media using knowledge-infused recurrent models. In <em>EACL</em>, pages 142&#x2013;151. ACL, 2017.</li>     <li id="BibPLXBIB0048" label="[48]">Lucas Sterckx, Thomas Demeester, Johannes Deleu, and Chris Develder. Using active learning and semantic clustering for noise reduction in distant supervision. In <em>AKBC at NIPS</em>, pages 1&#x2013;6, 2014.</li>     <li id="BibPLXBIB0049" label="[49]">Fabian&#x00A0;M Suchanek, Georgiana Ifrim, and Gerhard Weikum. Combining linguistic and statistical analysis to extract relations from web documents. In <em>Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, pages 712&#x2013;717. ACM, 2006.</li>     <li id="BibPLXBIB0050" label="[50]">Ang Sun and Ralph Grishman. Active learning for relation type extension with local and global data views. In <em>Proceedings of the 21st ACM international conference on Information and knowledge management</em>, pages 1105&#x2013;1112. ACM, 2012.</li>     <li id="BibPLXBIB0051" label="[51]">Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. <em>Journal of machine learning research</em>, 2(Nov):45&#x2013;66, 2001.</li>     <li id="BibPLXBIB0052" label="[52]">Ngoc&#x00A0;Thang Vu, Heike Adel, Pankaj Gupta, et&#x00A0;al. Combining recurrent and convolutional neural networks for relation classification. In <em>NAACL-HLT</em>, pages 534&#x2013;539, 2016.</li>     <li id="BibPLXBIB0053" label="[53]">Linlin Wang, Zhu Cao, Gerard de&#x00A0;Melo, and Zhiyuan Liu. Relation classification via multi-level attention cnns. In <em>ACL</em>, 2016.</li>     <li id="BibPLXBIB0054" label="[54]">Xiaobin Wang, Yu&#x00A0;Hong, Jianmin Yao, Qiaoming Zhu, and Guodong Zhou. A novel approach for relation extraction with few labeled data. pages 73&#x2013;84, 2016.</li>     <li id="BibPLXBIB0055" label="[55]">Zheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode active learning. <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>, 9(3):17, 2015.</li>     <li id="BibPLXBIB0056" label="[56]">Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In <em>Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</em>, pages 1954&#x2013;1963, 2015.</li>     <li id="BibPLXBIB0057" label="[57]">Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. Semantic relation classification via convolutional neural networks with simple negative sampling. <em>arXiv preprint arXiv:1506.07650</em>, 2015.</li>     <li id="BibPLXBIB0058" label="[58]">Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. Kernel methods for relation extraction. <em>Journal of machine learning research</em>, 3:1083&#x2013;1106, 2003.</li>     <li id="BibPLXBIB0059" label="[59]">Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao, et&#x00A0;al. Relation classification via convolutional deep neural network. In <em>COLING</em>, pages 2335&#x2013;2344, 2014.</li>     <li id="BibPLXBIB0060" label="[60]">Shubin Zhao and Ralph Grishman. Extracting relations with integrated information using kernel methods. In <em>ACL</em>, pages 419&#x2013;426. ACL, 2005.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>In many of such cases, the condition for which you are taking the drug is mentioned. E.g., &#x201C;I took aspirin for my headache&#x201D;.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://surdeanu.info/kbp2013/">http://surdeanu.info/kbp2013/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="http://www.askapatient.com/">http://www.askapatient.com/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Due to being computationally expensive to perform such an experiment with QUIRE, we do not include it in this experiment. However, we performed an experiment in which we train QUIRE with {5, 10, 20, 40, 50} examples per batch and the trend looks similar to our results for uncertainty and bald, with a performance lower than bald.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191546">https://doi.org/10.1145/3184558.3191546</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
