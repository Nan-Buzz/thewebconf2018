<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Contextual Web Summarization: A Supervised Ranking
  Approach</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3184558.3186951'>https://doi.org/10.1145/3184558.3186951</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186951'>https://w3id.org/oa/10.1145/3184558.3186951</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Contextual Web Summarization: A
          Supervised Ranking Approach</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Amit</span> <span class=
          "surName">Sarkar</span> Samsung R&amp;D Institute -
          Bangalore, Outer Ring RoadBangalore, India, <a href=
          "mailto:amit.srkr@samsung.com">amit.srkr@samsung.com</a>
        </div>
        <div class="author">
          <span class="givenName">G.</span> <span class=
          "surName">Srinivasaraghavan</span> IIIT - Bangalore,
          Electronic CityBangalore, India, <a href=
          "mailto:gsr@iiitb.ac.in">gsr@iiitb.ac.in</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186951"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186951</a><br />
        WWW '18: <a href=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186951/"
        target="_blank">Proceedings of The Web Conference 2018</a>,
        Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We investigate the task of reading context-biased
        web summarization, where the goal is to extract information
        relevant to the current reading context from a cited web
        article. In certain kind of linked document sets such as
        Wikipedia articles, scientific papers as well as news and
        blogs, such contextual summaries can be useful in providing
        additional related information to the user helping in the
        reading task. In this work, we focus on web articles only
        and try to find out the set of key components that
        contribute to building up the reading context. We build a
        supervised model for ranking sentences from the cited
        document according to their contextual salience. Initial
        evaluation based on annotated data-set of web articles show
        that our ranking model performs better than the generic
        summaries as well as baseline context-biased
        summaries.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Content ranking;</strong>
        <strong>Summarization;</strong> • <strong>Human-centered
        computing</strong> → <em>Hypertext /
        hypermedia;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Summarization;
          context-biased; salience ranking; information
          retrieval</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Amit Sarkar and G. Srinivasaraghavan. 2018. Contextual
          Web Summarization: A Supervised Ranking Approach. In
          <em>WWW '18 Companion: The 2018 Web Conference
          Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 2 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186951" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186951</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Hyper-links present in a web page article can provide more
      information about an event or object described in the source
      page by referring to another web page. They can provide
      additional information about the topic discussed in the
      immediate reading context, such as in Wikipedia or blogs.
      They can also act as an evidence to support the current
      article content, such as scientific papers or news articles.
      A generic summary of the cited article may not satisfy
      immediate information need of the user. In absence of
      summaries biased towards reading context, user may need to
      read the complete cited content to find relevant information.
      This leads to frequent context switch or loss of attention in
      the current reading context. If user need is explicitly
      captured in form of a short paragraph or text query,
      query-focused summaries can be useful. However in case of
      common reading task of web page articles, such query text is
      not available. Wan et al.&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>] tried to address this problem by
      extracting referred document sentences, most similar to the
      reading context. To the best of our knowledge, this is the
      closest research on generating reading context-biased
      summaries for web articles. Their Simple-Link method used
      anchor sentence as the reading context and cosine distance
      with term frequency vectors as similarity measure. This
      method may fail if matching terms are not found with the
      anchor sentence. Recently many researchers&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>] are using
      citation contexts to improve scientific article
      summarization. However these methods do not apply to our use
      case of web articles due to reasons such as dependency on
      discourse structure of scientific articles or requiring
      multiple citations.</p>
      <p>Main contributions of this paper are as follows: (1) We
      identify the set of features which mark a sentence's
      contextual salience and build a supervised ranking model
      using manually annotated Wikipedia and news articles; (2) Our
      method ranks cited article sentences according to their
      salience and is able to capture both in-document context and
      cross-document context simultaneously; (3) This method
      performs better than the existing baselines and expands well
      beyond Wikipedia articles.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> The Ranking
          Model</h2>
        </div>
      </header>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Features</h3>
          </div>
        </header>
        <p>In a manually authored web article, usually the author
        would keep the topically coherent sentences within a single
        block (e.g. within a para or</p>
        <p>tag), whereas sentences conveying different concept or
        context would be part of different blocks. The topic of the
        paragraph containing the anchor will be central to the
        reading context for that paragraph. Sometimes, the same
        topic may spread over multiple consecutive paragraphs. We
        consider paragraph text similarity (F1) with each target
        sentence as an important reading context capturing feature.
        However we ensure that the paragraph contains minimum
        threshold number of sentences to represent the reading
        context well. Apart from this, the anchor sentence
        similarity (F2) with each target sentence was calculated.
        For anchor texts spanning 3 or more non-stop words, we ran
        same word-length sliding window over each target sentence
        and recorded the best similarity score (F3). We extracted
        below features from the cited web article to capture
        in-document salience: 1) target sentence position (F4), 2)
        target sentence length (F5) and 3) similarity of target
        sentence with the cited web page title (F6). Few other
        features such as anchor-text length were also considered,
        but they got removed during recursive feature elimination
        steps.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Data
            Labeling &amp; Training</h3>
          </div>
        </header>
        <p>We selected and preprocessed anchor-linked document
        pairs from 21 different root topics of Wikipedia and 3
        topics (Politics, Tech and Sports) of various news
        articles. Two independent annotators ranked the sentences
        of each linked document on a scale of 0-5. The
        inter-annotator agreement computed using Cohen's Kappa was
        0.69. A total of 379 anchor-document pairs and 10387
        sentences were ranked. 279 pairs corresponding to 6701
        sentences were used for training our model. Remaining were
        used for testing.</p>
        <p>We experimented with both point-wise and pairwise
        ranking approach using Support Vector Machine learning
        method with different kernels. The models were trained and
        hyper-parameters were tuned with 5-fold cross-validation.
        Due to lack of space, we report the best results obtained
        from the pairwise approach of ranking SVM with ’rbf’ kernel
        only.</p>
      </section>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span>
          Experiments</h2>
        </div>
      </header>
      <p>For evaluation on the test data-set of 100 anchor-document
      pairs, we calculated normalized discounted cumulative gain
      for various rank positions and report the results for top 2,
      3 and 5 sentences here. We compared our methods with 3
      baselines: 1) BL1 uses LexRank algorithm&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>] which tries to find most
      central sentences using graph-based method; 2) BL2 uses
      cosine similarity between anchor text and target sentences;
      3) BL3 implements Simple Link method, which scored better
      than SVD-based ones&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>].</p>
      <p>We verified with different combinations of the features
      along with different types of similarity measures. We report
      3 variations of our methods: 1) CWSv1 model includes features
      F1, F2, F4, F5 and F6 while using cosine similarity with
      TF-IDF for all kinds of similarity measurements; 2) CWSv2.1
      uses same set of features as CWSv1 but uses LDA-based
      topic-vector similarity for F1, word-mover
      distance&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>] with word2vec word
      embeddings<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a> for F2, cosine similarity with
      TF-IDF vectors for F6; 3) CWSv2.2 is similar to CWSv2.1
      except that it includes feature F3 (applicable for anchors
      having 3 or more non-stop words), which is again measured
      with word-mover distance.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Evaluation of baselines vs proposed
          model</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:left;">
              <strong>Approach</strong></td>
              <td style="text-align:left;">
              <strong>NDCG@2</strong></td>
              <td style="text-align:left;">
              <strong>NDCG@3</strong></td>
              <td style="text-align:left;">
              <strong>NDCG@5</strong></td>
            </tr>
            <tr>
              <td style="text-align:left;">
                BL1&nbsp;[<a class="bib" data-trigger="hover"
                data-toggle="popover" data-placement="top" href=
                "#BibPLXBIB0002">2</a>]
              </td>
              <td style="text-align:left;">0.450</td>
              <td style="text-align:left;">0.463</td>
              <td style="text-align:left;">0.486</td>
            </tr>
            <tr>
              <td style="text-align:left;">BL2</td>
              <td style="text-align:left;">0.562</td>
              <td style="text-align:left;">0.543</td>
              <td style="text-align:left;">0.553</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                BL3:Simple-Link&nbsp;[<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0004">4</a>]
              </td>
              <td style="text-align:left;">0.629</td>
              <td style="text-align:left;">0.639</td>
              <td style="text-align:left;">0.639</td>
            </tr>
            <tr>
              <td style="text-align:left;">CWSv1</td>
              <td style="text-align:left;">0.689</td>
              <td style="text-align:left;">0.668</td>
              <td style="text-align:left;">0.672</td>
            </tr>
            <tr>
              <td style="text-align:left;">CWSv2.1</td>
              <td style="text-align:left;">0.717</td>
              <td style="text-align:left;">0.706</td>
              <td style="text-align:left;">0.677</td>
            </tr>
            <tr>
              <td style="text-align:left;">CWSv2.2</td>
              <td style="text-align:left;">
              <strong>0.734</strong></td>
              <td style="text-align:left;">
              <strong>0.711</strong></td>
              <td style="text-align:left;">
              <strong>0.688</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Results
            &amp; Analysis</h3>
          </div>
        </header>
        <p>Table&nbsp;<a class="tbl" href="#tab1">1</a> shows the
        results of various methods on the contextual summarization
        task for links on web articles. Our supervised ranking
        methods (CWS) performed better than the baseline methods.
        Methods such as LexRank&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>] which are independent of reading
        context, did not perform well. Overall, word embedding
        based cross-document features gave better results compared
        to TF-IDF based ones as it could capture semantic
        similarity between different terms. However for the page
        title similarity, TF-IDF based cosine distance produced
        better score mostly because the page title usually contains
        words from the same page content itself. It seems that
        topic similarity with paragraph helped in capturing the
        reading context better, resulting in improved score for
        CWSv2.1. Our analysis for the better performance of CWSv2.2
        model compared to CWSv2.1 is that it was able to capture
        finer grained match between anchor text and target
        sentences due to phrase-level one-to-one mapping of
        semantically similar words provided by word mover distance.
        A comparison of 3 sentences summary using our method with
        respect to base-line 3 is shown in Figure&nbsp;<a class=
        "fig" href="#fig1">1</a>. Here the reading context of
        citing article revolves around water scarcity problem of
        ’Bengaluru’ and cites an article from the BBC as evidence.
        BBC's article starts with Cape Town's water problem and
        describes water scarcity problem of 11 other cities
        including ’Bangalore’. The simple-link method extracts
        sentences having ’Beijing’ and ’Jakarta’ due to anchor
        sentence cosine similarity, but misses out ’Bangalore’
        failing to capture proper reading context. Our method
        extracts sentences related to reading context as well as
        the main topic of the cited article.</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186951/images/www18companion-1-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">(A) Simple-Link vs. (B)
            CWSv2.1 method. Source refers citing paragraph text.
            Sentence order maintained.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we implemented reading context-biased web
      summarization using supervised ranking model for tasks such
      as preview of links in web pages. Using a combination of
      certain cross-document and in-document features resulted in
      extraction of sentences which are simultaneously reading
      context sensitive as well as central to the cited article.
      Our SVM ranking model performed better than prior known
      reading context-biased summarization strategies. In future,
      we plan to do a large-scale task based evaluation of our
      model.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Arman Cohan and Nazli
        Goharian. 2015. Scientific Article Summarization Using
        Citation-Context and Article's Discourse Structure. In
        <em><em>Proceedings of the 2015 Conference on Empirical
        Methods in Natural Language Processing</em></em> .
        390–400.</li>
        <li id="BibPLXBIB0002" label="[2]">Günes Erkan and
        Dragomir&nbsp;R Radev. 2004. Lexrank: Graph-based lexical
        centrality as salience in text summarization.
        <em><em>Journal of Artificial Intelligence
        Research</em></em> 22 (2004), 457–479.</li>
        <li id="BibPLXBIB0003" label="[3]">Matt Kusner, Yu Sun,
        Nicholas Kolkin, and Kilian Weinberger. 2015. From word
        embeddings to document distances. In <em><em>International
        Conference on Machine Learning</em></em> . 957–966.</li>
        <li id="BibPLXBIB0004" label="[4]">Stephen Wan and Cécile
        Paris. 2008. In-browser summarisation: Generating
        elaborative summaries biased towards the reading context.
        In <em><em>Proceedings of the 46th Annual Meeting of the
        Association for Computational Linguistics on Human Language
        Technologies: Short Papers</em></em> . Association for
        Computational Linguistics, 129–132.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>We used
    pre-trained vectors:
    <tt>https://code.google.com/archive/p/word2vec/</tt></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186951">https://doi.org/10.1145/3184558.3186951</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
