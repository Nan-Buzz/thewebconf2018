<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>“Satisfaction with Failure” or “Unsatisfied Success”:
  Investigating the Relationship between Search Success and User
  Satisfaction</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3186065'>https://doi.org/10.1145/3178876.3186065</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186065'>https://w3id.org/oa/10.1145/3178876.3186065</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">“Satisfaction with Failure” or
          “Unsatisfied Success”: Investigating the Relationship
          between Search Success and User Satisfaction</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Mengyang</span> <span class=
          "surName">Liu</span>, Department of Computer Science
          &amp; Technology, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Yiqun</span> <span class=
          "surName">Liu</span>, Department of Computer Science
          &amp; Technology, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Jiaxin</span> <span class=
          "surName">Mao</span>, Department of Computer Science
          &amp; Technology, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Cheng</span> <span class=
          "surName">Luo</span>, Department of Computer Science
          &amp; Technology, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Min</span> <span class=
          "surName">Zhang</span>, Department of Computer Science
          &amp; Technology, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Shaoping</span> <span class=
          "surName">Ma</span>, Department of Computer Science &amp;
          Technology, Tsinghua University, Beijing, China, <a href=
          "mailto:yiqunliu@tsinghua.edu.cn">yiqunliu@tsinghua.edu.cn</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186065"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186065</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>User satisfaction has been paid much attention to
        in recent Web search evaluation studies. Although
        satisfaction is often considered as an important symbol of
        search success, it doesn't guarantee success in many cases,
        especially for complex search task scenarios. In this
        study, we investigate the differences between user
        satisfaction and search success, and try to adopt the
        findings to predict search success in complex search tasks.
        To achieve these research goals, we conduct a laboratory
        study in which search success and user satisfaction are
        annotated by domain expert assessors and search users,
        respectively. We find that both “Satisfaction with Failure”
        and “Unsatisfied Success” cases happen in these search
        tasks and together they account for as many as 40.3% of all
        search sessions. The factors (e.g. document readability and
        credibility) that lead to the inconsistency of search
        success and user satisfaction are also investigated and
        adopted to predict whether one search task is successful.
        Experimental results show that our proposed prediction
        method is effective in predicting search
        success.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Retrieval effectiveness;</strong> <em>Desktop
        search;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>search success; user
          satisfaction; search evaluation</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Mengyang Liu, Yiqun Liu, Jiaxin Mao, Cheng Luo, Min
          Zhang, and Shaoping Ma. 2018. “Satisfaction with Failure”
          or “Unsatisfied Success”: Investigating the Relationship
          between Search Success and User Satisfaction. In <em>WWW
          2018: The 2018 Web Conference,</em> <em>April 23–27,
          2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em>
          11 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186065" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186065</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-2">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Search evaluation is one of the central concerns in
      information retrieval (IR) studies. Besides the traditional
      system-oriented evaluation methodology, i.e. Cranfield
      paradigm, much attention has been paid to user-oriented
      evaluation methods. Researchers are trying to model users’
      subjective feelings with various document features
      (relevance, usefulness, etc.)&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>] or users’ implicit
      feedback signals (click, hover, scroll, etc.)&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>]. In this
      line of research, many existing studies focused on the
      estimation of two important variables: <em>user
      satisfaction</em> and <em>search success</em>.</p>
      <p><em>User satisfaction</em> measures users’ subjective
      feelings about their interactions with the system. It can be
      defined as the fulfillment of a specified information
      requirement&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>]. <em>Search success</em> measures
      the objective outcome of a search process&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0025">25</a>]. Different
      from user satisfaction, it is usually measured by predefined
      criteria&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>] or assessed by domain
      experts&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>]. Search success and user
      satisfaction are two variables that are both correlated with
      search performance but characterize different
      perspectives.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">An example session that user felt satisfied
          but did not succeed in finding correct information. For
          each document, we present the user's usefulness feedbacks
          and assessor's annotation of potential information gain
          (see Section 3 for more details).</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:left;"><img src=
              "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-graphic1.jpg"
              class="img-responsive" alt="" longdesc="" /></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>For search tasks with information needs which are simple
      and clear, search success is usually consistent with user
      satisfaction. That is to say, when a user is satisfied by
      enough useful information (that he/she believes), the search
      process can usually be regarded as a successful search as
      well. However, for search tasks with complex information
      needs, e.g. exploratory search, it is sometimes difficult for
      users to determine whether they have gained enough credible
      information to fulfill their information needs. In these
      scenarios, user satisfaction might be different from search
      success.</p>
      <p>Table&nbsp;<a class="tbl" href="#tab1">1</a> presents an
      example from our experimental studies (See Section 3.1 for
      more details). The user issued a query and sequentially
      viewed three landing pages. After manually checking the
      clicked documents, we find that the first and the third pages
      contain limited helpful information. However, the user
      considered that these two results are useful because they
      contained some unreliable content (some outdated personal
      opinions) seemingly to be able to answer the question.
      Meanwhile, the user regarded a document (the 2nd clicked
      document) that actually contains useful information as
      useless since the useful content is a bit far from the
      beginning of the Web page. From the searcher's point of view,
      he/she felt satisfied because he/she had found enough useful
      information in his/her opinion. However, he/she actually got
      very biased information and this task was not successful from
      a domain expert's point of view. In this example, we can see
      that user satisfaction may be different from search success
      because of content credibility or document readability.</p>
      <p>The differences between user satisfaction and search
      success may lead to two scenarios: “satisfaction with
      failure” and “unsatisfied success”. The “unsatisfied success”
      scenario just hurts users’ subjective feelings while the
      “satisfaction with failure” scenario can be really harmful to
      users. In recent studies, Frances et al.&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>] describe an accident in
      which a Chinese student unfortunately died because he was
      satisfied by a search result containing malicious information
      when he was searching for medical information. Therefore,
      besides bringing satisfaction to users, it is also very
      important to help them make a successful search, i.e. get
      sufficient correct information via search.</p>
      <p>To achieve this goal, the first and necessary step should
      be an investigation into the relationship between user
      satisfaction and search success. In this study, we try to
      make the first step by answering these research
      questions:</p>
      <ul class="list-no-style">
        <li id="list1" label="•"><strong>RQ1</strong> To what
        extent are search success and user satisfaction
        inconsistent with each other in complex search tasks, in
        particular, exploratory searches?<br /></li>
        <li id="list2" label="•"><strong>RQ2</strong> What are the
        factors that lead to these “Satisfaction with Failure” or
        “Unsatisfied Success” cases?<br /></li>
        <li id="list3" label="•"><strong>RQ3</strong> Can we
        predict search success in complex search sessions with
        different combinations of features?<br /></li>
      </ul>
      <p>To shed light on these research questions, we conducted a
      laboratory user study in which both subjective user feedbacks
      and objective judgments by domain experts are collected. With
      this constructed dataset<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a>, we investigate the relationship
      between search success and user satisfaction. Especially, we
      try to identify the reasons behind the inconsistency of these
      two important variables. To the best of our knowledge, we are
      among the first to perform this kind of investigation. The
      major contributions of this study are three folds:</p>
      <ul class="list-no-style">
        <li id="list4" label="•">We show the discrepancy between
        user satisfaction and search success. Specifically,
        inconsistent cases account for 40.3% of all sessions
        collected in our user study, and satisfied but unsuccessful
        sessions account for 70.2% of inconsistent
        cases.<br /></li>
        <li id="list5" label="•">We find the discrepancy mainly
        comes from the inconsistency among user-perceived
        usefulness and potential gain of clicked documents. We find
        that some factors will significantly affect user's
        perception of usefulness. For example, documents with
        higher readability will be considered to be more useful by
        users even when the potential gain provided by the document
        is not so high.<br /></li>
        <li id="list6" label="•">We propose a new metric to
        estimate search success and build a regression model to
        predict search success. We find that the features that are
        correlated with search satisfaction is not so effective in
        estimating search success, which further shows that user
        satisfaction and search success are decided by different
        mechanisms.<br /></li>
      </ul>
      <p>The remainder of this paper is organized as follows.
      Section 2 reviews existing studies related to this work.
      Section 3 describes the experimental user study and
      corresponding annotation processes. In Section 4, we present
      data analysis to answer <strong>RQ1</strong> and
      <strong>RQ2</strong>. To answer <strong>RQ3</strong>, we
      propose success-oriented evaluation metrics in Section 5 and
      models for search success prediction in Section 6. Finally,
      we give our conclusions and future work in Section 7.</p>
    </section>
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <section id="sec-4">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> User
            satisfaction</h3>
          </div>
        </header>
        <p>User satisfaction measures users’ subjective feelings
        about their interactions with the system, it can be
        understood as the fulfillment of a specified information
        requirement&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>]. It has been noticed that a more
        realistic evaluation of system performance can be made,
        with actual users’ explicit judgments&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0003">3</a>].</p>
        <p>A lot of studies investigate the relationship between
        user satisfaction and the search system's outcomes. Huffman
        and Hochster&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>] found a strong correlation between
        session-level satisfaction and some simple relevance
        metrics. Maskari et al.&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>] found that user satisfaction is
        strongly correlated with some evaluation metrics such as CG
        and DCG. Jiang et al.&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>] proposed the concept of graded
        search satisfaction and observed a strong correlation
        between satisfaction and average search outcome per
        effort.</p>
        <p>The relationship between user satisfaction and user's
        behavior have also been widely investigated. Wang et
        al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0028">28</a>] found the action-level
        satisfaction contributed to overall search satisfaction.
        Kim et al.&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0017">17</a>] found that the click-level
        satisfaction can be predicted with click dwell-time. Liu et
        al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>] extracted users’ mouse movement
        information on search result pages and proposed an
        effective method to predict user satisfaction.</p>
      </section>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Search
            Success</h3>
          </div>
        </header>
        <p>Ageev et al. proposed a conceptual model of an
        informational search success, which is referred to as
        <em>QRAV</em>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>]. The model consists of four stages:
        query formulation, result identification, answer extraction
        and verification of the answer. Some
        studies&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0010">10</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0027">27</a>] asked users to fill a predefined
        questionnaire to estimate their degrees of search success.
        Li et al.&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0019">19</a>] collected users’ explicit answer
        about a search task and regard the correctness as search
        success. In this study, we follow Li et al.’s approach and
        put the emphasis on what users have gained via interactions
        with retrieval system.</p>
        <p>Existing studies proposed different methods to predict
        search success. Hassan et al.&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0004">4</a>] proposed models which
        can predict session-level search success accurately with
        users’ behavior. Ageev et al. explored the strategies and
        behavior of successful searchers and proposed a game-like
        framework for modeling different types of web search
        success&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>]. Odijk et al. investigated the
        relationship between struggling and search
        success&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>]. Based on their analysis, they
        built a system to help searchers struggle less and succeed
        more.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Exploratory
            search</h3>
          </div>
        </header>
        <p>White and Roth&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>] pointed out that exploratory
        search can be defined as an information-seeking problem,
        which is open-ended, with persistent, opportunistic,
        iterative, multi-faceted processes aimed more at learning
        than answering a specific question. Compared to an ordinary
        search task, exploratory search is often accompanied by a
        cognitive, learning, and information-gathering process,
        users may have different behavior due to their limited
        knowledge&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0029">29</a>]. Liu et al.&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0021">21</a>] showed that task
        difficulty and domain knowledge will affect users’ search
        behavior. Eileen et al.&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>] explored the effect of domain
        knowledge and search expertise on search effectiveness.</p>
        <p>Due to the complexity of exploratory search, it is
        sometimes difficult for users to determine whether they
        have gained enough credible information to fulfill their
        information needs. Therefore, user satisfaction may not
        always be consistent with their search outcome. In this
        study, we try to make an in-depth investigation on the
        relationship between search success and user
        satisfaction.</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">Data collection
            procedure.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Data
          Collection</h2>
        </div>
      </header>
      <p>To investigate the relationship between user satisfaction
      and search success, we conducted an experimental study which
      consists of two steps (see Figure&nbsp;<a class="fig" href=
      "#fig1">1</a>) : I. User Study and II. Data Annotation. The
      following four kinds of data are collected in these two
      steps. (1) We collect users’ interactions during searching
      process, including query, click, examinations on results, and
      etc. (2) Before conducting a search task, the participants
      are required to report their perceived difficulty, prior
      knowledge, and interest about the topic. Once the task is
      finished, we explicitly ask them to report their satisfaction
      for each task and perceived usefulness for each result. (3)
      Users summarize their search outcome by answering task
      questions before and after conducting the search task, which
      can be used to infer to what degree they have achieved
      success. (4) We collect external assessors’ judgments from
      four aspects (relevance, readability, credibility,
      findability).</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> User
            study</h3>
          </div>
        </header>
        <p>In the laboratory user study, each participant needs to
        complete 6 tasks which comes from three domains:
        Environment, Medicine, and Politics. All tasks were
        designed by senior graduate students in corresponding
        departments (refered to as “experts” afterwards). The task
        descriptions are shown in Table <a class="tbl" href=
        "#tab2">2</a>. All tasks were designed based on the several
        criteria. Firstly, the task should be clearly stated so
        that all participants can interpret the description in the
        same way. Secondly, we make sure that the task should not
        be a trivial one and the participants can hardly finish it
        with only a few simple search interactions, since in this
        study we mainly focus on complex search scenarios. In
        addition, we ask the experts to provide a list of key
        points in answering the question of a certain task. The
        creation of key points is inspired by the concept of
        “information nugget” used by Clarke et al.&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0006">6</a>]. But it
        is also different from “information nugget” in which each
        key point is assigned with an importance score because it
        is more necessary to find the essential points. Key points
        are used to estimate the quality of a user's answer and
        also the <em>potential gain</em> of result documents. For
        example, the first task in Table&nbsp;<a class="tbl" href=
        "#tab2">2</a> has eight key points, including:
        <strong>(a1)</strong> the average annual pollution
        concentration is high (score = 5); <strong>(a2)</strong>
        the pollution concentration has a strong regional property
        (score = 4); . . . ; <strong>(a8)</strong> the pollution
        concentration decreased significantly in recent years
        (score = 3).</p>
        <p>The <em>potential gain</em> is defined as the percentage
        that the key points contained in a document cover a user's
        information needs, i.e [<em>g</em> <sub>1</sub>, <em>g</em>
        <sub>2</sub>, ..., <em>g<sub>m</sub></em> ]. The value of
        <em>g<sub>i</sub></em> is the importance score of each key
        point. Through our data annotation in Section 3.2, we can
        know whether each key point exists in the document. So the
        document can be represented as [<em>e</em> <sub>1</sub>,
        <em>e</em> <sub>2</sub>, ..., <em>e<sub>m</sub></em> ],
        <em>e<sub>i</sub></em> = 1/0 means that the key point
        exists/not-exist in the document, and the potential gain
        can be calculated as equation <a class="eqn" href=
        "#eq1">1</a>.</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            Potential\_Gain = \frac{\sum _{i=1}^m e_i*g_i}{\sum
            _{i=1}^m g_i} \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <p></p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Search tasks in the user study.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">
                <strong>Domain</strong></th>
                <th style="text-align:left;"><strong>Task
                Description</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Environment</td>
                <td style="text-align:left;">What are the
                characteristics of pollution particulate matter in
                China? Your answer should cover its compositions,
                its time-varying patterns, and its geographical
                characteristics.</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">Why ultraviolet
                disinfection cannot completely supplant
                chlorination when disinfecting drinking water? And
                what are the advantages and disadvantages of
                them?</td>
              </tr>
              <tr>
                <td style="text-align:left;">Medicine</td>
                <td style="text-align:left;">What are the most
                commonly-used methods for cancer treatment in
                clinics?</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">What are the potential
                applications of 3D printing for “Precision
                Medicine”?</td>
              </tr>
              <tr>
                <td style="text-align:left;">Politics</td>
                <td style="text-align:left;">Political scientists
                have noted that the trend of political polarization
                during the US presidential election is increasingly
                evident. What are the reasons behind it?</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">In order to achieve
                their own interests, what kind of strategies do the
                US interest groups often take?</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>An experimental search engine system is developed for
        the user study. When users submit queries to this system,
        it crawls corresponding results from a major commercial
        search engine and shows the results to users. In the
        results provided to the users, all query suggestions, ads,
        and sponsor search results are removed to reduce the
        potential impacts on users’ behavior. When performing
        tasks, participants can freely formulate queries during the
        search process. The interactions are recorded by an
        injected Javascript plugin, including query formulation,
        click, scroll, mouse movement, pagination, and etc.</p>
        <p>We recruited 30 undergraduate students, via email and
        poster on campus, to take part in the user study. 22
        participants were female and 8 were male. The ages of
        participants range from 19 to 22. All the participants were
        familiar with basic usage of web search engines, and most
        of them reported using web search engines daily. After
        deleting the data with log errors, there are 166 search
        sessions remaining in the data set.</p>
        <p>After a pre-experiment training stage, each participant
        was asked to perform six tasks in a random order. There was
        no time limit for each task and the participant could have
        a rest after finishing a task when they felt tired. As
        shown in Figure&nbsp;<a class="fig" href=
        "#fig1">1</a>&nbsp;(I), the experimental procedure
        contains:</p>
        <p>(I-1) In the first stage, the participant should read
        and memorize the task description in an initial page, and
        she is asked to repeat the task description without viewing
        it to ensure she has remembered it.</p>
        <p>(I-2) Next, the participant needs to finish a pre-search
        questionnaire including: her domain knowledge level,
        predicted difficulty level, and interest level of the task.
        She gives feedback through a 5-point Likert scale (1: not
        at all, 2: slightly, 3: somewhat, 4: moderately, 5: very).
        And then she needs to give a pre-task answer if she
        believes that she knows something about the task.</p>
        <p>(I-3) After that, the participant can perform searches
        as they usually do with commercial search engine. She is
        asked to mark whether the results were useful for her in a
        right-click popup menu at the landing page (1: not at all,
        2: somewhat, 3: fairly, 4: very). She can end search
        whenever she thinks enough information has been found, or
        she can find no more useful information.</p>
        <p>(I-4) Finally, she is required to give a post-search
        answer and an overall 5-level graded satisfaction feedback
        of search experience in the task.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Data
            annotation</h3>
          </div>
        </header>
        <p>The data annotation contains two parts. In the first
        part, we asked experts to annotate how many key points
        contained in users’ pre-task and post-task answers.</p>
        <p>After that, we recruited 30 assessors on our campuses to
        annotate the clicked documents. The assessors were
        graduates or undergraduate students. Before annotating,
        they needed to read an instruction:</p>
        <p><em>You will spend approximately two hours completing 60
        annotation tasks. For each annotation task, you will be
        given a task description and a document which you should
        read carefully. Then you need to label the relevance,
        credibility, readability, and how many key points are
        included in the document. . . . There is no time limit on
        each of the tasks, and no minimum time limit overall
        either. After a task, you can move on to the next task, or
        have a rest whenever you feel tired.</em></p>
        <p>Figure&nbsp;<a class="fig" href="#fig1">1</a>&nbsp;(II)
        shows the interface for annotation. For each annotation
        task, we showed the task description and a hyperlink which
        points to a document. More specifically, the assessors need
        to provide the following information: (1) Findability; (2)
        Relevance; (3) Credibility; (4) Readability. All measures
        are labeled with a 4-level graded annotation.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Annotation instructions
            shown to assessors.</span>
          </div>
        </figure>
        <p></p>
        <p>Figure&nbsp;<a class="fig" href="#fig2">2</a> shows the
        augmented search log with a detailed annotation instruction
        on the annotation page. We adopt the similar annotation
        criteria in a number of previous studies&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0015">15</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0023">23</a>].
        Assessors were required to examine the document before
        making decisions. Firstly, they need to determine whether a
        certain key point can be easily found in the document.
        After that, they are required to make the relevance,
        credibility, and readability judgments according to the
        instructions. Each document was annotated by three
        different assessors to reduce potential bias from
        individuals.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Statistics</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Distribution of document labels from both
            users (for Usefulness) and assessors (for Relevance,
            Credibility and Readability).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">
                <strong>Measures</strong></th>
                <th style="text-align:center;">
                <strong>1</strong></th>
                <th style="text-align:center;">
                <strong>2</strong></th>
                <th style="text-align:center;">
                <strong>3</strong></th>
                <th style="text-align:center;">
                <strong>4</strong></th>
                <th style="text-align:center;"><strong>4-level
                <em>κ</em></strong></th>
                <th style="text-align:center;"><strong>2-level
                <em>κ</em></strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Usefulness</td>
                <td style="text-align:center;">734</td>
                <td style="text-align:center;">179</td>
                <td style="text-align:center;">161</td>
                <td style="text-align:center;">120</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
              </tr>
              <tr>
                <td style="text-align:left;">Relevance</td>
                <td style="text-align:center;">356</td>
                <td style="text-align:center;">390</td>
                <td style="text-align:center;">308</td>
                <td style="text-align:center;">140</td>
                <td style="text-align:center;">0.326</td>
                <td style="text-align:center;">0.428</td>
              </tr>
              <tr>
                <td style="text-align:left;">Credibility</td>
                <td style="text-align:center;">292</td>
                <td style="text-align:center;">320</td>
                <td style="text-align:center;">487</td>
                <td style="text-align:center;">95</td>
                <td style="text-align:center;">0.249</td>
                <td style="text-align:center;">0.397</td>
              </tr>
              <tr>
                <td style="text-align:left;">Readability</td>
                <td style="text-align:center;">222</td>
                <td style="text-align:center;">410</td>
                <td style="text-align:center;">421</td>
                <td style="text-align:center;">141</td>
                <td style="text-align:center;">0.173</td>
                <td style="text-align:center;">0.319</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>By conducting the user study and data annotation, we
        collected both feedback from participants and judgments
        from assessors. The distribution of collected data is
        presented in Table&nbsp;<a class="tbl" href="#tab3">3</a>.
        For later analysis, each measure can be divided to two
        level (low/high), the division principle is to ensure the
        two part has a similar scale: usefulness (1/234), relevance
        (12/34), credibility (12/34), readability (12/34). We
        applied Fleiss’ <em>κ</em> (4-level and 2-level) to assess
        the inter-assessor agreements. According to Landis et
        al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>], fair inter-assessor agreements
        between assessors are observed, which indicates the
        annotation data are of reasonable quality. Considering the
        measurements we used (findability, readability, and etc.)
        are naturally affected by the subjective factors of
        assessors, e.g. cognitive ability, the inconsistency
        observed in our experiment is acceptable.</p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Data
          Analysis</h2>
        </div>
      </header>
      <p>In order to investigate search success, we need to
      quantify it first. Beyond Li et al.’s approach
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0019">19</a>],
      we propose a new method to measure search success. This
      method can take user's prior knowledge about the task into
      account and therefore adapt to user's personalized
      information needs. We divided all the sessions into four
      quadrants according to the measured values of user
      satisfaction and search success. To analyze the difference
      between search success and user satisfaction, a series of
      one-way ANOVAs are conducted, where we find that different
      factors have different impacts on user satisfaction and
      search success. A thorough inspection of the data suggests
      that the discrepancy between user satisfaction and search
      success at session-level results from the inconsistency
      between usefulness and potential gain at document-level.
      Furthermore, using two-way ANOVA, we find that user's
      usefulness judgments may be affected by some subjective and
      objective factors.</p>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Measuring
            search success and satisfaction</h3>
          </div>
        </header>
        <p>In this study, search success is defined as the
        percentage of correct information that a user has gained
        during the search sessions. User satisfaction is a users’
        subjective feelings about their search process. We use
        different methods to measure search success and
        satisfaction.</p>
        <p>As mentioned in Section 3, every search task can be
        fully answered by a set of key points given by domain
        expert. Each key point has a 5-level importance score
        <em>g<sub>i</sub></em> (a integer number ranging from 1 to
        5). We also collected users’ pre-search answers and
        post-search answers for each task. So a user's personalized
        information need for a search task can be represented as a
        set of key points that were not covered by the pre-search
        answer and the information gain during the search can be
        measured by how many previously unknown key points were
        then found in the post-search answer. We denote the
        importance scores of the previously unknown key points as
        [<em>g</em> <sub>1</sub>, <em>g</em> <sub>2</sub>, ...,
        <em>g<sub>m</sub></em> ] and the importance scores of the
        key points covered by the post-search answer as [<em>a</em>
        <sub>1</sub>, <em>a</em> <sub>2</sub>, ...,
        <em>a<sub>k</sub></em> ]. Then the search success can be
        measured by Equation <a class="eqn" href="#eq2">2</a>.</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} Success =
            \frac{\sum _{i=1}^k a_i}{\sum _{i=1}^m g_i}
            \end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <p></p>
        <p>For example, a search task has six key points, which
        have importance scores [<em>g</em> <sub>1</sub>, <em>g</em>
        <sub>2</sub>, <em>g</em> <sub>3</sub>, <em>g</em>
        <sub>4</sub>, <em>g</em> <sub>5</sub>, <em>g</em>
        <sub>6</sub>]. A user's pre-search answer contains key
        point 2 and key point 3 ([<em>g</em> <sub>2</sub>,
        <em>g</em> <sub>3</sub>]). So his latent information needs
        is the other key points ([<em>g</em> <sub>1</sub>,
        <em>g</em> <sub>4</sub>, <em>g</em> <sub>5</sub>,
        <em>g</em> <sub>6</sub>]). If his post-answer contains
        three key points ([<em>g</em> <sub>1</sub>, <em>g</em>
        <sub>5</sub>, <em>g</em> <sub>6</sub>]), then his search
        success can be calculated as (<em>g</em> <sub>1</sub> +
        <em>g</em> <sub>5</sub> + <em>g</em>
        <sub>6</sub>)/(<em>g</em> <sub>1</sub> + <em>g</em>
        <sub>4</sub> + <em>g</em> <sub>5</sub> + <em>g</em>
        <sub>6</sub>).</p>
        <p>As mentioned in Section 3, we collected users’ 5-level
        graded satisfaction feedbacks for all sessions. The
        collected user satisfaction label is an integer ranging
        from 1 to 5 so we further use the following operations to
        map it to (0, 1): (1) perform a z-score transformation to
        standardize it; (2) map the z-score to 0-1 via a sigmoid
        function <em>f</em>(<em>z</em>) = 1/(1 + <em>exp</em>(−
        <em>z</em>)).</p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Comparison
            of user satisfaction and search success</h3>
          </div>
        </header>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Distribution of user
            satisfaction and search success. Two blue lines means
            the value of user satisfaction or search success equals
            to 0.5.</span>
          </div>
        </figure>
        <p>We show the distribution of user satisfaction and search
        success in Figure &nbsp;<a class="fig" href="#fig3">3</a>.
        All sessions were divided into four quadrants
        (<strong>Q1-Q4</strong>) according to search success and
        satisfaction. The criteria and distribution of sessions in
        each quadrant are shown in Table <a class="tbl" href=
        "#tab4">4</a>. We can see that there are more sessions in
        <strong>Q1</strong> than in <strong>Q2</strong> (47 vs 20),
        indicating that when a user feels unsatisfied, he is less
        likely to be successful in the search task. On the other
        hand, when a user feels satisfied, for almost half of the
        times (47/99=46.5%) he does not succeed. This demonstrates
        that search success is not always consistent with user
        satisfaction.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">One-way ANOVA results of different
            measures between different quadrants (*/** indicates
            statistical significance at <em>p</em>&lt;0.05/0.01
            level).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:center;">
                <strong>Q1</strong></th>
                <th style="text-align:center;">
                <strong>Q2</strong></th>
                <th style="text-align:center;">
                <strong>Q3</strong></th>
                <th style="text-align:center;">
                <strong>Q4</strong></th>
                <th style="text-align:center;">
                <strong><em>p</em>-value</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Satisfaction</td>
                <td style="text-align:center;">Low</td>
                <td style="text-align:center;">Low</td>
                <td style="text-align:center;">High</td>
                <td style="text-align:center;">High</td>
                <td style="text-align:center;"></td>
              </tr>
              <tr>
                <td style="text-align:left;">Success</td>
                <td style="text-align:center;">Low</td>
                <td style="text-align:center;">High</td>
                <td style="text-align:center;">Low</td>
                <td style="text-align:center;">High</td>
                <td style="text-align:center;"></td>
              </tr>
              <tr>
                <td style="text-align:left;">#sessions</td>
                <td style="text-align:center;">47</td>
                <td style="text-align:center;">20</td>
                <td style="text-align:center;">47</td>
                <td style="text-align:center;">52</td>
                <td style="text-align:center;"></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                sessions<sub>ratio</sub></td>
                <td style="text-align:center;">28.3%</td>
                <td style="text-align:center;">12.0%</td>
                <td style="text-align:center;">28.3%</td>
                <td style="text-align:center;">31.3%</td>
                <td style="text-align:center;"></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>#queries</strong></td>
                <td style="text-align:center;">
                <strong>4.74</strong></td>
                <td style="text-align:center;">
                <strong>4.20</strong></td>
                <td style="text-align:center;">
                <strong>4.06</strong></td>
                <td style="text-align:center;">
                <strong>2.96</strong></td>
                <td style="text-align:center;">*</td>
              </tr>
              <tr>
                <td style="text-align:left;">#clicks</td>
                <td style="text-align:center;">7.94</td>
                <td style="text-align:center;">8.05</td>
                <td style="text-align:center;">6.85</td>
                <td style="text-align:center;">6.50</td>
                <td style="text-align:center;">−</td>
              </tr>
              <tr>
                <td style="text-align:left;">pre_difficulty</td>
                <td style="text-align:center;">2.98</td>
                <td style="text-align:center;">2.80</td>
                <td style="text-align:center;">2.66</td>
                <td style="text-align:center;">2.50</td>
                <td style="text-align:center;">−</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>pre_interest</strong></td>
                <td style="text-align:center;">
                <strong>2.96</strong></td>
                <td style="text-align:center;">
                <strong>3.15</strong></td>
                <td style="text-align:center;">
                <strong>3.43</strong></td>
                <td style="text-align:center;">
                <strong>3.67</strong></td>
                <td style="text-align:center;">**</td>
              </tr>
              <tr>
                <td style="text-align:left;">pre_knowledge</td>
                <td style="text-align:center;">1.74</td>
                <td style="text-align:center;">1.70</td>
                <td style="text-align:center;">2.09</td>
                <td style="text-align:center;">2.12</td>
                <td style="text-align:center;">−</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>Relevance</strong> <span class=
                "inline-equation"><span class="tex">$_{\textbf
                {max}}$</span></span></td>
                <td style="text-align:center;">
                <strong>2.62</strong></td>
                <td style="text-align:center;">
                <strong>2.93</strong></td>
                <td style="text-align:center;">
                <strong>2.68</strong></td>
                <td style="text-align:center;">
                <strong>3.05</strong></td>
                <td style="text-align:center;">**</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                Credibility<sub>max</sub></td>
                <td style="text-align:center;">2.78</td>
                <td style="text-align:center;">2.78</td>
                <td style="text-align:center;">2.84</td>
                <td style="text-align:center;">2.71</td>
                <td style="text-align:center;">−</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                Readability<sub>max</sub></td>
                <td style="text-align:center;">2.77</td>
                <td style="text-align:center;">2.96</td>
                <td style="text-align:center;">3.01</td>
                <td style="text-align:center;">2.93</td>
                <td style="text-align:center;">−</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In addition, we show the results of one-way ANOVA for
        different measures in Table <a class="tbl" href=
        "#tab4">4</a>, including (1) the number of issued queries
        and clicked documents, as a representation of users’
        effort; (2) users’ subjective feedbacks for the interest,
        knowledge, and perceived difficulty levels of search tasks;
        (3) the objective relevance, credibility, and readability
        of clicked documents annotated by assessors. From the
        variations of the numbers of issue queries and users’
        interest levels across the four quadrants, we can observe a
        significant decreasing trend in user satisfaction when more
        efforts are spent or the user is less interested in a task.
        On the other hand, there is a significant increasing trend
        in search success when a user has clicked more relevant
        documents in a session. These results further characterize
        the differences between search success and user
        satisfaction.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span>
            Investigating the inconsistency between search success
            and user satisfaction</h3>
          </div>
        </header>
        <p>In Table <a class="tbl" href="#tab1">1</a>, we show an
        example session in which a user felt satisfied but did not
        succeed. The user's usefulness feedback and potential gain
        annotations of the clicked documents are shown in the
        Table.</p>
        <p>From the user's point of view, some useful documents had
        been found during the session, so he feels satisfied. This
        result coincides with previous studies (e.g.
        &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0023">23</a>]). Some documents with high
        potential gain had been clicked, so the search session
        would be considered as a successful one when using existing
        evaluation methods&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0013">13</a>]. However, from the user's answer,
        we knew that this search was not successful. This example
        reveals that neither the objective potential gain nor the
        user-perceived usefulness of clicked documents is
        sufficient for the overall search success. Search success
        may depend on the <em>interactions</em> between the
        objective potential gain and subject user-perceived
        usefulness of clicked documents.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Distribution of usefulness
            and relevance with potential gain.</span>
          </div>
        </figure>
        <p></p>
        <p>Figure <a class="fig" href="#fig4">4</a>&nbsp;(a) shows
        that there is only a weak correlation (r = 0.29) between
        usefulness and potential gain. As a comparison, there is a
        strong correlation (r = 0.74) between the relevance and
        potential gain in Figure <a class="fig" href=
        "#fig4">4</a>&nbsp;(b). Considering the difference between
        usefulness and potential gain, there are three different
        cases to notice: (a) a user thinks a document is useless,
        then ignore the information contained in the document, thus
        the document would contribute to neither satisfaction nor
        search success. (b) a user thinks a non-relevant document
        is useful and mistakenly think he has got the right answer,
        so the document would contribute to satisfaction but not
        search success. (c) a user thinks a relevant document is
        useful and get the correct information. In this case, the
        document would contribute to both satisfaction and search
        success.</p>
        <p>Therefore, the inconsistency between user's usefulness
        judgment and potential gain of clicked documents may causes
        the discrepancy between user satisfaction and search
        success. More specifically, a satisfactory search session
        might be unsuccessful if the user makes wrong usefulness
        judgments that are not aligned with potential gain. Based
        on this finding, we propose a metric to estimate search
        success in Section 5.</p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Factors
            influencing usefulness judgment</h3>
          </div>
        </header>
        <p>As we show in Section 4.3, for a clicked document,
        user's usefulness judgment is not always consistent with
        the potential gain. Since the potential gain is an inherent
        attribute of the document, there may exist some factors
        that affect user's usefulness judgments. We further use
        two-way ANOVAs to explore the effect of these factors on
        usefulness judgments.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Two-way ANOVAs result of
            objective factors influence user's usefulness
            judgements.</span>
          </div>
        </figure>
        <p></p>
        <section id="sec-16">
          <p><em>4.4.1 Objective factors.</em> The readability and
          credibility of each document are measured by a 4-level
          graded annotation. We regard level 1-2 as low level, and
          level 3-4 as high level.</p>
          <p><strong>Readability</strong> The main effects of
          potential-gain (<em>F</em>(1,1190) = 55.01, <em>p</em>
          &lt; 0.001) and readability (<em>F</em>(1,1190) = 50.99,
          <em>p</em> &lt; 0.001) are significant. In
          figure&nbsp;<a class="fig" href="#fig5">5</a>&nbsp;(a),
          we show the average usefulness of documents under
          different conditions. We can see higher readability is
          associated with higher usefulness. This can be explained
          by that highly readable documents will attract more
          users’ reading. So the user has a higher probability to
          perceive the information, no matter correct or not, and
          consider the document as useful.</p>
          <p><strong>Credibility</strong> Only the main effect of
          potential-gain (<em>F</em>(1,1190) = 84.25, <em>p</em>
          &lt; 0.001) is significant. In figure&nbsp;<a class="fig"
          href="#fig5">5</a>&nbsp;(b), no matter the potential gain
          is low or high, the probability that a document is
          considered useful stays almost the same. The document's
          credibility seems to have little impact on user's
          usefulness judgment in our study.</p>
        </section>
        <section id="sec-17">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.4.2</span>
              Subjective factors</h4>
            </div>
          </header>
          <figure id="fig6">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186065/images/www2018-74-fig6.jpg"
            class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span>
              <span class="figure-title">Two-way ANOVAs result of
              subjective factors influence user's usefulness
              judgements.</span>
            </div>
          </figure>
          <p>We collected the user's interest, prior knowledge, and
          difficulty feedbacks for each task in a 5-level graded
          scale. We regard level 1-2 as low level, level 3 as
          medium level, and level 4-5 as high level. We calculate
          the two-way ANOVAs only considering the high level and
          the low level.</p>
          <p><strong>Difficulty</strong> The main effect of
          user-perceived difficulty (<em>F</em>(1,828) = 4.85,
          <em>p</em> = 0.028) and potential-gain (<em>F</em>(1,828)
          = 57.72, <em>p</em> &lt; 0.001) are significant, and the
          interaction effect (<em>F</em>(1,828) = 4.44, <em>p</em>
          = 0.035) is also significant. Figure&nbsp;<a class="fig"
          href="#fig6">6</a>&nbsp;(a) shows the average usefulness
          under different conditions. When the potential gain is
          low, task difficulty has limited effect on usefulness
          judgments. But when the potential gain is high, the more
          difficult the user thinks the task is, the less likely
          the user will think the document is useful. This
          indicates that when facing a difficult search task, the
          user will be more likely to regard a relevant document as
          useless.</p>
          <p><strong>Interest</strong> The main effect of
          potential-gain (<em>F</em>(1,876) = 68.04, <em>p</em>
          &lt; 0.001) and interaction effect (<em>F</em>(1,876) =
          4.34, <em>p</em> = 0.038) are significant, the main
          effect of user's interest (<em>F</em>(1,876) = 3.77,
          <em>p</em> = 0.052) is almost significant. From
          figure&nbsp;<a class="fig" href="#fig6">6</a>&nbsp;(b) we
          can see, when the potential gain is low, the probability
          that a document is considered useful is very small. On
          the other hand, when the potential gain is high, if a
          user is more interest in a search task, the probability
          that he thinks a document is useful will be larger. This
          may because that a user is more patient with the task
          that he is interested in, so he can notice more useful
          information.</p>
          <p><strong>Knowledge</strong> The main effect of user's
          knowledge (<em>F</em>(1,936) = 11.76, <em>p</em> &lt;
          0.001) and potential-gain (<em>F</em>(1,936) = 67.20,
          <em>p</em> &lt; 0.001) are significant. In spite of the
          interaction effect (<em>F</em>(1,936) = 2.85, <em>p</em>
          = 0.092) is not significant, we still can find some
          trends in figure&nbsp;<a class="fig" href=
          "#fig6">6</a>&nbsp;(c). When the potential gain is high,
          the average usefulness is almost the same. But when the
          potential gain is low, a user with rich knowledge can
          accurately judge the document is useless. This indicates
          that user's knowledge may help him avoid the effect of
          incorrect information and make more accurate usefulness
          judgment.</p>
        </section>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span>
          Success-oriented evaluation metrics</h2>
        </div>
      </header>
      <p>In Section 4, we showed that only a document with high
      potential gain and considered useful contributes to search
      success. While traditional evaluation metrics are not
      designed to evaluate objective search success, we proposed
      new metrics to estimate user's search success.</p>
      <p>Based on the analysis in Section 4, we assume the
      success-oriented metric should satisfy the following four
      criteria (referred to as <strong>C1 − C4</strong>
      henceforth):</p>
      <ul class="list-no-style">
        <li id="list7" label="•"><strong>C1</strong> The document
        with low potential gain should not be awarded by the
        metric.<br /></li>
        <li id="list8" label="•"><strong>C2</strong> The document
        with high potential gain but considered useless should not
        be awarded.<br /></li>
        <li id="list9" label="•"><strong>C3</strong> The document
        with high potential gain and considered useful should be
        awarded.<br /></li>
        <li id="list10" label="•"><strong>C4</strong> The
        information that a user has already known should not be
        awarded.<br /></li>
      </ul>
      <p>According to these four criteria, we design a
      success-oriented metric, <em>success<sub>p</sub></em> . To
      compute it, we use usefulness feedback from searchers and
      potential gain annotation from external annotators. Based on
      the clicked sequence, we calculate the weighted sum of each
      key point using Equation(<a class="eqn" href="#eq3">3</a>).
      <em>U<sub>j</sub></em> can be calculated as
      Equation(<a class="eqn" href="#eq4">4</a>), the
      <em>usefulness<sub>j</sub></em> is user's usefulness feedback
      of the j-th clicked document. <em>E<sub>ij</sub></em> is a
      binary value, <em>E<sub>ij</sub></em> = 1 when the i-th key
      point exists in the j-th document, otherwise
      <em>E<sub>ij</sub></em> = 0. The <em>g<sub>i</sub></em>
      represents the weight of the i-th key point.</p>
      <div class="table-responsive" id="eq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} success_p = \sum
          _{i=1}^m max(U_j * E_{ij}) * g_i
          \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq4">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} U_j =
          \frac{usefulness_j - 1}{3} \end{equation}</span><br />
          <span class="equation-number">(4)</span>
        </div>
      </div>
      <p></p>
      <div class="table-responsive" id="tab5">
        <div class="table-caption">
          <span class="table-number">Table 5:</span> <span class=
          "table-title">Correlation of different metric with user's
          search success. <strong>Ci</strong> is our proposed
          criteria, √/⊗ means a metric satisfy/unsatisfy the
          current criteria.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">
              <strong>metric</strong></th>
              <th style="text-align:center;">
              <strong>C1</strong></th>
              <th style="text-align:center;">
              <strong>C2</strong></th>
              <th style="text-align:center;">
              <strong>C3</strong></th>
              <th style="text-align:center;">
              <strong>C4</strong></th>
              <th style="text-align:center;"><strong>pearson's
              r</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class=
              "tex">$(sCG/\#queries)_U$</span></span></td>
              <td style="text-align:center;">⊗</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">⊗</td>
              <td style="text-align:center;">0.26<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class=
              "tex">$(sCG/\#queries)_R$</span></span></td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">⊗</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">⊗</td>
              <td style="text-align:center;">0.29<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <em>success<sub>m</sub></em></td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">⊗</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">0.38<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <em>success<sub>p</sub></em></td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">√</td>
              <td style="text-align:center;">0.51<sup>**</sup></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>Table <a class="tbl" href="#tab5">5</a> shows how
      different metrics satisfy the three criteria, and the
      corresponding correlations with search success.
      (<em>sCG</em>/#<em>queries</em>) <sub><em>U</em></sub> is
      average search outcome per query based on usefulness and
      (<em>sCG</em>/#<em>queries</em>) <sub><em>R</em></sub> is
      average search outcome per query based on relevance, previous
      studies show that they have a strong correlation with
      satisfaction [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>]. (<em>sCG</em>/#<em>queries</em>)
      <sub><em>U</em></sub> does not satisfy <strong>C1</strong>,
      because a document with low potential gain and high
      usefulness will increase this metric.
      (<em>sCG</em>/#<em>queries</em>) <sub><em>R</em></sub> does
      not satisfy <strong>C2</strong>, because a document with high
      potential gain should be relevant, so that it will increase
      this metric no matter it is considered useful or not. Both of
      (<em>sCG</em>/#<em>queries</em>) <sub><em>U</em></sub> and
      (<em>sCG</em>/#<em>queries</em>) <sub><em>R</em></sub> does
      not satisfy <strong>C4</strong>, because the correct
      information which users have already known will still
      increase to these two metrics. <em>Success<sub>p</sub></em>
      satisfies all four criteria. For comparison, we proposed
      <em>Success<sub>m</sub></em> which only considers the
      potential gain without the usefulness feedback and can be
      computed with Equation(<a class="eqn" href="#eq5">5</a>). It
      represents the maximum achievable search success from the
      clicked documents, it can be obtained from Equation(<a class=
      "eqn" href="#eq3">3</a>) if we let the <em>U<sub>j</sub></em>
      = 1. <em>Success<sub>m</sub></em> does not satisfy the
      <strong>C2</strong> for the same reason of
      (<em>sCG</em>/#<em>queries</em>) <sub><em>R</em></sub> .
      Results show that search success has a weak positive
      correlation with (<em>sCG</em>/#<em>queries</em>)
      <sub><em>U</em></sub> (<em>r</em> = 0.26) and
      (<em>sCG</em>/#<em>queries</em>) <sub><em>R</em></sub>
      (<em>r</em> = 0.29). There is also a weak correlation between
      search success and <em>success<sub>m</sub></em> (<em>r</em> =
      0.38). In comparison, a moderate correlation has been
      observed between the search success and
      <em>success<sub>p</sub></em> (<em>r</em> = 0.51).</p>
      <div class="table-responsive" id="eq5">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} success_m = \sum
          _{i=1}^m max(E_{ij}) * g_i \end{equation}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>
      <p></p>
      <p>In summary, user's search success has a higher correlation
      with the proposed metrics (<em>success<sub>p</sub></em> and
      <em>success<sub>m</sub></em> ) than the metrics
      ((<em>sCG</em>/#<em>queries</em>) <sub><em>u</em></sub> and
      (<em>sCG</em>/#<em>queries</em>) <sub><em>r</em></sub> ) that
      were designed to estimate user satisfaction. This indicates
      that there exists a discrepancy between user satisfaction and
      search success, as they can be reflected by different
      metrics. The <em>success<sub>p</sub></em> has the highest
      correlation with search success which not only shows its own
      validity in estimating search success but also supports our
      assumptions that the success-oriented metrics should meet all
      of the four criteria (<strong>C1 − C4</strong>). That is to
      say, it is necessary to consider both usefulness feedback and
      potential gain when estimating search success. This further
      confirms our findings in Section 4, that only a document with
      high potential gain and is considered useful will contribute
      to search success.</p>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Search Success
          Prediction</h2>
        </div>
      </header>
      <div class="table-responsive" id="tab6">
        <div class="table-caption">
          <span class="table-number">Table 6:</span> <span class=
          "table-title">Features adopted in search success
          prediction.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;">Group</th>
              <th style="text-align:left;">Features</th>
              <th style="text-align:left;">PCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;">Behavior Features</td>
              <td style="text-align:left;">
              <em>ClickDwell<sub>min</sub></em></td>
              <td style="text-align:left;">0.23<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>ClickDwell<sub>max</sub></em></td>
              <td style="text-align:left;">0.05</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>ClickDwell<sub>sum</sub></em></td>
              <td style="text-align:left;">0.06</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>ClickDwell<sub>avg</sub></em></td>
              <td style="text-align:left;">0.15</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryDwell<sub>min</sub></em></td>
              <td style="text-align:left;">0.09</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryDwell<sub>max</sub></em></td>
              <td style="text-align:left;">0.09</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryDwell<sub>sum</sub></em></td>
              <td style="text-align:left;">-0.06</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryDwell<sub>avg</sub></em></td>
              <td style="text-align:left;">0.11</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryLength<sub>min</sub></em></td>
              <td style="text-align:left;">-0.02</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryLength<sub>max</sub></em></td>
              <td style="text-align:left;">-0.20<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryLength<sub>sum</sub></em></td>
              <td style="text-align:left;">-0.26<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>QueryLength<sub>avg</sub></em></td>
              <td style="text-align:left;">-0.13</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class=
              "tex">$\#Query$</span></span></td>
              <td style="text-align:left;">-0.24<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class=
              "tex">$\#SatClick_{T{\gt}30}$</span></span></td>
              <td style="text-align:left;">0.09</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><em>SatClickRatio</em>
              <sub><em>T</em> &gt; 30</sub></td>
              <td style="text-align:left;">0.16<sup>*</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class=
              "tex">$\#DsatClick_{T{\lt}10}$</span></span></td>
              <td style="text-align:left;">-0.09</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><em>DsatClickRatio</em>
              <sub><em>T</em> &lt; 10</sub></td>
              <td style="text-align:left;">-0.17<sup>*</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">Annotation Features</td>
              <td style="text-align:left;">
              <em>Relevance<sub>max</sub></em></td>
              <td style="text-align:left;">0.32<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Relevance<sub>avg</sub></em></td>
              <td style="text-align:left;">0.22<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Credibility<sub>max</sub></em></td>
              <td style="text-align:left;">0.03</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Credibility<sub>avg</sub></em></td>
              <td style="text-align:left;">0.03</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Readability<sub>max</sub></em></td>
              <td style="text-align:left;">0.10</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Readability<sub>avg</sub></em></td>
              <td style="text-align:left;">0.04</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class=
              "tex">$(sCG/\#queries)_R$</span></span></td>
              <td style="text-align:left;">0.29<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class=
              "tex">$(sDCG/\#queries)_R$</span></span></td>
              <td style="text-align:left;">0.27<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">Combined Features</td>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class="tex">$\#{\it
              Useful}_{{\gt}1}\&amp;\;{\it
              Gain}_{{\gt}0.2}$</span></span></td>
              <td style="text-align:left;">0.37<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Success<sub>m</sub></em></td>
              <td style="text-align:left;">0.38<sup>**</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">
              <em>Success<sub>p</sub></em></td>
              <td style="text-align:left;">0.51<sup>**</sup></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>In this section, we use different categories of features
      to predict the search success to answer <strong>RQ3</strong>.
      We regard the search success prediction as a regression
      problem and evaluate the effectiveness of the regression
      models in terms of the correlations between the model
      predictions and the search success derived from participants’
      answers.</p>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span>
            Features</h3>
          </div>
        </header>
        <p>All features are listed in Table&nbsp;<a class="tbl"
        href="#tab6">6</a> and are categorized into three groups:
        Behavior features, Annotation features, and Combined
        features. Previous studies&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0014">14</a>] shows that the
        subjective search satisfaction can be effectively predicted
        by a set of user behavior features, so we include this
        features as Behavior features and test whether they are
        also effective in predicting the objective search success.
        In Section 4, we show that the credibility and readability
        of documents can affect user's usefulness judgment and
        therefore influence search success, so we include them in
        Annotation features. We also use the success-oriented
        metrics proposed in section 5 as Combined features because
        they are substantially correlated with search success.</p>
        <p><strong>Behavior features</strong> can be derived from
        search behavior logs and capture how users interact with
        the search engine. We adopt some features from previous
        studies&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>]. ClickDwell and QueryDwell are the
        click dwell time at document-level and query-level. Query
        length and Sat/Dsat clicks are also adopted in our study.
        We use the minimum, maximum, average, and sum values of the
        these measures in a session as features. From
        Table&nbsp;<a class="tbl" href="#tab6">6</a> we can find
        that the minimum dwell time has a positive correlation with
        search success. There is a negative correlation between
        search success and maximum query length, total query
        length, and the number of issuing queries. These features
        may reflect that the user is struggling in completing the
        search tasks, and therefore associate with a lower search
        success level. We also observe that search success only has
        a weak positive correlation with Sat clicks and a weak
        negative correlation with Dsat clicks, which indicates that
        Sat/Dsat clicks can not fully determine the overall search
        success.</p>
        <p><strong>Annotation features</strong> are based on
        annotations from external annotators. We collected
        relevance, credibility, and readability annotation of
        clicked documents and use the maximum and average value of
        these measures as features. The session cumulated gain
        (sCG) and session discount cumulated gain
        (sDCG)&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0013">13</a>] are widely used in search session
        evaluation. We compared different versions and chose
        average CG and DCG of all queries, denoted as <span class=
        "inline-equation"><span class=
        "tex">$(sCG/\#queries)_R$</span></span> and <span class=
        "inline-equation"><span class=
        "tex">$(sDCG/\#queries)_R$</span></span> , as annotation
        features in our study. From table&nbsp;<a class="tbl" href=
        "#tab6">6</a> we can find that the features related to
        credibility and readability do not significantly correlate
        with search success, indicating these two factors do not
        affect search success directly. As shown in Section 4.4,
        the credibility and readability of a clicked document has
        complex effects on usefulness and search success. We also
        see that features relate to relevance, including
        <span class="inline-equation"><span class=
        "tex">$(sCG/\#queries)_R$</span></span> and <span class=
        "inline-equation"><span class=
        "tex">$(sDCG/\#queries)_R$</span></span> , have a positive
        correlation with search success, which suggests that a
        relevant clicked document has a positive contribution to
        search success.</p>
        <p><strong>Combined features</strong> are derived from our
        findings that only the documents that contain right
        information and have been considered useful by the user
        will contribute to search success. To compute these
        features, we need to collect both the usefulness feedbacks
        from users and key point annotations from assessors.
        Table&nbsp;<a class="tbl" href="#tab6">6</a> shows that all
        of them have a significantly positive correlation with
        search success. Compared to Behavior features and
        Annotation features, the correlations are stronger.
        Especially, the correlation of <em>success<sub>p</sub></em>
        is significantly higher than all other features.</p>
      </section>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span> Prediction
            Results</h3>
          </div>
        </header>
        <p>We frame the search success prediction as a supervised
        regression problem and regard the search success calculated
        from users answer as the target value of the regression
        model. We predict the search success based on the features
        mentioned in Section 6.1 and we perform a 5 fold
        cross-validation to evaluate the performance of the
        regression model. We tried different learning models, such
        as Linear Regression, Support Vector Machine, and Gradient
        Boosting Regression Tree&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>]. The Linear Regression model
        performs the best so that we only present the results of
        this model in this paper.</p>
        <div class="table-responsive" id="tab7">
          <div class="table-caption">
            <span class="table-number">Table 7:</span> <span class=
            "table-title">Results for search success prediction
            (*/** indicates statistical significance at
            <em>p</em>&lt;0.05/0.01 level).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">
                <strong>Features</strong></th>
                <th style="text-align:left;">
                <strong>PCC</strong></th>
                <th style="text-align:left;">
                <strong>MSE</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Behavior</td>
                <td style="text-align:left;">0.26<sup>**</sup></td>
                <td style="text-align:left;">0.046</td>
              </tr>
              <tr>
                <td style="text-align:center;">Annotation</td>
                <td style="text-align:left;">0.28<sup>**</sup></td>
                <td style="text-align:left;">0.046</td>
              </tr>
              <tr>
                <td style="text-align:center;">Behavior +
                Annotation</td>
                <td style="text-align:left;">0.32<sup>**</sup></td>
                <td style="text-align:left;">0.045</td>
              </tr>
              <tr>
                <td style="text-align:center;">Combined</td>
                <td style="text-align:left;">0.49<sup>**</sup></td>
                <td style="text-align:left;">0.038</td>
              </tr>
              <tr>
                <td style="text-align:center;">Behavior +
                Annotation + Combined</td>
                <td style="text-align:left;">0.51<sup>**</sup></td>
                <td style="text-align:left;">0.037</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>As shown in table&nbsp;<a class="tbl" href=
        "#tab7">7</a>, different combinations of features has been
        tried. We measure the Pearson's <em>r</em> and Mean Squared
        Error (MSE) between predicted search success and true
        search success to evaluate the performance of the
        prediction models. The results show that Behavior features
        and Annotation features have limited power in search
        success prediction, in spite that they are effective in
        user satisfaction prediction as shown in previous studies.
        Comparing to using Behavior features and Annotation
        features, we can achieve a significantly better prediction
        performance with only Combined features (<em>r</em> =
        0.49). This performance is even comparable to the
        performance when we combine all the features (<em>r</em> =
        0.51).</p>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.3</span> Results
            Discussion</h3>
          </div>
        </header>
        <p>It is notable that both Behavior features and Annotation
        features have limited efficacy in search success
        prediction. We would expect this because on the one hand,
        although previous studies show that a user's behaviors can
        reflect his subjective feelings of satisfaction, this
        feelings may be inconsistent with the objective search
        success, especially when completing a complex search task.
        On the other hand, the annotations are objective measures
        of clicked documents. But once a user does not perceive the
        information contained in a document, the measures will not
        reflect the real information gain.</p>
        <p>In comparison, the Combined features take into account
        both the subjective and objective factors of clicked
        documents. That is to say, only when correct information
        has perceived by a user, the information will contribute to
        his search success. The prediction results and the analysis
        on the effectiveness of features further demonstrate that
        compared to user satisfaction, search success is determined
        by a rather different mechanism.</p>
      </section>
    </section>
    <section id="sec-23">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Conclusion</h2>
        </div>
      </header>
      <p>For complex search procedure such as exploratory search,
      there is a discrepancy between user satisfaction and search
      success. A search procedure that a user feels satisfied but
      not succeed can be very harmful. Therefore, it is necessary
      to consider the search success, along with user
      satisfaction.</p>
      <p>In this study, we conducted two laboratory studies in
      which we collected explicit usefulness and satisfaction
      feedback from users, and detail annotations of clicked
      documents from external assessors. We investigated the
      discrepancy between user satisfaction and search success
      through one-way ANOVAs and found that the inconsistency
      between usefulness and potential gain can lead to the
      discrepancy. We adopted two-way ANOVAs to analyze the factors
      that influence users’ usefulness judgements. Results show
      that both subjective and objective factors have impacts on
      users’ judgements. Based on our analyses, we organized four
      reasonable criteria that a metric should satisfy for
      estimating search success. We proposed a framework which
      satisfies all four criteria, and showed that our metric
      outperforms some existing evaluation metrics. Finally, we
      adopted linear regression models to predict search success
      with different feature combinations. Results show that search
      success can be predicted more accurately with Combined
      features than Behavioral features and simple Annotation
      features.</p>
      <p>As for future work, we would like to verify our findings
      and improve our method on a larger scale dataset. The impact
      of negative results on search success, which have not been
      considered in this paper, will be further investigated. The
      novelty, credibility, and readability of information will be
      included in our metric in future work.</p>
    </section>
    <section id="sec-24">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span>
          Acknowledgments</h2>
        </div>
      </header>
      <p>This work is supported by Natural Science Foundation of
      China (Grant No. 61622208, 61732008, 61532011) and National
      Key Basic Research Program (2015CB358700).</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Mikhail Ageev, Qi Guo,
        Dmitry Lagun, and Eugene Agichtein. 2011. Find it if you
        can: a game for modeling different types of web search
        success using interaction data. In <em><em>Proceeding of
        the 34th International ACM SIGIR Conference on Research and
        Development in Information Retrieval, SIGIR 2011, Beijing,
        China, July 25-29, 2011</em></em> . 345–354. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2009916.2009965" target="_blank">
          https://doi.org/10.1145/2009916.2009965</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">Azzah Al-Maskari, Mark
        Sanderson, and Paul&nbsp;D. Clough. 2007. The relationship
        between IR effectiveness measures and user satisfaction. In
        <em><em>SIGIR 2007: Proceedings of the 30th Annual
        International ACM SIGIR Conference on Research and
        Development in Information Retrieval, Amsterdam, The
        Netherlands, July 23-27, 2007</em></em> . 773–774.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1145/1277741.1277902" target=
          "_blank">https://doi.org/10.1145/1277741.1277902</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Rashid Ali and Mirza
        Mohd.&nbsp;Sufyan Beg. 2011. An overview of Web search
        evaluation methods. <em><em>Computers &amp; Electrical
        Engineering</em></em> 37, 6 (2011), 835–848. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1016/j.compeleceng.2011.10.005"
          target="_blank">https://doi.org/10.1016/j.compeleceng.2011.10.005</a>
        </li>
        <li id="BibPLXBIB0004" label="[4]">Ahmed&nbsp;Hassan
        Awadallah, Rosie Jones, and Kristina&nbsp;Lisa Klinkner.
        2010. Beyond DCG: user behavior as a predictor of a
        successful search. In <em><em>Proceedings of the Third
        International Conference on Web Search and Web Data Mining,
        WSDM 2010, New York, NY, USA, February 4-6, 2010</em></em>
        . 221–230. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/1718487.1718515" target="_blank">
          https://doi.org/10.1145/1718487.1718515</a>
        </li>
        <li id="BibPLXBIB0005" label="[5]">Ahmed&nbsp;Hassan
        Awadallah, Ryen&nbsp;W. White, Susan&nbsp;T. Dumais, and
        Yi-Min Wang. 2014. Struggling or exploring?: disambiguating
        long search sessions. In <em><em>Seventh ACM International
        Conference on Web Search and Data Mining, WSDM 2014, New
        York, NY, USA, February 24-28, 2014</em></em> . 53–62.
        <a class="link-inline force-break" href=
        "https://doi.org/10.1145/2556195.2556221" target="_blank">
          https://doi.org/10.1145/2556195.2556221</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">Charles L.&nbsp;A.
        Clarke, Maheedhar Kolla, Gordon&nbsp;V. Cormack, Olga
        Vechtomova, Azin Ashkan, Stefan Büttcher, and Ian
        MacKinnon. 2008. Novelty and diversity in information
        retrieval evaluation. In <em><em>Proceedings of the 31st
        Annual International ACM SIGIR Conference on Research and
        Development in Information Retrieval, SIGIR 2008,
        Singapore, July 20-24, 2008</em></em> . 659–666.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1145/1390334.1390446" target=
          "_blank">https://doi.org/10.1145/1390334.1390446</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Henry&nbsp;Allen Feild,
        James Allan, and Rosie Jones. 2010. Predicting searcher
        frustration. In <em><em>Proceeding of the 33rd
        International ACM SIGIR Conference on Research and
        Development in Information Retrieval, SIGIR 2010, Geneva,
        Switzerland, July 19-23, 2010</em></em> . 34–41.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1145/1835449.1835458" target=
          "_blank">https://doi.org/10.1145/1835449.1835458</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Steve Fox, Kuldeep
        Karnawat, Mark Mydland, Susan&nbsp;T. Dumais, and Thomas
        White. 2005. Evaluating implicit measures to improve web
        search. <em><em>ACM Trans. Inf. Syst.</em></em> 23, 2
        (2005), 147–168. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/1059981.1059982" target="_blank">
          https://doi.org/10.1145/1059981.1059982</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">Jerome&nbsp;H Friedman.
        2001. Greedy function approximation: a gradient boosting
        machine. <em><em>Annals of statistics</em></em> (2001),
        1189–1232.</li>
        <li id="BibPLXBIB0010" label="[10]">Qi Guo, Shuai Yuan, and
        Eugene Agichtein. 2011. Detecting success in mobile search
        from interaction. In <em><em>Proceeding of the 34th
        International ACM SIGIR Conference on Research and
        Development in Information Retrieval, SIGIR 2011, Beijing,
        China, July 25-29, 2011</em></em> . 1229–1230. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2009916.2010133" target="_blank">
          https://doi.org/10.1145/2009916.2010133</a>
        </li>
        <li id="BibPLXBIB0011" label="[11]">Chathra Hendahewa and
        Chirag Shah. 2017. Evaluating user search trails in
        exploratory search tasks. <em><em>Inf. Process.
        Manage.</em></em> 53, 4 (2017), 905–922. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1016/j.ipm.2017.04.001" target=
        "_blank">https://doi.org/10.1016/j.ipm.2017.04.001</a>
        </li>
        <li id="BibPLXBIB0012" label="[12]">Scott&nbsp;B. Huffman
        and Michael Hochster. 2007. How well does result relevance
        predict session satisfaction?. In <em><em>SIGIR 2007:
        Proceedings of the 30th Annual International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval, Amsterdam, The Netherlands, July 23-27,
        2007</em></em> . 567–574. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/1277741.1277839" target="_blank">
          https://doi.org/10.1145/1277741.1277839</a>
        </li>
        <li id="BibPLXBIB0013" label="[13]">Kalervo Järvelin,
        Susan&nbsp;L. Price, Lois M.&nbsp;L. Delcambre, and
        Marianne&nbsp;Lykke Nielsen. 2008. Discounted Cumulated
        Gain Based Evaluation of Multiple-Query IR Sessions. In
        <em><em>Advances in Information Retrieval , 30th European
        Conference on IR Research, ECIR 2008, Glasgow, UK, March
        30-April 3, 2008. Proceedings</em></em> . 4–15. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1007/978-3-540-78646-7_4" target=
        "_blank">https://doi.org/10.1007/978-3-540-78646-7_4</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Jiepu Jiang,
        Ahmed&nbsp;Hassan Awadallah, Xiaolin Shi, and Ryen&nbsp;W.
        White. 2015. Understanding and Predicting Graded Search
        Satisfaction. In <em><em>Proceedings of the Eighth ACM
        International Conference on Web Search and Data Mining,
        WSDM 2015, Shanghai, China, February 2-6, 2015</em></em> .
        57–66. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/2684822.2685319" target="_blank">
          https://doi.org/10.1145/2684822.2685319</a>
        </li>
        <li id="BibPLXBIB0015" label="[15]">Jiepu Jiang, Daqing He,
        and James Allan. 2017. Comparing In Situ and
        Multidimensional Relevance Judgments. In
        <em><em>Proceedings of the 40th International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval, Shinjuku, Tokyo, Japan, August 7-11,
        2017</em></em> . 405–414. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/3077136.3080840" target="_blank">
          https://doi.org/10.1145/3077136.3080840</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Diane Kelly. 2009.
        Methods for Evaluating Interactive Information Retrieval
        Systems with Users. <em><em>Foundations and Trends in
        Information Retrieval</em></em> 3, 1-2(2009), 1–224.
        <a class="link-inline force-break" href=
        "https://doi.org/10.1561/1500000012" target=
        "_blank">https://doi.org/10.1561/1500000012</a>
        </li>
        <li id="BibPLXBIB0017" label="[17]">Youngho Kim,
        Ahmed&nbsp;Hassan Awadallah, Ryen&nbsp;W. White, and Imed
        Zitouni. 2014. Modeling dwell time to predict click-level
        satisfaction. In <em><em>Seventh ACM International
        Conference on Web Search and Data Mining, WSDM 2014, New
        York, NY, USA, February 24-28, 2014</em></em> . 193–202.
        <a class="link-inline force-break" href=
        "https://doi.org/10.1145/2556195.2556220" target="_blank">
          https://doi.org/10.1145/2556195.2556220</a>
        </li>
        <li id="BibPLXBIB0018" label="[18]">J&nbsp;Richard Landis
        and Gary&nbsp;G Koch. 1977. The measurement of observer
        agreement for categorical data.
        <em><em>biometrics</em></em> (1977), 159–174.</li>
        <li id="BibPLXBIB0019" label="[19]">Xin Li, Yiqun Liu,
        Rongjie Cai, and Shaoping Ma. 2017. Investigation of User
        Search Behavior While Facing Heterogeneous Search Services.
        In <em><em>Proceedings of the Tenth ACM International
        Conference on Web Search and Data Mining, WSDM 2017,
        Cambridge, United Kingdom, February 6-10, 2017</em></em> .
        161–170. <a class="link-inline force-break" href=
        "http://dl.acm.org/citation.cfm?id=3018673" target=
        "_blank">http://dl.acm.org/citation.cfm?id=3018673</a>
        </li>
        <li id="BibPLXBIB0020" label="[20]">Chang Liu, Jingjing
        Liu, Michael Cole, Nicholas&nbsp;J Belkin, and Xiangmin
        Zhang. 2012. Task difficulty and domain knowledge effects
        on information search behaviors. <em><em>Proceedings of the
        Association for Information Science and
        Technology</em></em> 49, 1(2012), 1–10.</li>
        <li id="BibPLXBIB0021" label="[21]">Chang Liu, Xiangmin
        Zhang, and Wei Huang. 2016. The exploration of objective
        task difficulty and domain knowledge effects on users’
        query formulation. <em><em>Proceedings of the Association
        for Information Science and Technology</em></em> 53,
        1(2016), 1–9.</li>
        <li id="BibPLXBIB0022" label="[22]">Yiqun Liu, Ye Chen,
        Jinhui Tang, Jiashen Sun, Min Zhang, Shaoping Ma, and Xuan
        Zhu. 2015. Different Users, Different Opinions: Predicting
        Search Satisfaction with Mouse Movement Information. In
        <em><em>Proceedings of the 38th International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval, Santiago, Chile, August 9-13, 2015</em></em> .
        493–502. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/2766462.2767721" target="_blank">
          https://doi.org/10.1145/2766462.2767721</a>
        </li>
        <li id="BibPLXBIB0023" label="[23]">Jiaxin Mao, Yiqun Liu,
        Ke Zhou, Jian-Yun Nie, Jingtao Song, Min Zhang, Shaoping
        Ma, Jiashen Sun, and Hengliang Luo. 2016. When does
        Relevance Mean Usefulness and User Satisfaction in Web
        Search?. In <em><em>Proceedings of the 39th International
        ACM SIGIR conference on Research and Development in
        Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21,
        2016</em></em> . 463–472. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2911451.2911507" target="_blank">
          https://doi.org/10.1145/2911451.2911507</a>
        </li>
        <li id="BibPLXBIB0024" label="[24]">Gary Marchionini. 2006.
        Exploratory search: from finding to understanding. <em><em>
          Commun. ACM</em></em> 49, 4 (2006), 41–46. <a class=
          "link-inline force-break" href=
          "https://doi.org/10.1145/1121949.1121979" target=
          "_blank">https://doi.org/10.1145/1121949.1121979</a>
        </li>
        <li id="BibPLXBIB0025" label="[25]">Daan Odijk,
        Ryen&nbsp;W. White, Ahmed&nbsp;Hassan Awadallah, and
        Susan&nbsp;T. Dumais. 2015. Struggling and Success in Web
        Search. In <em><em>Proceedings of the 24th ACM
        International Conference on Information and Knowledge
        Management, CIKM 2015, Melbourne, VIC, Australia, October
        19 - 23, 2015</em></em> . 1551–1560. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2806416.2806488" target="_blank">
          https://doi.org/10.1145/2806416.2806488</a>
        </li>
        <li id="BibPLXBIB0026" label="[26]">Frances&nbsp;A Pogacar,
        Amira Ghenai, Mark&nbsp;D Smucker, and Charles&nbsp;LA
        Clarke. 2017. The Positive and Negative Influence of Search
        Results on People's Decisions about the Efficacy of Medical
        Treatments. (2017).</li>
        <li id="BibPLXBIB0027" label="[27]">Rohail Syed and Kevyn
        Collins-Thompson. 2017. Retrieval Algorithms Optimized for
        Human Learning. In <em><em>Proceedings of the 40th
        International ACM SIGIR Conference on Research and
        Development in Information Retrieval, Shinjuku, Tokyo,
        Japan, August 7-11, 2017</em></em> . 555–564. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/3077136.3080835" target="_blank">
          https://doi.org/10.1145/3077136.3080835</a>
        </li>
        <li id="BibPLXBIB0028" label="[28]">Hongning Wang, Yang
        Song, Ming-Wei Chang, Xiaodong He, Ahmed&nbsp;Hassan
        Awadallah, and Ryen&nbsp;W. White. 2014. Modeling
        action-level satisfaction for search task satisfaction
        prediction. In <em><em>The 37th International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval, SIGIR ’14, Gold Coast , QLD, Australia - July 06
        - 11, 2014</em></em> . 123–132. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2600428.2609607" target="_blank">
          https://doi.org/10.1145/2600428.2609607</a>
        </li>
        <li id="BibPLXBIB0029" label="[29]">Ryen&nbsp;W White, Bill
        Kules, Steven&nbsp;M Drucker, <em>et al.</em> 2006.
        Supporting exploratory search, introduction, special issue,
        communications of the ACM. <em><em>Commun. ACM</em></em>
        49, 4 (2006), 36–39.</li>
        <li id="BibPLXBIB0030" label="[30]">Ryen&nbsp;W. White and
        Resa&nbsp;A. Roth. 2009. <em><em>Exploratory Search: Beyond
        the Query-Response Paradigm</em></em> . Morgan &amp;
        Claypool Publishers. <a class="link-inline force-break"
        href="https://doi.org/10.2200/S00174ED1V01Y200901ICR003"
        target=
        "_blank">https://doi.org/10.2200/S00174ED1V01Y200901ICR003</a>
        </li>
        <li id="BibPLXBIB0031" label="[31]">Eileen Wood, Domenica
        De&nbsp;Pasquale, Julie&nbsp;Lynn Mueller, Karin Archer,
        Lucia Zivcakova, Kathleen Walkey, and Teena Willoughby.
        2016. Exploration of the relative contributions of domain
        knowledge and search expertise for conducting internet
        searches. <em><em>The Reference Librarian</em></em> 57, 3
        (2016), 182–204.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>This data set
    will be open to public after the double-blind review
    process</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186065">https://doi.org/10.1145/3178876.3186065</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
