<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Hybrid.AI: A Learning Search Engine for Large-scale Structured Data</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Hybrid.AI: A Learning Search Engine for Large-scale Structured Data</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Sean</span>      <span class="surName">Soderman</span>     Department of Computer Science, University of Texas at San Antonio     </div>     <div class="author">     <span class="givenName">Anusha</span>      <span class="surName">Kola</span>     Department of Computer Science, University of Texas at San Antonio     </div>     <div class="author">     <span class="givenName">Maksim</span>      <span class="surName">Podkorytov</span>     Department of Computer Science, University of Texas at San Antonio     </div>     <div class="author">     <span class="givenName">Michael</span>      <span class="surName">Geyer</span>     Department of Computer Science, University of Texas at San Antonio     </div>     <div class="author">     <span class="givenName">Michael</span>      <span class="surName">Gubanov</span>     Department of Computer Science, University of Texas at San Antonio     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191600" target="_blank">https://doi.org/10.1145/3184558.3191600</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Variety of Big data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0044">44</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0047">47</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0052">52</a>] is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information.</small>     </p>     <p>     <small>Here we describe <strong>Hybrid.AI</strong>, a learning search engine for large-scale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0033">33</a>] to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting <em>99 queries</em> and their results from users, and observe significant relevance gain.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Sean Soderman, Anusha Kola, Maksim Podkorytov, Michael Geyer, and Michael Gubanov. 2018. Hybrid.AI: A Learning Search Engine for Large-scale Structured Data. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018 (WWW &#x2019;18 Companion),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3191600" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191600</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>With the advent of large scale data management systems, data scientists and analysts have more information at their disposal than ever before. This influx of data makes retrieval of needed information challenging [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>]. Consider a data scientist working with structured data who has a recently mined large scale Web table dataset. If s/he wanted to enrich her information concerning weather in her region, s/he might be inclined to use keyword search in order to find the best records of interest in the dataset. However, a standard keyword-search over structured data, by nature, may provide inaccurate or incomplete search results, even when using sophisticated ranking functions, due to mismatches of relevant information to the query or the presence of relevant terms in irrelevant data rows [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. Also, most structured data search engines return entire tables instead of the most relevant rows fused from many tables.</p>    <p>Although the properties of human language and text are the root cause of these issues, it is possible to decrease them by analyzing the semantic properties of data. This is what we do in <SmallCap>Hybrid.AI</SmallCap>, an intelligent search engine that automatically generates machine learning classifiers to identify similar data tuples to return more relevant search results compared to standard keyword search.</p>    <p>In order to generate the aforementioned classifiers, we make use of user-specified keywords to search for rows from tables that contain these keywords as attributes. These keywords are meant to be correlated with a certain object, for example, if we wanted to create a classifier for &#x201C;jobs&#x201D;, our system would generate training data using keywords such as &#x201C;salary&#x201D;, &#x201C;date&#x201D;, and &#x201C;position&#x201D;. Once this is done, we automatically train the classifier, then use it to cluster table rows that are likely to be job-oriented. Finally, we extract <em>core attributes</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] from this set of classified rows. Informally <em>core attributes</em> are the most important attributes of an object (e.g. wings for a bird) a critical component of UFOs, a data structure used for fusing similar structured data objects that are represented in different ways [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. We use <em>core attributes</em> as soft constraints, improving the rank of results that are related to a specific object of interest. With this method, we get more relevant results, compared to the standard retrieval and ranking schemes for keyword search over structured data.</p>    <p>Our contributions in this paper are the following:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>Machine-learning augmented Information Retrieval Scheme for Large-scale Structured Data</strong>: We propose a new fusion-based information retrieval scheme for <em>structured</em> data that leverages machine learning classifiers and Unified Famous Objects (UFO) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. We extensively evaluate it over <em>99 queries</em>, using purely keyword-based retrieval as a baseline, on a large scale structured data set having millions of Web tables and observe significant retrieval relevance gain.<br/></li>     <li id="list2" label="&#x2022;"><strong>A Learning Search Engine for Large-scale Structured Data</strong> - <strong>Hybrid.AI</strong>, marrying <em>keyword-search</em> with generated machine learning classifiers.<br/></li>    </ul>    <p>We are not the first who made an attempt to search Web tables. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] describes techniques borrowed from Web search to index and search Web tables. Another recent project focused on Web table search was [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>], however, their domain was question answering instead of generalized search over structured data.</p>    <p>The rest of the paper is organized as follows. Section <a class="sec" href="#sec-3">2</a> discusses related work. Section <a class="sec" href="#sec-5">3</a> describes the system architecture and classifier generation. Section <a class="sec" href="#sec-13">4</a> describes a search scenario &#x0026; classification scenario that illustrate our system. <em>UFOs</em>: Unified Famous Objects, are a structure used to abstract away differences in data representation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. See Section <a class="sec" href="#sec-16">5</a> for a more in-depth discussion on UFOs. To rank tuples from a large-scale corpus of Web tables from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>], we designed and evaluated several ranking functions <em>optimized</em> for large-scale structured data, making use of the best one for our system. Refer to Section <a class="sec" href="#sec-17">6</a> for more details on ranking. To evaluate our ranking, we compare it to an enhanced version of a standard ranking function popular in structured data search [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>] in Section <a class="sec" href="#sec-20">7</a>. Future work is described in Section . We conclude in Section <a class="sec" href="#sec-21">8</a>.</p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related work</h2>     </div>    </header>    <p>We studied a variety of relevant systems in Web-search, large-scale data management, and information extraction/retrieval. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] describes different search methods for Web tables. A key technique the authors use for improved search results is through using the <SmallCap>AcsDB</SmallCap>, a database of statistics concerning the table attributes in the corpus. It is used to compute a coherency score that uses PMI, Pointwise Mutual Information, which rewards items more likely to be paired together. The major difference between their system and our own is that it is ranking entire tables, whereas ours ranks each row (potentially originating from different tables) individually. This allows us to get the most relevant tuples from the entire corpus and compose it into a concise result set. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>] describe a question answering (Q&#x0026;A) system for Web tables. This system retrieves individual cells from tables to answer a limited class of questions. For example, for the question - <em>what language do people in France speak</em>, it identifies the column <em>main language</em> in the table countries as the most relevant and lists languages from it. The authors also evaluate the effectiveness of their system using precision and recall.</p>    <p>We evaluate our system using nDCG [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] rather than precision and recall, because our system is a search-engine, not a Q&#x0026;A system. nDCG takes not only precision and recall, but also <em>ordering</em> of the results into account, which is very important for a search-engine.</p>    <p>DBxplorer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] is a keyword-search engine for relational databases that supports conjunctive keyword queries. It can also join tables, creating rows that contain <em>all</em> keywords from a search query, as well as attributes from different tables in this join result. This contrasts with our approach, which uses a more flexible <em>disjunctive</em> keyword search, fuses, and ranks tuples by relevance from millions of Web tables having different schemas. <em>Disjunctive</em> keyword-search allows retrieval of possibly relevant rows, despite not containing <em>all</em> search terms used. DBxplorer ranks rows by using the number of joins involved in the generation of a row rather than using weights of terms from the query and the tuples or more advanced techniques, outlined in Section <a class="sec" href="#sec-17">6</a>. The authors&#x2019; reasoning behind using the number of joins for ranking is that tables generated from several joins are harder to comprehend ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], Section 6.2). This is somewhat similar to using keyword-proximity to help with ranking because tables must be joined until all keywords are present, however it does not consider stronger signals for relevance in search results.</p>    <p>[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] developed a ranking system for database queries. Rather than returning all tuples that satisfy a query, it calculates the <em>top-k</em> relevant tuples. To calculate relevance, it uses a global score that depends on user preferences, as well as a conditional score that considers the correlations between specified and unspecified terms in a query. In contrast, our ranking functions are dependent on the terms in the query and the relevance score of a tuple from the Web table.</p>    <p>In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], a framework for defining the relatedness of tables based on whether or not they can be joined, as well as algorithms for detecting related tables that can be unioned or joined, was created. This was done to enable the retrieval of tables related to an input table, using this input table as a query rather than keywords. The authors defined two definitions for relatedness: <em>entity complement</em> and <em>schema complement</em>.</p>    <p>The former states that two tables <em>T</em>     <sub>1</sub> and <em>T</em>     <sub>2</sub>, derived from a possibly nonexistent table <em>T</em>, must be created with selections over the same set of attributes in <em>T</em>, using different predicates. In addition to this, the combination of the selected tuples in <em>T</em>     <sub>1</sub> and <em>T</em>     <sub>2</sub> must consist of everything in <em>T</em>. Lastly, any projections from these selections must be on the same sets of attributes, as well as include the <em>subject columns</em>, which define the entity being described in the table.</p>    <p>An example of this would be a table about car models. If table <em>T</em>     <sub>1</sub> had the attributes &#x201C;model&#x201D;, &#x201C;make&#x201D;, and &#x201C;year&#x201D;, and the other table <em>T</em>     <sub>2</sub> had attributes &#x201C;name&#x201D;, &#x201C;warranty&#x201D; and &#x201C;horsepower&#x201D;, it would be feasible that both of these tables were projected from a table containing all of these attributes, with some attribute expressing &#x201C;name&#x201D; or &#x201C;model&#x201D; since they are synonymous. To ensure that this measure of table relatedness makes sense, the authors also ensured the <em>coherency</em> of the virtual table. In order to do this, they ensured that the entities within <em>T</em>     <sub>1</sub> and <em>T</em>     <sub>2</sub> were of the same type, such as &#x201C;name&#x201D; and &#x201C;author&#x201D; both being identifiers. An example of an incoherent table would be one storing information about baseball cards and theater showtimes. Such a table would have a low coherency score. However, a table storing information about the 2016 and 2017 NBA championships would be deemed sensible.</p>    <p>Concerning <em>schema complement</em>, the two tables <em>T</em>     <sub>1</sub> and <em>T</em>     <sub>2</sub> would have to be created using queries <em>Q</em>     <sub>1</sub> and <em>Q</em>     <sub>2</sub> that have a similar structure. These queries must be in the form of a projection, additionally, they must select attributes so that <em>T</em>     <sub>1</sub> and <em>T</em>     <sub>2</sub> have at least one attribute not in common with one another, as well as at least one <strong>in</strong> common. Finally, the union of these two sets of project attributes must consist of the attributes in the virtual table <em>T</em>.</p>    <p>We create tables of related content using classifiers, instead of inferring whether tables could be <em>coherently</em> joined or unioned. Due to this, we do not employ such an artificial relatedness measure. We do not focus on ranking related tables, since we rank single tuples at a time in order to form a concise result set from thousands of tables. Also, because of our focus on single tuples, we have no use for inferring whether two results could be formed from similar SQL queries. Their objective of enhancing a user&#x0027;s tables is also different from our vision of unified retrieval of the most relevant tuples from the entire corpus. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] developed a Web table service built on top of data derived from Microsoft&#x0027;s Bing Web search engine. Their search service uses machine learning to identify the entity column, which contains entities described by the values of other attributes in the table. This is done for queries such as &#x201C;population of San Antonio&#x201D; and &#x201C;population of Bexar county&#x201D;, since both queries may match the same row even though the table may only describe the population of one of these entities. The authors also use static features such as number of rows and PageRank to aid in ranking tables, in addition to a feature based on the cell placement and column/row frequency of certain keyword matches. It is difficult to determine how effective their web table ranking is, since they do not provide an evaluation on it. We provide a thorough evaluation of our ranking in Section <a class="sec" href="#sec-20">7</a> using <em>nDCG</em>, a widely-used metric for assessing search result relevance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. Our method uses machine learning to identify rows belonging to objects of the same class, rather than identifying entities. We also rank using a combination of keyword intersection and <em>core attribute</em> matching, not taking into account the position of terms other than for our <em>keyword proximity</em> calculation. Finally, we return a combination of rows from different tables rather than an entire table at a time. This <em>consolidated</em> search result, including rows from many relevant tables, provides more relevant search results, compared to returning just one of a few entire relevant tables that may or may not have <em>all</em> rows relevant to the query.</p>    <section id="sec-4">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Unified Famous Object (UFO): Definitions &#x0026; Applications</h3>     </div>     </header>     <p>[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] developed a system that provides a unified, object-oriented way to query data from different data sources. Using UFOs hides structural differences between data sources and offers a query-able abstraction that is oblivious to this difference in metadata from different sources. It can also use the UFOs it has already constructed to identify new, familiar objects [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0048">48</a>]. This is different from our system since it is a system for <em>data integration</em> rather than <em>a search engine</em>. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] describe and evaluate algorithms for UFO [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] creation. The authors illustrate the pre-UFO technique for matching attributes, such as BuyItNowPrice to Buy-It-Now-Price, as well as more dissimilar ones such as ConvertedCurrentPrice to CurrentPrice. This was done using exact matching, tokenized matching, and an NLP-enhanced version of the tokenized matching that uses part-of-speech tagging. The authors define <em>core properties</em>, or core attributes, which are the attributes that must be present for a UFO to exist. See [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] for more details. Our system focuses on generating these sets of <em>core properties</em> using term matching to count their occurrences [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]. We use them to assert the relevance of rows within the dataset, rather than building UFOs for identifying similar objects. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>] applied UFOs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] to identify and fuse biomedical data, and used it for pre-diagnosing an early-onset Alzheimer&#x0027;s Disease. The authors demonstrated how UFOs simplified comparing a patient&#x0027;s DNA sequence with a reference sequence, resulting in a pre-diagnosis. Our system uses UFO&#x0027;s <em>core attributes</em> to boost search result rankings. Our search system also uses unstructured keyword queries rather than XQuery, translating them into SQL. Another key difference is how we return rows from many tables in a single, consolidated result.</p>    </section>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Architecture</h2>     </div>    </header>    <p>Figure <a class="fig" href="#fig1">1</a> displays an overview of our system&#x0027;s components. A brief description of each one follows. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191600/images/www18companion-339-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">       <SmallCap>Hybrid.AI</SmallCap> Architecture</span>     </div>     </figure>    </p>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Dataset</h3>     </div>     </header>     <p>We use a large-scale Web tables dataset of &#x2248; 86 million Web table tuples (Approximately 55 million non-spam tuples) from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0055">55</a>]. These instances came from Web tables pulled from sources such as online forums, social media sites, product offers, and others. They consist of data items or attributes from these web tables. We link each tuple to the web table it came from by storing an ID for that table in the tuple. We used [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0053">53</a>] to store said dataset.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Ingestion</h3>     </div>     </header>     <p>Similar to E-mail or Web pages, tables extracted from the Web also have spam (examples include empty tables, HTML formatting, junk advertisements, etc) and require cleaning before ingestion. We trained our own J48 web table spam classifier [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0055">55</a>] to filter out tables with these characteristics. Using 10-fold cross-validation, a technique for estimating model performance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] (in this case, the performance of our classifier), we observed 72.6% precision and 70.6% recall.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Generating training data with SQL</h3>     </div>     </header>     <p>After cleaning the data, once they are ingested into the parallel column store, we retrieve groups of similar objects using SQL queries. These queries are constructed from a set of keywords provided by the user (see Figure <a class="fig" href="#fig4">4</a>), such as &#x201C;album&#x201D;, &#x201C;title&#x201D;, and &#x201C;price&#x201D;, which in this case may have been chosen to generate training data for a classifier to identify song oriented data.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Generating Scalable Machine Learning Classifiers</h3>     </div>     </header>     <p>We automatically generate large-scale machine learning classifiers using the training data generated with the queries in the previous step. For example, the cluster of tables sharing attributes &#x201C;trailer&#x201D;, &#x201C;length&#x201D;, and &#x201C;director&#x201D; concerns movies, and after being trained on the data in this cluster, the classifier can identify more rows in the corpus associated with this movie data. Note that it is <strong>not</strong> necessarily the case that data positively labeled by this classifier will have the same set of attributes as the generated training data. We use <em>10-fold cross validation</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] to evaluate precision/recall of generated classifiers, observing an average of 92.5% precision and 92.1% recall across nine classes of objects (e.g. songs, job postings, blog postings, real estate postings, etc) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0051">51</a>].</p>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Metadata Classifier</h3>     </div>     </header>     <p>In order to extend our solution to be fully automated it is required that we identify those rows which contain attribute labels. That is, we must identify the <em>descriptive metadata</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] of a table. A set of 900,000 records was processed to create a vector space with 6,900 features. From these rows, the training data was gathered using a series of rules. For example, one such rule was that from within a single html table, at most one row could be considered metadata. Another rule was that rows with low word count were more likely to be metadata. This training data was then manually checked and pruned for accuracy resulting in 6,500 negative samples and 540 positive samples. Using this training data we produced a <em>Support Vector Machine</em> classifier with a <em>linear kernel</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] and a <em>one-vs-one decision function</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] to identify rows containing metadata. This model was selected to help compensate for the small training set compared to the size of our vector space. After performing <em>10-fold cross validation</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] to evaluate the precision and recall we observed an average of 80% precision and 64% recall.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.6</span> Search</h3>     </div>     </header>     <p>We designed a <em>machine-learning augmented</em> keyword search scheme using classifiers trained in the previous step to return the most relevant search results. Our ranking function uses the classifiers described in the previous paragraph, intersection, proximity, and UFOs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] to rank search results. Refer to Sections <a class="sec" href="#sec-17">6</a> and <a class="sec" href="#sec-20">7</a> for more details on the ranking algorithms and their extensive evaluation.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.7</span> Interface</h3>     </div>     </header>     <p>An interactive user interface is used for executing search queries on our Web tables corpus. It also functions as a tool for training machine learning classifiers, generating &#x0026; executing SQL queries for fetching training data (see Figure <a class="fig" href="#fig4">4</a>).</p>    </section>   </section>   <section id="sec-13">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Usage Scenarios</h2>     </div>    </header>    <p>In Figures <a class="fig" href="#fig2">2</a> and <a class="fig" href="#fig3">3</a> are screenshots of the web frontend to our search engine corresponding to our two search scenarios. In the first scenario, the user searches data with the default mode, that is, an intersection-based, keyword-proximity augmented keyword search over Web table tuples. The second scenario enhances this method using UFO <em>core attributes</em>, which were collected from classified subsets of our large-scale dataset. This is done whenever a user enters a tag, like &#x201C;:songs&#x201D;. Rather than filtering all data that does not topically match this tag, we use the attributes of web tables associated with this tag to increase the rank of results sharing those attributes. See Section <a class="sec" href="#sec-17">6</a> for more details regarding attribute-driven rank calculation.</p>    <p>After describing how our system carries out search, we illustrate how users can specify keywords, such as &#x201C;make&#x201D;, &#x201C;model&#x201D;, and &#x201C;year&#x201D;, to fetch training data with these attributes (see Figure <a class="fig" href="#fig4">4</a>). In order to use our improved ranking function in the search scenarios, we need to first train then use these classifiers to identify all rows that fall under a certain category, such as songs.</p>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Search Scenarios</h3>     </div>     </header>     <p>Here the user wishes to find articles on scientific discoveries. Using the standard popular ranking functions for structured data gives us vague search results (Figure <a class="fig" href="#fig2">2</a>). The main problem with these results is that they contain little useful information, and might actually be pointers to some articles.</p>     <p>With our UFO core-attribute augmented querying scheme in Figure <a class="fig" href="#fig3">3</a>, the user specifies a class of objects of interest in the query, here <em>articles</em>, which makes the system perform an attribute-matching algorithm after the initial keyword-based retrieval. This algorithm rewards rows that came from tables that share attributes with article-oriented data (see Section <a class="sec" href="#sec-17">6</a> for more details). <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191600/images/www18companion-339-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Search results from our large scale web-table corpus. Without a tag, we are faced with data that are most likely URLs or anchor text (rows 1 through 3) pertaining to scientific discovery, rather than titles of articles under this topic.</span>      </div>     </figure>     <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191600/images/www18companion-339-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Search results from our large scale web-table corpus. Using the tag &#x201C;articles&#x201D; improves the results drastically. We can see that the top two rows directly pertain to articles about scientific discoveries. The third is a book that debunks some scientific discoveries made by historically important scientists. Notice how each of these rows are <em>heterogeneous</em> in their structure, demonstrating the advantage of fetching individual rows as opposed to entire tables.</span>      </div>     </figure>     </p>     <p>The results in Figure <a class="fig" href="#fig3">3</a> are a major improvement over those in Figure <a class="fig" href="#fig2">2</a>. It is important to note that we do not only see an improvement in ranking (the results are actual scientific literature as opposed to what seem to be website elements), we also observe the ability of our system to return results with different attributes for the same query. This difference in attributes is apparent from the varied content of the data presented in the results in Figure <a class="fig" href="#fig3">3</a>.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Classifier Training Scenario</h3>     </div>     </header>     <p>Here, we describe how to automatically generate a machine learning classifier, identifying rows of a certain type among millions in the corpus of Web tables. The user enters a few descriptive keywords [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>] in the bottom left frame in Figure <a class="fig" href="#fig4">4</a>, for example &#x201C;make&#x201D;, &#x201C;model&#x201D;, &#x201C;year&#x201D;. We use these keywords to fetch rows corresponding to the web tables that contain them as attributes. In addition to the attribute names, the user can enter lower and upper bounds on the row-length and column-length. Row length is the number of characters in a tuple of a Web table. The column-length is defined as row-length, but with respect to a column of a Web table. Such parameters are needed since we store all of the Web tables in our corpus within a single database table, and the number of attributes as well as rows varies between tables. The user can also pick checkboxes corresponding to each column to indicate which ones the filters apply to. Our interface allows the user to select up to 7 columns in this way. Refer to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>] for more details on classifier generation.</p>     <p>Once the keywords are in and the filters are selected, the user clicks <em>Generate Training Data</em> to retrieve training data having these attributes. A sample is shown to the user in the right frame of Figure <a class="fig" href="#fig4">4</a>. Finally, the user can select the classifier type (e.g. J48, Naive Bayes) by toggling a radio-button. After that, the user clicks the <em>Generate Model</em> button, which triggers the process of training the machine-learning classifiers using these generated training data. The training set contains positively labeled training data, created as described above, and negatively labeled training data, which was made by selecting rows from the corpus without positive labels. We use 50% negative rows and 50% positive to create a balanced training set. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191600/images/www18companion-339-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Automatic generation and training of a large-scale machine learning ensemble recognizing an object of interest, given several descriptive attributes from the user (for example - make, model, year). The user can enter the keywords, click the <em>Generate Training Data</em> button that will use the keywords to generate training data. Then, clicking the <em>Generate Model</em> button will trigger the process of automatically training the classifier with the generated training data.</span>      </div>     </figure>     </p>     <p>After the model is trained, it is run to classify the Web table rows and output a sample to the user in the right frame of Figure <a class="fig" href="#fig4">4</a>. If the user is satisfied with the model performance, s/he clicks the <em>Add to schema</em> button and the model is added to the list of objects in the left frame. After that, the user can click it in the left frame, and the classified data rows will be displayed in the right frame.</p>    </section>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Unified Famous Object - Songs</h2>     </div>    </header>    <p>UFO is an abstraction introduced in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] to assist in data fusion of the same object from different data sources. An example of a UFO stored in JSON is illustrated in Listing&#x00A0;1. In this UFO for <em>Songs</em> the &#x201C;name&#x201D; attribute has a collection of its different representations accumulated from different data sources, including those in different languages.</p>    <p>With regards to our usage of <em>core attributes</em>, Listing&#x00A0;1 demonstrates a sample of the kinds of attributes we collect and use in the algorithm described in Section <a class="sec" href="#sec-17">6</a>. Attributes in different languages are still useful for gathering data that is associated with a certain object (here, &#x201C;songs&#x201D;). Refer to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] on UFO definition and more details on automatic UFO construction and data fusion using UFOs.</p>    <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191600/images/www18companion-339-graphic5.jpg" class="img-responsive" alt=""      longdesc=""/>    </p>   </section>   <section id="sec-17">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Ranking of Search Results</h2>     </div>    </header>    <p>Here, we formally describe the two ranking methods we evaluate in Section <a class="sec" href="#sec-20">7</a> below. With regards to our baseline method, we used a combination of term <em>intersection</em>, which we found to be more effective than a term-frequency based approach, as well as augmented it with keyword proximity. Thus, the effectiveness of this approach makes the challenge of improving it more difficult.</p>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Baseline Ranking</h3>     </div>     </header>     <p>We use a derivative of an intersection-based keyword ranking scheme with proximity as a baseline to compare with. It is one of the most widely-adapted popular search schemes for structured data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>], on the Web [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>], and in document search [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>].</p>     <p>     <strong>ti &#x2212; idi</strong> (<em>Term Intersection - Inverse Document Intersection</em>): Our <em>ranking function</em> for <span style="text-decoration: underline;">      <em>large-scale structured</em>     </span>     </p>     <p>     <em>      <span style="text-decoration: underline;">datasets</span> accounts for high redundancy of search keywords in arbitrary database rows, a phenomena we observed for such datasets. This could happen even when the row in question is not spam [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0055">55</a>]. An example of this would be a row containing a large amount of information concerning nobility, the word &#x201C;lady&#x201D; could appear multiple times. Such a row would expose the weakness of traditional <em>tf</em> &#x2212; <em>idf</em> ranking in the context of databases when a user issues a query like &#x201C;lady gaga songs&#x201D;. In this case, the irrelevant information about nobility would be ranked much higher than information about Lady Gaga&#x0027;s music, simply because the word &#x201C;lady&#x201D; appeared many times in the row. Thus, we defined the following ranking function to account for this problem, in addition to incorporating all standard ranking features of keyword search:</em>     <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} R_{int}(\rho , C, Q) = \sum _{t \in Q} ti(t, \rho) \times idi(C,t) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> Where <em>&#x03C1;</em> is the row we are ranking, <em>C</em> is our corpus and <em>Q</em> is the user&#x0027;s query. <em>t</em> is a single term &#x2208; <em>Q</em>. <em>ti</em> is defined as follows: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} ti(t, \rho) = \left\lbrace \begin{array}{ccc}1 &#x0026;\mbox{if } t \in \rho \\ 0 &#x0026;\mbox{otherwise} \\ \end{array} \right.\end{equation*} </span>       <br/>      </div>     </div> Notice that this function checks for the mere <em>existence</em> of a term within the row, so it is boolean. Hence, in Equation <a class="eqn" href="#eq1">1</a>, we are effectively selecting the <em>idi</em> weights we wish to sum. These weights are defined by the following measure: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} idi(C,t) = ln\left(\dfrac{|C|}{\sum _{i=0}^{|C|}|C_i \cap \lbrace t\rbrace |}\right) \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> Intuitively, the numerator is the number of rows within the corpus. The denominator is the number of rows term <em>t</em> appears in within the corpus. This is scaled logarithmically so the closer we are to the row-cardinality of the corpus on the bottom, the closer our weight gets to 0. See [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0049">49</a>] for justification &#x0026; formal details regarding IDF.</p>     <p>As a final measure in this base ranking scheme, we use <em>keyword proximity</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] to discount the rank of rows containing key terms that occur further apart. We take the total distance (starting at 1 when all words are adjacent) between the <em>closest</em> occurring keyword terms in a row and divide the <strong>ti &#x2212; idi</strong> score by the natural logarithm of this value. This is since we do not wish to penalize the score of possibly relevant rows too harshly. We discount the <strong>ti &#x2212; idi</strong> score by the natural logarithm of 100 if only <em>one</em> keyword is present in the row, as such a result is highly unlikely to be relevant. We came up with this constant experimentally, in order to prevent any rows with only one matched term from appearing if rows with more matched terms exist in the corpus.</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Using trained classifiers and Unified Famous Objects to get more relevant search results</h3>     </div>     </header>     <p>In this scheme, we first extract UFO <em>core attributes</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] from each Web tables&#x2019; subset labeled by our trained classifier to belong to a certain class (e.g. songs), then use them to retrieve the most relevant search results from the entire corpus. We extract this set of core attributes with a query that fetches attribute rows that belong to the classified data. Each row in our corpus contains a filename, identifying the Web table it came from. We join the set of classified <em>data</em> rows with all <em>metadata</em> rows from the corpus on their filename. We then iterate through this table of attributes, counting the occurrences of all attributes and inserting the attribute frequencies into our table of core attributes. This step is done offline per UFO to reduce overhead during query-time. See [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] for more detail on UFO construction and core attributes. During online query processing, we load the pre-computed set of core attributes into a Java HashMap. We then load a result set of the rows containing at least one term from the query into memory, ranking them according to Equation <a class="eqn" href="#eq1">1</a>. Finally, we increase the ranking of each tuple in the result set by matching each of its attributes to core attributes, increasing their rank by the natural logarithm of the sum of all matched core attribute frequencies. We perform this logarithmic scaling because of the high frequency of many of the attributes in our dataset. Doing this approximates matching an extra keyword for especially popular attributes, thus making it easier to see how core attributes affect the ranking of rows. An even better ranking could be achieved by using Unified Famous Objects (UFO) not only for core attributes, but also for attribute matching [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. This will enable us to go beyond simple keyword matching, matching words that are either synonymous or closely related to one another to improve a row&#x0027;s rank. See [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] for details on UFOs and object recognition evaluation for UFOs.</p>    </section>   </section>   <section id="sec-20">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Evaluation</h2>     </div>    </header>    <p>It is important to assess the effectiveness of using our Classifiers+UFOs ranking scheme for structured data. To gauge relevance gain over 99 different queries we collected from users, we compare it with the baseline ranking described above in the &#x201D;Baseline Ranking&#x201D; paragraph in Section <a class="sec" href="#sec-17">6</a>. <figure id="fig5">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191600/images/www18companion-339-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Search relevance evaluation of our system over 99 user queries. Each point is the delta in <em>nDCG</em>       <sub>15</sub> between our AI-augmented and baseline ranking functions.</span>     </div>     </figure>    </p>    <p>To evaluate relevance gain provided by our ranking function, we asked 28 students taking a database class to come up with a few queries they might be interested in executing over our Web tables dataset. After accumulating <em>99 different queries</em>, we asked two independent evaluators to assign relevance labels from 1-Bad to 5-Perfect to all (query, search result) pairs generated by our two ranking schemes for these queries. We took 15 top results for each query. We dropped a label whenever the evaluators disagreed. Based on these labels and ranking by our schemes, we took the top 15 results from each query to compute their <em>nDCG</em>     <sub>15</sub>, which is an industrial standard measure to evaluate search relevance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. It stands for &#x201C;Normalized Discounted Cumulative Gain&#x201D; and reflects relevance of search results to the specific query. It computes a value based on user-assigned relevance labels to rows in a result set, discounting their contribution to the overall <em>DCG</em> score the lower their position is in the result set. The following equation formalizes this intuition: <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} DCG_n(S) = \sum _{i=0}^n\dfrac{S_i.label}{log_2(i + 1)} \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>     </div> Where <em>S</em> is the set of ranked rows and <em>label</em> is the value of the user-assigned relevance label for the row, and <em>n</em> is the number of results from the result set we consider from the query. To compute the <em>normalized</em> DCG, we must first compute <em>iDCG</em>, the <em>DCG</em> value of a search engine that is able to rank the results perfectly. By sorting relevant <em>S</em> by <em>label</em> (in descending order), then computing <em>DCG</em>, we get <em>iDCG</em>. Dividing the <em>DCG</em> by <em>iDCG</em> then produces our <em>nDCG</em> value for a particular query. <em>nDCG</em> thusly rewards highly relevant results when their position is more appropriate (higher). Refer to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] for more formal details on <em>nDCG</em>.</p>    <p>In Figure <a class="fig" href="#fig5">5</a>, each point is the delta between the <em>nDCG</em>     <sub>15</sub> value of the same query run using two different query processing schemes. The first is <em>with</em> UFO core attributes and classifiers, the second is the baseline ranking scheme. Many queries get better as we can see from the graph, at the same time there are queries that got worse. For example, the largest relevance decrease we observed was for the query &#x201C;fifth element&#x201D;, where several of the most relevant results did not have proper metadata attributes in our dataset. This, combined with terms from iTunes being present in the set of film core attributes, caused the query to fetch more song data for this query instead of film data. More queries improve overall, which is reflected by an average 1.5% increase in nDCG over all 99 queries, which is considered significant in web search [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>].</p>   </section>   <section id="sec-21">    <header>     <div class="title-info">     <h2>      <span class="section-number">8</span> Conclusion</h2>     </div>    </header>    <p>We described <SmallCap>Hybrid.AI</SmallCap> - a learning search engine marrying machine learning with keyword-search. We justified that our <em>Machine Learning+UFO</em> augmented keyword-search returns more relevant search results than a standard ranking function for keyword search over structured data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>]. We gauged the relevance gain of our algorithms using nDCG, a de-facto standard relevance measure, employed by major Web-search engines [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], and observed significant gain on 99 user queries over a large scale Web table corpus.</p>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">9</span> Acknowledgments</h2>     </div>    </header>    <p>We would like to thank anonymous reviewers for their feedback on earlier drafts of this paper. This material is based upon work supported by the National Science Foundation (NSF) under Grant No. 1701081.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">[n. d.]. Hybrid.AI: An AI-Augmented Search Engine for Large-scale Structured Data. In <em>      <em>MIT Annual Database Research Conference</em>     </em>.</li>     <li id="BibPLXBIB0002" label="[2]">[n. d.]. Hybrid.Poly: An Interactive Large-scale In-memory Analytical Polystore. In <em>      <em>MIT Annual Database Research Conference</em>     </em>.</li>     <li id="BibPLXBIB0003" label="[3]">Ziawasch Abedjan, John Morcos, Michael Gubanov, Ihab&#x00A0;F. Ilyas, Michael Stonebraker, Paolo Papotti, and Mourad Ouzzani. 2015. Dataxformer: Leveraging the Web for Semantic Transformations. In <em>      <em>CIDR</em>     </em>.</li>     <li id="BibPLXBIB0004" label="[4]">Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving Web Search Ranking by Incorporating User Behavior Information. In <em>      <em>SIGIR</em>     </em>.</li>     <li id="BibPLXBIB0005" label="[5]">Sanjay Agrawal, Surajit Chaudhuri, and Gautam Das. 2002. DBXplorer: A system for keyword-based search over relational databases. In <em>      <em>ICDE</em>     </em>.</li>     <li id="BibPLXBIB0006" label="[6]">Albin Ahmeti, Javier&#x00A0;D. Fern&#x00E1;ndez, Axel Polleres, and Vadim Savenkov. 2017. Updating Wikipedia via DBpedia Mappings and SPARQL. In <em>      <em>ESWC</em>     </em>.</li>     <li id="BibPLXBIB0007" label="[7]">Bogdan Alexe, Michael Gubanov, Mauricio&#x00A0;A. Hern&#x00E1;ndez, C.&#x00A0;T.&#x00A0;Howard Ho, Jen-Wei Huang, Yannis Katsis, Lucian Popa, Barna Saha, and Ioana Stanoi. 2008. Simplifying Information Integration: Object-Based Flow-of-Mappings Framework for Integration. In <em>      <em>BIRTE</em>     </em>.</li>     <li id="BibPLXBIB0008" label="[8]">Ricardo Baeza-Yates and Walter Cunto. 1999. The ADT proximity and text proximity problems. In <em>      <em>SPIRS</em>     </em>. IEEE, 24&#x2013;30.</li>     <li id="BibPLXBIB0009" label="[9]">Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-scale Hypertextual Web Search Engine. In <em>      <em>WWW</em>     </em>.</li>     <li id="BibPLXBIB0010" label="[10]">Michael&#x00A0;J Cafarella, Alon Halevy, Daisy&#x00A0;Zhe Wang, Eugene Wu, and Yang Zhang. 2008. Webtables: exploring the power of tables on the web. <em>      <em>VLDB</em>     </em> (2008).</li>     <li id="BibPLXBIB0011" label="[11]">Michael&#x00A0;A. Casey, Christophe Rhodes, and Malcolm Slaney. 2008. Analysis of Minimum Distances in High-Dimensional Musical Spaces. <em>      <em>IEEE TASLP</em>     </em>16(2008), 1015&#x2013;1028.</li>     <li id="BibPLXBIB0012" label="[12]">Kaushik Chakrabarti, Surajit Chaudhuri, Zhimin Chen, Kris Ganjam, Yeye He, and WA Redmond. 2016. Data services leveraging Bing&#x0027;s data assets.<em>      <em>IEEE Data Eng. Bull.</em>     </em>(2016), 15&#x2013;28.</li>     <li id="BibPLXBIB0013" label="[13]">Surajit Chaudhuri, Gautam Das, Vagelis Hristidis, and Gerhard Weikum. 2004. Probabilistic ranking of database query results. In <em>      <em>VLDB</em>     </em>. 888&#x2013;899.</li>     <li id="BibPLXBIB0014" label="[14]">Surya Cheemalapati, Michael Gubanov, Michael&#x00A0;Del Vale, and Anna Pyayt. 2013. A real-time classification algorithm for emotion detection using portable EEG. In <em>      <em>IRI</em>     </em>.</li>     <li id="BibPLXBIB0015" label="[15]">Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. <em>      <em>Machine learning</em>     </em>20, 3 (1995), 273&#x2013;297.</li>     <li id="BibPLXBIB0016" label="[16]">Anish Das&#x00A0;Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding related tables. In <em>      <em>SIGMOD</em>     </em>.</li>     <li id="BibPLXBIB0017" label="[17]">Hady ElSahar, Elena Demidova, Simon Gottschalk, Christophe Gravier, and Fr&#x00E9;d&#x00E9;rique Laforest. 2018. Unsupervised Open Relation Extraction. <em>      <em>CoRR</em>     </em>abs/1801.07174(2018).</li>     <li id="BibPLXBIB0018" label="[18]">Jane Greenberg. 2005. Understanding metadata and metadata schemes. <em>      <em>Cataloging &#x0026; classification quarterly</em>     </em>40, 3-4 (2005), 17&#x2013;36.</li>     <li id="BibPLXBIB0019" label="[19]">Michael Gubanov. 2017. Hybrid: A Large-scale In-memory Image Analytics System. In <em>      <em>CIDR</em>     </em>.</li>     <li id="BibPLXBIB0020" label="[20]">Michael Gubanov. 2017. PolyFuse: A Large-scale Hybrid Data Fusion System. In <em>      <em>ICDE DESWeb</em>     </em>.</li>     <li id="BibPLXBIB0021" label="[21]">Michael Gubanov and Philip&#x00A0;A. Bernstein. 2006. Structural text search and comparison using automatically extracted schema.. In <em>      <em>WebDB</em>     </em>.</li>     <li id="BibPLXBIB0022" label="[22]">Michael Gubanov, Philip&#x00A0;A. Bernstein, and Alexander Moshchuk. 2008. Model Management Engine for Data Integration with Reverse-Engineering Support. In <em>      <em>ICDE</em>     </em>.</li>     <li id="BibPLXBIB0023" label="[23]">Michael Gubanov, Chris Jermaine, Zekai Gao, and Shangyu Luo. 2016. Hybrid: A Large-scale Linear-relational Database Management System. In <em>      <em>MIT Annual DB Conference</em>     </em>.</li>     <li id="BibPLXBIB0024" label="[24]">Michael Gubanov, Shangyu Luo, Zekai Gao, Luis Perez, and Christopher Jermaine. 2017. Scalable Linear Algebra on a Relational Database System. In <em>      <em>ICDE</em>     </em>.</li>     <li id="BibPLXBIB0025" label="[25]">Michael Gubanov, Shangyu Luo, Zekai Gao, Luis Perez, and Christopher Jermaine. 2018. Scalable Linear Algebra on a Relational Database System. In <em>      <em>to apear in TKDE</em>     </em>.</li>     <li id="BibPLXBIB0026" label="[26]">Michael Gubanov, Shangyu Luo, Zekai Gao, Luis Perez, and Christopher Jermaine. 2018. Scalable Linear Algebra on a Relational Database System. In <em>      <em>to appear in ACM SIGMOD Record</em>     </em>.</li>     <li id="BibPLXBIB0027" label="[27]">Michael Gubanov, Lucian Popa, Howard Ho, Hamid Pirahesh, Jeng-Yih Chang, and Shr-Chang Chen. 2009. IBM UFO repository: Object-oriented data integration. <em>      <em>VLDB</em>     </em> (2009).</li>     <li id="BibPLXBIB0028" label="[28]">Michael Gubanov, Manju Priya, and Maksim Podkorytov. 2017. CognitiveDB: An Intelligent Navigator for Large-scale Dark Structured Data. In <em>      <em>WWW</em>     </em>.</li>     <li id="BibPLXBIB0029" label="[29]">M. Gubanov and A. Pyayt. 2013. ReadFast: High-relevance Search-engine for Big Text. In <em>      <em>ACM CIKM</em>     </em>.</li>     <li id="BibPLXBIB0030" label="[30]">Michael Gubanov and Anna Pyayt. 2016. Type-aware Web search. In <em>      <em>EDBT</em>     </em>.</li>     <li id="BibPLXBIB0031" label="[31]">Michael Gubanov, Anna Pyayt, and Linda Shapiro. 2011. ReadFast: Browsing large documents through UFO. In <em>      <em>IRI</em>     </em>.</li>     <li id="BibPLXBIB0032" label="[32]">Michael Gubanov and Linda Shapiro. 2012. Using Unified Famous Objects (UFO) to Automate Alzheimer&#x0027;s Disease Diagnostics. In <em>      <em>BIBM</em>     </em>.</li>     <li id="BibPLXBIB0033" label="[33]">Michael Gubanov, Linda Shapiro, and Anna Pyayt. 2011. Learning Unified Famous Objects (UFO) to Bootstrap Information Integration. In <em>      <em>IRI</em>     </em>.</li>     <li id="BibPLXBIB0034" label="[34]">Michael Gubanov and Michael Stonebraker. 2014. Large-scale Semantic Profile Extraction. In <em>      <em>EDBT</em>     </em>.</li>     <li id="BibPLXBIB0035" label="[35]">Michael Gubanov, Michael Stonebraker, and Daniel Bruckner. 2014. Text and Structured Data Fusion in Data Tamer at Scale. In <em>      <em>ICDE</em>     </em>.</li>     <li id="BibPLXBIB0036" label="[36]">Lin Guo, Feng Shao, Chavdar Botev, and Jayavel Shanmugasundaram. 2003. XRANK: Ranked keyword search over XML documents. In <em>      <em>SIGMOD</em>     </em>. ACM.</li>     <li id="BibPLXBIB0037" label="[37]">Chih-Wei Hsu and Chih-Jen Lin. 2002. A comparison of methods for multiclass support vector machines. <em>      <em>TNN</em>     </em>13, 2 (2002), 415&#x2013;425.</li>     <li id="BibPLXBIB0038" label="[38]">Kalervo J&#x00E4;rvelin and Jaana Kek&#x00E4;l&#x00E4;inen. 2000. IR evaluation methods for retrieving highly relevant documents. In <em>      <em>SIGIR</em>     </em>. ACM.</li>     <li id="BibPLXBIB0039" label="[39]">Emilia Kacprzak, Laura&#x00A0;M. Koesten, Luis&#x00A0;Daniel Ib&#x00E1;&#x00F1;ez, Elena Simperl, and Jeni Tennison. 2017. A Query Log Analysis of Dataset Search. In <em>      <em>ICWE</em>     </em>.</li>     <li id="BibPLXBIB0040" label="[40]">Laura&#x00A0;M. Koesten, Emilia Kacprzak, Jenifer Fay&#x00A0;Alys Tennison, and Elena Simperl. 2017. The Trials and Tribulations of Working with Structured Data: a Study on Information Seeking Behaviour. In <em>      <em>CHI</em>     </em>.</li>     <li id="BibPLXBIB0041" label="[41]">Anusha Kola, Harshal More, Sean Soderman, and Michael Gubanov. 2017. Generating Unified Famous Objects (UFOs) from the classified object tables. In <em>      <em>IEEE Big Data</em>     </em>.</li>     <li id="BibPLXBIB0042" label="[42]">Marcel Kornacker and Alexander&#x00A0;Behm et al. 2015. Impala: A Modern, Open-Source SQL Engine for Hadoop. In <em>      <em>CIDR</em>     </em>.</li>     <li id="BibPLXBIB0043" label="[43]">Thomas&#x00A0;M. Mitchell. 1997. <em>      <em>Machine Learning</em>     </em>. McGraw-Hill, Inc., New York, NY, USA.</li>     <li id="BibPLXBIB0044" label="[44]">Tope Omitola, Sebasti&#x00E1;n&#x00A0;A. R&#x00ED;os, and John&#x00A0;G. Breslin. 2015. <em>      <em>Social Semantic Web Mining</em>     </em>. Morgan &#x0026; Claypool Publishers.</li>     <li id="BibPLXBIB0045" label="[45]">Steven Ortiz, Caner Enbatan, Maksim Podkorytov, Dylan Soderman, and Michael Gubanov. 2017. Hybrid.JSON: High-velocity Parallel In-Memory Polystore JSON Ingest. In <em>      <em>IEEE Bigdata</em>     </em>.</li>     <li id="BibPLXBIB0046" label="[46]">Manju Priya, Maxim Podkorytov, and Michael Gubanov. 2017. iLight: A Flashlight for Large-scale Dark Structured Data. In <em>      <em>MIT Annual DB Conference</em>     </em>.</li>     <li id="BibPLXBIB0047" label="[47]">Freddy Priyatna, Edna Ruckhaus, Nandana Mihindukulasooriya, &#x00D3;scar Corcho, and Nelson Saturno. 2017. MappingPedia: A Collaborative Environment for R2RML Mappings. In <em>      <em>ESWC</em>     </em>.</li>     <li id="BibPLXBIB0048" label="[48]">Anna Pyayt and Michael Gubanov. 2013. BigDB: Automatic Machine Learning Optimizer. <em>      <em>CoRR</em>     </em>abs/1301.1575(2013).</li>     <li id="BibPLXBIB0049" label="[49]">Stephen Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF. <em>      <em>Journal of documentation</em>     </em>60, 5 (2004), 503&#x2013;520.</li>     <li id="BibPLXBIB0050" label="[50]">G. Salton, A. Wong, and C.&#x00A0;S. Yang. 1975. A Vector Space Model for Automatic Indexing. <em>      <em>CACM</em>     </em>18, 11 (Nov. 1975), 613&#x2013;620.</li>     <li id="BibPLXBIB0051" label="[51]">Mark Simmons, Daniel Armstrong, Dylan Soderman, and Michael Gubanov. 2017. Hybrid.media: High Velocity Video Ingestion in an In-Memory Scalable Analytical Polystore. In <em>      <em>IEEE Bigdata</em>     </em>.</li>     <li id="BibPLXBIB0052" label="[52]">Michael Stonebraker. 2012. Big Data Means at Least Three Different Things.... In <em>      <em>NIST Big Data Workshop</em>     </em>.</li>     <li id="BibPLXBIB0053" label="[53]">Mike Stonebraker, Daniel Abadi, and Adam&#x00A0;Batkin et al. 2005. C-store: A Column-oriented DBMS. In <em>      <em>VLDB</em>     </em>.</li>     <li id="BibPLXBIB0054" label="[54]">Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. 2016. Table cell search for question answering. In <em>      <em>WWW</em>     </em>.</li>     <li id="BibPLXBIB0055" label="[55]">Santiago Villasenor, Tom Nguyen, Anusha Kola, Sean Soderman, and Michael Gubanov. 2017. Scalable spam classifier for web tables. In <em>      <em>IEEE Big Data</em>     </em>.</li>     <li id="BibPLXBIB0056" label="[56]">Ran Yu, Ujwal Gadiraju, Besnik Fetahu, and Stefan Dietze. 2017. FuseM: Query-Centric Data Fusion on Structured Web Markup. In <em>      <em>ICDE</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191600">https://doi.org/10.1145/3184558.3191600</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
