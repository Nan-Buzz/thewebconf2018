<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Ad Hoc Table Retrieval using Semantic Similarity</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186067"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186067'>https://doi.org/10.1145/3178876.3186067</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186067'>https://w3id.org/oa/10.1145/3178876.3186067</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Ad Hoc Table Retrieval using Semantic Similarity</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Shuo</span> <span class="surName">Zhang</span>, University of Stavanger, <a href="mailto:shuo.zhang@uis.no">shuo.zhang@uis.no</a>
        </div>
        <div class="author">
          <span class="givenName">Krisztian</span> <span class="surName">Balog</span>, University of Stavanger, <a href="mailto:krisztian.balog@uis.no">krisztian.balog@uis.no</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186067" target="_blank">https://doi.org/10.1145/3178876.3186067</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We introduce and address the problem of ad hoc table retrieval: answering a keyword query with a ranked list of tables. This task is not only interesting on its own account, but is also being used as a core component in many other table-based information access scenarios, such as table completion or table mining. The main novel contribution of this work is a method for performing semantic matching between queries and tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using a purpose-built test collection based on Wikipedia tables, we demonstrate significant and substantial improvements over a state-of-the-art baseline.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Similarity measures;</strong> <strong>Environment-specific retrieval;</strong> Learning to rank;</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Table retrieval</small>,</span> <span class="keyword"><small>table search</small>,</span> <span class="keyword"><small>semantic matching</small>,</span> <span class="keyword"><small>semantic representations</small>,</span> <span class="keyword"><small>semantic similarity</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Shuo Zhang and Krisztian Balog. 2018. Ad Hoc Table Retrieval using Semantic Similarity. In <em>WWW 2018: The 2018 Web Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186067" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3178876.3186067</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Tables are a powerful, versatile, and easy-to-use tool for organizing and working with data. Because of this, a massive number of tables can be found “out there,” on the Web or in Wikipedia, representing a vast and rich source of structured information. Recently, a growing body of work has begun to tap into utilizing the knowledge contained in tables. A wide and diverse range of tasks have been undertaken, including but not limited to (i) searching for tables (in response to a keyword query&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>] or a seed table&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]), (ii) extracting knowledge from tables (such as RDF triples&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]), and (iii) augmenting tables (with new columns&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>], rows&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>], cell values&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>], or links to entities&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]).</p>
      <p>Searching for tables is an important problem on its own, in addition to being a core building block in many other table-related tasks. Yet, it has not received due attention, and especially not from an information retrieval perspective. This paper aims to fill that gap. We define the <em>ad hoc table retrieval</em> task as follows: given a keyword query, return a ranked list of tables from a table corpus that are relevant to the query. See Fig.&nbsp;<a class="fig" href="#fig1">1</a> for an illustration.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Ad hoc table retrieval: given a keyword query, the system returns a ranked list of tables.</span>
        </div>
      </figure>
      <p></p>
      <p>It should be acknowledged that this task is not entirely new, in fact, it has been around for a while in the database community (also known there as <em>relation ranking</em>)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>]. However, public test collections and proper evaluation methodology are lacking, in addition to the need for better ranking techniques.</p>
      <p>Tables can be ranked much like documents, by considering the words contained in them&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]. Ranking may be further improved by incorporating additional signals related to table quality. Intuitively, high quality tables are topically coherent; other indicators may be related to the pages that contain them (e.g., if they are linked by other pages&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]). However, a major limitation of prior approaches is that they only consider lexical matching between the contents of tables and queries. This gives rise to our main research objective: <em>Can we move beyond lexical matching and improve table retrieval performance by incorporating semantic matching?</em></p>
      <p>We consider two main kinds of semantic representations. One is based on concepts, such as entities and categories. Another is based on continuous vector representations of words and of entities (i.e., word and graph embeddings). We introduce a framework that handles matching in different semantic spaces in a uniform way, by modeling both the table and the query as sets of semantic vectors. We propose two general strategies (early and late fusion), yielding four different measures for computing the similarity between queries and tables based on their semantic representations.</p>
      <p>As we have mentioned above, another key area where prior work has insufficiencies is evaluation. First, there is no publicly available test collection for this task. Second, evaluation has been performed using set-based metrics (counting the number of relevant tables in the top-<em>k</em> results), which is a very rudimentary way of measuring retrieval effectiveness. We address this by developing a purpose-built test collection, comprising of 1.6M tables from Wikipedia, and a set of queries with graded relevance judgments. We establish a learning-to-rank baseline that encompasses a rich set of features from prior work, and outperforms the best approaches known in the literature. We show that the semantic matching methods we propose can substantially and significantly improve retrieval performance over this strong baseline.</p>
      <p>In summary, this paper makes the following contributions:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We introduce and formalize the ad hoc table ranking task, and present both unsupervised and supervised baseline approaches (Sect.&nbsp;<a class="sec" href="#sec-4">2</a>).<br />
        </li>
        <li id="list2" label="•">We present a set of novel semantic matching methods that go beyond lexical similarity (Sect.&nbsp;<a class="sec" href="#sec-14">3</a>).<br />
        </li>
        <li id="list3" label="•">We develop a standard test collection for this task (Sect.&nbsp;<a class="sec" href="#sec-26">4</a>) and demonstrate the effectiveness of our approaches (Sect.&nbsp;<a class="sec" href="#sec-30">5</a>).<br />
        </li>
      </ul>
      <p>The test collection and the outputs of the reported methods are made available at <a class="link-inline force-break" href="https://github.com/iai-group/www2018-table">https://github.com/iai-group/www2018-table</a>.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Ad Hoc Table Retrieval</h2>
        </div>
      </header>
      <p>We formalize the ad hoc table retrieval task, explain what information is associated with a table, and introduce baseline methods.</p>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Problem Statement</h3>
          </div>
        </header>
        <p>Given a keyword query <em>q</em>, <em>ad hoc table retrieval</em> is the task of returning a ranked list of tables, (<em>T</em> <sub>1</sub>, …, <em>T<sub>k</sub></em> ), from a collection of tables <em>C</em>. Being an ad hoc task, the relevance of each returned table <em>T<sub>i</sub></em> is assessed independently of all other returned tables <em>T<sub>j</sub></em> , <em>i</em> ≠ <em>j</em>. Hence, the ranking of tables boils down to the problem of assigning a score to each table in the corpus: <em>score</em>(<em>q</em>, <em>T</em>). Tables are then sorted in descending order of their scores.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> The Anatomy of a Table</h3>
          </div>
        </header>
        <p>We shall assume that the following information is available for each table in the corpus; the letters refer to Figure&nbsp;<a class="fig" href="#fig2">2</a>.</p>
        <ol class="list-no-style">
          <li id="list4" label="•"><em>Page title</em>, where the table was extracted from.<br /></li>
          <li id="list5" label="•"><em>Section title</em>, i.e., the heading of the particular section where the table is embedded.<br /></li>
          <li id="list6" label="•"><em>Table caption</em>, providing a brief explanation.<br /></li>
          <li id="list7" label="•"><em>Table headings</em>, i.e., a list of column heading labels.<br /></li>
          <li id="list8" label="•"><em>Table body</em>, i.e., all table cells (including column headings).<br /></li>
        </ol>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Table embedded in a Wikipedia page.</span>
          </div>
        </figure>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Unsupervised Ranking</h3>
          </div>
        </header>
        <p>An easy and straightforward way to perform the table ranking task is by adopting standard document ranking methods. Cafarella et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>], ,<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] utilize web search engines to retrieve relevant documents; tables are then extracted from the highest-ranked documents. Rather than relying on external services, we represent tables as either single- or multi-field documents and apply standard documents retrieval techniques.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Baseline features for table retrieval.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"><strong>Query features</strong></th>
                <th style="text-align:left;"><strong>Source</strong></th>
                <th style="text-align:left;"><strong>Value</strong></th>
                <th></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">QLEN</td>
                <td style="text-align:left;">Number of query terms</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>]
                </td>
                <td style="text-align:left;">{1,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">IDF <sub><em>f</em></sub></td>
                <td style="text-align:left;">Sum of query IDF scores in field <em>f</em></td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]
                </td>
                <td style="text-align:left;">[0, ∞)</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Table features</strong></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">#rows</td>
                <td style="text-align:left;">The number of rows in the table</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">{1,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">#cols</td>
                <td style="text-align:left;">The number of columns in the table</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">{1,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">#of NULLs in table</td>
                <td style="text-align:left;">The number of empty table cells</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">PMI</td>
                <td style="text-align:left;">The ACSDb-based schema coherency score</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">(− ∞, ∞)</td>
              </tr>
              <tr>
                <td style="text-align:left;">inLinks</td>
                <td style="text-align:left;">Number of in-links to the page embedding the table</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">outLinks</td>
                <td style="text-align:left;">Number of out-links from the page embedding the table</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">pageViews</td>
                <td style="text-align:left;">Number of page views</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">tableImportance</td>
                <td style="text-align:left;">Inverse of number of tables on the page</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">(0, 1]</td>
              </tr>
              <tr>
                <td style="text-align:left;">tablePageFraction</td>
                <td style="text-align:left;">Ratio of table size to page size</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">(0, 1]</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Query-table features</strong></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">#hitsLC</td>
                <td style="text-align:left;">Total query term frequency in the leftmost column cells</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">#hitsSLC</td>
                <td style="text-align:left;">Total query term frequency in second-to-leftmost column cells</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">#hitsB</td>
                <td style="text-align:left;">Total query term frequency in the table body</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
                </td>
                <td style="text-align:left;">{0,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">qInPgTitle</td>
                <td style="text-align:left;">Ratio of the number of query tokens found in page title to total number of tokens</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">[0, 1]</td>
              </tr>
              <tr>
                <td style="text-align:left;">qInTableTitle</td>
                <td style="text-align:left;">Ratio of the number of query tokens found in table title to total number of tokens</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">[0, 1]</td>
              </tr>
              <tr>
                <td style="text-align:left;">yRank</td>
                <td style="text-align:left;">Rank of the table's Wikipedia page in Web search engine results for the query</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
                </td>
                <td style="text-align:left;">{1,...,n}</td>
              </tr>
              <tr>
                <td style="text-align:left;">MLM similarity</td>
                <td style="text-align:left;">Language modeling score between query and multi-field document repr. of the table</td>
                <td style="text-align:left;">
                  [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>]
                </td>
                <td style="text-align:left;">(− ∞,0)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <section id="sec-8">
          <p><em>2.3.1 Single-field Document Representation.</em> In the simplest case, all text associated with a given table is used as the table's representation. This representation is then scored using existing retrieval methods, such as BM25 or language models.</p>
        </section>
        <section id="sec-9">
          <p><em>2.3.2 Multi-field Document Representation.</em> Rather than collapsing all textual content into a single-field document, it may be organized into multiple fields, such as table caption, table headers, table body, etc. (cf. Sect.&nbsp;<a class="sec" href="#sec-6">2.2</a>). For multi-field ranking, Pimplikar and Sarawagi [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] employ a <em>late fusion</em> strategy&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0048">48</a>]. That is, each field is scored independently against the query, then a weighted sum of the field-level similarity scores is taken:</p>
          <div class="table-responsive" id="Xeq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \mathit {score}(q,T) = \sum _{i} w_i \times \mathit {score}(q, f_i) ~, \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>where <em>f<sub>i</sub></em> denotes the <em>i</em>th (document) field for table <em>T</em> and <em>w<sub>i</sub></em> is the corresponding field weight (such that ∑ <sub><em>i</em></sub> <em>w<sub>i</sub></em> = 1). <em>score</em>(<em>q</em>, <em>f<sub>i</sub></em> ) may be computed using any standard retrieval method. We use language models in our experiments.
          <p></p>
        </section>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.4</span> Supervised Ranking</h3>
          </div>
        </header>
        <p>The state-of-the-art in document retrieval (and in many other retrieval tasks) is to employ supervised learning&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>]. Features may be categorized into three groups: (i) document, (ii) query, and (iii) query-document features&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]. Analogously, we distinguish between three types of features: (i) table, (ii) query, and (iii) query-table features. In Table&nbsp;<a class="tbl" href="#tab1">1</a>, we summarize the features from previous work on table search&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]. We also include a number of additional features that have been used in other retrieval tasks, such as document and entity ranking; we do not regard these as novel contributions.</p>
        <section id="sec-11">
          <p><em>2.4.1 Query Features.</em> Query features have been shown to improve retrieval performance for document ranking&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>]. We adopt two query features from document retrieval, namely, the number of terms in the query&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>], and query IDF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>] according to: <em>IDF<sub>f</sub></em> (<em>q</em>) = ∑ <sub><em>t</em> ∈ <em>q</em></sub> <em>IDF<sub>f</sub></em> (<em>t</em>), where <em>IDF<sub>f</sub></em> (<em>t</em>) is the IDF score of term <em>t</em> in field <em>f</em>. This feature is computed for the following fields: page title, section title, table caption, table heading, table body, and “catch-all” (the concatenation of all textual content in the table).</p>
        </section>
        <section id="sec-12">
          <p><em>2.4.2 Table Features.</em> Table features depend only on the table itself and aim to reflect the quality of the given table (irrespective of the query). Some features are simple characteristics, like the number of rows, columns, and empty cells&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]. One important feature is Point-wise Mutual Information (PMI), which is taken from linguistics research, and expresses the coherency of a table. The correlation between two table headings cells, <em>h<sub>i</sub></em> and <em>h<sub>j</sub></em> , is given by: <em>PMI</em>(<em>h<sub>i</sub></em> , <em>h<sub>j</sub></em> ) = log (<em>P</em>(<em>h<sub>i</sub></em> , <em>h<sub>j</sub></em> )/(<em>P</em>(<em>h<sub>i</sub></em> )<em>P</em>(<em>h<sub>j</sub></em> ))). A table's PMI is computed by calculating the PMI values between all pairs of column headings of that table, and then taking their average. Following&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>], we compute PMI by obtaining frequency statistics from the Attribute Correlation Statistics Database (ACSDb)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], which contains table heading information derived from millions of tables extracted from a large web crawl.</p>
          <p>Another group of features has to do with the page that embeds the table, by considering its connectivity (inLinks and outLinks), popularity (pageViews), and the table's importance within the page (tableImportance and tablePageFraction).</p>
        </section>
        <section id="sec-13">
          <p><em>2.4.3 Query-Table Features.</em> Features in the last group express the degree of matching between the query and a given table. This matching may be based on occurrences of query terms in the page title (qInPgTitle) or in the table caption (qInTableTitle). Alternatively, it may be based on specific parts of the table, such as the leftmost column (#hitsLC), second-to-left column (#hitsSLC), or table body (#hitsB). Tables are typically embedded in (web) pages. The rank at which a table's parent page is retrieved by an external search engine is also used as a feature (yRank). (In our experiments, we use the Wikipedia search API to obtain this ranking.) Furthermore, we take the Mixture of Language Models (MLM) similarity score&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>] as a feature, which is actually the best performing method among the four text-based baseline methods (cf. Sect.&nbsp;<a class="sec" href="#sec-30">5</a>). Importantly, all these features are based on lexical matching. Our goal in this paper is to also enable semantic matching; this is what we shall discuss in the next section.</p>
          <figure id="fig3">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span> <span class="figure-title">Our methods for computing query-table similarity using semantic representations.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Semantic Matching</h2>
        </div>
      </header>
      <p>This section presents our main contribution, which is a set of novel semantic matching methods for table retrieval. The main idea is to go beyond lexical matching by representing both queries and tables in some semantic space, and measuring the similarity of those semantic (vector) representations. Our approach consists of three main steps, which are illustrated in Figure&nbsp;<a class="fig" href="#fig3">3</a>. These are as follows (moving from outwards to inwards on the figure):</p>
      <ol class="list-no-style">
        <li id="list9" label="(1)">The “raw” content of a query/table is represented as a set of terms, where terms can be either words or entities (Sect.&nbsp;<a class="sec" href="#sec-15">3.1</a>).<br />
        </li>
        <li id="list10" label="(2)">Each of the raw terms is mapped to a semantic vector representation (Sect.&nbsp;<a class="sec" href="#sec-20">3.2</a>).<br />
        </li>
        <li id="list11" label="(3)">The semantic similarity (matching score) between a query-table pair is computed based on their semantic vector representations (Sect.&nbsp;<a class="sec" href="#sec-23">3.3</a>).<br />
        </li>
      </ol>
      <p>We compute query-table similarity using all possible combinations of semantic representations and similarity measures, and use the resulting semantic similarity scores as features in a learning-to-rank approach. Table&nbsp;<a class="tbl" href="#tab2">2</a> summarizes these features.</p>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Content Extraction</h3>
          </div>
        </header>
        <p>We represent the “raw” content of the query/table as a set of terms, where terms can be either words (string tokens) or entities (from a knowledge base). We denote these as {<em>q</em> <sub>1</sub>, …, <em>q<sub>n</sub></em> } and {<em>t</em> <sub>1</sub>, …, <em>t<sub>m</sub></em> } for query <em>q</em> and table <em>T</em>, respectively.</p>
        <section id="sec-16">
          <p><em>3.1.1 Word-based.</em> It is a natural choice to simply use word tokens to represent query/table content. That is, {<em>q</em> <sub>1</sub>, …, <em>q<sub>n</sub></em> } is comprised of the unique words in the query. As for the table, we let {<em>t</em> <sub>1</sub>, …, <em>t<sub>m</sub></em> } contain all unique words from the title, caption, and headings of the table. Mind that at this stage we are only considering the presence/absence of words. During the query-table similarity matching, the importance of the words will also be taken into account (Sect.&nbsp;<a class="sec" href="#sec-24">3.3.1</a>).</p>
        </section>
        <section id="sec-17">
          <p><em>3.1.2 Entity-based.</em> Many tables are focused on specific entities&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>]. Therefore, considering the entities contained in a table amounts to a meaningful representation of its content. We use the DBpedia knowledge base as our entity repository. Since we work with tables extracted from Wikipedia, the entity annotations are readily available (otherwise, entity annotations could be obtained automatically, see, e.g., &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>]). Importantly, instead of blindly including all entities mentioned in the table, we wish to focus on salient entities. It has been observed in prior work&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>] that tables often have a <em>core column</em>, containing mostly entities, while the rest of the columns contain properties of these entities (many of which are entities themselves). We write <em>E<sub>cc</sub></em> to denote the set of entities that are contained in the core column of the table, and describe our core column detection method in Sect.&nbsp;<a class="sec" href="#sec-18">3.1.3</a>. In addition to the entities taken directly from the body part of the table, we also include entities that are related to the page title (<em>T<sub>pt</sub></em> ) and to the table caption (<em>T<sub>tc</sub></em> ). We obtain those by using the page title and the table caption, respectively, to retrieve relevant entities from the knowledge base. We write <em>R<sub>k</sub></em> (<em>s</em>) to denote the set of top-<em>k</em> entities retrieved for the query <em>s</em>. We detail the entity ranking method in Sect.&nbsp;<a class="sec" href="#sec-19">3.1.4</a>. Finally, the table is represented as the union of three sets of entities, originating from the core column, page title, and table caption: {<em>t</em> <sub>1</sub>, …, <em>t<sub>m</sub></em> } = <em>E<sub>cc</sub></em> ∪<em>R<sub>k</sub></em> (<em>T<sub>pt</sub></em> )∪<em>R<sub>k</sub></em> (<em>T<sub>tc</sub></em> ).</p>
          <p>To get an entity-based representation for the query, we issue the query against a knowledge base to retrieve relevant entities, using the same retrieval method as above. I.e., {<em>q</em> <sub>1</sub>, …, <em>q<sub>n</sub></em> } = <em>R<sub>k</sub></em> (<em>q</em>).</p>
        </section>
        <section id="sec-18">
          <p><em>3.1.3 Core Column Detection.</em> We introduce a simple and effective core column detection method. It is based on the notion of <em>column entity rate</em>, which is defined as the ratio of cells in a column that contain an entity. We write <em>cer</em>(<em>T</em> <sub><em>c</em>[<em>j</em>]</sub>) to denote the column entity rate of column <em>j</em> in table <em>T</em>. Then, the index of the core column becomes: <span class="inline-equation"><span class="tex">$\arg \max _{j=1..T_{|c|}} \mathit {cer}(T_{c[j]})$</span></span> , where <em>T</em> <sub>|<em>c</em>|</sub> is the number of columns in <em>T</em>.</p>
        </section>
        <section id="sec-19">
          <p><em>3.1.4 Entity Retrieval.</em> We employ a fielded entity representation with five fields (names, categories, attributes, similar entity names, and related entity names) and rank entities using the Mixture of Language Models approach&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>]. The field weights are set uniformly. This corresponds to the MLM-all model in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] and is shown to be a solid baseline. We return the top-<em>k</em> entities, where <em>k</em> is set to 10.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span> <span class="table-title">Semantic similarity features. Each row represents 4 features (one for each similarity matching method, cf. Table&nbsp;<a class="tbl" href="#tab3">3</a>). All features are in [ − 1, 1].</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;"><strong>Features</strong></th>
                  <th style="text-align:center;"><strong>Semantic repr.</strong></th>
                  <th style="text-align:center;"><strong>Raw repr.</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">Entity_*</td>
                  <td style="text-align:center;">Bag-of-entities</td>
                  <td style="text-align:center;">entities</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Category_*</td>
                  <td style="text-align:center;">Bag-of-categories</td>
                  <td style="text-align:center;">entities</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Word_*</td>
                  <td style="text-align:center;">Word embeddings</td>
                  <td style="text-align:center;">words</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Graph_*</td>
                  <td style="text-align:center;">Graph embeddings</td>
                  <td style="text-align:center;">entities</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Semantic Representations</h3>
          </div>
        </header>
        <p>Next, we embed the query/table terms in a semantic space. That is, we map each table term <em>t<sub>i</sub></em> to a vector representation <span class="inline-equation"><span class="tex">$\vec{t}_i$</span></span> , where <span class="inline-equation"><span class="tex">$\vec{t}_i[j]$</span></span> refers to the <em>j</em>th element of that vector. For queries, the process goes analogously. We discuss two main kinds of semantic spaces, bag-of-concepts and embeddings, with two alternatives within each. The former uses sparse and discrete, while the latter employs dense and continuous-valued vectors. A particularly nice property of our semantic matching framework is that it allows us to deal with these two different types of representations in a unified way.</p>
        <section id="sec-21">
          <p><em>3.2.1 Bag-of-concepts.</em> One alternative for moving from the lexical to the semantic space is to represent tables/queries using specific concepts. In this work, we use entities and categories from a knowledge base. These two semantic spaces have been used in the past for various retrieval tasks, in duet with the traditional bag-of-words content representation. For example, entity-based representations have been used for document retrieval&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>] and category-based representations have been used for entity retrieval&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>]. One important difference from previous work is that instead of representing the entire query/table using a single semantic vector, we map each individual query/table term to a separate semantic vector, thereby obtaining a richer representation.</p>
          <p>We use the entity-based raw representation from the previous section, that is, <em>t<sub>i</sub></em> and <em>q<sub>j</sub></em> are specific entities. Below, we explain how table terms <em>t<sub>j</sub></em> are projected to <span class="inline-equation"><span class="tex">$\vec{t_i}$</span></span> , which is a sparse discrete vector in the entity/category space; for query terms it follows analogously.</p>
          <ul class="list-no-style">
            <li id="uid33" label="Bag-of-entities">Each element in <span class="inline-equation"><span class="tex">$\vec{t}_i$</span></span> corresponds to a unique entity. Thus, the dimensionality of <span class="inline-equation"><span class="tex">$\vec{t}_i$</span></span> is the number of entities in the knowledge base (on the order of millions). <span class="inline-equation"><span class="tex">$\vec{t}_i[j]$</span></span> has a value of 1 if entities <em>i</em> and <em>j</em> are related (there exists a link between them in the knowledge base), and 0 otherwise.<br /></li>
            <li id="uid34" label="Bag-of-categories">Each element in <span class="inline-equation"><span class="tex">$\vec{t}_i$</span></span> corresponds to a Wikipedia category. Thus, the dimensionality of <span class="inline-equation"><span class="tex">$\vec{t}_i$</span></span> amounts to the number of Wikipedia categories (on the order hundreds of thousands). The value of <span class="inline-equation"><span class="tex">$\vec{t}_i[j]$</span></span> is 1 if entity <em>i</em> is assigned to Wikipedia category <em>j</em>, and 0 otherwise.<br /></li>
          </ul>
        </section>
        <section id="sec-22">
          <p><em>3.2.2 Embeddings.</em> Recently, unsupervised representation learning methods have been proposed for obtaining embeddings that predict a distributional context, i.e., word embeddings&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>] or graph embeddings&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>]. Such vector representations have been utilized successfully in a range of IR tasks, including ad hoc retrieval&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>], contextual suggestion&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], cross-lingual IR&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0043">43</a>], community question answering&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>], short text similarity&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>], and sponsored search&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>]. We consider both word-based and entity-based raw representations from the previous section and use the corresponding (pre-trained) embeddings as follows.</p>
          <ul class="list-no-style">
            <li id="uid36" label="Word embeddings">We map each query/table word to a word embedding. Specifically, we use word2vec&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] with 300 dimensions, trained on Google News data.<br />
            </li>
            <li id="uid37" label="Graph embeddings">We map each query/table entity to a graph embedding. In particular, we use RDF2vec&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>] with 200 dimensions, trained on DBpedia 2015-10.<br />
            </li>
          </ul>
        </section>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Similarity Measures</h3>
          </div>
        </header>
        <p>The final step is concerned with the computation of the similarity between a query-table pair, based on the semantic vector representations we have obtained for them. We introduce two main strategies, which yield four specific similarity measures. These are summarized in Table&nbsp;<a class="tbl" href="#tab3">3</a>.</p>
        <section id="sec-24">
          <p><em>3.3.1 Early Fusion.</em> The first idea is to represent the query and the table each with a single vector. Their similarity can then simply be expressed as the similarity of the corresponding vectors. We let <span class="inline-equation"><span class="tex">$\vec{C}_q$</span></span> be the centroid of the query term vectors (<span class="inline-equation"><span class="tex">$\vec{C}_q = \sum _{i=1}^n \vec{q}_i/n$</span></span> ). Similarly, <span class="inline-equation"><span class="tex">$\vec{C}_T$</span></span> denotes the centroid of the table term vectors. The query-table similarity is then computed by taking the cosine similarity of the centroid vectors. When query/table content is represented in terms of words, we additionally make use of word importance by employing standard TF-IDF term weighting. Note that this only applies to word embeddings (as the other three semantic representations are based on entities). In case of word embeddings, the centroid vectors are calculated as <span class="inline-equation"><span class="tex">$\vec{C}_T = \sum _{i=1}^m \vec{t}_i \times TFIDF (t_i)$</span></span> . The computation of <span class="inline-equation"><span class="tex">$\vec{C}_q$</span></span> follows analogously.</p>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span> <span class="table-title">Similarity measures.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"><strong>Measure</strong></th>
                  <th style="text-align:left;"><strong>Equation</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Early</td>
                  <td style="text-align:left;"><span class="inline-equation"><span class="tex">$\cos (\vec{C}_{q}, \vec{C}_{T})$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">Late-max</td>
                  <td style="text-align:left;"><span class="inline-equation"><span class="tex">$\mathrm{max}(\lbrace \cos (\vec{q}_i,\vec{t}_j) : i \in [1..n], j \in [1..m] \rbrace)$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">Late-sum</td>
                  <td style="text-align:left;"><span class="inline-equation"><span class="tex">$\mathrm{sum}(\lbrace \cos (\vec{q}_i,\vec{t}_j) : i \in [1..n], j \in [1..m] \rbrace)$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">Late-avg</td>
                  <td style="text-align:left;"><span class="inline-equation"><span class="tex">$\mathrm{avg}(\lbrace \cos (\vec{q}_i,\vec{t}_j) : i \in [1..n], j \in [1..m] \rbrace)$</span></span></td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-25">
          <p><em>3.3.2 Late Fusion.</em> Instead of combining all semantic vectors <em>q<sub>i</sub></em> and <em>t<sub>j</sub></em> into a single one, late fusion computes the pairwise similarity between all query and table vectors first, and then aggregates those. We let <em>S</em> be a set that holds all pairwise cosine similarity scores: <span class="inline-equation"><span class="tex">$S = \lbrace \cos (\vec{q}_i,\vec{t}_j) : i \in [1..n], j \in [1..m] \rbrace$</span></span> . The query-table similarity score is then computed as aggr(<em>S</em>), where aggr() is an aggregation function. Specifically, we use max (), sum() and avg() as aggregators; see the last three rows in Table&nbsp;<a class="tbl" href="#tab3">3</a> for the equations.</p>
        </section>
      </section>
    </section>
    <section id="sec-26">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Test Collection</h2>
        </div>
      </header>
      <p>We introduce our test collection, including the table corpus, test and development query sets, and the procedure used for obtaining relevance assessments.</p>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Table Corpus</h3>
          </div>
        </header>
        <p>We use the WikiTables corpus&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>], which comprises 1.6M tables extracted from Wikipedia (dump date: 2015 October). The following information is provided for each table: table caption, column headings, table body, (Wikipedia) page title, section title, and table statistics like number of headings rows, columns, and data rows. We further replace all links in the table body with entity identifiers from the DBpedia knowledge base (version 2015-10) as follows. For each cell that contains a hyperlink, we check if it points to an entity that is present in DBpedia. If yes, we use the DBpedia identifier of the linked entity as the cell's content; otherwise, we replace the link with the anchor text, i.e., treat it as a string.</p>
      </section>
      <section id="sec-28">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Queries</h3>
          </div>
        </header>
        <p>We sample a total of 60 test queries from two independent sources (30 from each): (1) <em>Query subset 1 (QS-1)</em>: Cafarella et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] collected 51 queries from Web users via crowdsourcing (using Amazon's Mechanical Turk platform, users were asked to suggest topics or supply URLs for a useful data table). (2) <em>Query subset 2 (QS-2)</em>: Venetis et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>] analyzed the query logs from Google Squared (a service in which users search for structured data) and constructed 100 queries, all of which are a combination of an instance class (e.g., “laptops”) and a property (e.g., “cpu”). Following&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>], we concatenate the class and property fields into a single query string (e.g., “laptops cpu”). Table&nbsp;<a class="tbl" href="#tab4">4</a> lists some examples.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class="table-title">Example queries from our query set.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"><strong>Queries from&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]</strong></th>
                <th style="text-align:left;"><strong>Queries from&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>]</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">video games</td>
                <td style="text-align:left;">asian coutries currency</td>
              </tr>
              <tr>
                <td style="text-align:left;">us cities</td>
                <td style="text-align:left;">laptops cpu</td>
              </tr>
              <tr>
                <td style="text-align:left;">kings of africa</td>
                <td style="text-align:left;">food calories</td>
              </tr>
              <tr>
                <td style="text-align:left;">economy gdp</td>
                <td style="text-align:left;">guitars manufacturer</td>
              </tr>
              <tr>
                <td style="text-align:left;">fifa world cup winners</td>
                <td style="text-align:left;">clothes brand</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-29">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Relevance Assessments</h3>
          </div>
        </header>
        <p>We collect graded relevance assessments by employing three independent (trained) judges. For each query, we pool the top 20 results from five baseline methods (cf. Sect.&nbsp;<a class="sec" href="#sec-33">5.3</a>), using default parameter settings. (Then, we train the parameters of those methods with help of the obtained relevance labels.) Each query-table pair is judged on a three point scale: 0 (non-relevant), 1 (somewhat relevant), and 2 (highly relevant). Annotators were situated in a scenario where they need to create a table on the topic of the query, and wish to find relevant tables that can aid them in completing that task. Specifically, they were given the following labeling guidelines: (i) a table is <em>non-relevant</em> if it is unclear what it is about (e.g., misses headings or caption) or is about a different topic; (ii) a table is <em>relevant</em> if some cells or values could be used from this table; and (iii) a table is <em>highly relevant</em> if large blocks or several values could be used from it when creating a new table on the query topic.</p>
        <p>We take the majority vote as the relevance label; if no majority agreement is achieved, we take the average of the scores as the final label. To measure inter-annotator agreement, we compute the Kappa test statistics on test annotations, which is 0.47. According to&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>], this is considered as moderate agreement. In total, 3120 query-table pairs are annotated as test data. Out of these, 377 are labeled as highly relevant, 474 as relevant, and 2269 as non-relevant.</p>
      </section>
    </section>
    <section id="sec-30">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Evaluation</h2>
        </div>
      </header>
      <div class="table-responsive" id="tab5">
        <div class="table-caption">
          <span class="table-number">Table 5:</span> <span class="table-title">Table retrieval evaluation results.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;"><strong>Method</strong></th>
              <th style="text-align:left;"><strong>NDCG@5</strong></th>
              <th style="text-align:left;"><strong>NDCG@10</strong></th>
              <th style="text-align:left;"><strong>NDCG@15</strong></th>
              <th style="text-align:left;"><strong>NDCG@20</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;">Single-field document ranking</td>
              <td style="text-align:left;">0.4315</td>
              <td style="text-align:left;">0.4344</td>
              <td style="text-align:left;">0.4586</td>
              <td style="text-align:left;">0.5254</td>
            </tr>
            <tr>
              <td style="text-align:left;">Multi-field document ranking</td>
              <td style="text-align:left;">0.4770</td>
              <td style="text-align:left;">0.4860</td>
              <td style="text-align:left;">0.5170</td>
              <td style="text-align:left;">0.5473</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                WebTable&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
              </td>
              <td style="text-align:left;">0.2831</td>
              <td style="text-align:left;">0.2992</td>
              <td style="text-align:left;">0.3311</td>
              <td style="text-align:left;">0.3726</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                WikiTable&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
              </td>
              <td style="text-align:left;">0.4903</td>
              <td style="text-align:left;">0.4766</td>
              <td style="text-align:left;">0.5062</td>
              <td style="text-align:left;">0.5206</td>
            </tr>
            <tr>
              <td style="text-align:left;">LTR baseline (this paper)</td>
              <td style="text-align:left;">0.5527</td>
              <td style="text-align:left;">0.5456</td>
              <td style="text-align:left;">0.5738</td>
              <td style="text-align:left;">0.6031</td>
            </tr>
            <tr>
              <td style="text-align:left;">STR (this paper)</td>
              <td style="text-align:left;"><strong>0.5951</strong></td>
              <td style="text-align:left;"><strong>0.6293</strong>†</td>
              <td style="text-align:left;"><strong>0.6590</strong>‡</td>
              <td style="text-align:left;"><strong>0.6825</strong>†</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>In this section, we list our research questions (Sect.&nbsp;<a class="sec" href="#sec-31">5.1</a>), discuss our experimental setup (Sect.&nbsp;<a class="sec" href="#sec-32">5.2</a>), introduce the baselines we compare against (Sect.&nbsp;<a class="sec" href="#sec-33">5.3</a>), and present our results (Sect.&nbsp;<a class="sec" href="#sec-34">5.4</a>) followed by further analysis (Sect.&nbsp;<a class="sec" href="#sec-35">5.5</a>).</p>
      <section id="sec-31">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Research Questions</h3>
          </div>
        </header>
        <p>The research questions we seek to answer are as follows.</p>
        <ul class="list-no-style">
          <li id="uid48" label="RQ1">Can semantic matching improve retrieval performance?<br /></li>
          <li id="uid49" label="RQ2">Which of the semantic representations is the most effective?<br /></li>
          <li id="uid50" label="RQ3">Which of the similarity measures performs better?<br /></li>
        </ul>
      </section>
      <section id="sec-32">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Experimental Setup</h3>
          </div>
        </header>
        <p>We evaluate table retrieval performance in terms of Normalized Discounted Cumulative Gain (NDCG) at cut-off points 5, 10, 15, and 20. To test significance, we use a two-tailed paired t-test and write †/‡ to denote significance at the 0.05 and 0.005 levels, respectively.</p>
        <p>Our implementations are based on Nordlys&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>]. Many of our features involve external sources, which we explain below. To compute the entity-related features (i.e., features in Table&nbsp;<a class="tbl" href="#tab1">1</a> as well as the features based on the bag-of-entities and bag-of-categories representations in Table&nbsp;<a class="tbl" href="#tab2">2</a>), we use entities from the DBpedia knowledge base that have an abstract (4.6M in total). The table's Wikipedia rank (yRank) is obtained using Wikipedia's MediaWiki API. The PMI feature is estimated based on the ACSDb corpus&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. For the distributed representations, we take pre-trained embedding vectors, as explained in Sect.&nbsp;<a class="sec" href="#sec-22">3.2.2</a>.</p>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class="table-title">Comparison of semantic features, used in combination with baseline features (from Table&nbsp;<a class="tbl" href="#tab1">1</a>), in terms of NDCG@20. Relative improvements are shown in parentheses. Statistical significance is tested against the LTR baseline in Table&nbsp;<a class="tbl" href="#tab5">5</a>.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"><strong>Sem. Repr.</strong></th>
                <th style="text-align:left;"><strong>Early</strong></th>
                <th style="text-align:left;"><strong>Late-max</strong></th>
                <th style="text-align:left;"><strong>Late-sum</strong></th>
                <th style="text-align:left;"><strong>Late-avg</strong></th>
                <th style="text-align:left;"><strong>ALL</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Bag-of-entities</td>
                <td style="text-align:left;">0.6754 (+11.99%)</td>
                <td style="text-align:left;">0.6407 (+6.23%)†</td>
                <td style="text-align:left;">0.6697 (+11.04%)‡</td>
                <td style="text-align:left;">0.6733 (+11.64%)‡</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6696 (+11.03%)‡</td>
              </tr>
              <tr>
                <td style="text-align:left;">Bag-of-categories</td>
                <td style="text-align:left;">0.6287 (+4.19%)</td>
                <td style="text-align:left;">0.6245 (+3.55%)</td>
                <td style="text-align:left;">0.6315 (+4.71%)†</td>
                <td style="text-align:left;">0.6240 (+3.47%)</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6149 (+1.96%)</td>
              </tr>
              <tr>
                <td style="text-align:left;">Word embeddings</td>
                <td style="text-align:left;">0.6181 (+2.49%)</td>
                <td style="text-align:left;">0.6328 (+4.92%)</td>
                <td style="text-align:left;">0.6371 (+5.64%)†</td>
                <td style="text-align:left;">0.6485 (+7.53%)†</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6588 (+9.24%)†</td>
              </tr>
              <tr>
                <td style="text-align:left;">Graph embeddings</td>
                <td style="text-align:left;">0.6326 (+4.89%)</td>
                <td style="text-align:left;">0.6142 (+1.84%)</td>
                <td style="text-align:left;">0.6223 (+3.18%)</td>
                <td style="text-align:left;">0.6316 (+4.73%)</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6340 (+5.12%)</td>
              </tr>
              <tr>
                <td style="text-align:left;">ALL</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6736 (+11.69%)†</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6631 (+9.95%)†</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6831 (+13.26%)‡</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6809 (+12.90%)‡</td>
                <td style="text-align:left;" bgcolor="gray">0.850.6825 (13.17%)‡</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-33">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Baselines</h3>
          </div>
        </header>
        <p>We implement four baseline methods from the literature.</p>
        <ul class="list-no-style">
          <li id="uid54" label="Single-field document ranking">In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] tables are represented and ranked as ordinary documents. Specifically, we use Language Models with Dirichlet smoothing, and optimize the smoothing parameter using a parameter sweep.<br />
          </li>
          <li id="uid55" label="Multi-field document ranking">Pimplikar and Sarawagi [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] represent each table as a fielded document, using five fields: Wikipedia page title, table section title, table caption, table body, and table headings. We use the Mixture of Language Models approach&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>] for ranking. Field weights are optimized using the coordinate ascent algorithm; smoothing parameters are trained for each field individually.<br />
          </li>
          <li id="uid56" label="WebTable">The method by Cafarella et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] uses the features in Table&nbsp;<a class="tbl" href="#tab1">1</a> with [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] as source. Following&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>], we train a linear regression model with 5-fold cross-validation.<br />
          </li>
          <li id="uid57" label="WikiTable">The approach by Bhagavatula et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] uses the features in Table&nbsp;<a class="tbl" href="#tab1">1</a> with [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] as source. We train a Lasso model with coordinate ascent with 5-fold cross-validation.<br />
          </li>
        </ul>
        <p>Additionally, we introduce a learning-to-rank baseline:</p>
        <ul class="list-no-style">
          <li id="uid58" label="LTR baseline">It uses the full set of features listed in Table&nbsp;<a class="tbl" href="#tab1">1</a>. We employ pointwise regression using the Random Forest algorithm.<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>We set the number of trees to 1000 and the maximum number of features in each tree to 3. We train the model using 5-fold cross-validation (w.r.t. NDCG@20); reported results are averaged over 5 runs.<br />
          </li>
        </ul>
        <p>The baseline results are presented in the top block of Table&nbsp;<a class="tbl" href="#tab5">5</a>. It can be seen from this table that our LTR baseline (row five) outperforms all existing methods from the literature; the differences are substantial and statistically significant. Therefore, in the remainder of this paper, we shall compare against this strong baseline, using the same learning algorithm (Random Forests) and parameter settings. We note that our emphasis is on the semantic matching features and not on the supervised learning algorithm.</p>
      </section>
      <section id="sec-34">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.4</span> Experimental Results</h3>
          </div>
        </header>
        <p>The last line of Table&nbsp;<a class="tbl" href="#tab5">5</a> shows the results for our semantic table retrieval (STR) method. It combines the baseline set of features (Table&nbsp;<a class="tbl" href="#tab1">1</a>) with the set of novel semantic matching features (from Table&nbsp;<a class="tbl" href="#tab2">2</a>, 16 in total). We find that these semantic features bring in substantial and statistically significant improvements over the LTR baseline. Thus, we answer RQ1 positively. The relative improvements range from 7.6% to 15.3%, depending on the rank cut-off.</p>
        <p>To answer RQ2 and RQ3, we report on all combinations of semantic representations and similarity measures in Table&nbsp;<a class="tbl" href="#tab6">6</a>. In the interest of space, we only report on NDCG@20; the same trends were observed for other NDCG cut-offs. Cells with a white background show retrieval performance when extending the LTR baseline with a single feature. Cells with a grey background correspond to using a given semantic representation with different similarity measures (rows) or using a given similarity measure with different semantic representations (columns). The first observation is that all features improve over the baseline, albeit not all of these improvements are statistically significant. Concerning the comparison of different semantic representations (RQ2), we find that bag-of-entities and word embeddings achieve significant improvements; see the rightmost column of Table&nbsp;<a class="tbl" href="#tab6">6</a>. It is worth pointing out that for word embeddings the four similarity measures seem to complement each other, as their combined performance is better than that of any individual method. It is not the case for bag-of-entities, where only one of the similarity measures (Late-max) is improved by the combination. Overall, in answer to RQ2, we find the bag-of-entities representation to be the most effective one. The fact that this sparse representation outperforms word embeddings is regarded as a somewhat surprising finding, given that the latter has been trained on massive amounts of (external) data.</p>
        <p>As for the choice of similarity measure (RQ3), it is difficult to name a clear winner when a single semantic representation is used. The relative differences between similarity measures are generally small (below 5%). When all four semantic representations are used (bottom row in Table&nbsp;<a class="tbl" href="#tab6">6</a>), we find that Late-sum and Late-avg achieve the highest overall improvement. Importantly, when using all semantic representations, all four similarity measures improve significantly and substantially over the baseline. We further note that the combination of all similarity measures do not yield further improvements over Late-sum or Late-avg. In answer to RQ3, we identify the late fusion strategy with sum or avg aggregation (i.e., Late-sum or Late-avg) as the preferred similarity method.</p>
      </section>
      <section id="sec-35">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.5</span> Analysis</h3>
          </div>
        </header>
        <p>We continue with further analysis of our results.</p>
        <section id="sec-36">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.5.1</span> Features</h4>
            </div>
          </header>
          <figure id="fig4">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span> <span class="figure-title">Normalized feature importance (measured in terms of Gini score).</span>
            </div>
          </figure>
          <p>Figure&nbsp;<a class="fig" href="#fig4">4</a> shows the importance of individual features for the table retrieval task, measured in terms of Gini importance. The novel features are distinguished by color. We observe that 8 out of the top 10 features are semantic features introduced in this paper.</p>
          <figure id="fig5">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span> <span class="figure-title">Distribution of query-level differences between the LTR baseline and a given semantic representation.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-37">
          <p><em>5.5.2 Semantic Representations.</em> To analyze how the four semantic representations affect retrieval performance on the level of individual queries, we plot the difference between the LTR baseline and each semantic representation in Figure&nbsp;<a class="fig" href="#fig5">5</a>. The histograms show the distribution of queries according to NDCG@20 score difference (<em>Δ</em>): the middle bar represents no change (<em>Δ</em> &lt; 0.05), while the leftmost and rightmost bars represents the number of queries that were hurt and helped substantially, respectively (<em>Δ</em> &gt; 0.25). We observe similar patterns for the bag-of-entities and word embeddings representations; the former has less queries that were significantly helped or hurt, while the overall improvement (over all topics) is larger. We further note the similarity of the shapes of the distributions for bag-of-categories and graph embeddings.</p>
        </section>
        <section id="sec-38">
          <p><em>5.5.3 Query Subsets.</em> On Figure&nbsp;<a class="fig" href="#fig6">6</a>, we plot the results for the LTR baseline and for our STR method according to the two query subsets, QS-1 and QS-2, in terms of NDCG@20. Generally, both methods perform better on QS-1 than on QS-2. This is mainly because QS-2 queries are more focused (each targeting a specific type of instance, with a required property), and thus are considered more difficult. Importantly, STR achieves consistent improvements over LTR on both query subsets.</p>
        </section>
        <section id="sec-39">
          <p><em>5.5.4 Individual Queries.</em> We plot the difference between the LTR baseline and STR for the two query subsets in Figure&nbsp;<a class="fig" href="#fig7">7</a>. Table&nbsp;<a class="tbl" href="#tab7">7</a> lists the queries that we discuss below. The leftmost bar in Figure&nbsp;7(a) corresponds to the query “<em>stocks</em>.” For this broad query, there are two relevant and one highly relevant tables. LTR does not retrieve any highly relevant tables in the top 20, while STR manages to return one highly relevant table in the top 10. The rightmost bar in Figure&nbsp;7(a) corresponds to the query “<em>ibanez guitars</em>.” For this query, there are two relevant and one highly relevant tables. LTR produces an almost perfect ranking for this query, by returning the highly relevant table at the top rank, and the two relevant tables at ranks 2 and 4. STR returns a non-relevant table at the top rank, thereby pushing the relevant results down in the ranking by a single position, resulting in a decrease of 0.29 in NDCG@20.</p>
          <p>The leftmost bar in Figure&nbsp;7(b) corresponds to the query “<em>board games number of players</em>.” For this query, there are only two relevant tables according to the ground truth. STR managed to place them in the 1st and 3rd rank positions, while LTR returned only one of them at position 13th. The rightmost bar in Figure&nbsp;7(b) is the query “<em>cereals nutritional value</em>.” Here, there is only one highly relevant result. LTR managed to place it in rank one, while it is ranked eighth by STR. Another interesting query is “<em>irish counties area</em>” (third bar from the left in Figure&nbsp;7(b)), with three highly relevant and three relevant results according to the ground truth. LTR returned two highly relevant and one relevant results at ranks 1, 2, and 4. STR, on the other hand, placed the three highly relevant results in the top 3 positions and also returned the three relevant tables at positions 4, 6, and 7.</p>
          <figure id="fig6">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig6.jpg" class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span> <span class="figure-title">Table retrieval results, LTR baseline vs. STR, on the two query subsets in terms of NDCG@20.</span>
            </div>
          </figure>
          <figure id="fig7">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186067/images/www2018-76-fig7.jpg" class="img-responsive" alt="Figure 7" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 7:</span> <span class="figure-title">Query-level differences on the two query subsets between the LTR baseline and STR. Positive values indicate improvements made by the latter.</span>
            </div>
          </figure>
          <p></p>
          <div class="table-responsive" id="tab7">
            <div class="table-caption">
              <span class="table-number">Table 7:</span> <span class="table-title">Example queries from our query set. Rel denotes table relevance level. LTR and STR refer to the positions on which the table is returned by the respective method.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"><strong>Query</strong></th>
                  <th>Rel</th>
                  <th>LTR</th>
                  <th>STR</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">QS-1-24: <em>stocks</em></td>
                  <td></td>
                  <td></td>
                  <td></td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Stocks for the Long Run / Key Data Findings: annual real returns</td>
                  <td>2</td>
                  <td>-</td>
                  <td>6</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; TOPIX / TOPIX New Index Series</td>
                  <td>1</td>
                  <td>9</td>
                  <td>-</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Hang Seng Index / Selection criteria for the HSI constituent stocks</td>
                  <td>1</td>
                  <td>-</td>
                  <td>-</td>
                </tr>
                <tr>
                  <td style="text-align:left;">QS-1-21: <em>ibanez guitars</em></td>
                  <td></td>
                  <td></td>
                  <td></td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Ibanez / Serial numbers</td>
                  <td>2</td>
                  <td>1</td>
                  <td>2</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Corey Taylor / Equipment</td>
                  <td>1</td>
                  <td>2</td>
                  <td>3</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Fingerboard / Examples</td>
                  <td>1</td>
                  <td>4</td>
                  <td>5</td>
                </tr>
                <tr>
                  <td style="text-align:left;">QS-2-27: <em>board games number of players</em></td>
                  <td></td>
                  <td></td>
                  <td></td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; List of Japanese board games</td>
                  <td>1</td>
                  <td>13</td>
                  <td>1</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; List of licensed Risk game boards / Risk Legacy</td>
                  <td>1</td>
                  <td>-</td>
                  <td>3</td>
                </tr>
                <tr>
                  <td style="text-align:left;">QS-2-21: <em>cereals nutritional value</em></td>
                  <td></td>
                  <td></td>
                  <td></td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Sesame / Sesame seed kernels, toasted</td>
                  <td>2</td>
                  <td>1</td>
                  <td>8</td>
                </tr>
                <tr>
                  <td style="text-align:left;">QS-2-20: <em>irish counties area</em></td>
                  <td></td>
                  <td></td>
                  <td></td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Counties of Ireland / List of counties</td>
                  <td>2</td>
                  <td>2</td>
                  <td>1</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; List of Irish counties by area / See also</td>
                  <td>2</td>
                  <td>1</td>
                  <td>2</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; List of flags of Ireland / Counties of Ireland Flags</td>
                  <td>2</td>
                  <td>-</td>
                  <td>3</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Provinces of Ireland / Demographics and politics</td>
                  <td>1</td>
                  <td>4</td>
                  <td>4</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Toponymical list of counties of the United Kingdom / Northern ...</td>
                  <td>1</td>
                  <td>-</td>
                  <td>7</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;&nbsp; Múscraige / Notes</td>
                  <td>1</td>
                  <td>-</td>
                  <td>6</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
    </section>
    <section id="sec-40">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Related Work</h2>
        </div>
      </header>
      <p>There is an increasing amount of work on tables, addressing a wide range of tasks, including table search, table mining, table extension, and table completion. Table search is a fundamental problem on its own, as well as used often as a core component in other tasks.</p>
      <p><em>Table Search</em>. Users are likely to search for tables when they need structured or relational data. Cafarella et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] pioneered the table search task by introducing the WebTables system. The basic idea is to fetch the top-ranked results returned by a web search engine in response to the query, and then extract the top-<em>k</em> tables from those pages. Further refinements to the same idea are introduced in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. Venetis et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>] leverage a database of class labels and relationships extracted from the Web, which are attached to table columns, for recovering table semantics. This information is then used to enhance table search. Pimplikar and Sarawagi [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] search for tables using column keywords, and match these keywords against the header, body, and context of tables. Google Web Tables<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> provides an example of a table search system interface; the developers’ experiences are summarized in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>]. To enrich the diversity of search results, Nguyen et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>] design a goodness measure for table search and selection. Apart from keyword-based search, tables may also be retrieved using a given “local” table as the query&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>]. We are not aware of any work that performs semantic matching of tables against queries.</p>
      <p><em>Table Extension/Completion</em>. <em>Table extension</em> refers to the task of extending a table with additional elements, which are typically new columns&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>]. These methods commonly use table search as the first step&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>]. Searching related tables is also used for row extension. In&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>], two tasks of entity complement and schema complement are addressed, to extend entity rows and columns respectively. Zhang and Balog [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>] populate row and column headings of tables that have an entity focus. <em>Table completion</em> is the task of filling in empty cells within a table. Ahmadov et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] introduce a method to extract table values from related tables and/or to predict them using machine learning methods.</p>
      <p><em>Table Mining</em>. The abundance of information in tables has raised great interest in table mining research&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0047">47</a>]. Munoz et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>] recover table semantics by extracting RDF triples from Wikipedia tables. Similarly, Cafarella et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] mine table relations from a huge table corpus extracted from a Google crawl. Tables could also be searched to answer questions or mined to extend knowledge bases. Yin et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>] take tables as a knowledge base to execute queries using deep neural networks. Sekhavat et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>] augment an existing knowledge base (YAGO) with a probabilistic method by making use of table information. Similar work is carried out in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], with tabular information used for knowledge base augmentation. Another line of work concerns table annotation and classification. Zwicklbauer et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0051">51</a>] introduce a method to annotate table headers by mining column content. Crestan and Pantel [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] introduce a supervised framework for classifying HTML tables into a taxonomy by examining the contents of a large number of tables. Apart from all the mentioned methods above, table mining also includes tasks like <em>table interpretation</em>&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>] and <em>table recognition</em>&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0051">51</a>]. In the problem space of table mining, table search is an essential component.</p>
    </section>
    <section id="sec-41">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we have introduced and addressed the problem of ad hoc table retrieval: answering a keyword query with a ranked list of tables. We have developed a novel semantic matching framework, where queries and tables can be represented using semantic concepts (bag-of-entities and bag-of-categories) as well as continuous dense vectors (word and graph embeddings) in a uniform way. We have introduced multiple similarity measures for matching those semantic representations. For evaluation, we have used a purpose-built test collection based on Wikipedia tables. Finally, we have demonstrated substantial and significant improvements over a strong baseline. In future work, we wish to relax our requirements regarding the focus on Wikipedia tables, and make our methods applicable to other types of tables, like scientific tables&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] or Web tables.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Ahmad Ahmadov, Maik Thiele, Julian Eberius, Wolfgang Lehner, and Robert Wrembel. 2015. Towards a Hybrid Imputation Approach Using Web Tables.. In <em><em>Proc. of BDC ’15</em></em> . 21–30.</li>
        <li id="BibPLXBIB0002" label="[2]">Sreeram Balakrishnan, Alon&nbsp;Y. Halevy, Boulos Harb, Hongrae Lee, Jayant Madhavan, Afshin Rostamizadeh, Warren Shen, Kenneth Wilder, Fei Wu, and Cong Yu. 2015. Applying WebTables in Practice. In <em><em>Proc. of CIDR ’15</em></em> .</li>
        <li id="BibPLXBIB0003" label="[3]">Krisztian Balog, Marc Bron, and Maarten De&nbsp;Rijke. 2011. Query modeling for entity search based on terms, categories, and examples. <em><em>ACM Trans. Inf. Syst.</em></em> 29, 4, Article 22 (Dec. 2011), 22:1–22:31&nbsp;pages.</li>
        <li id="BibPLXBIB0004" label="[4]">Chandra&nbsp;Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2013. Methods for Exploring and Mining Tables on Wikipedia. In <em><em>Proc. of IDEA ’13</em></em> . 18–26.</li>
        <li id="BibPLXBIB0005" label="[5]">Chandra&nbsp;Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. TabEL: Entity Linking in Web Tables. In <em><em>Proc. of ISWC ’15</em></em> . 425–441.</li>
        <li id="BibPLXBIB0006" label="[6]">Michael&nbsp;J. Cafarella, Alon Halevy, and Nodira Khoussainova. 2009. Data Integration for the Relational Web. <em><em>Proc. of VLDB Endow.</em></em> 2(2009), 1090–1101.</li>
        <li id="BibPLXBIB0007" label="[7]">Michael&nbsp;J. Cafarella, Alon Halevy, and Jayant Madhavan. 2011. Structured Data on the Web. <em><em>Commun. ACM</em></em> 54(2011), 72–79.</li>
        <li id="BibPLXBIB0008" label="[8]">Michael&nbsp;J. Cafarella, Alon Halevy, Daisy&nbsp;Zhe Wang, Eugene Wu, and Yang Zhang. 2008. Uncovering the Relational Web. In <em><em>Proc. of WebDB ’08</em></em> .</li>
        <li id="BibPLXBIB0009" label="[9]">Michael&nbsp;J. Cafarella, Alon Halevy, Daisy&nbsp;Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: Exploring the Power of Tables on the Web. <em><em>Proc. of VLDB Endow.</em></em> 1(2008), 538–549.</li>
        <li id="BibPLXBIB0010" label="[10]">Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An Empirical Study of Learning to Rank for Entity Search. In <em><em>Proc. of SIGIR ’16</em></em> . 737–740.</li>
        <li id="BibPLXBIB0011" label="[11]">Eric Crestan and Patrick Pantel. 2011. Web-scale Table Census and Classification. In <em><em>Proc. of WSDM ’11</em></em> . 545–554.</li>
        <li id="BibPLXBIB0012" label="[12]">Anish Das&nbsp;Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding Related Tables. In <em><em>Proc. of SIGMOD ’12</em></em> . 817–828.</li>
        <li id="BibPLXBIB0013" label="[13]">Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion. In <em><em>Proc. of KDD ’14</em></em> . 601–610.</li>
        <li id="BibPLXBIB0014" label="[14]">J.L. Fleiss and others. 1971. Measuring nominal scale agreement among many raters. <em><em>Psychological Bulletin</em></em> 76 (1971), 378–382.</li>
        <li id="BibPLXBIB0015" label="[15]">Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth&nbsp;J.F. Jones. 2015. Word Embedding Based Generalized Language Model for Information Retrieval. In <em><em>Proc. of SIGIR ’15</em></em> . 795–798.</li>
        <li id="BibPLXBIB0016" label="[16]">Kyle&nbsp;Yingkai Gao and Jamie Callan. 2017. Scientific Table Search Using Keyword Queries. <em><em>CoRR</em></em> abs/1707.03423(2017).</li>
        <li id="BibPLXBIB0017" label="[17]">Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, and Narayan Bhamidipati. 2015. Context- and Content-aware Embeddings for Query Rewriting in Sponsored Search. In <em><em>Proc. of SIGIR ’15</em></em> . 383–392.</li>
        <li id="BibPLXBIB0018" label="[18]">Faegheh Hasibi, Krisztian Balog, Darío Garigliotti, and Shuo Zhang. 2017. Nordlys: A Toolkit for Entity-Oriented and Semantic Search. In <em><em>Proceedings of SIGIR ’17</em></em> . 1289–1292.</li>
        <li id="BibPLXBIB0019" label="[19]">Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein&nbsp;Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. DBpedia-Entity V2: A Test Collection for Entity Search. In <em><em>Proc. of SIGIR ’17</em></em> . 1265–1268.</li>
        <li id="BibPLXBIB0020" label="[20]">Tom Kenter and Maarten de Rijke. 2015. Short Text Similarity with Word Embeddings. In <em><em>Proc. of CIKM ’15</em></em> . 1411–1420.</li>
        <li id="BibPLXBIB0021" label="[21]">Oliver Lehmberg, Dominique Ritze, Petar Ristoski, Robert Meusel, Heiko Paulheim, and Christian Bizer. 2015. The Mannheim Search Join Engine. <em><em>Web Semant.</em></em> 35(2015), 159–166.</li>
        <li id="BibPLXBIB0022" label="[22]">Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and Searching Web Tables Using Entities, Types and Relationships. <em><em>Proc. of VLDB Endow.</em></em> 3(2010), 1338–1347.</li>
        <li id="BibPLXBIB0023" label="[23]">Tie-Yan Liu. 2011. <em><em>Learning to Rank for Information Retrieval</em></em> . Springer Berlin Heidelberg.</li>
        <li id="BibPLXBIB0024" label="[24]">Craig Macdonald, Rodrygo L&nbsp;T Santos, and Iadh Ounis. 2012. On the Usefulness of Query Features for Learning to Rank. In <em><em>Proc. of CIKM ’12</em></em> . 2559–2562.</li>
        <li id="BibPLXBIB0025" label="[25]">Jayant Madhavan, Loredana Afanasiev, Lyublena Antova, and Alon&nbsp;Y. Halevy. 2009. Harnessing the Deep Web: Present and Future. <em><em>CoRR</em></em> abs/0909.1785(2009).</li>
        <li id="BibPLXBIB0026" label="[26]">Jarana Manotumruksa, Craig MacDonald, and Iadh Ounis. 2016. Modelling User Preferences using Word Embeddings for Context-Aware Venue Recommendation. <em><em>CoRR</em></em> abs/1606.07828(2016).</li>
        <li id="BibPLXBIB0027" label="[27]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In <em><em>Proc. of NIPS ’13</em></em> . 3111–3119.</li>
        <li id="BibPLXBIB0028" label="[28]">Bhaskar Mitra, Eric&nbsp;T. Nalisnick, Nick Craswell, and Rich Caruana. 2016. A Dual Embedding Space Model for Document Ranking. <em><em>CoRR</em></em> abs/1602.01137(2016).</li>
        <li id="BibPLXBIB0029" label="[29]">Emir Munoz, Aidan Hogan, and Alessandra Mileo. 2014. Using Linked Data to Mine RDF from Wikipedia's Tables. In <em><em>Proc. of WSDM ’14</em></em> . 533–542.</li>
        <li id="BibPLXBIB0030" label="[30]">Thanh&nbsp;Tam Nguyen, Quoc Viet&nbsp;Hung Nguyen, Weidlich Matthias, and Aberer Karl. 2015. Result Selection and Summarization for Web Table Search. In <em><em>ISDE ’15</em></em> . 231–242.</li>
        <li id="BibPLXBIB0031" label="[31]">Paul Ogilvie and Jamie Callan. 2003. Combining Document Representations for Known-item Search. In <em><em>Proc. of SIGIR ’03</em></em> . 143–150.</li>
        <li id="BibPLXBIB0032" label="[32]">Jeffrey Pennington, Richard Socher, and Christopher&nbsp;D Manning. 2014. GloVe: Global Vectors for Word Representation. In <em><em>Proc. of EMNLP ’14</em></em> . 1532–1543.</li>
        <li id="BibPLXBIB0033" label="[33]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In <em><em>Proc. of KDD ’14</em></em> . 701–710.</li>
        <li id="BibPLXBIB0034" label="[34]">Rakesh Pimplikar and Sunita Sarawagi. 2012. Answering Table Queries on the Web Using Column Keywords. <em><em>Proc. of VLDB Endow.</em></em> 5(2012), 908–919.</li>
        <li id="BibPLXBIB0035" label="[35]">Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval. <em><em>Inf. Retr.</em></em> 13, 4 (Aug 2010), 346–374.</li>
        <li id="BibPLXBIB0036" label="[36]">Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document Retrieval Using Entity-Based Language Models. In <em><em>Proc. of SIGIR ’16</em></em> . 65–74.</li>
        <li id="BibPLXBIB0037" label="[37]">Petar Ristoski and Heiko Paulheim. 2016. RDF2vec: RDF Graph Embeddings for Data Mining. In <em><em>Proc. of ISWC ’16</em></em> . 498–514.</li>
        <li id="BibPLXBIB0038" label="[38]">Sunita Sarawagi and Soumen Chakrabarti. 2014. Open-domain Quantity Queries on Web Tables: Annotation, Response, and Consensus Models. In <em><em>Proc. of KDD ’14</em></em> . 711–720.</li>
        <li id="BibPLXBIB0039" label="[39]">Yoones&nbsp;A. Sekhavat, Francesco&nbsp;Di Paolo, Denilson Barbosa, and Paolo Merialdo. 2014. Knowledge Base Augmentation using Tabular Data. In <em><em>Proc. of LDOW ’14</em></em> .</li>
        <li id="BibPLXBIB0040" label="[40]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. In <em><em>Proc. of WWW ’15</em></em> . 1067–1077.</li>
        <li id="BibPLXBIB0041" label="[41]">Stephen Tyree, Kilian&nbsp;Q Weinberger, Kunal Agrawal, and Jennifer Paykin. 2011. Parallel Boosted Regression Trees for Web Search Ranking. In <em><em>Proc. of WWW ’11</em></em> . 387–396.</li>
        <li id="BibPLXBIB0042" label="[42]">Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei Wu, Gengxin Miao, and Chung Wu. 2011. Recovering Semantics of Tables on the Web. <em><em>Proc. of VLDB Endow.</em></em> 4(2011), 528–538.</li>
        <li id="BibPLXBIB0043" label="[43]">Ivan Vulić and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings. In <em><em>Proc. of SIGIR ’15</em></em> . 363–372.</li>
        <li id="BibPLXBIB0044" label="[44]">Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-Entity Duet Representations for Document Ranking. In <em><em>Proc. of SIGIR ’17</em></em> . 763–772.</li>
        <li id="BibPLXBIB0045" label="[45]">Mohamed Yakout, Kris Ganjam, Kaushik Chakrabarti, and Surajit Chaudhuri. 2012. InfoGather: Entity Augmentation and Attribute Discovery by Holistic Matching with Web Tables. In <em><em>Proc. of SIGMOD ’12</em></em> . 97–108.</li>
        <li id="BibPLXBIB0046" label="[46]">Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao. 2016. Neural Enquirer: Learning to Query Tables in Natural Language. In <em><em>Proc. of IJCAI ’16</em></em> . 2308–2314.</li>
        <li id="BibPLXBIB0047" label="[47]">Meihui Zhang and Kaushik Chakrabarti. 2013. InfoGather+: Semantic Matching and Annotation of Numeric and Time-varying Attributes in Web Tables. In <em><em>Proc. of SIGMOD ’13</em></em> . 145–156.</li>
        <li id="BibPLXBIB0048" label="[48]">Shuo Zhang and Krisztian Balog. 2017. Design Patterns for Fusion-Based Object Retrieval. In <em><em>Proc. of ECIR ’17</em></em> . 684–690.</li>
        <li id="BibPLXBIB0049" label="[49]">Shuo Zhang and Krisztian Balog. 2017. EntiTables: Smart Assistance for Entity-Focused Tables. In <em><em>Proc. of SIGIR ’17</em></em> . 255–264.</li>
        <li id="BibPLXBIB0050" label="[50]">Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering. In <em><em>Proc. of ACL ’15</em></em> . 250–259.</li>
        <li id="BibPLXBIB0051" label="[51]">Stefan Zwicklbauer, Christoph Einsiedler, Michael Granitzer, and Christin Seifert. 2013. Towards Disambiguating Web Tables. In <em><em>Proc. of ISWC-PD ’13</em></em> . 205–208.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>We also experimented with Gradient Boosting regression and Support Vector Regression, and observed the same general patterns regarding feature importance. However, their overall performance was lower than that of Random Forests.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://research.google.com/tables">https://research.google.com/tables</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3178876.3186067">https://doi.org/10.1145/3178876.3186067</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
