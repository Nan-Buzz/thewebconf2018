<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Pitfalls of Affective Computing</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Pitfalls of Affective Computing</span>      <br/>      <span class="subTitle">       <SubTitle>How can the automatic visual communication of emotions lead to harm, and what can be done to mitigate such risks?</SubTitle>      </span>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Martin</span>      <span class="surName">Cooney</span>     Center for Applied Intelligent Systems Research (CAISR), Halmstad University,, Halmstad, Sweden     </div>     <div class="author">     <span class="givenName">Sepideh</span>      <span class="surName">Pashami</span>     Center for Applied Intelligent Systems Research (CAISR), Halmstad University,, Halmstad, Sweden     </div>     <div class="author">     <span class="givenName">Anita</span>      <span class="surName">Sant&#x0027;Anna</span>     Center for Applied Intelligent Systems Research (CAISR), Halmstad University,, Halmstad, Sweden     </div>     <div class="author">     <span class="givenName">Yuantao</span>      <span class="surName">Fan</span>     Center for Applied Intelligent Systems Research (CAISR), Halmstad University,, Halmstad, Sweden     </div>     <div class="author">     <span class="givenName">S&#x0142;awomir</span>      <span class="surName">Nowaczyk</span>     Center for Applied Intelligent Systems Research (CAISR), Halmstad University,, Halmstad, Sweden, <a href="mailto:martin.cooney@hh.se, sepideh.pashami@hh.se, anita.santanna@hh.se, yuantao.fan@hh.se, slawomir.nowaczyk@hh.se">martin.cooney@hh.se, sepideh.pashami@hh.se, anita.santanna@hh.se, yuantao.fan@hh.se, slawomir.nowaczyk@hh.se</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191611" target="_blank">https://doi.org/10.1145/3184558.3191611</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>What would happen in a world where people could &#x201C;see&#x201D; others&#x2019; hidden emotions directly through some visualizing technology? Would lies become uncommon and would we understand each other better? Or to the contrary, would such forced honesty make it impossible for a society to exist?</small>     </p>     <p>     <small>The science fiction television show Black Mirror has exposed a number of darker scenarios in which such futuristic technologies, by blurring the lines of what is private and what is not, could also catalyze suffering. Thus, the current paper first turns an eye towards identifying some potential pitfalls in emotion visualization which could lead to psychological or physical harm, miscommunication, and disempowerment. Then, some countermeasures are proposed and discussed &#x2013; including some level of control over what is visualized and provision of suitably rich emotional information comprising intentions &#x2013; toward facilitating a future in which emotion visualization could contribute toward people&#x0027;s well-being.</small>     </p>     <p>     <small>The scenarios presented here are not limited to web technologies, since one typically thinks about emotion recognition primarily in the context of direct contact. However, as interfaces develop beyond today&#x0027;s keyboard and monitor, more information becomes available also at a distance &#x2013; for example, speech-to-text software could evolve to annotate any dictated text with a speaker&#x0027;s emotional state.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <em>Sentiment analysis;</em> &#x2022;<strong> Security and privacy </strong>&#x2192; <em>Human and societal aspects of security and privacy;</em> &#x2022;<strong> Human-centered computing </strong>&#x2192; <em>Visualization systems and tools;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Affective computing; emotion visualization; Black Mirror; privacy; ethics; intention recognition</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Martin Cooney, Sepideh Pashami, Anita Sant&#x0027;Anna, Yuantao Fan, and S&#x0142;awomir Nowaczyk. 2018. Pitfalls of Affective Computing. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018 (WWW &#x2019;18 Companion),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 4 Pages. <a href="https://doi.org/10.1145/3184558.3191611" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191611</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Research in affective computing, emotion recognition, and sentiment analysis aims to improve people&#x0027;s well-being by enabling computers and robots to better make decisions and serve, through awareness of people&#x0027;s emotions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>]. Emotions can be recognized, with varying degrees of accuracy, from various signals, including facial expressions, gestures, and voices, using wearables or remote sensors (e.g., galvanic skin response, brain machine interfaces, and cameras) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>]. As more and more of such sensors begin to appear in our surroundings, the capability to recognize emotions can potentially become something available globally, irrespective of physical closeness, thus affecting also web technologies.</p>    <p>Recognition results can be not only used by computers to plan appropriate actions, but also visualized for humans. For example, in our previous work we explored the usage of a robot to paint based on emotions detected in a person using a brain machine interface and thermal camera, as shown in Fig. <a class="fig" href="#fig1">1</a>. Such a system could one day be useful to help people with autism, depression, trauma, or alexithymia (difficulty recognizing one&#x0027;s emotions) to communicate and investigate their emotions. Other examples, which did not require a robot, have shown emotions through movements or colored lights in kinetic clothing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>].</p>    <p>In the current paper we envision a future in which such technologies perform with high accuracy and are widespread, so that people&#x0027;s emotions can typically be seen by others. We note that humans are generally adept at recognizing certain types of emotions, but the process can certainly be facilitated and well-hidden emotions can require technology to expose (in fact, people might often not be <em>fully</em> aware of their own emotions, much less those of others). Such emotion visualization could offer a number of benefits in many areas, e.g., for artistic expression; facilitating empathizing and emotional resonance; detecting dangerous situations rapidly; and exposing potentially bias in legal workers such as judges or jurors, or falsehood in relationships. However, despite good intentions, such capabilities also present significant dangers, in exposing a person&#x0027;s inner, honest, and potentially vulnerable state for others to see. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191611/images/www18companion-350-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Example of emotion visualization: (a) A robot expresses anger through (b) color, lines, and composition, as well as (c) its facial expression, sounds, and gestures.</span>     </div>     </figure>    </p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Potential Pitfalls</h2>     </div>    </header>    <p>In considering the potential dangers of emotion visualization, we were reminded of some scenarios in Black Mirror, a television series which investigates how weaknesses in human nature could drive the misuse of future technologies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. In particular, episodes 2 and 3 of season 4, &#x201C;Arkangel&#x201D; and &#x201C;Crocodile&#x201D;, depict bleak scenarios in which a capability to see into another person&#x0027;s mind invites trouble. In Arkangel, a mother&#x0027;s urge to protect her daughter by secretly monitoring her actions, and manipulating her perception and relationships, leads to her daughter developing psychological problems, and violence. In Crocodile, technology exposing a dark secret leads a person to feel anguish and fear, which pushes them towards committing a string of murders. In a similar way, we foresee how emotion visualization could exhibit &#x201C;enantiodromia&#x201D; (the propensity of a system to proceed toward its opposite), by seeking to help but in practice potentially leading to psychological or physical harm, miscommunication, or empowerment of computers at the expense of humans; or one group of humans against another.</p>    <section id="sec-4">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Psychological harm</h3>     </div>     </header>     <p>Emotion visualization could harm people psychologically by revealing information which people do not wish to reveal, or contributing to a weakening of social skills, moral values, and &#x201C;lebensfreude&#x201D; (joy of life). For example, exposure of potential emotional problems (e.g. dementia, depression, or obsessive compulsive disorders) for all to see might be undesirable for some persons due to social stigma; we imagine that some people could also try to restrain their emotions, and feel fear that some hidden feeling might be expressed. Indeed, control over the displaying of emotions is often lacking; emotions can be leaked, and acts of deception revealed, through facial micro-expressions and body language, making such concealment highly difficult [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>].</p>     <p>Furthermore, if it is believed that if there is no need to speak or move to convey emotions, the result could be a weakening of many social skills. Making &#x201C;white lies&#x201D; impossible could appear to remove a person&#x0027;s responsibility to be compassionate: for example a person might feel that &#x201C;the system is to blame if the communication of my emotional state causes offense; it is not my fault&#x201D;. Likewise coerced honesty might result in some lack of development of a person&#x0027;s morals; people who receive less opportunities to exercise honesty might experience more difficulty learning how to do so. This relates to human nature in the sense that removing challenges can make humans weaker. Just as leg strength declines when a person is confined to using a wheelchair, or children who are not exposed early on to viruses and bacteria can become adults with a weakened immune system, people whose emotions are always in &#x201C;plain sight&#x201D; might end up being unable to make morally correct choices without the crutch of &#x201C;forced honesty&#x201D;.</p>     <p>Removing uncertainty can also make life more monotonous. For example, Shakespeare&#x0027;s plays might have been less interesting if Hamlet&#x0027;s mental state, the relationship between Petruchio and Katherina, and the mutual attraction between Romeo and Juliet had been perfectly clear to all &#x2013; in this last case, the families might have separated the lovers immediately; a much less exciting story.</p>    </section>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Physical harm</h3>     </div>     </header>     <p>Emotion visualization could also lead to physical harm. The ability to quickly detect threats and opportunities via negative or positive emotions could increase the incidence of fighting and violence, as well as promiscuity in dating scenarios and thereby sexually transmitted diseases. For example, a person could feel threatened by seeing their partner and a potential rival feel highly positive emotions toward one another, which could lead to anger and violence[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>].</p>     <p>Some individuals and groups might also seek to control others&#x2019; emotions. This could be carried out mentally, e.g., in the case of an employer who might seek to restrict which emotions are displayed by service staff, but also physically, e.g., a bully might derive pleasure from seeing a victim&#x0027;s fear. In the latter case, however, we note that emotion visualization could also have a positive or neutral effect. Acknowledgement of emotional states in others by a bully could potentially interfere with attempts to dehumanize, increase feelings of guilt, and render more likely the probability that a victim receives empathy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. Or, visualization might not exert a strong effect at all &#x2013; if &#x201C;reading&#x201D; emotions becomes easy and ubiquitous, it could lead to desensitization (in the way that people today can become accustomed to seeing disasters on television) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>].</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Miscommunication</h3>     </div>     </header>     <p>A risk of misunderstandings, unseen biases, and falsification also exists. For example, current emotion visualization systems typically assume a person feels one emotion at a time, and do not take into account the full complexity of human emotions, like that emotions are usually directed toward some referent and can coexist [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]; e.g., a person can feel angry when hearing that a loved family member was hurt without feeling anger toward that family member, and cry happy tears when hearing they are safe.</p>     <p>Emotional signals can also be ambiguous; for example, smiling is not always a sign of joy, and nodding can have many meanings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]. Misunderstandings based on such ambiguity could lead to fighting or unhappiness (e.g. if a person appears to feel joy when another is in pain). Another potential problem is that, like humans, recognition algorithms can also have bias, which could negatively affect someone&#x0027;s life if emotions are used for some evaluation (like in the Black Mirror episode &#x201C;Nosedive&#x201D;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]), and fuel self-fulfilling prophecies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>]; as an example of the latter, a person expecting an interaction to be awkward based on a system&#x0027;s prediction might behave different from usual, which could put others off-balance, leading to awkwardness and making it seem like a system had been correct in its estimation.</p>     <p>Emotion recognition is also a highly challenging task; errors can occur, and the &#x201C;objectivity&#x201D; of the output is by no means guaranteed. Thus, if a person&#x0027;s own inference about someone else&#x0027;s emotions differs from what is visualized, dissonance and distrust might be felt. Finally, a system could be hacked to make it seem as if a person genuinely feels a different emotion, which could benefit politicians, lawyers, or criminals.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Disempowering individuals</h3>     </div>     </header>     <p>If it becomes difficult for some humans to conceal thoughts and emotions, robots and computers could also gain an advantage in persuading people, potentially also for commercial gain. Today interactions with computers are common, e.g. for social media or online shopping; this tendency could be further exacerbated if some people turn to robots as &#x201D;safe&#x201D; companions, to avoid sharing private emotions with other human beings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>]. Such systems could be highly convincing: robots without emotions would not need to be careful with choosing words, thus presenting a persuasive air of confidence, free of distractions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. Moreover, emotions can also be used to deceive people to affect their decisions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]. For example, organizations could use such systems to adapt prices presented, e.g. for online products, based on detecting a potential customer&#x0027;s emotional state.</p>     <p>Some potential dangers associated with such a scenario relate to society, trust, and equality. Robots could leverage emotions to deceive people into liking them in order to persuade, resulting in relationships which might not be genuine and potential reductions in a person&#x0027;s social contact with other humans [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>]. Partially or completely false information (e.g., &#x201D;fake news&#x201D;) could be transmitted without fear of having emotions betray the secret, undermining trust; moreover, scams targeting certain demographics such as the elderly could also check that a victim truly believes a story and use emotional feedback to be more convincing, potentially leading to increased suffering. And, presenting different prices to different people could be unfair and promote inequality. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191611/images/www18companion-350-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Potential strategies for avoiding pitfalls: (a) A &#x201D;naive&#x201D; model for emotion visualization. A person senses, affecting their emotions, which are in turn visualized by systems which they or others own, and influence others&#x2019; emotions. (b) Controlling what is visualized. The person&#x0027;s visualization system can be turned off completely so nothing is shown, set to play back a sequence of false emotions (e.g., appearing cheerful for clients), or filter the input from recognition to only show some emotions. Other people&#x0027;s visualization systems can be blocked by artificially altering signals associated with emotions such as facial expressions, voice, and body language. (c) Conveying rich information. In scenarios in which misunderstandings could be highly undesirable, information about which emotions are being felt toward what referents can be conveyed.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Strategies for avoiding pitfalls</h2>     </div>    </header>    <p>To minimize the risk that emotion visualization technology will be misused, we propose that some capabilities can be incorporated into systems, as shown in Fig. <a class="fig" href="#fig2">2</a>: controls (an off-button, spoofing, and cloaking) and intention recognition. For the former proposal, the core concept is that a person&#x0027;s emotions should only be visualized with their consent, to trusted persons: possibly, a feeling could only be visualized by a person who offers their own feelings to be visualized, as in a mutual exchange of emotions by friends. For privacy in other scenarios, a person&#x0027;s emotion visualization could be turned off, false emotions could be expressed, or some emotions not visualized. Emotions could be hidden from others&#x2019; systems; for example, by shrouding facial expressions, like adversarial patches which can trick object detection algorithms [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], or by causing clothing to automatically move to inject noise into gestures so that their emotional significance is unclear. Thus, for instance, a person with depression would not be forced to expose their condition to total strangers, employees could avoid being continually monitored and afraid of losing their jobs for some emotional slip-up, and undesired consequences from recognition mistakes in challenging conditions such as low illumination could be mitigated.</p>    <p>It is, of course, a challenge how to ensure those capabilities from a technological perspective: regulations that enforce them can be subverted. And, there is a risk that this will evolve into an &#x201C;arms race&#x201D; between better and better spoofing systems being continuously developed, as better and better recognition algorithms evolve. This could lead to a world where the richest and most technologically advanced are the only ones who can hide their true emotions. <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191611/images/www18companion-350-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Example of our robot attempting to convey some enriched emotional information, comprising anger (face on the left), fear (face on the right), sadness (via blue color), and generally negative valence (via some descending &#x201D;mood lines&#x201D;).</span>     </div>     </figure>    </p>    <p>Another approach, to reduce the risk of misunderstandings, might involve intention recognition, i.e., recognizing why and towards what target a person expresses emotions. In previous work, we have proposed an initial approach for intention inference using a &#x201D;monosemy&#x201D; metric based on the term frequency-inverse document frequency (TF-IDF) heuristic to quantify the degree to which an individual behavior is typically associated with underlying intentions; in conjunction with leveraging temporal structure of activities, co-occurring signals in other modalities, and multiple observations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. However, much work remains to be conducted on this topic, which has been described as &#x201D;virtually unexplored&#x201D; [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. Fig. <a class="fig" href="#fig3">3</a> shows an attempt to concurrently convey multiple emotions. Referents could also be shown together with emotions in some structured manner; for example, in the case of a person crying happy tears, a feeling of joy that a loved one is safe, and arousal due to some avoided danger (e.g., a fire, or car accident), could be visualized as the foreground and background of an image.</p>    <p>If such technological countermeasures are impossible or impractical, an alternative could be to enact legislation to allow emotion visualization for medical, therapeutic, or private purposes only, under sufficient regulation. For example, under privacy regulation such as the General Data Protection Regulation (GDPR) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>], one&#x0027;s emotional state could be regarded as personal information and therefore subject to protections and transparencies which are already available today. We believe that such considerations of what problems can occur and how they can be avoided, will enable emotion visualization to contribute positively to people&#x0027;s well-being.</p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Acknowledgements</h2>     </div>    </header>    <p>The authors received funding for this work from the Swedish Knowledge Foundation (CAISR 2010/0271 and Sidus AIR no. 20140220). We thank all those who kindly contributed comments and thoughts!</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">[n. d.]. Black Mirror episode explanation. ([n. d.]). <a class="link-inline force-break" href="http://delivery.acm.org/10.1145/3200000/3191611/en.wikipedia.org/wiki/Black_Mirror"      target="_blank">en.wikipedia.org/wiki/Black_Mirror</a></li>     <li id="BibPLXBIB0002" label="[2]">[n. d.]. EU General Data Protection Regulation (GDPR). ([n. d.]). <a class="link-inline force-break" href="https://www.eugdpr.org/" target="_blank">https://www.eugdpr.org/</a></li>     <li id="BibPLXBIB0003" label="[3]">[n. d.]. Fear-Based Anger Is the Primary Motive for Violence. ([n. d.]). <a class="link-inline force-break"      href="https://www.psychologytoday.com/blog/wicked-deeds/201707/fear-based-anger-is-the-primary-motive-violence"      target="_blank">https://www.psychologytoday.com/blog/wicked-deeds/201707/fear-based-anger-is-the-primary-motive-violence</a></li>     <li id="BibPLXBIB0004" label="[4]">Robert Agnew. 2006. General strain theory: Current status and directions for further research. <em>      <em>Taking stock: The status of criminological theory</em>     </em>15 (2006), 101&#x2013;123.</li>     <li id="BibPLXBIB0005" label="[5]">T.&#x00A0;B. Brown, D. Man&#x00E9;, A. Roy, M. Abadi, and J. Gilmer. 2017. Adversarial Patch. <em>      <em>ArXiv e-prints</em>     </em> (Dec. 2017). arxiv:cs.CV/1712.09665</li>     <li id="BibPLXBIB0006" label="[6]">Joseph Bullington. 2005. &#x2019;Affective&#x2019; Computing and Emotion Recognition Systems: The Future of Biometric Surveillance?. In <em>      <em>Proceedings of the 2Nd Annual Conference on Information Security Curriculum Development</em>     </em>(<em>InfoSecCD &#x2019;05</em>). ACM, New York, NY, USA, 95&#x2013;99. <a class="link-inline force-break" href="https://doi.org/10.1145/1107622.1107644"      target="_blank">https://doi.org/10.1145/1107622.1107644</a></li>     <li id="BibPLXBIB0007" label="[7]">Martin Cooney, Shuichi Nishio, and Hiroshi Ishiguro. 2015. Affectionate interaction with a small humanoid robot capable of recognizing social touch behavior. <em>      <em>ACM Transactions on Interactive Intelligent Systems (TiiS)</em>     </em>4, 4(2015), 19.</li>     <li id="BibPLXBIB0008" label="[8]">Martin&#x00A0;D Cooney, Shuichi Nishio, and Hiroshi Ishiguro. 2015. Importance of touch for conveying affection in a multimodal interaction with a small humanoid robot. <em>      <em>International Journal of Humanoid Robotics</em>     </em>12, 01 (2015), 1550002.</li>     <li id="BibPLXBIB0009" label="[9]">Roddy Cowie. 2015. Ethical Issues in Affective Computing. (2015). <a class="link-inline force-break"      href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199942237.001.0001/oxfordhb-9780199942237-e-006"      target="_blank">http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199942237.001.0001/oxfordhb-9780199942237-e-006</a></li>     <li id="BibPLXBIB0010" label="[10]">Paul Ekman and Wallace&#x00A0;V Friesen. 1969. Nonverbal leakage and clues to deception. <em>      <em>Psychiatry</em>     </em>32, 1 (1969), 88&#x2013;106.</li>     <li id="BibPLXBIB0011" label="[11]">Thomas Grote and Oliver Korn. 2017. Risks and Potentials of Affective Computing. An Interdisciplinary View on the ACM Code of Ethics. In <em>      <em>CHI 2017 workshop on Ethical Encounters in HCI</em>     </em>.</li>     <li id="BibPLXBIB0012" label="[12]">Nick Haslam. 2006. Dehumanization: An integrative review. <em>      <em>Personality and social psychology review</em>     </em>10, 3 (2006), 252&#x2013;264.</li>     <li id="BibPLXBIB0013" label="[13]">Jeff&#x00A0;T Larsen, A&#x00A0;Peter McGraw, and John&#x00A0;T Cacioppo. 2001. Can people feel happy and sad at the same time?<em>      <em>Journal of personality and social psychology</em>     </em>81, 4(2001), 684.</li>     <li id="BibPLXBIB0014" label="[14]">Hong-Wei Ng, Viet&#x00A0;Dung Nguyen, Vassilios Vonikakis, and Stefan Winkler. 2015. Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning. In <em>      <em>Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</em>     </em>(<em>ICMI &#x2019;15</em>). ACM, New York, NY, USA, 443&#x2013;449. <a class="link-inline force-break" href="https://doi.org/10.1145/2818346.2830593"      target="_blank">https://doi.org/10.1145/2818346.2830593</a></li>     <li id="BibPLXBIB0015" label="[15]">Kohei Ogawa, Koichi Taura, and Hiroshi Ishiguro. 2012. Possibilities of androids as poetry-reciting agent. In <em>      <em>RO-MAN, 2012 IEEE</em>     </em>. IEEE, 565&#x2013;570.</li>     <li id="BibPLXBIB0016" label="[16]">Masaru Ohkubo, Miki Yamamura, Hiroko Uchiyama, and Takuya Nojima. 2014. Breathing clothes: artworks using the hairlytop interface. In <em>      <em>Proceedings of the 11th Conference on Advances in Computer Entertainment Technology</em>     </em>. ACM, 39.</li>     <li id="BibPLXBIB0017" label="[17]">Catherine O&#x0027;Neill. 2016. Weapons of Math Destruction. <em>      <em>How Big Data Increases Inequality and Threatens Democracy</em>     </em> (2016).</li>     <li id="BibPLXBIB0018" label="[18]">Rebeccah Pailes-Friedman. 2015. BioWear: a kinetic accessory that communicates emotions through wearable technology. In <em>      <em>Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers</em>     </em>. ACM, 627&#x2013;633.</li>     <li id="BibPLXBIB0019" label="[19]">Rosalind&#x00A0;W Picard. 1995. Affective Computing-MIT Media Laboratory Perceptual Computing Section Technical Report No. 321. <em>      <em>Cambridge, MA</em>     </em>2139(1995).</li>     <li id="BibPLXBIB0020" label="[20]">Rosalind&#x00A0;W. Picard and Jennifer Healey. 1997. Affective wearables. <em>      <em>Personal Technologies</em>     </em>1, 4 (01 Dec 1997), 231&#x2013;240. <a class="link-inline force-break" href="https://doi.org/10.1007/BF01682026"      target="_blank">https://doi.org/10.1007/BF01682026</a></li>     <li id="BibPLXBIB0021" label="[21]">Isabella Poggi, Francesca D&#x0027;Errico, and Laura Vincze. 2010. Types of Nods. The Polysemy of a Social Signal.. In <em>      <em>LREC</em>     </em>.</li>     <li id="BibPLXBIB0022" label="[22]">Anas Samara, Maria Luiza&#x00A0;Recena Menezes, and Leo Galway. 2016. Feature Extraction for Emotion Recognition and Modelling Using Neurophysiological Data. In <em>      <em>Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS), International Conference on</em>     </em>. IEEE, 138&#x2013;144.</li>     <li id="BibPLXBIB0023" label="[23]">Erica Scharrer. 2008. Media exposure and sensitivity to violence in news reports: Evidence of desensitization?<em>      <em>Journalism &#x0026; Mass Communication Quarterly</em>     </em>85, 2 (2008), 291&#x2013;310.</li>     <li id="BibPLXBIB0024" label="[24]">Amanda Sharkey and Natalie Wood. 2014. The Paro seal robot: demeaning or enabling. In <em>      <em>Proceedings of AISB</em>     </em>, Vol.&#x00A0;36.</li>     <li id="BibPLXBIB0025" label="[25]">Alessandro Vinciarelli, Hugues Salamin, and Maja Pantic. 2009. Social signal processing: Understanding social interactions through nonverbal behavior analysis. In <em>      <em>Computer Vision and Pattern Recognition Workshops, 2009. CVPR Workshops 2009. IEEE Computer Society Conference on</em>     </em>. IEEE, 42&#x2013;49.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191611">https://doi.org/10.1145/3184558.3191611</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
