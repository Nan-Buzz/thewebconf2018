<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Anomaly Detection with Partially Observed Anomalies</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Anomaly Detection with Partially Observed Anomalies</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Ya-Lin</span>      <span class="surName">Zhang</span>     National Key Lab for Novel Software Technology, Nanjing University, China, <a href="mailto:zhangyl@lamda.nju.edu.cn">zhangyl@lamda.nju.edu.cn</a>     </div>     <div class="author">     <span class="givenName">Longfei</span>      <span class="surName">Li</span>     Ant Financial Services Group, China, <a href="mailto:longyao.llf@antfin.com">longyao.llf@antfin.com</a>     </div>     <div class="author">     <span class="givenName">Jun</span>      <span class="surName">Zhou</span>     Ant Financial Services Group, China, <a href="mailto:jun.zhoujun@antfin.com">jun.zhoujun@antfin.com</a>     </div>     <div class="author">     <span class="givenName">Xiaolong</span>      <span class="surName">Li</span>     Ant Financial Services Group, China, <a href="mailto:xl.li@antfin.com">xl.li@antfin.com</a>     </div>     <div class="author">     <span class="givenName">Zhi-Hua</span>      <span class="surName">Zhou</span>     National Key Lab for Novel Software Technology, Nanjing University, China, <a href="mailto:zhouzh@lamda.nju.edu.cn">zhouzh@lamda.nju.edu.cn</a>     </div>         <Affiliation id="aff2">Ant Financial Services Group, China</Affiliation>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186580" target="_blank">https://doi.org/10.1145/3184558.3186580</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>In this paper, we consider the problem of anomaly detection. Previous studies mostly deal with this task in either supervised or unsupervised manner according to whether label information is available. However, there always exists settings which are different from the two standard manners. In this paper, we address the scenario when anomalies are partially observed, i.e., we are given a large amount of unlabeled instances as well as a handful labeled anomalies. We refer to this problem as anomaly detection with partially observed anomalies, and proposed a two-stage method <strong>ADOA</strong> to solve it. Firstly, by addressing the difference between the anomalies, the observed anomalies are clustered, while the unlabeled instances are filtered to get potential anomalies and reliable normal instances. Then, with the above instances, a weight is attached to each instance according to the confidence of its label, and a weighted multi-class model is built, which will be further used to distinguish different anomalies to the normal instances. Experimental results show that in the aforementioned setting, existing methods behave unsatisfactorily and the proposed method performs significantly better than all these methods, which validates the effectiveness of the proposed approach.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Anomaly Detection</small>, </span>     <span class="keyword">      <small> Observed Anomalies</small>, </span>     <span class="keyword">      <small> Two-Stage Method</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Ya-Lin Zhang, Longfei Li, Jun Zhou, Xiaolong Li, and Zhi-Hua Zhou. 2018. Anomaly Detection with Partially Observed Anomalies. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 9 Pages. <a href="https://doi.org/10.1145/3184558.3186580" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186580</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Anomaly detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] is a broadly used technique which aims at identifying the unexpected patterns from the usual behavior in a dataset. These unexpected patterns are always called anomalies or outliers, which are always generated by some kind of malicious purpose or illegal activity. Anomaly detection is important and can provide significant and critical help in various applications, such as intrusion detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], fraud detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], fault detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], suspicious transaction detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] and abnormal moving activity detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>], etc.</p>    <p>To deal with this task, machine learning based techniques have been widely employed during the past few years, and these techniques can be roughly classified into two categories: unsupervised learning based approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] and supervised learning based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. Traditionally, unsupervised learning based methods are developed, in which only unlabeled data are accessible. Distance based approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], density based approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] and isolation based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] are typical representatives along this way.</p>    <p>On the other hand, if sufficient labeled data are available, supervised learning based methods are explored, in which a classification model, such as support vector machine&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], decision tree&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] and k-nearest neighbor&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>], etc., can be trained to further classify unseen samples. Note that compared to unsupervised approaches, supervised methods can always provide better performance with the help of sufficient labeled data. In addition, by using both labeled and unlabeled data, semi-supervised learning based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] are explored, and by combining different techniques, hybrid approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] have also been developed to handle this problem.</p>    <p>However, there are some conditions in which adequate labeled samples are difficult to obtain, while we can access a small number of recognized anomalies, along with sufficient unlabeled samples. Let&#x0027;s take the task of malicious URL detection as an example, in some scenarios, apart from a large amount of unlabeled URL records, we can only obtain a handful of labeled malicious URLs, with the help of existing rule based systems. Different to supervised setting in which both positive and negative samples are provided, we only get a small set of positive (malicious) samples here, thus the supervised methods can not be directly employed. On the other hand, when compared to unsupervised learning setting, we additionally have some labeled samples, which may offer great help with proper utilization. In this paper, we refer to the this special anomaly detection setting as anomaly detection with partially observed anomalies).</p>    <p>There is one paradigm named PU (Positive and Unlabeled) learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], which has seemingly similar setting with the aforementioned one. However, in PU learning, the positive samples always belong to one concept center, which means that the positive samples are similar to each other, whereas in anomaly detection, the so-called positive samples (anomalies) are usually not similar to each other, and they can be seriously disparate. In another word, we can not claim that the difference between two outliers are smaller than that of an anomaly and a non-anomaly. Thus, direct applying of PU learning based techniques for anomaly detection task may not lead to satisfactory performance.</p>    <p>Another paradigm called semi-supervised clustering&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] deals with the cluster setting where the data are partially labeled or with other types of preliminary information, and the objective is to cluster the unlabeled samples to the appropriate clusters. It seems that semi-supervised learning deals with the similar task as we described. However, just as PU learning, the samples labeled in the same cluster should be similar to each other in semi-supervised clustering, while in anomaly detection, the observed anomalies do not conform to this.</p>    <p>In this paper, we consider the setting of anomaly detection with partially observed anomalies, and propose a method called <strong>ADOA</strong> (<strong>A</strong>nomaly <strong>D</strong>etection with partial <strong>O</strong>bserved <strong>A</strong>nomalies) to solve it. <strong>ADOA</strong> follows a two-stage manner. In the first stage, we address that the observed anomalies should not be simply regarded into one concept center, and by assuming that the anomalies belong to <em>k</em> different concept centers, the anomalies are firstly clustered into <em>k</em> clusters. After that, both potential anomalies and reliable normal samples are selected from the unlabeled samples according to the isolation degree and the similarity to the nearest anomaly cluster center. In stage two, a weight is set to each sample according to the confidence of its attached label, and a weighted multi-class classification model is built to distinguish different anomalies from the normal samples, using original anomalies and the selected samples. Experiments on different datasets and a real application task demonstrate the effectiveness of our approach.</p>    <p>The rest of this paper is organized as follows. In section 2, we review the related work.In section 3, we state the problem setting and present the proposed method. In section 4, we report the experimental results on different datasets. In section 5, we apply the proposed method to the problem of malicious URL detection and validate the effectiveness of the proposed method. Finally, we conclude the paper in section 6.</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>Anomaly detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] deals with the task of recognizing unexpected patterns from normal behavior. The detection of anomalies has significant influence and can provide critical help in many different fields. During its development, many machine learning based methods have been proposed to handle this problem&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>], and it has been widely applied in many applications, such as intrusion detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], fraud detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], fault detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], suspicious transaction detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] and abnormal moving activity detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>], etc.</p>    <p>Among the developed methods, unsupervised learning based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] build the model with unlabeled data. To name some representative, distance based approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>], density based approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>], isolation based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], and so on. These methods can be widely used since there is no need for labeling of the data. However, in many application fields, the unsupervised methods may not succeed to achieve the require performance.</p>    <p>On the other hand, with labeled data provided, supervised learning based methods are explored. Many supervised algorithms, such as support vector machine&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], decision tree&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] and k-nearest neighbor&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] are successively adopted to the task of anomaly detection. With proper use of the label information, supervised learning based methods can always achieve better performance. Beyond these two standard paradigms, other methods, including semi-supervised learning based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] and hybrid approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] have also been explored based on these techniques to handle this task.</p>    <p>In some conditions, only the samples following the normal behavior are provided&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], while the anomalies are unseen. Methods like one-class learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] and support vector data description&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] are developed for this setting. These methods focused on learning the hypersphere to describe the normal samples or learning a hyperplane to divide the data points from the origin with maximum-margin.</p>    <p>PU (Positive and Unlabeled) learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] is a special case of semi-supervised learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], which copes with the setting when only positive and unlabeled data are available, while no negative sample is labeled. During the past few years, a mass of methods have been proposed to deal with this task. Roughly speaking, these methods can be divided into three families. Two-step approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] try to recognize some reliable negative samples from the unlabeled data, then a traditional supervised learning or semi-supervised learning technique can be applied. Cost-sensitive learning techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] for binary classification with unequal misclassification cost are also readily available for handling this problem&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. What&#x0027;s more, convex methods have also been proposed to deal with this task&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. Note that if we regard anomalies as positive samples here, PU learning is somewhat similar to anomaly detection with partially observed anomalies. However, the most striking difference is that, the positive samples in PU learning are similar to each other, thus we can find one positive concept for them, while in anomaly detection, the anomalies are always diversified, and they can rarely cluster into one concept cluster, making the standard PU learning technique not suitable to handle anomaly detection task.</p>    <p>Semi-supervised clustering&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] deals with the problem when the provided data are partially labeled or with other types of preliminary information, and the goal is to try to assign the unlabeled samples to the proper clusters. Many methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] for this task are generalized from the traditional clustering algorithms, with modification to make sure that the constraints are satisfied. However, just as PU learning, the samples labeled in the same cluster should be similar to each other in semi-supervised clustering, while in anomaly detection, the observed anomalies do not conform to this. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186580/images/www18companion-134-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">The overall framework of the proposed method.</span>     </div>     </figure>    </p>    <p>In this paper, we focus on a special setting of anomaly detection, i.e., anomaly detection with partially observed anomalies. Different to totally unsupervised anomaly detection scenario, we have some preliminary information, i.e., the observed anomalies. Different to supervised setting, we only have a small amount of anomalies, while the other samples are totally unlabeled. Different to PU learning and semi-supervised clustering, the labeled anomalies are usually not similar to each other.</p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Anomaly Detection with Partially Observed Anomalies</h2>     </div>    </header>    <p>In this section, we will first state our problem setting, and then present the proposed method <strong>ADOA</strong> (<strong>A</strong>nomaly <strong>D</strong>etection with partial <strong>O</strong>bserved <strong>A</strong>nomalies) .</p>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Problem Statement and Notations</h3>     </div>     </header>     <p>Let <span class="inline-equation"><span class="tex">$\mathcal {X}=\mathbb {R}^d$</span>     </span> denotes the instance space and <span class="inline-equation"><span class="tex">$\mathcal {Y}=\lbrace -1, +1\rbrace$</span>     </span> denotes label space, respectively. Let <em>y</em> = +1 indicates the anomalies and <em>y</em> = &#x2212;1 for normal samples. We are given a data set with <em>m</em> training samples <span class="inline-equation"><span class="tex">$\mathcal {D}=\lbrace ({x}_1,y_1),\ldots ,({x}_l,y_l), {x}_{l+1},\ldots , {x}_m\rbrace$</span>     </span>, where <span class="inline-equation"><span class="tex">$ {x}_{i}\in \mathcal {X}$</span>     </span> representing a sample. The first <em>l</em> samples are labeled as anomalies, which are denoted by <span class="inline-equation"><span class="tex">$\mathcal {D}^l=\lbrace ({x}_1,y_1),\ldots ,({x}_l,y_l)\rbrace$</span>     </span>, and the other <em>m</em> &#x2212; <em>l</em> samples are unlabeled, which are denoted by <span class="inline-equation"><span class="tex">$\mathcal {D}^u=\lbrace {x}_{l+1},\ldots , {x}_m\rbrace$</span>     </span>. Note that although the first <em>l</em> samples are all with the label <em>y</em> = +1, the can be totally different to each other. The goal is to build a model <span class="inline-equation"><span class="tex">$f:\mathcal {X}\rightarrow \mathcal {Y}$</span>     </span>, so that it can be further used to distinguish the diversified anomalies from the normal ones for future-coming data, thus the anomalies can be recognized.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Proposed Method</h3>     </div>     </header>     <p>     <strong>ADOA</strong> follows the two-stage manner. In the first stage, both observed anomalies and unlabeled samples are manipulated. We address that the observed anomalies are different to each other, and they should not be simply classified into one concept center. Since the anomalies are really diversified, we first try to separate them into different clusters, so that the samples in each cluster are similar to each other. For unlabeled data, we aim at sufficiently exploring the information of them. Thus, we try to filter both potential anomalies and reliable normal samples from them, with consideration of the isolation score (to be explained shortly) and their similarity score to the observed anomalies. The intuition is that, on one hand, the potential anomalies should be different to normal samples (i.e., can be easily isolated); on the other hand, they should be similar to some observed anomalies. In the second stage, we build a weighted multi-class model to distinguish different anomalies from the normal samples. For the observed anomalies, the weights are set to 1, and for the filtered samples, the weights are set according to the confidence of their attached labels. The overall procedure is shown in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, and the details of <strong>ADOA</strong> are presented below.</p>     <p>     <strong>In stage one</strong>, by addressing the difference of the anomalies, we first cluster observed anomalies in <span class="inline-equation"><span class="tex">$\mathcal {D}^l$</span>     </span> into <em>k</em> clusters <span class="inline-equation"><span class="tex">$\mathcal {C}=\lbrace C_1,C_2,\ldots ,C_k\rbrace$</span>     </span>. We can employ different cluster algorithms. Here we simply run <em>k</em>-means algorithm (with normalization performs in advance). Specifically, the distance between each two samples is measured using squared Euclidean distance, which is as below <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} dist({x}_i, {x}_{i^{\prime }}) = \sum _{j=1}^{d} ({x}_{ij}- {x}_{i^{\prime }j})^2 \,, \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> in which <em>d</em> is the dimension of the samples. The following square error is minimized to learn the centers and the assignments of the samples, <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} E = \sum _{i=1}^{k}\sum _{ {x} \in C_i} dist({x}- {\mu }_i) \,, \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> in which <span class="inline-equation"><span class="tex">$ {\mu }_i=\frac{1}{|C_i|}\sum _{ {x} \in C_i} {x}$</span>     </span> is the center of the <em>i</em>-th anomaly cluster, i.e., the anomaly concept center. Note that other cluster algorithms, such as hierarchical clustering and density-based clustering, can also be explored, and this may lead to further improvement of the performance.</p>     <p>For samples in unlabeled set <span class="inline-equation"><span class="tex">$\mathcal {D}^u$</span>     </span>, we select the potential anomalies and reliable normal samples based on the isolation score and the similarity score to the nearest cluster center.</p>     <p>     <strong>Isolation Score</strong>: The concept of isolation was first proposed in &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. They showed that an extremely random tree forest can be used for isolating samples. Each tree in the forest is built by randomly choosing an attribute and a corresponding split value for subsequent growing at each node. Since the anomalies are few and different, they are always isolated closer to the root of the tree, whereas normal samples will go to the deeper leaf of the tree. To get the isolation score, each sample is delivered to a tree until it arrives at a leaf, the path length in each tree is then obtained, and the average path length can be calculated for the isolation forest. Based on the average path lengths on the trees, the isolation score <em>IS</em>(x) can be calculated to describe the probability of a sample x being anomaly. Let <em>h</em>(x) denotes the path length of a sample x on a tree, and <em>E</em>(<em>h</em>(x)) indicates the average path length of a collection of isolation trees. Assume that there are <em>n</em> samples, and let <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} c(n)=2H(n)-(2(n-1)/n) \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> denotes the average path length of unsuccessful search in Binary Search Tree, which is the same as the estimation of average <em>h</em>(x) for external node terminations. Here <em>H</em>(<em>n</em>) is the harmonic number, which can be estimated by <em>ln</em>(<em>n</em>) + 0.5772156649 (Euler&#x0027;s constant). <em>c</em>(<em>n</em>) is used as the normalization parameter to calculate the isolation score <em>IS</em>(x), which is as following: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} IS({x})=2^{-\frac{E(h({x}))}{c(n)}} \,. \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>     </p>     <p>The higher is the score <em>IS</em>(x) (close to 1), the more likely that x being an anomaly.</p>     <p>     <strong>Similarity Score</strong>: On the other hand, it is reasonable that the closer is a sample to a known anomaly concept center, the more likely that the sample being a potential anomaly, thus we calculate the similarity score <em>SS</em>(x) between a sample x and its nearest anomaly concept center. Specifically, <em>SS</em>(x) is calculate as following: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} SS({x})= \max _{i=1}^k{e^{-({x}- {\mu }_i)^2}} \,, \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> in which &#x03BC;<sub>      <em>i</em>     </sub> denotes the <em>i</em>-th concept center, and <em>k</em> is the number of anomaly concept centers.</p>     <p>     <strong>Total Score</strong>:To filter the potential anomalies and reliable normal samples from the unlabeled samples, we take both the isolation score and the similarity score into consideration, and the total score for an instance is denoted as <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} TS({x})=\theta IS({x})+ (1-\theta) SS({x}) \,, \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> in which <em>&#x03B8;</em> &#x2208; [0, 1] is a parameter to balance the importance of isolation score and similarity score.</p>     <p>Let <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \alpha =\frac{1}{l}\sum _{i=1}^{l}TS({x}_i) \, \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> indicates the average score of observed anomalies. We then select the instances with <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} TS({x}) \ge \alpha \, \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> as potential anomalies, and put them into their nearest anomaly clusters. We select the instances with <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} TS({x}) \le \beta \, \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> as reliable normal samples, where <em>&#x03B2;</em> is a predefined parameter. The smaller is <em>&#x03B2;</em>, the more reliable are the selected samples.</p>     <p>So far, we have not only the observed anomalies, but also the selected potential anomalies and the reliable normal samples, and the anomalies are separated into <em>k</em> different clusters, so that in each cluster, the anomalies are with high similarity to each other within the cluster.</p>     <p>     <strong>In stage two</strong>,we first set weights for all selected samples and the observed anomalies. Specifically, all observed anomalies are with weight 1, and for the selected anomalies, as shown in Eq.&#x00A0;<a class="eqn" href="#eq8">10</a>, the higher is the score <em>TS</em>(x) , the more weight will the instance get. <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w({x})= \frac{TS({x})}{\max _{x}{TS({x})}} \,. \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> For selected reliable normal samples, the smaller is the score, the more weight it will get. The details are as below: <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w({x})= \frac{\max _{x}{TS({x})} - TS({x})}{\max _{x}{TS({x})}-\min _{x}{TS({x})}} \,. \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div>     </p>     <p>With the above mentioned samples and their weights, a weighted (<em>k</em> + 1)-class model can be trained to separate different anomalies to the normal samples. Particularly, each anomaly cluster is regarded as one class, so there are <em>k</em> + 1 classes in all (<em>k</em> anomaly classes and one normal class). The following objective is minimized, <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \sum _{i} w_il(y_i,f({x}_i)) + \lambda R({w}) \,, \end{equation} </span>       <br/>       <span class="equation-number">(12)</span>      </div>     </div> in which <em>w<sub>i</sub>     </em> denotes the weight of the instance x<sub>      <em>i</em>     </sub>, <em>l</em>(<em>y<sub>i</sub>     </em>, <em>f</em>(x<sub>      <em>i</em>     </sub>)) is the loss term, and <em>R</em>(w) the the regularization term. In this work, support vector machine is used, so the loss term and regularization term are set to be hinge-loss and <em>L</em>2-norm.</p>     <p>After obtaining the multi-class model, the new-coming samples can be classified. When applying this model to an unseen instance, no matter which of the <em>k</em> anomaly clusters is the new-coming instance classified to, it will be regarded as an anomaly.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Details of the datasets information and experiments setups. &#x2018;Dimension&#x2019; denotes the dimension of the datasets. &#x2018;Observed&#x2019;, &#x2018;Unlabeled&#x2019; and &#x2018;Test&#x2019; denotes the number of observed anomalies, training unlabeled samples and test samples, respectively. &#x2018;Class prior&#x2019; denotes the proportion of anomalies in unlabeled and test data.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Dimension</td>        <td style="text-align:center;">Observed</td>        <td style="text-align:center;">Unlabeled</td>        <td style="text-align:center;">Test</td>        <td style="text-align:center;">Class prior</td>       </tr>       <tr>        <td style="text-align:center;">synthetic</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">20</td>        <td style="text-align:center;">10000</td>        <td style="text-align:center;">10000</td>        <td style="text-align:center;">0.01</td>       </tr>       <tr>        <td style="text-align:center;">arrhythmia</td>        <td style="text-align:center;">274</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">200</td>        <td style="text-align:center;">200</td>        <td style="text-align:center;">0.1</td>       </tr>       <tr>        <td style="text-align:center;">vowel</td>        <td style="text-align:center;">12</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">0.02</td>       </tr>       <tr>        <td style="text-align:center;">letter</td>        <td style="text-align:center;">32</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">0.08</td>       </tr>       <tr>        <td style="text-align:center;">musk</td>        <td style="text-align:center;">166</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">0.04</td>       </tr>       <tr>        <td style="text-align:center;">thyroid</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">0.03</td>       </tr>       <tr>        <td style="text-align:center;">speech</td>        <td style="text-align:center;">400</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">1500</td>        <td style="text-align:center;">1500</td>        <td style="text-align:center;">0.013</td>       </tr>       <tr>        <td style="text-align:center;">satimage</td>        <td style="text-align:center;">36</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">2500</td>        <td style="text-align:center;">2500</td>        <td style="text-align:center;">0.01</td>       </tr>       <tr>        <td style="text-align:center;">smtp</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">40000</td>        <td style="text-align:center;">40000</td>        <td style="text-align:center;">0.00025</td>       </tr>       <tr>        <td style="text-align:center;">ionosphere</td>        <td style="text-align:center;">33</td>        <td style="text-align:center;">20</td>        <td style="text-align:center;">150</td>        <td style="text-align:center;">150</td>        <td style="text-align:center;">0.33</td>       </tr>       <tr>        <td style="text-align:center;">breastw</td>        <td style="text-align:center;">9</td>        <td style="text-align:center;">20</td>        <td style="text-align:center;">300</td>        <td style="text-align:center;">300</td>        <td style="text-align:center;">0.33</td>       </tr>       <tr>        <td style="text-align:center;">optdigits</td>        <td style="text-align:center;">64</td>        <td style="text-align:center;">20</td>        <td style="text-align:center;">2000</td>        <td style="text-align:center;">2000</td>        <td style="text-align:center;">0.025</td>       </tr>       <tr>        <td style="text-align:center;">pendigits</td>        <td style="text-align:center;">16</td>        <td style="text-align:center;">20</td>        <td style="text-align:center;">3000</td>        <td style="text-align:center;">3000</td>        <td style="text-align:center;">0.017</td>       </tr>       <tr>        <td style="text-align:center;">pima</td>        <td style="text-align:center;">8</td>        <td style="text-align:center;">50</td>        <td style="text-align:center;">300</td>        <td style="text-align:center;">300</td>        <td style="text-align:center;">0.33</td>       </tr>       <tr>        <td style="text-align:center;">cardio</td>        <td style="text-align:center;">21</td>        <td style="text-align:center;">50</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">0.1</td>       </tr>       <tr>        <td style="text-align:center;">mammography</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">50</td>        <td style="text-align:center;">5000</td>        <td style="text-align:center;">5000</td>        <td style="text-align:center;">0.02</td>       </tr>       <tr>        <td style="text-align:center;">satellite</td>        <td style="text-align:center;">36</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">2000</td>        <td style="text-align:center;">2000</td>        <td style="text-align:center;">0.25</td>       </tr>       <tr>        <td style="text-align:center;">annthyroid</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">2000</td>        <td style="text-align:center;">2000</td>        <td style="text-align:center;">0.1</td>       </tr>       <tr>        <td style="text-align:center;">shuttle</td>        <td style="text-align:center;">9</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">10000</td>        <td style="text-align:center;">10000</td>        <td style="text-align:center;">0.1</td>       </tr>       <tr>        <td style="text-align:center;">ForestCover</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">100000</td>        <td style="text-align:center;">100000</td>        <td style="text-align:center;">0.01</td>       </tr>       <tr>        <td style="text-align:center;">http</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">200000</td>        <td style="text-align:center;">200000</td>        <td style="text-align:center;">0.005</td>       </tr>      </tbody>     </table>     </div>    </section>   </section>   <section id="sec-12">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>In this section, we run experiments on both synthetic data and real-world data to validate the performance of the proposed method. We compare the proposed method with different baselines including unsupervised approach, supervised approach and PU learning approach. Firstly, unsupervised method Isolation Forest&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] is considered, since it has been proved as a powerful method for anomaly detection. Secondly, supervised method support vector machine&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] is considered. By simply regarding all unlabeled samples as negative ones, supervised method is tested. Thirdly, PU learning based method, i.e., the cost sensitive strategy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], is compared too, since the setting is somewhat similar to the setting of PU learning, and we want to validate whether PU learning is suitable for the problem of anomaly detection.</p>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Experiments on Synthetic Data</h3>     </div>     </header>     <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186580/images/www18companion-134-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">The sampled examples of synthetic dataset.</span>     </div>     </figure>     <p>We first perform experiments on synthetic dataset, to test the performance of each method in the scenario that the anomalies are diversified. To generate the dataset for anomaly detection, we first generate the normal examples, which are sampled from the multivariate Gaussian distribution <span class="inline-equation"><span class="tex">$P_0=\mathcal {N}({\mu }_0, {\Sigma }_0)$</span>     </span>, in which &#x03BC;<sub>0</sub> = [5, 5] is the mean vector, and &#x03A3;<sub>0</sub> = [[5, 0], [0, 5]] is the covariance matrix. For anomalies, we assume that there are three different concept clusters, and they are sampled from multivariate Gaussian distribution <span class="inline-equation"><span class="tex">$P_1=\mathcal {N}({\mu }_1, {\Sigma })$</span>     </span>, <span class="inline-equation"><span class="tex">$P_2=\mathcal {N}({\mu }_2, {\Sigma })$</span>     </span> and <span class="inline-equation"><span class="tex">$P_3=\mathcal {N}({\mu }_3, {\Sigma })$</span>     </span>, in which &#x03BC;<sub>1</sub> = [1, 1], &#x03BC;<sub>2</sub> = [1, 10], &#x03BC;<sub>3</sub> = [9, 0] and &#x03A3; = [[0.6, 0], [0, 0.5]]. The sampled examples are shown in Fig.&#x00A0;<a class="fig" href="#fig2">2</a>. As we can see, the anomalies from different clusters are really different from each other, and the anomalies from the same cluster are pretty similar to each other.</p>     <p>To generate the setting of anomaly detection with partially observed anomalies, we randomly sample 20 examples (which may come from any of the three different clusters) from the anomalies as observed anomalies. We then sample examples to construct unlabeled training set and test set, and the number of unlabeled and test examples are both set to 10000, among which only 1% of them are anomalies. The details are shown in the first line of Table&#x00A0;<a class="tbl" href="#tab1">1</a>, with the name &#x2018;synthetic&#x2019;. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186580/images/www18companion-134-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">The accuracy with different k value.</span>      </div>     </figure>     </p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">AUC score on different datasets. The highest AUC score is marked in bold.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Proposed Method</td>        <td style="text-align:center;">Unsupervised Method</td>        <td style="text-align:center;">Supervised Method</td>        <td style="text-align:center;">PU Method</td>       </tr>       <tr>        <td style="text-align:center;">synthetic</td>        <td style="text-align:center;">        <strong>0.989</strong>        </td>        <td style="text-align:center;">0.954</td>        <td style="text-align:center;">0.944</td>        <td style="text-align:center;">0.978</td>       </tr>       <tr>        <td style="text-align:center;">arrhythmia</td>        <td style="text-align:center;">        <strong>0.840</strong>        </td>        <td style="text-align:center;">0.515</td>        <td style="text-align:center;">0.665</td>        <td style="text-align:center;">0.564</td>       </tr>       <tr>        <td style="text-align:center;">vowels</td>        <td style="text-align:center;">        <strong>0.984</strong>        </td>        <td style="text-align:center;">0.774</td>        <td style="text-align:center;">0.972</td>        <td style="text-align:center;">0.979</td>       </tr>       <tr>        <td style="text-align:center;">letter</td>        <td style="text-align:center;">        <strong>0.671</strong>        </td>        <td style="text-align:center;">0.625</td>        <td style="text-align:center;">0.633</td>        <td style="text-align:center;">0.535</td>       </tr>       <tr>        <td style="text-align:center;">musk</td>        <td style="text-align:center;">        <strong>1.000</strong>        </td>        <td style="text-align:center;">0.995</td>        <td style="text-align:center;">0.880</td>        <td style="text-align:center;">        <strong>1.000</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">thyroid</td>        <td style="text-align:center;">        <strong>0.994</strong>        </td>        <td style="text-align:center;">0.971</td>        <td style="text-align:center;">0.718</td>        <td style="text-align:center;">0.989</td>       </tr>       <tr>        <td style="text-align:center;">speech</td>        <td style="text-align:center;">        <strong>0.730</strong>        </td>        <td style="text-align:center;">0.524</td>        <td style="text-align:center;">0.613</td>        <td style="text-align:center;">0.690</td>       </tr>       <tr>        <td style="text-align:center;">satimage</td>        <td style="text-align:center;">        <strong>0.992</strong>        </td>        <td style="text-align:center;">0.990</td>        <td style="text-align:center;">0.947</td>        <td style="text-align:center;">0.975</td>       </tr>       <tr>        <td style="text-align:center;">smtp</td>        <td style="text-align:center;">        <strong>0.902</strong>        </td>        <td style="text-align:center;">0.833</td>        <td style="text-align:center;">0.788</td>        <td style="text-align:center;">0.876</td>       </tr>       <tr>        <td style="text-align:center;">ionosphere</td>        <td style="text-align:center;">        <strong>0.934</strong>        </td>        <td style="text-align:center;">0.846</td>        <td style="text-align:center;">0.705</td>        <td style="text-align:center;">0.899</td>       </tr>       <tr>        <td style="text-align:center;">breastw</td>        <td style="text-align:center;">        <strong>0.993</strong>        </td>        <td style="text-align:center;">0.988</td>        <td style="text-align:center;">0.824</td>        <td style="text-align:center;">0.992</td>       </tr>       <tr>        <td style="text-align:center;">optdigits</td>        <td style="text-align:center;">        <strong>0.999</strong>        </td>        <td style="text-align:center;">0.811</td>        <td style="text-align:center;">0.972</td>        <td style="text-align:center;">        <strong>0.999</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">pendigits</td>        <td style="text-align:center;">        <strong>0.996</strong>        </td>        <td style="text-align:center;">0.955</td>        <td style="text-align:center;">0.978</td>        <td style="text-align:center;">0.995</td>       </tr>       <tr>        <td style="text-align:center;">pima</td>        <td style="text-align:center;">        <strong>0.791</strong>        </td>        <td style="text-align:center;">0.691</td>        <td style="text-align:center;">0.660</td>        <td style="text-align:center;">0.775</td>       </tr>       <tr>        <td style="text-align:center;">cardio</td>        <td style="text-align:center;">        <strong>0.991</strong>        </td>        <td style="text-align:center;">0.892</td>        <td style="text-align:center;">0.970</td>        <td style="text-align:center;">0.987</td>       </tr>       <tr>        <td style="text-align:center;">mammography</td>        <td style="text-align:center;">        <strong>0.938</strong>        </td>        <td style="text-align:center;">0.845</td>        <td style="text-align:center;">0.605</td>        <td style="text-align:center;">0.909</td>       </tr>       <tr>        <td style="text-align:center;">satellite</td>        <td style="text-align:center;">        <strong>0.855</strong>        </td>        <td style="text-align:center;">0.694</td>        <td style="text-align:center;">0.735</td>        <td style="text-align:center;">0.838</td>       </tr>       <tr>        <td style="text-align:center;">annthyroid</td>        <td style="text-align:center;">        <strong>0.922</strong>        </td>        <td style="text-align:center;">0.775</td>        <td style="text-align:center;">0.642</td>        <td style="text-align:center;">0.850</td>       </tr>       <tr>        <td style="text-align:center;">shuttle</td>        <td style="text-align:center;">0.989</td>        <td style="text-align:center;">        <strong>0.994</strong>        </td>        <td style="text-align:center;">0.703</td>        <td style="text-align:center;">0.984</td>       </tr>       <tr>        <td style="text-align:center;">ForestCover</td>        <td style="text-align:center;">        <strong>0.999</strong>        </td>        <td style="text-align:center;">0.925</td>        <td style="text-align:center;">0.842</td>        <td style="text-align:center;">        <strong>0.999</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">http</td>        <td style="text-align:center;">0.997</td>        <td style="text-align:center;">        <strong>0.999</strong>        </td>        <td style="text-align:center;">0.994</td>        <td style="text-align:center;">0.995</td>       </tr>      </tbody>     </table>     </div>     <p>We repeat experiments for 30 times by using the dataset generation procedure to generate observed anomalies, unlabeled training data and test sets. The AUC score is shown in the first line of Table&#x00A0;<a class="tbl" href="#tab2">2</a>. As we can see, the proposed method performs better than all other existing methods, which demonstrate the effectiveness of the proposed method.</p>     <p>Furthermore, we vary the value of parameter <em>k</em> to examine the influence of it. The results in Fig.&#x00A0;<a class="fig" href="#fig3">3</a> show that the behavior tends to get better as the value of <em>k</em> getting closed to the ground-truth. Besides, when <em>k</em> gets to be a little bigger, our method still works fine, which means that our method is not that sensitive with bigger <em>k</em>. However, when we set the value of <em>k</em> to 1(which means that we assume the anomaly are similar), the performance is showed to be pretty unsatisfactory, which validate the necessity of addressing the difference for the anomalies.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Experiments on Real-world Data</h3>     </div>     </header>     <p>To explore the result on real-world data, the experiments are performed on lots of different benchmark datasets which come from different fields&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. Note that in our setting, we are given a handful of observed anomalies, with a large amount of unlabeled samples. To construct this setting, we simply sample a small amount of anomalies as observed ones, as well as plenty of unlabeled samples. The dimension of the data, number of observed anomalies, unlabeled train samples and test samples are shown in Table&#x00A0;<a class="tbl" href="#tab1">1</a>, with &#x2018;class prior&#x2019; indicating the proportion of anomalies in unlabeled and test data. As we can see, the datasets are very diversified, with different dimension, different numbers of samples and different class prior. What&#x0027;s more, we need to address that the number of observed anomalies are pretty small, i.e., as few as 10 for some datasets and at most 100.</p>     <p>For each datasets, normalization is performed, and we repeat experiments for 30 times by using the dataset generation procedure to generate observed anomalies, unlabeled training data and test sets.</p>     <p>The AUC scores are shown in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. As we can see from the table, the proposed method performs significantly better than all other methods (wins 18 times among all 20 datasets), validating the effectiveness of the proposed method. Unsupervised method Isolation Forest reaches the first place on &#x2018;shuttle&#x2019; and &#x2018;http&#x2019; dataset, while on some datasets (e.g., &#x2018;arrhuthmia&#x2019; and &#x2018;speech&#x2019; dataset), the behavior may be pretty unsatisfactory. One interesting result is that the supervised method performs pretty awful, indicating that we should not simply regard all unlabeled samples as negative. One possible explanation is that, if we simple regard all unlabeled samples as negative, the noises will seriously deteriorates the performance.</p>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Accuracy on different datasets. The highest accuracy score is marked in bold.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Proposed Method</td>        <td style="text-align:center;">Unsupervised Method</td>        <td style="text-align:center;">Supervised Method</td>        <td style="text-align:center;">PU Method</td>       </tr>       <tr>        <td style="text-align:center;">Date1</td>        <td style="text-align:center;">        <strong>0.878</strong>        </td>        <td style="text-align:center;">0.609</td>        <td style="text-align:center;">0.849</td>        <td style="text-align:center;">0.847</td>       </tr>       <tr>        <td style="text-align:center;">Date2</td>        <td style="text-align:center;">        <strong>0.883</strong>        </td>        <td style="text-align:center;">0.620</td>        <td style="text-align:center;">0.844</td>        <td style="text-align:center;">0.849</td>       </tr>       <tr>        <td style="text-align:center;">Date3</td>        <td style="text-align:center;">        <strong>0.891</strong>        </td>        <td style="text-align:center;">0.611</td>        <td style="text-align:center;">0.854</td>        <td style="text-align:center;">0.860</td>       </tr>       <tr>        <td style="text-align:center;">Date4</td>        <td style="text-align:center;">        <strong>0.881</strong>        </td>        <td style="text-align:center;">0.613</td>        <td style="text-align:center;">0.849</td>        <td style="text-align:center;">0.847</td>       </tr>       <tr>        <td style="text-align:center;">Date5</td>        <td style="text-align:center;">        <strong>0.876</strong>        </td>        <td style="text-align:center;">0.588</td>        <td style="text-align:center;">0.847</td>        <td style="text-align:center;">0.848</td>       </tr>      </tbody>     </table>     </div>     <p>On some datasets, the PU learning based method can also perform well, while on some dataset such as the &#x2018;arrhythmia&#x2019; dataset, the performance is pretty terrible. This maybe because that, for some datasets, the anomalies are not very diversified, i.e., we can nearly find a concept center for the anomalies, making PU learning strategy feasible. However, when the anomalies get to be diversified, the PU learning based method will fail to reach the goal. Furthermore, as we can see, the proposed method never performs worse than PU learning based method. This is reasonable, because if we set the parameter <em>k</em> of <strong>ADOA</strong> to 1, it is degenerated to a special case of PU learning based strategy.</p>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Application to malicious URL detection</h2>     </div>    </header>    <p>In this section, we apply the proposed method to the problem of malicious URL detection and validate the performance of different methods on this problem.</p>    <p>With the fast development of Internet, more and more kinds of URL attacks have arisen, which becomes a serious threat to cyber-security. During the past years, many methods have been developed for this problem. For example, traditional techniques which based on blacklists or rule lists are first explored. However, these methods lack the ability of detecting potential attacks, making it awkward for cyber-security engineers to efficiently discover newly generated URL attacks.</p>    <p>Machine learning based methods are then explored to provide better generalization performance for this problem. However, as we discussed before, they are mainly focused on supervised and unsupervised setting, and when we are given a small set of recognized malicious URLs (which will be regarded as anomalies) and a large amount of unlabeled URLs, traditional approaches will fail to apply. In this section, we apply our proposed method to it. <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186580/images/www18companion-134-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">The generic syntax of URLs</span>     </div>     </figure>    </p>    <p>We first extract numerical feature from the original URLs. As shown in Fig.&#x00A0;<a class="fig" href="#fig4">4</a>, the URLs can always be separated into different parts, including the scheme part, the authority part (user, password), the path part (host, path), the query part and fragment parts, etc. In our scenario, the first few parts are restricted, and the attacks mainly come from the malicious modification of the fragment parts. Thus, we extract feature for each URL based on the fragment parts. The fragments are always formed as &#x2018;<span class="inline-equation"><span class="tex">$key_1=value_1\&#x0026;\cdots \&#x0026;key_n=value_n$</span>     </span>&#x2019;, and the value may be arbitrarily modified by the attackers to make an attack.</p>    <p>Even more specifically, given a set of URLs, we firstly divide each of them into the aforementioned parts, and then we extract the key-value pairs from the fragments of each URL. Secondly, since we are focused on discovering the trait of <em>malicious</em> URLs, we try to filter the key-value pairs and only keep the top-<em>N</em> keys that appear mostly in the <em>malicious</em> URLs, while the rest of the key-value pairs for each URL are collected together as one key-value pair, thus there will be at most (<em>N</em> + 1) key-value pairs extracted from each URL. In this way, the feature vector will not get to be that tedious. Finally, based on domain knowledge, we heuristically extract eight different statistical information from each of the filtered values, including the count of <em>all</em> characters, letters, numbers, punctuations in the value, and the count of <em>different</em> characters, letters, numbers, punctuations in the value. Thus each URL will be described by a (<em>N</em> + 1)*8 dimensional feature vector.</p>    <p>When running the experiments, the used data is sampled from the daily-arrived URL requests. The data mainly contains two parts: a large set of unlabeled URLs and a handful of malicious URLs which have been already marked by the existing system, and different attack types may appear among the malicious ones, including XXE (XML External Entity Injection), XSS (Cross SiteScript) and SQL injection, etc. Note that the only label information is whether a URL is recognized as a malicious one, the exact type of the attack is unknown. Since the total dataset is too large, we sample more than 10 millions of URLs from one month&#x0027;s requests, in which the number of observed malicious URLs by the existing system is less than 10 thousand. The model is trained using the sampled data, and will be used to predict the scores of each day&#x0027;s new-coming <em>unlabeled</em> URLs. When extracting key-value pairs, <em>N</em> is set to be 99, so that each URL is described by a 800 dimensional vector. Min-max normalization is used to process the features to same scale. What&#x0027;s more, all SVM classifiers are replaced by logistic regression in the experiment, since the dataset is too large.</p>    <p>Since we have no supervision information for the daily-arrived new URLs, we use the help of the cyber-security engineers to manually review the results and verify the effectiveness of the proposed method. It is very time-consuming to check the results, so we select the top-1000-scored potential malicious URLs from each day&#x0027;s data, and cyber-security engineers will manually check whether the selected URLs are malicious or benign with their domain knowledge.</p>    <p>Table&#x00A0;<a class="tbl" href="#tab3">3</a> shows the accuracy of the selected potential malicious URLs on 5 different dates. As we can see, the proposed method perform significantly better than all other methods, which demonstrates the effectiveness of the proposed method on the malicious URL detection task.</p>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>     </div>    </header>    <p>In this paper, we address the problem of anomaly detection. Different to traditional strategies, which formalize this problem as a supervised (with labeled data provided) or unsupervised learning problem (without any labeled samples), we consider the setting when we are given a small amount of observed anomalies, as well as plenty of unlabeled samples. We call this problem as anomaly detection with partially observed anomalies, which can not be directly handled by traditional techniques.</p>    <p>Previous methods are not suitable for this problem. Since no negative samples are provided, supervised learning based method is unfeasible for this task. As for unsupervised learning based methods, without using the information of observed anomalies, the performance may become pretty unsatisfactory. PU learning deal with the task where labeled positive and unlabeled samples are provided. However, the anomalies are not similar to each other, making the PU learning assumption not that suitable.</p>    <p>We propose a method called <strong>ADOA</strong> (Anomaly Detection with partial Observed Anomalies) to solve it. <strong>ADOA</strong> follows a two-stage manner. In stage one, we address the difference between observed anomalies, thus we first cluster the recognized anomalies into <em>k</em> different clusters. Later, we address that the anomalies can first be easily isolated from normal samples, at the same time, they should be similar to the observed anomalies, thus we filter potential anomalies and reliable normal samples from the unlabeled samples according to the isolation score and the similarity score to their nearest anomalies cluster centers. In stage two, we built a multi-class model to distinguish different anomalies from the normal samples. We run experiments on both synthetic and lots of different real-world datasets, which comes from diversified fields. The results on different datasets validate that existing approaches can not perform satisfactorily on this problem and the proposed method performs significantly better than existing methods. Furthermore, we apply the proposed method to the problem of malicious URL detection, the result also demonstrates the effectiveness of the method.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-17">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This research was partially supported by NSFC (61333014). Most work was conducted during Ya-Lin Zhang&#x0027;s internship in Ant Financial.</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Eric Bair. 2013. Semi-supervised Clustering Methods. <em>      <em>Wiley Interdisciplinary Reviews: Computational Statistics</em>     </em>5, 5(2013), 349&#x2013;361.</li>     <li id="BibPLXBIB0002" label="[2]">PS Bradley, KP Bennett, and Ayhan Demiriz. 2000. Constrained k-means Clustering. <em>      <em>Microsoft Research, Redmond</em>     </em>(2000), 1&#x2013;8.</li>     <li id="BibPLXBIB0003" label="[3]">Markus&#x00A0;M Breunig, Hans-Peter Kriegel, Raymond&#x00A0;T Ng, and J&#x00F6;rg Sander. 2000. LOF: Identifying Density-based Local Outliers. In <em>      <em>ACM Sigmod Record</em>     </em>, Vol.&#x00A0;29. ACM, 93&#x2013;104.</li>     <li id="BibPLXBIB0004" label="[4]">Guilherme&#x00A0;O Campos, Arthur Zimek, J&#x00F6;rg Sander, Ricardo&#x00A0;JGB Campello, Barbora Micenkov&#x00E1;, Erich Schubert, Ira Assent, and Michael&#x00A0;E Houle. 2016. On the Evaluation of Unsupervised Outlier Detection: Measures, Datasets, and an Empirical Study. <em>      <em>Data Mining and Knowledge Discovery</em>     </em>30, 4 (2016), 891&#x2013;927.</li>     <li id="BibPLXBIB0005" label="[5]">Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly Detection: A Survey. <em>      <em>ACM Computing Surveys (CSUR)</em>     </em>41, 3 (2009), 15.</li>     <li id="BibPLXBIB0006" label="[6]">O Chapelle, B Sch&#x00F6;lkopf, and A Zien. 2006. Semi-supervised Learning. (2006).</li>     <li id="BibPLXBIB0007" label="[7]">Yunqiang Chen, Xiang&#x00A0;Sean Zhou, and Thomas&#x00A0;S Huang. 2001. One-class SVM for Learning in Image Retrieval. In <em>      <em>Proceeding of 2001 International Conference on Image Processing</em>     </em>, Vol.&#x00A0;1. IEEE, 34&#x2013;37.</li>     <li id="BibPLXBIB0008" label="[8]">Marthinus&#x00A0;C du Plessis, Gang Niu, and Masashi Sugiyama. 2014. Analysis of Learning From Positive and Unlabeled Data. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 703&#x2013;711.</li>     <li id="BibPLXBIB0009" label="[9]">Marthinus&#x00A0;C du Plessis, Gang Niu, and Masashi Sugiyama. 2015. Convex Formulation for Learning from Positive and Unlabeled Data. In <em>      <em>Proceeding of the 32nd International Conference on Machine Learning</em>     </em>. 1386&#x2013;1394.</li>     <li id="BibPLXBIB0010" label="[10]">Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, and Sal Stolfo. 2002. A Geometric Framework for Unsupervised Anomaly Detection: Detecting Intrusions in Unlabeled Data. <em>      <em>Applications of Data Mining in Computer Security</em>     </em>6 (2002), 77&#x2013;102.</li>     <li id="BibPLXBIB0011" label="[11]">Yong Ge, Hui Xiong, Zhi-hua Zhou, Hasan Ozdemir, Jannite Yu, and Kuo&#x00A0;Chu Lee. 2010. Top-eye: Top-k Evolving Trajectory Outlier Detection. In <em>      <em>Proceedings of the 19th ACM International Conference on Information and Knowledge Management</em>     </em>. ACM, 1733&#x2013;1736.</li>     <li id="BibPLXBIB0012" label="[12]">Nico G&#x00F6;rnitz, Marius&#x00A0;Micha Kloft, Konrad Rieck, and Ulf Brefeld. 2013. Toward Supervised Anomaly Detection. <em>      <em>Journal of Artificial Intelligence Research</em>     </em> (2013).</li>     <li id="BibPLXBIB0013" label="[13]">Marti&#x00A0;A. Hearst, Susan&#x00A0;T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. 1998. Support Vector Machines. <em>      <em>IEEE Intelligent Systems and Their Applications</em>     </em>13, 4(1998), 18&#x2013;28.</li>     <li id="BibPLXBIB0014" label="[14]">Rolf Isermann and Peter Balle. 1997. Trends in the Application of Model-based Fault Detection and Diagnosis of Technical Processes. <em>      <em>Control Engineering Practice</em>     </em>5, 5 (1997), 709&#x2013;719.</li>     <li id="BibPLXBIB0015" label="[15]">Edwin&#x00A0;M Knorr, Raymond&#x00A0;T Ng, and Vladimir Tucakov. 2000. Distance-based Outliers: Algorithms and Applications. <em>      <em>the International Journal on Very Large Data Bases</em>     </em>8, 3-4 (2000), 237&#x2013;253.</li>     <li id="BibPLXBIB0016" label="[16]">Yufeng Kou, Chang-Tien Lu, Sirirat Sirwongwattana, and Yo-Ping Huang. 2004. Survey of Fraud Detection Techniques. In <em>      <em>Proceeding of the 11st IEEE International Conference on Networking, Sensing and Control</em>     </em>, Vol.&#x00A0;2. IEEE, 749&#x2013;754.</li>     <li id="BibPLXBIB0017" label="[17]">Wee&#x00A0;Sun Lee and Bing Liu. 2003. Learning with Positive and Unlabeled Examples using Weighted Logistic Regression. In <em>      <em>Proceeding of the 20th International Conference on Machine Learning</em>     </em>, Vol.&#x00A0;3. 448&#x2013;455.</li>     <li id="BibPLXBIB0018" label="[18]">Elizabeth Leon, Olfa Nasraoui, and Jonatan Gomez. 2004. Anomaly Detection based on Unsupervised Niche Clustering with Application to Network Intrusion Detection. In <em>      <em>IEEE Conference on Evolutionary Computation</em>     </em>, Vol.&#x00A0;1. IEEE, 502&#x2013;508.</li>     <li id="BibPLXBIB0019" label="[19]">Bing Liu, Yang Dai, Xiaoli Li, Wee&#x00A0;Sun Lee, and Philip&#x00A0;S Yu. 2003. Building Text Classifiers Using Positive and Unlabeled Examples. In <em>      <em>Proceeding of the 3rd IEEE International Conference on Data Mining</em>     </em>. IEEE, 179&#x2013;186.</li>     <li id="BibPLXBIB0020" label="[20]">Bing Liu, Wee&#x00A0;Sun Lee, Philip&#x00A0;S Yu, and Xiaoli Li. 2002. Partially Supervised Classification of Text Documents. In <em>      <em>Proceeding of the 19th International Conference on Machine Learning</em>     </em>, Vol.&#x00A0;2. 387&#x2013;394.</li>     <li id="BibPLXBIB0021" label="[21]">Bo Liu, Yanshan Xiao, Longbing Cao, Zhifeng Hao, and Feiqi Deng. 2013. SVDD-based Outlier Detection on Uncertain Data. <em>      <em>Knowledge and Information Systems</em>     </em>(2013), 1&#x2013;22.</li>     <li id="BibPLXBIB0022" label="[22]">Fei&#x00A0;Tony Liu, Kai&#x00A0;Ming Ting, and Zhi-Hua Zhou. 2008. Isolation Forest. In <em>      <em>Proceeding of the 8th IEEE International Conference on Data Mining</em>     </em>. IEEE, 413&#x2013;422.</li>     <li id="BibPLXBIB0023" label="[23]">Fei&#x00A0;Tony Liu, Kai&#x00A0;Ming Ting, and Zhi-Hua Zhou. 2012. Isolation-based Anomaly Detection. <em>      <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>     </em>6, 1(2012), 3.</li>     <li id="BibPLXBIB0024" label="[24]">Xu-Ying Liu and Zhi-Hua Zhou. 2011. Towards Cost-sensitive Learning for Real-world Applications. In <em>      <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>     </em>. Springer, 494&#x2013;505.</li>     <li id="BibPLXBIB0025" label="[25]">Zhigang Liu, Wenzhong Shi, Deren Li, and Qianqing Qin. 2006. Partially Supervised Classification: Based on Weighted Unlabeled Samples Support Vector Machine. <em>      <em>International Journal of Data Warehousing and Mining (IJDWM)</em>     </em>2, 3(2006), 42&#x2013;56.</li>     <li id="BibPLXBIB0026" label="[26]">Gerhard M&#x00FC;nz, Sa Li, and Georg Carle. 2007. Traffic Anomaly Detection using k-means Clustering. In <em>      <em>GI/ITG Workshop MMBnet</em>     </em>.</li>     <li id="BibPLXBIB0027" label="[27]">Hoang&#x00A0;Vu Nguyen, Hock&#x00A0;Hee Ang, and Vivekanand Gopalkrishnan. 2010. Mining Outliers with Ensemble of Heterogeneous Detectors on Random Subspaces. In <em>      <em>International Conference on Database Systems for Advanced Applications</em>     </em>. Springer, 368&#x2013;383.</li>     <li id="BibPLXBIB0028" label="[28]">Salima Omar, Asri Ngadi, and Hamid&#x00A0;H Jebur. 2013. Machine Learning Techniques for Anomaly Detection: an Overview. <em>      <em>International Journal of Computer Applications</em>     </em>79, 2(2013).</li>     <li id="BibPLXBIB0029" label="[29]">Shebuti Rayana. 2016. ODDS Library. (2016). <a class="link-inline force-break" href="http://odds.cs.stonybrook.edu"      target="_blank">http://odds.cs.stonybrook.edu</a></li>     <li id="BibPLXBIB0030" label="[30]">Rowland&#x00A0;R Sillito and Robert&#x00A0;B Fisher. 2008. Semi-supervised Learning for Anomalous Trajectory Detection.. In <em>      <em>the British Machine Vision Conference (BMVC)</em>     </em>, Vol.&#x00A0;27. 1025&#x2013;1044.</li>     <li id="BibPLXBIB0031" label="[31]">Ming-Yang Su. 2011. Real-time Anomaly Detection Systems for Denial-of-Service Attacks by Weighted k-nearest-neighbor Classifiers. <em>      <em>Expert Systems with Applications</em>     </em>38, 4 (2011), 3492&#x2013;3498.</li>     <li id="BibPLXBIB0032" label="[32]">Swee&#x00A0;Chuan Tan, Kai&#x00A0;Ming Ting, and Tony&#x00A0;Fei Liu. 2011. Fast Anomaly Detection for Streaming Data. In <em>      <em>Proceedings of the 22nd International Joint Conference on Artificial Intelligence</em>     </em>, Vol.&#x00A0;22. 1511.</li>     <li id="BibPLXBIB0033" label="[33]">Hua Tang and Zhuolin Cao. 2009. Machine Learning-based Intrusion Detection Algorithm. <em>      <em>Journal of Computational Information Systems</em>     </em>5, 6 (2009), 1825&#x2013;1831.</li>     <li id="BibPLXBIB0034" label="[34]">Kiri Wagstaff, Claire Cardie, Seth Rogers, Stefan Schr&#x00F6;dl, and others. 2001. Constrained k-means Clustering with Background Knowledge. In <em>      <em>Proceeding of the 18th International Conference on Machine Learning</em>     </em>, Vol.&#x00A0;1. 577&#x2013;584.</li>     <li id="BibPLXBIB0035" label="[35]">Su-Yun Wu and Ester Yen. 2009. Data Mining-based Intrusion Detectors. <em>      <em>Expert Systems with Applications</em>     </em>36, 3 (2009), 5605&#x2013;5612.</li>     <li id="BibPLXBIB0036" label="[36]">Xiaojin Zhu. 2006. Semi-supervised Learning Literature Survey. <em>      <em>Computer Science, University of Wisconsin-Madison</em>     </em>2, 3(2006), 4.</li>     <li id="BibPLXBIB0037" label="[37]">Arthur Zimek, Erich Schubert, and Hans-Peter Kriegel. 2012. A Survey on Unsupervised Outlier Detection in High-dimensional Numerical Data. <em>      <em>Statistical Analysis and Data Mining: The ASA Data Science Journal</em>     </em>5, 5(2012), 363&#x2013;387.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186580">https://doi.org/10.1145/3184558.3186580</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
