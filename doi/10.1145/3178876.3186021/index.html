<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>MemeSequencer: Sparse Matching for Embedding Image
  Macros</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186021'>https://doi.org/10.1145/3178876.3186021</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186021'>https://w3id.org/oa/10.1145/3178876.3186021</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">MemeSequencer: Sparse Matching
          for Embedding Image Macros</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Abhimanyu</span> <span class=
          "surName">Dubey</span>, Massachusetts Institute of
          Technology, <a href=
          "mailto:dubeya@mit.edu">dubeya@mit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Esteban</span> <span class=
          "surName">Moro</span>, Massachusetts Institute of
          Technology, Universidad Carlos III de Madrid, <a href=
          "mailto:emoro@math.uc3m.es">emoro@math.uc3m.es</a>
        </div>
        <div class="author">
          <span class="givenName">Manuel</span> <span class=
          "surName">Cebrian</span>, Massachusetts Institute of
          Technology, <a href=
          "mailto:cebrian@mit.edu">cebrian@mit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Iyad</span> <span class=
          "surName">Rahwan</span>, Massachusetts Institute of
          Technology, <a href=
          "mailto:irahwan@mit.edu">irahwan@mit.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186021"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186021</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The analysis of the creation, mutation, and
        propagation of social media content on the Internet is an
        essential problem in computational social science,
        affecting areas ranging from marketing to political
        mobilization. A first step towards understanding the
        evolution of images online is the analysis of rapidly
        modifying and propagating memetic imagery or ‘memes’.
        However, a pitfall in proceeding with such an investigation
        is the current incapability to produce a robust semantic
        space for such imagery, capable of understanding
        differences in Image Macros. In this study, we provide a
        first step in the systematic study of image evolution on
        the Internet, by proposing an algorithm based on sparse
        representations and deep learning to decouple various types
        of content in such images and produce a rich semantic
        embedding. We demonstrate the benefits of our approach on a
        variety of tasks pertaining to memes and Image Macros, such
        as image clustering, image retrieval, topic prediction and
        virality prediction, surpassing the existing methods on
        each. In addition to its utility on quantitative tasks, our
        method opens up the possibility of obtaining the first
        large-scale understanding of the evolution and propagation
        of memetic imagery.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>image
          virality</small>,</span> <span class=
          "keyword"><small>image macros</small>,</span>
          <span class="keyword"><small>feature
          extraction</small>,</span> <span class=
          "keyword"><small>sparse representation</small>,</span>
          <span class="keyword"><small>embeddings</small>,</span>
          <span class="keyword"><small>social network
          analysis</small>,</span> <span class=
          "keyword"><small>content understanding</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Abhimanyu Dubey, Esteban Moro, Manuel Cebrian, and Iyad
          Rahwan. 2018. MemeSequencer: Sparse Matching for
          Embedding Image Macros. In <em>WWW 2018: The 2018 Web
          Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 11 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186021" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186021</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Social networks have increasingly become an integral part
      of modern life. Recent research in computational social
      science has focused on detecting the most shared content, the
      extent and pace of sharing of content, and the most
      influential content-sharing agents in a social
      network&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>]. This line of inquiry, based on the
      study of predicting and understanding <em>content
      virality</em> has also risen interest in computer
      science&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0052">52</a>]. Content diffusion online can be
      understood as a product of two intertwined properties: i) the
      nature of the content, its evolution and mutations, and ii)
      the properties of the social network on which it
      propagates.</p>
      <p>Diffusion of content and <em>cascade prediction</em> have
      received substantial attention in this domain. Several lines
      of recent research have focused on understanding and
      predicting cascades&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>], the probabilities of information
      diffusion in cascades&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0047">47</a>], and the recurrence of
      cascades&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>]. These cascades are crucial in
      understanding the influence of the underlying social network
      on predicting the extent of propagation (popularity or
      virality) and provide strong insights into the importance of
      strong community structures in content
      propagation&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0054">54</a>]. Extensive research has also been
      done in understanding the strength and extent of online
      community structures and their impact on information
      diffusion&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0053">53</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0055">55</a>].</p>
      <p>With increased online big data collection and processing,
      research has focused on understanding content virality
      through the information contained in online imagery or
      text&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>]. Contrary
      to the earlier mentioned research, this line of focus looks
      at the impact of content in predicting virality,
      independently from the network structure and its constituent
      effects of social reinforcement, homophily and spreading
      pattern. Using computer vision techniques, studies have
      looked at regions of images that promote content
      virality&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>].</p>
      <p>An interesting combination of these two different lines of
      research is the study of evolution of information in social
      networks&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>]. Since many memes exist in the social
      network that persist by mutating constantly&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>],
      understanding the mutations that are responsible for
      accelerating or hindering the popularity of a meme can be
      influential in content creation and understanding the
      cultural composition of online communities. An issue,
      however, with this line of study is the difficulty in
      isolating the <em>underlying cultural meme</em> from its
      various manifestations in online content&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0034">34</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0048">48</a>].</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">A sample image macro (“Futurama Fry”) with
          its most common overlays.</span>
        </div>
      </figure>
      <p></p>
      <p>Identifying latent cultural memes from content such as
      tweets has been attempted first by Leskovec&nbsp;<em>et
      al.</em>[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0037">37</a>],
      utilizing topic modeling, without explicitly considering
      mutations in content. Approaches such as n-grams and deep
      neural representations of text&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>] have also been utilized
      to some success. When operating on Twitter data, hashtags
      provide a grounded and less noisy representation of a meme,
      that has been utilized in studying the propagation of
      associated content on social networks&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0042">42</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0046">46</a>]. The work of
      Coscia&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0016">16</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>] has
      studied the nature of competition between image macros
      online. The systematic study of mutations, however, still
      remains elusive under these approaches, and to the best of
      our knowledge, there is no work on the evolution of
      image-based memes.</p>
      <p>In this work, we provide a systematic framework and
      associated semantic feature space to study memetic imagery.
      Unlike the evolution of text, images mutate and evolve in a
      relatively controlled manner on the Internet, typical of
      which is the propagation of Image Macros, the most common
      type of online visual meme&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>]. As described by Knobel and
      Lankshear&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>], an Image Macro is the
      representation of an idea using an image superimposed with
      text optional alternative imagery. This form of representing
      a meme has been incredibly successful at dissemination, and
      is extremely popular on social networks&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0025">25</a>].</p>
      <p>In their most common form, Image Macros usually possess
      one or two lines of text flanking the template image in the
      center. Additionally, they may have altered imagery
      superimposed on the template image as well. Their etymology
      stems from the usage of the word “macro” in computer science,
      as a ‘rule or a pattern that maps an input to an
      output’&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0038">38</a>]. This highlights the usage of the
      Image Macro as a general purpose meme representation, that
      can be altered to fit the context specified by the overlaid
      text. The combination of the overarching memetic theme
      provided by the instantly-recognizable template image with
      the subtle contextual information provided by the overlaid
      text or imagery creates an instantly perceivable new meme
      that is versatile and adapted to the targeted community,
      justifying its prevalence in social media.</p>
      <p>Scientific inquiry involving the propagation of these
      Image Macros can hence provide a stronger signal in
      understanding the transformation of cultural memes. The
      primary problem, which we focus on in this work, is the
      creation of a semantic representation for Image Macros that
      preserve the semantic information in each image macro while
      mapping different instances of image macros created from the
      same base template image close together, preserving the
      global context (supplied by the image), while additionally
      maintaining the individual context provided by each image
      macro. The baseline technique to solve a problem such as this
      would be to use deep neural network features, which have
      shown tremendous capabilities in encapsulating information
      from images. However, deep convolutional networks cannot
      decouple the overlaid imagery and template imagery, and
      process the overlay as noise, when the overlay in fact
      provides critical contextual information about the macro
      itself. This results in a loss of information from the
      macros, mapping most of them to the similar representation,
      which only amplifies as more overlays are made.</p>
      <p>In this study, we create an algorithm that first uses the
      idea of sparse representation to identify template images
      from each Image Macro, and then using the obtained template,
      decouples the overlaid information from the base template. We
      then proceed to extract multimodal features from each image,
      resulting in a rich, informative and robust feature
      representation. Using this feature representation, we
      demonstrate remarkable improvements across several
      qualitiative and quantitative tasks involving social media
      imagery, demonstrating the conceptual and functional
      superiority of our approach from other baseline techniques.
      In cases where the template set is not known beforehand, we
      also provide an algorithm that can recover the template image
      from a set of sample macros based on median blending of
      images.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Method</h2>
        </div>
      </header>
      <p>Our method is based on a strong underlying assumption –
      memetic imagery online contains substantial amounts of Image
      Macros, that are constructed from a set of template images by
      choosing a template image and overlaying text and/or
      additional imagery on it. This assumption is exploited in our
      formulation. We begin the algorithmic description with
      preliminaries:</p>
      <p><strong>Target Set</strong>: Our target set
      <strong>T</strong> is the set of images that we wish to embed
      in a semantically-grounded space. In our experiments, this
      usually is the dataset that we conduct experiments on,
      scraped from websites such as Memegenerator&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>] or
      Quickmeme&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>]. We use this set of images to
      construct the set of template images, following
      Algorithm&nbsp;2, that we then use for Sparse Matching and
      feature extraction.</p>
      <p><strong>Template Set</strong>: The template set
      <strong>S</strong> is the set of images with no overlays that
      we match each image in the target set with, to obtain the
      decoupled <em>Image Macro</em> representation. This template
      set can be supplied beforehand, but in case it is not, we
      construct it from the Target Set itself using an algorithm
      involving Sparse Matching and Median Blending.</p>
      <p><strong>Overlay Types</strong>: Figure&nbsp;<a class="fig"
      href="#fig1">1</a> specifies the typical kinds of overlays on
      template images to produce Image Macros. The most common
      overlay is simple text in a white font (as shown in the
      section ‘text overlay’). There can be modifications in the
      color or the addition of an image, which fall under the
      category of ‘image overlay’. Additionally, both these things
      may be present together, images of which fall in the
      ‘combined overlay’ category. We take care of minute
      variations in color across images by contrast
      normalization.</p>
      <p>The goal of our Sparse Matching algorithm is to obtain the
      template image any sample <em>Image Macro</em> has been
      constructed from. Using this template image, we can then
      decouple the image overlay from the Macro, and process the
      overlay and corresponding template separately to decouple the
      local context (specified by the overlay) and the global
      context (specified by the template).</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Algorithm
            Overview</h3>
          </div>
        </header>
        <p>Our task can be summarized as learning an embedding
        function <span class="inline-equation"><span class="tex">$f
        : \mathbb {R}^n \rightarrow \mathbb {R}^d$</span></span>
        that maps images to a low-dimensional embedding that
        preserves semantic content. To create an embedding, our
        method follows three distinct subroutines as described
        below:</p>
        <p>(1) <strong>Overlay Decoupling</strong>: The first step
        of the algorithm is to identify and separate the overlaid
        content from the template image. To do this we employ
        global image contrast normalization followed by
        ℓ<sup>1</sup>-sparse reconstruction, first introduced in
        the seminal work on face recognition by Wright et
        al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0056">56</a>].</p>
        <p>(2) <strong>Image Feature Extraction</strong>: Image
        features learnt through deep convolutional neural networks
        (CNNs) for object classification have been shown to be
        tremendously powerful at capturing semantic information,
        and have excelled at a variety of inference
        tasks&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>]. To capture the semantic content
        of the imagery, we use deep CNNs trained on image
        classification, and then finetuned on our target task.</p>
        <p>(3) <strong>Textual Feature Extraction</strong>: To
        augment the information provided by the image features, we
        additionally extract text present in the images using
        optical character recognition (OCR), and augment our
        embedding with features extracted from this text data. To
        learn these text features, we use a deep recurrent neural
        network&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0039">39</a>], inspired by their immense success
        in a variety of inference tasks in the domain of natural
        language processing&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0027">27</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0061">61</a>].</p>
        <p>After overlay decoupling and multimodal feature
        extraction, we concatenate the obtained text and image
        features to produce a powerful embedding for image
        retrieval and clustering of the target meme imagery, whose
        representational strength we verify in several experiments
        as described later.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Decoupling
            Overlays from Templates</h3>
          </div>
        </header>
        <p>The first subroutine of our method involves decoupling
        the image overlay from the template it was produced from.
        This can be done using a straightforward pixel-wise
        subtraction, however, we do not have the source template of
        each image <em>a priori</em>, which makes the first task
        the identification or matching of the correct template
        image from the provided test image. We begin by first
        normalizing the color in the image with a global pixel-wise
        mean normalization, to remove the slight aberrations in
        color across images. We then downsample each image in both
        the template image set <strong>S</strong> and target image
        set <strong>T</strong> to a fixed resolution of 48 × 48
        pixels. Consider these downsampled sets as
        <strong>S</strong> <sub><em>d</em></sub> (template set) and
        <strong>T</strong> <sub><em>d</em></sub> (target set)
        respectively. Given these sets of normalized, downsampled
        images, we now describe the sparse representation
        algorithm.</p>
        <section id="sec-10">
          <p><em>2.2.1 Sparse Representation.</em> Introduced by
          Wright et al.&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0056">56</a>], the sparse representation
          algorithm provides a framework for inference under the
          assumption that the training samples lie on a subspace.
          Hence, each input test point can be written as a sparse
          linear combination of the training points. If we consider
          the training set to be the set of downsampled template
          meme images <strong>S</strong> <sub><em>d</em></sub> ,
          and test set to be each image in the target set
          <strong>T</strong> <sub><em>d</em></sub> , we can apply
          the sparse representation algorithm to match each sample
          in <strong>T</strong> <sub><em>d</em></sub> to a sample
          in <strong>S</strong> <sub><em>d</em></sub> . By this
          process, we can effectively recover the original template
          the macro was created from, and decouple the template
          from the overlay.</p>
          <p><strong>Matching</strong>: Let the total number of
          images present across all templates be <em>m</em>, and
          images in each class <em>i</em> be given by
          <em>m<sub>i</sub></em> . Hence, <span class=
          "inline-equation"><span class="tex">$\sum _{i=1}^k m_i =
          m$</span></span> . Given a set <strong>S</strong>
          <sub><em>d</em>, <em>i</em></sub> of
          <em>m<sub>i</sub></em> images, represented as a matrix
          <span class="inline-equation"><span class="tex">$[\mathbf
          {s}_{1,i}, ..., \mathbf {s}_{m_i,i}] \in \mathbb {R}^{n
          \times m_i}$</span></span> belonging to class <em>i</em>,
          any new target sample <span class=
          "inline-equation"><span class="tex">$\mathbf {y} \in
          \mathbf {T}_d \subset \mathbb {R}^n$</span></span>
          belonging to template <em>i</em> will approximately lie
          in the linear span of the training samples of
          <strong>S</strong> <sub><em>d</em>, <em>i</em></sub> .
          Hence:</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathbf {y}
              &amp;= \alpha _{1,i} \mathbf {s}_{1,i} + \alpha
              _{2,i} \mathbf {s}_{2,i} + \alpha _{3,i} \mathbf
              {s}_{3,i} + ... + \alpha _{m_i,i} \mathbf {s}_{m_i,i}
              \end{align}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>Where <span class="inline-equation"><span class=
          "tex">$\alpha _{i,j} \in \mathbb {R}$</span></span> . If
          we consider the set of all classes <span class=
          "inline-equation"><span class="tex">$\mathbf {S}_d = \cup
          _i^k \ (\mathbf {S}_{d,i})$</span></span> , we can write
          the matrix for all the <em>m<sub>i</sub></em> samples
          from each of the <em>k</em> classes as:
          <div class="table-responsive" id="eq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathbf {A}
              &amp;:= [\mathbf {S}_{d,1}, \mathbf {S}_{d,2}, ...,
              \mathbf {S}_{d,k}] = [\mathbf {s}_{1,1}, \mathbf
              {s}_{2,1},...,\mathbf {s}_{m_k, k}]
              \end{align}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>Using this, we can write the linear representation
          of <strong>y</strong> over all training samples as:
          <div class="table-responsive" id="eq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathbf {y}
              &amp;= \mathbf {A} \mathbf {x}_0 \ \in \mathbb {R}^n
              \end{align}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>where, <span class="inline-equation"><span class=
          "tex">$\mathbf {x}_0 = [0,...,0,\alpha _{1,i},\alpha
          _{2,i},...,\alpha _{m_i,i}, 0, ..., 0]^\top \in \mathbf
          {R}^m$</span></span> is a coefficient vector with zero
          entries except for samples belonging to class <em>i</em>.
          As proposed in [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0056">56</a>], to obtain the sparsest solution
          of the above equation, we have to solve the following
          ℓ<sup>0</sup> optimization problem:
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \hat{\mathbf
              {x}}_0 = \arg \min \Vert \mathbf {x} \Vert _0 \ \ \
              \text{, subject to} \ \ \ \mathbf {A} \mathbf {x} =
              \mathbf {y} \end{align}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>This corresponds to finding a coefficient vector
          <span class="inline-equation"><span class=
          "tex">$\hat{\mathbf {x}}_0$</span></span> that contains
          the smallest number of non-zero entries. This problem in
          its current form is to find the sparsest solution of an
          underdetermined linear system, and is NP-hard, and even
          difficult to approximate&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0004">4</a>]. As
          proposed in&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0022">22</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0056">56</a>], under certain assumptions of
          sparsity, the ℓ<sup>0</sup> minimization solution is
          equivalent to the ℓ<sup>1</sup> minimization solution. We
          therefore solve the following problem:
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \hat{\mathbf
              {x}}_1 = \arg \min \Vert \mathbf {x} \Vert _1 \ \ \
              \text{, subject to} \ \ \ \mathbf {A} \mathbf {x} =
              \mathbf {y} \end{align}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>This corresponds to finding a coefficient vector
          <span class="inline-equation"><span class=
          "tex">$\hat{\mathbf {x}}_0$</span></span> that has the
          minimum ℓ<sup>1</sup>-norm (<span class=
          "inline-equation"><span class="tex">$\sum _{i=1}^m
          |\mathbf {x}^{(i)}_i|)$</span></span> . Using standard
          linear programming methods, one can solve this problem in
          polynomial time&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0012">12</a>]. As described in detail
          in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0056">56</a>], this algorithm recovers the
          correct training class even in the presence of severe
          occlusion (as provided by the overlaid text and/or
          pictures to an image), and heavy amounts of random noise.
          Hence, this method is ideal in our application case.
          <p></p>
          <p><img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-Algo1.jpg"
          class="img-responsive" alt="" longdesc="" /></p>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">Sample recovery of the
              template images “Willy Wonka” and “Success Kid” from
              the target set examples. We show the obtained
              template at (a) 0 iterations, (b) 10 iterations, (c)
              20 iterations, (d) 50 iterations and (e) 100
              iterations.</span>
            </div>
          </figure>
          <p></p>
          <p>Our algorithm is described in Algorithm 1 . We begin
          by computing our downsampled template and target sets
          <strong>S</strong> <sub><em>d</em></sub> and
          <strong>T</strong> <sub><em>d</em></sub> respectively,
          and set the output set |<em>mathbfO<sub>d</sub></em> to
          the empty set. For each image <strong>t</strong>
          <sub><em>i</em></sub> in the target set, we first compute
          the sparse representation weights <strong>x</strong> as
          described earlier. Using the components of the sparse
          representation, we then proceed to evaluate the number of
          non-zero weights present for samples of each template
          class and store it in variable <em>z<sub>c</sub></em> for
          template class <em>c</em>. Owing to the sparse
          representation formulation, we find that only the matched
          template class possesses non-zero components. We then
          assign the template class by choosing the class with the
          maximum number of non-zero sparse weights, and assign the
          blank template image as the corrected set for the input
          sample <strong>t</strong> <sub><em>i</em></sub> . In case
          no matching template is found (the error in
          reconstruction is larger than a threshold
          <em>t<sub>r</sub></em> , that is <span class=
          "inline-equation"><span class="tex">$\Vert \mathbf {A}
          \hat{\mathbf {x}} - \mathbf {t}_i \Vert _2 {\gt}
          t_r$</span></span> ), we return the target image itself
          as the matched template.</p>
          <p>Once we have the assigned template image <span class=
          "inline-equation"><span class="tex">$\mathbf
          {s}_{1,\hat{z}^{(i)}}$</span></span> , we can decouple
          the overlay from the template by computing the decoupled
          overlay as <span class="inline-equation"><span class=
          "tex">$\mathbf {t}_i - \mathbf
          {s}_{1,\hat{z}^{(i)}}$</span></span> , which can then
          help us do decoupled feature extraction. A caveat of this
          algorithm is that it requires an exhaustive template set
          to filter images successfully. However, in practical
          cases, we might not have the template set
          <strong>S</strong>, in which case the next algorithm to
          construct the template image set <strong>S</strong> from
          the target image set <strong>T</strong> can be used.</p>
          <p><img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-Algo2.jpg"
          class="img-responsive" alt="" longdesc="" /></p>
          <p><strong>Creation of Template Set</strong>: To
          construct a template set automatically from the target
          set, we describe an algorithm that utilizes the concept
          of median blending in image processing&nbsp;[<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0059">59</a>].
          Median Blending is a commonly used technique in image
          processing to obtain a stable image from several noisy
          images. The central idea is to iteratively grow the
          template set and refine the template via successive
          median blending.</p>
          <figure id="fig3">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-fig3.jpg"
            class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span>
              <span class="figure-title">The central feature
              extraction pipeline.</span>
            </div>
          </figure>
          <p></p>
          <p>We begin with an empty pre-augmented template set
          <strong>S</strong> <sub><em>d</em>, <em>t</em></sub> ,
          and iterate over the target set. For the first iteration,
          we simply add the input image itself to
          <strong>S</strong> <sub><em>d</em>, <em>t</em></sub> ,
          since our template set is empty. From here on, we
          maintain a set of template images <span class=
          "inline-equation"><span class="tex">$\mathbf {U}_{\mathbf
          {t}_i}$</span></span> for every template
          <strong>t</strong> <sub><em>i</em></sub> we identify
          (hence, for the first iteration, we add the first image
          to the set <span class="inline-equation"><span class=
          "tex">$\mathbf {U}_{\mathbf {t}_i}$</span></span> ). For
          every subsequent image, we compute the sparse
          coefficients <span class="inline-equation"><span class=
          "tex">$\mathbf {s}_{\hat{z}^{(i)}}$</span></span> using
          <strong>S</strong> <sub><em>d</em>, <em>t</em></sub> ,
          and if the input image matches with a template (even if a
          template is garbled, the sparse representation will
          ensure a sparse match, which we evaluate via the
          reconstruction error <span class=
          "inline-equation"><span class="tex">$\Vert \mathbf {A}
          \hat{\mathbf {x}} - \mathbf {t}_i \Vert _2$</span></span>
          ). If a match is found, we add the input image to the set
          of images corresponding to the matched template image
          (<span class="inline-equation"><span class="tex">$\mathbf
          {U}_{\mathbf {s}_{\hat{z}^{(i)}}}$</span></span> ). We
          construct the new version of the template by blending all
          of the images in the set of matched images corresponding
          to that template. This blending is done by creating a new
          image where each pixel is the median of the corresponding
          pixels in all the images (referred to as
          PixelWiseMedianBlending). For every input image, we
          proceed in a similar manner until the new obtained median
          image is within a small error of the median image in the
          previous iteration (we check if <span class=
          "inline-equation"><span class="tex">$\Vert \mathbf {v} -
          \mathbf {s}_{\hat{z}^{(i)}} \Vert _2 \le
          t_b$</span></span> , if yes, we reach convergence for
          image <span class="inline-equation"><span class=
          "tex">$\mathbf {s}_{\hat{z}^{(i)}}$</span></span> , and
          set <span class="inline-equation"><span class=
          "tex">$c_{\mathbf {s}_{\hat{z}^{(i)}}} = 1$</span></span>
          ). After convergence, we do not alter the template image.
          Figure&nbsp;<a class="fig" href="#fig2">2</a> describes
          how the median image evolves with increasing
          iterations.</p>
          <p>Once we have passed through all images in the target
          set, we augment the produced template set by random flips
          and crops of each template image (procedure described as
          ‘Augment’). This is done since several Image Macros are
          also created from flipped and cropped versions of the
          template image, and this method ensures that every test
          image is mapped to a template correctly. This algorithm
          is described in Algorithm&nbsp;2 .</p>
        </section>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Image
            Feature Extraction</h3>
          </div>
        </header>
        <p>Once we have completed the first procedure involving
        decoupling the overlay, we are ready to extract features
        that encapsulate the semantic visual content present in the
        imagery. A wide variety of image features have been
        experimented with in computer vision literature pertaining
        to web and social media&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0032">32</a>]. An emerging consensus in computer
        vision has been the extreme efficiency of deep neural
        network features in capturing semantic content for
        classification and understanding, as described in Donahue
        et al.&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>]. From this, we continue all image
        feature extraction using on deep neural network models.</p>
        <section id="sec-12">
          <p><em>2.3.1 Convolutional Neural Networks.</em> The
          immense popularity of convolutional neural networks
          across a wide variety of computer vision tasks have made
          it our default choice for feature extraction. Following
          standard practice in computer vision&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0020">20</a>,
          <a class="bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0024">24</a>], we
          consider neural network models trained on the image
          classification dataset ImageNet&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0019">19</a>], and
          additionally consider models first trained on ImageNet
          followed by further fine-tuning on virality prediction
          datasets.</p>
        </section>
        <section id="sec-13">
          <p><em>2.3.2 Decoupled Feature Extraction.</em> For each
          image <strong>t</strong> <sub><em>i</em></sub> in the
          target set <strong>T</strong>, we extract features from
          two separate images. We first extract features from the
          matched image from the template set <strong>o</strong>
          <sub><em>i</em></sub> ∈ <strong>O</strong>, and then
          compute the difference image <strong>d</strong>
          <sub><em>i</em></sub> = <strong>t</strong>
          <sub><em>i</em></sub> − <strong>o</strong>
          <sub><em>i</em></sub> , and extract features from this
          image as well, eventually concatenating the two sets of
          features to form our final feature vector
          <strong>v</strong> <sub><em>i</em></sub> . Hence,</p>
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathbf {v}_i =
              [f(\mathbf {o}_i), f(\mathbf {t}_i - \mathbf {o}_i)]
              \end{align}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>Here, <em>f</em>() is a function that maps an image
          to a multidimensional semantic feature vector, and is
          obtained from a CNN. Since the basic overlaying of text
          or additional imagery would be treated as noise by the
          CNN, we separate the two components and extract features
          separately. This ensures that images belonging to the
          same base template have features that are closer together
          in semantic space.
          <p></p>
        </section>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.4</span> Textual
            Feature Extraction</h3>
          </div>
        </header>
        <p>Since most memetic imagery possess modfications in the
        form of overlaid text, we can exploit them to produce
        stronger representations by a text extraction pipeline. For
        this, we first run Optical Character Recognition(OCR) to
        obtain the text (if any) contained in the image. Following
        this, we extract deep neural network features based on
        standard practices in the field of natural language
        processing using deep learning&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0040">40</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0058">58</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0060">60</a>].</p>
        <p><strong>Optical Character Recognition:</strong> We use
        the Tesseract&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0051">51</a>] package for OCR, and trim excess
        trailing text.</p>
        <section id="sec-15">
          <p><em>2.4.1 Word2Vec Pooling.</em> Here we extract
          word2vec&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0040">40</a>] representations (following
          &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0041">41</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0058">58</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0060">60</a>]) for each word present in the
          sentence, and to produce the final representation, we
          average over the individual word2vec representations. The
          word2vec implementation used is GenSim&nbsp;[<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0045">45</a>] with
          dimensionality 1000.</p>
        </section>
        <section id="sec-16">
          <p><em>2.4.2 Skip-Thought Vectors.</em> Kiros et
          al.&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0033">33</a>] introduce Skip-Thought Vectors,
          a generic encoding scheme for sentences. The immense
          versatility of skip-thought vectors on a wide variety of
          sentence classification tasks makes them a good choice
          for sentence embedding creation. To extract the
          skip-thought (ST) features, we simply supply the
          extracted text to the skip-thought model, and extract the
          penultimate layer features.</p>
          <p>If in the OCR phase, we do not find any text present
          in the image, we later replace the features with the mean
          text feature for that template across all images in the
          Target Set, in order to minimize the impact on nearest
          neighbor retrieval and classification. We provide
          ablation and comparative studies to assess the individual
          performance of each of the 2 abovementioned techniques in
          a variety of classification tasks, and find impressive
          results across the board, as summarized in the
          experiments section.</p>
          <figure id="fig4">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-fig4.jpg"
            class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span>
              <span class="figure-title">Evaluation of Sparse
              Matching on Clustering - variation of Silhouette
              Score (left), and Davies-Bouldin Index
              (right).</span>
            </div>
          </figure>
          <p></p>
          <p><strong>Algorithm Summary</strong>: After image
          correction and individual extraction of text and image
          features, as described in Figure &nbsp;<a class="fig"
          href="#fig3">3</a>, we obtain an informative and flexible
          feature representation, that preserves semantic content
          in the presence of overlaid text and/or imagery. We
          analyse the effectiveness of our obtained representation
          across a variety of experiments, as described in the next
          section.</p>
        </section>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Evaluation</h2>
        </div>
      </header>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span>
            Experimental Setup</h3>
          </div>
        </header>
        <p>To showcase the effectiveness of our representation for
        memetic imagery, we perform a variety of qualitative and
        quantitative assessments. For all experiments, we use a
        setup involving NVIDIA TITAN X GPUs, and our implementation
        is done using the PyTorch&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0043">43</a>] and
        TensorFlow&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>] frameworks. For each of the
        individual language and vision models, we use their
        publicly available weights and implementations (we use
        GenSim&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0045">45</a>] for Word2Vec).</p>
        <p>The Sparse Matching technique has two components - the
        image feature extractor (denoted as <em>I</em>), and the
        text feature extractor (denoted as <em>T</em>), giving us
        results of the form <strong>Sparse
        Matching</strong>(<em>I</em>, <em>T</em>) for that
        particular choice of feature extractors. For each
        experiment we conduct, we vary both the feature extractors
        to create different variants of the algorithm, which are
        all compared with (i) baseline feature extractors, (ii)
        previous state-of-the-art algorithms on our target tasks,
        (iii) each feature extractor individually and (iv) the
        naive combination of both feature extractors, providing an
        exhaustive set of comparative techniques. The CNN feature
        extractors we use are AlexNet&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>],
        VGGNet-16&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0049">49</a>], and ResNet-18&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0028">28</a>]. As our
        results depict, the more powerful image feature
        extractors(ResNets and VGGNet-16) provide consistently
        higher performance.</p>
        <p>The target datasets for our task are datasets pertaining
        to memetic imagery, typically scraped from websites such as
        Memegenerator, Reddit or Quickmeme. There has been interest
        from both the computer vision and computational social
        science communities in understanding such imagery, and we
        use the popular datasets used in these studies, with a
        total of 5 test splits used:</p>
        <p>(1) <strong>Viral Images Dataset&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>]</strong>: Introduced
        in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>], the Viral Images Dataset is a set
        of viral images filtered from the original dataset
        collected by Lakkaraju et al.&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0036">36</a>] from Reddit over a
        period of 4 years. The dataset contains 10,078 images along
        with a metric for virality from 20 different image
        categories. This dataset contains 3 testing splits of data
        - (i)<strong>Viral-Complete</strong>(VCom), which is a
        collection of randomly selected pairs of images,
        (ii)<strong>Viral-Pairs</strong>(VPair), which is a
        collection image pairs where an image from the top 250
        most-viral images is paired with an image from the top 250
        least viral images, and (iii)
        <strong>Viral-RandomPairs</strong>(VRPairs), which is a set
        of image pairs, where one image is sampled from the top 250
        most viral images, and the other is sampled at random. We
        utilise the predetermined training, validation and test
        splits.</p>
        <p>(2) <strong>Memegenerator Dataset &nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0017">17</a>]</strong>: This dataset
        is a collection of meme images scraped from the website
        Memegenerator, from a period of June 27th, 2013 to July 6,
        2013, accompanied with the number of upvotes for each meme
        as well. With a total of 326,181 images, this is the
        largest meme dataset, of which we use a random 70% for
        training, 10% images for validation and 20% for
        testing.</p>
        <p>(3) <strong>Quickmeme Dataset &nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0016">16</a>]</strong>: Similar to
        the Memegenerator Dataset, this dataset is scraped from the
        website Quickmeme from October 2012. This dataset has a
        total of 178,801 images, and here as well, we use an
        70-20-10 train-test-val split.</p>
        <p>The end product of our algorithm is a versatile and
        efficient feature representation which can be potentially
        useful for several inference tasks involving online
        imagery, especially those that are propagated through
        social media, such as memes. To evalaute the efficacy of
        our proposed algorithm, the most natural initial experiment
        would be the image clustering and retrieval. A common task
        online would be phrased as “given a certain test image
        <span class="inline-equation"><span class="tex">$\textbf
        {I}$</span></span> and image set <span class=
        "inline-equation"><span class="tex">$\mathcal {S}_{\mathbf
        {I}}$</span></span> , what are the images in <span class=
        "inline-equation"><span class="tex">$\mathcal {S}_{\mathbf
        {I}}$</span></span> that are most similar (in semantic
        content) to the test image <span class=
        "inline-equation"><span class="tex">$\textbf
        {I}$</span></span> ?”, and indeed, this is the first target
        task we design our algorithm for, and to evaluate our
        algorithm on this task, we look at three separate
        qualitative and quantitative experiments, as summarized
        below:</p>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Image
            Clustering and Retrieval</h3>
          </div>
        </header>
        <p><strong>Clustering Analysis</strong>: A summarizable
        version of the retrieval task is the task of image
        clustering - understanding how our feature representation
        embeds the space of images. Considering that these images
        form smaller subsets in semantic space, we would expect
        distinct clusters being formed in the embedding space.
        Since we do not necessarily have labels for classes in most
        of our datasets, to assess the quality of clustering we
        first compare the <em>internal clustering metrics</em> for
        our representation with the existing state-of-the-art and
        baseline methods. The Silhouette Score&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0044">44</a>](SS) and the
        Davies-Bouldin Index&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>](DBI) are two popular internal
        metrics used to quantifiably determine the quality of
        clustering of a representation. We proceed by clustering
        points from the Memegenerator Dataset&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0016">16</a>] and vary the number of
        desired clusters, to obtain the clustering indices for each
        feature extraction algorithm as functions of the number of
        clusters required. Here, we use the clustering algorithm
        K-means, with an off-the-shelf implementation in
        Scipy&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>].</p>
        <p>The desiderata for efficient clustering are separable
        clusters, with no significant overlap, and high similarity
        within clusters. Or, put succintly, high intra-cluster
        similarity, and low inter-cluster similarity. If we
        consider the Silhouette Score (SS), for clusters that are
        balanced in size and fairly separable, we desire a low mean
        value of SS across clusters, and a low deviation of SS
        across clusters as well - representing the average
        deviation of cluster items from the cluster mean.
        Figure&nbsp;<a class="fig" href="#fig4">4</a> a summarizes
        the results for SS analysis, which demonstrates the
        distinctive clustering our algorithm provides. Similarly,
        for the metric DBI (Figure&nbsp;<a class="fig" href=
        "#fig4">4</a> b), we consider the Euclidean distance as the
        distance metric, and observe consistently lower values,
        consolidating the efficiency of our algorithm in creating
        rich feature representations.</p>
        <p><strong>Image Retrieval and Visualization</strong>: The
        previous set of results provide a quantiative summary of
        the improved clustering provided by our algorithm, but do
        not make precise an intuition to validate our
        representation. To strengthen the qualitative intuition
        behind the performance of our algorithm, we visualize using
        a nearest neighbor retrieval.</p>
        <p>We provide a set of example image retrieval queries, and
        compare the retrieved images for the nearest neighbours of
        the query in feature space. As summarized in
        Figure&nbsp;<a class="fig" href="#fig5">5</a>, the sparse
        matching algorithm is robust to changes in text
        information, and by decoupling the text, base template and
        overlaid imagery, we see a richer embedding that preserves
        these changes in semantic space, whereas other algorithms
        simply treat an aberration or overlaid text as noise,
        providing poorer retrieval results. Just simply using image
        features retrieves visually similar images, yet, their text
        content is dissimilar. Using simply text features proceeds
        with the reverse effect, with various imagery being
        returned. Our method, however, returns a semantically
        aligned set of images.</p>
        <p>Observing the robustness of our representation and the
        qualitiative improvements in image retrieval, a natural
        extension would be understanding the performance of the
        feature representation on more complex tasks involving
        online imagery, such as inferring the popularity, content
        and timeline of an image. These quantitative evaluation
        tasks are motivated by several observations on online
        imagery and online content itself. It has long been
        established in social science literature that memetic
        content on the internet is ephemeral and rapidly
        evolving&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>], which makes the challenge of
        predicting the nature of content and the timeline of its
        existence an interesting problem. Building on this line of
        thought, we now describe the quantitative experiments we
        perform to assess our algorithm.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">On both tasks of topic and timeline
            prediction, we observe that Sparse Matching provides
            substantial improvements in performance.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">
                <strong>Algorithm</strong></th>
                <th colspan="2" style="text-align:center;">
                  <strong>Accuracy(%)</strong>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:center;">
                <strong>Topic</strong></th>
                <th style="text-align:center;">
                <strong>Timeline</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">
                  SVM + Image Features [<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0020">20</a>]
                </td>
                <td style="text-align:center;">41.09</td>
                <td style="text-align:center;">24.14</td>
              </tr>
              <tr>
                <td style="text-align:left;">SVM + 2x2 Dense HOG
                Features</td>
                <td style="text-align:center;">43.08</td>
                <td style="text-align:center;">25.18</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Finetuned AlexNet [<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0035">35</a>]
                </td>
                <td style="text-align:center;">53.89</td>
                <td style="text-align:center;">32.15</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Finetuned VGGNet16 [<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0049">49</a>]
                </td>
                <td style="text-align:center;">57.21</td>
                <td style="text-align:center;">33.02</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Finetuned ResNet18[<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0049">49</a>]
                </td>
                <td style="text-align:center;">58.17</td>
                <td style="text-align:center;">35.16</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  SVM + Word2VecPooling &nbsp;[<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0040">40</a>]
                </td>
                <td style="text-align:center;">55.19</td>
                <td style="text-align:center;">18.07</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  SVM + SkipThought Features &nbsp;[<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0033">33</a>]
                </td>
                <td style="text-align:center;">58.81</td>
                <td style="text-align:center;">19.15</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  SVM + ResNet18 + Word2VecPooling&nbsp;[<a class=
                  "bib" data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0040">40</a>]
                </td>
                <td style="text-align:center;">65.35</td>
                <td style="text-align:center;">32.15</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  SVM + ResNet18 + SkipThought
                  Features&nbsp;[<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0033">33</a>]
                </td>
                <td style="text-align:center;">69.18</td>
                <td style="text-align:center;">34.06</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Xiao and Lee [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0057">57</a>]
                </td>
                <td style="text-align:center;">70.57</td>
                <td style="text-align:center;">38.18</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Singh and Lee [<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0050">50</a>]
                </td>
                <td style="text-align:center;">71.85</td>
                <td style="text-align:center;">37.09</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Dubey and Agarwal [<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0024">24</a>]
                </td>
                <td style="text-align:center;">75.57</td>
                <td style="text-align:center;">40.17</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (AlexNet, Word2VecPooling)</td>
                <td style="text-align:center;">72.18</td>
                <td style="text-align:center;">37.15</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (VGGNet16, Word2VecPooling)</td>
                <td style="text-align:center;">76.08</td>
                <td style="text-align:center;">39.57</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (ResNet18, Word2VecPooling)</td>
                <td style="text-align:center;">77.85</td>
                <td style="text-align:center;">40.88</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (AlexNet, SkipThought)</td>
                <td style="text-align:center;">74.23</td>
                <td style="text-align:center;">43.44</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (VGGNet16, SkipThought)</td>
                <td style="text-align:center;">78.98</td>
                <td style="text-align:center;">45.78</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (ResNet18, SkipThought)</td>
                <td style="text-align:center;">
                <strong>80.69</strong></td>
                <td style="text-align:center;">
                <strong>46.91</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Quantitative Evaluation</h3>
          </div>
        </header>
        <p>We perform a series of varied inference tasks to assess
        the quality of representations generated by our algorithm.
        The general framework for evaluation in these experiments
        is as follows - (i) we first select a CNN-based image
        feature extractor from the set of CNNs described earlier,
        and select one of two text feature extractors, (ii) the CNN
        feature extractor is fine-tuned naively on the target task
        using the assigned training set, and (iii) we extract both
        image features and text features from the designated neural
        networks, following the Sparse Matching algorithm. Finally,
        once the features are obtained, we train a multiclass SVM
        classifier&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0029">29</a>] for each of the following
        inference tasks. If the task is a ranking task (as Virality
        Prediction), we train a RankSVM&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0030">30</a>] instead.</p>
        <p><strong>Topic Prediction</strong>: The Viral Images
        Dataset&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>] is constructed of images scraped
        from Reddit&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>], along with the subreddits these
        images were posted to. The subreddits are curated lists
        belonging to a particular genre or topic of content, and
        example subreddits are <tt>/r/funny, /r/aww, /r/wtf,
        /r/gaming</tt> and <tt>/r/atheism</tt>, and the resulting
        inference task is that of predicting the correct topic or
        subreddit the image was posted to.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Comparison of performance on the task of
            virality prediction. We observe that with our feature
            representation performance is unmatched compared to the
            existing state-of-the-art, by a significant
            margin.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">
                <strong>Algorithm</strong></th>
                <th colspan="5" style="text-align:center;">
                  <strong>Percentage Accuracy on Dataset</strong>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:center;">
                  VCom[<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0020">20</a>]
                </th>
                <th style="text-align:center;">
                  VPairs[<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0020">20</a>]
                </th>
                <th style="text-align:center;">
                  VRPairs[<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0020">20</a>]
                </th>
                <th style="text-align:center;">
                  MemeGenerator[<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0016">16</a>]
                </th>
                <th style="text-align:center;">
                  QuickMeme[<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0017">17</a>]
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">
                  RankSVM + Image Features [<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0020">20</a>]
                </td>
                <td style="text-align:center;">53.40</td>
                <td style="text-align:center;">61.60</td>
                <td style="text-align:center;">58.49</td>
                <td style="text-align:center;">59.12</td>
                <td style="text-align:center;">57.05</td>
              </tr>
              <tr>
                <td style="text-align:left;">RankSVM + 2x2 Dense
                HOG Features</td>
                <td style="text-align:center;">52.75</td>
                <td style="text-align:center;">58.81</td>
                <td style="text-align:center;">56.92</td>
                <td style="text-align:center;">58.87</td>
                <td style="text-align:center;">55.56</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  RankSVM + AlexNet fc7 Features [<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0035">35</a>]
                </td>
                <td style="text-align:center;">54.41</td>
                <td style="text-align:center;">61.93</td>
                <td style="text-align:center;">58.58</td>
                <td style="text-align:center;">59.91</td>
                <td style="text-align:center;">58.03</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  RankSVM + VGGNet-16 fc7 Features [<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0049">49</a>]
                </td>
                <td style="text-align:center;">55.18</td>
                <td style="text-align:center;">63.01</td>
                <td style="text-align:center;">59.15</td>
                <td style="text-align:center;">60.12</td>
                <td style="text-align:center;">61.12</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  RankSVM + Word2VecPooling&nbsp;[<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0040">40</a>]
                </td>
                <td style="text-align:center;">60.11</td>
                <td style="text-align:center;">61.78</td>
                <td style="text-align:center;">59.94</td>
                <td style="text-align:center;">61.49</td>
                <td style="text-align:center;">62.02</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  RankSVM + SkipThought&nbsp;[<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0033">33</a>]
                </td>
                <td style="text-align:center;">63.06</td>
                <td style="text-align:center;">64.12</td>
                <td style="text-align:center;">60.23</td>
                <td style="text-align:center;">65.57</td>
                <td style="text-align:center;">64.28</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  RankSVM + ResNet18 +
                  Word2VecPooling&nbsp;[<a class="bib"
                  data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0040">40</a>]
                </td>
                <td style="text-align:center;">66.05</td>
                <td style="text-align:center;">70.98</td>
                <td style="text-align:center;">70.33</td>
                <td style="text-align:center;">71.06</td>
                <td style="text-align:center;">69.45</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  RankSVM + ResNet18 + SkipThought&nbsp;[<a class=
                  "bib" data-trigger="hover" data-toggle="popover"
                  data-placement="top" href=
                  "#BibPLXBIB0033">33</a>]
                </td>
                <td style="text-align:center;">69.36</td>
                <td style="text-align:center;">74.51</td>
                <td style="text-align:center;">72.09</td>
                <td style="text-align:center;">75.53</td>
                <td style="text-align:center;">73.81</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Xiao and Lee [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0057">57</a>]
                </td>
                <td style="text-align:center;">63.25</td>
                <td style="text-align:center;">75.23</td>
                <td style="text-align:center;">73.22</td>
                <td style="text-align:center;">70.11</td>
                <td style="text-align:center;">71.21</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Singh and Lee [<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0050">50</a>]
                </td>
                <td style="text-align:center;">65.87</td>
                <td style="text-align:center;">76.20</td>
                <td style="text-align:center;">74.38</td>
                <td style="text-align:center;">72.25</td>
                <td style="text-align:center;">70.08</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  Dubey and Agarwal [<a class="bib" data-trigger=
                  "hover" data-toggle="popover" data-placement=
                  "top" href="#BibPLXBIB0024">24</a>]
                </td>
                <td style="text-align:center;">68.09</td>
                <td style="text-align:center;">78.38</td>
                <td style="text-align:center;">76.95</td>
                <td style="text-align:center;">74.43</td>
                <td style="text-align:center;">74.51</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (AlexNet, Word2VecPooling)</td>
                <td style="text-align:center;">70.02</td>
                <td style="text-align:center;">82.21</td>
                <td style="text-align:center;">80.04</td>
                <td style="text-align:center;">79.53</td>
                <td style="text-align:center;">79.95</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (VGGNet16, Word2VecPooling)</td>
                <td style="text-align:center;">70.93</td>
                <td style="text-align:center;">83.03</td>
                <td style="text-align:center;">81.15</td>
                <td style="text-align:center;">79.94</td>
                <td style="text-align:center;">80.22</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (ResNet18, Word2VecPooling)</td>
                <td style="text-align:center;">71.87</td>
                <td style="text-align:center;">84.19</td>
                <td style="text-align:center;">81.96</td>
                <td style="text-align:center;">80.01</td>
                <td style="text-align:center;">80.27</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (AlexNet, SkipThought)</td>
                <td style="text-align:center;">70.86</td>
                <td style="text-align:center;">83.05</td>
                <td style="text-align:center;">81.29</td>
                <td style="text-align:center;">80.25</td>
                <td style="text-align:center;">80.16</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (VGGNet, SkipThought)</td>
                <td style="text-align:center;">71.54</td>
                <td style="text-align:center;">83.98</td>
                <td style="text-align:center;">82.34</td>
                <td style="text-align:center;">81.16</td>
                <td style="text-align:center;">80.87</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Sparse
                Matching</strong> (ResNet18, SkipThought)</td>
                <td style="text-align:center;">
                <strong>73.03</strong></td>
                <td style="text-align:center;">
                <strong>85.15</strong></td>
                <td style="text-align:center;">
                <strong>82.62</strong></td>
                <td style="text-align:center;">
                <strong>81.80</strong></td>
                <td style="text-align:center;">
                <strong>80.9</strong>1</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The task is challenging primarily owing to the overlap
        in imagery across categories, and the overlap in context as
        well. For example, several categories may contain images of
        different animals or people, as well as a significant
        overlap in the template images that might be used. We
        compare our performance on this task with several benchmark
        and preivous state-of-the-art methods on this task,
        summarized in Table&nbsp;<a class="tbl" href="#tab1">1</a>.
        Since there are 20 different categories, random chance
        performs at 5%. Due to the immense overlap in content,
        naive algorithms perform poorly, with less than 50%
        prediction accuracy.</p>
        <p>By just considering the text content itself, we see a
        slight increase in performance, which increases further
        still when both feature modalities are combined (image CNN
        is fine-tuned). However, as hypothesized, these features
        are corrupted by the coupling of text and image content,
        and hence perform averagely. Approaches that take context
        into account, such as the work of Singh and
        Lee&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0050">50</a>], and Dubey and
        Agarwal&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>] perform much better, however, they
        face challenges in decoupling text information. Finally, we
        see that Sparse Matching performs substantially better,
        with a maximum gain (with ResNet18 visual features and Skip
        Thought language features), of 5.12% in performance.</p>
        <p><strong>Timeline Prediction</strong>: The results of the
        preivous experiment provide us with evidence regarding the
        representational power of our method. An extension of the
        first experiment would be to predict, along with the topic
        of the content, the time period it was posted in.
        Applications of such a task are present in content
        creation, and prediction of popularity trends in social
        media. Since content on viral image websites and memes are
        continually evolving, we see that the content alone can be
        a strong indicator of the time period it represents, as
        displayed by our prediction results in Column 2 of
        Table&nbsp;<a class="tbl" href="#tab1">1</a>.</p>
        <p>We observe that naive baselines for both basic image
        features and text features fail miserably at this task,
        capturing very little information about the context of an
        image, crucial for a task like timeline prediction. Deep
        image features perform slightly better, and a combination
        of deep image features and text features provides good
        performance, which is ousted by previous virality
        prediction techniques, and Sparse Matching. The decoupled
        context provides a signficant boost over the previous
        state-of-the-art, with an improvement of <span class=
        "inline-equation"><span class="tex">${\bf
        6.74\%}$</span></span> .</p>
        <p><strong>Virality Prediction</strong>: With the
        impressive increase in performance on timeline prediction,
        we have evidence supporting the improved ability of our
        method to identify temporal distinctions in memetic
        imagery. This demonstration leads us to our final
        quantitative experiment, virality prediction. As summarized
        by recent work on detection of viral content and cascades
        in social networks&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>], capturing virality is a difficult
        problem, with challenges arising from the presence
        short-lived content, rapidly evolving diverse imagery. With
        our model, however, we hypothesize that predicting virality
        would be possible with greater efficiency owing to the
        stronger, robust representation.</p>
        <p>Hence, we evaluate the prediction performance on the
        primary inference task of virality prediction, following
        the methodology introduced in&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0024">24</a>]. Here, the prediction
        of virality is taken to be a pairwise classification task,
        where given an input pair of images, the algorithm needs to
        predict the more viral image of the two. This evaluation
        scheme has been shown to be robust to network
        effects&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>], and is representative of
        evaluation of the image virality based on the content
        alone. We follow the same procedure, replacing our SVM
        classifier with a Ranking SVM&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0030">30</a>] classifier to learn a
        function that provides pairwise predictions. We evaluate
        our algorithm on 5 different datasets - the 3 test splits
        of the Viral Images Dataset&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>] - (i) Viral-Complete,
        (ii) Viral Pairs, and (iii) Viral-RandomPairs, and the
        Memegenerator&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>] and Quickmeme
        datasets&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0017">17</a>]. For all experiments where a
        train-test split is not predefined, we use a 70-20-10
        (train-test-val) split, following standard protocol.</p>
        <p>Our experiment is summarized in Table&nbsp;<a class=
        "tbl" href="#tab2">2</a>. Note here that since this is a
        pairwise prediction task, random chance performs at 50%.
        Again, as seen in the previous experiments, naive
        image-based algorithms perform only slightly better than
        random chance, with accuracies around 50-60%. Operating
        only on extracted text data gives us slightly better
        performance, but it fails to generalize since a lot of
        imagery do not possess any relevant text at all, which
        provides the algorithms with only noise. Sparse Matching
        provides large improvements, as high as
        <strong>7.37%</strong> in some cases. Even the weakest
        versions of Sparse Matching perform <strong>3.8%</strong>
        better than the previous state-of-the-art.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">4-Nearest neighbour image
            retrieval results for two sample queries.</span>
          </div>
        </figure>
        <p></p>
        <p>Finally, we attempt to analyse the durability of our
        method in virality prediction. Since the nature of viral
        content changes rapidly over time, we would want to learn a
        classifier that can perform well for longer periods of
        time, given an input training set taken at any prior time.
        We would not want to keep re-training our algorithm often
        as new data keeps coming in, and reduce the time spent
        updating our model. To ascertain the invariance of methods
        in such a manner, we devise a final quantitative
        experiment.</p>
        <p><strong>Temporal Robustness Analysis</strong>: To
        examine the resilience of our algorithm to evolving data,
        we devise the following experiment: The Memegenerator
        dataset&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>] contains images from 13 different
        time periods, as described earlier. We select data from the
        <em>i<sup>th</sup></em> time period as our training data,
        and test the trained algorithm on the <em>k</em> −
        <em>i</em> sets of test images from subsequent time
        periods. We perform <span class=
        "inline-equation"><span class="tex">$\sum _{i=1}^{k-1}
        (k-i) = \frac{k(k-1)}{2}$</span></span> total experiments,
        which in our case is 78. We report the average obtained
        performance for each algorithm, summarized in
        Table&nbsp;<a class="tbl" href="#tab3">3</a>.</p>
        <p>It is vital to understand the difficulty of this task
        compared to the previous virality prediction experiment.
        For each time period, we have (on average) only 7% of the
        previously available training data, which provides a very
        sparse signal. This is consistent with the observation that
        most methods (including naive deep learning algorithms)
        perform very close to random chance, given the lower
        correlation between the observed training data and the test
        data. We still observe, however, that specialized methods
        perform much better than random chance, and Sparse Matching
        provides the best performance, showcasing again the
        richness and versatility of the method in capturing
        context.</p>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Discussion and
          Future Work</h2>
        </div>
      </header>
      <p>In this work, we introduced the task of understanding the
      creation, mutation and propagation of image macros online as
      a representation for the growth and evolution of a meme. We
      introduced a method that succesfully maps any arbitrary image
      macro to the template it was created from, and then embeds it
      into a powerful semantic space. We demonstrated the
      robustness and richness of the semantic embedding space via a
      variety of relevant experiments on online imagery - image
      retrieval, clustering, content analysis, popularity analysis
      and temporal evolution, all with remarkable performance.</p>
      <div class="table-responsive" id="tab3">
        <div class="table-caption">
          <span class="table-number">Table 3:</span> <span class=
          "table-title">Evaluation of Sparse Matching on the
          Temporal Robustness task.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;">
              <strong>Algorithm</strong></th>
              <th style="text-align:center;">
              <strong>Accuracy(%)</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;">
                SVM + Image Features [<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0020">20</a>]
              </td>
              <td style="text-align:center;">51.02</td>
            </tr>
            <tr>
              <td style="text-align:left;">SVM + 2x2 Dense HOG
              Features</td>
              <td style="text-align:center;">51.18</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                Finetuned AlexNet [<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0035">35</a>]
              </td>
              <td style="text-align:center;">51.75</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                Finetuned VGGNet16 [<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0049">49</a>]
              </td>
              <td style="text-align:center;">51.56</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                Finetuned ResNet18 [<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0049">49</a>]
              </td>
              <td style="text-align:center;">51.97</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                SVM + Word2VecPooling &nbsp;[<a class="bib"
                data-trigger="hover" data-toggle="popover"
                data-placement="top" href="#BibPLXBIB0040">40</a>]
              </td>
              <td style="text-align:center;">52.02</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                SVM + SkipThought Features &nbsp;[<a class="bib"
                data-trigger="hover" data-toggle="popover"
                data-placement="top" href="#BibPLXBIB0033">33</a>]
              </td>
              <td style="text-align:center;">52.95</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                SVM + Finetuned ResNet18 +
                Word2VecPooling&nbsp;[<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0040">40</a>]
              </td>
              <td style="text-align:center;">55.68</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                SVM + Finetuned ResNet18 + SkipThought
                Features&nbsp;[<a class="bib" data-trigger="hover"
                data-toggle="popover" data-placement="top" href=
                "#BibPLXBIB0033">33</a>]
              </td>
              <td style="text-align:center;">56.35</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                Xiao and Lee [<a class="bib" data-trigger="hover"
                data-toggle="popover" data-placement="top" href=
                "#BibPLXBIB0057">57</a>]
              </td>
              <td style="text-align:center;">62.67</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                Singh and Lee [<a class="bib" data-trigger="hover"
                data-toggle="popover" data-placement="top" href=
                "#BibPLXBIB0050">50</a>]
              </td>
              <td style="text-align:center;">61.98</td>
            </tr>
            <tr>
              <td style="text-align:left;">
                Dubey and Agarwal [<a class="bib" data-trigger=
                "hover" data-toggle="popover" data-placement="top"
                href="#BibPLXBIB0024">24</a>]
              </td>
              <td style="text-align:center;">62.83</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Sparse
              Matching</strong> (AlexNet, Word2VecPooling)</td>
              <td style="text-align:center;">61.78</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Sparse
              Matching</strong> (VGGNet16, Word2VecPooling)</td>
              <td style="text-align:center;">61.97</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Sparse
              Matching</strong> (ResNet18, Word2VecPooling)</td>
              <td style="text-align:center;">63.49</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Sparse
              Matching</strong> (AlexNet, SkipThought)</td>
              <td style="text-align:center;">64.07</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Sparse
              Matching</strong> (VGGNet16, SkipThought)</td>
              <td style="text-align:center;">64.18</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Sparse
              Matching</strong> (ResNet18, SkipThought)</td>
              <td style="text-align:center;">
              <strong>65.12</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>Our work is the first of its kind in the domain of web
      content analysis that looks at the virality prediction
      problem through the lens of the Image Macro structure, and
      successfully exploits it. While prior work in the domain of
      virality prediction from content has focused on the
      originality of memes&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>], popularity-promoting regions within
      meme images&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>], relative attributes and category
      analysis&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>], our approach provides a technique
      to systematically study the evolution of ideas and memes
      online.</p>
      <p>As a demonstration of this technique, we provide a sample
      phylogenetic tree constructed from the pairwise distances
      between different image macros, obtained using our algorithm
      (see Figure&nbsp;<a class="fig" href="#fig6">6</a>). This
      tree displays the mutation of memes from a popular template,
      created using MATLAB's <tt>seqlinkage</tt> command (the
      figure displays selected nodes). We see that our embedding is
      powerful enough to capture subtle evolutionary patterns in
      the mutation of the meme, with children being semantically
      linked either through imagery (note the subtree with the hat
      overlay), or through text (subtree with pokemon text). With
      large amounts of time-series meme imagery, our embedding
      technique paves way for the study of systematic evolution of
      ideas on the Internet.</p>
      <figure id="fig6">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186021/images/www2018-30-fig6.jpg"
        class="img-responsive" alt="Figure 6" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 6:</span> <span class=
          "figure-title">Pruned phylogenetic tree constructed from
          1 meme template and its neighbours using distances
          generated by Sparse Matching.</span>
        </div>
      </figure>
      <p></p>
      <p>All previous efforts in the study of image virality have
      been futile at extracting these subtle variations in memes,
      and creating evolutionary trees such as the one displayed,
      despite having success at the task of predicting
      virality&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>]. Our contribution through this work
      enables the large-scale study and analysis of such
      evolutionary trends online, with numerous applications in
      combining the effects of content and network structure in
      understanding information diffusion and evolution. Such a
      study can be influential in creating methods for identifying
      original content and their sources, and creating a robust
      science for understanding how multimedia propagates on the
      Internet.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">[n. d.]. reddit: the
        front page of the internet. <a href=
        "http://www.reddit.com/" target=
        "_blank">http://www.reddit.com/</a>. ([n. d.]). Accessed:
        2017-09-30.
        </li>
        <li id="BibPLXBIB0002" label="[2]">Martín Abadi, Paul
        Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
        Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
        Michael Isard, <em>et al.</em> [n. d.]. TensorFlow: A
        System for Large-Scale Machine Learning.</li>
        <li id="BibPLXBIB0003" label="[3]">Lada&nbsp;A Adamic,
        Thomas&nbsp;M Lento, Eytan Adar, and Pauline&nbsp;C Ng.
        2016. Information evolution in social networks. In
        <em><em>Proceedings of the Ninth ACM International
        Conference on Web Search and Data Mining</em></em> . ACM,
        473–482.</li>
        <li id="BibPLXBIB0004" label="[4]">Edoardo Amaldi and Viggo
        Kann. 1998. On the approximability of minimizing nonzero
        variables or unsatisfied relations in linear systems.
        <em><em>Theoretical Computer Science</em></em> 209, 1-2
        (1998), 237–260.</li>
        <li id="BibPLXBIB0005" label="[5]">Eytan Bakshy, Itamar
        Rosenn, Cameron Marlow, and Lada Adamic. 2012. The role of
        social networks in information diffusion. In
        <em><em>Proceedings of the 21st international conference on
        World Wide Web</em></em> . ACM, 519–528.</li>
        <li id="BibPLXBIB0006" label="[6]">Albert-László Barabási.
        2016. <em><em>Network science</em></em> . Cambridge
        university press.</li>
        <li id="BibPLXBIB0007" label="[7]">Jonah Berger and
        Katherine&nbsp;L Milkman. 2012. What makes online content
        viral? <em><em>Journal of marketing research</em></em> 49,
        2 (2012), 192–205.</li>
        <li id="BibPLXBIB0008" label="[8]">Jonah Berger and
        Eric&nbsp;M Schwartz. 2011. What drives immediate and
        ongoing word of mouth? <em><em>Journal of Marketing
        Research</em></em> 48, 5 (2011), 869–880.</li>
        <li id="BibPLXBIB0009" label="[9]">Vincent Buskens and
        Jeroen Weesie. 2000. Cooperation via social networks.
        <em><em>Analyse &amp; Kritik</em></em> 22, 1 (2000),
        44–74.</li>
        <li id="BibPLXBIB0010" label="[10]">Vincent Buskens and
        Kazuo Yamaguchi. 1999. A new model for information
        diffusion in heterogeneous social networks.
        <em><em>Sociological methodology</em></em> 29, 1 (1999),
        281–325.</li>
        <li id="BibPLXBIB0011" label="[11]">Emmanuel&nbsp;J Candes,
        Justin&nbsp;K Romberg, and Terence Tao. 2006. Stable signal
        recovery from incomplete and inaccurate measurements.
        <em><em>Communications on pure and applied
        mathematics</em></em> 59, 8(2006), 1207–1223.</li>
        <li id="BibPLXBIB0012" label="[12]">Scott&nbsp;Shaobing
        Chen, David&nbsp;L Donoho, and Michael&nbsp;A Saunders.
        2001. Atomic decomposition by basis pursuit. <em><em>SIAM
        review</em></em> 43, 1 (2001), 129–159.</li>
        <li id="BibPLXBIB0013" label="[13]">Justin Cheng, Lada
        Adamic, P&nbsp;Alex Dow, Jon&nbsp;Michael Kleinberg, and
        Jure Leskovec. 2014. Can cascades be predicted?. In
        <em><em>Proceedings of the 23rd international conference on
        World wide web</em></em> . ACM, 925–936.</li>
        <li id="BibPLXBIB0014" label="[14]">Justin Cheng,
        Lada&nbsp;A Adamic, Jon&nbsp;M Kleinberg, and Jure
        Leskovec. 2016. Do Cascades Recur?. In <em><em>Proceedings
        of the 25th International Conference on World Wide
        Web</em></em> . International World Wide Web Conferences
        Steering Committee, 671–681.</li>
        <li id="BibPLXBIB0015" label="[15]">Ronan Collobert and
        Jason Weston. 2008. A unified architecture for natural
        language processing: Deep neural networks with multitask
        learning. In <em><em>Proceedings of the 25th international
        conference on Machine learning</em></em> . ACM,
        160–167.</li>
        <li id="BibPLXBIB0016" label="[16]">Michele Coscia. 2013.
        Competition and Success in the Meme Pool: A Case Study on
        Quickmeme. com.</li>
        <li id="BibPLXBIB0017" label="[17]">Michele Coscia. 2014.
        Average is boring: How similarity kills a meme's success.
        <em><em>Scientific reports</em></em> 4(2014), 6477.</li>
        <li id="BibPLXBIB0018" label="[18]">David&nbsp;L Davies and
        Donald&nbsp;W Bouldin. 1979. A cluster separation measure.
        <em><em>IEEE transactions on pattern analysis and machine
        intelligence</em></em> 2 (1979), 224–227.</li>
        <li id="BibPLXBIB0019" label="[19]">Jia Deng, Wei Dong,
        Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.
        Imagenet: A large-scale hierarchical image database. In
        <em><em>Computer Vision and Pattern Recognition, 2009. CVPR
        2009. IEEE Conference on</em></em> . IEEE, 248–255.</li>
        <li id="BibPLXBIB0020" label="[20]">Arturo Deza and Devi
        Parikh. 2015. Understanding image virality. In
        <em><em>Proceedings of the IEEE Conference on Computer
        Vision and Pattern Recognition</em></em> . 1818–1826.</li>
        <li id="BibPLXBIB0021" label="[21]">Jeff Donahue, Yangqing
        Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng,
        and Trevor Darrell. 2014. Decaf: A deep convolutional
        activation feature for generic visual recognition. In
        <em><em>International conference on machine
        learning</em></em> . 647–655.</li>
        <li id="BibPLXBIB0022" label="[22]">David&nbsp;L Donoho.
        2006. For most large underdetermined systems of linear
        equations the minimal ℓ<sub>1</sub>-norm solution is also
        the sparsest solution. <em><em>Communications on pure and
        applied mathematics</em></em> 59, 6(2006), 797–829.</li>
        <li id="BibPLXBIB0023" label="[23]">Cícero&nbsp;Nogueira
        Dos&nbsp;Santos and Maira Gatti. 2014. Deep Convolutional
        Neural Networks for Sentiment Analysis of Short Texts.. In
        <em><em>COLING</em></em> . 69–78.</li>
        <li id="BibPLXBIB0024" label="[24]">Abhimanyu Dubey and
        Sumeet Agarwal. 2017. Modeling Image Virality with Pairwise
        Spatial Transformer Networks. In <em><em>Proceedings of the
        2017 ACM on Multimedia Conference, MM 2017, Mountain View,
        CA, USA, October 23-27, 2017</em></em> . 663–671.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1145/3123266.3123333" target=
          "_blank">https://doi.org/10.1145/3123266.3123333</a>
        </li>
        <li id="BibPLXBIB0025" label="[25]">Phil Edwards. [n. d.].
        The reason every meme uses that one font. <a href=
        "https://www.vox.com/2015/7/26/9036993/meme-font-impact/"
        target=
        "_blank">https://www.vox.com/2015/7/26/9036993/meme-font-impact/</a>.
        ([n. d.]).Accessed: 2017-09-30.
        </li>
        <li id="BibPLXBIB0026" label="[26]">James&nbsp;P Gleeson,
        Kevin&nbsp;P O'Sullivan, Raquel&nbsp;A Baños, and Yamir
        Moreno. 2016. Effects of network structure, competition and
        memory time on social spreading phenomena. <em><em>Physical
        Review X</em></em> 6, 2 (2016), 021019.</li>
        <li id="BibPLXBIB0027" label="[27]">Alex Graves and Jurgen
        Schmidhuber. 2005. Framewise phoneme classification with
        bidirectional LSTM and other neural network architectures.
        <em><em>Neural Networks</em></em> 18, 5 (2005),
        602–610.</li>
        <li id="BibPLXBIB0028" label="[28]">Kaiming He, Xiangyu
        Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
        learning for image recognition. In <em><em>Proceedings of
        the IEEE conference on computer vision and pattern
        recognition</em></em> . 770–778.</li>
        <li id="BibPLXBIB0029" label="[29]">Chih-Wei Hsu and
        Chih-Jen Lin. 2002. A comparison of methods for multiclass
        support vector machines. <em><em>IEEE transactions on
        Neural Networks</em></em> 13, 2 (2002), 415–425.</li>
        <li id="BibPLXBIB0030" label="[30]">Thorsten Joachims.
        2002. Optimizing search engines using clickthrough data. In
        <em><em>Proceedings of the eighth ACM SIGKDD international
        conference on Knowledge discovery and data mining</em></em>
        . ACM, 133–142.</li>
        <li id="BibPLXBIB0031" label="[31]">Eric Jones, Travis
        Oliphant, Pearu Peterson, <em>et al.</em> 2001–. SciPy:
        Open source scientific tools for Python. (2001–).
          <a href="http://www.scipy.org/" target=
          "_blank">http://www.scipy.org/</a>[Online; accessed
          9/30/2017].
        </li>
        <li id="BibPLXBIB0032" label="[32]">Aditya Khosla, Atish
        Das&nbsp;Sarma, and Raffay Hamid. 2014. What makes an image
        popular?. In <em><em>Proceedings of the 23rd international
        conference on World wide web</em></em> . ACM, 867–876.</li>
        <li id="BibPLXBIB0033" label="[33]">Ryan Kiros, Yukun Zhu,
        Ruslan&nbsp;R Salakhutdinov, Richard Zemel, Raquel Urtasun,
        Antonio Torralba, and Sanja Fidler. 2015. Skip-thought
        vectors. In <em><em>Advances in neural information
        processing systems</em></em> . 3294–3302.</li>
        <li id="BibPLXBIB0034" label="[34]">Michele Knobel and
        Colin Lankshear. [n. d.]. Online memes, affinities, and
        cultural production. ([n. d.]).</li>
        <li id="BibPLXBIB0035" label="[35]">Alex Krizhevsky, Ilya
        Sutskever, and Geoffrey&nbsp;E Hinton. 2012. Imagenet
        classification with deep convolutional neural networks. In
        <em><em>Advances in neural information processing
        systems</em></em> . 1097–1105.</li>
        <li id="BibPLXBIB0036" label="[36]">Himabindu Lakkaraju,
        Julian&nbsp;J McAuley, and Jure Leskovec. 2013. What's in a
        Name? Understanding the Interplay between Titles, Content,
        and Communities in Social Media.(2013).</li>
        <li id="BibPLXBIB0037" label="[37]">Jure Leskovec, Lars
        Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the
        dynamics of the news cycle. In <em><em>Proceedings of the
        15th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em> . ACM, 497–506.</li>
        <li id="BibPLXBIB0038" label="[38]">M&nbsp;Douglas McIlroy.
        1960. Macro instruction extensions of compiler languages.
        <em><em>Commun. ACM</em></em> 3, 4 (1960), 214–220.</li>
        <li id="BibPLXBIB0039" label="[39]">LR Medsker and LC Jain.
        2001. Recurrent neural networks. <em><em>Design and
        Applications</em></em> 5 (2001).</li>
        <li id="BibPLXBIB0040" label="[40]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Greg&nbsp;S Corrado, and Jeff Dean.
        2013. Distributed representations of words and phrases and
        their compositionality. In <em><em>Advances in neural
        information processing systems</em></em> . 3111–3119.</li>
        <li id="BibPLXBIB0041" label="[41]">Nasrin Mostafazadeh,
        Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
        Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A
        corpus and evaluation framework for deeper understanding of
        commonsense stories. <em><em>arXiv preprint
        arXiv:1604.01696</em></em> (2016).</li>
        <li id="BibPLXBIB0042" label="[42]">Ruth Page. 2012. The
        linguistics of self-branding and micro-celebrity in
        Twitter: The role of hashtags. <em><em>Discourse &amp;
        communication</em></em> 6, 2 (2012), 181–201.</li>
        <li id="BibPLXBIB0043" label="[43]">Adam Paszke, Sam Gross,
        and Soumith Chintala. 2017. PyTorch. (2017).</li>
        <li id="BibPLXBIB0044" label="[44]">Slobodan Petrovic.
        2006. A comparison between the silhouette index and the
        davies-bouldin index in labelling ids clusters. In
        <em><em>Proceedings of the 11th Nordic Workshop of Secure
        IT Systems</em></em> . 53–64.</li>
        <li id="BibPLXBIB0045" label="[45]">R Rehurek and P Sojka.
        2011. Gensim–python framework for vector space modelling.
        <em><em>NLP Centre, Faculty of Informatics, Masaryk
        University, Brno, Czech Republic</em></em> 3, 2(2011).</li>
        <li id="BibPLXBIB0046" label="[46]">Daniel&nbsp;M Romero,
        Brendan Meeder, and Jon Kleinberg. 2011. Differences in the
        mechanics of information diffusion across topics: idioms,
        political hashtags, and complex contagion on twitter. In
        <em><em>Proceedings of the 20th international conference on
        World wide web</em></em> . ACM, 695–704.</li>
        <li id="BibPLXBIB0047" label="[47]">Kazumi Saito, Ryohei
        Nakano, and Masahiro Kimura. 2008. Prediction of
        information diffusion probabilities for independent cascade
        model. In <em><em>Knowledge-based intelligent information
        and engineering systems</em></em> . Springer, 67–75.</li>
        <li id="BibPLXBIB0048" label="[48]">Limor Shifman. 2013.
        Memes in a digital world: Reconciling with a conceptual
        troublemaker. <em><em>Journal of Computer-Mediated
        Communication</em></em> 18, 3 (2013), 362–377.</li>
        <li id="BibPLXBIB0049" label="[49]">Karen Simonyan and
        Andrew Zisserman. 2014. Very deep convolutional networks
        for large-scale image recognition. <em><em>arXiv preprint
        arXiv:1409.1556</em></em> (2014).</li>
        <li id="BibPLXBIB0050" label="[50]">Krishna&nbsp;Kumar
        Singh and Yong&nbsp;Jae Lee. 2016. End-to-end localization
        and ranking for relative attributes. In <em><em>European
        Conference on Computer Vision</em></em> . Springer,
        753–769.</li>
        <li id="BibPLXBIB0051" label="[51]">Ray Smith. 2007. An
        overview of the Tesseract OCR engine. In <em><em>Document
        Analysis and Recognition, 2007. ICDAR 2007. Ninth
        International Conference on</em></em> , Vol.&nbsp;2. IEEE,
        629–633.</li>
        <li id="BibPLXBIB0052" label="[52]">Thomas&nbsp;W Valente.
        1995. Network models of the diffusion of
        innovations.(1995).</li>
        <li id="BibPLXBIB0053" label="[53]">Lilian Weng and Filippo
        Menczer. 2015. Topicality and impact in social media:
        diverse messages, focused messengers. <em><em>PloS
        one</em></em> 10, 2 (2015), e0118410.</li>
        <li id="BibPLXBIB0054" label="[54]">Lilian Weng, Filipspo
        Menczer, and Yong-Yeol Ahn. 2013. Virality prediction and
        community structure in social networks. <em><em>Scientific
        reports</em></em> 3(2013), 2522.</li>
        <li id="BibPLXBIB0055" label="[55]">Lilian Weng, Filippo
        Menczer, and Yong-Yeol Ahn. 2014. Predicting Successful
        Memes Using Network and Community Structure.. In
        <em><em>ICWSM</em></em> .</li>
        <li id="BibPLXBIB0056" label="[56]">John Wright,
        Allen&nbsp;Y Yang, Arvind Ganesh, S&nbsp;Shankar Sastry,
        and Yi Ma. 2009. Robust face recognition via sparse
        representation. <em><em>IEEE transactions on pattern
        analysis and machine intelligence</em></em> 31, 2(2009),
        210–227.</li>
        <li id="BibPLXBIB0057" label="[57]">Fanyi Xiao and Yong
        Jae&nbsp;Lee. 2015. Discovering the spatial extent of
        relative attributes. In <em><em>Proceedings of the IEEE
        International Conference on Computer Vision</em></em> .
        1458–1466.</li>
        <li id="BibPLXBIB0058" label="[58]">Zichao Yang, Diyi Yang,
        Chris Dyer, Xiaodong He, Alexander&nbsp;J Smola, and
        Eduard&nbsp;H Hovy. 2016. Hierarchical Attention Networks
        for Document Classification.. In
        <em><em>HLT-NAACL</em></em> . 1480–1489.</li>
        <li id="BibPLXBIB0059" label="[59]">Chenxi Zhang, Jizhou
        Gao, Oliver Wang, Pierre Georgel, Ruigang Yang, James
        Davis, Jan-Michael Frahm, and Marc Pollefeys. 2014.
        Personal photograph enhancement using internet photo
        collections. <em><em>IEEE transactions on visualization and
        computer graphics</em></em> 20, 2(2014), 262–275.</li>
        <li id="BibPLXBIB0060" label="[60]">Ye Zhang and Byron
        Wallace. 2015. A sensitivity analysis of (and
        practitioners’ guide to) convolutional neural networks for
        sentence classification. <em><em>arXiv preprint
        arXiv:1510.03820</em></em> (2015).</li>
        <li id="BibPLXBIB0061" label="[61]">Xiaoqing Zheng, Hanyang
        Chen, and Tianyu Xu. 2013. Deep Learning for Chinese Word
        Segmentation and POS Tagging.. In <em><em>EMNLP</em></em> .
        647–657.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186021">https://doi.org/10.1145/3178876.3186021</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
