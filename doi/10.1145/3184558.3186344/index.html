<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Analyzing and Predicting Emoji Usages in Social Media</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186344'>https://doi.org/10.1145/3184558.3186344</a> 
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186344'>https://w3id.org/oa/10.1145/3184558.3186344</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Analyzing and Predicting Emoji Usages in Social Media</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Peijun</span>     <span class="surName">Zhao</span>     Department of Computer Science and Technology, Tsinghua University, <a href="mailto:zhaopeijun0328@163.com">zhaopeijun0328@163.com</a>    </div>    <div class="author">     <span class="givenName">Jia</span>     <span class="surName">Jia</span>     Department of Computer Science and Technology, Tsinghua University, <a href="mailto:jjia@tsinghua.edu.cn">jjia@tsinghua.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Yongsheng</span>     <span class="surName">An</span>     Academy of Arts &#x0026; Design, Tsinghua University, <a href="mailto:316991611@qq.com">316991611@qq.com</a>    </div>    <div class="author">     <span class="givenName">Jie</span>     <span class="surName">Liang</span>     Academy of Arts &#x0026; Design, Tsinghua University, <a href="mailto:liang-j17@tsinghua.edu.cn">liang-j17@tsinghua.edu.cn</a>    </div>    <div class="author">     <span class="givenName">Lexing</span>     <span class="surName">Xie</span>     Department of Computer Science, Australian National University, <a href="mailto:lexing.xie@anu.edu.au">lexing.xie@anu.edu.au</a>    </div>    <div class="author">     <span class="givenName">Jiebo</span>     <span class="surName">Luo</span>     Department of Computer Science, University of Rochester, <a href="mailto:jiebo.luo@gmail.com">jiebo.luo@gmail.com</a>    </div>        <Affiliation id="aff2">Key Laboratory of Pervasive Computing, Ministry of Education</Affiliation>    <Affiliation1 id="aff3">Tsinghua National Laboratory for Information Science and Technology (TNList)</Affiliation1>        <Affiliation1 id="aff5">Key Laboratory of Pervasive Computing, Ministry of Education</Affiliation1>    <Affiliation1 id="aff6">Tsinghua National Laboratory for Information Science and Technology (TNList)</Affiliation1>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3186344" target="_blank">https://doi.org/10.1145/3184558.3186344</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Emojis can be regarded as a language for graphical expression of emotions, and have been widely used in social media. They can express more delicate feelings beyond textual information and improve the effectiveness of computer-mediated communication. Recent advances in machine learning make it possible to automatic compose text messages with emojis. However, the usages of emojis can be complicated and subtle so that analyzing and predicting emojis is a challenging problem. In this paper, we first construct a benchmark dataset of emojis with tweets and systematically investigate emoji usages in terms of tweet content, tweet structure and user demographics. Inspired by the investigation results, we further propose a multitask multimodality gated recurrent unit (mmGRU) model to predict the categories and positions of emojis. The model leverages not only multimodality information such as text, image and user demographics, but also the strong correlations between emoji categories and their positions. Our experimental results show that the proposed method can significantly improve the accuracy for predicting emojis for tweets (+9.0% in F1-value for category and +4.6% in F1-value for position). Based on the experimental results, we further conduct a series of case studies to unveil how emojis are used in social media.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Emoji; GRU; Multimodality; Multitask</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Peijun Zhao, Jia Jia, Yongsheng An, Jie Liang, Lexing Xie, and Jiebo Luo. 2018. Analyzing and Predicting Emoji Usages in Social Media. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3186344" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3186344</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Emojis have been widely used in social networks to express more delicate feelings beyond textual information and make computer-mediated communication more effective. Vulture&#x0027;s Lindsey Weber, who co-curated the &#x201C;Emoji&#x201D; art show<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, says that she use emojis in personal emails all the time, because she feel like she is softening the email. According to the study in Instagram[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], emojis are present in up to 40% of online messages in many countries. For example, &#x201C;Face with Tears of Joy&#x201D;, an emoji that means the person has an extremely good mood, is regarded as the 2015 word of the year by The Oxford Dictionary.</p>    <p>Since most people have experiences in using emojis for posting tweets, the problem of automatically generating emojis becomes interesting and useful for online users. Existing works usually study emoji usages based on textual information. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] uses Affective Trajectory Model for emoji category recommendation based on textual information. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] analyzes the emotion of the textual information published by users and proposes an emoji recommendation method based on the emotive statements of users and their past selections of emojis. However, whether there are regularities or norms in the choices of emojis and what information is related to them are still open problems, which make the problem of generating emojis challenging.</p>    <p>The usages of emojis depend on many complex factors. Since emojis are an integral part of tweets and play an important role in expressing emotion, the choices of emojis are mainly affected by the following factors: 1) Tweet content. Not only textual information, but also visual information can enrich the expression of emotion for tweets. How can the multimodality information affect the choices of emojis? 2) Tweet structure. Different emojis may appear at different positions in tweets. Whether there are correlations between the categories and positions of emojis has not yet been confirmed. 3) User demographics. Different users may have different habits on using emojis. How can user demographics such as geographica region information influence assigning emojis? <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Overview of the Proposed Emoji Prediction Method. Far-left: Example tweets from Twitter. Mid-left: Textual, visual and user demographic features. Mid-right: The mmGRU model. Far-right: Predictions results.</span>     </div>    </figure>    </p>    <p>In this paper, we first construct a benchmark dataset of emojis from Twitter. It contains 164,880 tweets of the 35 most frequently used emojis. Using the dataset, we systematically study emoji usages with tweet content, tweet structure and user demographics and obtain a series of data-driven observations. Inspired by the observations, we propose a multitask multimodality gated recurrent unit (mmGRU) model (Figure <a class="fig" href="#fig1">1</a>) to quantitatively study the correlations between emojis and the above three aspects. The multimodality function is used to incorporate multimodality information such as text, image, and user demographics, while the multitask framework contributes to leveraging the strong correlations between emoji categories and their positions for improving the performance of both tasks. We apply the proposed model to predict the emoji categories and positions of online tweets in our dataset. Our experimental results show that our model can significantly improve the prediction performance of emoji category by +9.0% in F1-value and emoji position by +4.6% in F1-value, on the average, compared with several alternative baselines. Based on the experimental results, we further conduct a series of case studies to reveal how emojis are used in social media.</p>    <p>Our main contributions are as follows:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">&#x00A0;We construct a large-scale benchmark dataset for emoji prediction in terms of emoji categories and positions. Each tweet in our dataset contains both textual, visual and the user demographic information. We release the dataset and related information upon the publication of this work<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>.<br/></li>    <li id="list2" label="&#x2022;">&#x00A0;We investigate the correlation between emoji usages and three key aspects. We then propose an mmGRU model to predict the categories and positions of emojis. Our experimental results validate the rationality and effectiveness of the proposed mmGRU model.<br/></li>    <li id="list3" label="&#x2022;">&#x00A0;We unveil several interesting findings based on our data, method and experiments, including: 1) users in different geographic regions have different diversity in emoji selection; 2) users in different regions have different understanding on the emotions of the same emojis; 3) emojis in different shapes (i.e. heart shape or face shape) tend to appear at different positions.<br/></li>    </ul>    <p>The remainder of this paper is organized as follows. First, we introduce the related work in section 2. We then define several notations and formulate the learning tasks in section 3 and discuss data observation in section 4. Next, we describe the proposed emoji prediction method in section 5. Experiments are given in section 6 and section 7 concludes this paper. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Observations on the following aspects: (a) Correlation between regions and emoji categories. X-axis represents the seven typical regions and the average. Y-axis represents the ratio of the emojis. The legend represents the top-10 frequently used emojis. (b) Correlations between emoji categories and positions. (c) Correlations between regions and emoji positions.</span>     </div>    </figure>    </p>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>    <strong>Social emotion analysis</strong>. People often use emojis to express emotions. Research on emotion analysis or prediction has been discussed in many works. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] describe sentiment analysis methods using textual and visual contents. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] proposes a new neighborhood classier based on the similarity of two instances described by the fusion of textual and visual features. Recent years, studies have found that user demographics can also help for sentiment analysis for online users. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] studies the influence of user demographics on image sentiment analysis. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] combines the textual, visual and the social information of users and propose a user-level emotion prediction method. The above works reveal that adopting multimodality information and user demographics can improve personalized emotion prediction.</p>    <p>    <strong>Emoji Usages and Recommendation</strong>. Traditional works usually study emoji usages through two aspects: treat emojis as sentimental labels, and recommend emojis with textual information. For sentiment analysis, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] divides emojis into four kinds of emotions and then uses them to study emotion changes of online users. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] studies the multilingual sentiment analysis using emojis and keywords. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] labels the sentiment scores of emojis. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] builds an emoji space model for sentiment analysis. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] studies the sentiment and semantic misconstrue of emojis on different platforms. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] studies the intentions and sentiment effects of using emojis during communications. While for emoji recommendation, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] uses Affective Trajectory Model for emoji category recommendation based on textual information. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] analyzes the emotion of the textual information and proposes an emoji recommendation method based on the emotive statements of users and their past selections of emojis. The above works construct their recommendation methods mainly using textual information and ignore the complexity of emoji usages such as tweet structure and users&#x2019; personalized information.</p>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Problem Formulation</h2>    </div>    </header>    <p>To formulate our problem, we first define some notations.</p>    <p>Definition 1. <strong>Emoji category and position.</strong> The emoji category of a tweet is denoted as <em>C<sub>i</sub>    </em>, <em>i</em> &#x2208; [0, <em>C</em>), where <em>C</em> is the number of emoji categories. As for the emoji position, we define two representations: fine-grained position and coarse-grained position. The fine-grained position of a tweet is denoted as <em>FP<sub>i</sub>    </em>, <em>i</em> &#x2208; [0, <em>P</em>), where <em>P</em> is the max length of the tweets. The coarse-grained position of a tweet is denoted as <em>CP<sub>i</sub>    </em>, <em>i</em> &#x2208; [0, 3), where <em>CP</em>    <sub>0</sub>, <em>CP</em>    <sub>1</sub>, <em>CP</em>    <sub>2</sub> represent the start, middle and end positions.</p>    <p>Definition 2. <strong>mmGRU network.</strong> The mmGRU network is denoted as <em>G</em> = (<strong>X</strong>, <strong>V</strong>, <strong>U</strong>), where <strong>X</strong> is <em>l</em> &#x00D7; <em>d</em> attribute matrix, each row corresponds to a tweet and each column represents one dimension of the textual features. <strong>V</strong> is <em>l</em> &#x00D7; <em>e</em> attribute matrix, each row corresponds to a tweet and each column represents one dimension of the visual features. <strong>U</strong> is <em>l</em> &#x00D7; <em>f</em> attribute matrix, each row corresponds to a tweet and each column represents one dimension of the user demographic features. <em>l</em> represents the number of tweets and <em>d</em>, <em>e</em>, <em>f</em> represent the dimensions of each feature, respectively.</p>    <p>Problem 1. <strong>Learning Task.</strong> The problem we focus on is to predict an emoji for a tweet and a suitable position to put it in. We use a multitask multimodality gated recurrent unit (mmGRU) model <em>G</em>    <sub>1</sub> to learn a prediction function <em>f</em>    <sub>1</sub> to predict the emoji category and coarse-grained position of every tweet, and a multimodality gated recurrent unit model <em>G</em>    <sub>2</sub> to learn a prediction function <em>f</em>    <sub>2</sub> to predict the fine-grained position of every tweet, defined as: <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} &#x0026;f_1:G_1=({\bf X},{\bf V},{\bf U}) \rightarrow {\bf C}\ and\ {\bf CP}\ \\ &#x0026;f_2:G_2=({\bf X},{\bf V},{\bf U}, {\bf CP}) \rightarrow {\bf FP}\ \\ \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div>    </p>   </section>   <section id="sec-14">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Data-based Observations</h2>    </div>    </header>    <p>The choices of emoji categories and positions depend on a number of complex factors, such as tweet content, tweet structure and user demographics. Since people use emojis as the direct reflection on tweet contents, we mainly focus on obtaining a series of observations to further reveal whether tweet structure and user demographics have influences on emoji usage.</p>    <p>To obtain the observations, we first construct a benchmark emoji dataset from Twitter which contains 164,880 tweets of the 35 most-frequently used (0.62%-15.7%) emojis for experimental analysis. Each tweet contains textual, visual, user demographic information and only one emoji. For each of the selected emojis, there are more than 1,000 tweets.</p>    <div class="table-responsive" id="tab1">    <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">User number of the select regions</span>    </div>    <table class="table">     <tbody>      <tr>       <td style="text-align:left;">Region</td>       <td style="text-align:center;">America</td>       <td style="text-align:center;">England</td>       <td style="text-align:center;">China</td>       <td style="text-align:center;">Netherlands</td>      </tr>      <tr>       <td style="text-align:left;">User number</td>       <td style="text-align:center;">30,361</td>       <td style="text-align:center;">4,763</td>       <td style="text-align:center;">3,348</td>       <td style="text-align:center;">2,978</td>      </tr>      <tr>       <td style="text-align:left;">Region</td>       <td style="text-align:center;">Brazil</td>       <td style="text-align:center;">Thailand</td>       <td style="text-align:center;">France</td>       <td style="text-align:center;"/>      </tr>      <tr>       <td style="text-align:left;">User number</td>       <td style="text-align:center;">2,410</td>       <td style="text-align:center;">1,423</td>       <td style="text-align:center;">760</td>       <td style="text-align:center;"/>      </tr>     </tbody>    </table>    </div>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Observations on Emoji Categories and Positions</h3>     </div>    </header>    <p>Figure 2(b) shows the correlation between the top 10 frequently used emojis and their probabilities of appearing at different positions. The average position ranges from 0 (the beginning of the tweets) to 1 (the end of the tweets). The three rows represent the percentage of emojis that appeared at the start, middle and end of the tweets.</p>    <p>On the average, 70% of the emojis are used at the end of the tweets and only 2.6% are used at the beginning of the tweets. &#x201C;Heart Suit&#x201D; and &#x201C;Red Heart&#x201D; are more likely to be used in the front part of the tweets. Comparing with other emojis, &#x201C;Raising Hands&#x201D; and &#x201C;Face with Tears of Joy&#x201D; are more likely to be used at the end of tweets.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Observations on Users&#x2019; Demographics and Their Choices of Emojis</h3>     </div>    </header>    <p>Cultural background will affect the habit of using emojis. Geographic region information has a great influence on the cultural background of users. Therefore, we take region information as an example to represent the demographics.</p>    <p>     <strong>Category</strong>. We select seven regions that with the largest number of users which are shown in Table <a class="tbl" href="#tab1">1</a>. The selected regions are America, China, England, France, Netherlands, Brazil and Thailand. Figure 2(a) shows the correlation between the regions and the top 10 frequently used emojis. We find that people in different regions have their preferred emojis. For example, &#x201C;Heart Suit&#x201D;, which is used in card games for the heart suit, is more likely to be used by users in France, Brazil, and Thailand but rarely by the users in America. &#x201C;Red Heart&#x201D;, which looks like &#x201C;Heart Suit&#x201D;, is used for expressions of love according to the explanation in Emojipedia<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. This emoji is more popular in Brazil and less popular in China. Users in America, China and Nederlands are more likely to use &#x201C;Loudly Crying Face&#x201D;, while users in France almost never use that emoji, which means that users in France rarely share their pessimistic emotion online. The region closest to the average (emoji distributions for all users) is Netherlands.</p>    <p>     <strong>Position</strong>. Figure 2(c) shows the correlations between the regions and positions of emojis. We count the percentage of emojis that appeared at the start, middle and end of the tweets and the average emoji position for every region. Users in Brazil and Thailand are more likely to employ emojis in the front part of the tweets. Users in America and China are more likely to employ emojis at the end of tweets, and Chinese users barely employ emojis at the start of tweets.</p>    <p>     <strong>Summarization</strong>. The observations can be summarized as follows:</p>    <ul class="list-no-style">     <li id="list4" label="&#x2022;">&#x00A0;There exists correlation between emoji categories and positions. In particular, most of the emojis are mainly used at the end of the tweets and some are more likely to be used in the front and the middle part of the tweets, such as &#x201C;Heart Suit&#x201D; and &#x201C;Red Heart&#x201D;.<br/></li>     <li id="list5" label="&#x2022;">&#x00A0;Regional culture can affect the users&#x2019; habits of using emojis, which suggests that demographics such as region information should be considered to improve emoji prediction.<br/></li>    </ul>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Proposed Method</h2>    </div>    </header>    <p>Inspired by the observations, we propose a multitask multimodality gated recurrent unit (mmGRU) model to quantitatively describe the correlations between emojis and tweet content, tweet structure, user demographics. We use GRU[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] as the framework because it can overcome the problem of gradient vanishing compared with recurrent neural network (RNN). The multimodality function is used to incorporate multimodality information such as text, image and user demographics, while the multitask framework helps leverage the strong correlation between emoji categories and their positions to improve the performance of both tasks.</p>    <p>An overview of the proposed method for emoji prediction is shown in Figure <a class="fig" href="#fig1">1</a>. In the first step, we extract the textual, visual and the user demographic features from a tweet. Then, the three kinds of features are sent into the proposed mmGRU model to learn an output representation. Next, we use a softmax classifier to obtain the predicted results of emoji categories and coarse-grained positions together according to the their correlation as mentioned in section 4. As for fine-grained position, we use another softmax classifier and the results of coarse-grained position to get the predicted results. Finally, we can obtain the target emojis and their positions in the raw tweets based on the well-trained model.</p>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Model structure</h3>     </div>    </header>    <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">One slice of the proposed mmGRU model for position prediction</span>     </div>    </figure>    <p>We explain our model in Figure <a class="fig" href="#fig3">3</a>. The rectangular box represents a cell of GRU. Compared with LSTM cell, it only contains two gates: a reset gate and an update gate. The reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. The equations of the GRU cell are as follows: <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026; r_{t}=\sigma \left(W_{xr}x_{t}+W_{hr}h_{t-1} \right) \\ &#x0026; z_{t}=\sigma \left(W_{xz}x_{t}+W_{hz}h_{t-1} \right) \\ &#x0026; \widetilde{h_{t}}=tanh \left(W_{xr}x_{t}+W_{ho}h_{t-1} \right) \\ &#x0026; h_{t}=\left(1-z_{t}\right) \odot h_{t-1} + z_{t} \odot \widetilde{h_{t}} \\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <em>r<sub>t</sub>     </em> is the reset vector, <em>z<sub>t</sub>     </em> is the update vector, <span class="inline-equation"><span class="tex">$\widetilde{h_{t}}$</span>     </span> is the candidate output vector, <em>h<sub>t</sub>     </em> is the output vector, <em>W<sub>xr</sub>     </em>, <em>W<sub>hr</sub>     </em>, <em>W<sub>xz</sub>     </em>, <em>W<sub>hz</sub>     </em> are the weight matrix parameters, <em>&#x03C3;</em> is the sigmoid function and tanh is a hyperbolic tangent.</p>    <p>For textual features <strong>X</strong> = <em>x<sub>i</sub>     </em>, <em>i</em> &#x2208; [1, <em>t</em>], we use word embedding method<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> to convert the one-hot features into low-dimensional vectors. Then we get the input vectors <span class="inline-equation"><span class="tex">$\widetilde{{\bf X}}=\widetilde{x_{i}},i\in [1,t]$</span>     </span>.</p>    <p>For visual features and user demographic features, we use the joint learning method proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>]. We use <em>W<sub>v</sub>     </em> and <em>b<sub>v</sub>     </em> to convert the visual features <strong>V</strong> and user demographic features <strong>U</strong> into textual feature space. After the new vector is learned, we treat it as the initial state of the model before we input each word in every tweet, which means we send the image into the model at time 0. We combine the vector <span class="inline-equation"><span class="tex">$\widetilde{x_{0}}$</span>     </span> with the textual features <span class="inline-equation"><span class="tex">$\widetilde{x_{i}},i\in [1,t]$</span>     </span> we get from the embedding layer. <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \widetilde{x_{0}}=W_v\left({\bf V} + {\bf U} \right) + b_v \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <em>W<sub>v</sub>     </em> is the weight matrices and <em>b<sub>v</sub>     </em> is the bias vector.</p>    <p>The hidden layer is composed of a bidirectional GRU (BI-GRU) hidden layer, which is used for building output vectors <em>y<sub>i</sub>     </em>, <em>i</em> &#x2208; [0, <em>t</em>] of input features. <div class="table-responsive" id="Xeq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} y_{0},y_{1},\cdots ,y_{t}=BI-GRU\left(\widetilde{x_{0}},\widetilde{x_{1}},\cdots ,\widetilde{x_{t}} \right)\\ \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>    </p>    <p>We use max pooling operation to convert the output vectors <em>y<sub>i</sub>     </em>, <em>i</em> &#x2208; [0, <em>t</em>] into the final output vector <strong>o</strong>. <div class="table-responsive" id="Xeq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\bf o}=MaxPooling\left(y_{0},y_{1},\cdots ,y_{t} \right) \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>    </p>    <p>We use a linear translation and softmax regression to gain the probability distribution of categories and coarse-grained positions. Then we use the results of coarse-grained position to calculate the probability distribution of fine-grained positions. <div class="table-responsive" id="Xeq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026; {\bf o}_c^{s}=W_c \times {\bf o} + b_c\\ &#x0026; p_{ci}=\frac{exp({{\bf o}_c^{s}}_{i})}{\sum _{n=1}^C exp({{\bf o}_c^{s}}_{n})},i \in [0,C)\\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div>     <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026; {\bf o}_{cp}^{s}=W_{cp} \times ({\bf o}+p_{c}) + b_{cp}\\ &#x0026; p_{cpi}=\frac{exp({{\bf o}_{cp}^{s}}_{i})}{\sum _{n=1}^C exp({{\bf o}_{cp}^{s}}_{n})},i \in [0,3)\\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>     <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026; \widetilde{{\bf o}_c^{s}}=\widetilde{W_c} \times ({\bf o}+p_{cp}) + \widetilde{b_c}\\ &#x0026; \widetilde{p_{ci}}=\frac{exp(\widetilde{{\bf o}_c^{s}}_{i})}{\sum _{n=1}^C exp(\widetilde{{\bf o}_c^{s}}_{n})},i \in [0,C)\\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>     <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026; {\bf o}_{fp}^{s}=W_{fp} \times ({\bf o}+p_{cp}) + b_{fp}\\ &#x0026; p_{fpi}=\frac{exp({{\bf o}_{fp}^{s}}_{i})}{\sum _{n=1}^C exp({{\bf o}_{fp}^{s}}_{n})},i \in [0,P)\\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\widetilde{p_{ci}}$</span>     </span>/<em>p<sub>cpi</sub>     </em>/<em>p<sub>fpi</sub>     </em> is the probability for the i-th category/ coarse-grained position/ fine-grained position. <em>p<sub>ci</sub>     </em> is intermediate results for category. C is the number of categories. <span class="inline-equation"><span class="tex">${\bf o}_c^{s},{\bf o}_{cp}^{s},\widetilde{{\bf o}_c^{s}},{\bf o}_{fp}^{s}$</span>     </span> are the output vectors after linear translation. <em>W<sub>c</sub>     </em>, <em>b<sub>c</sub>     </em>, <em>W<sub>cp</sub>     </em>, <em>b<sub>cp</sub>     </em>, <em>W<sub>fp</sub>     </em>, <em>b<sub>fp</sub>     </em>, <span class="inline-equation"><span class="tex">$\widetilde{W_c}$</span>     </span> and <span class="inline-equation"><span class="tex">$\widetilde{b_c}$</span>     </span> are the parameters of softmax regression that need to be learned.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Model learning</h3>     </div>    </header>    <p>. As for category and coarse-grained position, we calculate the loss function by incorporating the loss values of both two prediction tasks and train to minimize the value of the loss function <em>J</em>     <sub>1</sub>. As for the fine-grained position, we calculate the loss function <em>J</em>     <sub>2</sub> alone. The training process uses Adam optimization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>] to perform the classifier-to-neural network feedback and feeds back into the input word vector layer. <div class="table-responsive" id="Xeq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026; {\theta _1}^{*}=\arg \min _{\theta _1} J_1(\widetilde{W_c}, \widetilde{b_c}, W_{cp}, b_{cp})\\ &#x0026; {\theta _2}^{*}=\arg \min _{\theta _2} J_2(W_{fp}, b_{fp})\\ \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div>    </p>    </section>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Experiments</h2>    </div>    </header>    <figure id="fig4">    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 4:</span>     <span class="figure-title">Experimental results: (a) top-5 accuracy of category; (b) top-5 accuracy of fine-grain position; (c) feature contribution analysis of category; (d) feature contribution analysis of fine-grain position; (e) effects of the cell layer number; (f) effects of the dropout rate; (g) effects of the number of hidden states; (h) effects of the batch size.</span>    </div>    </figure>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Experimental Setup</h3>     </div>    </header>    <p>     <strong>Data set</strong>. We use the dataset collected in section 4. It contains 164,880 Twitter tweets. And each tweet contains textual, visual, regional information and an emoji belongs to the 35 most frequently used emojis. The dataset is collected from the raw dataset that mentioned in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>], which is streamed from Twitter API using a set of keywords related to YouTube and its videos. For the experimental setup, we design a prediction task for the select 35 emojis and their positions in tweets.</p>    <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-img1.svg" class="img-responsive" alt=""      longdesc=""/>    </p>    <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-img2.svg" class="img-responsive" alt=""      longdesc=""/>    </p>    <p>     <strong>Comparison methods</strong>. We conduct performance comparison experiments to demonstrate the effectiveness of our model. We select five methods as follows: Random selection, Logistic Regression (LR), Support Vector Machine (SVM), Deep Neural Network (DNN), Gated Recurrent Unit (GRU) and the proposed model.</p>    <ul class="list-no-style">     <li id="list6" label="&#x2022;">&#x00A0;<strong>Random</strong>. We adopt the random selection method as the first baseline method. The results of this method are equal to the max proportion of all the prediction choices.<br/></li>     <li id="list7" label="&#x2022;">&#x00A0;<strong>LR</strong>. Logistic regression is commonly used in classification problems. Here we use the scikit-learn [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>] implementation to build our baseline method.<br/></li>     <li id="list8" label="&#x2022;">&#x00A0;<strong>SVM</strong>. SVM is a frequently-used method of solving classification problems. Here we use LIBSVM<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a> to build it.<br/></li>     <li id="list9" label="&#x2022;">&#x00A0;<strong>DNN</strong>. It has been proved that DNN can achieve good performance in multimodality classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>]. We use the DNN model it mentioned as the baseline method in our experiments.<br/></li>     <li id="list10" label="&#x2022;">&#x00A0;<strong>GRU</strong>. Basic GRU model[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>] that trains emoji categories and positions separately.<br/></li>     <li id="list11" label="&#x2022;">&#x00A0;<strong>The proposed method</strong>. The mmGRU method we propose in this paper.<br/></li>    </ul>    <p>     <strong>Metrics</strong>. To quantitatively evaluate the category and position prediction performance, we use micro <em>Precision</em>, <em>Recall</em> and <em>F1-value</em><a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> as metrics, which are calculated by the weighted average method. Because our goal is a multi-classification problem. We also provide <em>Top-k accuracy</em> as another metric for category prediction performance.</p>    <p>We implement, train and evaluate our method on Tensorflow<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>. We perform five-fold cross validation to obtain the average prediction performance.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Feature Extraction</h3>     </div>    </header>    <p>     <strong>Textual Features.</strong> First, we load the words of the Twitter corpus and translate each word into a one-hot vector. Then we replace the words that appeared less than 5 times with a symbol called &#x201C; < rare > &#x201D;. Next we formulate all tweets into the same length by adding a symbol called &#x201C; < blank > &#x201D; to the end of tweets.</p>    <p>     <strong>Visual Features.</strong> We use VGG-19 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] to help build our visual feature representations. VGG-19 is a deep convolutional network with 19 layers which is designed for feature extraction tasks and classification tasks of images. We choose the second to the last layer as the feature representation since it can represent the global information of the images. Then we obtain a vector with 4,096 dimensions from each picture as the visual features.</p>    <p>     <strong>User Demographic Features.</strong> Considering the users&#x2019; privacy, we only choose region information as an example of the user demographic features in our experiments. We collect the region features such as the country information of the users from every tweet and then transform them into one-hot vectors.</p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Results and Analysis</h3>     </div>    </header>    <p>     <strong>Performance analysis</strong>. First, we aim to validate if our model is effective for the prediction tasks. We compare the proposed model with several baseline methods.</p>    <p>Table <a class="tbl" href="#tab2">2</a> lists the average prediction results of the mentioned models. The proposed mmGRU model clearly shows the best performance than other models. In terms of <em>F1-value</em> for category, our model achieves 14.4% improvement compared with LR, 13.3% improvement compared with SVM, 7.5% improvement compared with DNN and 0.8% improvement compared with GRU. As for coarse-grained (fine-grained) position, our model achieves 6.6% (7.9%) improvement compared with LR, 5.0% (6.7%) improvement compared with SVM, 3.2% (5.9%) improvement compared with DNN and 0.8% (0.9%) improvement compared with GRU on average. We use T-test<a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a> to confirm if there is a significant difference between our method and the baseline methods. The results show that the P-value<a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a> is less than 0.05, which means our method has a significant improvement over baseline methods. Specifically, we list the F1-value of the top 10 mostly used emojis in Table <a class="tbl" href="#tab3">3</a>. The results show that our model can achieve the best performance of most emojis. All of the above experiments confirm that the multimodal multitask framework and the GRU network are effective in modeling emoji usage. We also notice that for &#x201C;Red Heart&#x201D;, &#x201C;Two Hearts&#x201D; and &#x201C;Loudly Crying Face&#x201D;, the performance of our model are worse than the performance of GRU model. It is probably because the correlation between the categories and positions of these emojis are not significant.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Comparison of results using different models</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:center;"/>       <td colspan="3" style="text-align:center;">Category<hr/>       </td>       <td style="text-align:center;">C-Position</td>       <td style="text-align:center;">F-Position</td>       </tr>       <tr>       <td style="text-align:center;">Metrics</td>       <td style="text-align:center;">Precision</td>       <td style="text-align:center;">Recall</td>       <td style="text-align:center;">F1-score</td>       <td style="text-align:center;">F1-score</td>       <td style="text-align:center;">F1-score</td>       </tr>       <tr>       <td style="text-align:center;">Random</td>       <td style="text-align:center;">0.157</td>       <td style="text-align:center;">0.157</td>       <td style="text-align:center;">0.157</td>       <td style="text-align:center;">0.700</td>       <td style="text-align:center;">0.700</td>       </tr>       <tr>       <td style="text-align:center;">LR</td>       <td style="text-align:center;">0.321</td>       <td style="text-align:center;">0.316</td>       <td style="text-align:center;">0.235</td>       <td style="text-align:center;">0.785</td>       <td style="text-align:center;">0.742</td>       </tr>       <tr>       <td style="text-align:center;">SVM</td>       <td style="text-align:center;">0.325</td>       <td style="text-align:center;">0.326</td>       <td style="text-align:center;">0.246</td>       <td style="text-align:center;">0.801</td>       <td style="text-align:center;">0.754</td>       </tr>       <tr>       <td style="text-align:center;">DNN</td>       <td style="text-align:center;">0.354</td>       <td style="text-align:center;">0.379</td>       <td style="text-align:center;">0.304</td>       <td style="text-align:center;">0.819</td>       <td style="text-align:center;">0.762</td>       </tr>       <tr>       <td style="text-align:center;">GRU</td>       <td style="text-align:center;">0.375</td>       <td style="text-align:center;">0.390</td>       <td style="text-align:center;">0.371</td>       <td style="text-align:center;">0.843</td>       <td style="text-align:center;">0.812</td>       </tr>       <tr>       <td style="text-align:center;">mmGRU</td>       <td style="text-align:center;">        <strong>0.380</strong>       </td>       <td style="text-align:center;">        <strong>0.402</strong>       </td>       <td style="text-align:center;">        <strong>0.379</strong>       </td>       <td style="text-align:center;">        <strong>0.851</strong>       </td>       <td style="text-align:center;">        <strong>0.821</strong>       </td>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Category performance on the top 10 frequently used emojis (in terms of F1-value)</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:left;">Emoji</td>       <td style="text-align:center;">Random</td>       <td style="text-align:center;">LR</td>       <td style="text-align:center;">SVM</td>       <td style="text-align:center;">DNN</td>       <td style="text-align:center;">GRU</td>       <td style="text-align:center;">mmGRU</td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic14.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.157</td>       <td style="text-align:center;">0.699</td>       <td style="text-align:center;">0.697</td>       <td style="text-align:center;">0.702</td>       <td style="text-align:center;">0.724</td>       <td style="text-align:center;">        <strong>0.730</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic15.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.141</td>       <td style="text-align:center;">0.412</td>       <td style="text-align:center;">0.427</td>       <td style="text-align:center;">0.448</td>       <td style="text-align:center;">0.524</td>       <td style="text-align:center;">        <strong>0.530</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic16.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.114</td>       <td style="text-align:center;">0.268</td>       <td style="text-align:center;">0.296</td>       <td style="text-align:center;">0.348</td>       <td style="text-align:center;">        <strong>0.479</strong>       </td>       <td style="text-align:center;">0.465</td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic17.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.060</td>       <td style="text-align:center;">0.349</td>       <td style="text-align:center;">0.361</td>       <td style="text-align:center;">0.415</td>       <td style="text-align:center;">0.457</td>       <td style="text-align:center;">        <strong>0.487</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic18.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.049</td>       <td style="text-align:center;">0.210</td>       <td style="text-align:center;">0.255</td>       <td style="text-align:center;">0.265</td>       <td style="text-align:center;">        <strong>0.360</strong>       </td>       <td style="text-align:center;">0.328</td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic19.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.040</td>       <td style="text-align:center;">0.078</td>       <td style="text-align:center;">0.086</td>       <td style="text-align:center;">0.189</td>       <td style="text-align:center;">        <strong>0.285</strong>       </td>       <td style="text-align:center;">0.282</td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic20.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.038</td>       <td style="text-align:center;">0.030</td>       <td style="text-align:center;">0.034</td>       <td style="text-align:center;">0.077</td>       <td style="text-align:center;">0.195</td>       <td style="text-align:center;">        <strong>0.202</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic21.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.034</td>       <td style="text-align:center;">0.074</td>       <td style="text-align:center;">0.075</td>       <td style="text-align:center;">0.243</td>       <td style="text-align:center;">0.360</td>       <td style="text-align:center;">        <strong>0.368</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic22.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.033</td>       <td style="text-align:center;">0.022</td>       <td style="text-align:center;">0.056</td>       <td style="text-align:center;">0.238</td>       <td style="text-align:center;">0.308</td>       <td style="text-align:center;">        <strong>0.342</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">         <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-graphic23.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>       <td style="text-align:center;">0.031</td>       <td style="text-align:center;">0.022</td>       <td style="text-align:center;">0.039</td>       <td style="text-align:center;">0.148</td>       <td style="text-align:center;">0.245</td>       <td style="text-align:center;">        <strong>0.255</strong>       </td>       </tr>      </tbody>     </table>    </div>    <p>Meanwhile, considering emoji recommendation, top-k most likely emojis are very useful results. We also design an experiment of top-k accuracy metrics. In our task, the top-k emojis are selected by the probability distribution of emojis in the prediction results. We set <em>k</em> to 1, 3, 5, and compare our method with DNN and GRU, which achieves better performance than the other three baseline methods. As Figure 4(a) shows, our method achieves 11.7<span class="inline-equation"><span class="tex">$\%$</span>     </span> improvement compared with DNN, 2.1<span class="inline-equation"><span class="tex">$\%$</span>     </span> improvement compared with GRU and reach an accuracy of 66.1<span class="inline-equation"><span class="tex">$\%$</span>     </span> for top-5 emoji category recommendation. As Figure 4(b) shows, our method achieves 2.7<span class="inline-equation"><span class="tex">$\%$</span>     </span> improvement compared with DNN, 0.6<span class="inline-equation"><span class="tex">$\%$</span>     </span> improvement compared with GRU and reach an accuracy of 95.4<span class="inline-equation"><span class="tex">$\%$</span>     </span> for top-5 emoji position recommendation.</p>    <p>     <strong>Feature contribution analysis</strong>. In our work, we utilize the textual, visual and user demographic features and send them into an mmGRU model for emoji prediction tasks. To investigate whether these features benefit the prediction tasks, we investigate the contributions of every kind of feature. Every time, we take each of the features out from the primitive model while keeping the other features and then examine the performance.</p>    <p>Figure 4(c) and 4(d) shows the category and fine-grained position performance of different feature combinations. The model involving all factors achieves the best performance both in category and fine-grained position. Textual features achieve the most contribution (+23.5% in <em>F1-value</em> for category and +20.3% in <em>F1-value</em> for position) among all features. For the last two kind of features, visual feature (+0.7% in <em>F1-value</em>) achieves greater contribution than user demographic feature (+0.4% in <em>F1-value</em>) for category prediction. On the contrary, user demographic feature (+1.5% in <em>F1-value</em>) achieves greater contribution than visual feature (+0.9% in <em>F1-value</em>) for position prediction. The above results verify the effectiveness of the multimodal features and the multimodal function of our mmGRU model.</p>    <p>     <strong>Parameter sensitivity analysis</strong>. We conduct experiments for parameter adjustment in our training process. We show how the changes of parameters in mmGRU affect the performance of emoji category prediction by comparing the average performance in five-fold cross validation.</p>    <ul class="list-no-style">     <li id="list12" label="&#x2022;">&#x00A0;Cell layer, the number of cell layers. Visualized in Figure 4(e), as the number of layers in mmGRU increases, the performance turns better at first and then declines. The performance achieves the highest value when the number of layers is 2.<br/></li>     <li id="list13" label="&#x2022;">&#x00A0;Dropout rate, the dropout probability of the input. Figure 4(f) shows the performance of the dropout rate. As the dropout rate increases, the performance turns also better at first and then declines. The performance achieves the highest value when the dropout rate is 0.5.<br/></li>     <li id="list14" label="&#x2022;">&#x00A0;Hidden size, the number of the hidden states. Figure 4(g) shows the performance of it. The performance achieves the highest value when the hidden size is 100.<br/></li>     <li id="list15" label="&#x2022;">&#x00A0;Batch size. The performance shown in Figure 4(h) illustrates that when batch size is 64, the performance achieves the highest value.<br/></li>    </ul>    <p>Based on the parameter sensitivity analysis, we select the final parameters as follows: In the cell of GRU, we adopt two cell layers. The batch size is 64, dropout rate is 0.5 and hidden size is 100. Our experiments are conducted on a X64 machine with K80 GPU and 128G RAM.</p>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.4</span> Case Study</h3>     </div>    </header>    <p>In this part, we would like to give three interesting case studies of our data, method and experiments in Figure <a class="fig" href="#fig5">5</a>.</p>    <p>     <strong>People in different regions have different diversity in emoji selection.</strong> We find that the accuracy of the predictions is different in different regions, indicating that people in different regions have different diversity in emoji selection. Users in Brazil are the most consistent in using emojis while users in China are the most diverse in using emojis.</p>    <p>     <strong>Users in different regions have different understandings on the emotions of the same emojis.</strong> &#x201C;Face with Tears of Joy&#x201D; has both positive or negative meanings. We analyze the expressions of positive, negative and neutral emotions of this emoji in different regions by calculating the sentiments (the composition of positive, neutral and negative emotion and the sentiment score)<a class="fn" href="#fn10" id="foot-fn10"><sup>10</sup></a> of textual information in tweets. It shows that Brazilians prefer to express their positive emotions with it while French prefer to express their negative emotions with it.</p>    <p>     <strong>Emojis in different shapes (i.e. heart shape or face shape) tend to appear at different positions.</strong> Heart emojis are more likely to appear in the middle of the sentences, while face emojis are more likely to appear at the end of the sentences. It shows that heart emojis are often used to express the emotions locally in the sentence, while the face emojis are often used to enhance the emotions of the sentence globally. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186344/images/www18companion-106-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Three interesting case studies of our data, method and experiments. Top: prediction accuracy of different regions. Middle: sentiment understanding of &#x201C;Face with Tears of Joy&#x201D; in different regions. Bottom: distributions of positions on face emojis and heart emojis.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion</h2>    </div>    </header>    <p>In this paper, we examine the correlation between emoji usages and tweet content, tweet structure, user demographics. We then propose an mmGRU model for predicting emoji categories and positions motivated by the observations. The multimodality function is used to incorporate multimodality information such as text, image, and user demographics, while the multitask framework helps leveraging the strong correlation between emoji categories and their positions for improving the performance of both tasks. Extensive experiments have shown the effectiveness of the proposed method. The goal of our work is to generate reasonable emojis for the input of online users which can make the delivery of semantics more accurate and make computer-mediated communication more effective. Our work has many concrete applications, including improving user-to-user as well as user-to-chatbot interactions in social networks by building emoji recommendation systems or designing emoji input methods for online users. We plan to release the benchmark emoji-labeled dataset to facilitate more research in this area.</p>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">8</span> Acknowledgments</h2>    </div>    </header>    <p>This work is supported by the National Key Research and Development Plan (2016YFB1001200), the Innovation Method Fund of China (2016IM010200), the National Natural and Science Foundation of China (61521002), and the (US) National Science Foundation (1704309). We would also like to thank Tiangong Institute for Intelligent Computing, Tsinghua University for its support.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Donglin Cao, Rongrong Ji, Dazhen Lin, and Shaozi Li. 2014. A cross-media public sentiment analysis system for microblog. <em>      <em>Multimedia Systems</em>     </em> (2014), 1&#x2013;8.</li>    <li id="BibPLXBIB0002" label="[2]">Kyunghyun Cho, Bart&#x00A0;Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. <em>      <em>Computer Science</em>     </em> (2014).</li>    <li id="BibPLXBIB0003" label="[3]">Bhuwan Dhingra, Hanxiao Liu, William&#x00A0;W. Cohen, and Ruslan Salakhutdinov. 2016. Gated-Attention Readers for Text Comprehension. (2016).</li>    <li id="BibPLXBIB0004" label="[4]">Thomas Dimson. 2015. Emojineering Part 1: Machine Learning for Emoji Trends. http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji. (2015).</li>    <li id="BibPLXBIB0005" label="[5]">Tianran Hu, Han Guo, Hao Sun, Thuyvy&#x00A0;Thi Nguyen, and Jiebo Luo. 2017. Spice up Your Chat: The Intentions and Sentiment Effects of Using Emoji. In <em>      <em>International AAAI Conference on Web and Social Media</em>     </em>. 102&#x2013;111.</li>    <li id="BibPLXBIB0006" label="[6]">Fei Jiang, Yiqun Liu, Huanbo Luan, Min Zhang, and Shaoping Ma. 2014. <em>      <em>Microblog Sentiment Analysis with Emoticon Space Model</em>     </em>. Springer Berlin Heidelberg. 76&#x2013;87.</li>    <li id="BibPLXBIB0007" label="[7]">Diederik Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. <em>      <em>Computer Science</em>     </em> (2014).</li>    <li id="BibPLXBIB0008" label="[8]">Wei-Bin Liang, Hsien-Chang Wang, Yi-An Chu, and Chung-Hsien Wu. 2014. Emoticon recommendation in microblog using affective trajectory model. In <em>      <em>Asia-Pacific Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA)</em>     </em>. IEEE, 1&#x2013;5.</li>    <li id="BibPLXBIB0009" label="[9]">Huijie Lin, Jia Jia, Quan Guo, Yuanyuan Xue, Qi Li, Jie Huang, Lianhong Cai, and Ling Feng. 2014. User-level psychological stress detection from social media using deep neural network. In <em>      <em>ACM International Conference on Multimedia</em>     </em>. 507&#x2013;516.</li>    <li id="BibPLXBIB0010" label="[10]">Hannah Miller, Jacob Thebault-Spieker, Shuo Chang, Isaac Johnson, Loren Terveen, and Brent Hecht. 2016. &#x201D;Blissfully happy&#x201D; or &#x201D;ready to fight&#x201D;: Varying Interpretations of Emoji. In <em>      <em>International AAAI Conference on Web and Social Media</em>     </em>.</li>    <li id="BibPLXBIB0011" label="[11]">Petra&#x00A0;Kralj Novak, Jasmina Smailovi&#x0107;, Borut Sluban, and Igor Mozeti&#x010D;. 2015. Sentiment of Emojis. <em>      <em>Plos One</em>     </em>10, 12 (2015).</li>    <li id="BibPLXBIB0012" label="[12]">Fabian Pedregosa, Ga Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, and Vincent Dubourg. 2013. Scikit-learn: Machine Learning in Python. <em>      <em>Journal of Machine Learning Research</em>     </em>12, 10 (2013), 2825&#x2013;2830.</li>    <li id="BibPLXBIB0013" label="[13]">Marian&#x00A0;Andrei Rizoiu, Lexing Xie, Scott Sanner, Manuel Cebrian, Honglin Yu, and Pascal&#x00A0;Van Hentenryck. 2017. Expecting to be HIP: Hawkes Intensity Processes for Social Media Popularity. In <em>      <em>International Conference on World Wide Web</em>     </em>. 735&#x2013;744.</li>    <li id="BibPLXBIB0014" label="[14]">Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>      <em>Computer Science</em>     </em> (2014).</li>    <li id="BibPLXBIB0015" label="[15]">Georgios&#x00A0;S. Solakidis, Konstantinos&#x00A0;N. Vavliakis, and Pericles&#x00A0;A. Mitkas. 2014. Multilingual Sentiment Analysis Using Emoticons and Keywords. In <em>      <em>Ieee/wic/acm International Joint Conferences on Web Intelligence</em>     </em>. 102&#x2013;109.</li>    <li id="BibPLXBIB0016" label="[16]">Yuki Urabe, Rafal Rzepka, and Kenji Araki. 2014. Emoticon Recommendation System to Richen Your Online Communication. <em>      <em>International Journal of Multimedia Data Engineering &#x0026; Management</em>     </em>5, 1 (2014), 20.</li>    <li id="BibPLXBIB0017" label="[17]">Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 3156&#x2013;3164.</li>    <li id="BibPLXBIB0018" label="[18]">Boya Wu, Jia Jia, Yang Yang, Peijun Zhao, and Jie Tang. 2015. Understanding the emotions behind social images: Inferring with user demographics. In <em>      <em>IEEE International Conference on Multimedia and Expo</em>     </em>. 1&#x2013;6.</li>    <li id="BibPLXBIB0019" label="[19]">Yun Yang, Peng Cui, Wenwu Zhu, and Shiqiang Yang. 2013. User interest and social influence based emotion prediction for individuals. In <em>      <em>ACM International Conference on Multimedia</em>     </em>. 785&#x2013;788.</li>    <li id="BibPLXBIB0020" label="[20]">Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. 2016. Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia. In <em>      <em>ACM International Conference on Web Search and Data Mining</em>     </em>.</li>    <li id="BibPLXBIB0021" label="[21]">Yaowen Zhang, Lin Shang, and Xiuyi Jia. 2015. Sentiment Analysis on Microblogging by Integrating Text and Image Features. In <em>      <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>     </em>. 52&#x2013;63.</li>    <li id="BibPLXBIB0022" label="[22]">Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012. Moodlens: an emoticon-based sentiment analysis system for chinese tweets. In <em>      <em>Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</em>     </em>. ACM, 1528&#x2013;1531.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://www.emojishow.com/">http://www.emojishow.com/</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://emoticonprediction.droppages.com/">http://emoticonprediction.droppages.com/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>https://emojipedia.org/</p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>embedding_lookup function in Tensorflow</p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>A library for SVM designed by Chang and Lin</p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class="link-inline force-break" href="https://en.wikipedia.org/wiki/F1_score">https://en.wikipedia.org/wiki/F1_score</a>   </p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break" href="http://www.tensorflow.org">www.tensorflow.org</a>   </p>   <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a><a class="link-inline force-break"    href="https://en.wikipedia.org/wiki/Statistical_significance">https://en.wikipedia.org/wiki/Statistical_significance</a>   </p>   <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a><a class="link-inline force-break" href="https://en.wikipedia.org/wiki/P-value">https://en.wikipedia.org/wiki/P-value</a>   </p>   <p id="fn10"><a href="#foot-fn10"><sup>10</sup></a><a class="link-inline force-break"    href="http://www.nltk.org/api/nltk.sentiment.html">http://www.nltk.org/api/nltk.sentiment.html</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186344">https://doi.org/10.1145/3184558.3186344</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
