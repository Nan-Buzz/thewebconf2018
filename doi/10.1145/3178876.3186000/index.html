<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>HighLife: Higher-arity Fact Harvesting</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">HighLife: Higher-arity Fact
          Harvesting</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Patrick</span> <span class=
          "surName">Ernst</span>, Max Planck Institute for
          Informatics, Saarland Informatics CampusSaarbrücken,
          Germany, <a href=
          "mailto:pernst@mpi-inf.mpg.de">pernst@mpi-inf.mpg.de</a>
        </div>
        <div class="author">
          <span class="givenName">Amy</span> <span class=
          "surName">Siu</span>, Max Planck Institute for
          Informatics, Saarland Informatics CampusSaarbrücken,
          Germany, <a href=
          "mailto:siu@mpi-inf.mpg.de">siu@mpi-inf.mpg.de</a>
        </div>
        <div class="author">
          <span class="givenName">Gerhard</span> <span class=
          "surName">Weikum</span>, Max Planck Institute for
          Informatics, Saarland Informatics CampusSaarbrücken,
          Germany, <a href=
          "mailto:weikum@mpi-inf.mpg.de">weikum@mpi-inf.mpg.de</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186000"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186000</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Text-based knowledge extraction methods for
        populating knowledge bases have focused on binary facts:
        relationships between two entities. However, in advanced
        domains such as health, it is often crucial to consider
        ternary and higher-arity relations. An example is to
        capture which drug is used for which disease at which
        dosage (e.g. 2.5 mg/day) for which kinds of patients (e.g.,
        children vs. adults). In this work, we present an approach
        to harvest higher-arity facts from textual sources. Our
        method is distantly supervised by seed facts, and uses the
        fact-pattern duality principle to gather fact candidates
        with high recall. For high precision, we devise a
        constraint-based reasoning method to eliminate false
        candidates. A major novelty is in coping with the
        difficulty that higher-arity facts are often expressed only
        partially in texts and strewn across multiple sources. For
        example, one sentence may refer to a drug, a disease and a
        group of patients, whereas another sentence talks about the
        drug, its dosage and the target group without mentioning
        the disease. Our methods cope well with such partially
        observed facts, at both pattern-learning and
        constraint-reasoning stages. Experiments with
        health-related documents and with news articles demonstrate
        the viability of our method.</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Patrick Ernst, Amy Siu, and Gerhard Weikum. 2018.
          HighLife: Higher-arity Fact Harvesting. In <em>WWW 2018:
          The 2018 Web Conference,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 11 Pages.
          <a href="https://doi.org/10.1145/3178876.3186000" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186000</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p><strong>Motivation and Problem:</strong> Large knowledge
      bases (KBs) about entities, their properties and their
      relationships have become key components for broad
      applications. These include search engines and recommender
      systems as well as domain-specific use cases, such as health
      care (e.g., curation of biological databases&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0025">25</a>], medical
      question answering&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0050">50</a>], and guided search and exploration
      of biomedical literature&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>]). The largest publicly available KBs
      are <tt>babelnet.org</tt>, <tt>dbpedia.org</tt>,
      <tt>wikidata.org</tt> and <tt>yago-knowledge.org</tt>. They
      contain many millions of entities and billions of facts.
      However, a major limitation is that almost all of their facts
      refer to <em>binary</em> relations only, in the form of
      subject-predicate-object (SPO) triples following the RDF data
      model. For example, DBpedia knows that Marie Curie has won
      the Nobel Prize in Physics, but it does not have any
      knowledge on which contribution it was for. YAGO knows that
      Marie Curie has won a Nobel Prize in 1903 and another one in
      1911, but it does not keep the fields (Physics and Chemistry)
      as explicit predicates. Freebase (now shut down) represented
      such complex relationships by means of compound value types,
      thus deviating from the RDF data model. Information
      extraction (IE) methods that distill knowledge from text
      documents hardly capture these situations at all; they almost
      exclusively focus on binary relations. Note that it is not
      always possible to decompose ternary or higher-arity
      relations into binary facts without losing information. If we
      only knew that Curie won both physics and chemistry Nobel
      prizes and we knew that she won prizes in 1903 and 1911, we
      would have no way to infer which prize was won in which year
      (and for which contribution). We believe that going beyond
      the binary case is often crucial to capture more complete and
      deeper knowledge about events or multi-entity relationships.
      The following examples demonstrate this by text snippets that
      contain ternary or quaternary facts on prizes, business
      acquisitions and health (with relevant arguments for
      relations underlined).</p>
      <ul class="list-no-style">
        <li id="list1" label="•">In <span style=
        "text-decoration: underline;">1978</span>, Carl Sagan won
        the Pulitzer Prize for <span style=
        "text-decoration: underline;">The Dragons of
        Eden</span>.<br /></li>
        <li id="list2" label="•">Google acquired Nest for
        <span style="text-decoration: underline;"><font style=
        "normal">$</font>3.2 billion</span> in <span style=
        "text-decoration: underline;">January
        2014</span>.<br /></li>
        <li id="list3" label="•"><span style=
        "text-decoration: underline;">2.5 mg</span> Albuterol may
        be used to treat acute exacerbations, particularly in
        <span style=
        "text-decoration: underline;">children</span>.<br /></li>
        <li id="list4" label="•">Salmonella infection is a common
        cause of bacteremia in <span style=
        "text-decoration: underline;">Africa</span>.<br /></li>
      </ul>
      <p>The problem that we tackle in this paper is to
      automatically extract higher-arity facts from sentences of
      this kind.</p>
      <p><strong>State of the Art and its Limitations:</strong>
      Prior work on this problem is scarce. The IE method of
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0024">24</a>], learns
      extraction rules for higher-arity relations based on training
      facts and dependency-parsing patterns. However, this method
      produces a large number of rules with fine-grained parse
      trees as rule body – these rules do not generalize beyond
      specific patterns. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>] published a resource of
      syntactic-semantic graph patterns. However, this pattern
      collection is small and relies on manual curation. In
      contrast, our work is automated (with minimal supervision),
      scales well and can robustly cope with inputs that contain
      some but not all arguments of a higher-arity fact.</p>
      <p>A related mature line of research is <em>Semantic Role
      Labeling</em>&nbsp;(SRL)&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>]. SRL methods are based on
      constrained learning, using fine-grained syntactic and
      lexical features. They depend heavily on training sentences,
      and are typically geared for the fixed set of frames in
      PropBank&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>] or FrameNet&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>]. In our experiments, we
      use the state-of-the-art SRL system [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0038">38</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0041">41</a>] of the Illinois NLP
      Curator software [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>] as a baseline.</p>
      <p>Generally, distant supervision approaches such as
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>] have been
      widely used for harvesting facts. They usually rely on
      patterns incorporating syntactical and lexical features
      extracted from dependency parse trees. However, most of the
      earlier approaches focus exclusively on binary facts and
      neglect higher-arity facts. Our approach overcomes this
      limitation and is more general. We are able to harvest
      higher-arity relations by utilizing more complex pattern
      representations, i.e. trees instead of pure sequence
      patterns, and by considering partial facts, i.e. facts with
      unknown arguments.</p>
      <p><strong>Approach and Contribution:</strong> Our method is
      twofold. We use seed facts as distant supervision to learn
      patterns, apply these patterns to extract fact candidates,
      and iterate these steps. This extends the fact-pattern
      duality paradigm [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>] to higher-arity cases. While
      achieving high recall, this approach is susceptible to noise
      and target drifts. Therefore, we use constraint reasoning to
      eliminate spurious fact candidates. To this end, we extend
      the MaxSat-based reasoner of [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>] to the higher-arity case. For
      example, we can apply type constraints to identify when facts
      about winning the Pulitzer prize are for movies or songs
      (instead of books), and we can exploit value constraints when
      confusing the numbers for amount and year on a company
      acquisition.</p>
      <p>A key difficulty in this approach lies in the observation
      that higher-arity facts are often expressed only partially:
      with some but not all of their arguments. For example, we
      could have inputs such as “Google acquired Nest in 2014”
      without stating the amount, or “Google bought Nest for 3.2
      Billion” without giving a date. We address this issue by
      extending our framework to partial facts, partial patterns
      and reasoning over the consistency and composability of
      partial fact candidates into full facts.</p>
      <p>Our method is general and applicable to any domain and a
      wide range of text genres. For experimental studies, we test
      our method on two kinds of text corpora: i) health-related
      texts about diseases and therapies from PubMed and from
      online communities, and ii) news articles about business
      acquisitions and athletes winning medals. For unbiased
      evaluation, we obtain assessments via crowdsourcing, using
      CrowdFlower. The experiments include comparisons to a
      state-of-the-art SRL system as a baseline.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <p><strong>Knowledge Bases:</strong> Contrary to knowledge
      bases, such as YAGO&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>], WikiData&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0048">48</a>] and Freebase, which
      extract n-ary facts from pre-structured resources (e.g.
      Wikipedia Infoboxes) or rely on human input, we focus on
      harvesting n-ary facts from text.</p>
      <p><strong>Open Information Extraction:</strong> Open
      information extraction approaches, such as
      OLLIE&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0027">27</a>],
      ClausIE&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>], and EXAMPLAR&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0030">30</a>] are constrained to
      predefined syntactic patterns on parse trees for extracting
      n-ary facts and canonicalize neither relations nor entities
      to a knowledge base. Thus, they suffer from ambiguous
      extractions.</p>
      <p><strong>Semantic role labeling:</strong> Semantic Role
      Labeling (SRL)&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>] aims to map single sentences onto
      structured frames with slots filled based on the
      verb-argument structure of a sentence, using supervised
      learning over fine-grained syntactic and lexical features.
      SRL methods strongly rely on labeled training data, and are
      focused on the frame repositories provided by
      PropBank&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>] or FrameNet&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>]. Adapting these methods to
      new domains is expensive, since it entails the specification
      of new frame types along with a large amount of manually
      labeled training data. In contrast, our distantly supervised
      approach requires only a moderate amount of seed facts and no
      explicit labeling at all. Since SRL is nevertheless closest
      to our approach, the experiments presented in Section 8
      compare our method to the state-of-the-art baseline
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0038">38</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0041">41</a>], which is
      part of the Illinois NLP Curator software [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0009">9</a>].</p>
      <p><strong>Temporal and Spatial Anchoring of Facts:</strong>
      The scope of temporal and spatial anchoring approaches is
      limited to assigning location or time information to facts
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0012">12</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0015">15</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0045">45</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0049">49</a>]. The goal
      of the TAC Knowledge Base Population task on Temporal Slot
      Filling [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0043">43</a>]
      is related to this line of work. The systems for this task
      typically train classifiers with additional constraints, like
      temporal ordering or spatial consistency, which are not
      applicable to a general setting.</p>
      <p><strong>Event Extraction:</strong> Event extraction
      methods identify occurrences of events from a predefined set
      of event types within a text corpus. For example, extraction
      of Movement, Transfer, Creation and Destruction events was a
      task within the Automatic Content Extraction (ACE)
      program&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>]. Named Event Mining distills
      structured event representations from text&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>]. Events
      consist of a topic and multiple entities as actors, but they
      do not include relations between the actors beyond
      participation in the same event. Story mining aims to extract
      structured representations for linking different
      events&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0040">40</a>].
      Here, events are just topics, i.e. potentially ambiguous
      phrases, and links merely connect events without any further
      semantics. This is different to our use case, where clear
      semantics and canonicalization of entities are crucial for
      populating a knowledge base. In the biomedical domain, event
      extraction mostly focuses on binary relations between
      molecular entities, like protein-protein interaction or
      gene-drug relations (e.g., [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0047">47</a>]). Approaches in this area are
      typically based on dependency parsing and supervised
      learning, using different graph similarity kernels [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0033">33</a>].</p>
      <p><strong>N-ary Fact Harvesting:</strong> The Xart
      system&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>]
      applies association rule mining to find highly co-occurring
      entities in dependency parse trees. Since the extracted rules
      require manual validation, the system relies on input by
      domain experts to discover instances of predefined n-ary
      medical relations from text. McDonald et al.&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>] first
      trains a classifier to identify pairs of related entities
      which they use as input to construct a graph of all related
      entities within a sentence. Higher-arity relations then
      correspond to maximal cliques in the graph. The work
      by&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0021">21</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0024">24</a>] applies a
      distantly supervised approach for learning extraction rules
      for n-ary relations from dependency graphs. These rules are
      highly specific and do not generalize well. Consequently, the
      method needs a large number of seed facts: several thousands
      per relation even for simple relations such as marriage (with
      date and place as additional attributes), while achieving
      moderate precision of ca. 50%. Sar-graphs&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>] aggregate
      this style of rules and incorporate lexical knowledge to
      construct an easily re-usable linguistic resource. However,
      this resource is manually constructed and small. None of
      these methods is applicable to our setting with large-scale
      input corpora and a limited amount of distant
      supervision.</p>
      <p>[<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0037">37</a>] presents a
      graph-oriented LSTM neural network for learning how to
      extract ternary relations when the arguments are scattered
      across multiple sentences. However, this method is geared for
      named entities as arguments and does not cover arguments that
      are phrases for quantities (e.g., medical dosages) or general
      concepts (e.g., denominations for awards such as physics,
      medicine, peace, best actor, etc.). Experiments exclusively
      focus on the ternary interaction of genes, drugs and gene
      mutations, and use extensive supervision from high-quality
      knowledge bases.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186000/images/www2018-9-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The HighLife System.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> System
          Overview</h2>
        </div>
      </header>
      <p>The goal of the HighLife system is to harvest n-ary facts
      from text corpora. One key feature is composing higher-arity
      facts from partial observations by joining arguments, e.g.
      one sentence referring to a drug, a disease and a target
      group and another one referring to the same drug, same target
      group, a dosage but not the disease are joined into a single
      fact containing all 4 pieces of information.</p>
      <p></p>
      <div class="definition" id="enc1">
        <label>Definition 1 (fact).</label>
        <p>A fact is an instance of an n-ary relation with a given
        type signature and arity. We write a fact in the form
        <em>R</em>(<em>a</em> <sub>1</sub>, …,
        <em>a<sub>n</sub></em> ) where <em>R</em> is an n-ary
        relation predicate and <em>a</em> <sub>1</sub> through
        <em>a<sub>n</sub></em> are constants (i.e., entities or
        literals including short phrases) of types that fit with
        the type signature of <em>R</em>.</p>
      </div>
      <p></p>
      <p></p>
      <div class="definition" id="enc2">
        <label>Definition 2 (partial fact).</label>
        <p>A partial fact is a fact where some arguments are
        unknown. We write unknown arguments as variables, for
        example <em>R</em>(<em>a</em> <sub>1</sub>, <em>a</em>
        <sub>2</sub>, <em>X</em> <sub>3</sub>, <em>a</em>
        <sub>4</sub>) with variable <em>X</em> <sub>3</sub>.
        Logically, this denotes a formula ∃<em>X</em> <sub>3</sub>:
        <em>R</em>(<em>a</em> <sub>1</sub>, <em>a</em>
        <sub>2</sub>, <em>X</em> <sub>3</sub>, <em>a</em>
        <sub>4</sub>)</p>
      </div>
      <p></p>
      <p></p>
      <div class="example" id="enc3">
        <label>Example 1.</label>
        <p>For the health domain, consider the relation</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;<em>DrugTreatsDisease</em>:
        <em>drug</em> × <em>disease</em> × <em>dosage</em> ×
        <em>targetgroup</em></p>
        <p>An example fact is</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;<em>DrugTreatsDisease(Albuterol,
        exacerbations, 2.5 mg, children)</em>.</p>
        <p>A partial fact could be</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;<em>DrugTreatsDisease(Albuterol,
        exacerbations, Z, children)</em>.</p>
      </div>
      <p></p>
      <p></p>
      <div class="example" id="enc4">
        <label>Example 2.</label>
        <p>For the sports domain, consider the relation</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;<em>AthleteWonAward</em>:
        <em>athlete</em> × <em>award</em> × <em>sport</em> ×
        <em>date</em> × <em>place</em></p>
        <p>An example fact is</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;<em>AthleteWonAward(Kerber,
        Olympic Silver, tennis, 2016, Rio)</em>.</p>
        <p>A partial fact could be</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;<em>AthleteWonAward(Kerber, Y,
        tennis, Z, Rio)</em>.</p>
      </div>
      <p></p>
      <p></p>
      <div class="definition" id="enc5">
        <label>Definition 3 (seed fact).</label>
        <p>A seed fact for relation <em>R</em> is a fact or a
        partial fact whose truth has been verified.</p>
      </div>
      <p></p>
      <p>Figure&nbsp;<a class="fig" href="#fig1">1</a>a gives an
      overview of the HighLife system. To show the versatility of
      the approach, two different domains are considered in our
      experiments, namely health and news. A Named Entity
      Recognition and Disambiguation (NERD) component extracts a
      variety of entities from sentences. To identify fact
      candidates our system then constructs trees from parsed
      dependency graphs spanning over the entities. These trees
      either express a complete fact or have missing entities
      leading to unknown arguments and partial facts. Guided by
      distant supervision using seed facts, the extracted trees are
      analyzed and statistically weighted to determine good n-ary
      fact candidates. A logical consistency reasoner incorporates
      these weighted candidates together with specialized
      consistency rules as well as semantic information from
      knowledge bases to identify a consistent subset of true facts
      with a high total weight. Further, the reasoner composes
      complete facts out of partially expressed fact candidates as
      well as estimates an appropriate weight. The result is a set
      of n-ary facts, where each fact binds arguments that trace
      back to multiple, separate sources in the input texts.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> NERD</h2>
        </div>
      </header>
      <p>The HighLife system incorporates different entity
      recognition and disambiguation components that recognize
      entities from text and link them to knowledge bases. This
      allows us to incorporate a large variety of different kinds
      of entities into our fact extraction. As preprocessing,
      <a class="link-inline force-break" href=
      "http://Stanford%20CoreNLP%20software">Stanford CoreNLP
      software</a> is applied on all texts.</p>
      <p><strong>Biomedical Entities</strong> We rely on the
      Unified Medical Language System (UMLS) as biomedical entity
      dictionary, covering &nbsp; 3,221,702 entities with
      12,842,558 names. To efficiently find matching candidates we
      employ a method using locality sensitive hashing with
      min-wise independent permutations. Type information and
      UMLS'es ranked list of entity preferences are used to
      disambiguate between multiple candidates matched to the same
      noun chunk.</p>
      <p><strong>Quantities</strong> Numerical quantities are
      important quantifiers for many relations. Our system detects
      such information in text using powerful regular expressions
      incorporating entity types, POS tags, words and word classes.
      We developed a small set of expressions to detect quantities
      such as prices, percentages, and measurements among others.
      For instance, the expression</p>
      <p><tt>word:/USD|<font style="normal">$</font>/ [ word:IS_NUM
      | ner:MONEY ]+</tt></p>
      <p>denotes dollar prices such as <em>USD 1 billion</em>.</p>
      <p><strong>YAGO Entities</strong> To recognize and
      disambiguate entities in news we apply the AIDA
      system&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0019">19</a>]
      which links entities to YAGO&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>].</p>
      <p><strong>WordNet Concepts</strong> We apply a most frequent
      sense disambiguation to map remaining noun chunks to concepts
      in WordNet.</p>
      <p><strong>Temporal Expressions</strong> Using Stanford's
      CoreNLP sutime module we detect and normalize time
      expressions.</p>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Tree
          Mining</h2>
        </div>
      </header>
      <p>Our method relies on constructing trees, called pattern
      trees, from typed dependency graphs to identify n-ary fact
      candidates in text. A fact candidate can be fully expressed
      by such a pattern tree or only partially. The goal is to
      construct pattern trees, which describe n-ary facts
      <em>R</em>(<em>e</em> <sub>1</sub>, …, <em>e<sub>n</sub></em>
      ) and reflect their complex structure.</p>
      <p></p>
      <div class="definition" id="enc6">
        <label>Definition 4 (target).</label>
        <p>For a given sentence s with dependency-parse tree
        <em>T</em>(<em>s</em>), the targets are the nodes of
        <em>T</em>(<em>s</em>) denoting arguments of a (partial)
        fact (i.e., entities, quantities, informative phrases).</p>
      </div>
      <p></p>
      <p>We assume that the targets in a sentence are already
      canonicalized whenever appropriate; for example, entity
      mentions are disambiguated into an entity of a KB, quantities
      are normalized, etc. Since targets may actually be multi-word
      phrases, we transform the dependency-parse tree to collapse
      all nodes that constitute a target phrase into a single node.
      This combined node is placed at the position of the phrase's
      head word in the original parse tree.</p>
      <p></p>
      <div class="definition" id="enc7">
        <label>Definition 5 (pairwise paths).</label>
        <p>For sentence s, the set of pairwise paths
        <em>PP</em>(<em>s</em>) contains all parse-tree paths
        linking a pair of targets.</p>
      </div>
      <p></p>
      <p></p>
      <div class="definition" id="enc8">
        <label>Definition 6 (matching tree).</label>
        <p>For sentence s, the matching tree is the parse tree
        reduced to having only the sentence's targets as leaf nodes
        and all pairwise paths.</p>
      </div>
      <p></p>
      <p></p>
      <div class="definition" id="enc9">
        <label>Definition 7 (pattern tree).</label>
        <p>Given a sentence s, the pattern tree
        <em>P</em>(<em>s</em>) is the matching tree with the
        sentence's subject target as the root and all pairwise
        paths that start or end at the root. Common subpaths are
        represented only once (e.g. as constructed in
        Figure&nbsp;<a class="fig" href="#fig1">1</a>b).</p>
      </div>
      <p></p>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186000/images/www2018-9-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">Tree Analysis.</span>
        </div>
      </figure>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Tree
          Analysis</h2>
        </div>
      </header>
      <p>The tree mining results in a large set of harvested
      pattern trees constructed from our input sentences. However,
      these trees are often too large and over-specific, e.g. a
      sentence's pattern tree can contain more entities than there
      are possible arguments for a valid fact. However, a subset of
      the entities and thus a subtree of the pattern tree could
      lead to a true fact. Also, not all internal vertices are
      often needed to express a relation and we only want to
      consider the necessary ones. Figure&nbsp;<a class="fig" href=
      "#fig2">2</a>a shows the contrast between a constructed
      pattern tree and more general subtrees. Consequently, the
      goals of the tree analysis component are to generalize the
      harvested trees, to determine salient seed trees that
      syntactically and lexically express n-ary relations with high
      confidence, and to determine n-ary fact candidates from such
      trees.</p>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span> Salient
            Seed Tree Mining</h3>
          </div>
        </header>
        <p><strong>Tree Generalization</strong> We generalize the
        harvested trees to find salient trees by mining frequent
        subtrees satisfying a given support threshold. For this
        purpose, we adapt the FreeTreeMiner
        algorithm&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0008">8</a>]. We also incorporate lexical and
        semantic information into the tree mining, i.e. if vertices
        in trees do not occur often enough, our algorithm lifts
        them to either their part-of-speech tags, to a general
        wildcard, or to their semantic type.</p>
        <div class="definition" id="enc10">
          <label>Definition 8.</label>
          <p>(pattern subtree) Given a sentence <em>s</em>, the
          pattern subtree <em>PS</em>(<em>s</em>) is a mined
          subtree of the sentence's pattern tree, which only has
          the sentence's targets as leaf nodes and which occurs
          more often than a predefined threshold.</p>
        </div>
        <p><strong>Salient Seed Tree Mining</strong> Relying on a
        set of seed facts we determine seed trees within the mined
        subtrees.</p>
        <div class="definition" id="enc11">
          <label>Definition 9 (seed tree).</label>
          <p>A seed tree for a relation <em>R</em> is a mined
          subtree where the root and leaf nodes are the targets
          (i.e. arguments) of a seed fact. Such a tree could
          represent a partial seed fact by matching the fact only
          partially.</p>
        </div>
        <p>For example, the trees depicted in Figure&nbsp;<a class=
        "fig" href="#fig2">2</a>a are seed trees assuming that
        <em>DrugTreatsDisease(Albuterol, exacerbations, 2.5 mg,
        children)</em> is a seed fact. Furthermore, the more
        strongly a seed tree correlates with arguments of a
        particular relation in the seed facts, the more confident
        we are that the tree expresses the relation. To describe
        this intuition we rely on the following criteria:</p>
        <div class="definition" id="enc12">
          <label>Definition 10 (support).</label>
          <p>Given a corpus of sentences <em>S</em> and a set of
          entity tuples <em>X</em>, the support of a subtree
          <em>PS</em>(<em>s</em>) is computed as:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray*}
              support(PS(s),X) =\\ |\lbrace s \in S ~|~ \exists
              (e_1, \ldots , e_n) \in X \wedge PS(s) \ has\ root \
              e_1 \ and\ leafs \ e2, \ldots , e_n\rbrace
              |\end{eqnarray*}</span><br />
            </div>
          </div>
          <p></p>
        </div>
        <div class="definition" id="enc13">
          <label>Definition 11 (confidence).</label>
          <p>The confidence of a subtree <em>PS</em>(<em>s</em>)
          for a relation <em>R</em> is:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation*}
              confidence(PS(s)) =
              \frac{support(PS(s),SX(R))}{support(PS(s), SX(R)\cup
              CX(R))}\end{equation*}</span><br />
            </div>
          </div>where <em>SX</em>(<em>R</em>) denotes the set
          covering all entity tuples of true facts of relation
          <em>R</em> in our seed facts, and <em>CX</em>(<em>R</em>)
          denotes negative entity tuples, i.e. valid arguments of a
          relation, not leading to a true fact.
          <p></p>
        </div>
        <div class="definition" id="enc14">
          <label>Definition 12.</label>
          <p>(salient seed tree) A salient seed tree is a mined
          subtree having a confidence larger than a specific
          threshold. A few example salient seed trees can be seen
          in Figure&nbsp;<a class="fig" href="#fig2">2</a>b .</p>
        </div>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span> Partial
            N-ary Fact Candidates</h3>
          </div>
        </header>
        <p>An n-ary fact candidate is a mined subtree occurring
        with a tuple of entities together with a derived weight
        describing the goodness that the tree expresses a
        particular relation. The candidates do not need to express
        facts completely, in which case they lead to partial fact
        candidates. To quantify the weight of a subtree to be a
        fact candidate, the tree is matched against the salient
        seed trees. The derived matching similarity is used as
        weight stating the confidence that the tree expresses a
        particular relation.</p>
        <p><strong>Tree Matching</strong> A sufficient condition
        for two trees to be matched is that they have the same
        number of leaf nodes. To define a similarity measure
        between trees we split them into their pairwise paths (see
        Definition&nbsp;<a class="enc" href="#enc7">5</a> and
        Figure&nbsp;<a class="fig" href="#fig1">1</a>b), introduce
        a similarity between such paths as in
        Definition&nbsp;<a class="enc" href="#enc15">13</a>, and
        then aggregate the gathered scores as described below in
        Definition&nbsp;<a class="enc" href="#enc16">14</a>.</p>
        <p>First, we introduce 1 <sub><em>T</em></sub> (<em>v</em>
        <sub>1</sub>, <em>v</em> <sub>2</sub>) as the function that
        indicates if two vertices are equal.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation*} 1_T(v_1,
            v_2)= {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}1,
            \text{if }v_1 \text{ and } v_2 \text{ are equal}\\ 0,
            \text{otherwise}
            \end{array}\right.}\end{equation*}</span><br />
          </div>
        </div>
        <p></p>
        <div class="definition" id="enc15">
          <label>Definition 13 (similarity between pairwise
          paths).</label>
          <p>The similarity <em>sim<sub>P</sub></em> (<em>p</em>
          <sub>1</sub>, <em>p</em> <sub>2</sub>) between two paths
          <em>p</em> <sub>1</sub> and <em>p</em> <sub>2</sub>, is
          defined as:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation*}
              sim_P(p_{1},p_{2}) = {\left\lbrace
              \begin{array}{@{}l@{\quad }l@{}}\frac{\sum
              _{i=0}^{|p_{1}|} 1_T(p_{1_i}, p_{2_i})}{|p_{1}|},
              \text{if }|p_{1}| = |p_{2}| \\ 0, \text{otherwise}
              \end{array}\right.}\end{equation*}</span><br />
            </div>
          </div>where | · | denotes the length of a path.
          <p></p>
        </div>
        <div class="definition" id="enc16">
          <label>Definition 14 (similarity between trees).</label>
          <p>Given two trees <em>t</em> <sub>1</sub>, <em>t</em>
          <sub>2</sub> with possible arrangements of their pairwise
          paths <span class="inline-equation"><span class=
          "tex">$P_1 = (p_{1_1}, \ldots , p_{1_n})$</span></span>
          and <span class="inline-equation"><span class="tex">$P_2
          = (p_{2_1}, \ldots , p_{2_n})$</span></span> we define a
          similarity measure between the two trees as follows:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation*}
              sim_T(t_1,t_2) = \mathrm{argmax}_{P_1, P_2} \prod
              _{i=1}^{n} sim_P(p_{1_i},
              p_{2_i})\end{equation*}</span><br />
            </div>
          </div>
          <p></p>
        </div>
        <p>All mined subtrees are matched against the salient seed
        trees. An n-ary fact candidate is a matched subtree
        occurring with an entity tuple in an input sentence, and
        has an associated weight, which is a combination of the
        matching similarity and confidence of the salient seed
        tree.</p>
        <p>The resulting candidate set can be formally described
        as:</p>
        <div class="definition" id="enc17">
          <label>Definition 15 (n-ary fact candidates).</label>
          <p>For a set of sentences <em>S</em> and a set of salient
          seed trees <em>Q</em>, the n-ary tree fact candidate
          multi-set <em>C</em>(<em>S</em>, <em>Q</em>) is:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{align*} C(S, Q) =&amp;
              \lbrace (PS(s), e_1, \ldots , e_n)[w] | \exists s \in
              S: \\ &amp; PS(s) \ has\ root \ e_1 \ and\ leafs \
              e2, \ldots , e_n \wedge \\ &amp; w = \max \lbrace
              sim_T(PS(s), q) \times confidence(q) | q \in Q
              \rbrace \rbrace\end{align*}</span><br />
            </div>
          </div>
          <p></p>
        </div>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">HighLife Predicates.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Source</th>
                <th style="text-align:left;">Predicate</th>
                <th style="text-align:left;">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Textual</td>
                <td style="text-align:left;">
                <em>Express</em>(<em>T</em>, <em>R</em>)</td>
                <td style="text-align:left;">tree <em>T</em>
                expresses relation <em>R</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">Evidence</td>
                <td style="text-align:left;">
                <em>Occur</em>(<em>T</em>, <em>X</em> <sub>1</sub>,
                …, <em>X<sub>n</sub></em> )</td>
                <td style="text-align:left;"><em>T</em> occurs with
                <em>n</em> entities in text</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;">Relation</td>
                <td style="text-align:left;">
                <em>Type</em>(<em>X</em>, <em>S</em>)</td>
                <td style="text-align:left;">type <em>S</em> of an
                entity <em>E</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">Properties</td>
                <td style="text-align:left;">
                <em>Sig</em>(<em>R</em>, <em>S</em> <sub>1</sub>,
                …, <em>S<sub>n</sub></em> )</td>
                <td style="text-align:left;">argument type
                signature</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">of an n-ary relation
                <em>R</em></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;"></td>
                <td style="text-align:left;">
                <em>OrganPartOf</em>(<em>X</em>, <em>Y</em>)</td>
                <td style="text-align:left;">organ <em>X</em> is
                part of</td>
              </tr>
              <tr>
                <td style="text-align:left;">Domain</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">organ <em>Y</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">Knowledge</td>
                <td style="text-align:left;">
                <em>GroupInCountry</em>(<em>X</em>,
                <em>Y</em>)</td>
                <td style="text-align:left;">ethnic group
                <em>X</em> lives</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">...</td>
                <td style="text-align:left;">in country
                <em>Y</em></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;">Derived</td>
                <td style="text-align:left;">
                <em>CompanyAcquired</em>(<em>X</em> <sub>1</sub>,
                <em>X</em> <sub>2</sub>, <em>X</em> <sub>3</sub>,
                <em>X</em> <sub>4</sub>, <em>X</em>
                <sub>5</sub>)</td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">Output</td>
                <td style="text-align:left;">
                <em>Diagnoses</em>(<em>X</em> <sub>1</sub>,
                <em>X</em> <sub>2</sub>, <em>X</em>
                <sub>3</sub>)</td>
                <td style="text-align:left;">N-ary fact
                hypotheses</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">...</td>
                <td></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Consistency Constraints.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Type</th>
                <th style="text-align:left;">Rule</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Tree pattern-fact
                duality</td>
                <td style="text-align:left;">
                <em>Express</em>(<em>T</em>,
                <em>R</em>)∧<em>Occur</em>(<em>T</em>, <em>X</em>
                <sub>1</sub>, …, <em>X<sub>n</sub></em>
                )∧<em>Sig</em>(<em>R</em>, <em>S</em> <sub>1</sub>,
                …, <em>S<sub>n</sub></em>
                )∧<em>Type</em>(<em>X</em> <sub>1</sub>, <em>S</em>
                <sub>1</sub>)∧…∧<em>Type</em>(<em>X<sub>n</sub></em>
                , <em>S<sub>n</sub></em> )</td>
              </tr>
              <tr>
                <td style="text-align:center;">(Fact Hypotheses
                Generation)</td>
                <td style="text-align:left;">⇒<em>R</em>(<em>E</em>
                <sub>1</sub>, …, <em>E<sub>n</sub></em> )</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">Mutual
                Exclusion</td>
                <td style="text-align:left;">
                <em>Causes</em>(<em>X</em> <sub>1</sub>, <em>X</em>
                <sub>2</sub>, <em>X</em> <sub>3</sub>, <em>X</em>
                <sub>4</sub>)⇒¬<em>Treats</em>(<em>X</em>
                <sub>1</sub>, <em>X</em> <sub>2</sub>, <em>X</em>
                <sub>3</sub>, <em>X</em> <sub>4</sub>)</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left;">
                <em>CompanyAcquired</em>(<em>X</em> <sub>1</sub>,
                <em>X</em> <sub>2</sub>, <em>X</em> <sub>3</sub>,
                <em>X</em> <sub>4</sub>, <em>X</em>
                <sub>5</sub>)⇒¬<em>CompanyAcquired</em>(<em>X</em>
                <sub>2</sub>, <em>X</em> <sub>1</sub>, <em>X</em>
                <sub>3</sub>, <em>X</em> <sub>4</sub>, <em>X</em>
                <sub>5</sub>)</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">Domain
                Constraints</td>
                <td style="text-align:left;">
                <em>Diagnoses</em>(<em>X</em> <sub>1</sub>,
                <em>X</em> <sub>2</sub>, <em>X</em>
                <sub>3</sub>)∧<em>Diagnoses</em>(<em>X</em>
                <sub>1</sub>, <em>X</em> <sub>2</sub>, <em>Y</em>
                <sub>3</sub>)⇒<em>OrganPartOf</em>(<em>X</em>
                <sub>3</sub>, <em>Y</em> <sub>3</sub>)</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left;">...</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">Equality
                Constraints</td>
                <td style="text-align:left;">
                <em>AthleteWonAward</em>(<em>X</em> <sub>1</sub>,
                <em>X</em> <sub>2</sub>, <em>X</em> <sub>3</sub>,
                <em>X</em> <sub>4</sub>, <em>X</em> <sub>5</sub>,
                <em>X</em>
                <sub>6</sub>)∧<em>AthleteWonAward</em>(<em>X</em>
                <sub>1</sub>, <em>X</em> <sub>2</sub>, <em>Y</em>
                <sub>3</sub>, <em>X</em> <sub>4</sub>, <em>X</em>
                <sub>5</sub>, <em>X</em> <sub>6</sub>)⇒<em>E</em>
                <sub>3</sub> = <em>Y</em> <sub>3</sub></td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left;">
                <em>CompanyAcquired</em>(<em>X</em> <sub>1</sub>,
                <em>X</em> <sub>2</sub>, <em>X</em> <sub>3</sub>,
                <em>X</em> <sub>4</sub>, <em>X</em>
                <sub>5</sub>)∧<em>CompanyAcquired</em>(<em>X</em>
                <sub>1</sub>, <em>X</em> <sub>2</sub>, <em>X</em>
                <sub>3</sub>, <em>Y</em> <sub>4</sub>, <em>X</em>
                <sub>5</sub>)⇒<em>E</em> <sub>4</sub> = <em>Y</em>
                <sub>4</sub></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Consistency
          Reasoning</h2>
        </div>
      </header>
      <p>The fact candidate multi-set describes weighted trees,
      which potentially lead to full or partial facts. HighLife
      uses consistency rules to determine when such a tree becomes
      a true n-ary fact, i.e. the rules prune false positives out
      of the set of n-ary fact candidates and their supporting tree
      patterns provided by the tree analysis.</p>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.1</span> Consistency
            Constraints</h3>
          </div>
        </header>
        <p>Consistency constraints are manually encoded as rules
        that are composed of multiple different predicates. A
        predicate (see Table&nbsp;<a class="tbl" href=
        "#tab1">1</a>) can describe evidence extracted from text,
        logical relation properties, domain knowledge from a
        knowledge base using schema information, or it is derived
        as a result of executing a rule. The rules enforce
        consistency over the set of fact candidates and handle
        conflicting candidates. We rely on the different types of
        consistency constraints, shown in Table&nbsp;<a class="tbl"
        href="#tab2">2</a>. Tree pattern-fact duality constraints
        describe when a tree pattern candidate becomes a fact
        candidate. Mutual exclusion constraints between relations
        rule out different fact candidates, which overlap in their
        arguments but conflict in their relations. Domain
        constraints restrict possible results by incorporating
        prior domain knowledge. Rules can also impose equality
        restrictions, specifying when arguments of two different
        facts are equal. Such constraints could express that facts
        making statements about the same athlete winning a medal on
        the same date must overlap in the athlete's type of
        sport.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.2</span> Partial
            Fact Reasoning</h3>
          </div>
        </header>
        <p>To reason with the aforementioned constraints, we ground
        the rules into weighted logical clauses. The clauses’
        weights are derived from the weights of the tree analysis
        phase. The goal is to compute a consistent subset of
        clauses with the largest total weight, i.e. to identify a
        subset of most plausible fact candidates. This task can be
        cast into a Weighted Max-Sat problem&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0042">42</a>]. However, facts can
        have unknown arguments (partial facts), which cannot be
        handled by the weighted Max-Sat solver. The problem of
        determining constants for <em>X</em> and <em>Y</em> and
        groundings for unknown arguments in partial facts can be
        reduced to a unification problem between logical
        literals.</p>
        <div class="example" id="enc18">
          <label>Example 3.</label>
          <p>The partially grounded fact candidates</p>
          <p><em>AthleteWonAward</em>(<em>Kerber</em>,
          <em>OlympicSilver</em>, <em>tennis</em>, 2016,
          <em>X</em>)</p>
          <p><em>AthleteWonAward</em>(<em>Kerber</em>,
          <em>OlympicSilver</em>, <em>Y</em>, 2016, <em>Rio</em>)
          etc.</p>
          <p>could be applied to the following formula:</p>
          <p>∃<em>X</em>,
          <em>YAthleteWonAward</em>(<em>Kerber</em>,
          <em>OlympicSilver</em>, <em>tennis</em>, 2016,
          <em>X</em>)∧</p>
          <p><em>AthleteWonAward</em>(<em>Kerber</em>,
          <em>OlympicSilver</em>, <em>Y</em>, 2016,
          <em>Rio</em>)</p>
        </div>
        <p>We unify two literals, i.e. partial n-ary fact
        candidates, if we can find a substitution between them, a
        mapping assigning constants to unknown arguments of partial
        facts. We use equality constraints defined as consistency
        rules to determine when arguments of two partial facts can
        be considered equal and thus can be substituted. For
        example, using a constraint which expresses that an athlete
        cannot win medals in more than one sports discipline on the
        same date, we can determine that <em>Y</em> can only be
        substituted with <em>tennis</em> in Example&nbsp;<a class=
        "enc" href="#enc18">3</a>. Exploiting these constraints for
        defining equivalences, we implement the algorithm mentioned
        in [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
        to find most general unifiers between logical literals.
        This enables us to unify partially grounded fact candidates
        resulting in new fully grounded clauses. This unification
        combines information scattered in separate textual sources
        into a single, full-fledged n-ary fact, e.g. by
        substituting <em>X</em> and <em>Y</em> with constants
        (<em>Rio</em> for <em>X</em>, tennis for <em>Y</em>), we
        obtain the clause:</p>
        <p>
        &nbsp;&nbsp;&nbsp;&nbsp;<em>AthleteWonAward</em>(<em>Kerber</em>,
        <em>OlympicSilver</em>, <em>tennis</em>, 2016,
        <em>Rio</em>)</p>
        <p>However, we need to assign a weight to the clause to use
        it in the reasoning. The weights for the partial candidates
        correspond to observations of marginals over a 5-variate
        distribution. We need to estimate the hypothetical
        frequency for the full clause. In the absence of any other
        information, we can use a maximum-entropy estimator. This
        estimation problem is isomorphic to the cardinality
        estimation issue over multivariate datasets [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0026">26</a>]. However, not all
        partial facts can be unified into fully grounded clauses.
        Therefore, we introduce special unknown arguments as
        placeholders to ground such facts. Due to the NP-hardness
        of the Weighted Max-Sat problem, we use an approximation
        algorithm (SOFIE&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0042">42</a>]) to reason over the created
        hypotheses space of grounded and weighted clauses which
        produces a set of n-ary facts we accept as plausible.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">HighLife's Harvested Relations with Type
            Signatures.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Domain</th>
                <th style="text-align:left;">Relation</th>
                <th style="text-align:center;">Arity</th>
                <th style="text-align:left;">Signature</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Biomedical</td>
                <td style="text-align:left;"><em>Treats</em></td>
                <td style="text-align:center;">5</td>
                <td style="text-align:left;"><em>Drug</em> ×
                <em>Disease</em> × <em>Dosage</em> ×
                <em>DosageForm</em> × <em>Targetgroup</em></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">ReducesRisk</td>
                <td style="text-align:center;">4</td>
                <td style="text-align:left;">
                (<em>Drug</em>∪<em>Behavior</em>∪<em>Ecofactor</em>)
                × <em>Disease</em> × <em>Targetgroup</em> ×
                <em>Condition</em></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">Causes</td>
                <td style="text-align:center;">4</td>
                <td style="text-align:left;"><em>Disease</em> ×
                <em>Disease</em> × <em>Targetgroup</em> ×
                <em>Condition</em></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">Diagnoses</td>
                <td style="text-align:center;">3</td>
                <td style="text-align:left;">
                <em>DiagnosticProcedure</em> × <em>Disease</em> ×
                (<em>BodyPart</em>∪<em>Organ</em>)</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;">News</td>
                <td style="text-align:left;">
                <em>AthleteWonAward</em></td>
                <td style="text-align:center;">6</td>
                <td style="text-align:left;"><em>Athlete</em> ×
                <em>Award</em> × <em>TypeOfSport</em> ×
                <em>Event</em> × <em>Location</em> ×
                <em>Time</em></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">
                <em>CompanyAcquired</em></td>
                <td style="text-align:center;">5</td>
                <td style="text-align:left;"><em>Organization</em>
                × <em>Organization</em> × <em>Date</em> ×
                <em>Price</em> × <em>Organization</em></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span>
          Experiments</h2>
        </div>
      </header>
      <p>For empirical studies of the viability and comparative
      performance of our HighLife method, we designed various
      experiments using input texts and target relations from two
      domains: general news (on business, sports, etc.) and
      biomedical health. First, we compare HighLife to a
      state-of-the-art SRL baseline (Subsection 8.2). Second, we
      test the scalability of HighLife on two large corpora
      (Subsection 8.3). Third, we perform an ablation study with
      various components of HighLife enabled or disabled
      (Subsection 8.4). We start this section by discussing the
      general setup for these experiments.</p>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">8.1</span> Setup</h3>
          </div>
        </header>
        <p><strong>Datasets.</strong> We run experiments on two
        different input corpora:</p>
        <ul class="list-no-style">
          <li id="list5" label="•">
            <strong>News articles:</strong> a large collection of
            news articles, compiled from the STICS
            project&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0017">17</a>] and the New York Times
            archive.<br />
          </li>
          <li id="list6" label="•">
            <strong>Biomedical texts:</strong>a large and diverse
            collection of documents on biomedicine and health,
            consisting of i) PubMed abstracts (Medline) as well as
            entire articles (Central) with scientific content and
            specialized jargon, and ii) Web portals and
            encyclopedic articles (from MayoClinic, Wikipedia,
            etc.) with information geared for patients and doctors
            (see [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0014">14</a>]).<br />
          </li>
        </ul>
        <p>Table <a class="tbl" href="#tab4">4</a> shows the size
        and other properties of these corpora. HighLife is able to
        harvest n-ary relations as long as seed facts are provided.
        For our evaluation, we selected a set of relations with
        different arities, ranging from ternary to 6-ary. Table
        <a class="tbl" href="#tab3">3</a> gives an overview of
        these relations, and Table <a class="tbl" href=
        "#tab10">10</a> shows sample facts extracted by HighLife. A
        relation's arity is defined by its type signature and
        provided seed facts. The composition of the selected
        relations is described in Subsections <a class="sec" href=
        "#sec-17">8.2</a> and <a class="sec" href=
        "#sec-20">8.3.2</a>.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Text Corpora for Experiments.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th>Domain</th>
                <th style="text-align:center;">Genre</th>
                <th>Source</th>
                <th>Documents</th>
                <th>Sentences</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td></td>
                <td></td>
                <td>Drugs.com</td>
                <td>31,837</td>
                <td>7,586,236</td>
              </tr>
              <tr>
                <td></td>
                <td>Encyclopedic</td>
                <td style="text-align:center;">Mayo Clinic</td>
                <td>2,166</td>
                <td>570,325</td>
              </tr>
              <tr>
                <td>Biomedical</td>
                <td style="text-align:center;">Articles</td>
                <td>Medline Plus</td>
                <td>3,076</td>
                <td>197,055</td>
              </tr>
              <tr>
                <td></td>
                <td></td>
                <td>RxList</td>
                <td>2,515</td>
                <td>1,102,791</td>
              </tr>
              <tr>
                <td></td>
                <td></td>
                <td>Wikipedia Health</td>
                <td>20,893</td>
                <td>787,148</td>
              </tr>
              <tr>
                <td></td>
                <td style=
                "border-top: solid 2px; text-align:center;">
                Scientific</td>
                <td style="border-top: solid 2px">PubMed
                Medline</td>
                <td style="border-top: solid 2px">580,892</td>
                <td style="border-top: solid 2px">5,875,006</td>
              </tr>
              <tr>
                <td></td>
                <td style="text-align:center;">Publications</td>
                <td>PubMed Central</td>
                <td>12,532</td>
                <td>2,765,580</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td></td>
                <td style="text-align:center;">STICS Corpus</td>
                <td>1,462,294</td>
                <td>30,252,627</td>
                <td></td>
              </tr>
              <tr>
                <td></td>
                <td style="text-align:center;">News</td>
                <td>New York Times</td>
                <td>1,407,299</td>
                <td>82,934,909</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td></td>
                <td></td>
                <td><strong>Total</strong></td>
                <td><strong>3,523,504</strong></td>
                <td><strong>132,071,677</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p><strong>Evaluation Metrics.</strong> To assess the
        quality and coverage of the knowledge bases that HighLife
        can automatically build, we i) evaluate the correctness of
        randomly sampled facts and ii) report on the size of
        large-scale extractions (i.e., the number of extracted
        facts per relation). The former is a precision measure,
        aggregated over all samples per relation. The latter can be
        seen as a proxy for recall. Note that the actual recall, in
        the sense of IR evaluations, cannot be estimated as it
        would require annotating a large number of entire documents
        with their maximally extractable facts. We also discuss the
        impact of the arity of facts (i.e., the number of extracted
        arguments) on the resulting precision.</p>
        <p><strong>Ground Truth via Crowdsourcing.</strong> To
        gather human judgments of extraction correctness and
        conduct unbiased experiments, we utilized crowdsourcing
        through the CrowdFlower platform.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186000/images/www2018-9-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">CrowdFlower Task.</span>
          </div>
        </figure>
        <p></p>
        <p>To assess an extracted fact by judges of the
        crowdworkers pool we turn every fact into a short
        questionnaire, asking the judge if the fact is true or
        false. We provide two kinds of evidence to the judges: i)
        the textual sources from our input corpus where the fact
        was extracted from, and ii) additional descriptions of the
        entities appearing as fact arguments. Figure&nbsp;<a class=
        "fig" href="#fig3">3</a> shows an example for the
        CrowdFlower task on a candidate fact for the relation
        <em>CompanyAcquired</em>.</p>
        <p>We took several measure for quality assurance. First, we
        designed a set of test questions for every task, which are
        prejudged and cross-checked with external sources by
        ourselves. Second, we balanced the numbers of true and
        false candidate facts shown to judges, so that crowdworkers
        were not biased towards quickly guessing the assessment. To
        prevent judges from giving superficial results without
        carefully reading the question and context, we specifically
        included test questions with false components in the
        candidate facts: differences in the numerical quantities,
        textual statements that contain negations, and entities
        that spuriously co-occur without any real relationship. We
        paid 0.5 cents for each judgement on business and sports
        news, and 0.83 cents for each judgement on biomedical
        health (the latter requiring more expertise and careful
        reading).The final ground truth for the samples to be
        assessed was determined by a weighted voting scheme among
        the judgements for each sample. The weights were
        proportional to the confidence of each judge, derived from
        the test questions with prejudged truth. On average, each
        sampled fact was assessed by three crowdworkers.</p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">8.2</span> Evaluation
            of Extraction Quality</h3>
          </div>
        </header>
        <p>In this subsection, we compare the extraction quality of
        HighLife against a state-of-the-art SRL system. We focus on
        the two relations from general news articles:
        <em>CompanyAcquired</em> and <em>AthleteWonAward</em>, for
        which the SRL system has high-quality frame types and has
        been intensively trained on. For the biomedical relations,
        it would be unfair to the baseline to compare HighLife
        against SRL without specific engineering and training. We
        evaluate the precision of the extracted facts, for varying
        arities (by ignoring some arguments of the relations),
        based on samples assessed by the CrowdFlower judges (see
        Subsection 8.1).</p>
        <p><strong>Seed Facts.</strong> For the relation
        <em>CompanyAcquired</em>, the Freebase knowledge base
        (formerly run by Google, now no longer online) provides us
        with ternary seed facts: the acquiring and the acquired
        company as well as the date of the acquisition. We manually
        extended these ternary facts to 5 arguments by
        incorporating acquisition prices and including the previous
        owner of an acquired company. For the relation
        <em>AthleteWonAward</em> , we gathered the seed facts from
        the WikiData knowledge base. WikiData stores the events
        (e.g. 2016 Summer Olympics) an athlete participates in
        together with medals won and the specific date. Combining
        this with other WikiData facts, such as the type of sport
        an athlete performs and the location of the event (e.g. Rio
        for the 2016 Olympics), we constructed instances of the
        6-ary <em>AthleteWonAward</em> relation</p>
        <p>Overall, we compiled 593 binary, 279 ternary, 45
        quaternary, and 3 quintary seed facts, together with 42
        binary and 28 ternary negative seed facts manually defined.
        Note that no 6-ary facts were spotted in any of the
        sentences of the corpus. However, HighLife can still
        extract 6-ary facts by combining lower-arity facts from
        different sentences in the reasoning stage.</p>
        <p><strong>Competitors.</strong> As discussed in Sections
        <a class="sec" href="#sec-4">1</a> and <a class="sec" href=
        "#sec-5">2</a>, Semantic Role Labeling (SRL) is the prior
        work most related to HighLife. Therefore, we selected the
        state-of-the-art SRL system of the UIUC Illinois NLP
        Curator software [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>] as our baseline (for the software
        and an online demo, see
        <tt>cogcomp.org/page/software_view/Curator</tt>). The
        system integrates named entity recognition and
        disambiguation [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0039">39</a>] and nominal relation modeling
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0041">41</a>] into SRL
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0038">38</a>].</p>
        <p>The target relations are mapped to frames in PropBank
        (which is the basis for SRL) as follows.
        <em>CompanyAcquired</em> is modeled by the Propbank roleset
        <em>acquisition.01</em>, with five argument slots
        corresponding to the arguments of the HighLife relation.
        For <em>AthleteWonAward</em>, by disregarding the third
        argument (<em>TypeOfSport</em>), we are able to map it to
        the roleset <em>win</em>.01 in PropBank. This makes our
        relations compatible with the SRL frames.</p>
        <p>Since SRL methods and HighLife are still not fully
        comparable, we further added two pre-processing steps to
        the SRL system, giving it additional benefits. First, we
        increase the coverage of rolesets by considering all
        predicates that i) specify the same type of roles and ii)
        fall into the same verb classes as defined by PropBank. For
        example, for acquisitions, we manually incorporated also
        the predicates <em>buy</em>, <em>purchase</em> and
        <em>get</em> and their respective frame types. Second, we
        restrict the input in the experiment to sentences in the
        corpus where at least two possible arguments of the
        relation are mentioned. For example, sentences mentioning
        two companies are candidates for <em>CompanyAcquired</em>,
        and sentences mentioning an athlete and a medal are
        candidates for <em>AthleteWonAward</em>. We further
        implemented an extended version of the SRL system, by
        incorporating type constraints for the candidate
        extractions, thus giving SRL more power closer to what
        HighLife does.</p>
        <p>In the following we present results for four
        competitors:</p>
        <ul class="list-no-style">
          <li id="list7" label="•">SRL: the native SRL system (with
          the pre-processing steps added as benefit),<br /></li>
          <li id="list8" label="•">SRL-T: the extended SRL with
          type constraints,<br /></li>
          <li id="list9" label="•">HighLife-Full: the full-fledge
          HighLife extractor,<br /></li>
          <li id="list10" label="•">HighLife-NT: the HighLife
          extractor without type constraints (making HighLife as
          type-agnostic as the native SRL).<br /></li>
        </ul>
        <p><strong>Results.</strong> Table <a class="tbl" href=
        "#tab5">5</a> shows the results of this comparison. For
        each of the two relations, 500 samples were evaluated by
        crowdsourcing.</p>
        <p>The different columns for precision refer to different
        arities of the two target relations. We projected the
        extracted facts onto subsets of their arguments. Smaller
        arities focus on the main arguments (e.g., the acquiring
        and the acquired company and the date, but ignoring the
        price); so smaller arities are easier to extract
        correctly.</p>
        <p>The results in Table <a class="tbl" href="#tab5">5</a>
        show that SRL in its type-extended variant SRL-T performs
        well for lower arities. For <em>CompanyAcquired</em> SRL-T
        is even the best system when focusing only on the 2 or 3
        main arguments of the relation. For
        <em>AthleteWonAward</em> it is slightly better than
        HighLife for the cases of 3 and 4 arguments. SRL without
        type awareness is substantially inferior to all other
        competitors.</p>
        <p>HighLife-Full consistently performs close to the best
        competitor, and is the clear winner when all arguments of
        the relations are to be extracted. In these full-arity
        cases, both of the SRL variants degrade. For example, for
        the 5-ary <em>CompanyAcquired</em> relation, the native SRL
        extracts only incorrect facts – hence precision 0.0; the
        type-enhanced SRL-T does not yield any output facts at all
        in this case. The type-agnostic HighLife-NT also drops
        significantly in output quality compared to HighLife-Full,
        but mostly stays at a reasonable level.</p>
        <p>Overall, HighLife-Full shows its robustness and
        superiority over the SRL approach, although SRL is given
        the benefits of pre-filtered sentences and even when it is
        extended with type constraints.</p>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">Comparison of HighLife against SRL
            Baselines.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th>Relation by</th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:center;">Extracted
                Arguments</th>
                <th style="text-align:left; border-right: double">
                System</th>
                <th style="text-align:center;">Precision</th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:left; border-right: double">
                </th>
                <th style="text-align:center;">2</th>
                <th style="text-align:center;">3</th>
                <th style="text-align:center;">4</th>
                <th>5</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">AthleteWonAward</td>
                <td style="text-align:left; border-right: double">
                HighLife-FULL</td>
                <td style="text-align:center;">0.80</td>
                <td style="text-align:center;">0.80</td>
                <td style="text-align:center;">0.81</td>
                <td>0.81</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left; border-right: double">
                HighLife-NT</td>
                <td style="text-align:center;">0.37</td>
                <td style="text-align:center;">0.70</td>
                <td style="text-align:center;">0.66</td>
                <td>0.00</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left; border-right: double">
                SRL-T</td>
                <td style="text-align:center;">0.68</td>
                <td style="text-align:center;">0.86</td>
                <td style="text-align:center;">0.86</td>
                <td>0.67</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left; border-right: double">
                SRL</td>
                <td style="text-align:center;">0.47</td>
                <td style="text-align:center;">0.38</td>
                <td style="text-align:center;">0.20</td>
                <td>0.00</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">CompanyAcquired</td>
                <td style="text-align:left; border-right: double">
                HighLife-FULL</td>
                <td style="text-align:center;">0.70</td>
                <td style="text-align:center;">0.77</td>
                <td style="text-align:center;">0.88</td>
                <td>0.88</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left; border-right: double">
                HighLife-NT</td>
                <td style="text-align:center;">0.23</td>
                <td style="text-align:center;">0.53</td>
                <td style="text-align:center;">0.78</td>
                <td>0.83</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left; border-right: double">
                SRL-T</td>
                <td style="text-align:center;">0.88</td>
                <td style="text-align:center;">0.87</td>
                <td style="text-align:center;">0.78</td>
                <td>-</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left; border-right: double">
                SRL</td>
                <td style="text-align:center;">0.39</td>
                <td style="text-align:center;">0.20</td>
                <td style="text-align:center;">0.08</td>
                <td>0.00</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class=
            "table-title">Precision in the Biomedical
            Domain.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left; border-right: double">
                Relation by</th>
                <th style="text-align:center;">Precision</th>
                <th style="text-align:center;">
                <strong>Micro</strong></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:left; border-right: double">
                Extracted Arguments</th>
                <th style="text-align:center;">2</th>
                <th style="text-align:center;">3</th>
                <th style="text-align:center;">4</th>
                <th style="text-align:center;">5</th>
                <th><strong>Average</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left; border-right: double">
                <em>Treats</em></td>
                <td style="text-align:center;">0.85</td>
                <td style="text-align:center;">0.85</td>
                <td style="text-align:center;">0.93</td>
                <td style="text-align:center;">1.00</td>
                <td><strong>0.86</strong></td>
              </tr>
              <tr>
                <td style="text-align:left; border-right: double">
                <em>ReducesRisk</em></td>
                <td style="text-align:center;">0.81</td>
                <td style="text-align:center;">0.83</td>
                <td style="text-align:center;">0.99</td>
                <td style="text-align:center;"></td>
                <td><strong>0.82</strong></td>
              </tr>
              <tr>
                <td style="text-align:left; border-right: double">
                <em>Causes</em></td>
                <td style="text-align:center;">0.80</td>
                <td style="text-align:center;">0.80</td>
                <td style="text-align:center;">0.85</td>
                <td style="text-align:center;"></td>
                <td><strong>0.80</strong></td>
              </tr>
              <tr>
                <td style="text-align:left; border-right: double">
                <em>Diagnoses</em></td>
                <td style="text-align:center;">0.88</td>
                <td style="text-align:center;">0.96</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;"></td>
                <td><strong>0.89</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab7">
          <div class="table-caption">
            <span class="table-number">Table 7:</span> <span class=
            "table-title">Harvested Facts in the Biomedical
            Domain.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Relation by</th>
                <th style="text-align:left;">#Facts</th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:left;">Extracted
                Arguments</th>
                <th style="text-align:left;">2</th>
                <th style="text-align:left;">3</th>
                <th style="text-align:left;">4</th>
                <th style="text-align:left;">5</th>
                <th style="text-align:left;">
                <strong>Sum</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;"><em>Treats</em></td>
                <td style="text-align:left;">10,472</td>
                <td style="text-align:left;">3004</td>
                <td style="text-align:left;">198</td>
                <td style="text-align:left;">5</td>
                <td style="text-align:left;">
                <strong>13,769</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <em>ReducesRisk</em></td>
                <td style="text-align:left;">5,339</td>
                <td style="text-align:left;">1,541</td>
                <td style="text-align:left;">72</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">
                <strong>6,952</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"><em>Causes</em></td>
                <td style="text-align:left;">21,254</td>
                <td style="text-align:left;">2,517</td>
                <td style="text-align:left;">70</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">
                <strong>23,841</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <em>Diagnoses</em></td>
                <td style="text-align:left;">5,607</td>
                <td style="text-align:left;">1,170</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">
                <strong>6,777</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab8">
          <div class="table-caption">
            <span class="table-number">Table 8:</span> <span class=
            "table-title">HighLife Ablation Study Precision.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Relation by</th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:left;">Extracted
                Arguments</th>
                <th style="text-align:left; border-right: double">
                HighLife</th>
                <th style="text-align:center;">Precision</th>
                <th style="text-align:center;">
                <strong>Micro</strong></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left; border-right: double">
                Config.</th>
                <th style="text-align:center;">2</th>
                <th style="text-align:center;">3</th>
                <th style="text-align:center;">4</th>
                <th style="text-align:center;">5</th>
                <th style="text-align:center;">6</th>
                <th><strong>Avg.</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">AthleteWonAward</td>
                <td style="text-align:left; border-right: double">
                FULL</td>
                <td style="text-align:center;">0.80</td>
                <td style="text-align:center;">0.81</td>
                <td style="text-align:center;">0.82</td>
                <td style="text-align:center;">0.82</td>
                <td style="text-align:center;">1.0</td>
                <td><strong>0.80</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT</td>
                <td style="text-align:center;">0.37</td>
                <td style="text-align:center;">0.70</td>
                <td style="text-align:center;">0.66</td>
                <td style="text-align:center;">0.0</td>
                <td style="text-align:center;">-</td>
                <td><strong>0.39</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NR</td>
                <td style="text-align:center;">0.73</td>
                <td style="text-align:center;">0.76</td>
                <td style="text-align:center;">0.76</td>
                <td style="text-align:center;">0.76</td>
                <td style="text-align:center;">1.0</td>
                <td><strong>0.74</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT-NR</td>
                <td style="text-align:center;">0.28</td>
                <td style="text-align:center;">0.56</td>
                <td style="text-align:center;">0.53</td>
                <td style="text-align:center;">0.5</td>
                <td style="text-align:center;">1.0</td>
                <td><strong>0.27</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NU</td>
                <td style="text-align:center;">0.83</td>
                <td style="text-align:center;">0.80</td>
                <td style="text-align:center;">0.77</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td><strong>0.82</strong></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;">CompanyAcquired</td>
                <td style="text-align:left; border-right: double">
                FULL</td>
                <td style="text-align:center;">0.70</td>
                <td style="text-align:center;">0.77</td>
                <td style="text-align:center;">0.88</td>
                <td style="text-align:center;">0.88</td>
                <td style="text-align:center;"></td>
                <td><strong>0.74</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT</td>
                <td style="text-align:center;">0.23</td>
                <td style="text-align:center;">0.53</td>
                <td style="text-align:center;">0.78</td>
                <td style="text-align:center;">0.83</td>
                <td style="text-align:center;"></td>
                <td><strong>0.30</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NR</td>
                <td style="text-align:center;">0.70</td>
                <td style="text-align:center;">0.76</td>
                <td style="text-align:center;">0.87</td>
                <td style="text-align:center;">0.88</td>
                <td style="text-align:center;"></td>
                <td><strong>0.74</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT-NR</td>
                <td style="text-align:center;">0.33</td>
                <td style="text-align:center;">0.35</td>
                <td style="text-align:center;">0.63</td>
                <td style="text-align:center;">0.57</td>
                <td style="text-align:center;"></td>
                <td><strong>0.34</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NU</td>
                <td style="text-align:center;">0.67</td>
                <td style="text-align:center;">0.73</td>
                <td style="text-align:center;">0.87</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;"></td>
                <td><strong>0.70</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab9">
          <div class="table-caption">
            <span class="table-number">Table 9:</span> <span class=
            "table-title">HighLife Ablation Study Harvested
            Facts.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Relation by</th>
                <th style="text-align:left; border-right: double">
                HighLife</th>
                <th style="text-align:center;">#Facts</th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:left;">Extracted
                Arguments</th>
                <th style="text-align:left; border-right: double">
                Config.</th>
                <th style="text-align:center;">2</th>
                <th style="text-align:center;">3</th>
                <th style="text-align:center;">4</th>
                <th style="text-align:center;">5</th>
                <th style="text-align:center;">6</th>
                <th><strong>Sum</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">AthleteWonAward</td>
                <td style="text-align:left; border-right: double">
                FULL</td>
                <td style="text-align:center;">3,804</td>
                <td style="text-align:center;">1,089</td>
                <td style="text-align:center;">224</td>
                <td style="text-align:center;">23</td>
                <td style="text-align:center;">2</td>
                <td><strong>5,142</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT</td>
                <td style="text-align:center;">40,728</td>
                <td style="text-align:center;">2,206</td>
                <td style="text-align:center;">11</td>
                <td style="text-align:center;">2</td>
                <td style="text-align:center;">0</td>
                <td><strong>42,947</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NR</td>
                <td style="text-align:center;">3,939</td>
                <td style="text-align:center;">1,873</td>
                <td style="text-align:center;">243</td>
                <td style="text-align:center;">17</td>
                <td style="text-align:center;">2</td>
                <td><strong>6,074</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT-NR</td>
                <td style="text-align:center;">40,728</td>
                <td style="text-align:center;">2,246</td>
                <td style="text-align:center;">265</td>
                <td style="text-align:center;">23</td>
                <td style="text-align:center;">2</td>
                <td><strong>43,264</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NU</td>
                <td style="text-align:center;">3,804</td>
                <td style="text-align:center;">1,078</td>
                <td style="text-align:center;">44</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">0</td>
                <td><strong>4,926</strong></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;">CompanyAcquired</td>
                <td style="text-align:left; border-right: double">
                FULL</td>
                <td style="text-align:center;">2,304</td>
                <td style="text-align:center;">1,253</td>
                <td style="text-align:center;">452</td>
                <td style="text-align:center;">11</td>
                <td style="text-align:center;"></td>
                <td><strong>4,020</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT</td>
                <td style="text-align:center;">19,027</td>
                <td style="text-align:center;">4,090</td>
                <td style="text-align:center;">787</td>
                <td style="text-align:center;">17</td>
                <td style="text-align:center;"></td>
                <td><strong>23,921</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NR</td>
                <td style="text-align:center;">2,649</td>
                <td style="text-align:center;">1,505</td>
                <td style="text-align:center;">583</td>
                <td style="text-align:center;">13</td>
                <td style="text-align:center;"></td>
                <td><strong>4,750</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NT-NR</td>
                <td style="text-align:center;">20,805</td>
                <td style="text-align:center;">4,584</td>
                <td style="text-align:center;">993</td>
                <td style="text-align:center;">22</td>
                <td style="text-align:center;"></td>
                <td><strong>26,404</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left; border-right: double">
                NU</td>
                <td style="text-align:center;">2,306</td>
                <td style="text-align:center;">1,263</td>
                <td style="text-align:center;">165</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;"></td>
                <td><strong>3,734</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab10">
          <div class="table-caption">
            <span class="table-number">Table 10:</span>
            <span class="table-title">Harvested N-ary Fact
            Examples.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Fact</th>
                <th style="text-align:left;">Textual
                Evidence/Observations</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">
                <em>AthletewonAward</em>(<em>DavidReid</em>,
                <em>GoldMedal</em>, <em>SummerGames</em>,</td>
                <td style="text-align:left;"><span style=
                "text-decoration: underline;">Reid</span> won the
                United States’ only <span style=
                "text-decoration: underline;">gold medal</span> in
                <span style=
                "text-decoration: underline;">boxing</span>, on
                <span style=
                "text-decoration: underline;">Sunday</span>.</td>
              </tr>
              <tr>
                <td style="text-align:left;"><em>Atlanta</em>,
                1996, <em>Boxing</em>)</td>
                <td style="text-align:left;"><span style=
                "text-decoration: underline;">David Reid</span> won
                a <span style="text-decoration: underline;">gold
                medal</span> in the <span style=
                "text-decoration: underline;">Summer Olympic
                Games</span> in <span style=
                "text-decoration: underline;">Atlanta</span> in
                <span style=
                "text-decoration: underline;">1996</span>.</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <em>CompanyAcquired</em>(<em>Hewlett</em>,
                <em>Compaq</em>, 2002/05,
                <em>USD</em> 19 <em>bln</em>,
                <em>Unknown</em>)</td>
                <td style="text-align:left;">Yesterday's report was
                the second filing of results since <span style=
                "text-decoration: underline;">Hewlett-Packard</span>
                acquired <span style=
                "text-decoration: underline;">Compaq</span> last
                <span style=
                "text-decoration: underline;">May</span>.</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;"><span style=
                "text-decoration: underline;">Hewlett</span> has
                not achieved the promised benefits from its
                <span style=
                "text-decoration: underline;"><font style=
                "normal">$</font>19 billion purchase</span> of
                <span style="text-decoration: underline;">Compaq
                Computer</span>.</td>
              </tr>
              <tr>
                <td style="text-align:left;"><span class=
                "inline-equation"><span class="tex">$Treats(Immune
                Globulin,Immunodeficiencies,10\%,Intravenous,Humans)$</span></span></td>
                <td style="text-align:left;"><span style=
                "text-decoration: underline;">Immune
                Globulin</span> <span style=
                "text-decoration: underline;">Intravenous</span>
                (<span style=
                "text-decoration: underline;">human</span>)
                <span style=
                "text-decoration: underline;">10%</span> is
                indicated for the treatment of <span style=
                "text-decoration: underline;">immunodeficiency
                disorders.</span></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <em>Causes</em>(<em>Smoking</em>,
                <em>Miscarriage</em>, <em>Unknown</em>,
                <em>Pregnancy</em>)</td>
                <td style="text-align:left;">Smoking cigarettes
                during pregnancy can cause low birth weight,
                <span style=
                "text-decoration: underline;">miscarriage</span>,
                or stillbirth.</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">8.3</span> Large-Scale
            Experiments</h3>
          </div>
        </header>
        <p>In order to demonstrate that our proposed method works
        well across different domains and to demonstrate that the
        method scales well, we performed large-scale experiments
        for news and biomedicine.</p>
        <section id="sec-19">
          <p><em>8.3.1 News Text.</em> Contrary to the extraction
          quality experiment we apply HighLife on the entire corpus
          by using the same seeds. In addition,
          <em>AthleteWonAward</em> incorporates one more possible
          argument increasing the arity from five to six, since we
          do not need to be compatible with Propbank frames.</p>
          <p><strong>Results</strong> The best performing system
          configuration (FULL) achieves an average precision of
          0.77%. Table&nbsp;<a class="tbl" href="#tab6">6</a> and
          &nbsp;<a class="tbl" href="#tab7">7</a> show the
          precision and the number of harvested facts. In terms of
          sources of error, our results suffered most from
          unquestioningly accepting statements of speculation as
          facts. Speculation is prevalent in news, especially for
          acquisitions when some company is reported to consider
          acquiring another company prior to the actual
          transaction. We believe that speculation detection such
          as [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0044">44</a>] is a complementary method that
          can be orthogonally applied in addition to our method for
          fact harvesting. Another source of error is the
          repetitive nature of <em>CompanyAcquired</em>’s type
          signature. Coupling the triple appearance of organization
          in the signature with the numerous
          non-acquisition-related relationships (such as companies
          suing, competing with, etc. one another) between them,
          the signature is not distinctive enough to separate the
          arguments. Annotators were presented with 600 randomly
          selected facts <em>AthleteWonAward</em> and 500
          <em>CompanyAcquired</em> facts. As for inter-annotator
          agreement, the value of Fleiss’ Kappa was 0.568 for
          <em>CompanyAcquired</em> and 0.483 for
          <em>AthleteWonAward</em>, which indicates a moderate
          agreement among annotators.</p>
        </section>
        <section id="sec-20">
          <p><em>8.3.2 Biomedical Text.</em> The biomedical
          relations have signatures with 3 to 5 types, some of
          which are applicable in multiple relations (see
          Table&nbsp;<a class="tbl" href="#tab3">3</a>). The
          relation <em>Treats</em> describes not only drug
          treatments for diseases, but also critical information
          about dosage (e.g. 2.5 mg), dosage form (e.g. topical
          cream), and target groups (children or women).
          <em>ReducesRisk</em> facts describe a drug, a behavior
          (e.g. exercise), or an ecological factor (e.g. sunlight)
          that reduces the risk of a disease for a certain target
          group carrying a condition (e.g. pregnancy).
          <em>Causes</em> describes one disease that causes another
          disease in the context of a target group and a certain
          condition. <em>Diagnoses</em> states which medical
          procedure diagnoses which disease manifesting in a
          certain body part or organ.</p>
          <p><strong>Seed Facts</strong> We manually collected 474
          seed facts from medical online portals <a class=
          "link-inline force-break" href=
          "http://uptodate.com">uptodate.com</a>, <a class=
          "link-inline force-break" href=
          "http://drugs.com">drugs.com</a>. 294 seed facts are
          binary, 165 ternary, 14 quaternary, and 2 quinary.</p>
          <p><strong>Results</strong> Our system achieved an
          average precision of 0.83%. Table&nbsp;<a class="tbl"
          href="#tab6">6</a> shows the precision and
          Table&nbsp;<a class="tbl" href="#tab7">7</a> the number
          of harvested facts under different numbers of known
          arguments. As for inter-annotator agreement, the values
          of Fleiss’ Kappa were between 0.46 to 0.49 for relations
          <em>Treats</em>, <em>Causes</em> and <em>Diagnoses</em>,
          which indicates a moderate agreement among annotators;
          for <em>ReducesRisk</em> it was 0.37 which indicates fair
          agreement. Precision is promising, with the lowest at
          0.80 and other settings above 0.90. Contrary to the
          intuition that the higher the arity, i.e. more known
          arguments, the more difficult it is to correctly capture
          all the arguments thus leading to a lower precision, our
          results instead show that precision increases with arity.
          When the arity is higher, the trees gathered are more
          comprehensive, which in turn contribute to more
          expressive patterns for capturing a relation. On the
          other hand, without unification the number of
          higher-arity facts drops significantly, effectively
          shutting down the possibility of harvesting facts with 5
          or more known arguments. Errors made by our method can be
          attributed to two main sources. First, sentence
          structures are often complex in biomedical text,
          especially in scientific publications. This leads to
          errors in dependency parse trees, which further cascades
          into errors in the tree patterns. Second, entity typing
          in UMLS is not fine-grained enough to support clear-cut
          delineation in the relation property predicates during
          constraint reasoning.</p>
        </section>
      </section>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">8.4</span> Ablation
            Study</h3>
          </div>
        </header>
        <p>We perform an ablation study in order to assess the
        contribution of the individual components by deactivating
        each of them and comparing the partial system to the full
        system. Table&nbsp;<a class="tbl" href="#tab8">8</a> and
        <a class="tbl" href="#tab9">9</a> show the precision and
        the number of harvested facts under different
        configurations and numbers of known arguments. FULL refers
        to the full system with all components enabled, which
        performs the best. When entity types are disregarded (NT),
        the type signatures no longer apply and all entities in
        matching patterns lead to fact candidates. As shown in
        Table&nbsp;<a class="tbl" href="#tab8">8</a>, the number of
        facts increased tremendously, but precision also drops
        tremendously. We see similar but less severe effects when
        consistency rules are not applied (NR) and conflicting fact
        candidates are no longer pruned out. The configuration
        NR-NT denotes disabled entity type and consistency
        constraint checking, leading to an increased number of
        higher-arity facts while binary and ternary facts remain
        largely unaffected. We observe the trends for all
        configurations that the higher the arity, the higher the
        precision. When deactivating unification (NU), partial
        facts are no longer combined to form more complete facts
        and the harvesting of higher-arity facts is negatively
        impacted and thus the number of facts drops. We observe
        that unification is essential to harvesting facts with 4 or
        more arguments.</p>
      </section>
    </section>
    <section id="sec-22">
      <header>
        <div class="title-info">
          <h2><span class="section-number">9</span>
          Conclusions</h2>
        </div>
      </header>
      <p>We presented HighLife, an approach to harvest higher-arity
      facts from texts. Our method combines the mining of tree
      patterns from dependency parses for high recall of fact
      candidates with consistency reasoning to prune out false
      candidates for high precision of eventually accepted facts. A
      key feature of the proposed consistency reasoning is the use
      of unification, which merges multiple partial facts into full
      facts. We showed the validity and versatility of our approach
      by conducting extensive experiments, 1) which compare
      different HighLife variants against different SRL baselines,
      2) which analyse the performance of HighLife for two
      large-scale settings (Biomedicine and News), and 3) which
      give an in-depth analysis of different HighLife
      configurations in an ablation study. The experiments
      demonstrated that our approach is able to harvest facts with
      higher-arity as well as high precision.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">C.F. Baker. FrameNet,
        current collaborations and future goals. <em><em>Language
        Resources and Evaluation</em></em> 46 (2): 269–286,
        2012.</li>
        <li id="BibPLXBIB0002" label="[2]">H.&nbsp;Bast,
        B.&nbsp;Buchhold. An index for efficient semantic full-text
        search.In <em><em>ACM Conference on Information and
        Knowledge Management (CIKM)</em></em>, pages 369–378,
        2013.</li>
        <li id="BibPLXBIB0003" label="[3]">S.&nbsp;L. Berrahou,
        P.&nbsp;Buche, J.&nbsp;Dibie, M.&nbsp;Roche. Xart system:
        Discovering and extracting correlated arguments of n-ary
        relations from text. In <em><em>International Conference on
        Web Intelligence, Mining and Semantics (WIMS)</em></em>,
        pages 8:1–8:12, 2016.</li>
        <li id="BibPLXBIB0004" label="[4]">R.&nbsp;Brachman,
        H.&nbsp;Levesque. <em><em>Knowledge Representation and
        Reasoning</em></em>. <em>The Morgan Kaufmann Series in
        Artificial Intelligence Series</em>. Morgan Kaufmann,
        2004.</li>
        <li id="BibPLXBIB0005" label="[5]">S.&nbsp;Brin. Extracting
        patterns and relations from the World Wide Web. In
        <em><em>International Workshop on The World Wide Web and
        Databases (WebDB)</em></em>, pages 172–183, 1998.</li>
        <li id="BibPLXBIB0006" label="[6]">R.&nbsp;C. Bunescu,
        R.&nbsp;J. Mooney. A shortest path dependency kernel for
        relation extraction. In <em><em>Conference on Human
        Language Technology and Empirical Methods in Natural
        Language Processing (EMNLP-HLT)</em></em>, pages 724–731,
        2005.</li>
        <li id="BibPLXBIB0007" label="[7]">A.&nbsp;Carlson,
        J.&nbsp;Betteridge, R.&nbsp;C. Wang, E.&nbsp;R. Hruschka,
        Jr., T.&nbsp;M. Mitchell. Coupled semi-supervised learning
        for information extraction. In <em><em>International
        Conference on Web Search and Data Mining (WSDM)</em></em>,
        pages 101–110, 2010.</li>
        <li id="BibPLXBIB0008" label="[8]">Y.&nbsp;Chi,
        Y.&nbsp;Yang, R.&nbsp;R. Muntz. Canonical forms for
        labelled trees and their applications in frequent subtree
        mining. <em><em>Knowledge and Information
        Systems</em></em>, 8 (2): 203–234, 2005.</li>
        <li id="BibPLXBIB0009" label="[9]">J. Clarke, V. Srikumar,
        M. Sammons, D. Roth. An NLP Curator (or: How I Learned to
        Stop Worrying and Love NLP Pipelines). In
        <em><em>International Conference on Language Resources and
        Evaluation (LREC:)</em></em> pages 3276–3283, 2012.</li>
        <li id="BibPLXBIB0010" label="[10]">L.&nbsp;Del&nbsp;Corro,
        R.&nbsp;Gemulla. Clausie: Clause-based open information
        extraction. In <em><em>International Conference on World
        Wide Web (WWW)</em></em>, pages 355–366, 2013.</li>
        <li id="BibPLXBIB0011" label="[11]">
        G.&nbsp;R.&nbsp;Doddington, A.&nbsp;Mitchell,
        M.&nbsp;A.&nbsp;Przybocki, L.&nbsp;A.&nbsp;Ramshaw,
        S.&nbsp;Strassel, R.&nbsp;M.&nbsp;Weischedel. The Automatic
        Content Extraction (ACE) Program-Tasks, Data, and
        EvaluationIn <em><em>Automatic Content Extraction (ACE)
        Program-Tasks, Data, and Evaluation (LREC)</em></em>,
        2004.</li>
        <li id="BibPLXBIB0012" label="[12]">M.&nbsp;Dylla,
        I.&nbsp;Miliaraki, M.&nbsp;Theobald. A
        temporal-probabilistic database model for information
        extraction. <em><em>VLDB (Very Large Data Bases)
        Endowment</em></em>, 6 (14): 1810–1821, 2013.</li>
        <li id="BibPLXBIB0013" label="[13]">P.&nbsp;Ernst,
        A.&nbsp;Siu, D.&nbsp;Milchevski, J.&nbsp;Hoffart,
        G.&nbsp;Weikum. DeepLife: An entity-aware search, analytics
        and exploration platform for health and life sciences. In
        <em><em>Annual Meeting of the Association for Computational
        Linguistics (ACL)</em></em>, pages 1017–1024, 2016.</li>
        <li id="BibPLXBIB0014" label="[14]">P.&nbsp;Ernst,
        A.&nbsp;Siu, G.&nbsp;Weikum. KnowLife: A versatile approach
        for constructing a large knowledge graph for biomedical
        sciences. <em><em>BMC Bioinformatics</em></em>, 16 (1):
        1–13, 2015.</li>
        <li id="BibPLXBIB0015" label="[15]">G.&nbsp;Garrido,
        A.&nbsp;Peñas, B.&nbsp;Cabaleiro, A.&nbsp;Rodrigo.
        Temporally anchored relation extraction. In <em><em>Annual
        Meeting of the Association for Computational Linguistics
        (ACL)</em></em>, pages 107–116, 2012.</li>
        <li id="BibPLXBIB0016" label="[16]">D.&nbsp;Gildea,
        D.&nbsp;Jurafsky. Automatic labeling of semantic roles.
        <em><em>Computational Linguistics</em></em>, 28 (3):
        245–288, 2002.</li>
        <li id="BibPLXBIB0017" label="[17]">J.&nbsp;Hoffart,
        D.&nbsp;Milchevski, G.&nbsp;Weikum. Stics: Searching with
        strings, things, and cats. In <em><em>International ACM
        SIGIR Conference on Research &amp; Development in
        Information Retrieval (SIGIR)</em></em>, pages 1247–1248,
        2014.</li>
        <li id="BibPLXBIB0018" label="[18]">J.&nbsp;Hoffart,
        F.&nbsp;M. Suchanek, K.&nbsp;Berberich, G.&nbsp;Weikum.
        YAGO2: A spatially and temporally enhanced knowledge base
        from wikipedia. <em><em>Artificial Intelligence</em></em>,
        194:28–61, 2013.</li>
        <li id="BibPLXBIB0019" label="[19]">J.&nbsp;Hoffart,
        M.&nbsp;A. Yosef, I.&nbsp;Bordino, H.&nbsp;Fürstenau,
        M.&nbsp;Pinkal, M.&nbsp;Spaniol, B.&nbsp;Taneva,
        S.&nbsp;Thater, G.&nbsp;Weikum. Robust disambiguation of
        named entities in text. In <em><em>Conference on Empirical
        Methods in Natural Language Processing (EMNLP)</em></em>,
        pages 782–792, 2011.</li>
        <li id="BibPLXBIB0020" label="[20]">S.&nbsp;Krause,
        L.&nbsp;Hennig, A.&nbsp;Moro, D.&nbsp;Weissenborn,
        F.&nbsp;Xu, H.&nbsp;Uszkoreit, R.&nbsp;Navigli. Sar-graphs:
        A language resource connecting linguistic knowledge with
        semantic relations from knowledge graphs. <em><em>Web
        Semantics: Science, Services and Agents on the World Wide
        Web, 37</em></em>, 38:112–131, 2016.</li>
        <li id="BibPLXBIB0021" label="[21]">S.&nbsp;Krause,
        H.&nbsp;Li, H.&nbsp;Uszkoreit, F.&nbsp;Xu. Large-scale
        learning of relation-extraction rules with distant
        supervision from the web. In <em><em>International Semantic
        Web Conference (ISWC)</em></em>, pages 263–278, 2012.</li>
        <li id="BibPLXBIB0022" label="[22]">E.&nbsp;Kuzey,
        J.&nbsp;Vreeken, G.&nbsp;Weikum. A fresh look on knowledge
        bases: Distilling named events from news. In <em><em>ACM
        International Conference on Information and Knowledge
        Management (CIKM)</em></em>, pages 1689–1698, 2014.</li>
        <li id="BibPLXBIB0023" label="[23]">E.&nbsp;Kuzey,
        G.&nbsp;Weikum. Extraction of temporal facts and events
        from wikipedia. In <em><em>Temporal Web Analytics Workshop
        (TempWeb)</em></em>, pages 25–32, 2012.</li>
        <li id="BibPLXBIB0024" label="[24]">H. Li, S. Krause, F.
        Xu, A. Moro, H. Uszkoreit, R. Navigli. Improvement of n-ary
        Relation Extraction by Adding Lexical Semantics to
        Distant-Supervision Rule LearningIn <em><em>International
        Conference on Agents and Artificial Intelligence
        (ICAART)</em></em>, Volume 2, 2015.</li>
        <li id="BibPLXBIB0025" label="[25]">H.&nbsp;Liu,
        L.&nbsp;Hunter, V.&nbsp;Keselj, K.&nbsp;Verspoor.
        Approximate subgraph matching-based literature mining for
        biomedical events and relations. <em><em>PLoS
        ONE</em></em>, 8 (4): 1–16, 2013.</li>
        <li id="BibPLXBIB0026" label="[26]">V.&nbsp;Markl,
        P.&nbsp;J. Haas, M.&nbsp;Kutsch, N.&nbsp;Megiddo,
        U.&nbsp;Srivastava, T.&nbsp;M. Tran. Consistent selectivity
        estimation via maximum entropy. <em><em>The VLDB (Very
        Large Data Bases) Journal</em></em>, 16 (1): 55–76,
        2007.</li>
        <li id="BibPLXBIB0027" label="[27]">Mausam,
        M.&nbsp;Schmitz, R.&nbsp;Bart, S.&nbsp;Soderland,
        O.&nbsp;Etzioni. Open language learning for information
        extraction. In <em><em>Joint Conference on Empirical
        Methods in Natural Language Processing and Computational
        Natural Language Learning (EMNLP-CoNLL)</em></em>, pages
        523–534, 2012.</li>
        <li id="BibPLXBIB0028" label="[28]">D.&nbsp;McClosky,
        S.&nbsp;Riedel, M.&nbsp;Surdeanu, A.&nbsp;McCallum,
        C.&nbsp;D. Manning. Combining joint models for biomedical
        event extraction. <em><em>BMC Bioinformatics</em></em>, 13
        (11): 1–12, 2012.</li>
        <li id="BibPLXBIB0029" label="[29]">R.&nbsp;McDonald,
        F.&nbsp;Pereira, S.&nbsp;Kulick, S.&nbsp;Winters,
        Y.&nbsp;Jin, P.&nbsp;White. Simple algorithms for complex
        relation extraction with applications to biomedical ie.In
        <em><em>Annual Meeting on Association for Computational
        Linguistics (ACL)</em></em>, pages 491–498, 2005.</li>
        <li id="BibPLXBIB0030" label="[30]">F.&nbsp;Mesquita,
        J.&nbsp;Schmidek, D.&nbsp;Barbosa. Effectiveness and
        efficiency of open relation extraction. In
        <em><em>Conference on Empirical Methods in Natural Language
        Processing (EMNLP)</em></em>, pages 447–457, 2013.</li>
        <li id="BibPLXBIB0031" label="[31]">M.&nbsp;Mintz,
        S.&nbsp;Bills, R.&nbsp;Snow, D.&nbsp;Jurafsky. Distant
        supervision for relation extraction without labeled data.
        In <em><em>Annual Meeting on Association for Computational
        Linguistics (ACL)</em></em>, pages 1003–1011, 2009.</li>
        <li id="BibPLXBIB0032" label="[32]">M.&nbsp;Miwa,
        P.&nbsp;Thompson, J.&nbsp;McNaught, D.&nbsp;B. Kell,
        S.&nbsp;Ananiadou. Extracting semantically enriched events
        from biomedical literature. <em><em>BMC
        Bioinformatics</em></em>, 13 (1): 1–24, 2012.</li>
        <li id="BibPLXBIB0033" label="[33]">A.&nbsp;Moschitti.
        Making tree kernels practical for natural language
        learning. In <em><em>European Chapter of the Association
        for Computational Linguistics (EACL)</em></em>, pages
        113–120, 2006.</li>
        <li id="BibPLXBIB0034" label="[34]">N.&nbsp;Nakashole,
        M.&nbsp;Theobald, G.&nbsp;Weikum. Scalable knowledge
        harvesting with high precision and high recall. In
        <em><em>International Conference on Web Search and Data
        Mining (WSDM)</em></em>, pages 227–236, 2011.</li>
        <li id="BibPLXBIB0035" label="[35]">M.&nbsp;Palmer,
        D.&nbsp;Gildea, P.&nbsp;KingsburyThe Proposition Bank: An
        Annotated Corpus of Semantic Roles. <em><em>Computational
        Linguistics 28</em></em>, 31 (1): 71–106, 2005.</li>
        <li id="BibPLXBIB0036" label="[36]">M.&nbsp;Palmer,
        D.&nbsp;Gildea, N.&nbsp;XueSemantic role labeling
        <em><em>Synthesis Lectures on Human Language
        Technologies</em></em>, 3 (1): 1-103, 2011.</li>
        <li id="BibPLXBIB0037" label="[37]">N. Peng, H. Poon, C.
        Quirk, K. Toutanova, W. Yih. Cross-Sentence N-ary Relation
        Extraction with Graph LSTMs. <em><em>Transactions of the
        ACL (TACL)</em></em> 5:101–115, 2017.</li>
        <li id="BibPLXBIB0038" label="[38]">V. Punyakanok, D. Roth,
        W. Yih. The Importance of Syntactic Parsing and Inference
        in Semantic Role Labeling. <em><em>Computational
        Linguistics 28</em></em>, 34 (2): 257–287, 2008.</li>
        <li id="BibPLXBIB0039" label="[39]">L. Ratinov, D. Roth.
        Design Challenges and Misconceptions in Named Entity
        Recognition. In <em><em>Conference on Computational Natural
        Language Learning (CoNLL)</em></em>, pages 147–155,
        2009.</li>
        <li id="BibPLXBIB0040" label="[40]">D.&nbsp;Shahaf,
        C.&nbsp;Guestrin. Connecting two (or less) dots:
        Discovering structure in news articles. <em><em>ACM
        Transactions on Knowledge Discovery from Data</em></em>, 5
        (4): 24:1–24:31, 2012.</li>
        <li id="BibPLXBIB0041" label="[41]">V. Srikumar, D. Roth.
        Modeling Semantic Relations Expressed by Prepositions.
        <em>Transactions of the ACL (TACL)</em>1, pages 231–242,
        2013.</li>
        <li id="BibPLXBIB0042" label="[42]">F.&nbsp;Suchanek,
        M.&nbsp;Sozio, G.&nbsp;Weikum. Sofie: A self-organizing
        framework for information extraction. In
        <em><em>International World Wide Web Conference
        (WWW)</em></em>, pages 631–640, 2009.</li>
        <li id="BibPLXBIB0043" label="[43]">M.&nbsp;Surdeanu,
        J.&nbsp;Heng. Overview of the English slot filling track at
        the TAC2014 knowledge base population evaluation. In
        <em><em>Text Analysis Conference Knowledge Base Population
        Workshop (TAC-KBP)</em></em>, 2014.</li>
        <li id="BibPLXBIB0044" label="[44]">G.&nbsp;Szarvas,
        V.&nbsp;Vincze, R.&nbsp;Farkas, G.&nbsp;Mora,
        I.&nbsp;Gurevych. Cross-genre and cross-domain detection of
        semantic uncertainty. <em><em>Computational
        Linguistics</em></em>, 38 (2): 335–367, 2012.</li>
        <li id="BibPLXBIB0045" label="[45]">P.&nbsp;P. Talukdar,
        D.&nbsp;Wijaya, T.&nbsp;Mitchell. Coupled temporal scoping
        of relational facts. In <em><em>ACM International
        Conference on Web Search and Data Mining (WSDM)</em></em>,
        pages 73–82, 2012.</li>
        <li id="BibPLXBIB0046" label="[46]">M.&nbsp;Valenzuela,
        V.&nbsp;Ha, O.&nbsp;Etzioni. Identifying meaningful
        citations. In <em><em>Workshop on Scholarly Big Data at
        AAAI</em></em>, 2015.</li>
        <li id="BibPLXBIB0047" label="[47]">
        S.&nbsp;Van&nbsp;Landeghem, J.&nbsp;Börne, C.-H. Wei,
        K.&nbsp;Hakala, S.&nbsp;Pyysalo, S.&nbsp;Ananiadou, H.-Y.
        Kao, Z.&nbsp;Lu, T.&nbsp;Salakoski, Y.&nbsp;Van&nbsp;de
        Peer, F.&nbsp;Ginter. Large-scale event extraction from
        literature with multi-level gene normalization.
        <em><em>PLoS ONE</em></em>, 8 (4): 1–12, 2013.</li>
        <li id="BibPLXBIB0048" label="[48]">D.&nbsp;Vrandecic,
        M.&nbsp;Krötzsch. Wikidata: A free collaborative
        knowledgebase. <em><em>Communications of the ACM</em></em>,
        57 (10): 78–85, 2014.</li>
        <li id="BibPLXBIB0049" label="[49]">Y.&nbsp;Wang,
        B.&nbsp;Yang, L.&nbsp;Qu, M.&nbsp;Spaniol, G.&nbsp;Weikum.
        Harvesting facts from textual web sources by constrained
        label propagation. In <em><em>ACM International Conference
        on Information and Knowledge Management (CIKM)</em></em>,
        pages 837–846, 2011.</li>
        <li id="BibPLXBIB0050" label="[50]">C.&nbsp;Wang,
        J.&nbsp;Fan. Medical Relation Extraction with Manifold
        ModelsIn <em><em>Annual Meeting of the Association for
        Computational Linguistics (ACL)</em></em>, pages 828–838,
        2014.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186000">https://doi.org/10.1145/3178876.3186000</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
