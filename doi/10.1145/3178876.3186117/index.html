<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>SIDE: Representation Learning in Signed Directed Networks</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">SIDE: Representation Learning in Signed Directed Networks</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Junghwan</span>      <span class="surName">Kim</span>,     Seoul National University, Seoul, Korea,     <a href="mailto:kjh900809@snu.ac.kr">kjh900809@snu.ac.kr</a></div>     <div class="author">     <span class="givenName">Haekyu</span>      <span class="surName">Park</span>,     Seoul National University, Seoul, Korea,     <a href="mailto:hkpark627@snu.ac.kr">hkpark627@snu.ac.kr</a></div>     <div class="author">     <span class="givenName">Ji-Eun</span>      <span class="surName">Lee</span>,     Seoul National University, Seoul, Korea,     <a href="mailto:dreamhunter@snu.ac.kr">dreamhunter@snu.ac.kr</a></div>     <div class="author">     <span class="givenName">U</span>      <span class="surName">Kang</span>,     Seoul National University, Seoul, Korea, <a href="mailto:ukang@snu.ac.kr">ukang@snu.ac.kr</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186117" target="_blank">https://doi.org/10.1145/3178876.3186117</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Given a signed directed network, how can we learn node representations which fully encode structural information of the network including sign and direction of edges? Node representation learning or network embedding learns a mapping of each node to a vector. The mapping encodes structural information on network, providing low-dimensional dense node features for general machine learning and data mining frameworks. Since many social networks allow trust (friend) and distrust (enemy) relationships described by signed and directed edges, generalizing network embedding method to learn from sign and direction information in networks is crucial. In addition, social theories are critical tool in signed network analysis. However, none of the existing methods supports all of the desired properties: considering sign, direction, and social theoretical interpretation.</small>     </p>     <p>     <small>In this paper, we propose SIDE, a general network embedding method that represents both sign and direction of edges in the embedding space. SIDE carefully formulates and optimizes likelihood over both direct and indirect signed connections. We provide socio-psychological interpretation for each component of likelihood function. We prove linear scalability of our algorithm and propose additional optimization techniques to reduce the training time and improve accuracy. Through extensive experiments on real-world signed directed networks, we show that SIDE effectively encodes structural information into the learned embedding.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Networks </strong>&#x2192; <strong>Online social networks;</strong> &#x2022;<strong> Information systems </strong>&#x2192; <em>Data mining;</em> &#x2022;<strong> Computing methodologies </strong>&#x2192; <em>Machine learning;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Network Embedding</small>, </span>     <span class="keyword">      <small> Representation Learning</small>, </span>     <span class="keyword">      <small> Signed Directed Network</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Junghwan Kim, Haekyu Park, Ji-Eun Lee, and U Kang. 2018. SIDE: Representation Learning in Signed Directed Networks. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA 10 Pages. <a href="https://doi.org/10.1145/3178876.3186117" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186117</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Given a network with signed directed edges, how can we learn a vector representation of each node, encoding rich information on the network topology? Network embedding is one of the fundamental problems in network analysis and has received much interest from the data mining community recently [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]. Network embedding maps nodes into low-dimensional vector space that summarizes various aspects of network topology and link structure. Consequently, the mapped vectors provide features for conventional machine learning and data mining frameworks to solve various tasks, involving node classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], link prediction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] and clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. In addition, the proximity structure in embedding space provides alternative to random walk with restart methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>] for ranking nodes.</p>    <p>Many social networks, such as Epinions<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> and Slashdot<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, allow users to form positive (trust, friendship) or negative (distrust, opposition) connections to other users. Negative links contain additional information&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] that boosts the performance in multiple tasks such as link sign prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] and node classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] in signed networks. In addition, link directions are significant predictors of future link formation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. Most existing network embedding methods, however, only focus on modeling basic symmetric link structure, failing to exploit additional useful information in negative links and link directions. Modeling sign and direction in network embedding framework introduces the following challenges: consistent interpretation of both signs, representation of individual linking tendency, and utilization of multi-step connections.</p>    <p>There are several previous works on network embedding methods for signed networks. Spectral approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] cannot represent asymmetry in direction due to constraints that spectral theory imposes. Wang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] adopt neural network architecture to separate positive and negative connections, but their work also does not apply to directed network. Yuan et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] exploit log-bilinear model which exploits blackbox model of embedding space and thus does not provide social theoretical interpretation. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186117/images/www2018-126-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Visualization of the Eastern Central Highlands of New Guinea network<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0024">24</a>]. Figure <a class="fig" href="#fig1">1</a> a represents the input network with favorable (blue lines) and hostile (red lines) relations. Figures <a class="fig" href="#fig1">1</a> b, <a class="fig" href="#fig1">1</a> c, and <a class="fig" href="#fig1">1</a> d show the embedding result of SIDE, SiNE, and SNE, respectively. SIDE best preserves the group structure marked by different shapes of nodes.</span>     </div>     </figure>    </p>    <p>In this paper, we propose SIDE (SIgned Directed network Embedding), a general network embedding method for signed directed networks. SIDE successfully represents proximity in signed directed network as a compact low-dimensional vector. For example, as presented in Figure <a class="fig" href="#fig1">1</a>, SIDE separates clusters more clearly in the embedding space than other baseline methods. In the embedding space, the friendly (blue lines) and unfriendly (red lines) relationships among social entities are effectively represented as distances between entities in the embedding space. Table <a class="tbl" href="#tab1">1</a> compares SIDE with other algorithms in various perspectives; SIDE is the only method which satisfies the desired properties: 1) considering sign and direction, 2) accurate, and 3) fast.</p>    <p>We base our network embedding method on truncated random walk and devise a general likelihood formulation for signed directed connections that represent both positive and negative edges consistently. Bias factors are employed in the likelihood function to model individual connectivity. We also generalize random walk sampling process and likelihood formulation to model multi-step relationship including both sign and direction. SIDE closely associates vector space geometry with social phenomena in networks such as homophily, preferential attachment, and balance theoretic behaviours. The association decomposes dual factors of link formation and provides a solid basis to apply our method in general analysis of signed directed networks.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Comparison of SIDE and other embedding algorithms. Our proposed method SIDE applies to the most general settings and shows the best performance for all metrics.</span>     </div>     <table class="table"> 			 <thead>      <tr>       <th style="text-align:center;">        <strong>Method</strong>       </th>       <th style="text-align:center;">        <strong>Consider</strong>       </th>       <th style="text-align:center;">        <strong>Consider</strong>       </th>       <th style="text-align:center;">        <strong>Predictive</strong>       </th>       <th style="text-align:center;">        <strong>Speed</strong>       </th>      </tr>      <tr>       <th style="text-align:center;"/>       <th style="text-align:center;">        <strong>sign?</strong>       </th>       <th style="text-align:center;">        <strong>direction?</strong>       </th>       <th style="text-align:center;">        <strong>accuracy</strong>       </th>       <th style="text-align:center;"/>      </tr> 					 </thead>     <tbody>      <tr>       <td style="text-align:center;">N2V&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0007">7</a>]</td>       <td style="text-align:center;">No</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">Low</td>       <td style="text-align:center;">Fast</td>      </tr>      <tr>       <td style="text-align:center;">MF&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0008">8</a>]</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">Medium</td>       <td style="text-align:center;">Medium</td>      </tr>      <tr>       <td style="text-align:center;">BNS&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0038">38</a>]</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">No</td>       <td style="text-align:center;">Medium</td>       <td style="text-align:center;">Slow</td>      </tr>      <tr>       <td style="text-align:center;">SiNE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0031">31</a>]</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">No</td>       <td style="text-align:center;">Medium</td>       <td style="text-align:center;">Slow</td>      </tr>      <tr>       <td style="text-align:center;">SNE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0037">37</a>]</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">Yes</td>       <td style="text-align:center;">Low</td>       <td style="text-align:center;">Medium</td>      </tr>      <tr>       <td style="text-align:center;">SIDE</td>       <td style="text-align:center;">        <strong>Yes</strong>       </td>       <td style="text-align:center;">        <strong>Yes</strong>       </td>       <td style="text-align:center;">        <strong>High</strong>       </td>       <td style="text-align:center;">        <strong>Fast</strong>       </td>      </tr>     </tbody>     </table>    </div>    <p>Our contributions are listed as follows:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>Algorithm.</strong> We propose SIDE, a novel network embedding method on signed directed network (Section <a class="sec" href="#sec-9">3</a>). By leveraging both sign and direction in the network, it achieves the state-of-the-art accuracy in link sign prediction task.<br/></li>     <li id="list2" label="&#x2022;"><strong>Analysis.</strong> We perform detailed analysis of SIDE. We analyze and show that components in our formulation are associated with the socio-psychological interpretation (Section <a class="sec" href="#sec-13">3.4</a> and <a class="sec" href="#sec-19">4.3</a>). In addition, complexity analysis proves that our algorithm shows scalability, linear in the number of nodes (Section <a class="sec" href="#sec-15">3.6</a>).<br/></li>     <li id="list3" label="&#x2022;"><strong>Performance.</strong>     <br/>SIDE shows outstanding performance in terms of both accuracy and speed (Section <a class="sec" href="#sec-16">4</a>). SIDE achieves the state-of-the-art performance in link sign prediction task. In addition, the learning process is up to 1.8 &#x00D7; faster than other embedding methods with neural architecture.<br/></li>    </ul>    <p>The code of our method and datasets used in this paper are available at <a class="link-inline force-break" href="http://datalab.snu.ac.kr/side">http://datalab.snu.ac.kr/side</a>. The rest of this paper is organized as follows. We describe preliminaries in Section <a class="sec" href="#sec-6">2</a>. Our proposed model SIDE is presented with detailed analysis in Section <a class="sec" href="#sec-9">3</a>, followed by experimental results in Section <a class="sec" href="#sec-16">4</a>. After discussing related works in Section <a class="sec" href="#sec-22">5</a>, we conclude in Section <a class="sec" href="#sec-23">6</a>.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>     </div>    </header>    <p>In this section, we first describe socio-psychological theories which provide better understanding of the link formation process and the link sign structure (Section <a class="sec" href="#sec-7">2.1</a>). Then we present random walk based network embedding formulation (Section <a class="sec" href="#sec-8">2.2</a>). Table <a class="tbl" href="#tab2">2</a> lists the symbols used in this paper.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Table of symbols.</span>     </div>     <table class="table"> 			 <thead>      <tr>       <th style="text-align:center;">        <strong>Symbol</strong>       </th>       <th style="text-align:left;">        <strong>Definition</strong>       </th>      </tr> 					 </thead>     <tbody>      <tr>       <td style="text-align:center;">        <em>G</em> = (<em>V</em>, <em>E</em>)</td>       <td style="text-align:left;">input network</td>      </tr>      <tr>       <td style="text-align:center;">        <em>s</em>       </td>       <td style="text-align:left;">sign assignment function</td>      </tr>      <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>        </span>       </td>       <td style="text-align:left;">the set of co-occurring node pairs</td>      </tr>      <tr>       <td style="text-align:center;">        <em>u</em>, <em>v</em>       </td>       <td style="text-align:left;">nodes</td>      </tr>      <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$v_j^{\prime }$</span>        </span>       </td>       <td style="text-align:left;">a noise node</td>      </tr>      <tr>       <td style="text-align:center;">        <em>W<sup>out</sup>        </em>       </td>       <td style="text-align:left;">out embedding matrix</td>      </tr>      <tr>       <td style="text-align:center;">        <em>W<sup>in</sup>        </em>       </td>       <td style="text-align:left;">in embedding matrix</td>      </tr>      <tr>       <td style="text-align:center;">        <em>b</em>        <sup>        <em>in</em>, +</sup>, <em>b</em>        <sup>        <em>in</em>, &#x2212;</sup>       </td>       <td style="text-align:left;">positve/negative in-bias vectors</td>      </tr>      <tr>       <td style="text-align:center;">        <em>b</em>        <sup>        <em>out</em>, +</sup>, <em>b</em>        <sup>        <em>out</em>, &#x2212;</sup>       </td>       <td style="text-align:left;">positive/negative out-bias vectors</td>      </tr>      <tr>       <td style="text-align:center;">        <em>&#x03C3;</em>       </td>       <td style="text-align:left;">sigmoid function</td>      </tr>      <tr>       <td style="text-align:center;">        <em>w</em>       </td>       <td style="text-align:left;">the number of walks per node</td>      </tr>      <tr>       <td style="text-align:center;">        <em>l</em>       </td>       <td style="text-align:left;">the number of steps per walk</td>      </tr>      <tr>       <td style="text-align:center;">        <em>k</em>       </td>       <td style="text-align:left;">the context window size</td>      </tr>      <tr>       <td style="text-align:center;">        <em>n</em>       </td>       <td style="text-align:left;">the number of noise sampling</td>      </tr>      <tr>       <td style="text-align:center;">        <em>d</em>       </td>       <td style="text-align:left;">the embedding dimension</td>      </tr>      <tr>       <td style="text-align:center;">        <em>&#x03BB;</em>       </td>       <td style="text-align:left;">the regularization parameter</td>      </tr>     </tbody>     </table>    </div>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Socio-Psychological Theories</h3>     </div>     </header>     <p>We employ socio-psychological theories to understand the formation of signed directed links. We first identify homophily and preferential attachment as two driving forces of link formation. Then we describe balance theory to deduce sign of multi-step connection.</p>     <p>     <strong>Link Formation.</strong> We decompose causes of link formation into two factors: <em>similarity</em> and <em>connectivity</em>. Homophily [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] explains similarity factor in link formation. Homophily is portrayed as &#x201C;Birds of a feather flock together&#x201D;, which indicates that a node is more likely to connect with nodes that have similar properties. Similarity is naturally symmetric and transitive. Therefore, homophily is well modeled with distance in metric space by putting similar nodes closely together and dissimilar nodes far apart from each other.</p>     <p>Connectivity factor is characterized by preferential attachment [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>]. It states that nodes with higher connectivity are more likely to form additional links. According to preferential attachment, the link formation likelihood is asymmetric and determined by individual connectivity of nodes. For example, following celebrity in real world directed networks is not likely to be reciprocated. In signed directed network, positive/negative and in/out links determine four different types of connectivity. For example, celebrities in a social network tend to receive more in-links than out-links due to their popularity, while advertisers tend to form many out-links in order to disseminate information. Because of the asymmetric and personalized nature, symmetric metric space is not adequate to model preferential attachment.</p>     <p>     <strong>Link sign structure.</strong> Balance theory&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] is a well-established socio-psychological theory that states the following four rules: &#x201C;A friend of my friend is my friend,&#x201D; &#x201C;A friend of my enemy is my enemy,&#x201D; &#x201C;An enemy of my friend is my enemy,&#x201D; and &#x201C;An enemy of my enemy is my friend.&#x201D; The theory permits only even number of negative edges in a triad of signed network.</p>     <p>Balance theory provides basic rules to infer signs of multi-step relations. Given a path from a node <em>u</em> to another node <em>v</em>, adding hypothetical edges from <em>u</em> to every node on the path forms multiple triads. By sequentially applying balance theory on these triads, eventually the sign of the relationship between <em>u</em> and <em>v</em> can be inferred. As a result, the sign of multi-step connection is a successive multiplication of signs on edges along the path.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Network Embedding Formulation</h3>     </div>     </header>     <p>Network embedding method learns a mapping of each node to a vector and encodes a link structure of network into a proximity structure in vector space. By preserving the information on network topology, the embedding provides informative features for off-the-shelf machine learning algorithms to solve tasks on networks such as node classification, link prediction and community detection.</p>     <p>The network embedding problem is formulated as follows. Let <em>G</em> = (<em>V</em>, <em>E</em>) be a given network with nodes <em>V</em> and edges <em>E</em>. The network embedding method aims to learn a function <span class="inline-equation"><span class="tex">$f: V \rightarrow \mathbb {R}^d$</span>     </span> which maps each node <em>v</em> &#x2208; <em>V</em> to a <em>d</em>-dimensional vector. The embedding function is parameterized by a |<em>V</em>| &#x00D7; <em>d</em> embedding matrix <em>W</em> which consists of the <em>d</em>-dimensional row vector for each node.</p>     <p>Random walk based network embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] learns two embedding functions corresponding to &#x201C;target&#x201D; and &#x201C;neighborhood&#x201D;, respectively. The embeddings encode network structure by exploiting a language model. In the rest of this section, we describe the method in two stages: random walk generation and likelihood optimization. By generating multiple truncated random walks, the method transforms graph structure into sequential structure. Then the likelihood adopted from the language model is optimized to learn proximity structure of the graph from the node co-occurrence frequencies in the random walks.</p>     <p>     <strong>Random Walk Generation.</strong> In the first stage, the method generates multiple truncated random walks on the graph. The result is node sequences which reveal proximity among nodes. Each step of walks chooses next node according to transition probabilities proportional to weights on edges. Then we generate multiple co-occurring pairs for calculation and optimization of the likelihood. Two nodes are defined to be a co-occurring pair if two nodes are placed within short distance in random walk sequences. If node pairs are connected with more heavily weighted and shorter paths, they are more likely to be within fewer steps from each other in the random walk node sequences. Consequently, the co-occurrence statistic of node pairs encodes proximity between nodes.</p>     <p>The truncated random walk is efficient in generating sample pairs of proximate nodes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. While each step of walk requires only one random number generation for choosing the next node, the chosen node forms pairs with multiple proximate nodes increasing the effective sampling rate. Consequently, better time complexity is achieved by leveraging increased effective sampling rate.</p>     <p>     <strong>Likelihood Optimization.</strong> In the second stage, the method learns vector representation according to a neural language model, especially skipgram with negative sampling (SGNS) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>]. SGNS is formulated as maximum likelihood on co-occurrence of node pairs. Direct prediction on neighboring nodes from target node using softmax function requires infeasible amount of parameter updates for each node pair. In order to limit the number of parameters updated in each step, SGNS employs negative sampling approach which defines binary classification on co-occurrence of node pairs rather than the direct prediction of co-occurring node. Therefore, likelihood model of SGNS predicts whether a pair of nodes co-occurs or not in the simulated random walk. The likelihood of <em>u</em> linking to <em>v</em> is formulated as follows: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(u, v) = \sigma (W_u \cdot W^{\prime }_v) = \frac{1}{1 + \exp {(- W_u \cdot W^{\prime }_v)}} \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <em>&#x03C3;</em> is a sigmoid function. <em>W<sub>u</sub>     </em> and <span class="inline-equation"><span class="tex">$W_v^{\prime }$</span>     </span> are the &#x201C;target&#x201D; and &#x201C;neighborhood&#x201D; embedding vectors, respectively. The likelihood performs binary classification on whether node pairs co-occur or not. Intuitively, a pair of nodes with larger inner product value <span class="inline-equation"><span class="tex">$W_u \cdot W^{\prime }_v$</span>     </span> has a higher likelihood of co-occurrence.</p>     <p>The negative log-likelihood objective is defined as follows: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \sum _{(u, v) \in \mathcal {D}} [- \log P(u, v) + \sum _{j=1}^n - \log (1 - P(u, v_j^{\prime }))] \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span> is a set of co-occurring node pairs in random walk corpus and <span class="inline-equation"><span class="tex">$v_j^{\prime }$</span>     </span> is a randomly sampled noise node. Each co-occurring example pair <span class="inline-equation"><span class="tex">$(u, v) \in \mathcal {D}$</span>     </span>, sampled from the first stage, is used to take one step of gradient descent to optimize the first part of the Equation (<a class="eqn" href="#eq2">2</a>). For each co-occurring example (<em>u</em>, <em>v</em>), multiple noise pairs are defined as <span class="inline-equation"><span class="tex">$(u, v_j^{\prime })$</span>     </span> where <span class="inline-equation"><span class="tex">$v_j^{\prime }$</span>     </span> are randomly sampled nodes. The noise pairs constitute negative examples for the binary classification and are used in the second part of Equation (<a class="eqn" href="#eq2">2</a>). Gradient descent update for noise samples push two randomly sampled nodes apart from each other. This prevents all nodes from converging to a single point and penalizes unconnected node pairs of being close to each other. Multiple noise pairs are sampled for each co-occurring node pair to account for the imbalance of positive and noise pairs due to the link sparsity in real world network.</p>    </section>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Proposed Method</h2>     </div>    </header>    <p>In this section, we describe SIDE, our proposed method for network embedding on signed directed network.</p>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Overview</h3>     </div>     </header>     <p>Let <em>G</em> be a given signed directed network with nodes <em>V</em>, edges <em>E</em> and sign assignment function <em>s</em>: <em>E</em> &#x2192; { &#x2212; 1, +1}. We aim to learn target and neighborhood embedding functions <span class="inline-equation"><span class="tex">$f, g: V \rightarrow \mathbb {R}^d$</span>     </span> which depict link structure including sign and direction. We exploit the random walk based network embedding framework described in Section <a class="sec" href="#sec-8">2.2</a>; embedding function is derived by maximizing likelihood over node pairs sampled from directed random walk. Embedding signed directed network entails the following challenges:</p>     <ol class="list-no-style">     <li id="list4" label="(1)"><strong>Multi-step relationship.</strong> How can we generalize single-step sign and direction to multi-step neighbors?<br/></li>     <li id="list5" label="(2)"><strong>Negative edges.</strong> How can we encode negative links in embedding space without hindering positive proximity?<br/></li>     <li id="list6" label="(3)"><strong>Individual connectivity.</strong> How can we separately model connectivity factor from similarity factor in link formation?<br/></li>     </ol>     <p>We propose the following main ideas to address the challenges.</p>     <ol class="list-no-style">     <li id="list7" label="(1)"><strong>Sign and direction aggregation</strong> generalizes the notion of single-step sign and direction to multi-step connections. Signs along the path are aggregated according to balance theory and direction follows topological order of the graph (Section <a class="sec" href="#sec-11">3.2</a>).<br/></li>     <li id="list8" label="(2)"><strong>Signed proximity term</strong> assigns high likelihood for proximate positively connected pairs and distant negatively connected pairs. Positive pull and negative push are balanced under the maximum likelihood framework (Section <a class="sec" href="#sec-12">3.3</a>).<br/></li>     <li id="list9" label="(3)"><strong>Bias terms</strong> model individual node connectivity separately from inner product similarity. We distinguish positive/negative and in/out biases to incorporate asymmetry in connecting pattern of each node (Section <a class="sec" href="#sec-12">3.3</a>).<br/></li>     </ol>     <p>Algorithm 1 summarizes the procedure of SIDE and shows how the main ideas are implemented. SIDE first simulates multiple random walks (lines 3 to 7) and generates co-occurring pairs (<em>u</em>, <em>v</em>) of node from the simulated walks (line 9). Then, for each pair (<em>u</em>, <em>v</em>), gradient descent steps for both sampled pairs (<em>u</em>, <em>v</em>) (line 10) and noise pairs (<em>u</em>, <em>v</em>&#x2032;) (line 13) are performed to update embedding vectors and biases. In the following sections, we describe our framework details, present social theoretical analysis, introduce additional optimization techniques, and perform complexity analysis of SIDE.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Sign and Direction Aggregation</h3>     </div>     </header>     <p>In the first stage of SIDE, the pairs of nodes are sampled from a random walk process. The sampling process first generates truncated random walks starting from each seed node. Each step of the walk follows directed edges until the required length is satisfied; the order of nodes in the random walk conforms to the topological order in the directed network. If the random walk encounters dead end, the remaining steps restart from the seed node. The resulting walk sequence consists of visited nodes and signs on the followed edges. Then co-occurring pairs of nodes within a window of size <em>k</em> are selected from the walk sequences. We define sign and direction for each co-occurring pair aggregating information along the path.</p>     <p>Sign of the node pair is inferred according to the balance theory as described in Section <a class="sec" href="#sec-7">2.1</a>: sign on multi-step connection is multiplication of edge signs along the path. The sign is negative if there are odd number of negative edges along the path while it is positive if there are even number of negative edges.</p>     <p>We define directed node pairs (<em>u</em>, <em>v</em>) if <em>u</em> precedes <em>v</em> from a sequential order in a random walk. This means that there is a directed path from <em>u</em> to <em>v</em> in the network since a sequential order in random walk conforms to a topological order in the network. By identifying and parameterizing source and destination nodes separately, it is possible to model asymmetric direction.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Likelihood Formulation</h3>     </div>     </header>     <p>We now describe maximum likelihood formulation that links embedding vectors to likelihood of sampled pairs. The objective function is defined as follows: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{multline} J = \sum _{(u, v) \in \mathcal {D}} [- \log P(u, v) + \sum _{j=1}^{n} - \log P(u, v_j^{\prime })]\\ + \frac{\lambda }{2} (\Vert b^{in, +} \Vert ^2 + \Vert b^{in, -} \Vert ^2 + \Vert b^{out, +} \Vert ^2 + \Vert b^{out, -} \Vert ^2)\end{multline} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$(u, v) \in \mathcal {D}$</span>     </span> is a node pair with aggregated sign and direction as defined in the Section <a class="sec" href="#sec-11">3.2</a>. For each pair (<em>u</em>, <em>v</em>), <em>n</em> noise samples <span class="inline-equation"><span class="tex">$v_j^\prime$</span>     </span> are randomly selected to form noise pairs. The latter part of the objective function regularizes bias terms in the likelihood function.</p> <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186117/images/www2018-126-img1.svg" class="img-responsive" alt="" longdesc=""/> <p>The likelihood <em>P</em>(<em>u</em>, <em>v</em>) is defined as follows: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(u, v) = {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}\sigma (W^{out}_u \cdot W^{in}_v + b_u^{out, +} + b_v^{in, +}) &#x0026; {\bf if} sign(u, v) {\gt} 0\\ \sigma (- W^{out}_u \cdot W^{in}_v + b_u^{out, -} + b_v^{in, -}) &#x0026; {\bf if} sign(u, v) {\lt} 0\\ \sigma (- W^{out}_u \cdot W^{in}_v) &#x0026; {\bf if} v \textrm { is a noise}\\ \end{array}\right.} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <em>sign</em>(<em>u</em>, <em>v</em>) represents the aggregate sign defined in Section&#x00A0;<a class="sec" href="#sec-11">3.2</a>. The first, second, and third equations define likelihood for positive, negative, and noise pairs, respectively. The Equations (<a class="eqn" href="#eq4">4</a>) consist of two components: signed proximity term <span class="inline-equation"><span class="tex">$\pm W^{out}_u \cdot W^{in}_v$</span>     </span> and bias terms <span class="inline-equation"><span class="tex">$b_u^{out, \pm }$</span>     </span>, <span class="inline-equation"><span class="tex">$b_v^{in, \pm }$</span>     </span>.</p>     <p>     <strong>Signed proximity term.</strong> The first component of the likelihood function is signed inner product proximity term <span class="inline-equation"><span class="tex">$\pm W^{out}_u \cdot W^{in}_v$</span>     </span>. For positively connected nodes, likelihood value increases as the inner product term increases. Negative and noise pairs, on the other hand, have higher likelihood value when the inner product similarity is lower. By maximizing the objective function, embedding vectors are learned to assign high similarity values for positively connected nodes while low similarity values for negatively connected nodes. The balance between positive pull and negative push is achieved within the maximum likelihood framework. As a result, nodes with similar signs on in- and out- links learn similar in- and out- embedding vectors, respectively. On the other hand, nodes with oppositely signed neighborhood structure are placed far apart from each other in the embedding vector space.</p>     <p>     <strong>Bias terms.</strong> We employ bias terms <span class="inline-equation"><span class="tex">$b_u^{out, \pm }$</span>     </span>, <span class="inline-equation"><span class="tex">$b_v^{in, \pm }$</span>     </span> as a second component of the likelihood function. According to preferential attachment, larger connectivity induces higher likelihood of additional link formation. Our likelihood formulation models this connectivity with bias terms. High bias term values result in high likelihood of link formation even if signed proximity term is small. The bias terms depict asymmetric and personalized nature of node connectivity. Bias terms are separately defined for each node and each type of role in link formation.</p>     <p>As shown in Equation (<a class="eqn" href="#eq4">4</a>), likelihood function of each link contains two bias terms: out-link bias of a source node and in-link bias of a destination node. Source and destination are determined by the aggregated direction as described in Section <a class="sec" href="#sec-11">3.2</a>. As a result, it is possible to assign asymmetric likelihood values on two reciprocal edges that link the same node pair. In addition, the likelihood of each link depends on which nodes participate in the link and formulates personalized nature of the link formation. Sign and direction define four types of role that a node can participate in link formation. For each node <em>u</em> &#x2208; <em>V</em>, we define four distinct bias factors corresponding to each type of role: positive in-link bias <span class="inline-equation"><span class="tex">$b_u^{in, +}$</span>     </span>, negative in-link bias <span class="inline-equation"><span class="tex">$b_u^{in, -}$</span>     </span>, positive out-link bias <span class="inline-equation"><span class="tex">$b_u^{out, +}$</span>     </span>, and negative out-link bias <span class="inline-equation"><span class="tex">$b_u^{out, -}$</span>     </span>. We denote |<em>V</em>| &#x00D7; 1 bias vectors corresponding to each type as <em>b</em>     <sup>      <em>in</em>, +</sup>, <em>b</em>     <sup>      <em>in</em>, &#x2212;</sup>, <em>b</em>     <sup>      <em>out</em>, +</sup>, and <em>b</em>     <sup>      <em>out</em>, &#x2212;</sup>.</p>     <p>     <strong>Gradient descent optimization.</strong> We train our model using gradient descent optimization. The derivative required to update each parameter one step in gradient descent stage is distinct for co-occurring pair and noise pair. For co-occurring pair (<em>u</em>, <em>v</em>), two weight vectors <span class="inline-equation"><span class="tex">$W^{out}_u$</span>     </span>, <span class="inline-equation"><span class="tex">$W^{in}_v$</span>     </span> and two bias factors <span class="inline-equation"><span class="tex">$b_u^{out, sign(u, v)}$</span>     </span>, <span class="inline-equation"><span class="tex">$b_v^{in, sign(u, v)}$</span>     </span> are updated. The derivative of objective function <span class="inline-equation"><span class="tex">$J_{(u, v)} = -\log P(u, v) + \frac{\lambda }{2} (|b_u^{out, sign(u, v)}|^2 + |b_v^{in, sign(u, v)}|^2)$</span>     </span> corresponding to the weight vectors and the bias factors of co-occurring pair (<em>u</em>, <em>v</em>) is as follows: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \frac{\partial J_{(u, v)}}{\partial W^{out}_u} &#x0026;= - sign(u, v) W^{in}_v (1 - P(u, v))\\\frac{\partial J_{(u, v)}}{\partial W^{in}_v} &#x0026;= - sign(u, v) W^{out}_u (1 - P(u, v))\\\frac{\partial J_{(u, v)}}{\partial b_u^{out, sign(u, v)}} &#x0026;= - (1 - P(u, v)) + \lambda b_u^{out, sign(u, v)}\\\frac{\partial J_{(u, v)}}{\partial b_v^{in, sign(u, v)}} &#x0026;= - (1 - P(u, v)) + \lambda b_v^{in, sign(u, v)} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where the likelihood <em>P</em>(<em>u</em>, <em>v</em>) is determined according to <em>sign</em>(<em>u</em>, <em>v</em>) as in the Equation (<a class="eqn" href="#eq4">4</a>). The regularization on bias terms is essential in order to prevent bias values from diverging. If regularization of bias terms were not applied, the gradient updates for bias are always positive and keep increasing the value of the bias.</p>     <p>For noise pair <span class="inline-equation"><span class="tex">$(u, v^{\prime }_j)$</span>     </span>, only two weight vectors <span class="inline-equation"><span class="tex">$W^{out}_u$</span>     </span>, <span class="inline-equation"><span class="tex">$W^{in}_{v^{\prime }_j}$</span>     </span> are updated. The derivative of objective function <span class="inline-equation"><span class="tex">$J_{(u, v^{\prime }_j)} = -\log P(u, v^{\prime }_j)$</span>     </span> corresponding to noise pair <span class="inline-equation"><span class="tex">$(u, v^{\prime }_j)$</span>     </span> is as follows: <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \frac{\partial J_{(u, v^{\prime }_j)}}{\partial W^{out}_u} &#x0026;= W^{in}_{v_j^\prime } (1 - P(u, v_j^\prime))\\\frac{\partial J_{(u, v^{\prime }_j)}}{\partial W^{in}_{v_j^\prime }} &#x0026;= W^{out}_u (1 - P(u, v_j^\prime)) \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where the likelihood <em>P</em>(&#x00B7;, &#x00B7;) is defined as the third equation in Equation (<a class="eqn" href="#eq4">4</a>).</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Social Theoretical Analysis</h3>     </div>     </header>     <p>In this section, we describe social theoretical interpretation of our model. Firstly, we claim that signed proximity term is consistent with balance theory. Then, we identify the role of bias terms in preferential attachment. These two components in the likelihood formulation establish two factors determining link formation as discussed in Section <a class="sec" href="#sec-7">2.1</a>: homophily and preferential attachment.</p>     <p>According to balance theory, positively connected nodes tend to have similar sign on their shared neighbors and have similar neighborhood structure. Through signed proximity term, our model places nodes with similarly signed neighborhood closely together and nodes with oppositely signed neighborhood far apart. We deduce that positively connected nodes have closely placed embedding vectors while negatively connected nodes are placed far apart in embedding space. Balance theory is consistent with this embedding distance structure. For example, a triad with all positive edges constructs three nodes closely placed together, a triad with two negative edges constructs one node far apart from the other two closely placed nodes, and a triad with all negative edges places all three nodes far apart from each other. The triad not allowed in balance theory is incompatible with the transitivity of closeness in the embedding space. Therefore, our embedding method naturally encourages the embedding vectors to be learned to follow balance theory.</p>     <p>The bias terms model individual connectivity, while inner product term models symmetric similarity between nodes. In the learning process, every pair example increases bias values of corresponding type. We interpret bias terms of each node as a node connectivity; it is expected that large out-link bias and large in-link bias are learned for nodes with high out-degree and nodes with high in-degree, respectively. According to the likelihood formulation in (<a class="eqn" href="#eq4">4</a>), link formation likelihood increases as the bias term increases. Therefore, the bias terms model preferential attachment process where bias terms for nodes with high connectivity are inclined toward increasing link formation likelihood.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Additional Optimization Techniques</h3>     </div>     </header>     <p>We suggest two additional strategies to accelerate the learning process of networks: deletion of nodes with degree one and subsampling of high-degree nodes.</p>     <p>A large portion of nodes in real-world networks have degree one, since degree distributions in real-world networks follow a power-law distribution. For example, more than 20% of nodes in our dataset have degree one as shown in Table <a class="tbl" href="#tab3">3</a>. In the random walk sequences, a node with positive degree one appears with the only node linked to it. Consequently the degree-one node shares all of its neighborhood structure with its only neighbor; therefore the learning process performs huge number of redundant calculations. To prevent this, we eliminate nodes with positive degree-one and learn embedding for reduced network. Then, for each eliminated positive degree-one node, embedding vector is copied from the embedding vector of the only neighbor. By eliminating nodes with degree one, we reduce the number of parameters to learn in the optimization process.</p>     <p>Real-world networks also contain nodes with very high degrees according to the power-law distribution. High-degree nodes are connected to a huge number of nodes, but most of the connections are uninformative at inferring proximity structure of either high-degree nodes themselves or neighboring nodes. Inspired by subsampling in SGNS model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>], we subsample high-degree nodes in the process of random walk generation. Each node <em>u</em> is thrown away with probability <span class="inline-equation"><span class="tex">$1 - \sqrt {\frac{t}{p(u)}}$</span>     </span> where <em>p</em>(<em>u</em>) is the degree of <em>u</em> divided by the number of edges and <em>t</em> is a threshold hyperparameter for <em>p</em>(<em>u</em>). We fix <em>t</em> to 0.001 which is a frequent default value in the SGNS model. Subsampling enables more informative random walk sampling process because of the increased effective window size.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.6</span> Time Complexity Analysis</h3>     </div>     </header>     <p>In this section, we provide proofs on linear scalability of the time complexity of SIDE. Theorem <a class="enc" href="#enc4">3.4</a>, which is the main result of this section, is proved using three lemmas for each stage of SIDE. We assume that the graph is stored in memory so that each step in random walk requires constant time.</p>     <div class="lemma" id="enc1">     <Label>Lemma 3.1.</Label>     <p>(Time Complexity of Random Walk Generation) Random walk generation takes <em>O</em>(|<em>V</em>|<em>wl</em>) time.</p>     </div>     <div class="proof" id="proof1">     <Label>Proof.</Label>     <p>We generate <em>l</em> steps in <em>w</em> random walks for |<em>V</em>| nodes. Each random walk step requires random choice of out-linked nodes and takes constant time. Combining the above two results, the time complexity of random walk generation process is <em>O</em>(|<em>V</em>|<em>wl</em>).</p>     </div>     <div class="lemma" id="enc2">     <Label>Lemma 3.2.</Label>     <p>(Time Complexity of Pair Sampling Process) Pair sampling takes <em>O</em>(|<em>V</em>|<em>wlk</em>      <sup>2</sup>) time.</p>     </div>     <div class="proof" id="proof2">     <Label>Proof.</Label>     <p> We generate <em>k</em> node pairs for each random walk step, which sums up to |<em>V</em>|<em>wlk</em>. Each node pair sampling requires the calculation of signs which takes <em>O</em>(<em>k</em>) time in average, because we need to count the number of positive and negative edges in the path between the node pairs. Therefore, the time complexity for pair sampling is <em>O</em>(|<em>V</em>|<em>wlk</em>      <sup>2</sup>).</p>     </div>     <div class="lemma" id="enc3">     <Label>Lemma 3.3.</Label>     <p>(Time Complexity of Gradient Descent Learning) Gradient descent learning takes <em>O</em>(|<em>V</em>|<em>wlk</em>(<em>n</em> + 1)<em>d</em>) time.</p>     </div>     <div class="proof" id="proof3">     <Label>Proof.</Label>     <p>We perform <em>n</em> noise sampling for |<em>V</em>|<em>wlk</em> connected pairs. The number of node pairs sampled from random walk is |<em>V</em>|<em>wlk</em> and the number of node pairs from noise sampling is |<em>V</em>|<em>wlkn</em>, summing up to |<em>V</em>|<em>wlk</em>(<em>n</em> + 1). Since a gradient descent update for each node pair takes <em>O</em>(<em>d</em>), the total time complexity of gradient descent learning is <em>O</em>(|<em>V</em>|<em>wlk</em>(<em>n</em> + 1)<em>d</em>).</p>     </div>     <div class="theorem" id="enc4">     <Label>Theorem 3.4.</Label>     <p>(Time Complexity of SIDE) The training of SIDE takes <em>O</em>(|<em>V</em>|) time.</p>     </div>     <div class="proof" id="proof4">     <Label>Proof.</Label>     <p> SIDE consists of three stages: random walk generation, pair sampling, and gradient descent learning. Combining the preceding three Lemmas, and considering that the parameters <em>w</em>, <em>l</em>, <em>k</em>, <em>n</em>, and <em>d</em> are constants, SIDE shows the time complexity linear in the number of nodes |<em>V</em>|.</p>     </div>    </section>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>In this section, we experimentally evaluate the performance of SIDE to answer the following questions.</p>    <ul class="list-no-style">     <li id="list10" label="&#x2022;"><strong>Q1. (Sign Prediction)</strong>How predictive is SIDE at inferring unobserved link signs? (Section <a class="sec" href="#sec-18">4.2</a>)<br/></li>     <li id="list11" label="&#x2022;"><strong>Q2. (Representation)</strong>How effectively does SIDE represent signed directed relationships? (Section <a class="sec" href="#sec-19">4.3</a>)<br/></li>     <li id="list12" label="&#x2022;"><strong>Q3. (Speed and Scalability)</strong>How fast and scalable is the training of SIDE compared to the baselines? (Section <a class="sec" href="#sec-20">4.4</a>)<br/></li>     <li id="list13" label="&#x2022;"><strong>Q4. (Optimization)</strong>How effective are optimization techniques of SIDE in terms of time and accuracy? (Section <a class="sec" href="#sec-21">4.5</a>)<br/></li>    </ul>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Experimental Setup</h3>     </div>     </header>    <p>     <strong>Data.</strong> We conduct experiments on three real-world datasets of signed directed social networks: Epinions, Slashdot, and Wikipedia. Epinions is a product review site where users can form trust and distrust relationships with other users. Slashdot is a technology news site which allows users to annotate other users as friends or foes. Wikipedia is an encyclopedia site where users vote for or against other users in order to determine admin promotion. The statistics of the datasets are summarized in Table <a class="tbl" href="#tab3">3</a>.</p>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Statistic of the Datasets.</span>     </div>     <table class="table"> 				 <thead>       <tr>        <th style="text-align:left;"/>        <th style="text-align:right;">        <strong><a class="fn" href="#fn4" id="foot-fn4"><sup>a</sup></a> Epinions</strong>        </th>        <th style="text-align:right;">        <strong><a class="fn" href="#fn5" id="foot-fn5"><sup>b</sup></a> Slashdot</strong>        </th>        <th style="text-align:right;">        <strong><a class="fn" href="#fn3" id="foot-fn3"><sup>c</sup></a> Wikipedia</strong>        </th>       </tr> 						</thead>      <tbody>       <tr>        <td style="text-align:left;"># of nodes</td>        <td style="text-align:right;">131,828</td>        <td style="text-align:right;">82,140</td>        <td style="text-align:right;">7,118</td>       </tr>       <tr>        <td style="text-align:left;"># of edges</td>        <td style="text-align:right;">841,372</td>        <td style="text-align:right;">549,202</td>        <td style="text-align:right;">103,675</td>       </tr>       <tr>        <td style="text-align:left;">% of positive edges</td>        <td style="text-align:right;">85.3 %</td>        <td style="text-align:right;">76.1 %</td>        <td style="text-align:right;">78.4 %</td>       </tr>       <tr>        <td style="text-align:left;">% of negative edges</td>        <td style="text-align:right;">14.7 %</td>        <td style="text-align:right;">23.9 %</td>        <td style="text-align:right;">21.6 %</td>       </tr>       <tr>        <td style="text-align:left;">% of nodes with</td>        <td style="text-align:right;">38.2 %</td>        <td style="text-align:right;">25.5 %</td>        <td style="text-align:right;">24.9 %</td>       </tr>       <tr>        <td style="text-align:left;">positive degree one</td>        <td/>        <td/>        <td/>       </tr>      </tbody>      <tfoot>       <tr>        <td>        <Footnote id="fn4">         <p id="fn4">          <Label><sup>a</sup></Label><a href="http://www.trustlet.org/wiki/Extended_Epinions_dataset" target="_blank">http://www.trustlet.org/wiki/Extended_Epinions_dataset</a> </p>        </Footnote>        <Footnote id="fn5">         <p id="fn5">          <Label><sup>b</sup></Label><a href="http://dai-labor.de/IRML/datasets" target="_blank">http://dai-labor.de/IRML/datasets</a> </p>        </Footnote>        <Footnote id="fn6">         <p id="fn6">          <Label><sup>c</sup></Label><a href="http://snap.stanford.edu/data/wiki-Vote.html" target="_blank">http://snap.stanford.edu/data/wiki-Vote.html</a> </p>        </Footnote>        </td>        <td/>        <td/>        <td/>       </tr>      </tfoot>     </table>     </div>     <p>     <strong>Baselines.</strong> We compare SIDE to a feature engineering method and five embedding methods. The competitors are as follows:</p>     <ul class="list-no-style">     <li id="list14" label="&#x2022;">Feature Engineering (FE)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>]. This model defines two types of hand-engineered features for each edge: signed/directed degrees of two participating nodes and sign/direction configurations of two-step paths from the source to the target.<br/></li>     <li id="list15" label="&#x2022;">Node2vec (N2V)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0007">7</a>]. This is a random walk based network embedding method for unsigned network. Embedding is learned on positive subgraph with this method.<br/></li>     <li id="list16" label="&#x2022;">Matrix Factorization (MF)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0008">8</a>]. This method learns low rank structure of signed network by matrix factorization.<br/></li>     <li id="list17" label="&#x2022;">Balanced Normalized Signed Laplacian (BNS)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0038">38</a>]. This method applies a spectral embedding on modified Laplacian matrix which balances positive and negative links.<br/></li>     <li id="list18" label="&#x2022;">Signed Network Embedding (SNE)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0037">37</a>]. This method utilizes log-bilinear model with random walk sampling.<br/></li>     <li id="list19" label="&#x2022;">Signed Network Embedding (SiNE)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>]. This model trains deep neural network to make a distinction between positively connected nodes and negatively connected nodes.<br/></li>     </ul>     <p>     <strong>Parameter.</strong> For random walk generation and embedding dimension, we use the parameter settings in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>]: <em>w</em> = 80, <em>l</em> = 40, and <em>d</em> = 128. We set optimal context size <em>k</em>, noise sampling size <em>n</em>, and regularization parameter <em>&#x03BB;</em> differently to each dataset to gain the best performance. For baseline methods, we set the dimension to 128 and use the same settings for other parameters as those suggested in their papers, respectively.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Link Sign Prediction</h3>     </div>     </header>     <p>We assess the predictive performance of SIDE in link sign prediction task. Link sign prediction is the task of predicting unobserved signs of existing edges in the test set. In order to focus on the performance of embedding methods, we train a simple logistic regression model with embedding vectors as features. Since an embedding vector is defined for each node, we derive an edge feature by combining two vectors of source and destination nodes. We use two methods for the combination: element-wise product and concatenation. Concatenation always outperforms element-wise product except for the results in Epinions dataset. We use 5-fold cross validation. Each training set is used to train both embedding vectors and logistic regression model.</p>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Performance on prediction of link sign. SIDE shows the state-of-the-art performance in link sign prediction task.</span>     </div>     <table class="table"> 				 <thead>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">        <strong>SIDE</strong>        </th>        <th style="text-align:center;">FE</th>        <th style="text-align:center;">N2V</th>        <th style="text-align:center;">MF</th>        <th style="text-align:center;">BNS</th>        <th style="text-align:center;">SNE</th>        <th>SiNE</th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">        <strong>(proposed)</strong>        </th>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th/>       </tr> 						</thead>      <tbody>       <tr>        <td style="text-align:center;">AUC</td>        <td style="text-align:center;">Epinions</td>        <td style="text-align:center;">        <strong>0.967</strong>        </td>        <td style="text-align:center;">0.951</td>        <td style="text-align:center;">0.764</td>        <td style="text-align:center;">0.920</td>        <td style="text-align:center;">0.893</td>        <td style="text-align:center;">0.820</td>        <td>0.860</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Slashdot</td>        <td style="text-align:center;">        <strong>0.889</strong>        </td>        <td style="text-align:center;">        <strong>0.889</strong>        </td>        <td style="text-align:center;">0.697</td>        <td style="text-align:center;">0.877</td>        <td style="text-align:center;">0.842</td>        <td style="text-align:center;">0.746</td>        <td>0.816</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Wiki</td>        <td style="text-align:center;">        <strong>0.901</strong>        </td>        <td style="text-align:center;">0.879</td>        <td style="text-align:center;">0.648</td>        <td style="text-align:center;">0.875</td>        <td style="text-align:center;">0.861</td>        <td style="text-align:center;">0.762</td>        <td>0.790</td>       </tr>       <tr>        <td style="text-align:center;">F1</td>        <td style="text-align:center;">Epinions</td>        <td style="text-align:center;">        <strong>0.972</strong>        </td>        <td style="text-align:center;">0.960</td>        <td style="text-align:center;">0.893</td>        <td style="text-align:center;">0.957</td>        <td style="text-align:center;">0.948</td>        <td style="text-align:center;">0.924</td>        <td>0.922</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Slashdot</td>        <td style="text-align:center;">        <strong>0.911</strong>        </td>        <td style="text-align:center;">0.906</td>        <td style="text-align:center;">0.811</td>        <td style="text-align:center;">0.910</td>        <td style="text-align:center;">0.895</td>        <td style="text-align:center;">0.874</td>        <td>0.887</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">Wiki</td>        <td style="text-align:center;">        <strong>0.918</strong>        </td>        <td style="text-align:center;">0.907</td>        <td style="text-align:center;">0.879</td>        <td style="text-align:center;">0.913</td>        <td style="text-align:center;">0.901</td>        <td style="text-align:center;">0.882</td>        <td>0.882</td>       </tr>      </tbody>     </table>     </div>     <p>We report <em>AUC</em> and <em>F</em>1 &#x2212; <em>score</em> of methods as shown in Table <a class="tbl" href="#tab4">4</a>. The higher the metrics are, the higher the predictive accuracy is. We make the following observations:</p>     <ul class="list-no-style">     <li id="list20" label="&#x2022;">SIDE achieves the best accuracy at link sign prediction in terms of both <em>AUC</em> and <em>F</em>1 &#x2212; <em>score</em>. This proves the superior predictive accuracy of our method.<br/></li>     <li id="list21" label="&#x2022;">Feature engineering (FE) method is the only comparable competitor. FE uses features specifically targeted for link sign prediction task and outperforms all the generic embedding methods except for SIDE.<br/></li>     <li id="list22" label="&#x2022;">SIDE and FE are the only methods that are based on socio-psychological theories. This shows the importance of social theory in explaining sign structure in the real-world networks.<br/></li>     <li id="list23" label="&#x2022;">Node2vec (N2V) method, which is learned on positive subgraph, shows the worst accuracy because no sign information is fed in the learning process. This shows the importance of leveraging sign information to accurately infer unobserved edge sign.<br/></li>     </ul>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Embedding Analysis</h3>     </div>     </header>     <p>We examine the correctness of our algorithm by showing that learned parameter values are consistent with our design goals. We inspect two components in our likelihood formulation: signed proximity term and bias terms.</p>     <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Analysis of Signed proximity terms. Signed proximity average values for positive and negative links are clearly separated, more than standard deviation.</span>     </div>     <table class="table"> 				 <thead>       <tr>        <th style="text-align:center;"/>        <th colspan="2" style="text-align:center;">Positive<hr/>        </th>        <th colspan="2" style="text-align:center;">Negative<hr/>        </th>        <th colspan="2" style="text-align:center;">P-value<hr/>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">avg.</th>        <th style="text-align:center;">sth.</th>        <th style="text-align:center;">avg.</th>        <th style="text-align:center;">sth.</th>        <th style="text-align:center;">WT</th>        <th>KS</th>       </tr> 						</thead>      <tbody>       <tr>        <td style="text-align:center;">Epinions</td>        <td style="text-align:center;">3.685</td>        <td style="text-align:center;">1.827</td>        <td style="text-align:center;">-2.763</td>        <td style="text-align:center;">1.631</td>        <td style="text-align:center;"> < 0.0001</td>        <td>< 0.0001</td>       </tr>       <tr>        <td style="text-align:center;">Slashdot</td>        <td style="text-align:center;">3.060</td>        <td style="text-align:center;">1.661</td>        <td style="text-align:center;">-0.745</td>        <td style="text-align:center;">1.408</td>        <td style="text-align:center;"> < 0.0001</td>        <td>< 0.0001</td>       </tr>       <tr>        <td style="text-align:center;">Wikipedia</td>        <td style="text-align:center;">2.538</td>        <td style="text-align:center;">1.073</td>        <td style="text-align:center;">-0.392</td>        <td style="text-align:center;">1.573</td>        <td style="text-align:center;"> < 0.0001</td>        <td>< 0.0001</td>       </tr>      </tbody>     </table>     </div>     <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186117/images/www2018-126-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Correlation of bias and degree. Bias value increases as the degree quintile increases. This is consistent with the preferential attachment interpretation of bias terms.</span>     </div>     </figure>     <p>We design signed proximity term to distinguish distances of positive connections from those of negative connections. To check whether the inner product similarity term successfully represents link sign structure, we perform Welch&#x0027;s t-test and Kolmogorov-Smirnov test on inner product values between sets of positive and negative links. Welch&#x0027;s t-test examines whether two populations have equal means without any assumption on variance. Kolmogorov-Smirnov test is a test on the equality of two probability distribution. Smaller magnitude of p-values for both tests indicates a more distinct distribution of inner product similarity between positive edges and negative edges. Table <a class="tbl" href="#tab5">5</a> shows the result of the tests with average and standard deviation of positive and negative links in our dataset. P-values of the tests demonstrate the strong evidence on difference of both means and distributions between two edge sets. In addition, the averages for positive and negative link sets are clearly separated, more than the magnitude of corresponding standard deviations.</p>     <p>We verify the preferential attachment interpretation of the learning process of our bias terms. We divide nodes into 5 buckets according to degree quintiles. Then we calculate the average sum of positive and negative bias values for each bucket. We add positive and negative bias because indirect connections might increase bias of opposite signs. Figure <a class="fig" href="#fig2">2</a> shows the relationship between average bias values and node degree. As node degree increases, the expected value of bias term increases in both in-degree and out-degree cases. This shows the validity of our model assumption that high degree nodes would learn high bias values. We only show the result for Epinions dataset since other datasets show similar pattern with different scale of bias values.</p>    </section>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Speed and Scalability</h3>     </div>     </header>     <p>To show the computational efficiency of SIDE, we compare SIDE with two recently proposed signed network embedding methods: SNE and SiNE. We extract principal submatrices of the Epinions network varying the number of nodes from 30,000 to 120,000 and estimate the training time of each method. In this experiment, we set both context size <em>k</em> and noise sampling size <em>n</em> to 5 which is optimal for Epinions dataset. SiNE is trained with only one epoch instead of 100 epochs as suggested in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>] because even one epoch takes much larger time than SIDE.</p>     <p>As shown in Figure <a class="fig" href="#fig3">3</a>, the running time of SIDE linearly increases with regard to the number of nodes, as described in Theorem&#x00A0;<a class="enc" href="#enc4">3.4</a>. Also, SIDE runs up to 1.8 &#x00D7; faster than other methods with smaller slopes. The superior speed of SIDE comes from the concise likelihood formulation and the limited number of parameters for each gradient descent update. In contrast, SiNE updates additional parameters of the deep neural network architecture and SNE updates embedding vectors for all nodes along the random walk sequence. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186117/images/www2018-126-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Scalability of SIDE. Our algorithm runs up to 1.8x faster than baseline methods, and shows linear scalability on the number of nodes with the smallest slope.</span>      </div>     </figure>     </p>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Effect of Optimization</h3>     </div>     </header>     <p>We show the effects of optimization techniques, subsampling of high-degree nodes, and deletion of one-degree nodes, on the performance of SIDE. We set optimal context size <em>k</em> and noise sampling size <em>n</em> to 7 and 1, respectively. In Figure <a class="fig" href="#fig4">4</a>, <em>S</em> denotes subsampling of high-degree nodes and <em>D</em> denotes deletion of one-degree nodes. The activation of each optimization technique is indicated as O/X sign next to S and D. Note that the two techniques give a trade-off between the running time and the accuracy. The subsampling technique increases accuracy with longer running time. Subsampling discards high-degree node randomly; since the random walk continues until the required length is met, the sampling process takes a little longer. However, by skipping some uninformative nodes, sampled nodes provide more information to the algorithm. On the other hand, deletion of degree-one nodes decreases the accuracy with faster running time. The discarded degree-one node copies the embedding vector of the only node linked to it. Because of the smaller number of parameters to learn, the training time decreases at a cost of small loss in accuracy. Overall, employing both optimization techniques leads to increased accuracy and decreased training time as shown in Figure <a class="fig" href="#fig4">4</a>. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186117/images/www2018-126-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Running time-accuracy tradeoff of optimization techniques for SIDE. Subsampling increases accuracy while deletion of degree-one node decreases training time in the expense of small accuracy loss. Better running time-accuracy tradeoff is achieved by using both optimization techniques.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Related Works</h2>     </div>    </header>    <p>Recently, there has been much interests on network embedding in data mining community [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>]. There are three general categories which existing methods fall into: models based on matrix factorization, models exploiting deep neural networks, and models learning from truncated random walks.</p>    <p>Matrix factorization is one of the most straightforward approach to reduce the adjacency matrix into low-dimensional space. Spectral embedding method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] applies eigendecomposition on Laplacian matrix of the network in order to get useful low-dimensional representations. Cao et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] suggested factorizing k-step transition matrix to learn global structural information. Mingdong et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] introduced SVD on high-order proximity matrix to capture asymmetric transitivity in directed network.</p>    <p>Other models exploit the deep neural network to learn non-linear structural information. Tian et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] leveraged stacked autoencoder to reconstruct normalized similarity matrix. Wang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] combined deep autoencoder and Laplacian eigenmap to model second-order and first-order proximities, respectively.</p>    <p>Random walk based methods utilize language model, especially skipgram model, to learn co-occurrence statistic from simulated random walks. Perozzi et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] first suggested the framework of random walk based network embedding method. Grover and Leskovec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] further extended the random walk process with distorted probability to mimic breadth first search (BFS) and depth first search (DFS). Chen et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] proposed to additionally utilize group label and model group structure by group embedding.</p>    <p>However, none of the above approaches has straightforward generalization to learn network embedding for signed directed network. Some methods tackle signed network embedding in the context of spectral embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. These methods, however, are applicable only to undirected networks and are not scalable because of the complexity of eigendecomposition. Wang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] exploit a deep learning framework for signed network. Although the framework leverages nonlinearity and complexity of neural network architectures, it does not model edge directions. Yuan et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] extended log-bilinear model to support sign and direction modeling. Log-bilinear formulation does not consider balance theoretic nature of multi-step connections and shows suboptimal efficiency in training process. SIDE provides fast and linearly scalable method to learn network embedding in the most general settings where edges have both sign and direction. In addition, our embedding space is intuitively explainable in terms of socio-psychological theories.</p>   </section>   <section id="sec-23">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions</h2>     </div>    </header>    <p>We propose SIDE, a fast and accurate network embedding method to represent signed directed network. We construct SIDE to overcome three challenges: consistent interpretation of both signs, representation of individual linking tendency, and utilization of multi-step connections. SIDE interprets negative edges as an indication of remoteness, and models asymmetric direction with biases. Sign and direction aggregation along multi-step connections preserves sign and direction information. Social theoretical analysis shows the expressiveness and interpretability of our model components. We also propose techniques to optimize truncated random walk based network embedding frameworks. We prove that the complexity of our method is linear in the number of nodes. Experimental results show that SIDE outperforms competitors in link sign prediction task, well represents sign and direction into embedding space, and is learned efficiently.</p>   </section>   <section id="sec-24">    <header>     <div class="title-info">     <h2>Acknowledgment</h2>     </div>    </header>    <p>This work was supported by the ICT R&#x0026;D program of MSIT/IITP (No.2017-0-01772 , Development of QA systems for Video Story Understanding to pass the Video Turing Test). The ICT at Seoul National University provides research facilities for this study. The Institute of Engineering Research at Seoul National University provided research facilities for this work. U Kang is the corresponding author.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In <em>      <em>Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</em>     </em>. ACM, 891&#x2013;900.</li>     <li id="BibPLXBIB0002" label="[2]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for learning graph representations. In <em>      <em>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</em>     </em>. AAAI Press, 1145&#x2013;1152.</li>     <li id="BibPLXBIB0003" label="[3]">Dorwin Cartwright and Frank Harary. 1956. Structural balance: a generalization of Heider&#x0027;s theory.<em>      <em>Psychological review</em>     </em>63, 5 (1956), 277.</li>     <li id="BibPLXBIB0004" label="[4]">Jifan Chen, Qi Zhang, and Xuanjing Huang. 2016. Incorporate group information to enhance network embedding. In <em>      <em>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</em>     </em>. ACM, 1901&#x2013;1904.</li>     <li id="BibPLXBIB0005" label="[5]">Fan&#x00A0;RK Chung. 1997. <em>      <em>Spectral graph theory</em>     </em>. Vol.&#x00A0;92. American Mathematical Soc.</li>     <li id="BibPLXBIB0006" label="[6]">Scott&#x00A0;A Golder and Sarita Yardi. 2010. Structural predictors of tie formation in twitter: Transitivity and mutuality. In <em>      <em>Social Computing (SocialCom), 2010 IEEE Second International Conference on. IEEE</em>     </em>, 88&#x2013;95.</li>     <li id="BibPLXBIB0007" label="[7]">Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In <em>      <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 855&#x2013;864.</li>     <li id="BibPLXBIB0008" label="[8]">Cho-Jui Hsieh, Kai-Yang Chiang, and Inderjit&#x00A0;S Dhillon. 2012. Low rank modeling of signed networks. In <em>      <em>Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 507&#x2013;515.</li>     <li id="BibPLXBIB0009" label="[9]">Xiao Huang, Jundong Li, and Xia Hu. 2017. Label Informed Attributed Network Embedding. In <em>      <em>Proceedings of 10th ACM International Conference on Web Search and Data Mining (WSDM)</em>     </em>. ACM.</li>     <li id="BibPLXBIB0010" label="[10]">Jinhong Jung, Woojeong Jin, Lee Sael, and U. Kang. 2016. Personalized Ranking in Signed Networks Using Signed Random Walk with Restart. In <em>      <em>IEEE 16th International Conference on Data Mining, ICDM 2016, December 12-15, 2016, Barcelona, Spain</em>     </em>. 973&#x2013;978.</li>     <li id="BibPLXBIB0011" label="[11]">Jinhong Jung, Namyong Park, Lee Sael, and U. Kang. 2017. BePI: Fast and Memory-Efficient Method for Billion-Scale Random Walk with Restart. In <em>      <em>Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017</em>     </em>. 789&#x2013;804.</li>     <li id="BibPLXBIB0012" label="[12]">Jinhong Jung, Kijung Shin, Lee Sael, and U. Kang. 2016. Random Walk with Restart on Large Graphs Using Block Elimination. <em>      <em>ACM Trans. Database Syst.</em>     </em>41, 2 (2016), 12. <a class="link-inline force-break" href="https://doi.org/10.1145/2901736"      target="_blank">https://doi.org/10.1145/2901736</a></li>     <li id="BibPLXBIB0013" label="[13]">J&#x00E9;r&#x00F4;me Kunegis, Julia Preusse, and Felix Schwagereit. 2013. What is the added value of negative links in online social networks?. In <em>      <em>Proceedings of the 22nd international conference on World Wide Web</em>     </em>. ACM, 727&#x2013;736.</li>     <li id="BibPLXBIB0014" label="[14]">J&#x00E9;r&#x00F4;me Kunegis, Stephan Schmidt, Andreas Lommatzsch, J&#x00FC;rgen Lerner, Ernesto&#x00A0;W De&#x00A0;Luca, and Sahin Albayrak. 2010. Spectral analysis of signed graphs for clustering, prediction and visualization. In <em>      <em>Proceedings of the 2010 SIAM International Conference on Data Mining</em>     </em>. SIAM, 559&#x2013;570.</li>     <li id="BibPLXBIB0015" label="[15]">Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010. Predicting positive and negative links in online social networks. In <em>      <em>Proceedings of the 19th international conference on World Wide Web</em>     </em>. ACM, 641&#x2013;650.</li>     <li id="BibPLXBIB0016" label="[16]">David Liben-Nowell and Jon Kleinberg. 2007. The link-prediction problem for social networks. <em>      <em>Journal of the Association for Information Science and Technology</em>     </em>58, 7(2007), 1019&#x2013;1031.</li>     <li id="BibPLXBIB0017" label="[17]">Miller McPherson, Lynn Smith-Lovin, and James&#x00A0;M Cook. 2001. Birds of a feather: Homophily in social networks. <em>      <em>Annual review of sociology</em>     </em>27, 1 (2001), 415&#x2013;444.</li>     <li id="BibPLXBIB0018" label="[18]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. <em>      <em>arXiv preprint arXiv:1301.3781</em>     </em>(2013).</li>     <li id="BibPLXBIB0019" label="[19]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&#x00A0;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 3111&#x2013;3119.</li>     <li id="BibPLXBIB0020" label="[20]">Michael Mitzenmacher. 2004. A brief history of generative models for power law and lognormal distributions. <em>      <em>Internet mathematics</em>     </em>1, 2 (2004), 226&#x2013;251.</li>     <li id="BibPLXBIB0021" label="[21]">Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric transitivity preserving graph embedding. In <em>      <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 1105&#x2013;1114.</li>     <li id="BibPLXBIB0022" label="[22]">Haekyu Park, Jinhong Jung, and U. Kang. 2017. A comparative study of matrix factorization and random walk with restart in recommender systems. In <em>      <em>2017 IEEE International Conference on Big Data, BigData 2017, Boston, MA, USA, December 11-14, 2017</em>     </em>. 756&#x2013;765.</li>     <li id="BibPLXBIB0023" label="[23]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In <em>      <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 701&#x2013;710.</li>     <li id="BibPLXBIB0024" label="[24]">Kenneth&#x00A0;E Read. 1954. Cultures of the central highlands, New Guinea. <em>      <em>Southwestern Journal of Anthropology</em>     </em>10, 1 (1954), 1&#x2013;43.</li>     <li id="BibPLXBIB0025" label="[25]">Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network data. <em>      <em>AI magazine</em>     </em>29, 3 (2008), 93.</li>     <li id="BibPLXBIB0026" label="[26]">Kijung Shin, Jinhong Jung, Lee Sael, and U Kang. 2015. BEAR: Block Elimination Approach for Random Walk with Restart on Large Graphs. In <em>      <em>SIGMOD</em>     </em>.</li>     <li id="BibPLXBIB0027" label="[27]">Jiliang Tang, Charu Aggarwal, and Huan Liu. 2016. Node classification in signed social networks. In <em>      <em>Proceedings of the 2016 SIAM International Conference on Data Mining</em>     </em>. SIAM, 54&#x2013;62.</li>     <li id="BibPLXBIB0028" label="[28]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In <em>      <em>Proceedings of the 24th International Conference on World Wide Web</em>     </em>. ACM, 1067&#x2013;1077.</li>     <li id="BibPLXBIB0029" label="[29]">Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. 2014. Learning Deep Representations for Graph Clustering. In <em>      <em>Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</em>     </em>. AAAI Press, 1293&#x2013;1299.</li>     <li id="BibPLXBIB0030" label="[30]">Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In <em>      <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 1225&#x2013;1234.</li>     <li id="BibPLXBIB0031" label="[31]">Suhang Wang, Jiliang Tang, Charu Aggarwal, Yi Chang, and Huan Liu. 2017. Signed Network Embedding in Social Media. In <em>      <em>Proceedings of the 2017 SIAM international conference on data mining</em>     </em>. SIAM.</li>     <li id="BibPLXBIB0032" label="[32]">Scott White and Padhraic Smyth. 2005. A spectral clustering approach to finding communities in graphs. In <em>      <em>Proceedings of the 2005 SIAM international conference on data mining</em>     </em>. SIAM, 274&#x2013;285.</li>     <li id="BibPLXBIB0033" label="[33]">Cheng Yang and Zhiyuan Liu. 2015. Comprehend deepwalk as matrix factorization. <em>      <em>arXiv preprint arXiv:1501.00358</em>     </em>(2015).</li>     <li id="BibPLXBIB0034" label="[34]">Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting Semi-Supervised Learning with Graph Embeddings. In <em>      <em>Proceedings of The 33rd International Conference on Machine Learning</em>     </em>. 40&#x2013;48.</li>     <li id="BibPLXBIB0035" label="[35]">Minji Yoon, Woojeong Jin, and U. Kang. 2018. Fast and Accurate Random Walk with Restart on Dynamic Graphs with Guarantees. In <em>      <em>The Web Conference, 2018</em>     </em>.</li>     <li id="BibPLXBIB0036" label="[36]">Minji Yoon, Jinhong Jung, and U. Kang. 2018. TPA: Fast, Scalable, and Accurate Method for Approximate Random Walk with Restart on Billion Scale Graphs. In <em>      <em>34th IEEE International Conference on Data Engineering, ICDE 2018, Paris, France, April 16-19, 2018</em>     </em>.</li>     <li id="BibPLXBIB0037" label="[37]">Shuhan Yuan, Xintao Wu, and Yang Xiang. 2017. SNE: Signed Network Embedding. In <em>      <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>     </em>. Springer, 183&#x2013;195.</li>     <li id="BibPLXBIB0038" label="[38]">Quan Zheng and David&#x00A0;B Skillicorn. 2015. Spectral embedding of signed networks. In <em>      <em>Proceedings of the 2015 SIAM International Conference on Data Mining</em>     </em>. SIAM, 55&#x2013;63.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://www.epinions.com">www.epinions.com</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://slashdot.org">slashdot.org</a></p> <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="http://konect.uni-koblenz.de/networks/ucidata-gama">http://konect.uni-koblenz.de/networks/ucidata-gama</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/> ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186117">https://doi.org/10.1145/3178876.3186117</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
