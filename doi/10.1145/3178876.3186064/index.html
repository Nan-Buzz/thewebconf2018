<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Learning on Partial-Order Hypergraphs</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Learning on Partial-Order
          Hypergraphs</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Fuli</span> <span class=
          "surName">Feng</span>, National University of Singapore,
          <a href=
          "mailto:fulifeng93@gmail.com">fulifeng93@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Xiangnan</span> <span class=
          "surName">He</span>, National University of
          Singapore<a class="fn" href="#fn1" id=
          "foot-fn1"><sup>⁎</sup></a>, <a href=
          "mailto:xiangnanhe@gmail.com">xiangnanhe@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Yiqun</span> <span class=
          "surName">Liu</span>, Tsinghua University, <a href=
          "mailto:yiqunliu@tsinghua.edu.cn">yiqunliu@tsinghua.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Liqiang</span> <span class=
          "surName">Nie</span>, Shandong University, <a href=
          "mailto:nieliqiang@gmail.com">nieliqiang@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Tat-Seng</span> <span class=
          "surName">Chua</span>, National University of Singapore,
          <a href=
          "mailto:chuats@comp.nus.edu.sg">chuats@comp.nus.edu.sg</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186064"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186064</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Graph-based learning methods explicitly consider
        the relations between two entities (<em>i.e.,</em>
        vertices) for learning the prediction function. They have
        been widely used in semi-supervised learning, manifold
        ranking, and clustering, among other tasks. Enhancing the
        expressiveness of simple graphs, hypergraphs formulate an
        edge as a link to multiple vertices, so as to model the
        higher-order relations among entities. For example,
        hyperedges in a hypergraph can be used to encode the
        similarity among vertices.</small></p>
        <p><small>To the best of our knowledge, all existing
        hypergraph structures represent the hyperedge as an
        unordered set of vertices, without considering the possible
        ordering relationship among vertices. In real-world data,
        ordering relations commonly exist, such as in graded
        categorical features (<em>e.g.,</em> users’ ratings on
        movies) and numerical features (<em>e.g.,</em> monthly
        income of customers). When constructing a hypergraph,
        ignoring such ordering relations among entities will lead
        to severe information loss, resulting in suboptimal
        performance of the subsequent learning
        algorithms.</small></p>
        <p><small>In this work, we address the inherent limitation
        of existing hypergraphs by proposing a new data structure
        named <em>Partial-Order Hypergraph</em>, which specifically
        injects the partially ordering relations among vertices
        into a hyperedge. We develop regularization-based learning
        theories for partial-order hypergraphs, generalizing
        conventional hypergraph learning by incorporating logical
        rules that encode the partial-order relations. We apply our
        proposed method to two applications: university ranking
        from Web data and popularity prediction of online content.
        Extensive experiments demonstrate the superiority of our
        proposed partial-order hypergraphs, which consistently
        improve over conventional hypergraph methods.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Web mining;</strong> <em>Web applications;</em> •
        <strong>Computing methodologies</strong> → <em>Spectral
        methods;</em> Semi-supervised learning settings; •
        <strong>Applied computing</strong> →
        <em>Education;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Hypergraph</small>,</span>
          <span class="keyword"><small>Partial-Order
          Hypergraph</small>,</span> <span class=
          "keyword"><small>Graph-based Learning</small>,</span>
          <span class="keyword"><small>University
          Ranking</small>,</span> <span class=
          "keyword"><small>Popularity Prediction</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Fuli Feng, Xiangnan He, Yiqun Liu, Liqiang Nie, and
          Tat-Seng Chua. 2018. Learning on Partial-Order
          Hypergraphs. In <em>WWW 2018: The 2018 Web
          Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186064" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186064</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Graphs naturally represent relational data and have been
      widely used to model the relationships between entities.
      Simple graphs intuitively connect two vertices
      (<em>i.e.,</em> entities of interest) with an edge
      (<em>i.e.,</em> the relationship to model), which can be
      either undirected or directed depending on whether the
      pairwise relationship between entities is symmetric. For
      example, given a set of entities with feature vectors, we can
      construct an undirected graph by forming the adjacency matrix
      with a similarity metric&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0048">48</a>]. The World Wide Web is a well-known
      instance of directed graphs, where vertices represent
      webpages, and edges represent hyperlinks. With such graph
      representations of entities and their relations, many
      graph-based learning methods have been developed to address
      various tasks, such as semi-supervised learning [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0037">37</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0047">47</a>], manifold
      ranking [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0020">20</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0030">30</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0049">49</a>], and
      clustering [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0040">40</a>],
      personalized recommendation [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>], and so on.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">An example of using graph methods to
          tackle the university ranking task. (a) Input data, where
          each row represents a university and its features: city
          and salary level; for salary level, smaller index
          indicates higher salary (<em>i.e., s</em> <sub>1</sub>
          &gt; <em>s</em> <sub>2</sub> &gt; <em>s</em> <sub>3</sub>
          &gt; <em>s</em> <sub>4</sub>). (b) A simple graph, where
          an edge connects a vertex and its two-nearest vertices.
          (c) A hypergraph, where a hyperedge connects vertices
          with a same attribute: either in the same city or having
          the same salary level. (d) A partial-order hypergraph,
          where the directed edges within an hyperedge represent
          the partially ordering relationship between vertices on
          the salary level.</span>
        </div>
      </figure>
      <p></p>
      <p>Hypergraph is a generalizaton of simple graph, in which an
      edge (<em>aka.</em> hyperedge) can connect any number of
      vertices rather than just two. As such, it can model
      high-order relations among multiple entities that cannot be
      naturally represented by simple graphs. Figure&nbsp;<a class=
      "fig" href="#fig1">1</a> shows an illustrative example of
      using graph methods to tackle the university ranking
      task&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>].
      Each university has two features: the located city and the
      salary level of its graduates (Figure&nbsp;<a class="fig"
      href="#fig1">1</a> a). A simple graph can be constructed by
      connecting a university with its two nearest neighbors
      (Figure&nbsp;<a class="fig" href="#fig1">1</a> b); then
      performing a manifold ranking on the simple graph can obtain
      a ranked list of universities. Further, we can build a
      hypergraph by connecting universities with a same attribute
      (Figure&nbsp;<a class="fig" href="#fig1">1</a> c),
      <em>e.g.,</em> universities that are located in the same
      city, which is a high-order relation among universities
      missed by the simple graph.</p>
      <p>In existing research, hyperedges in a hypergraph are
      typically formed by linking similar entities — either
      globally similar such as a cluster of entities that are close
      to each other&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0038">38</a>], or locally similar such as sharing
      a same attribute&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0043">43</a>]. However, we argue that many
      real-world applications need to deal with far more complex
      relations than similarities. One particular type is the
      ordering relationship among entities, which commonly exists
      in graded categorical features and numerical features. Taking
      the university ranking task in Figure&nbsp;<a class="fig"
      href="#fig1">1</a> as an example. Two universities <em>u</em>
      <sub>5</sub> and <em>u</em> <sub>6</sub> are located in the
      same city, while <em>u</em> <sub>5</sub> has a salary level
      much higher than <em>u</em> <sub>6</sub> — an evidence that
      <em>u</em> <sub>5</sub> might be ranked higher than
      <em>u</em> <sub>6</sub>. Unfortunately, the hypergraph
      constructed in Figure&nbsp;<a class="fig" href="#fig1">1</a>
      c encodes the similarity information only, thus fails to
      capture the ordering information on salary. To address this
      limitation, an intuitive solution is to incorporate the
      ordering relations by adding directed edges between entities
      of a hyperedge, as shown in Figure&nbsp;<a class="fig" href=
      "#fig1">1</a> d. It is worth noting that not every two
      entities within a hyperedge have an ordering relation; for
      example, two entities may have the same graded categorical
      feature (see <em>u</em> <sub>1</sub> and <em>u</em>
      <sub>2</sub> in Figure <a class="fig" href="#fig1">1</a> d)
      or the difference on the target numerical feature is not
      significant enough. As such, we term such generalized
      hypergraph with partial-order relations on vertices as a
      <em>Partial-Order Hypergraph</em> (POH), which is a new data
      structure that has never been explored before.</p>
      <p>In this work, we formalize the concept of POHs and further
      develop regularization-based graph learning theories on them.
      We express the partial-order relations with logical rules,
      which can be used to encode prior domain knowledge. In the
      previous example of university ranking, one example of domain
      knowledge can be that for two universities
      <em>u<sub>i</sub></em> and <em>u<sub>j</sub></em> in the same
      city, <em>u<sub>i</sub></em> tends to be ranked higher than
      <em>u<sub>j</sub></em> if the salary level of
      <em>u<sub>i</sub></em> is higher than that of
      <em>u<sub>j</sub></em> . The corresponding logical rule can
      be written as:</p>
      <div class="table-responsive" id="Xeq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} city_{=}(u_i,
          u_j) \wedge salary_{{\gt}}(u_i, u_j) \rightarrow
          score_{{\gt}}(u_i, u_j). \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>We extend conventional hypergraph
      learning&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0048">48</a>] to incorporate such logical rules
      for an effective learning on POHs. Besides the improved
      accuracy, we can further enhance the interpretability of
      hypergraph learning. Specifically, we can interpret the
      learning results by verifying the logical rules, rather than
      relying on the <em>smoothness</em> factor only. To justify
      our proposed partial-order hypergraph and the learning
      method, we employ them to address two applications:
      university ranking [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>] and popularity prediction [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>]; the two
      tasks are representatives of two machine learning tasks:
      unsupervised ranking and semi-supervised regression,
      respectively. Extensive results demonstrate the superiority
      of our proposed method, which significantly outperforms
      existing simple graph and hypergraph methods.
      <p></p>
      <p>The key contributions of the paper are summarized as
      follows.</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We propose a novel
        <em>partial-order hypergraph</em> to represent the
        partial-order relations among vertices, which are missed by
        the traditional hypergraph.<br /></li>
        <li id="list2" label="•">We generalize existing graph-based
        learning methods to partial-order hypergraphs to encode
        domain knowledge in the format of logical rules.<br /></li>
        <li id="list3" label="•">We empirically demonstrate the
        effectiveness of our POH and learning method on two machine
        learning tasks of unsupervised ranking and semi-supervised
        regression.<br /></li>
      </ul>
      <p>The remainder of this paper is organized as follows.
      Section&nbsp;<a class="sec" href="#sec-7">2</a> introduces
      some preliminary knowledge. In Section&nbsp;<a class="sec"
      href="#sec-10">3</a>, we present our proposed methods,
      followed by reviewing related work in Section&nbsp;<a class=
      "sec" href="#sec-15">4</a>. In Section&nbsp;<a class="sec"
      href="#sec-16">5</a> and&nbsp;<a class="sec" href=
      "#sec-25">6</a>, we employ POH learning to address the two
      tasks of university ranking and popularity prediction,
      respectively. We conclude the work in Section&nbsp;<a class=
      "sec" href="#sec-33">7</a>.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span>
          Preliminaries</h2>
        </div>
      </header>
      <p>We introduce some notations first. We use bold capital
      letters (<em>e.g.,</em> <strong>X</strong>) and bold
      lowercase letters (<em>e.g.,</em> <strong>x</strong>) to
      denote matrices and vectors, respectively. Scalars and
      hyperparameters are respectively represented as normal
      lowercase letters (e.g., <em>x</em>) and Greek letters (e.g.,
      <em>λ</em>). If not otherwise specified, all vectors are in a
      column form, and <em>X<sub>ij</sub></em> denotes the entry at
      the <em>i</em>-th row and the <em>j</em>-th column of
      <strong>X</strong>.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Hypergraph</h3>
          </div>
        </header>
        <p>As aforementioned, the vertices and hyperedges of a
        hypergraph represent the entities of interest and their
        relations, respectively. Given <em>N</em> entities with
        their features <span class="inline-equation"><span class=
        "tex">$\mathbf {X}=[\mathbf {x_1},\mathbf {x_2},\cdots
        ,\mathbf {x_N}]^{T}\in \mathbb {R}^{N \times
        D}$</span></span> , we can construct a hypergraph with
        <em>N</em> vertices and <em>M</em> hyperedges, for which
        the structure can be represented as an incidence matrix
        <span class="inline-equation"><span class="tex">$\mathbf
        {H} \in \mathbb {R}^{N \times M}$</span></span> . Similar
        to the incidence matrix of a simple graph,
        <strong>H</strong> is a binary matrix, where
        <em>H<sub>ij</sub></em> = 1 if the <em>i</em>-th vertex is
        connected by the <em>j</em>-th hyperedge, otherwise
        <em>H<sub>ij</sub></em> = 0. There are two ways to define a
        hyperedge in existing work: attribute-based&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0035">35</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0043">43</a>] and
        neighbor-based&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0027">27</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0038">38</a>]. An attribute-based hyperedge
        connects vertices with same value on one or multiple target
        attributes (<em>i.e.,</em> features). A neighbor-based
        hyperedge connects vertices nearby, based on these vertices
        with similarity larger than a threshold or simply using the
        <em>k</em>-nearest neighbors.</p>
        <p>Moreover, we use the diagonal matrix <span class=
        "inline-equation"><span class="tex">$\mathbf {E} \in
        \mathbb {R}^{M \times M}$</span></span> to denote the
        degrees of hyperedges, <em>i.e.,</em> <span class=
        "inline-equation"><span class="tex">$E_{jj} = \sum _{i =
        1}^{N} H_{ij}$</span></span> denotes the number of vertices
        connected by the <em>j</em>-th hyperedge. Analogous to
        simple graph that an edge typically has a weight to model
        the strength of the relation, a hyperedge in hypergraphs
        also has a weight to denote the density of the vertices it
        connected. Such weights are represented as a diagonal
        matrix <span class="inline-equation"><span class=
        "tex">$\mathbf {W} \in \mathbb {R}^{M \times
        M}$</span></span> . To estimate the hyperedge weight, many
        methods have been proposed, among which the most popular
        one is to use the average pairwise similarity between
        vertices connected by the hyperedge:</p>
        <div class="table-responsive" id="Xeq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} W_{jj} =
            \frac{1}{E_{jj}} \sum _{H_{ij} = 1} g(\mathbf {x_i},
            \mathbf {x_j}), \end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where <em>g</em> denotes the similarity function on
        feature vectors. In graph-based methods, one common choice
        for <em>g</em> is the radial basis function (RBF) kernel,
        <em>i.e.,</em> <span class="inline-equation"><span class=
        "tex">$g(\mathbf {x_i}, \mathbf {x_j}) = \exp (\frac{-
        \Vert \mathbf {x_i} - \mathbf {x_j} \Vert ^2}{2 \sigma
        ^2})$</span></span> . Given the weights for hyperedges, we
        can further derive the degree of a vertex <em>i</em>:
        <span class="inline-equation"><span class="tex">$V_{ii} =
        \sum _{j = 1}^{M} W_{jj} H_{ij}$</span></span> ,
        <em>i.e.,</em> the sum of weights of hyperedges that are
        connected with <em>i</em>. We use the diagonal matrix
        <span class="inline-equation"><span class="tex">$\mathbf
        {V}\in \mathbb {R}^{N \times N}$</span></span> to denote
        the vertex degree matrix.
        <p></p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Learning on
            Hypergraphs</h3>
          </div>
        </header>
        <p>Graph-based learning has been applied to various machine
        learning tasks such as manifold ranking, semi-supervised
        learning, and clustering&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0044">44</a>]. The general problem setting is to
        learn a prediction function <span class=
        "inline-equation"><span class="tex">$\mathbf {\hat{y}} =
        f(\mathbf {x})$</span></span> , which maps an entity from
        the feature space to the target label space. It is usually
        achieved by minimizing an objective function abstracted
        as:</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            \end{align}</span><br />
            <span class="equation-number">(mid1)</span>
          </div>
        </div>
        <p></p>
        <p>= G + L, \eqno{3} where <span class=
        "inline-equation"><span class="tex">$\mathcal
        {L}$</span></span> is a task-specific loss function that
        measures the error between prediction <span class=
        "inline-equation"><span class="tex">$\mathbf
        {\hat{y}}$</span></span> and ground-truth
        <strong>y</strong>, <span class=
        "inline-equation"><span class="tex">$\mathcal
        {G}$</span></span> is a graph regularization term that
        smooths the prediction over the graph, and <em>λ</em> is a
        hyperparameter to balance the two terms. The regularization
        term typically implements the <em>smoothness</em>
        assumption that similar vertices tend to have similar
        predictions. For hypergraphs, a widely used <span class=
        "inline-equation"><span class="tex">$\mathcal
        {G}$</span></span> is the hypergraph Laplacian term,
        defined as:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathcal {G} =
            \sum _{i=1}^{N}\sum _{j=1}^N \underbrace{g(\mathbf
            {x_i}, \mathbf {x_j}) \sum \limits _{k=1}^{M} H_{ik}
            W_{kk} H_{jk}}_{\text{strength of smoothness}}
            \underbrace{\left\Vert \frac{f(\mathbf {x_i})}{\sqrt
            {V_{ii}}}-\frac{f(\mathbf {x_j})}{\sqrt
            {V_{jj}}}\right\Vert ^2}_{\text{smoothness}}.
            \end{align}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>The regularization term operates smoothness on each
        pair of entities, enforcing their predictions (after
        normalized by their degrees) to be close to each other. The
        strength of smoothness is determined by the similarity over
        their feature vectors
        <em>g</em>(<strong>x<sub>i</sub></strong> ,
        <strong>x<sub>j</sub></strong> ) and the hyperedges that
        connect the two vertices. It can be equivalently written in
        a more concise matrix form:
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathcal {G} =
            trace(\mathbf {\hat{Y}} (\mathbf {S} \odot \mathbf {L})
            \mathbf {\hat{Y}}^{T}), \end{align}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\mathbf {\hat{Y}} = [\mathbf {\hat{y_1}}, \mathbf
        {\hat{y_2}}, \cdots , \mathbf {\hat{y_N}}]$</span></span> ,
        each element of <strong>S</strong> is
        <em>S<sub>ij</sub></em> =
        <em>g</em>(<strong>x<sub>i</sub></strong> ,
        <strong>x<sub>j</sub></strong> ), and <strong>L</strong> is
        defined as <strong>L</strong> = <strong>V</strong> <sup>−
        1/2</sup>(<strong>V</strong> − <strong>HWH</strong>
        <sup><em>T</em></sup> )<strong>V</strong> <sup>− 1/2</sup>,
        known as the <em>hypergraph Laplacian matrix</em>. Note
        that <strong>L</strong> is typically a sparse matrix, where
        an entry <em>L<sub>ij</sub></em> is nonzero only if vertex
        <em>i</em> and <em>j</em> are connected by at least one
        hyperedge. Thus, the time complexity of calculating
        <span class="inline-equation"><span class="tex">$\mathcal
        {G}$</span></span> is linear <em>w.r.t.</em> the number of
        nonzero entries in <strong>L</strong>, which is far smaller
        than <em>N</em> <sup>2</sup>.
        <p></p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Partial-Order
          Hypergraph</h2>
        </div>
      </header>
      <p>Distinct from the typical problem setting of hypergraph
      learning, we further associate the problem with a set of
      logic rules, which can be used to encode the partial-order
      relations between entities:</p>
      <div class="table-responsive" id="eq4">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \left\lbrace p^{r}
          (\mathbf {x_i}, \mathbf {x_j}) \rightarrow q^{r}
          (f(\mathbf {x_i}), f(\mathbf {x_j}))\ |\ r = 1, 2, \cdots
          , R \right\rbrace , \end{align}</span><br />
          <span class="equation-number">(6)</span>
        </div>
      </div>where <em>r</em> denotes a partial-order relation, and
      there can be multiple relations (in total <em>R</em>) for a
      problem. For example, in the university ranking task, we can
      have a partial-order relation based on salary level, number
      of students, research grants among other features. For each
      partial-order relation <em>r</em>, <em>q<sup>r</sup></em> is
      a binary function indicating whether the prediction of two
      entities satisfies a certain relation. For example, in a
      ranking/regression task, <em>q<sup>r</sup></em> can indicate
      whether <em>f</em>(<strong>x</strong> <sub><em>i</em></sub> )
      is higher than <em>f</em>(<strong>x</strong>
      <sub><em>j</em></sub> ); in a classification task,
      <em>q<sup>r</sup></em> can indicate whether the probability
      of <strong>x<sub>i</sub></strong> being a class is higher
      than that of <strong>x<sub>j</sub></strong> . The
      <em>p<sup>r</sup></em> (<strong>x<sub>i</sub></strong> ,
      <strong>x<sub>j</sub></strong> ) denotes whether
      <strong>x</strong> <sub><em>i</em></sub> and
      <strong>x</strong> <sub><em>j</em></sub> have the
      partial-order relation <em>r</em>. A partial-order relation
      is a pairwise relation satisfying the following basic
      properties on the entities connected by any hyperedge:
      <p></p>
      <p>• Irreflexivity: not <em>p<sup>r</sup></em>
      (<strong>x<sub>i</sub></strong> ,
      <strong>x<sub>i</sub></strong> ). • Asymmetry: if
      <em>p<sup>r</sup></em> (<strong>x<sub>i</sub></strong> ,
      <strong>x<sub>j</sub></strong> ) then not
      <em>p<sup>r</sup></em> (<strong>x<sub>j</sub></strong> ,
      <strong>x<sub>i</sub></strong> ). • Transitivity:
      <em>p<sup>r</sup></em> (<strong>x<sub>i</sub></strong> ,
      <strong>x<sub>j</sub></strong> ) and <em>p<sup>r</sup></em>
      (<strong>x<sub>j</sub></strong> ,
      <strong>x<sub>k</sub></strong> ) implies
      <em>p<sup>r</sup></em> (<strong>x<sub>i</sub></strong> ,
      <strong>x<sub>k</sub></strong> ).</p>
      <p>In what follows, we first present how to construct and
      represent a POH. We then elaborate our proposed regularized
      learning on POHs. Lastly, we analyze its time complexity.</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span>
            Construction and Representation</h3>
          </div>
        </header>
        <p>To jointly represent the partial-order relations and the
        higher-order relations among entities, we first construct a
        normal hypergraph, and then use directed edges to connect
        vertices that have any partial-order relation. Note that it
        is possible that there are multiple edges between two
        vertices, since multiple partial-order relations are
        considered. Concerning the efficiency of downtream
        graph-based learning applications, we constrain that
        directed edges only exist on vertices connected by at least
        one hyperedge. Such a constraint guarantees that the number
        of directed edges constructed from a partial-order relation
        is no larger than the number of nonzero entries in the
        hypergraph Laplacian matrix. As such, a learning algorithm
        that enumerates all directed edges will not increase the
        time complexity of calculating the hypergraph Laplacian
        term.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">A toy example to illustrate
            the construction of matrix representation of a POH.
            Given the feature matrix <strong>X</strong> and a
            partial-order relation <em>r</em>, we construct the
            incidence matrix <strong>H</strong> of the hypergraph
            and the binary relation matrix <strong>P</strong>
            <sup><em>r</em></sup> , respectively. We then generate
            the co-occurrence matrix <strong>C</strong> from
            <strong>H</strong> and apply element-wise product to
            <strong>C</strong> and <strong>P</strong>
            <sup><em>r</em></sup> to get the partial incidence
            matrix <strong>H</strong> <sup><em>r</em></sup>
            .</span>
          </div>
        </figure>
        <p></p>
        <p>As described in Section <a class="sec" href=
        "#sec-8">2.1</a>, after constructing a hypergraph, we use
        several matrices to represent it, such as the incidence
        matrix <strong>H</strong> and hypergraph Laplacian matrix
        <strong>L</strong>. In addition to these matrices, we
        further introduce a partial incidence matrix <span class=
        "inline-equation"><span class="tex">$\mathbf {H^{r}} \in
        \mathbb {R}^{N \times N}$</span></span> to represent the
        directed edges of a partial-order relation <em>r</em>. As
        shown in Figure <a class="fig" href="#fig2">2</a>, given a
        partial-order relation <em>r</em>, we first construct a
        binary relation matrix <span class=
        "inline-equation"><span class="tex">$\mathbf {P^{r}} \in
        \mathbb {R}^{N \times N}$</span></span> , where
        <span class="inline-equation"><span class="tex">$P^{r}_{ij}
        = 1$</span></span> if <em>p<sup>r</sup></em>
        (<strong>x<sub>i</sub></strong> ,
        <strong>x<sub>j</sub></strong> ) is true. Based on the
        incidence matrix <strong>H</strong> of the hypergraph, we
        further build a co-occurrence matrix <span class=
        "inline-equation"><span class="tex">$\mathbf {C} \in
        \mathbb {R}^{N \times N}$</span></span> , where each
        element <em>C<sub>ij</sub></em> denotes the number of
        hyperedges connecting vertex <em>i</em> and <em>j</em>.
        Then the partial incidence matrix <strong>H</strong>
        <sup><em>r</em></sup> can be derived,</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathbf {H^{r}} =
            \mathbf {P^{r}} \odot \mathbf {C},
            \end{align}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>where ⊙ is the element-wise matrix multiplication. In
        the partial incidence matrix, a non-zero entry <span class=
        "inline-equation"><span class=
        "tex">$H^{r}_{ij}$</span></span> means that the
        <em>i</em>-th and <em>j</em>-th vertices have the
        <em>r</em>-th partial-order relation, and they are
        simultaneously connected by <span class=
        "inline-equation"><span class=
        "tex">$H^{r}_{ij}$</span></span> hyperedges. In other
        words, we assign higher weights to vertex pairs that are
        connected by more hyperedges, accounting for the effect
        that vertex pairs with higher co-occurrence are more
        important.
        <p></p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Learning on
            Partial-Order Hypergraphs</h3>
          </div>
        </header>
        <p>After constructing a POH, we have several matrices to
        represent it, including the general ones describing a
        conventional hypergraph (<em>e.g.,</em> the incidence
        matrix <strong>H</strong> and edge weight matrix
        <strong>W</strong>), and the specific partial incidence
        matrices {<strong>H<sup>r</sup></strong> |<em>r</em> = 1,
        2, ⋅⋅⋅, <em>R</em>} to model partial-order relations. We
        now consider how to extend the conventional hypergraph
        learning methods for POHs.</p>
        <p>The key problem is the encoding of the partial-order
        relations and the corresponding logical rules into the
        learning framework of Equation&nbsp;(<a class="sec" href=
        "#sec-9">3</a>). Generally speaking, there are two
        solutions — either injecting the rules into the predictive
        model (<em>i.e.,</em> the definition of
        <em>f</em>(<strong>x</strong>)), or using the rules to
        regularize the learning (<em>i.e.,</em> augmenting the
        objective function <em>Γ</em>). The first solution may need
        different ways to encode the rules for different predictive
        models, such as logistic regression, factorization
        machines&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>], and neural
        networks&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>]. As such, we opt for the second
        solution, aiming to provide a generic solution for POH
        learning. Specifically, we append an additional
        regularization term <span class=
        "inline-equation"><span class="tex">$\mathcal
        {P}$</span></span> that encodes partial-order rules to the
        objective function:</p>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \Gamma = \mathcal
            {G} + \lambda \mathcal {L} + \beta \mathcal {P},
            \end{align}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>where <em>β</em> is a hyperparameter to balance
        <span class="inline-equation"><span class="tex">$\mathcal
        {P}$</span></span> and the other two terms. Similar to the
        smoothness regularizer <span class=
        "inline-equation"><span class="tex">$\mathcal
        {G}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\mathcal
        {P}$</span></span> should also operate on the predicted
        label space to regularize the learning process. We define
        <span class="inline-equation"><span class="tex">$\mathcal
        {P}$</span></span> as:
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \mathcal {P}_0
            &amp;= \sum \limits _{r=1}^{R} a_{r} \mathbb {E}_{(i,
            j) \sim H^{r}_{ij} \ne 0} \left[ 1 - q^{r}(\mathbf
            {\hat{y_i}}, \mathbf {\hat{y_j}}) \right], \nonumber
            \\\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>= r=1R ar| Hr | {i,j | Hrij 0 } 1 - qr(yi, yj),</p>
        <p>\eqno{9} where <span class=
        "inline-equation"><span class="tex">$\mathbf {\hat{y_i}} =
        f(\mathbf {x_i})$</span></span> is the prediction of
        <strong>x<sub>i</sub></strong> ,
        |<strong>H<sup>r</sup></strong> | denotes the number of
        nonzero entries in <strong>H<sup>r</sup></strong> , and
        <em>a<sub>r</sub></em> is the hyperparameter to control the
        importance of the logical rule of the <em>r</em>-th
        partial-order relation. The core idea of this
        regularization term is to enforce the predictions of
        vertices that have a partial-order relation to be
        consistent with the corresponding rule. To be more
        specific, small values of <span class=
        "inline-equation"><span class="tex">$\mathcal
        {P}_0$</span></span> can be achieved if <span class=
        "inline-equation"><span class="tex">$q^{r}(\mathbf
        {\hat{y_i}}, \mathbf {\hat{y_j}})$</span></span> is 1,
        meaning that <em>p<sup>r</sup></em>
        (<strong>x<sub>i</sub></strong> ,
        <strong>x<sub>j</sub></strong> ) is true (evidenced by
        <span class="inline-equation"><span class=
        "tex">$H^r_{ij}\ne 0$</span></span> ) and the rule
        <em>p<sup>r</sup></em> (<strong>x<sub>i</sub></strong> ,
        <strong>x<sub>j</sub></strong> ) → <em>q<sup>r</sup></em>
        (<em>f</em>(<strong>x<sub>i</sub></strong> ),
        <em>f</em>(<strong>x<sub>j</sub></strong> )) is satisfied.
        However, this definition treats all vertex pairs of a
        partial-order relation equally, without considering their
        relative strengths. This assumption may decrease modelling
        fidelity for practical applications. To address this
        problem, we revise the regularizer as:</p>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathcal {P}_1 =
            \sum \limits _{r=1}^{R} \frac{a_{r}}{\left| \mathbf
            {H^{r}} \right|} \sum _{\lbrace i,j | H^{r}_{ij} \ne 0
            \rbrace } (1 - q^{r}(\mathbf {\hat{y_i}}, \mathbf
            {\hat{y_j}})) H^{r}_{ij},\end{align}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>\eqno{10}which incorporates <span class=
        "inline-equation"><span class=
        "tex">$H^{r}_{ij}$</span></span> as a coefficient to
        rescale the regularization strength of a vertex pair. With
        such a formulation, we enforce stronger partial-order
        regularization for vertex pairs that are connected by more
        hyperedges. Lastly, to get rid of the difficulties in
        discrete optimization, we replace the binary logic function
        <em>q<sup>r</sup></em> with a continuous function
        <em>s<sup>r</sup></em> that approximates it. Such
        approximation trick allows stable optimization and has been
        widely used in probabilistic soft logics&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0002">2</a>]. This
        leads to the final version of our proposed partial-order
        regularizer:
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathcal {P} =
            \sum \limits _{r=1}^{R} \frac{a_{r}}{\left| \mathbf
            {H^{r}} \right|} \sum _{\lbrace i,j | H^{r}_{ij} \ne 0
            \rbrace } s^{r}(\mathbf {\hat{y_i}}, \mathbf
            {\hat{y_j}}) H^{r}_{ij}, \end{align}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>where <em>s<sup>r</sup></em> is a self-defined
        function adjustable for different problems. For instance,
        <em>s<sup>r</sup></em> might be the subtraction between the
        predicted ranks <span class="inline-equation"><span class=
        "tex">$\mathbf {\hat{y_i}} - \mathbf
        {\hat{y_j}}$</span></span> in a ranking problem, while in a
        classification problem, <em>s<sup>r</sup></em> could be the
        gap between the predicted probabilities on a specific
        class.
        <p></p>
        <section id="sec-13">
          <p><em>3.2.1 Optimization.</em> By minimizing the
          objective function of Equation&nbsp;(<a class="eqn" href=
          "#eq6">8</a>), we can achieve the prediction function
          that is smooth over the hypergraph and satisfies the
          logical rules on partial-order relations. Note that the
          regularization terms <span class=
          "inline-equation"><span class="tex">$\mathcal
          {L}$</span></span> and <span class=
          "inline-equation"><span class="tex">$\mathcal
          {P}$</span></span> operate on the predicted label space
          only and do not introduce extra model parameters. As
          such, the only model parameters to learn come from the
          predictive model <em>f</em>(<strong>x</strong>). Given
          that <em>f</em> is a differentiable
          function&nbsp;(<em>e.g.,</em> logistic regression and
          neural networks), we can optimize the objective function
          with standard gradient-based methods, such as the
          stochastic (or batch) gradient descent&nbsp;[<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0012">12</a>].
          Moreover, one can also directly learn
          <em>f</em>(<strong>x</strong>) without specifying an
          explicit form of the predictive model. This will make the
          prediction function comply with the regularization to the
          maximum degree. We will empirically study how would this
          affect the prediction performance for downstream
          applications in Section&nbsp;<a class="sec" href=
          "#sec-25">6</a>.</p>
        </section>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Time
            Complexity Analysis</h3>
          </div>
        </header>
        <p>In this subsection, we analyze the time complexity of
        POH learning by comparing with conventional hypergraphs. As
        discussed in [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>] and Section&nbsp;<a class="sec"
        href="#sec-9">2.2</a>, the computational complexity of
        conventional hypergraph learning methods are
        <em>O</em>(<em>J</em>), where <em>J</em> denotes the number
        of nonzero entries in the sparse hypergraph Laplacian
        matrix <strong>L</strong>. In contrast, the additional
        computational cost of our POH learning comes from the
        construction of the partial incidence matrices
        {<strong>H<sup>r</sup></strong> |<em>r</em> = 1, 2, ⋅⋅⋅,
        <em>R</em>} and the partial-order regularization term
        <span class="inline-equation"><span class="tex">$\mathcal
        {P}$</span></span> . To compute
        <strong>H<sup>r</sup></strong> , we need to obtain the
        co-occurrence matrix <strong>C</strong> first, and then
        evaluate the element-wise product
        <strong>C</strong>⊙<strong>P<sup>r</sup></strong> . For
        <strong>C</strong>, we can achieve it by traversing all
        nonzero entries on the incidence matrix <strong>H</strong>,
        for which the complexity is <em>O</em>(<em>J</em>). Then
        for each nonzero element <em>C<sub>ij</sub></em> in
        <strong>C</strong>, we check whether <strong>p</strong>
        <sup><em>r</em></sup> (<strong>x</strong>
        <sub><em>i</em></sub> , <strong>x</strong>
        <sub><em>j</em></sub> ) is true or not to obtain
        <strong>C</strong>⊙<strong>P<sup>r</sup></strong> . As
        such, the complexity of constructing a
        <strong>H<sup>r</sup></strong> is <em>O</em>(<em>J</em>),
        and the overall complexity of constructing all <em>R</em>
        partial incidence matrices is <em>O</em>(<em>RJ</em>).
        Similarly, the computation of <span class=
        "inline-equation"><span class="tex">$\mathcal
        {P}$</span></span> can be done in <em>O</em>(<em>RJ</em>)
        time. In a real-world application, the number of
        partial-order relations <em>R</em> is typically a small
        number, since we need to account for the most prominent
        numerical or graded categorical features only. As such, the
        overall time complexity of our POH learning is essentially
        <em>O</em>(<em>J</em>), which is the same as that of
        conventional hypergraph learning.</p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Related
          Work</h2>
        </div>
      </header>
      <p>Our work is directly related to the recent work on
      hypergraphs, which can be seperated into two main categories:
      hypergraph construction and hypergraph utilization. Since
      proposed in [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0048">48</a>],
      researchers have paid lots of attention on how to construct
      hypergraphs. For instances, Wang et al. leveraged a sparse
      representation of the entity feature space to generate
      hyperedges and learn the relationship among hyperedges and
      the connected vertices[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0038">38</a>]. Instead of simply learning a sparse
      representation, Liu et al. employed an elastic net to
      regulate the representation learing [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>]. Besides, Feng et al.
      jointly learned hypergraph representations of multiple
      hypergraphs by further encouraging the consitancy among
      different hypergraph representations [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0010">10</a>]. However, none of the
      aforementioned work is able to incorporate the partial-order
      relations among entities that exist in graded categorical
      features and numerical features during the construction of
      hypergraphs. They thus lead to severe information loss and
      limit the expressiveness of the constructed hypergraphs.</p>
      <p>Besides, hypergraph and hypergraph-based learning have
      been widely applied on many machine learning tasks, including
      clustering, embedding, ranking, semi-supervised
      classification and regression [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0043">43</a>]. For instance, the authors of
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>]
      constructed a hypergraph to represent the correlations among
      news readers, news articles, topics and name entities and
      then ranked the news articles on the hypergraph to make
      recommendation for readers. In [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>], the authors utilized a hypergraph to
      represent the relations among candidate sentences and made a
      graph-based extractive document summarization. Yoshida and
      Yuichi used a hypergraph to estimate the betweenness
      centrality and importance of vertices [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0042">42</a>]. Huang et al. constrcuted
      a hypergraph of images and predicted the attributes of the
      images with hypergraph-based label propagation [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>]. Tran
      <em>et al.</em>built a hypergraph where vertices and
      hyperedges respectively represent features and training
      samples to represent the sparse pattern of the training data
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0035">35</a>]. Hmimida
      and Kanawati depicted the relation among social users,
      resources, and the tags of resources assigned by the users.
      They then employed hypergraph-based ranking to recommend
      candidate tags for resources [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>]. These articles indicated the
      popularity and usability of hypergraphs and learning on
      hypergraphs. However they only utilized conventional
      hypergraphs instead of simultaneously improving the
      representation ability of the conventional hypergraphs and
      the corresponding learning methods.</p>
    </section>
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> University
          Ranking</h2>
        </div>
      </header>
      <p>Following the previous work [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>], we formulated university ranking as
      an unsupervised ranking (<em>i.e.,</em> re-ranking) problem.
      Given <em>N</em> universities with a feature matrix
      <span class="inline-equation"><span class="tex">$\mathbf {X}
      \in \mathbb {R}^{N \times M}$</span></span> and a historical
      ranking result <span class="inline-equation"><span class=
      "tex">$\mathbf {y} \in \mathbb {R}^{N}$</span></span> , the
      target is to learn a new ranking <span class=
      "inline-equation"><span class="tex">$\mathbf {f} \in \mathbb
      {R}^N$</span></span> . To solve the problem, we manually
      selected several partial-order relations and constructed a
      partial-order hypergraph (POH) to represent the given
      universities. Upon the constructed POH, we learned
      <strong>f</strong> by minimizing a ranking instantiation of
      the POH learning objective function. Specifically, we set the
      loss term in Equation&nbsp;(<a class="eqn" href="#eq6">8</a>)
      as <span class="inline-equation"><span class="tex">$\mathcal
      {L} = \Vert \mathbf {y} - \mathbf {f}\Vert
      ^2_F$</span></span> , which encourages the learned ranking to
      be consistent and smooth with the historical one. Besides, we
      set the soft logic functions {<em>s<sup>r</sup></em>
      |<em>r</em> = 1, 2, ⋅⋅⋅, <em>R</em>} as
      <em>s<sup>r</sup></em> (<em>f<sub>i</sub></em> ,
      <em>f<sub>j</sub></em> ) = <em>f<sub>i</sub></em> −
      <em>f<sub>j</sub></em> , <em>i.e.,</em> university <em>i</em>
      is encouraged to be ranked ahead of university <em>j</em> if
      the two universities have the selected partial-order
      relations. By specifying the above application-specific
      terms, we derived the objective function for the task,</p>
      <div class="table-responsive" id="eq9">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \Gamma = \mathbf
          {f}^T \mathbf {L} \mathbf {f} + \lambda \Vert \mathbf {f}
          - \mathbf {y} \Vert ^2_F + \beta \sum \limits _{r=1}^{R}
          \frac{a_{r}}{\left| \mathbf {H^{r}} \right|} \sum
          _{\lbrace i,j | H^{r}_{ij} \ne 0 \rbrace } ReLU((f_i -
          f_j)H^{r}_{ij}), \end{equation}</span><br />
          <span class="equation-number">(12)</span>
        </div>
      </div>where we further used the rectifier function (ReLU)
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0013">13</a>] on the
      partial-order regularization term, so as to guarantee the
      objective function to be non-negative for stable
      optimization.
      <p></p>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Experiment
            Settings</h3>
          </div>
        </header>
        <section id="sec-18">
          <p><em>5.1.1 Dataset.</em> To investigate the
          effectiveness of the proposed method, we employed the
          dataset of Chinese university ranking collected by
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0009">9</a>]. This dataset contains 743
          Chinese universities with data collected between January
          1st, 2015 and May 1st, 2016. For each university, Web
          data from five channels were collected, including
          official, mass media, academic, employment, and general
          user channels. The official channel contains the primary
          information of a university, such as student quality,
          official activities, and development plans. In the mass
          media channel, they collected news reports mentioning the
          given university from mass media. The academic and
          employment channels contain university's academic status
          and graduate students employment statistics,
          respectively. The general user channel contains public
          impressions, attitudes, and sentiment polarities of
          universities shared in social media posts. To describe
          the universities, we also employed the rich set of
          features extracted by the authors of [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0009">9</a>]. Among
          the 743 universities, we selected 438 top-tier ones in
          China<a class="fn" href="#fn2" id=
          "foot-fn2"><sup>1</sup></a> since they have more academic
          and research activities. Towards the historical ranking
          result <strong>y</strong>, we merged the ranking results
          in 2015 from four well-known ranking systems of Chinese
          universities, namely, CUAA, WSL, WH, and iPIN<a class=
          "fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>. The
          ranking ground-truth is generated in the same way, but
          based on the rankings of the four systems in the year
          2016. As such, the task can be understood as using the
          past year's ranking and this year's features to predict
          the ranking of universities in this year. We normalized
          the constructed historical ranking result and the
          ground-truth into range [0, 1] by scaling them with
          1/<em>N</em>.</p>
        </section>
        <section id="sec-19">
          <p><em>5.1.2 Evaluation.</em> We performed 5-fold
          cross-validation, employing three metrics to evaluate the
          ranking performance: mean absolute error (MAE) [<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0041">41</a>],
          Kendall's tau (Tau) [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0028">28</a>], and Spearman's rank<a class=
          "fn" href="#fn4" id="foot-fn4"><sup>3</sup></a> (Rho)
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0034">34</a>]. The three metrics have been
          widely used to evaluate pointwise, pairwise, and listwise
          ranking methods&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0025">25</a>]. Note that better performance is
          evidenced by smaller MAE, larger Tau and Rho scores.
          Moreover, we carried out the student's t-test and
          reported the p-values where necessary.</p>
        </section>
        <section id="sec-20">
          <p><em>5.1.3 Methods.</em> We compared with following
          baselines<a class="fn" href="#fn5" id=
          "foot-fn5"><sup>4</sup></a>:</p>
          <ul class="list-no-style">
            <li id="list4" label="•">
              <strong>Simple Graph</strong> [<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0049">49</a>]:
              It first constructs a simple graph to represent the
              universities, where the edge weight between two
              vertices is evaluated using the RBF kernel.<br />
              We set the radius parameter <em>σ</em> as the median
              of the Euclidean distances of all pairs. The method
              then calculates the Laplacian matrix
              <strong>L</strong>, learning <strong>f</strong> by
              minimizing the objective function <strong>f</strong>
              <strong>L</strong> <sup><em>T</em></sup>
              <strong>f</strong> + <em>λ</em>‖<strong>y</strong> −
              <strong>f</strong>‖<sup>2</sup>. We experimented with
              different values of <em>λ</em> and reported the best
              performance.<br />
            </li>
            <li id="list5" label="•">
              <strong>Hypergraph</strong> [<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0003">3</a>]: It
              first calculates the similarities between
              universities, and then constructs the hypergraph
              using neighbor-based methods. Specifically, the
              <em>i</em>-th hyperedge connects the <em>k</em>
              universities that are most similar to university
              <em>i</em>. The learning of <strong>f</strong> is
              performed by minimizing Equation&nbsp;(<a class="sec"
              href="#sec-9">3</a>). We tuned the two
              hyperparameters <em>k</em> and <em>λ</em>.<br />
            </li>
            <li id="list6" label="•">
              <strong>GMR</strong> [<a class="bib" data-trigger=
              "hover" data-toggle="popover" data-placement="top"
              href="#BibPLXBIB0009">9</a>]: This is the
              state-of-the-art method for the university ranking
              task.<br />
              It builds a simple graph from the features of each
              channel, modelling the relations between channels to
              reach a consensus ranking on all simple graphs. We
              used the same hyperparameter settings as reported in
              their paper.<br />
            </li>
          </ul>
          <p>We evaluate several POH methods that incorporate
          different partial-order relations on the same hypergraph
          structure of <strong>Hypergraph</strong>:</p>
          <ul class="list-no-style">
            <li id="list7" label="•"><strong>POH-Salary</strong>:
            This method considers the partial-order relation on the
            salary feature. We encoded the logical rule
            <em>salary</em>
            <sub>&gt;</sub>(<strong>x<sub>i</sub></strong> ,
            <strong>x<sub>j</sub></strong> ) → <em>rank</em>
            <sub>&lt;</sub>(<strong>x<sub>i</sub></strong> ,
            <strong>x<sub>j</sub></strong> ), meaning that
            <strong>x<sub>i</sub></strong> tends to be ranked
            higher than <strong>x<sub>j</sub></strong> if the
            salary feature of <strong>x<sub>i</sub></strong> is
            higher than that of <strong>x<sub>j</sub></strong>
            .<br /></li>
            <li id="list8" label="•"><strong>POH-NCEE</strong>:
            This method considers the partial-order relation on the
            NCEE feature, which stands for a university's admission
            requirement on the score of National College Entrance
            Examination. The logical rule to be encoded is
            naturally <em>NCEE</em>
            <sub>&gt;</sub>(<strong>x<sub>i</sub></strong> ,
            <strong>x<sub>j</sub></strong> ) → <em>rank</em>
            <sub>&lt;</sub>(<strong>x<sub>i</sub></strong> ,
            <strong>x<sub>j</sub></strong> ), meaning universities
            with a higher NCEE score tend to have a better
            quality.<br /></li>
            <li id="list9" label="•"><strong>POH-All</strong>: In
            this method, we model both partial-order relations as
            encoded in <strong>POH-Salary</strong> and
            <strong>POH-NCEE</strong>. We set the importance
            hyperparameters for the regularizers of the two rules
            as <em>a</em> <sub>1</sub> and 1 − <em>a</em>
            <sub>1</sub>, respectively.<br /></li>
          </ul>
        </section>
        <section id="sec-21">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.1.4</span>
              Hyperparameter Tuning</h4>
            </div>
          </header>
          <div class="table-responsive" id="tab1">
            <div class="table-caption">
              <span class="table-number">Table 1:</span>
              <span class="table-title">Performance comparison on
              university ranking.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <td style="text-align:center;">
                  <strong>Methods</strong></td>
                  <td style="text-align:center;">
                  <strong>MAE</strong></td>
                  <td style="text-align:center;">
                  <strong>Tau</strong></td>
                  <td style="text-align:center;">
                  <strong>Rho</strong></td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;"><strong>Simple
                  Graph</strong></td>
                  <td style="text-align:center;">0.074 ± 9e-3</td>
                  <td style="text-align:center;">0.870 ± 2e-2</td>
                  <td style="text-align:center;">0.970 ± 8e-3</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>Hypergraph</strong></td>
                  <td style="text-align:center;">0.067 ± 7e-3</td>
                  <td style="text-align:center;">0.876 ± 9e-3</td>
                  <td style="text-align:center;">0.974 ± 5e-3</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>GMR</strong></td>
                  <td style="text-align:center;">0.065 ± 7e-3</td>
                  <td style="text-align:center;">0.871 ± 3e-2</td>
                  <td style="text-align:center;">0.970 ± 1e-2</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH-Salary</strong></td>
                  <td style="text-align:center;">0.054 ±
                  1e-2<sup>*</sup></td>
                  <td style="text-align:center;">0.892 ±
                  1e-2<sup>*</sup></td>
                  <td style="text-align:center;">0.979 ±
                  5e-3<sup>*</sup></td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH-NCEE</strong></td>
                  <td style="text-align:center;">0.055 ±
                  1e-2<sup>*</sup></td>
                  <td style="text-align:center;">0.893 ±
                  9e-3<sup>*</sup></td>
                  <td style="text-align:center;">0.978 ±
                  5e-3<sup>*</sup></td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH-All</strong></td>
                  <td style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$\mathbf
                  {0.053\pm \text{{1e-2}}}^{*}$</span></span></td>
                  <td style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$\mathbf
                  {0.898\pm \text{{1e-2}}}^{**}$</span></span></td>
                  <td style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$\mathbf
                  {0.980\pm \text{{6e-3}}}^{**}$</span></span></td>
                </tr>
              </tbody>
              <tfoot>
                <tr>
                  <td colspan="4">* and ** denote that the
                  corresponding performance is significantly better
                  (<em>p</em>-value &lt; 0.05) than all baselines
                  and all other methods, respectively.</td>
                </tr>
              </tfoot>
            </table>
          </div>
          <p>We employed grid search to select the optimal
          hyperparameters for POH methods based on the results of
          Tau. The optimal hyperparameter setting and
          implementation of the compared methods can be publicly
          accessed<a class="fn" href="#fn6" id=
          "foot-fn6"><sup>5</sup></a>. For
          <strong>POH-Salary</strong> and
          <strong>POH-NCEE</strong>, we tuned one implicit
          (<em>k</em>) and two explicit hyperparameters (<em>λ</em>
          and <em>β</em>). To validate the strength of the proposed
          POH over traditional hypergraph, we set <em>k</em> and
          <em>λ</em> as the optimal ones of the baseline
          <strong>Hypergraph</strong>, and then searched <em>β</em>
          in the range of [1<em>e</em>-4, 1<em>e</em>1]. For
          <strong>POH-All</strong>, we tuned one more
          hyperparameter <em>a</em> <sub>1</sub>, which controls
          the importance of logical rules and is in the range of
          [0, 1].</p>
          <p>Note that we have intentionally fixed <em>k</em> and
          <em>λ</em> to the optimal ones of
          <strong>Hypergraph</strong>, which also simplifies the
          tuning process. Further tuning <em>k</em> and <em>λ</em>
          based on the performance of POH methods can lead to even
          better performance&nbsp;(see Figure&nbsp;<a class="fig"
          href="#fig4">4</a>). Figure <a class="fig" href=
          "#fig3">3</a> shows the performance of
          <strong>POH-All</strong> <em>w.r.t. β</em> and <em>a</em>
          <sub>1</sub>. This was accomplished by varying one
          parameter and fixing the other to the optimal value. As
          can be seen, our method is rather insensitive to
          hyperparameters around their optimal settings.</p>
        </section>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Experiment
            Results</h3>
          </div>
        </header>
        <section id="sec-23">
          <p><em>5.2.1 Method Comparison.</em> Table <a class="tbl"
          href="#tab1">1</a> summarizes the performance comparison
          on university ranking, from which we have the following
          observations: (1) <strong>Hypergraph</strong> performs
          better than <strong>Simple Graph</strong>, which verifies
          that considering the higher-order relations among
          universities is effective for the ranking task. (2) All
          POH-based methods outperform baselines by a large margin
          (<em>e.g.,</em> <strong>POH-All</strong> ourperforms
          <strong>GMR</strong> with an improvement of 18.46%,
          3.10%, and 1.03% <em>w.r.t.</em> MAE, Tau, and Rho,
          respectively). It demonstrates the effectiveness of our
          proposed POH and regularized learning in integrating
          partial-order relations. (3) <strong>POH-All</strong>
          outperforms both <strong>POH-Salary</strong> and
          <strong>POH-NCEE</strong>. It further verifies the
          advantage of POH-based learning methods and reflects that
          jointly modelling multiple partial-order relations and
          rules is helpful. (4) The <em>p</em>-values of student's
          t-test between POH-based methods and all the other
          methods are smaller than 0.05, indicating the
          significance of the performance improvements.</p>
          <figure id="fig3">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig3.jpg"
            class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span>
              <span class="figure-title">Procedure of tuning
              <em>β</em> and <em>a</em> <sub>1</sub> for POH-All.
              The red dotted line marked the optimal
              settings</span>
            </div>
          </figure>
          <p></p>
          <p>As we constructed hypergraphs by connecting a vertex
          with its <em>k</em>-nearest vertices, larger <em>k</em>
          makes the POH-based methods consider more vertex pairs
          with the given partial-order relations (these pairs would
          be eliminated if the vertex pair is not connected by any
          hyperedge). It is thus interesting to see how does the
          setting of <em>k</em> impact the performance of POH
          learning. Figure&nbsp;<a class="fig" href="#fig4">4</a>
          shows the performance of Tau of
          <strong>Hypergraph</strong> and our POH-based methods on
          different <em>k</em>. Note that other hyperparameters
          have been fairly tuned for each setting of <em>k</em>. As
          can be seen, all POH-based methods outperform the
          <strong>Hypergraph</strong> on all settings. It
          demonstrates that the proposed POH learning consistently
          outperforms the conventional hypergraph, regardless of
          the underlying hypergraph structure. Moreover, all
          POH-based methods achieve performances better than those
          reported in Table <a class="tbl" href="#tab1">1</a>,
          which shows the performance of POH methods on the optimal
          <em>k</em> of <strong>Hypergraph</strong> only. It
          reveals the potential of POH-based methods on further
          improvements if a better hyperparameter tuning strategy
          is applied.</p>
        </section>
        <section id="sec-24">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.2.2</span> Result
              Analysis</h4>
            </div>
          </header>
          <figure id="fig4">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig4.jpg"
            class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span>
              <span class="figure-title">Performance comparison on
              Tau of Hypergraph and POH-based methods
              <em>w.r.t.</em> different <em>k</em>.</span>
            </div>
          </figure>
          <figure id="fig5">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig5.jpg"
            class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span>
              <span class="figure-title">Distribution of absolute
              rank errors.</span>
            </div>
          </figure>
          <figure id="fig6">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig6.jpg"
            class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span>
              <span class="figure-title">Percentage of correctly
              ranked university pairs.</span>
            </div>
          </figure>
          <p>To understand the results better, we performed
          finer-grained error analysis. Given the result generated
          by a method, we generated an array of rank positions
          (integers from 1 to <em>N</em>) for universities,
          computing the absolute error on the rank position on each
          university.</p>
          <p>Figure <a class="fig" href="#fig5">5</a> depicts the
          distribution of the absolute rank errors as a boxplot. As
          can be seen, the rank error distribution of POH-based
          methods is more dense and centralizes at smaller medians
          than that of <strong>Simple Graph</strong> and
          <strong>Hypergraph</strong>. It provides sufficient
          evidence on the better ranking generated by the POH-based
          methods. Moreover, we find that <strong>Simple
          Graph</strong> and <strong>Hypergraph</strong> make
          errors larger than 5 on about 25% of the universities,
          which is rarely seen from POH-based methods. Meanwhile,
          the largest error made by <strong>Simple Graph</strong>
          and <strong>Hypergraph</strong> is almost two times that
          of POH-based methods. These results demonstrate that
          POH-based methods are more robust, thus being more
          applicable in real-world applications. Among the
          baselines, <strong>GMR</strong> achieves the smallest
          rank error, which is comparable with
          <strong>POH-Salary</strong>. This signifies the
          usefulness of modelling the relations among data from
          different channels, which could be a future direction to
          be explored by POH-based methods.</p>
          <p>Besides the investigation on pointwise rank errors, we
          further performed an analysis on pairwise ranks. For each
          method, we counted the number of university pairs that
          are ranked correctly, and drew the percentage of correct
          pairs in Figure <a class="fig" href="#fig6">6</a>. As
          shown, POH-based methods manage to generate ranks with
          correct order on 1% more university pairs than
          <strong>Simple Graph</strong>,
          <strong>Hypergraph</strong>, and <strong>GMR</strong>,
          further demonstrating the accuracy and advantage of
          POH-based methods. Considering that there are more than
          80,000 university pairs, an improvement of 1% (correctly
          ranking 800+ pairs) is a significant improvement.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span>
              <span class="table-title">Pairwise and listwise
              correlations between the ranking results of POH-All
              and four Chinese university ranking systems. Entries
              in bold denote the most correlated result to the
              method of the corresponding column.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <td style="text-align:center;"></td>
                  <td colspan="5" style="text-align:center;">
                    <strong>Tau</strong>
                    <hr />
                  </td>
                  <td colspan="5" style="text-align:center;">
                    <strong>Rho</strong>
                    <hr />
                  </td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">
                  <strong>CUAA</strong></td>
                  <td style="text-align:center;">
                  <strong>WSL</strong></td>
                  <td style="text-align:center;">
                  <strong>WH</strong></td>
                  <td style="text-align:center;">
                  <strong>iPIN</strong></td>
                  <td style="text-align:center;">
                  <strong>POH</strong></td>
                  <td style="text-align:center;">
                  <strong>CUAA</strong></td>
                  <td style="text-align:center;">
                  <strong>WSL</strong></td>
                  <td style="text-align:center;">
                  <strong>WH</strong></td>
                  <td style="text-align:center;">
                  <strong>iPIN</strong></td>
                  <td style="text-align:center;">
                  <strong>POH</strong></td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">
                  <strong>CUAA</strong></td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.763</td>
                  <td style="text-align:center;">0.778</td>
                  <td style="text-align:center;">0.431</td>
                  <td style="text-align:center;">0.765</td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.903</td>
                  <td style="text-align:center;">0.903</td>
                  <td style="text-align:center;">0.573</td>
                  <td style="text-align:center;">0.886</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>WSL</strong></td>
                  <td style="text-align:center;">0.763</td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.823</td>
                  <td style="text-align:center;">0.459</td>
                  <td style="text-align:center;">0.806</td>
                  <td style="text-align:center;">
                  <strong>0.903</strong></td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.950</td>
                  <td style="text-align:center;">0.627</td>
                  <td style="text-align:center;">0.943</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>WH</strong></td>
                  <td style="text-align:center;">
                  <strong>0.778</strong></td>
                  <td style="text-align:center;">
                  <strong>0.823</strong></td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.469</td>
                  <td style="text-align:center;">
                  <strong>0.832</strong></td>
                  <td style="text-align:center;">
                  <strong>0.903</strong></td>
                  <td style="text-align:center;">
                  <strong>0.950</strong></td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.645</td>
                  <td style="text-align:center;">
                  <strong>0.961</strong></td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>iPIN</strong></td>
                  <td style="text-align:center;">0.431</td>
                  <td style="text-align:center;">0.459</td>
                  <td style="text-align:center;">0.469</td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.569</td>
                  <td style="text-align:center;">0.573</td>
                  <td style="text-align:center;">0.627</td>
                  <td style="text-align:center;">0.645</td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.750</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH</strong></td>
                  <td style="text-align:center;">0.765</td>
                  <td style="text-align:center;">0.806</td>
                  <td style="text-align:center;">0.832</td>
                  <td style="text-align:center;">
                  <strong>0.569</strong></td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">0.886</td>
                  <td style="text-align:center;">0.943</td>
                  <td style="text-align:center;">
                  <strong>0.961</strong></td>
                  <td style="text-align:center;">
                  <strong>0.750</strong></td>
                  <td style="text-align:center;">-</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Finally, we studied whether the ranking generated by
          our method <strong>POH-All</strong> is consistent with
          the four popular Chinese university ranking systems of
          year 2016 (<em>cf.</em> Section&nbsp;<a class="sec" href=
          "#sec-18">5.1.1</a>). We evaluated the pairwise
          correlation (Tau) and listwise correlation (Rho) between
          the ranked lists of the five ranking systems
          (<strong>POH-All</strong> and the four existing systems).
          The results are shown in Table&nbsp;<a class="tbl" href=
          "#tab2">2</a>. We can see that our method achieves rather
          high correlations with existing ranking systems, and it
          correlates most with WH. This shows that our result is
          relatively consistent with these manually devised ranking
          systems, implying that our POH-based re-ranking method
          shall be acceptable by the general readers.</p>
        </section>
      </section>
    </section>
    <section id="sec-25">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Popularity
          Prediction</h2>
        </div>
      </header>
      <p>Predicting the popularity of online content is a hot
      research topic in social media mining and has varying problem
      statements&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0033">33</a>]. Following the recent work on
      micro-video popularity prediction&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>], we formulated the task as
      a semi-supervised regression problem. Given <em>N</em> +
      <em>U</em> items with a feature matrix <span class=
      "inline-equation"><span class="tex">$\mathbf {X} \in \mathbb
      {R}^{(N + U) \times M}$</span></span> and the ground-truth
      popularity of the <em>N</em> items <span class=
      "inline-equation"><span class="tex">$\mathbf {y} \in \mathbb
      {R}^{N}$</span></span> , the objective is to learn a function
      <span class="inline-equation"><span class="tex">$\hat{y_i} =
      f(\mathbf {x_i})$</span></span> that maps an item from the
      feature space to the popularity space. To solve the problem,
      we first constructed a POH with partial-order relations on
      some important numerical features (detailed later in
      experiments). We then derived an instantiation of the general
      framework Equation (<a class="eqn" href="#eq6">8</a>) for the
      semi-supervised regression task as follows:</p>
      <div class="table-responsive" id="eq10">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \Gamma =
          \hat{\mathbf {y}}^T \mathbf {L} \hat{\mathbf {y}} +
          \lambda \sum \limits _{i=1}^{N} (\hat{y_i} - y_i)^2 +
          \beta \sum \limits _{r=1}^{R} \frac{a_{r}}{\left| \mathbf
          {H^{r}} \right|} \sum _{\lbrace i,j | H^{r}_{ij} \ne 0
          \rbrace } ReLU((\hat{y_j} - \hat{y_i})H^{r}_{ij}),
          \end{equation}</span><br />
          <span class="equation-number">(13)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\hat{\mathbf {y}} = [\hat{y}_1, \cdots , \hat{y}_N,
      \hat{y}_{N+1}, \cdots , \hat{y}_{N+U}] \in \mathbb {R}^{N +
      U}$</span></span> , denoting the prediction of all items
      (both with labels and without labels).
      <p></p>
      <section id="sec-26">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span> Experiment
            Settings</h3>
          </div>
        </header>
        <section id="sec-27">
          <p><em>6.1.1 Dataset.</em> We employed the same dataset
          as [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0005">5</a>] for experiments. It contains
          9,719 micro-videos collected from Vine<a class="fn" href=
          "#fn7" id="foot-fn7"><sup>6</sup></a>, posted between
          July 1st and October 1st, 2015. Each micro-video has
          visual, audio, and textual contents, as well as the
          profile of the user who posted it. With these data, the
          authors of [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0005">5</a>] extracted a rich set of
          popularity-oriented features, such as user activities,
          object distribution, aesthetic description, sentence
          embedding, and textual sentiment polarity, to represent a
          micro-video. To measure the popularity of a micro-video,
          they employed four popularity-related indicators, namely,
          the number of comments (<span class=
          "inline-equation"><span class=
          "tex">$n\_comments$</span></span> ), the number of likes
          (<span class="inline-equation"><span class=
          "tex">$n\_likes$</span></span> ), the number of reposts
          (<span class="inline-equation"><span class=
          "tex">$n\_reposts$</span></span> ), and the number of
          loops (<span class="inline-equation"><span class=
          "tex">$n\_loops$</span></span> ); the four indicators
          were averagely fused (<span class=
          "inline-equation"><span class="tex">$(n\_comments +
          n\_likes + n\_reposts + n\_loops) / 4$</span></span> ) as
          the popularity ground-truth for a micro-video.</p>
        </section>
        <section id="sec-28">
          <p><em>6.1.2 Evaluation.</em> We performed 10-fold
          cross-validation and evaluated the performance in terms
          of three metrics. From the regression perspective, we
          followed the previous work [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0005">5</a>] and employed normalized mean
          square error (nMSE). Meanwhile, we utilized two
          ranking-oriented metrics, Tau and Rho correlation
          coefficients. Besides, we carried out the student's
          t-test and reported the p-values where necessary.</p>
        </section>
        <section id="sec-29">
          <p><em>6.1.3 Methods.</em> We compare with following
          baselines:</p>
          <ul class="list-no-style">
            <li id="list10" label="•">
              <strong>Simple Graph</strong> [<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0049">49</a>]:
              We applied the same setting as the <strong>Simple
              Graph</strong> described in Section <a class="sec"
              href="#sec-20">5.1.3</a>.<br />
            </li>
            <li id="list11" label="•">
              <strong>Hypergraph</strong> [<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0003">3</a>]: We
              also adopted the same setting as the
              <strong>Hypergraph</strong> in Section <a class="sec"
              href="#sec-20">5.1.3</a>.<br />
            </li>
            <li id="list12" label="•">
              <strong>TMALL</strong> [<a class="bib" data-trigger=
              "hover" data-toggle="popover" data-placement="top"
              href="#BibPLXBIB0005">5</a>]: This method first
              calculates a simple graph Laplacian matrix with
              features from each modality (visual, audio,
              <em>etc.</em>). It then learns a common space
              Laplacian matrix by considering the relations among
              different modalities and fusing the corresponding
              graph Laplacian matrices. It finally performs a
              simple graph learning like <strong>Simple
              Graph</strong> on the common Laplacian matrix. We
              followed the settings as reported in their
              paper.<br />
            </li>
            <li id="list13" label="•">
              <strong>GCN</strong> [<a class="bib" data-trigger=
              "hover" data-toggle="popover" data-placement="top"
              href="#BibPLXBIB0021">21</a>]: This is the
              state-of-the-art graph learning method by using graph
              convolutional neural networks.<br />
              We replaced the log loss term in their implementation
              with the same mean squared loss in Equation
              (<a class="eqn" href="#eq10">13</a>) for a fair
              comparison. We carefully tuned four hyperparameters,
              namely, learning rate, dropout ratio, <em>l</em>
              <sub>2</sub>-norm weight and hidden layer size.<br />
            </li>
            <li id="list14" label="•"><strong>LR-HG</strong>:<br />
            This method is similar to <strong>Hypergraph</strong>.
            Instead of directly learning <span class=
            "inline-equation"><span class=
            "tex">$\hat{y}$</span></span> , we parameterized it as
            a linear regression (LR) model on features. The
            optimization process learns the parameters of LR, which
            is then used to predict <span class=
            "inline-equation"><span class=
            "tex">$\hat{y}$</span></span> .<br /></li>
          </ul>
          <p>We evaluated several POH methods on the same
          hypergraph structure of <strong>Hypergraph</strong>:</p>
          <ul class="list-no-style">
            <li id="list15" label="•"><strong>POH-Follow</strong>:
            This method considers a partial-order relation on the
            follower feature (<em>i.e.,</em> the number of
            followers of the user who posted the video). It encodes
            the logical rule <em>followers</em>
            <sub>&gt;</sub>(<strong>x<sub>i</sub></strong> ,
            <strong>x<sub>j</sub></strong> ) → <em>popularity</em>
            <sub>&gt;</sub>(<strong>x<sub>i</sub></strong> ,
            <strong>x<sub>j</sub></strong> ), meaning that
            <strong>x<sub>i</sub></strong> would be more popular
            than <strong>x<sub>j</sub></strong> if the user of
            <strong>x<sub>i</sub></strong> has more followers than
            that of <strong>x<sub>j</sub></strong> .<br /></li>
            <li id="list16" label="•"><strong>POH-Loop</strong>:
            This method has the same setting as
            <strong>POH-Follow</strong>, besides that it encodes
            another partial-order relation on the loop feature
            (<em>i.e.,</em> total number of views of all videos
            posted by a user).<br /></li>
            <li id="list17" label="•"><strong>POH-All</strong>:
            This method jointly encodes the two partial-order
            relations in <strong>POH-Follow</strong> and
            <strong>POH-Loop</strong>. We set the corresponding
            rule importance hyperparameters as <em>a</em>
            <sub>1</sub> and 1 − <em>a</em> <sub>1</sub>,
            respectively.<br /></li>
            <li id="list18" label="•"><strong>LR-POH</strong>:
            Similar to <strong>LR-HG</strong>, this method
            parameterizes the <span class=
            "inline-equation"><span class=
            "tex">$\hat{y}$</span></span> of
            <strong>POH-All</strong> as a linear regression model
            on input features.<br /></li>
          </ul>
        </section>
        <section id="sec-30">
          <p><em>6.1.4 Hyperparameter Tuning.</em> We employed the
          same procedure as described in Section <a class="sec"
          href="#sec-21">5.1.4</a> to tune the hyperparameters of
          POH methods. Optimal hyperparameter settings of each
          compared method will be released together with their
          codes. We investigated the sensitivity of our proposed
          POH-based methods by taking <strong>POH-All</strong> as
          an example. Figure <a class="fig" href="#fig7">7</a>
          illustrates the performance of <strong>POH-All</strong>
          while varying one hyperparameter and fixing the others
          with optimal values. Again, the results demonstrate that
          our model is not sensitive to the parameters around their
          optimal settings.</p>
          <figure id="fig7">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig7.jpg"
            class="img-responsive" alt="Figure 7" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 7:</span>
              <span class="figure-title">Procedure of tuning
              <em>β</em> and <em>a</em> <sub>1</sub> for POH-All.
              The red dotted line marks the optimal
              settings.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
      <section id="sec-31">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span> Experiment
            Results</h3>
          </div>
        </header>
        <section id="sec-32">
          <header>
            <div class="title-info">
              <h4><span class="section-number">6.2.1</span> Method
              Comparison</h4>
            </div>
          </header>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span>
              <span class="table-title">Performance comparison on
              popularity prediction.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <td style="text-align:center;">
                  <strong>Methods</strong></td>
                  <td style="text-align:center;">
                  <strong>nMSE</strong></td>
                  <td style="text-align:center;">
                  <strong>Tau</strong></td>
                  <td style="text-align:center;">
                  <strong>Rho</strong></td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;"><strong>Simple
                  Graph</strong></td>
                  <td style="text-align:center;">0.999 ± 1e-3</td>
                  <td style="text-align:center;">0.137 ± 2e-2</td>
                  <td style="text-align:center;">0.200 ± 2e-2</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>Hypergraph</strong></td>
                  <td style="text-align:center;">1.000 ± 4e-5</td>
                  <td style="text-align:center;">0.165 ± 3e-2</td>
                  <td style="text-align:center;">0.240 ± 4e-2</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                    <strong>TMALL</strong><a class="fn" href="#fn8"
                    id="foot-fn8"><sup>7</sup></a>
                  </td>
                  <td style="text-align:center;">0.979 ± 9e-3</td>
                  <td style="text-align:center;">-</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH-Follow</strong></td>
                  <td style="text-align:center;">1.000 ± 4e-4</td>
                  <td style="text-align:center;">0.393 ±
                  3e-2<sup>*</sup></td>
                  <td style="text-align:center;">0.562 ±
                  3e-2<sup>*</sup></td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH-Loop</strong></td>
                  <td style="text-align:center;">0.997 ± 2e-3</td>
                  <td style="text-align:center;">0.376 ±
                  2e-2<sup>*</sup></td>
                  <td style="text-align:center;">0.540 ±
                  3e-2<sup>*</sup></td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>POH-All</strong></td>
                  <td style="text-align:center;">0.989 ± 9e-3</td>
                  <td style="text-align:center;">
                  <strong>0.419</strong> ± <strong> 2e − 2</strong>
                  <sup>**</sup></td>
                  <td style="text-align:center;">
                  <strong>0.592</strong> ± <strong> 3e − 2</strong>
                  <sup>**</sup></td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>GCN</strong></td>
                  <td style="text-align:center;">0.919 ± 6e-2</td>
                  <td style="text-align:center;">0.171 ± 2e-2</td>
                  <td style="text-align:center;">0.252 ± 3e-2</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>LR-HG</strong></td>
                  <td style="text-align:center;">0.846 ±
                  1e-1<sup>*</sup></td>
                  <td style="text-align:center;">0.117 ± 2e-2</td>
                  <td style="text-align:center;">0.182 ± 3e-2</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>LR-POH</strong></td>
                  <td style="text-align:center;">
                  <strong>0.724</strong> ± <strong> 2e − 1</strong>
                  <sup>**</sup></td>
                  <td style="text-align:center;">0.350 ±
                  2e-2<sup>*</sup></td>
                  <td style="text-align:center;">0.496 ±
                  3e-2<sup>*</sup></td>
                </tr>
              </tbody>
              <tfoot>
                <tr>
                  <td colspan="4">* and ** denote that the
                  corresponding performance is significantly better
                  (<em>p</em>-value &lt; 0.05) than all baselines
                  and all other methods, respectively.</td>
                </tr>
              </tfoot>
            </table>
          </div>
          <p>We first investigated the effectiveness of the
          proposed methods. Table <a class="tbl" href="#tab3">3</a>
          shows the performance of all the compared methods. We
          have the following findings:</p>
          <p>(1) <strong>Hypergraph</strong> outperforms
          <strong>Simple Graph</strong> <em>w.r.t.</em> Tau and
          Rho, although they achieve the same performance level on
          nMSE. It verifies that considering the higher-order
          relations among videos leads to popularity prediction
          with more accurate relative orders.</p>
          <p>(2) <strong>POH-Follow</strong> and
          <strong>POH-Loop</strong> further surpass
          <strong>Hypergraph</strong> with an average improvement
          of 133.03% and 129.58% on the pairwise and listwise
          ranking metrics; meanwhile, slight improvement is
          obtained on the pointwise regression metric nMSE. This
          indicates that considering meaningful partial-order
          relations is particularly helpful for better predicting
          the relative order of the videos.</p>
          <p>(3) <strong>POH-All</strong> outperforms
          <strong>POH-Follow</strong> and <strong>POH-Loop</strong>
          with a significant average improvement on Tau (+8.97%)
          and Rho (+7.44%) as well as a slight improvement on nMSE.
          It validates that jointly considering multiple
          partial-order relations is useful.</p>
          <p>(4) Comparing <strong>Hypergraph</strong> with
          <strong>LR-HG</strong>, we can see that better nMSE can
          be achieved by using LR as the predictive model, but the
          two ranking metrics become worse. The same situation can
          be observed for <strong>POH-All</strong> and
          <strong>LR-POH</strong>. This provides evidence that
          using a sophisticated model can better fit the labels and
          help to minimize the regression loss, however, the
          ranking performance may not be necessarily improved. The
          same finding has been observed before in popularity
          prediction&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0016">16</a>] and another orthogonal
          application of item recommendation&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0007">7</a>]. In our
          case of graph-based learning, the regularizers (for
          smoothness and partial-order rules) carry strong signals
          for learning the relative orders between vertices.
          However, the regularization effects might be weakened
          when a specialized model is used to fit the label in the
          meantime. We leave more detailed exploration of this
          hypothesis as future work. (5) <strong>GCN</strong>
          outperforms <strong>POH-All</strong> <em>w.r.t.</em>
          nMSE, while Tau and Rho indicate that its ranking
          performance is worse. The lower nMSE of
          <strong>GCN</strong> can be credit to the strong
          representation power of the underlying neural network,
          which can fit the labels well. However,
          <strong>GCN</strong> may overfit the data and fail to
          predict the popularity ranking well without
          regularization on the relative orders of vertices.</p>
          <p>(6) <strong>LR-POH</strong> achieves the best
          performance with significantly better nMSE than all the
          other compared methods as well as tremendously better Tau
          and Rho than all the baseline methods. This further
          demonstrates the effectiveness of our proposed POH
          learning.</p>
          <figure id="fig8">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186064/images/www2018-73-fig8.jpg"
            class="img-responsive" alt="Figure 8" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 8:</span>
              <span class="figure-title">Performance comparison on
              Tau of Hypergraph and POH-based methods
              <em>w.r.t.</em> different <em>k</em>.</span>
            </div>
          </figure>
          <p></p>
          <p>We further studied whether the performance
          improvements of the proposed POH-based methods are
          consistent under different hypergraph settings. We
          compared the optimal performance of
          <strong>Hypergraph</strong>, <strong>POH-Follow</strong>,
          <strong>POH-Loop</strong>, and <strong>POH-All</strong>
          under different values of <em>k</em>, which controls the
          number of videos connected by a hyperedge. As illustrated
          in Figure <a class="fig" href="#fig8">8</a>, all
          POH-based methods outperform the
          <strong>Hypergraph</strong> under all the values of
          <em>k</em> by a large margin. It is worth noting that the
          optimal performance of POH methods are better than that
          shown in Table <a class="tbl" href="#tab3">3</a> (Table
          <a class="tbl" href="#tab3">3</a> shows the results of
          POH on the optimal setting of
          <strong>Hypergraph</strong>). This is consistent with the
          university ranking task, which implies the potential of
          further improving POH learning with a better
          hyperparameter tuning strategy.</p>
        </section>
      </section>
    </section>
    <section id="sec-33">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Conclusions</h2>
        </div>
      </header>
      <p>In this paper, we proposed a novel partial-order
      hypergraph that improves conventional hypergraphs by encoding
      the partial-order relations among vertices. We then
      generalized existing graph-based learning methods to
      partial-order hypergraphs by integrating the second-order
      logic rules that encode the partial-order relations;
      moreover, the time complexity of learning remains unchanged.
      Experimental results on university ranking and video
      popularity prediction demonstrate the effectiveness of our
      proposed methods.</p>
      <p>In future, we will explore the proposed POH to address
      more graph-based applications. Besides, we will further
      improve POH learning by replacing the linear prediction
      function from feature to label spaces with the advanced deep
      neural networks. Moreover, we plan to optimize the spatial
      complexity of POH with discrete hashing techniques like
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0046">46</a>].
      Furthermore, we would like to investigate the automatic
      extraction of partial-order relations and logical rules to
      construct POH.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Charu&nbsp;C Aggarwal
        and Chandan&nbsp;K Reddy. 2013. <em><em>Data clustering:
        algorithms and applications</em></em> . CRC press.</li>
        <li id="BibPLXBIB0002" label="[2]">Stephen&nbsp;H. Bach,
        Matthias Broecheler, Bert Huang, and Lise Getoor. 2017.
        Hinge-Loss Markov Random Fields and Probabilistic Soft
        Logic. <em><em>Journal of Machine Learning
        Research</em></em> (2017).</li>
        <li id="BibPLXBIB0003" label="[3]">Abdelghani Bellaachia
        and Mohammed Al-Dhelaan. 2014. Multi-document
        hyperedge-based ranking for text summarization. In
        <em><em>CIKM</em></em> . 1919–1922.</li>
        <li id="BibPLXBIB0004" label="[4]">Chen Chen, Cewu Lu,
        Qixing Huang, Qiang Yang, Dimitrios Gunopulos, and Leonidas
        Guibas. 2016. City-Scale Map Creation and Updating Using
        GPS Collections. In <em><em>SIGKDD</em></em> .
        1465–1474.</li>
        <li id="BibPLXBIB0005" label="[5]">Jingyuan Chen, Xuemeng
        Song, Liqiang Nie, Xiang Wang, Hanwang Zhang, and Tat-Seng
        Chua. 2016. Micro tells macro: predicting the popularity of
        micro-videos via a transductive model. In
        <em><em>MM</em></em> . 898–907.</li>
        <li id="BibPLXBIB0006" label="[6]">Zhiyong Cheng and Jialie
        Shen. 2016. On effective location-aware music
        recommendation. <em><em>Transactions on Information
        System</em></em> 34, 2 (2016), 13.</li>
        <li id="BibPLXBIB0007" label="[7]">Paolo Cremonesi, Yehuda
        Koren, and Roberto Turrin. 2010. Performance of Recommender
        Algorithms on Top-n Recommendation Tasks. In
        <em><em>RecSys</em></em> . 39–46.</li>
        <li id="BibPLXBIB0008" label="[8]">Inderjit&nbsp;S Dhillon,
        Yuqiang Guan, and Brian Kulis. 2004. Kernel k-means:
        spectral clustering and normalized cuts. In
        <em><em>SIGKDD</em></em> . 551–556.</li>
        <li id="BibPLXBIB0009" label="[9]">Fuli Feng, Liqiang Nie,
        Xiang Wang, Richang Hong, and Tat-Seng Chua. 2017.
        Computational social indicators: a case study of chinese
        university ranking. In <em><em>SIGIR</em></em> .
        455–464.</li>
        <li id="BibPLXBIB0010" label="[10]">Xiaodong Feng, Sen Wu,
        and Wenjun Zhou. 2017. Multi-Hypergraph Consistent Sparse
        Coding. <em><em>Transactions on Intelligent Systems and
        Technology</em></em> 8, 6(2017), 75.</li>
        <li id="BibPLXBIB0011" label="[11]">David&nbsp;F Gleich and
        Michael&nbsp;W Mahoney. 2015. Using local spectral methods
        to robustify graph-based learning algorithms. In
        <em><em>SIGKDD</em></em> . 359–368.</li>
        <li id="BibPLXBIB0012" label="[12]">Ian Goodfellow, Yoshua
        Bengio, and Aaron Courville. 2016. <em><em>Deep
        learning</em></em> . MIT press.</li>
        <li id="BibPLXBIB0013" label="[13]">Richard&nbsp;HR
        Hahnloser, Rahul Sarpeshkar, Misha&nbsp;A Mahowald,
        Rodney&nbsp;J Douglas, and H&nbsp;Sebastian Seung. 2000.
        Digital selection and analogue amplification coexist in a
        cortex-inspired silicon circuit. <em><em>Nature</em></em>
        405, 6789 (2000), 947.</li>
        <li id="BibPLXBIB0014" label="[14]">Kaiming He, Xiangyu
        Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
        learning for image recognition. In <em><em>CVPR</em></em> .
        770–778.</li>
        <li id="BibPLXBIB0015" label="[15]">Xiangnan He and
        Tat-Seng Chua. 2017. Neural Factorization Machines for
        Sparse Predictive Analytics. In <em><em>SIGIR</em></em> .
        355–364.</li>
        <li id="BibPLXBIB0016" label="[16]">Xiangnan He, Ming Gao,
        Min-Yen Kan, Yiqun Liu, and Kazunari Sugiyama. 2014.
        Predicting the Popularity of Web 2.0 Items Based on User
        Comments. In <em><em>SIGIR</em></em> . 233–242.</li>
        <li id="BibPLXBIB0017" label="[17]">Xiangnan He, Ming Gao,
        Min-Yen Kan, and Dingxian Wang. 2017. Birank: Towards
        ranking on bipartite graphs. <em><em>Transactions on
        Knowledge and Data Engineering</em></em> 29, 1(2017),
        57–71.</li>
        <li id="BibPLXBIB0018" label="[18]">Manel Hmimida and
        Rushed Kanawati. 2016. A Graph-Coarsening Approach for Tag
        Recommendation. In <em><em>WWW</em></em> . 43–44.</li>
        <li id="BibPLXBIB0019" label="[19]">Sheng Huang, Mohamed
        Elhoseiny, Ahmed Elgammal, and Dan Yang. 2015. Learning
        hypergraph-regularized attribute predictors. In
        <em><em>CVPR</em></em> . 409–417.</li>
        <li id="BibPLXBIB0020" label="[20]">Jyun-Yu Jiang, Pu-Jen
        Cheng, and Wei Wang. 2017. Open Source Repository
        Recommendation in Social Coding. In <em><em>SIGIR</em></em>
        . 1173–1176.</li>
        <li id="BibPLXBIB0021" label="[21]">Thomas&nbsp;N Kipf and
        Max Welling. 2017. Semi-supervised classification with
        graph convolutional networks. <em><em>ICLR</em></em>
        (2017).</li>
        <li id="BibPLXBIB0022" label="[22]">Lei Li and Tao Li.
        2013. News recommendation via hypergraph learning:
        encapsulation of user behavior and news content. In
        <em><em>WSDM</em></em> . 305–314.</li>
        <li id="BibPLXBIB0023" label="[23]">David&nbsp;C Liu,
        Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk,
        Kevin&nbsp;C Ma, Zhigang Zhong, Jenny Liu, and Yushi Jing.
        2017. Related pins at pinterest: The evolution of a
        real-world recommender system. In <em><em>WWW</em></em> .
        583–592.</li>
        <li id="BibPLXBIB0024" label="[24]">Qingshan Liu, Yubao
        Sun, Cantian Wang, Tongliang Liu, and Dacheng Tao. 2017.
        Elastic net hypergraph learning for image clustering and
        semi-supervised classification. <em><em>Transactions on
        Image Processing</em></em> 26, 1 (2017), 452–463.</li>
        <li id="BibPLXBIB0025" label="[25]">Tie-Yan Liu. 2011.
        <em><em>Learning to rank for information
        retrieval</em></em> . Springer Science &amp; Business
        Media.</li>
        <li id="BibPLXBIB0026" label="[26]">Tao Mei, Yong Rui,
        Shipeng Li, and Qi Tian. 2014. Multimedia search reranking:
        A literature survey. <em><em>Comput. Surveys</em></em> 46,
        3 (2014), 38.</li>
        <li id="BibPLXBIB0027" label="[27]">Michael Mitzenmacher,
        Jakub Pachocki, Richard Peng, Charalampos Tsourakakis, and
        Shen&nbsp;Chen Xu. 2015. Scalable large near-clique
        detection in large-scale networks via sampling. In
        <em><em>SIGKDD</em></em> . 815–824.</li>
        <li id="BibPLXBIB0028" label="[28]">RB Nelsen. 2001.
        Kendall tau metric. <em><em>Encyclopaedia of
        Mathematics</em></em> 3 (2001), 226–227.</li>
        <li id="BibPLXBIB0029" label="[29]">Liqiang Nie, Meng Wang,
        Zheng-Jun Zha, and Tat-Seng Chua. 2012. Oracle in image
        search: a content-based approach to performance prediction.
        <em><em>Transactions on Information System</em></em> 30, 2
        (2012), 13.</li>
        <li id="BibPLXBIB0030" label="[30]">Liqiang Nie, Shuicheng
        Yan, Meng Wang, Richang Hong, and Tat-Seng Chua. 2012.
        Harvesting visual concepts for image search with complex
        queries. In <em><em>MM</em></em> . 59–68.</li>
        <li id="BibPLXBIB0031" label="[31]">Adi Omari, David
        Carmel, Oleg Rokhlenko, and Idan Szpektor. 2016. Novelty
        Based Ranking of Human Answers for Community Questions. In
        <em><em>SIGIR</em></em> . 215–224.</li>
        <li id="BibPLXBIB0032" label="[32]">Xiang Ren, Ahmed
        El-Kishky, Chi Wang, Fangbo Tao, Clare&nbsp;R. Voss, and
        Jiawei Han. 2015. ClusType: Effective Entity Recognition
        and Typing by Relation Phrase-Based Clustering. In
        <em><em>SIGKDD</em></em> . 995–1004.</li>
        <li id="BibPLXBIB0033" label="[33]">Marian-Andrei Rizoiu,
        Lexing Xie, Scott Sanner, Manuel Cebrian, Honglin Yu, and
        Pascal Van&nbsp;Hentenryck. 2017. Expecting to Be HIP:
        Hawkes Intensity Processes for Social Media Popularity. In
        <em><em>WWW</em></em> . 735–744.</li>
        <li id="BibPLXBIB0034" label="[34]">Charles Spearman. 1987.
        The proof and measurement of association between two
        things. <em><em>The American journal of
        psychology</em></em> 100 (1987), 441–471.</li>
        <li id="BibPLXBIB0035" label="[35]">Kenneth Tran, Saghar
        Hosseini, Lin Xiao, Thomas Finley, and Mikhail Bilenko.
        2015. Scaling up stochastic dual coordinate ascent. In
        <em><em>SIGKDD</em></em> . 1185–1194.</li>
        <li id="BibPLXBIB0036" label="[36]">Charalampos&nbsp;E
        Tsourakakis, Jakub Pachocki, and Michael Mitzenmacher.
        2017. Scalable motif-aware graph clustering. In
        <em><em>WWW</em></em> . 1451–1460.</li>
        <li id="BibPLXBIB0037" label="[37]">Meng Wang, Weijie Fu,
        Shijie Hao, Dacheng Tao, and Xindong Wu. 2016. Scalable
        semi-supervised learning by efficient anchor graph
        regularization. <em><em>Transactions on Knowledge and Data
        Engineering</em></em> 28, 7(2016), 1864–1877.</li>
        <li id="BibPLXBIB0038" label="[38]">Meng Wang, Xueliang
        Liu, and Xindong Wu. 2015. Visual Classification by
        ℓ<sub>1</sub> -Hypergraph Modeling. <em><em>TKDE</em></em>
        27, 9 (2015), 2564–2574.</li>
        <li id="BibPLXBIB0039" label="[39]">Xiang Wang, Xiangnan
        He, Liqiang Nie, and Tat-Seng Chua. 2017. Item silk road:
        Recommending items from information domains to social
        users. In <em><em>SIGIR</em></em> . 185–194.</li>
        <li id="BibPLXBIB0040" label="[40]">Xiaoqian Wang, Feiping
        Nie, and Heng Huang. 2016. Structured Doubly Stochastic
        Matrix for Graph Based Clustering: Structured Doubly
        Stochastic Matrix. In <em><em>SIGKDD</em></em> .
        1245–1254.</li>
        <li id="BibPLXBIB0041" label="[41]">Cort&nbsp;J Willmott
        and Kenji Matsuura. 2005. Advantages of the mean absolute
        error (MAE) over the root mean square error (RMSE) in
        assessing average model performance. <em><em>Climate
        research</em></em> 30, 1 (2005), 79–82.</li>
        <li id="BibPLXBIB0042" label="[42]">Yuichi Yoshida. 2014.
        Almost linear-time algorithms for adaptive betweenness
        centrality using hypergraph sketches. In
        <em><em>SIGKDD</em></em> . 1416–1425.</li>
        <li id="BibPLXBIB0043" label="[43]">Hsiang-Fu Yu, Cho-Jui
        Hsieh, Hyokun Yun, SVN Vishwanathan, and Inderjit&nbsp;S
        Dhillon. 2015. A scalable asynchronous distributed
        algorithm for topic modeling. In <em><em>WWW</em></em> .
        1340–1350.</li>
        <li id="BibPLXBIB0044" label="[44]">Rose Yu, Huida Qiu,
        Zhen Wen, ChingYung Lin, and Yan Liu. 2016. A survey on
        social media anomaly detection. <em><em>SIGKDD</em></em>
        18, 1 (2016), 1–14.</li>
        <li id="BibPLXBIB0045" label="[45]">Dongxiang Zhang, Long
        Guo, Xiangnan He, Jie Shao, Sai Wu, and Heng&nbsp;Tao Shen.
        2018. A Graph-Theoretic Fusion Framework for Unsupervised
        Entity Resolution. In <em><em>ICDE</em></em> .</li>
        <li id="BibPLXBIB0046" label="[46]">Hanwang Zhang, Fumin
        Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-Seng Chua.
        2016. Discrete collaborative filtering. In
        <em><em>SIGIR</em></em> . 325–334.</li>
        <li id="BibPLXBIB0047" label="[47]">Denny Zhou, Olivier
        Bousquet, Thomas&nbsp;N Lal, Jason Weston, and Bernhard
        Schölkopf. 2004. Learning with local and global
        consistency. In <em><em>NIPS</em></em> . 321–328.</li>
        <li id="BibPLXBIB0048" label="[48]">Denny Zhou, Jiayuan
        Huang, and Bernhard Schölkopf. 2007. Learning with
        hypergraphs: Clustering, classification, and embedding. In
        <em><em>NIPS</em></em> . 1601–1608.</li>
        <li id="BibPLXBIB0049" label="[49]">Denny Zhou, Jason
        Weston, Arthur Gretton, Olivier Bousquet, and Bernhard
        Schölkopf. 2004. Ranking on data manifolds. In
        <em><em>NIPS</em></em> . 169–176.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Xiangnan He is
    the corresponding author. This research is part of the NExT++
    project, supported by the National Research Foundation, Prime
    Minister's Office, Singapore under its IRC@Singapore Funding
    Initiative.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>In China,
    universities were officially separated into three tiers by
    Ministry of Education (<a class="link-inline force-break" href=
    "https://tinyurl.com/moe-univ-list/.">https://tinyurl.com/moe-univ-list/.</a>).</p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a>CUAA: <a class=
    "link-inline force-break" href=
    "http://www.cuaa.net/cur/.">http://www.cuaa.net/cur/.</a> WSL:
    <a class="link-inline force-break" href=
    "http://edu.sina.com.cn/gaokao/wushulian/.">http://edu.sina.com.cn/gaokao/wushulian/.</a>
    WH: <a class="link-inline force-break" href=
    "http://www.nseac.com/html/168/.">http://www.nseac.com/html/168/.</a>
    iPIN: <a class="link-inline force-break" href=
    "https://www.wmzy.com/api/rank/schList/.">https://www.wmzy.com/api/rank/schList/.</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a>Note that we
    omitted listwise ranking evaluation metrics like
    Precision@<em>K</em> [<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0006">6</a>], Recall@<em>K</em> [<a class="bib"
    data-trigger="hover" data-toggle="popover" data-placement="top"
    href="#BibPLXBIB0039">39</a>], and NDCG@<em>K</em> [<a class=
    "bib" data-trigger="hover" data-toggle="popover"
    data-placement="top" href="#BibPLXBIB0029">29</a>] since they
    are sensitive to the selection of <em>K</em></p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a>Note that we
    omitted the comparison with <strong>TMALL</strong> and
    <strong>GCN</strong> mentioned in Section <a class="sec" href=
    "#sec-29">6.1.3</a>. Because [<a class="bib" data-trigger=
    "hover" data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0009">9</a>] has shown that <strong>TMALL</strong>
    (similar to the <strong>JL</strong> baseline in their paper) is
    less effective than <strong>GMR</strong>; <strong>GCN</strong>
    is not fit for this unsupervised ranking task as it is designed
    for semi-supervised and supervised tasks [<a class="bib"
    data-trigger="hover" data-toggle="popover" data-placement="top"
    href="#BibPLXBIB0021">21</a>]</p>
    <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/hennande/Partial_Order_Hypergraph">https://github.com/hennande/Partial_Order_Hypergraph</a></p>
    <p id="fn7"><a href="#foot-fn7"><sup>6</sup></a><a class=
    "link-inline force-break" href=
    "https://vine.co/.">https://vine.co/.</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>7</sup></a>This result is
    directly copied from the corresponding paper since we exactly
    followed their experimental settings.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186064">https://doi.org/10.1145/3178876.3186064</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
