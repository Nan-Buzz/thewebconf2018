<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Computational Creative Advertisements</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191549'>https://doi.org/10.1145/3184558.3191549</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191549'>https://w3id.org/oa/10.1145/3184558.3191549</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Computational Creative Advertisements</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Wei</span> <span class="surName">Sun</span> IBM Research, 1101 Kitchawan RdYorktown Heights, New York 10591, <a href="mailto:sunw@us.ibm.com">sunw@us.ibm.com</a>
        </div>
        <div class="author">
          <span class="givenName">Ying</span> <span class="surName">Li</span> IBM, 1 New Orchard RdArmonk, New York 10504, <a href="mailto:yingli@us.ibm.com">yingli@us.ibm.com</a>
        </div>
        <div class="author">
          <span class="givenName">Anshul</span> <span class="surName">Sheopuri</span> IBM, 1 New Orchard RdArmonk, New York 10504, <a href="mailto:sheopuri@us.ibm.com">sheopuri@us.ibm.com</a>
        </div>
        <div class="author">
          <span class="givenName">Thales</span> <span class="surName">Teixeira</span> Harvard Business School, Soldiers FieldBoston, Massachusetts 02163, <a href="mailto:thales@hbs.edu">thales@hbs.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191549" target="_blank">https://doi.org/10.1145/3184558.3191549</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Making successful video advertisements has long been considered a combination of art and business acumen. In this work, we propose a system to assist human designers to produce more effective advertisements with predictable outcomes. We formalize this concept with a dynamic Bayesian network (DBN), where we represent the knowledge base with data collected from large-scale field experiments in a novel setting. Face and eye tracking which continuously measures viewers emotional responses and viewing interest on 169 television advertisements for 2334 participants, along with moment-to-moment branding activities in the advertisements are used to estimate the model. The resulting DBN represents relationships across advertisement content, viewers emotional responses, as well as effectiveness metrics such as ad avoidance, sharing and influence on purchase. Conditioned on the specified requirement on the ad, a human designer can draw high scoring samples from the DBN, which represent the optimized sequences of branding activities and entertainment content.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Computational creativity; advertisement design; artificial intelligence; dynamic Bayesian network; inference</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Wei Sun, Ying Li, Anshul Sheopuri, and Thales Teixeira. 2018. Computational Creative Advertisements. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3191549" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3191549</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Advertising is an expensive investment in a brand. On average, a 30-second commercial in broadcast prime time for the 2017-18 season costs <font style="normal">$</font>134,009, a 6% increase from the previous season. The most expensive program in broadcast during the period is NBC's “Sunday Night Football”, with an average price for 30 seconds of ad time tagged at <font style="normal">$</font>699,602. CBS and NBC broadcasts of “Thursday Night Football” are tied for No. 2, averaging about <font style="normal">$</font>550,000 for 30 seconds<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>.</p>
      <p>The primary objective of this work is to develop a system which assists human designers to produce quantifiably effective advertisements. One of the key challenges is to teach a machine to answer “What constitutes a good ad?” Part of the challenge is that in both theory and practice, there are still ongoing debates on principles behind successful ads. Despite a considerable amount of research has been conducted to demystify the process, it remains inherently complex and error-prone, because of the tremendous variety of creative options and the large number of external factors compounding cognitive challenges for human designers.</p>
      <p>To address this challenge, we start by constructing a knowledge base of video advertisements and corresponding viewer responses. For each advertisement, we record second-by-second branding activities, such as placement and size of logo, frequency and duration of logo appearance and the presence of brand in audio track etc. To collect viewer responses, in addition to self-reported metrics, we also utilize eye tracking and facial expressions to provide moment-to-moment measurements on exactly where viewers are focused on screen at any given moment and discrete emotions such as joy and surprise. The tracking data which provides a real-time lens into subconscious consumer reactions is more effective than traditional methods like surveys, as it is a passive, unconscious expression of sentiment and engagement. Utilizing data collected from large-scale field experiments with 2334 participants and 169 television advertisements, we construct a knowledge base on ad designs via a dynamic Bayesian network (DBN), which simultaneously represents a multitude of relationship across advertisement content, viewers’ emotional responses, as well as effectiveness metrics such as ad avoidance, sharing, influence on purchase. With this knowledge base, recommendations on ad designs are obtained via inference. Conditioned on the specified requirements on an ad (e.g., product category, target audience, prominent display of brand logo in the end of the ad), a human designer can draw high scoring samples from the DBN, which represent the optimized sequences of branding activities and entertainment content that the system recommends. Figure <a class="fig" href="#fig1">1</a> shows its overall architecture.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Overall architecture of the proposed system.</span>
        </div>
      </figure>
      <p></p>
      <p>Developed in the 1980s, Bayesian networks are a mathematical formalism that can simultaneously represent probablistic relationships between many variables in a system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>]. They have been used in a wide variety of applications such as computational biology [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], information retrieval [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>], computer vision [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>] and risk analysis [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>]. Bayesian networks allow both predictive and diagnostic inference, which is crucial for reasoning about a complex process such as advertisement designs.</p>
      <p>The idea of using a knowledge-based system for advertisement design has been tossed around in the past [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. The major challenge in building such a system is creating the knowledge base. Prior work solely relies on input from experts to specify key variables and relationships to form a database of rules. As expert knowledge in this domain is often perceived as “soft”, qualitative and subjective, existing systems are unable to provide prescriptive recommendations. Moreover, they also have no mechanism for “creative thinking”, because recommendations are template-like rules based on what has been found to be effective in situations with the same set of characteristics. In this work, we take a different, data-driven approach in extracting knowledge via learning the relationships between content in advertisements and the corresponding viewer responses. Moreover, the system built on top of this knowledge base is able to assist human designers in thinking outside the box and expanding their exploration boundaries beyond what has existed.</p>
      <p>There have been some efforts on automatic generation of videos in the multimedia research area. For instance, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] proposes to automatically generate personalized music sports videos by first detecting semantic events from sports video content then stitching these events together and mixing in music. Similar ideas are also presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>], where the authors propose to automatically compose a music video from personal home videos. Nevertheless, prior work mainly focuses on the extraction of relevant snippets from given videos and the composition of these snippets with music, with an objective to represent existing content in a more pleasing and enjoyable way. Producing video ads, which involves many more creative decisions, is far more challenging. Besides generating entertaining content, advertisements also intend to achieve multiple objectives such as influencing sales, building brand awareness and mass communication.</p>
      <p>On a high level, the theme of this work is on computational creativity, a subfield of Artificial Intelligence research, which is concerned with machine systems that produce novel and high-quality artifacts and ideas [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>]. The study of these generative systems has been explored in a in a plethora of domains including music [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>], jokes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>], stories [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>], poems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>], games [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>], and culinary recipes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>]. Our work adds onto this body of literature in the domain of advertising, where human and computer creativity forms a two-way relationship, where hypothesis and rules learnt from human creativity is used to realize computational creativity, which in turn generates new hypothesis and rules to empower creativity in humans.</p>
      <p>The paper is outlined as follows: In Section 2 we describe the experiments and the data. We give a brief overview of Bayesian network and describe the learning of our network in in Section 3. In Section 4 we discuss how to use the network for recommendations. Finally, Section 5 concludes the paper and discusses ongoing research.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Experiments and Data Collection</h2>
        </div>
      </header>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Data Overview</h3>
          </div>
        </header>
        <p>The data for this research were collected from four large-scale controlled experiments on ad-viewer responses [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]. A sample of 169 television commercials were selected. They consist of a wide variety of product categories (e.g., beverages, consumer package goods, telecom), featuring both well-known brands (e.g., Coca-cola, T-Mobile) and obscure names (e.g., Sobe, Radio 538). 2334 consumers who are ad viewers were paid for participation. Each participant was exposed to a subset of commercials in a randomized order. On average, each advertisement was watched by 66 participants.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Data description.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>Feature name</strong></td>
                <td style="text-align:left;"><strong>Description</strong></td>
                <td style="text-align:left;"><strong>Mean</strong></td>
                <td style="text-align:left;"><strong>Range</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">Gender</td>
                <td style="text-align:left;">Partipant's gender (binary, 1 = male)</td>
                <td style="text-align:left;">0.475</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Age</td>
                <td style="text-align:left;">Participant's age in years (discrete)</td>
                <td style="text-align:left;">36.50</td>
                <td style="text-align:left;">17 - 62</td>
              </tr>
              <tr>
                <td style="text-align:left;">Ad Familiarity</td>
                <td style="text-align:left;">Participant's prior familiarity with the ad (binary, 1 = familiar)</td>
                <td style="text-align:left;">0.336</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Brand Familiarity</td>
                <td style="text-align:left;">Participant's prior familiarity with the brand (binary, 1 = familiar)</td>
                <td style="text-align:left;">0.726</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Entertain</td>
                <td style="text-align:left;">Participant's perception of brand as entertaining (Ordinal, 1 = not at all, 5 = very)</td>
                <td style="text-align:left;">3.117</td>
                <td style="text-align:left;">1 - 5</td>
              </tr>
              <tr>
                <td style="text-align:left;">Extravert</td>
                <td style="text-align:left;">Participant's self-reported degree of extraversion (Ordinal, 1 - 9 point scale)</td>
                <td style="text-align:left;">5.020</td>
                <td style="text-align:left;">1 - 9</td>
              </tr>
              <tr>
                <td style="text-align:left;">Other</td>
                <td style="text-align:left;">Participant's self-reported degree of other-directedness (Ordinal, 1 - 11 point scale)</td>
                <td style="text-align:left;">5.213</td>
                <td style="text-align:left;">1 - 11</td>
              </tr>
              <tr>
                <td style="text-align:left;">Ad Length</td>
                <td style="text-align:left;">Length of ad in seconds (continuous)</td>
                <td style="text-align:left;">39.64</td>
                <td style="text-align:left;">15 - 102</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logo Presence</td>
                <td style="text-align:left;">Whether the brand is currently appearing in the ad (binary, 1 = present)</td>
                <td style="text-align:left;">0.303</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Audio Brand</td>
                <td style="text-align:left;">Presence of brand name in audio track of ad (binary, 1 = present)</td>
                <td style="text-align:left;">0.353</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Competing</td>
                <td style="text-align:left;">Whether the brand is visually separated from the background (binary, 1 = separated)</td>
                <td style="text-align:left;">0.204</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Position</td>
                <td style="text-align:left;">Whether the brand is centrally located in the video (binary, 1 = central)</td>
                <td style="text-align:left;">0.118</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Size</td>
                <td style="text-align:left;">Size of the brand as a percentage of the pixels onscreen (continuous)</td>
                <td style="text-align:left;">0.035</td>
                <td style="text-align:left;">0 - 0.615</td>
              </tr>
              <tr>
                <td style="text-align:left;">Cardinality</td>
                <td style="text-align:left;">The number of times the brand has appeared up to this point in time (discrete)</td>
                <td style="text-align:left;">1.220</td>
                <td style="text-align:left;">0 - 39</td>
              </tr>
              <tr>
                <td style="text-align:left;">SC</td>
                <td style="text-align:left;">Complexity of the image onscreen measured by a compression algorithm (continuous)</td>
                <td style="text-align:left;">134.06</td>
                <td style="text-align:left;">1.90 - 662</td>
              </tr>
              <tr>
                <td style="text-align:left;">FTJoy</td>
                <td style="text-align:left;">Face tracking of the emotion joy (continuous)</td>
                <td style="text-align:left;">0.064</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">FTSurprise</td>
                <td style="text-align:left;">Face tracking of the emotion surprise (continuous)</td>
                <td style="text-align:left;">0.014</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">FTEyebrow</td>
                <td style="text-align:left;">Face tracking of eyebrow movement (continuous)</td>
                <td style="text-align:left;">0.272</td>
                <td style="text-align:left;">.013 - .919</td>
              </tr>
              <tr>
                <td style="text-align:left;">IAD</td>
                <td style="text-align:left;">The distance between a participant's point of focus compared to the group (continuous)</td>
                <td style="text-align:left;">-0.257</td>
                <td style="text-align:left;">-0.71 - 20.18</td>
              </tr>
              <tr>
                <td style="text-align:left;">Watch time</td>
                <td style="text-align:left;">The percentage of an ad that the participant has finished watching (continuous)</td>
                <td style="text-align:left;">0.759</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Zap</td>
                <td style="text-align:left;">Whether the participant skipped the ad at a particular time (binary, 1 = skipped)</td>
                <td style="text-align:left;">0.344</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Forward</td>
                <td style="text-align:left;">Whether the participant forwarded the link of the ad (binary, 1 = forwarded)</td>
                <td style="text-align:left;">0.273</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">NumForward</td>
                <td style="text-align:left;">The number of people to whom the participant forwarded the ad (discrete)</td>
                <td style="text-align:left;">0.663</td>
                <td style="text-align:left;">1 - 6</td>
              </tr>
              <tr>
                <td style="text-align:left;">Seedview</td>
                <td style="text-align:left;">The number of views garnered from the link forwarded by the participant</td>
                <td style="text-align:left;">5.327</td>
                <td style="text-align:left;">0 - 73</td>
              </tr>
              <tr>
                <td style="text-align:left;">Click</td>
                <td style="text-align:left;">Whether the participant clicked on a link on the brand (binary, 1 = clicked)</td>
                <td style="text-align:left;">0.118</td>
                <td style="text-align:left;">0 - 1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Purchase</td>
                <td style="text-align:left;">Participant's purchase intent after viewing the ad (ordinal, 1 = not at all, 10 = high)</td>
                <td style="text-align:left;">4.863</td>
                <td style="text-align:left;">1 - 10</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>A summary of data collected from the experiments is shown in Table 1. It includes viewer demographics data (e.g., gender, age, familiarity with the brand and the ad) and static ad information such as product category and ad length. For each ad, we also collected its dynamic advertising content, which includes branding activity data and entertainment content data. Branding activity which refers to the appearances of brand identity (name, logo, trademark) in the advertisements is used to promote awareness and persuasiveness. Ads are analyzed to identify the brand's presence, size, position, duration across frames and visual complexity of the scene.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Facial expression footage from each participant is collected via a webcam. The light colored lines over the face represent the virtual mask used to measure facial point deviations in the emotion analytics algorithms. They hone in on points on a face, such as the corners of eyes or eyebrows to detect texture variations that occur when people laugh, frown, or smirk.</span>
          </div>
        </figure>
        <p></p>
        <p>In order to effectively and systematically measure viewers’ responses, we collect both facial tracking and eye tracking data. We use facial tracking and emotion analytics to measure viewer's moment-to-moment emotional states in terms of joy and surprise. These measurements on emotions are used as proxies for entertainment content in ads, based on the notion that entertainment induces the emotional gratifications that viewers experience [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>]. To collect this data, facial expression footage from each participant is collected via a webcam. This approach unobstrusively captures expressed feelings without significantly impacting viewer's decisions to stop viewing. The video images serve as input to the emotion analytics software, which works by fitting a virtual facemask to the video image of the face (see Figure <a class="fig" href="#fig2">2</a>). This facemask adjusts to the form of the face (eyes, nose, face, and mouth) so as to capture line segments that form the basis of the facial coding. If a participant smiles or laughs, for example, a few deviations in line segment will increase, such as the one linking both corners of the lips, while others will decrease, such as the one linking corners of the lips to the cheekbone. The output of the system is a probability measure associated with the intensity of an expression. Studies have shown that the algorithm can identify emotions such as joy with accuracy over 90% [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>]. Infrared corneal-reflection eye-tracking technology is used to record the focal positions of the viewers’ right eye, in Cartesian <em>X</em> and <em>Y</em> coordinate. The output pinpoints a viewer's focal attention in a commercial, and determines where someone is looking and for how long.</p>
        <p>To accurately measure ad effectiveness in a realistic experimental setup, participants are allowed to skip past an advertisement, in contrast to previous studies with forced exposure where viewers cannot avoid the ads [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]. The percentage of an ad that a viewer chooses to watch provides a more accurate measure on ad effectiveness in terms of viewer engagement. We also conduct questionaries to obtain self-reported metrics such as the viewer's purchase intent, whether he/she clicks on a web link to find out more about the brand after watching an ad, and ad virality based on action of sharing the ad in the social network context. The combination of both tracking and self-reported methods provide us with a more complete and complementary understanding of audience response to advertising.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Data Processing</h3>
          </div>
        </header>
        <p>As advertisements have different lengths, we vary the sampling rate so as to make them comparable. For each ad, we divide it into <em>K</em> segments with equal time interval, and aggregate the time varying features such as branding activity and emotion tracking data, for each segment. In the work, we have analyzed the data for <em>K</em> = 3 and 10. When <em>K</em> = 3, the dynamic features represent the aggregated values in the beginning, the middle and the end of an ad respectively. When <em>K</em> = 10, for a 30-second ad, we evaluate it at 3-second intervals.</p>
        <p>Our data consists of discrete (e.g., whether a viewer has skipped an ad and whether he has shared the ad with his friends) and continuous features (e.g., joy and surprise intensities), many of which do not exhibit normality. Thus, we focus on constructing a discrete Bayesian network, where both the global and the local distributions are multinomial, and the latter are represented as conditional probability tables. We discretize the continuous variables using Hartemink's method which preserves pairwise dependencies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], as compared to marginal discretization methods such as using quantiles or fixed-length intervals. We then jointly discretize all the variables into a small number of intervals by iteratively collapsing the intervals defined by their quantiles. The process stops when each variable has 3 levels, that is, low, medium and high intensities.</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Knowledge Representation for Ad Designs</h2>
        </div>
      </header>
      <figure id="fig3">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 3:</span> <span class="figure-title">A trained dynamic Bayesian network on ad-viewer response with 137 nodes.</span>
        </div>
      </figure>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Overview on Bayesian Networks</h3>
          </div>
        </header>
        <p>Bayesian networks are a class of graphical models that allow a concise representation of the probabilistic dependencies among a given set of random variables <strong>X</strong> = {<em>X</em> <sub>1</sub>, <em>X</em> <sub>2</sub>, ⋅⋅⋅, <em>X<sub>n</sub></em> } as a directed acyclic graph <em>G</em> with nodes <span class="inline-equation"><span class="tex">$\mathcal {V}$</span></span> and arcs <span class="inline-equation"><span class="tex">$\mathcal {A}$</span></span> . Each node <span class="inline-equation"><span class="tex">$V_i \in \mathcal {V}$</span></span> corresponds to a random variable <em>X<sub>i</sub></em> . The Markov property of Bayesian networks specifies that every random variable <em>X<sub>i</sub></em> directly depends only on its parents <span class="inline-equation"><span class="tex">$\Pi _{X_i}$</span></span> . As a result, the factorization of the joint probability distribution <strong>P<sub>X</sub></strong> can be expressed as the following in case of discrete random variables,</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathbf {P_X}(\mathbf {X}) = \prod _{i=1}^n \mathbf {P}_{X_i}(X_i \mid \Pi _{X_i}). \]</span><br />
          </div>
        </div>
        <p></p>
        <p>Dynamic Bayesian networks (DBNs) extend the fundamental ideas behind static Bayesian networks to model associations arising from the temporal dynamics. Each variable in a DBN is represented by several nodes across time steps. We focus on multivariate time series which are sequences of multivariate random variables measured at successive time steps, and utilize a DBN to describe a discrete-time stochastic process <strong>X</strong>, ={<em>X<sub>i</sub></em> (<em>t</em>); <em>i</em> = 1, ⋅⋅⋅, <em>n</em>; <em>t</em> = 1, ⋅⋅⋅, <em>T</em>}, that takes values in <strong>R</strong> <sup><em>n</em></sup> at every time step <em>t</em>.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> DBN Representation</h3>
          </div>
        </header>
        <p>Various representations have been proposed to handle temporal dependence in DBNs, often with the goal to reduce the complexity introduced by the time dimension. One common approach of modeling DBNs is to consider a uniform structure, where the network contains identical structure at every time step with identical temporal conditional dependencies (arcs) between time steps. Thus, the resulting DBN is built over a static BN, by replicating them for each time slice and adding arcs between states across two consecutive time slices, if they are temporally dependent. Such networks enable easy separation of DBNs in time and efficient inference algorithm when unrolling the network [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>]. Unfortunately, the time-invariant nature of this representation imposes an unrealistic assumption for our application, because there is plenty of empirical evidence which suggests that viewers’ responses are highly dependent on time. For example, the tendency to skip an ad is naturally higher in the middle of the ad than in the beginning or at the very end.</p>
        <p>Another popular approach of modeling DBNs is to represent the temporal dependence relationships by a vector auto-regressive (VAR) process, where the value at a given time, <em>X<sub>i</sub></em> (<em>t</em>), is given as a linear combination of those at earlier time points. Specifically, for a process of order <em>p</em>, the variables observed at any time <em>t</em> ≥ <em>p</em> are assumed to satisfy</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathbf {X}(t) = \mathbf {A}_1\mathbf {X}(t-1) + \mathbf {A}_2\mathbf {X}(t-2) + \cdots \mathbf {A}_p\mathbf {X}(t-p) + \mathbf {B} + \epsilon (t), \]</span><br />
          </div>
        </div>where nonzero coefficients in the matrix <strong>A</strong> <sub><em>k</em></sub> defines the arc set for lag <em>k</em>, i.e., if the element <em>a<sub>ij</sub></em> (<em>i</em> ≠ <em>j</em>) is nonzero, then the network includes an arc from <em>X<sub>i</sub></em> (<em>t</em> − <em>k</em>) to <em>X<sub>j</sub></em> (<em>t</em>). Matrix <strong>B</strong> represents the baseline measurement for each variable and ϵ(<em>t</em>) is a noise vector, with zero mean. However, with this representation, arcs between state variables within a time slice are disallowed. As our data consists of both the ad data and the viewer responses which are highly likely to be dependent at every time step, the VAR representation is inappropriate for our application.
        <p></p>
        <p>In our work, we do not impose limiting network structures a priori. Each variable at every time step <em>X<sub>i</sub></em> (<em>t</em>) is included into the system as an additional random variable. We explicitly encode the temporal relationships by enforcing constraints during structural learning, which state that dynamic nodes (e.g., <em>X<sub>i</sub></em> (<em>t</em>)) could influence all other nodes in current time step as well as those in the later steps (i.e., <em>X<sub>j</sub></em> (<em>t</em> + <em>k</em>) for all <em>j</em> and <em>k</em> ≥ 0). However, the reverse arcs from the future steps to the earlier time steps are strictly disallowed.</p>
        <p>While this method allows a great degree of flexibility in representing the underlying system, one limitation is that the network size and complexity increase as time steps increase. For example, when we increase the time steps (i.e., total segment of each ad) from 3 to 10, the total number of nodes in the DBN rises from 53 to 137.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Learning</h3>
          </div>
        </header>
        <p>We learn the structure of the DBN via hill-climbing with random restarts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], which is a greedy algorithm that explores the search space starting from an empty network and adding, deleting, or reversing one arc at a time until the score can no longer be improved. Besides enforcing time dependence as discussed earlier, prior knowledge on the data is integrated into structural learning to prune the existence of equivalent classes of networks that are indistinguishable from a probabilistic point of view. For instance, we set constraints which state that viewer attention and emotion states cannot influence ads, that is, the arcs from eye tracking data and emotion tracking data to ad branding activity are prohibited.</p>
        <p>Since the result on learning can be noisy, we use nonparametric bootstrap and learn one network from each bootstrap sample. We repeat structure learning for 500 times and the network structures were averaged to reduce the impact of locally optimal but globally suboptimal networks. Arcs are considered significant if they appear in at least 95% of the networks and in the direction that appears most frequently. The procedure of model averaging is known to result in a better predictive performance than choosing a single, high-scoring network [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. Once that the network structure is specified, we fit the parameters of the local distributions, which take the form of conditional probability tables, via posterior Bayesian estimation.</p>
        <p>The resulting network for <em>K</em> = 10 (i.e., each ad consists of 10 equally spaced segments) is shown in Figure 3. The learnt network reveals arcs representing conditional dependence among variables within a same time slice, as well as ones that represent temporal dependence among variables between consecutive time steps and across several time steps. The network consists of with 137 nodes, which are shaded with different colors to indicate the data type. It shows two large clusters of nodes mainly in blue and orange, which represent ad branding activities and viewer tracking data respectively.</p>
        <p>There are several interesting observations revealed by the network. For instance, within the cluster of blue nodes on branding activity, nodes corresponding to viewer eye tracking data (orange nodes labeled IAD, see Table 1 for its definition) are closely linked to logo duration and frequency, suggesting the impact of branding activity on viewers’ attention. However, the ad effectiveness metrics (pink nodes) are directly linked to the viewer tracking data nodes in terms of joy and eyebrow movement, which is in agreement with empirical observations on the use of entertainment content on viewer engagement [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]. Among the viewer tracking data, we observe that most nodes representing surprise are isolated from the rest of the network. The use of surprise in advertisements occurs much less frequently compared to the use of joy. In addition, surprise is a more fleeting emotion than joy. It is plausible that at the current level of data aggregation, the effect of surprise has been washed out and its influence would be more accurately captured when we refine the model by increasing the total segments of ads.</p>
        <p>The learnt network also reveals that static features from self-reported surveys are useful in deciphering how to make effective advertisements. For instance, purchase intent has two incoming arcs from nodes which indicate whether a viewer is familiar with an ad and whether his or her perception of the brand is entertaining. The relationship between purchase intent and ad familiarity is relatively well studied, in the sense that repetitive advertising exposure reinforces brand loyalty and product preference [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]. The latter relationship is less obvious, implying that having an “entertaining” brand image is related to its ads’ attractiveness and persuasiveness for increasing purchases.</p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Recommendations with DBN</h2>
        </div>
      </header>
      <p>Given a Bayesian network with structure <em>G</em> and parameters <em>Θ</em> , the effects of a new piece of evidence <strong>E</strong> on the distribution of <strong>X</strong> using the knowledge encoded in the model, can be investigated by two types of queries, namely, maximum a posteriori and conditional probability queries.</p>
      <p>Maximum a posteriori queries are concerned with finding the configuration <strong>q</strong> <sup>*</sup> that has the highest posterior probability for a given subset of variables <strong>Q</strong>, i.e.,</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \mathbf {q^*} = \arg \max _\mathbf {q} \mathbf {P}(\mathbf {Q} = \mathbf {q} \mid \mathbf {E}, G,\Theta). \]</span><br />
        </div>
      </div>Thus, conditioned on the specified requirement on the ad (e.g., target audience, product category, last 5 seconds must show logo), a human designer can determine the optimal combinations of ad content based on the existing ad data encoded in the DBN.
      <p></p>
      <p>Conditional probability queries focus on the distribution of a subset of variables <strong>Q</strong> given evidence <strong>E</strong> on another set of variables in <strong>X</strong>, which is computed as the posterior probability, <strong>P</strong>(<strong>Q</strong>∣<strong>E</strong>, <em>G</em>, <em>Θ</em>). Conditioned on the specified requirement on the ad as evidence, the samples drawn from the posterior distribution represent “design proposals” consisting of sequences of branding activities and entertainment content. The versatility in the inference enables the system to be “creative”, as some sequences do not exist in the data.</p>
      <figure id="fig4">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class="figure-title">200 sequences of joyful content sampled from the posterior distributions, conditioned on having low engagement (left) and high engagement (right) respectively.</span>
        </div>
      </figure>
      <p></p>
      <p>To illustrate the ad generation process, we condition on one of the viewer engagement metrics, which is measured by the percentage of an ad that a viewer has finished watching. Specifically, conditioned on two scenarios, i.e., having low engagement where viewers skip early in the ad and high engagement, we generate 200 samples from the corresponding posterior distribution on the entertainment content and branding activities. Every sample is a multi-dimensional time series, showing how features evolve with time. Take joyful content as an example, intensities correspond to the beginning, the middle and the end segments of the ads are shown in Figure <a class="fig" href="#fig4">4</a> for both scenarios. By comparing the two sets of trajectories, we observe that the high engagement posterior has generated more “up-and-down” sequences than those with low engagement. This coincides with empirical observation on building an “emotional roller coaster” to improve viewer engagement, by briefly terminating viewers’ positive feelings and then quickly restoring them [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>].</p>
      <p>Once samples are generated from the above process, we can evaluate their effectiveness by applying predictive models on various engagement metrics. For instance, a random utility framework with time-varying parameters which associates an ad's content and viewer information to the likelihood of sharing the ad and purchase intent has been constructed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>]. Figure <a class="fig" href="#fig5">5</a> depicts the distributions of the two metrics for the 200 ad samples generated above. The dashed line represents the median of the respective metrics. It shows that on average, the samples conditioned on high viewer engagement are more likely to lead to sharing and higher purchase intent. We also observe that the improvement in forwarding is more significant than that in purchase intent. The learnt DBN in Figure 3 shows that the node representing purchase intent is directly linked to nodes “Ad Familarity” and “Entertain”. In this experiment, we are only varying joyful content while keeping all other parameters at their median values as observed in the input data. The improvement in purchase intent is likely to be higher if we restrict to a population of viewers who are familiar with the ads and consider the brand entertaining.</p>
      <p>Similarly, we can also predict the likelihood that an viewer will skip an ad. Figure <a class="fig" href="#fig6">6</a> shows the distribution on ad avoidance with respect to different time segments of 200 ad samples. It shows that on average, the zapping probability is lower for samples generated by the high engagement posterior. In particular, we see that the reduction in ad avoidance is relatively small in the beginning of an ad (typically viewers are less likely to skip an ad in the first few seconds regardless of its content) and becomes more prominent in the middle and near the end of an ad. Thus, with such a system, a human designer can generate quantifiably effective advertisements by selecting design proposals with high scoring samples.</p>
      <figure id="fig5">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 5:</span> <span class="figure-title">Predicted effectiveness of ads in terms of likelihood of forwarding and purchase intent, corresponding to samples conditioned on having high and low engagement respectively.</span>
        </div>
      </figure>
      <figure id="fig6">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig6.jpg" class="img-responsive" alt="Figure 6" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 6:</span> <span class="figure-title">Predicted ad avoidance likelihood in the beginning, the middle and the end of ads, conditioned on having high and low engagement.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Ongoing Work and Conclusion</h2>
        </div>
      </header>
      <p>This paper reports our initial work on the cognitive advertisement design system, which focuses on knowledge representation and recommendations. Our ongoing research directions are three folds: 1) construct more refined models so as to offer recommendations at a finer granularity; 2) create ads based on the model recommendations; 3) validate the model via user studies.</p>
      <p>To create ads by applying recommendations from this system, we are currently working with nViso (<a class="link-inline force-break" href="http://www.nviso.ch/">http://www.nviso.ch/</a>), a Swiss company specialized in 3D facial imaging and affective computing measurement analytics, to measure viewers’ emotional responses to a few hundreds of stock footage clips. Figure <a class="fig" href="#fig7">7</a> shows a screenshot of viewer responses for one of the video clips, where various emotions including sadness, surprise, disgust, anger, fear and happiness, are captured on a second-by-second basis. The change in emotion measurements and the trend line representing the overall evolvement for each emotion are shown. For this particular video, it appears to be a highly joyful clip, indicated by the high level of happiness measurements.</p>
      <p>Once all clips have been labelled with the moment-to-moment joy and surprise intensities, we will use them to form the video repository for ad creation. More specifically, for a given ad recommendation, appropriate clips from the repository with specified entertainment content will be selected and then stitched together. Next, logo and product appearances will be inserted into the content according to the recommended branding activities. Finally, music, voiceover as well as transition effects, will be mixed into the ad to make it more aurally and visually pleasing.</p>
      <p>Lastly, we are planning to conduct user studies to assess the effectiveness of ads generated by this system and will utilize viewers’ responses to iteratively enrich the knowledge base and improve the recommendation engine.</p>
      <figure id="fig7">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191549/images/www18companion-288-fig7.jpg" class="img-responsive" alt="Figure 7" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 7:</span> <span class="figure-title">Sample output of audiences’ emotional responses for a stock footage clip</span>
        </div>
      </figure>
      <p></p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">William&nbsp;E Baker, Heather Honea, and Cristel&nbsp;A Russell. 2004. Do not wait to reveal the brand name: The effect of brand-name placement on television advertising effectiveness. <em><em>Journal of Advertising</em></em> 33, 3 (2004), 77–85.</li>
        <li id="BibPLXBIB0002" label="[2]">Kim Binsted and Graeme Ritchie. 1997. Computational rules for generating punning riddles. <em><em>HUMOR-International Journal of Humor Research</em></em> 10, 1 (1997), 25–76.</li>
        <li id="BibPLXBIB0003" label="[3]">Remco&nbsp;Ronaldus Bouckaert. 1995. <em><em>Bayesian belief networks: from construction to inference</em></em> . Universiteit Utrecht, Faculteit Wiskunde en Informatica.</li>
        <li id="BibPLXBIB0004" label="[4]">Hichem Boudali and Joanne&nbsp;Bechta Dugan. 2005. A discrete-time Bayesian network reliability modeling and analysis framework. <em><em>Reliability Engineering &amp; System Safety</em></em> 87, 3 (2005), 337–349.</li>
        <li id="BibPLXBIB0005" label="[5]">Raymond&nbsp;R Burke, Arvind Rangaswamy, Jerry Wind, and Jehoshua Eliashberg. 1990. A knowledge-based system for advertising design. <em><em>Marketing Science</em></em> 9, 3 (1990), 212–229.</li>
        <li id="BibPLXBIB0006" label="[6]">Gerda Claeskens and Nils&nbsp;Lid Hjort. 2008. <em><em>Model selection and model averaging</em></em> . Vol.&nbsp;330. Cambridge University Press Cambridge.</li>
        <li id="BibPLXBIB0007" label="[7]">Simon Colton, Geraint&nbsp;A Wiggins, <em>et al.</em> 2012. Computational creativity: The final frontier?. In <em><em>ECAI</em></em> , Vol.&nbsp;12. 21–26.</li>
        <li id="BibPLXBIB0008" label="[8]">Luis&nbsp;M de Campos, Juan&nbsp;M Fernández-Luna, and Juan&nbsp;F Huete. 2004. Bayesian networks and information retrieval: an introduction to the special issue. <em><em>Information processing &amp; management</em></em> 40, 5 (2004), 727–733.</li>
        <li id="BibPLXBIB0009" label="[9]">Russell&nbsp;H Fazio, Paul&nbsp;M Herr, and Martha&nbsp;C Powell. 1992. On the development and strength of category–brand associations in memory: The case of mystery ads. <em><em>Journal of Consumer Psychology</em></em> 1, 1 (1992), 1–13.</li>
        <li id="BibPLXBIB0010" label="[10]">Jeff Forbes, Timothy Huang, Keiji Kanazawa, and Stuart Russell. 1995. The batmobile: Towards a bayesian automated taxi. In <em><em>IJCAI</em></em> , Vol.&nbsp;95. 1878–1885.</li>
        <li id="BibPLXBIB0011" label="[11]">Nir Friedman, Michal Linial, Iftach Nachman, and Dana Pe'er. 2000. Using Bayesian networks to analyze expression data. <em><em>Journal of computational biology</em></em> 7, 3-4 (2000), 601–620.</li>
        <li id="BibPLXBIB0012" label="[12]">Jacob Goldenberg, David Mazursky, and Sorin Solomon. 1999. The fundamental templates of quality ads. <em><em>Marketing science</em></em> 18, 3 (1999), 333–351.</li>
        <li id="BibPLXBIB0013" label="[13]">Alexander&nbsp;J Hartemink, David&nbsp;K Gifford, Tommi Jaakkola, Richard&nbsp;A Young, <em>et al.</em> 2001. Using graphical models and genomic expression data to statistically validate models of genetic regulatory networks.. In <em><em>Pacific symposium on biocomputing</em></em> , Vol.&nbsp;6. World Scientific, 266.</li>
        <li id="BibPLXBIB0014" label="[14]">X. Hua, L. Lu, and H. Zhang. 2004. Automatic music video generation based on temporal pattern analysis. <em><em>ACM Multimedia</em></em> (2004), 472–475.</li>
        <li id="BibPLXBIB0015" label="[15]">Michael Isard. 2003. PAMPAS: Real-valued graphical models for computer vision. In <em><em>Proceedings of Computer Vision and Pattern Recognition. IEEE Computer Society Conference</em></em> , Vol.&nbsp;1. IEEE, I–613.</li>
        <li id="BibPLXBIB0016" label="[16]">Xia Jiang, Richard&nbsp;E Neapolitan, M&nbsp;Michael Barmada, and Shyam Visweswaran. 2011. Learning genetic epistasis using Bayesian network scoring criteria. <em><em>BMC bioinformatics</em></em> 12, 1 (2011), 89.</li>
        <li id="BibPLXBIB0017" label="[17]">Anna Kantosalo, Jukka&nbsp;M Toivanen, Ping Xiao, and Hannu Toivonen. 2014. From Isolation to Involvement: Adapting Machine Creativity Software to Support Human-Computer Co-Creation.. In <em><em>ICCC</em></em> . 1–7.</li>
        <li id="BibPLXBIB0018" label="[18]">Christos&nbsp;L Koumenides and Nigel&nbsp;R Shadbolt. 2012. Combining link and content-based information in a Bayesian inference model for entity search. In <em><em>Proceedings of the 1st Joint International Workshop on Entity-Oriented and Semantic Search</em></em> . ACM, 3.</li>
        <li id="BibPLXBIB0019" label="[19]">Antonios Liapis, Georgios&nbsp;N Yannakakis, and Julian Togelius. 2014. Computational Game Creativity.. In <em><em>ICCC</em></em> . Citeseer, 46–53.</li>
        <li id="BibPLXBIB0020" label="[20]">Daniel McDuff, Rana El&nbsp;Kaliouby, Thibaud Senechal, Mohammed Amr, Jeffrey&nbsp;F Cohn, and Rosalind Picard. 2013. Affectiva-MIT Facial Expression Dataset (AM-FED): Naturalistic and Spontaneous Facial Expressions Collected” In-the-Wild”. In <em><em>Proceedings of Computer Vision and Pattern Recognition Workshops (CVPRW)</em></em> . IEEE, 881–888.</li>
        <li id="BibPLXBIB0021" label="[21]">Daniel McDuff, RE Kaliouby, and Rosalind&nbsp;W Picard. 2012. Crowdsourcing facial responses to online videos. <em><em>Affective Computing, IEEE Transactions on</em></em> 3, 4 (2012), 456–468.</li>
        <li id="BibPLXBIB0022" label="[22]">Kristine&nbsp;P Monteith. 2012. <em>Automatic Generation of Music for Inducing Emotive and Physiological Responses</em>. Ph.D. Dissertation. Brigham Young University.</li>
        <li id="BibPLXBIB0023" label="[23]">Vladimir Pavlović, James&nbsp;M Rehg, Tat-Jen Cham, and Kevin&nbsp;P Murphy. 1999. A dynamic bayesian network approach to figure tracking using learned dynamic models. In <em><em>Proceedings of the Seventh IEEE International Conference</em></em> , Vol.&nbsp;1. IEEE, 94–101.</li>
        <li id="BibPLXBIB0024" label="[24]">Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. (1988).</li>
        <li id="BibPLXBIB0025" label="[25]">Federico Peinado and Pablo Gervás. 2006. Evaluation of automatic generation of basic stories. <em><em>New Generation Computing</em></em> 24, 3 (2006), 289–302.</li>
        <li id="BibPLXBIB0026" label="[26]">Thales Teixeira. 2012. The new science of viral ads. <em><em>Harvard Business Review</em></em> 90, 3 (2012), 25–27.</li>
        <li id="BibPLXBIB0027" label="[27]">Thales Teixeira, Rosalind Picard, and Rana el Kaliouby. 2014. Why, when, and how much to entertain consumers in advertisements? A web-based facial tracking field study. <em><em>Marketing Science</em></em> 33, 6 (2014), 809–827.</li>
        <li id="BibPLXBIB0028" label="[28]">Thales Teixeira, Michel Wedel, and Rik Pieters. 2010. Moment-to-moment optimal branding in TV commercials: Preventing avoidance by pulsing. <em><em>Marketing Science</em></em> 29, 5 (2010), 783–804.</li>
        <li id="BibPLXBIB0029" label="[29]">Thales Teixeira, Michel Wedel, and Rik Pieters. 2012. Emotion-induced engagement in internet video advertisements. <em><em>Journal of Marketing Research</em></em> 49, 2 (2012), 144–159.</li>
        <li id="BibPLXBIB0030" label="[30]">Gerard&nbsp;J Tellis. 1988. Advertising exposure, loyalty, and brand purchase: A two-stage model of choice. <em><em>Journal of marketing research</em></em> (1988), 134–144.</li>
        <li id="BibPLXBIB0031" label="[31]">Paolo Trucco, Enrico Cagno, Fabrizio Ruggeri, and Ottavio Grande. 2008. A Bayesian Belief Network modelling of organisational factors in risk analysis: A case study in maritime transportation. <em><em>Reliability Engineering &amp; System Safety</em></em> 93, 6 (2008), 845–856.</li>
        <li id="BibPLXBIB0032" label="[32]">Lav&nbsp;R Varshney, Florian Pinel, Kush&nbsp;R Varshney, Angela Schörgendorfer, and Yi-Min Chee. 2013. Cognition as a part of computational creativity. In <em><em>Cognitive Informatics &amp; Cognitive Computing (ICCI* CC), 2013 12th IEEE International Conference on</em></em> . IEEE, 36–43.</li>
        <li id="BibPLXBIB0033" label="[33]">J. Wang, C. Xu, E. Chng, L. Duan, K. Wan, and Q. Tian. 2005. Automatic generation of personalized music sports video. <em><em>ACM Multimedia</em></em> (2005), 735–744.</li>
        <li id="BibPLXBIB0034" label="[34]">Geraint Wiggins, George Papadopoulos, Somnuk Phon-Amnuaisuk, and Andrew Tuson. 1998. Evolutionary methods for musical composition. <em><em>Dai Research Paper</em></em> (1998).</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>http://adage.com/article/news/tv-ad-pricing-chart/310429/</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191549">https://doi.org/10.1145/3184558.3191549</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
