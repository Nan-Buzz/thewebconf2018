<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Fully Dynamic k-Center Clustering&#x204E;&#x204E;This research was partially supported by a grant from the PROCORE France-Hong Kong Joint Research Scheme sponsored by the Research Grants Council of Hong Kong and the Consulate General of France in Hong Kong under the project F-HKU702/16.</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3186124'>https://doi.org/10.1145/3178876.3186124</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186124'>https://w3id.org/oa/10.1145/3178876.3186124</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Fully Dynamic <em>k</em>-Center Clustering<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>      </span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">T-H. Hubert</span>      <span class="surName">Chan</span>,     University of Hong Kong Hong Kong, <a href="mailto:hubert@cs.hku.hk">hubert@cs.hku.hk</a>     </div>     <div class="author">     <span class="givenName">Arnaud</span>      <span class="surName">Guerquin</span>,     LTCI, T&#x00E9;l&#x00E9;com ParisTech University Paris, France, <a href="mailto:arnaud.guerquin@ens-paris-saclay.fr">arnaud.guerquin@ens-paris-saclay.fr</a>     </div>     <div class="author">     <span class="givenName">Mauro</span>      <span class="surName">Sozio</span>,     LTCI, T&#x00E9;l&#x00E9;com ParisTech University Paris, France, <a href="mailto:sozio@telecom-paristech.fr">sozio@telecom-paristech.fr</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186124" target="_blank">https://doi.org/10.1145/3178876.3186124</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Static and dynamic clustering algorithms are a fundamental tool in any machine learning library. Most of the efforts in developing dynamic machine learning and data mining algorithms have been focusing on the sliding window model (where at any given point in time only the most recent data items are retained) or more simplistic models. However, in many real-world applications one might need to deal with arbitrary deletions and insertions. For example, one might need to remove data items that are not necessarily the oldest ones, because they have been flagged as containing inappropriate content or due to privacy concerns. Clustering trajectory data might also require to deal with more general update operations.</small>     </p>     <p>     <small>We develop a (2 + &#x03F5;)-approximation algorithm for the <em>k</em>-center clustering problem with &#x201C;small&#x201D; amortized cost under the fully dynamic adversarial model. In such a model, points can be added or removed arbitrarily, provided that the adversary does not have access to the random choices of our algorithm. The amortized cost of our algorithm is poly-logarithmic when the ratio between the maximum and minimum distance between any two points in input is bounded by a polynomial, while <em>k</em> and &#x03F5; are constant. Our theoretical results are complemented with an extensive experimental evaluation on dynamic data from Twitter, Flickr, as well as trajectory data, demonstrating the effectiveness of our approach.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       T-H. Hubert Chan, Arnaud Guerquin, and Mauro Sozio. 2018. Fully Dynamic <em>k</em>-Center Clustering. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 9 Pages. <a href="https://doi.org/10.1145/3178876.3186124" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186124</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>With over 6000 tweets per second being posted on Twitter, Google processing over 40000 queries every second, and more than 400 hours worth of youtube videos uploaded every minute, there is an urgent need to develop dynamic machine learning and data mining algorithms. Most of the efforts in this direction have been focusing on the sliding window model of computation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>], where at any given point in time only the most recent data items are retained, or more simplistic models.</p>    <p>However, in many real-world applications one might need to deal with arbitrary deletions and insertions. For example, one might need to remove data items that are not necessarily the oldest ones, because they have been flagged as containing inappropriate content or due to privacy concerns. The latter case has increasingly become commonplace, due to the &#x2018;right to be forgotten&#x2019; principle. Clustering trajectory data might also require to deal with more general update operations than those modeled by the sliding window model.</p>    <p>Clustering algorithms provide a fundamental tool in any machine learning library. There have been increasing efforts in recent years to study clustering problems from both a theoretical and practical point of view&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>].</p>    <p>In our work, we consider a fully dynamic adversarial model, where points can be added or removed arbitrarily, provided that the adversary does not have access to the random choices of our algorithm. Moreover, our algorithm does not know the update operations in advance. We focus on the <em>k</em>-center clustering problem with a long-term goal of studying other machine learning and data mining problems in a fully dynamic environment. We develop a (2 + &#x03F5;)-approximation algorithm for the <em>k</em>-center clustering problem, which requires &#x201C;small&#x201D; expected amortized cost under the fully dynamic adversarial model. In particular, the expected amortized cost is poly-logarithmic when the ratio between the maximum and minimum distance between any two points in input is bounded by a polynomial, while <em>k</em> and &#x03F5; are constant. We also prove that the running time of our algorithm is concentrated around its expectation with high probability.</p>    <p>Our theoretical results are complemented with an extensive experimental evaluation on dynamic data from Twitter, Flickr, as well as trajectory data, demonstrating the effectiveness of our approach. We also evaluate our algorithm against the approach proposed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] for the same problem, under the sliding window model. Our experimental evaluation shows that our algorithm delivers clustering solutions with lower maximum radius than&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>], although this comes at the price of a slightly worse average running time and more space. Another advantage of our algorithm is that it is efficient under a fully adversarial model, in contrast with&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] which works under the sliding window model.</p>    <p>The rest of the paper is organized as follows. We discuss the related work in Section <a class="sec" href="#sec-5">2</a>, while Section&#x00A0;<a class="sec" href="#sec-6">3</a> introduce the necessary definitions and notations. In Section <a class="sec" href="#sec-7">4</a> we present our main algorithm, while its theoretical analysis is provided in Section&#x00A0;<a class="sec" href="#sec-12">5</a>. Section&#x00A0;<a class="sec" href="#sec-15">6</a> contains an experimental evaluation of our algorithm against the state-of-the-art approaches on real-world data. Finally, Section&#x00A0;<a class="sec" href="#sec-22">7</a> contains our conclusion and future work.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>There have been increasingly more efforts in studying clustering algorithms from both a practical and theoretical point of view, in recent years&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. One of the first dynamic clustering algorithms has been presented in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], where the authors developed an 8-approximation algorithm for the case with only insertions. A (2 + &#x03F5;)-approximation algorithm was later developed by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] building on the results of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. In the same work, the authors also studied the case with outliers (where a limited number of points can be deleted from the input) for which they developed a (4 + &#x03F5;)-approximation algorithm.</p>    <p>The literature for clustering in the streaming model is rich. We mention the work in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], where the authors give the first single-pass constant approximation algorithm for <em>k</em>-median which has been improved later on in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>].</p>    <p>The work that is most relevant to ours is&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>], where the authors studied the <em>k</em>-center clustering problem under the sliding window model. They developed a (6 + &#x03F5;)-approximation algorithm requiring <span class="inline-equation"><span class="tex">$O(k \cdot \frac{\log \delta }{\epsilon })$</span>     </span> time per update (on average), where <em>&#x03B4;</em> is an upper bound on the ratio between the maximum and minimum distance between any two points in input. The algorithm requires <span class="inline-equation"><span class="tex">$O(k \cdot \frac{\log \delta }{\epsilon })$</span>     </span> space. They studied their algorithm mainly from a theoretical point of view. One of the contributions of our work is an experimental evaluation of the algorithm in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] on real-world data.</p>    <p>Observe that our algorithm requires <span class="inline-equation"><span class="tex">$O(N \cdot \frac{\log (\delta)}{\epsilon })$</span>     </span> space where <em>N</em> is an upper bound on the maximum number of points occurring at any point in time, while the algorithm proposed&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] requires <span class="inline-equation"><span class="tex">$O(k\cdot \frac{\log (\delta)}{\epsilon })$</span>     </span> space. However, in our case cluster membership can be tested in <em>O</em>(1) time, while all points in a same cluster <em>C</em> of a given point can be produced in time <em>O</em>(|<em>C</em>|). This is an appealing property when monitoring sensor data or trajectories evolving over time. Another advantage of our algorithm is its approximation guarantee which is a factor of 2 + &#x03F5;. Observe that any approximation ratio less than 2 would imply <em>P</em> = <em>NP</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. Moreover, as proved in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] any algorithm with an approximation ratio of less than 4 requires <span class="inline-equation"><span class="tex">$\Omega (N^\frac{1}{3})$</span>     </span> space. Table&#x00A0;<a class="tbl" href="#tab1">1</a> summarizes the two algorithms in terms of approximation guarantee, average running time, space requirement and query time.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Summary of our algorithm (FD) and the one proposed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>] (<SmallCap>SW</SmallCap>).</span>     </div>     <table class="table">     <thead>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">approximation</td>       <td style="text-align:center;">avg. run. time</td>       <td style="text-align:center;">space</td>       <td style="text-align:center;">model</td>       <td style="text-align:center;">        <em>x</em> &#x2208; <em>C</em>?</td>       <td style="text-align:center;">list all <em>y</em> s.t. <em>x</em>, <em>y</em> &#x2208; <em>C</em>       </td>      </tr>     </thead>      <tbody>      <tr>       <td style="text-align:center;">FD</td>       <td style="text-align:center;">2(1 + &#x03F5;)</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$k^2\cdot \frac{\log (\delta)}{\epsilon }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$ N \cdot \frac{\log (\delta)}{\epsilon }$</span>        </span>       </td>       <td style="text-align:center;">Fully Dynamic</td>       <td style="text-align:center;">        <em>O</em>(1)</td>       <td style="text-align:center;">|<em>C</em>|</td>      </tr>      <tr>       <td style="text-align:center;">        <SmallCap>SW</SmallCap>       </td>       <td style="text-align:center;">6(1 + &#x03F5;)</td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$k\cdot \frac{\log (\delta)}{\epsilon }$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$k\cdot \frac{\log (\delta)}{\epsilon }$</span>        </span>       </td>       <td style="text-align:center;">Sliding Window</td>       <td style="text-align:center;">        <em>O</em>(<em>k</em>)</td>       <td style="text-align:center;">        <em>k</em> &#x00B7; <em>N</em>       </td>      </tr>     </tbody>     </table>    </div>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Notation and Definitions</h2>     </div>    </header>    <p>We study the <em>k</em>-center clustering problem, which is formally defined as follows.</p>    <p>     <div class="definition" id="enc1">     <Label>Definition 3.1 (k-Center Clustering).</Label>     <p> We are given a set <em>S</em> of points equipped with some metric&#x00A0;<em>d</em> and an integer <em>k</em> > 0. We wish to find a set <em>C</em> = {<em>c</em>      <sub>1</sub>, &#x2026;, <em>c<sub>k</sub>      </em>} of <em>k</em> points (centers) so as to minimize the quantity max&#x2009;<sub>       <em>x</em> &#x2208; <em>S</em>      </sub>      <em>d</em>(<em>x</em>, <em>C</em>), where <em>d</em>(<em>x</em>, <em>C</em>) = min&#x2009;<sub>       <em>c</em> &#x2208; <em>C</em>      </sub>      <em>d</em>(<em>x</em>, <em>c</em>).</p>     </div>    </p>    <p>Observe that a set of centers defines a partition of <em>S</em> (clustering) into <em>k</em> sets. We consider the following adversarial model of computation.</p>    <p>     <strong>Adversarial Model.</strong> We assume the adversary first fixes a (possibly countably infinite) sequence <em>O</em> of operations, where for each <span class="inline-equation"><span class="tex">$t \in \mathbb {N}$</span>     </span>, the operation <em>o<sub>t</sub>     </em> &#x2208; <em>X</em> &#x00D7; { &#x2212;, +} consists of a point <em>x<sub>t</sub>     </em> &#x2208; <em>X</em> in the metric space and a flag to indicate whether it is an insertion&#x00A0;(+) or deletion&#x00A0;(&#x2212;). By naturally extending the metric space to <span class="inline-equation"><span class="tex">$X \times \mathbb {N}$</span>     </span>, we can assume without loss of generality that at most one copy of a point is inserted in the sequence. We also assume that any point to be removed has been inserted earlier on. The algorithm does not know the sequence <em>O</em> in advance. However, we assume that any randomness used by the algorithm is generated after the adversary fixes the sequence. We refer to this adversarial model as the <em>fully dynamic model</em>.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Algorithm</h2>     </div>    </header>    <p>To illustrate the main ideas of our algorithm, we start describing a simple (2 + &#x03F5;)-approximation for <em>k</em>-center, &#x03F5; > 0. Let <em>&#x03B2;</em> be a guess for the value of an optimum solution. We pick one point <em>c</em>     <sub>1</sub> arbitrarily from <em>X</em> and we create a cluster <em>C</em>     <sub>1</sub> containing <em>c</em>     <sub>1</sub> as well as all points in <em>X</em>     <sub>1</sub> being within distance 2<em>&#x03B2;</em> from <em>c</em>     <sub>1</sub>. The <em>i</em>th cluster, 1 < <em>i</em> &#x2264; <em>k</em> is built as follows. If <span class="inline-equation"><span class="tex">$X \setminus \cup _{j=1}^{i-1} C_j$</span>     </span> is empty we let <em>C<sub>i</sub>     </em> be the empty set and we stop. Otherwise, we pick one point <em>x<sub>i</sub>     </em> from <span class="inline-equation"><span class="tex">$X \setminus \cup _{j=1}^{i-1} C_j$</span>     </span>, arbitrarily. We then create a new cluster <em>C<sub>i</sub>     </em> with <em>x<sub>i</sub>     </em> as center and containing all the points in <span class="inline-equation"><span class="tex">$X \setminus \cup _{i=1}^{t-1} C_j$</span>     </span> within distance 2<em>&#x03B2;</em> from <em>x<sub>i</sub>     </em>.</p>    <p>It can be shown that if <em>&#x03B2;</em> is equal to the value of an optimum solution, then such an algorithm gives a 2-approximation. This follows from the fact that if <em>k</em> clusters have been formed and <span class="inline-equation"><span class="tex">$X \setminus \cup _{j=1}^{k} C_j$</span>     </span> is not empty, then there are <em>k</em> + 1 points whose pairwise distance is greater than 2<em>&#x03B2;</em>, which implies that <em>k</em> clusters with radius at most <em>&#x03B2;</em> cannot be formed. This would contradict our assumption on <em>&#x03B2;</em>. If the value of an optimum solution is not known, we would run the previous algorithm for any <em>&#x03B2;</em> in <span class="inline-equation"><span class="tex">$\Gamma =\lbrace (1+\epsilon)^i: d_{\min } \le (1+\epsilon)^i \le (1+\epsilon) \cdot d_{\max }, i \in \mathbb {N} \rbrace$</span>     </span>, where <em>d</em>     <sub>max&#x2009;</sub> and <em>d</em>     <sub>min&#x2009;</sub> denote the max and min distance between any two points in <em>X</em>, respectively. Observe that if <em>&#x03B2;</em> is too small, we might not be able to cluster all points, which might result in a set of unclustered points <em>U</em>. The smallest <em>&#x03B2;</em> which allows to partition all points in <em>X</em> (i.e. <span class="inline-equation"><span class="tex">$\cup _{j=1}^k C_j=X$</span>     </span>) gives a (2 + &#x03F5;)-approximation.</p>    <p>A simple (but inefficient) incremental algorithm can be derived from the previous algorithm as follows. At any point in time, for each <em>&#x03B2;</em> in <em>&#x0393;</em>, we maintain the following invariant. We either maintain <em>k</em> + 1 points (<em>k</em> of which are centers) whose pairwise distance is greater than 2<em>&#x03B2;</em> or a clustering of all the points with maximum radius 2<em>&#x03B2;</em>. The former property guarantees that there cannot be any clustering with maximum radius <em>&#x03B2;</em>. For each <em>&#x03B2;</em> in <em>&#x0393;</em>, we wish to maintain the same set of clusters that would be computed using the algorithm discussed above. Whenever a new point <em>x</em> is inserted, we proceed as follows. Let <em>&#x03B2;</em> in <em>&#x0393;</em> and let <em>C</em>     <sub>1</sub>, &#x2026;, <em>C<sub>k</sub>     </em>, <em>U</em> be the corresponding clusters with centers <em>c</em>     <sub>1</sub>, &#x2026;, <em>c<sub>k</sub>     </em>, respectively. We insert <em>x</em> in cluster <em>C<sub>i</sub>     </em> if it is within distance 2<em>&#x03B2;</em> from <em>c<sub>i</sub>     </em>. Otherwise, if there is no such a center, we insert <em>x</em> in <em>U</em>. This is repeated for each <em>&#x03B2;</em> in <em>&#x0393;</em>, which ensures that the invariant is maintained. Such an algorithm gives a (2 + &#x03F5;)-approximation with amortized cost <span class="inline-equation"><span class="tex">$O(k \cdot \frac{1}{\epsilon } \cdot \log \frac{d_{\max }}{d_{\min }})$</span>     </span> and <span class="inline-equation"><span class="tex">$O(|X| \cdot \frac{1}{\epsilon } \cdot \log \frac{d_{\max }}{d_{\min }})$</span>     </span> space.</p>    <p>Now, suppose that points are deleted uniformly at random. If none of the <em>k</em> centers are deleted, the invariant is not violated. Otherwise, we might have to re-cluster all the points in order to maintain the invariant. However, the probability to remove any such a point is <span class="inline-equation"><span class="tex">$\frac{k}{n}$</span>     </span> while the cost of reclustering is at most <em>k</em> &#x00B7; <em>n</em>, where <em>n</em> is number of points. Therefore, the expected amortized cost of such a randomized algorithm is <span class="inline-equation"><span class="tex">$O(k^2 \cdot \frac{1}{\epsilon } \cdot \log \frac{d_{\max }}{d_{\min }})$</span>     </span>. Unfortunately, such an algorithm might not be efficient if deletions are not random.</p>    <p>To overcome this problem we equip the algorithm with some randomness, so that it would be efficient even in the case with adversarial deletions. For each <em>&#x03B2;</em> in <em>&#x0393;</em>, we create and maintain a set of clusters <em>C</em>     <sub>1</sub>, &#x2026;, <em>C<sub>k</sub>     </em> as follows. Let <em>X</em> be the current set of points. The first center <em>c</em>     <sub>1</sub> is chosen uniformly at random from <em>X</em> and a cluster <em>C</em>     <sub>1</sub> is built, as in the previous algorithm. For 1 < <em>i</em> &#x2264; <em>k</em>, <em>c<sub>i</sub>     </em> is chosen uniformly at random from <span class="inline-equation"><span class="tex">$X \setminus \cup _{j=1}^{i-1} C_j$</span>     </span>. When a center <em>c<sub>i</sub>     </em> is deleted, we re-cluster only the points in <span class="inline-equation"><span class="tex">$A=\cup _{j=i}^k C_j \cup U$</span>     </span>, which requires at most <em>k</em> &#x00B7; |<em>A</em>| operations. Observe that the probability that <em>c<sub>i</sub>     </em> is selected as center is at most <span class="inline-equation"><span class="tex">$\frac{1}{|A|}$</span>     </span>, as each of the points in <em>A</em> were possible candidates when <em>c<sub>i</sub>     </em> was selected. In other words, the probability of selecting <em>c<sub>i</sub>     </em> as center is inversely proportional to the cost of handling the deletion of <em>c<sub>i</sub>     </em> (times <em>k</em>). Let <em>o</em>     <sub>1</sub>, &#x2026;, <em>o<sub>m</sub>     </em> be a set of update operations which are fixed by the adversary before the execution of the algorithm, where <em>o<sub>i</sub>     </em> = (<em>x</em>, +) if <em>x</em> is added at step <em>i</em> or <em>o<sub>i</sub>     </em> = (<em>x</em>, &#x2212;) if <em>x</em> is removed. Consider the operation <em>o<sub>i</sub>     </em> = (<em>x</em>, &#x2212;). The algorithm maintains the following invariant: the probability of <em>x</em> being a center, when <em>o<sub>i</sub>     </em> is executed, is inversely proportional to the cost of handling the deletion of <em>c<sub>i</sub>     </em>. This suggests that the expected amortized cost is somehow limited. The analysis of the expected amortized cost of the algorithm is non-trivial, in that, deletions and insertions might intertwine arbitrarily. More efforts are needed to show that the amortized cost is concentrated around its expected value. A theoretical analysis of the algorithm is presented in Section&#x00A0;<a class="sec" href="#sec-12">5</a>. A formal description of the algorithm together with its pseudocode is given in Sections&#x00A0;<a class="sec" href="#sec-8">4.1</a>-<a class="sec" href="#sec-11">4.4</a>. In particular, Algorithms1 and&#x00A0;2 show the pseudocode of the procedures handling deletions.</p>    <p>Observe that the variant where we re-cluster all the points whenever a center is deleted might not be efficient. In particular, the adversary might be able to force a given point to become center and then repeatedly remove and insert back such a point. Each such an update operation would then incur in a total cost of <em>k</em> &#x00B7; <em>n</em>, where <em>n</em> is the current number of points.</p>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Data Structure for Dynamic Clustering</h3>     </div>     </header>     <p>We shall denote with <em>d</em>     <sub>min&#x2009;</sub> and <em>d</em>     <sub>max&#x2009;</sub> the minimum and maximum distance between any two points that are ever inserted, respectively. We allow the set of update operations to be countably infinite, however, we assume that <em>d</em>     <sub>min&#x2009;</sub> and <em>d</em>     <sub>max&#x2009;</sub> always provide a lower or upper bound on the minimum and maximum distance between any two points, respectively.</p>     <p>We start describing the algorithm assuming that <em>d</em>     <sub>min&#x2009;</sub> and <em>d</em>     <sub>max&#x2009;</sub> are known in advance, while discussing the more general case in Section&#x00A0;<a class="sec" href="#sec-11">4.4</a>. Let <span class="inline-equation"><span class="tex">$\Gamma := \lbrace (1+\epsilon)^i: d_{\min } \le (1+\epsilon)^i \le d_{\max }, i \in \mathbb {N} \rbrace$</span>     </span>, and denote <span class="inline-equation"><span class="tex">$\gamma := |\Gamma | = O(\frac{1}{\epsilon } \cdot \log \frac{d_{max}}{d_{\min }})$</span>     </span>. For each <em>&#x03B2;</em> &#x2208; <em>&#x0393;</em>, with respect to the current set&#x00A0;<em>X</em> of points, we maintain a data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span> consisting of the following components and invariants:</p>     <ul class="list-no-style">     <li id="list1" label="&#x2022;">A list <em>S<sub>&#x03B2;</sub>      </em> = {<em>c</em>      <sub>1</sub>, <em>c</em>      <sub>2</sub>, &#x2026;, <em>c<sub>&#x03BA;</sub>      </em>} of <em>&#x03BA;</em> &#x2264; <em>k</em> centers such that for all <em>x</em> &#x2260; <em>y</em> &#x2208; <em>S<sub>&#x03B2;</sub>      </em>, <em>d</em>(<em>x</em>, <em>y</em>) > 2<em>&#x03B2;</em>.<br/></li>     <li id="list2" label="&#x2022;">A collection <span class="inline-equation"><span class="tex">$\mathcal {C}_\beta = \lbrace C_1, C_2, \ldots , C_\kappa \rbrace$</span>      </span> of disjoint clusters such that for all&#x00A0;<em>i</em> &#x2208; [<em>&#x03BA;</em>], for all <em>x</em> &#x2208; <em>C<sub>i</sub>      </em>, <em>d</em>(<em>x</em>, <em>c<sub>i</sub>      </em>) &#x2264; 2<em>&#x03B2;</em>.<br/></li>     <li id="list3" label="&#x2022;">A set <em>U<sub>&#x03B2;</sub>      </em> = <em>X</em>&#x2216;(&#x222A;<sub>       <em>i</em> &#x2208; [<em>&#x03BA;</em>]</sub>      <em>C<sub>i</sub>      </em>) of unclustered points such that for all <em>i</em> &#x2208; [<em>&#x03BA;</em>], for all <em>x</em> &#x2208; <em>U<sub>&#x03B2;</sub>      </em>, <em>d</em>(<em>c<sub>i</sub>      </em>, <em>x</em>) > 2<em>&#x03B2;</em>. Moreover, we require that <em>U</em> &#x2260; &#x2205; implies that <em>&#x03BA;</em> = <em>k</em>.<br/></li>     </ul>     <p>Observe that we can store the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span> using <em>O</em>(|<em>X</em>|) space such that for any <em>x</em> &#x2208; <em>X</em>, it takes <em>O</em>(1) time to return the cluster containing&#x00A0;<em>x</em> and decide whether <em>x</em> is one of the centers in <em>S<sub>&#x03B2;</sub>     </em>.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Arbitrary Insertions</h3>     </div>     </header>     <p>Inserting a new point&#x00A0;<em>x</em> is straightforward. For each <em>&#x03B2;</em> &#x2208; <em>&#x0393;</em> consider the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta = (S_\beta , \mathcal {C}_\beta , U_\beta)$</span>     </span>, where <span class="inline-equation"><span class="tex">$\kappa = |S_\beta |= |\mathcal {C}_\beta |$</span>     </span>. First check whether there is <em>c<sub>i</sub>     </em> &#x2208; <em>S<sub>&#x03B2;</sub>     </em> such that <em>d</em>(<em>x</em>, <em>c<sub>i</sub>     </em>) &#x2264; 2<em>&#x03B2;</em>. If this is the case insert <em>x</em> into the cluster <em>C<sub>i</sub>     </em>. If no such <em>c<sub>i</sub>     </em> is found and <em>&#x03BA;</em> < <em>k</em>, then set <em>c</em>     <sub>      <em>&#x03BA;</em> + 1</sub> &#x2254; <em>x</em> and <em>C</em>     <sub>      <em>&#x03BA;</em> + 1</sub> &#x2254; {<em>x</em>}. Otherwise, if there is no such <em>c<sub>i</sub>     </em> and <em>&#x03BA;</em> = <em>k</em> insert <em>x</em> into <em>U<sub>&#x03B2;</sub>     </em>.</p>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Arbitrary Deletions</h3>     </div>     </header>     <p>Maintaining <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span> to support deletion of some point&#x00A0;<em>x</em> is slightly trickier. The easy case is when <em>x</em>&#x2209;<em>S<sub>&#x03B2;</sub>     </em> is not one of the centers, and so the point&#x00A0;<em>x</em> can simply be removed from its cluster in <em>O</em>(1) time. However, when <em>x</em> = <em>c<sub>i</sub>     </em> for some&#x00A0;<em>i</em>, then we need to rebuild the data structure for the remaining points in (&#x222A;<sub>      <em>j</em> &#x2265; <em>i</em>     </sub>     <em>C<sub>j</sub>     </em>)&#x222A;<em>U<sub>&#x03B2;</sub>     </em>.</p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-img1.svg" class="img-responsive" alt="" longdesc=""/>     </p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-img2.svg" class="img-responsive" alt="" longdesc=""/>     </p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> All Pieces Together and Practical Aspects</h3>     </div>     </header>     <p>The first step of the algorithm consists of initializing the <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span>&#x2019;s, for each <em>&#x03B2;</em> in <em>&#x0393;</em>, so that <span class="inline-equation"><span class="tex">$S_\beta =\mathcal {C}_{\beta }=U_{\beta } \leftarrow \emptyset$</span>     </span>. Then, the algorithm waits for an update operation <em>o</em>. If <em>o</em> = (<em>x</em>, +) then the insertion procedure is executed, otherwise the deletion procedure depicted in Algorithm&#x00A0;1 and Algorithm&#x00A0;2 are executed. Observe, that the invariants discussed in Section&#x00A0;<a class="sec" href="#sec-8">4.1</a> are always maintained. Therefore, a (2 + &#x03F5;)-approximation for the <em>k</em>-center clustering problem is given by the clustering <em>C<sub>&#x03B2;</sub>     </em> with <em>&#x03B2;</em> &#x2208; <em>&#x0393;</em> being smallest such that <em>U<sub>&#x03B2;</sub>     </em> is the empty set.</p>     <p>For ease of presentation, we made the assumption that <em>d</em>     <sub>min&#x2009;</sub> and <em>d</em>     <sub>max&#x2009;</sub> are known in advance. In practice, this might not always be the case. If they are not known, one could compute <em>d</em>     <sub>min&#x2009;</sub> and <em>d</em>     <sub>max&#x2009;</sub> for the first <em>k</em> inserted points, while updating them throughout the execution of the algorithm. Observe that whenever <em>d</em>     <sub>min&#x2009;</sub> or <em>d</em>     <sub>max&#x2009;</sub> change, one would need to compute <em>S<sub>&#x03B2;</sub>     </em>, <em>C<sub>&#x03B2;</sub>     </em>, and <em>U<sub>&#x03B2;</sub>     </em> for each <em>&#x03B2;</em> in <em>&#x0393;</em> that is missing. Removing such an assumption makes the algorithm more efficient in practice, without affecting the theoretical guarantees on the expected amortized cost.</p>     <p>We shall refer to our algorithm as <SmallCap>FullyDynClust</SmallCap>. In the next section, we shall prove strong theoretical guarantees on the expected amortized cost of our algorithm, while showing that its amortized cost is close to its expected value with high probability.</p>    </section>   </section>   <section id="sec-12">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Amortized Analysis</h2>     </div>    </header>    <p>We analyze the expected amortized cost of the deletion operation for the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span>. In Section&#x00A0;<a class="sec" href="#sec-13">5.1</a>, we perform a warmup analysis for the case in which a deletion operation removes a random point uniformly at random. In Section&#x00A0;<a class="sec" href="#sec-14">5.2</a>, we extend the analysis to the case of arbitrary insertions and deletions.</p>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Warmup: Random Deletions</h3>     </div>     </header>     <p>Observe that a deletion is costly only when one of at most <em>k</em> centers is removed. Hence, we readily have the following lemma.</p>     <div class="lemma" id="enc2">     <Label>Lemma 5.1.</Label>     <p> For the removal of a uniformly random point, the Delete operation in Algorithm&#x00A0;1 has expected cost&#x00A0;<em>O</em>(<em>k</em>      <sup>2</sup>).</p>     </div>     <div class="proof" id="proof1">     <Label>Proof.</Label>     <p> Observe that the cost is <em>O</em>(<em>nk</em>) when a center in <em>S<sub>&#x03B2;</sub>      </em> is removed, where <em>n</em> = |<em>X</em>| is the number of current points stored in the data structure; otherwise, the cost is <em>O</em>(1). Since the probability that a center is removed is at most <span class="inline-equation"><span class="tex">$\frac{k}{n}$</span>      </span>, it follows that the expected cost is <em>O</em>(<em>k</em>      <sup>2</sup>).</p>     </div>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Arbitrary Insertions and Deletions</h3>     </div>     </header>     <p>For <span class="inline-equation"><span class="tex">$t \in \mathbb {N}$</span>     </span>, we use the superscript <em>t</em> to indicate the state of the data structure at the end of the <em>t</em>-th step. For instance, we use <em>X<sup>t</sup>     </em> to denote the set of points that are currently stored in the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span> after step <em>t</em>, and we let <em>n<sup>t</sup>     </em> &#x2254; |<em>X<sup>t</sup>     </em>|.</p>     <p>     <strong>Intuition of Charging Scheme.</strong> Each Insert operation on the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>     </span> has cost <em>O</em>(<em>k</em>) for every <em>&#x03B2;</em> in <em>&#x0393;</em>. However, in order to pay for the cost of the Delete operations, we shall charge an extra cost of <em>k</em>     <sup>2</sup> for each Insert operation that will be stored as a credit for future Delete operations. When a Delete operation is called, its cost will be paid for with (i) some of the credits stored (denoted by <em>F<sup>t</sup>     </em>), and (ii) possibly some extra cost (denoted by <em>Z<sup>t</sup>     </em>). Observe that a Delete operation is expensive only if one of the centers in <em>S<sub>&#x03B2;</sub>     </em> is removed. The following formal description defines <em>F<sup>t</sup>     </em> and <em>Z<sup>t</sup>     </em> explicitly.</p>     <p>     <strong>Formal Description of Charging Scheme.</strong> We describe our charging scheme in details as follows. Suppose at step&#x00A0;<em>t</em>, we have either of the following operations:</p>     <ul class="list-no-style">     <li id="list4" label="&#x2022;">Insert<span class="inline-equation"><span class="tex">$(\mathcal {L}_\beta , x)$</span>      </span>. A credit of <em>k</em>      <sup>2</sup> is stored at the point&#x00A0;<em>x</em> that is inserted. In this case, define <em>Z<sup>t</sup>      </em> = <em>F<sup>t</sup>      </em> &#x2254; 0.<br/></li>     <li id="list5" label="&#x2022;">Delete<span class="inline-equation"><span class="tex">$(\mathcal {L}_\beta ,x)$</span>      </span>. If the point&#x00A0;<em>x</em> to be deleted is not one of the centers, then the cost is <em>O</em>(1), and we set <em>Z<sup>t</sup>      </em> = <em>F<sup>t</sup>      </em> &#x2254; 0. Otherwise, suppose the point&#x00A0;<em>x</em> to be deleted is the center <em>c<sub>i</sub>      </em> out of the current <em>&#x03BA;</em> centers. Then, in this case, the rebuild cost is <span class="inline-equation"><span class="tex">$O(k \cdot |\widehat{X}|)$</span>      </span>, where <span class="inline-equation"><span class="tex">$\widehat{X} := ((\cup _{j \ge i} C_j) \cup U_\beta) \setminus \lbrace x\rbrace$</span>      </span> are the points that need to be reclustered. Our charging scheme pays a cost of <em>k</em> for each point <span class="inline-equation"><span class="tex">$u \in \widehat{X}$</span>      </span>. Specifically, we decompose the reclustering cost <span class="inline-equation"><span class="tex">$k \cdot |\widehat{X}| = F^t + Z^t$</span>      </span> in the following way, and we will analyze <em>F<sup>t</sup>      </em> and <em>Z<sup>t</sup>      </em> separately.<br/>      <ul class="list-no-style">       <li id="uid17" label="(a)">Denote <span class="inline-equation"><span class="tex">$X_{\sf new} := \lbrace u \in \widehat{X}:$</span>        </span> when <em>c<sub>i</sub>        </em> was chosen as the center, the point <em>u</em> is not yet inserted}. Define <span class="inline-equation"><span class="tex">$F^t := k \cdot |X_{\sf new}|$</span>        </span>. In this case, for each <span class="inline-equation"><span class="tex">$u \in X_{\sf new}$</span>        </span>, we will use <em>k</em> of the credits stored at&#x00A0;<em>u</em> to pay for the cost; hence, this cost will not contribute towards <em>Z<sup>t</sup>        </em>. We shall prove in Lemma&#x00A0;<a class="enc" href="#enc3">5.2</a> that we will always have enough credits stored at the points in <span class="inline-equation"><span class="tex">$X_{\sf new}$</span>        </span> to pay this way.<br/></li>       <li id="uid18" label="(b)">Denote <span class="inline-equation"><span class="tex">$X_{\sf old} := \lbrace u \in \widehat{X}:$</span>        </span> when <em>c<sub>i</sub>        </em> was chosen as the center, the point <em>u</em> is already inserted}. Define <span class="inline-equation"><span class="tex">$Z^t := k \cdot |X_{\sf old}|$</span>        </span>. Hence, for each such point <span class="inline-equation"><span class="tex">$u \in X_{\sf old}$</span>        </span>, we will incur a cost <em>k</em> that contributes towards <em>Z<sup>t</sup>        </em>.<br/>Observe that such a point <em>u</em> can be reclustered many times after its insertion. Apart from the initial times that can be paid in case 2(a), for each subsequent reclustering in some step&#x00A0;<em>&#x03C4;</em>, its reclustering cost will be counted towards the corresponding <em>Z<sup>&#x03C4;</sup>        </em>.<br/></li>      </ul></li>     </ul>     <div class="lemma" id="enc3">     <Label>Lemma 5.2 (Credits for Reclustering New Points).</Label>     <p> Fix <span class="inline-equation"><span class="tex">$t \in \mathbb {N}$</span>      </span>. Suppose <em>a<sub>t</sub>      </em> is the number of insertions from the beginning up to (and including) step&#x00A0;<em>t</em>. Then, with probability 1, we have <span class="inline-equation"><span class="tex">$\sum _{\tau = 1}^t F^\tau \le a_t \cdot k^2$</span>      </span>.</p>     </div>     <div class="proof" id="proof2">     <Label>Proof.</Label>     <p> Observe that <em>a<sub>t</sub>      </em> &#x00B7; <em>k</em>      <sup>2</sup> is the total number of credits received by the points inserted up to step&#x00A0;<em>t</em>. In order to show the required inequality, it suffices to show that the <em>k</em>      <sup>2</sup> credits stored at each inserted point&#x00A0;<em>u</em> will be enough to pay for its reclustering cost under case&#x00A0;2(a) in the charging scheme.</p>     <p>Suppose at the moment when <em>u</em> was inserted, the centers were <span class="inline-equation"><span class="tex">$\lbrace c^{\prime }_1, c^{\prime }_2, \ldots , c^{\prime }_k\rbrace$</span>      </span> and the clusters were <span class="inline-equation"><span class="tex">$\lbrace C^{\prime }_1, \ldots , C^{\prime }_k\rbrace$</span>      </span> together with the unclustered set <em>U</em>&#x2032;. Suppose <em>r</em> &#x2208; [<em>k</em>] is the largest index such that <span class="inline-equation"><span class="tex">$u \in (\cup _{j \ge r} C^{\prime }_j) \cup U^{\prime }$</span>      </span>.</p>     <p>Then, the credits stored at&#x00A0;<em>u</em> will be consumed only when the points in <span class="inline-equation"><span class="tex">$W := \lbrace c^{\prime }_1, \ldots , c^{\prime }_r\rbrace$</span>      </span> are removed in some subsequent steps. Since there are at most <em>k</em> such centers in <em>W</em>, and <em>k</em> credits are consumed from <em>u</em> whenever such a center in <em>W</em> is removed, we can conclude that the <em>k</em>      <sup>2</sup> credits stored at&#x00A0;<em>u</em> will be enough. This completes the proof of the lemma.</p>     </div>     <p>For each <span class="inline-equation"><span class="tex">$t \in \mathbb {N}$</span>     </span>, we also write <span class="inline-equation"><span class="tex">$Z^t := \sum _{i = 1}^k Z^t_i$</span>     </span>, where <span class="inline-equation"><span class="tex">$Z^t_i$</span>     </span> is the extra cost incurred when the center <em>c<sub>i</sub>     </em> is deleted (at the beginning of step&#x00A0;<em>t</em>). Observe that for a fixed <em>t</em>, there is at most one <em>i</em> &#x2208; [<em>k</em>] such that <span class="inline-equation"><span class="tex">$Z^t_i$</span>     </span> is non-zero.</p>     <div class="lemma" id="enc4">     <Label>Lemma 5.3 (Bounding Expectation).</Label>     <p> For each <span class="inline-equation"><span class="tex">$t \in \mathbb {N}$</span>      </span>, <span class="inline-equation"><span class="tex">$\mathbb {E}[Z^t] \le k^2$</span>      </span>.</p>     </div>     <div class="proof" id="proof3">     <Label>Proof.</Label>     <p> Recall that <span class="inline-equation"><span class="tex">$Z^t := \sum _{i = 1}^k Z^t_i$</span>      </span>, where <span class="inline-equation"><span class="tex">$Z^t_i$</span>      </span> is the extra cost incurred when center <em>c<sub>i</sub>      </em> is deleted. Hence, it suffices to prove that <span class="inline-equation"><span class="tex">$E[Z^t_i] \le k$</span>      </span>. Observe that <em>c<sub>i</sub>      </em> is a random object.</p>     <p>Observe that the center <em>c<sub>i</sub>      </em> was chosen in line&#x00A0;6 of some invocation of Algorithm&#x00A0;2 . We use <em>U<sub>i</sub>      </em> to denote the set&#x00A0;<em>U</em> in line&#x00A0;6 at the moment when <em>c<sub>i</sub>      </em> was chosen. Again, observe that <em>U<sub>i</sub>      </em> is a random object.</p>     <p>We next analyze <span class="inline-equation"><span class="tex">$\mathbb {E}[Z^t_i|U_i]$</span>      </span> using the randomness used in line&#x00A0;6 . Observe that when the point to be deleted is&#x00A0;<em>c<sub>i</sub>      </em> only points in <em>U<sub>i</sub>      </em> contribute towards <span class="inline-equation"><span class="tex">$Z^t_i$</span>      </span>. However, some points in <em>U<sub>i</sub>      </em> could have been deleted by time&#x00A0;<em>t</em>. Let <span class="inline-equation"><span class="tex">$\widehat{U} \subseteq U_i$</span>      </span> denote the points that still remain at the beginning of step&#x00A0;<em>t</em>.</p>     <p>Therefore, <span class="inline-equation"><span class="tex">$Z^t_i$</span>      </span> is non-zero only if the point <span class="inline-equation"><span class="tex">$x_t \in \widehat{U}$</span>      </span> to be deleted at step <em>t</em> was chosen as the center in line&#x00A0;6, which happens with probability at most <span class="inline-equation"><span class="tex">$\frac{1}{|\widehat{U}|}$</span>      </span> (conditioning on <em>U<sub>i</sub>      </em>).</p>     <p>Therefore, it follows that <span class="inline-equation"><span class="tex">$\mathbb {E}[Z^t_i|U_i] \le \frac{1}{|\widehat{U}|} \cdot k \cdot |\widehat{U}| = k$</span>      </span>.</p>     <p>At this point, we would like to remind the reader that <em>Z<sup>t</sup>      </em> only accounts for the reclustering cost in case&#x00A0;2(b) of the description. For points that are inserted after <em>c<sub>i</sub>      </em> was chosen as the center, their reclustering costs are paid using the credits stored in themselves as in case 2(a) of the formal description of the charging scheme.</p>     <p>Taking expectation of the random variable <span class="inline-equation"><span class="tex">$\mathbb {E}[Z^t_i|U_i]$</span>      </span> gives <span class="inline-equation"><span class="tex">$\mathbb {E}[Z^t_i] \le k$</span>      </span>, as required.</p>     </div>     <div class="theorem" id="enc5">     <Label>Theorem 5.4.</Label>     <p> For any &#x03F5; > 0, for any sequence of <span class="inline-equation"><span class="tex">$T \in \mathbb {N}$</span>      </span> insert/delete operations, the <SmallCap>FullyDynClust</SmallCap> algorithm maintains a (2 + &#x03F5;)-approximation solution for the <em>k</em>-center problem, while requiring <span class="inline-equation"><span class="tex">$O(\frac{\log \delta }{\epsilon } \cdot k^2 T)$</span>      </span> expected time and <span class="inline-equation"><span class="tex">$O(\frac{\log \delta }{\epsilon } \cdot |N|)$</span>      </span> space under the fully dynamic model, where <em>N</em> &#x2254; max&#x2009;<sub>       <em>t</em> &#x2208; [<em>T</em>]</sub>|<em>X<sup>t</sup>      </em>|. Clustering membership can be tested in <em>O</em>(1) time, while producing in output all points in the cluster <em>C</em> of a given point requires <em>O</em>(<em>k</em> + |<em>C</em>|) time.</p>     </div>     <div class="proof" id="proof4">     <Label>Proof.</Label>     <p> The algorithm always maintains all the invariants discussed in Section&#x00A0;<a class="sec" href="#sec-8">4.1</a>. Therefore, the clustering <em>C<sub>&#x03B2;</sub>      </em> where <em>&#x03B2;</em> is smallest such that <em>U<sub>&#x03B2;</sub>      </em> is the empty set, gives a (2 + &#x03F5;)-approximation. For any given <em>&#x03B2;</em> in <em>&#x0393;</em>, the total cost due to deletion operations in <span class="inline-equation"><span class="tex">$\mathcal {L}_{\beta }$</span>      </span> is <span class="inline-equation"><span class="tex">$\sum _{t = 1}^T O(F^t + X^t)$</span>      </span>. Lemmas&#x00A0;<a class="enc" href="#enc3">5.2</a> and&#x00A0;<a class="enc" href="#enc4">5.3</a> imply that its expectation is at most <em>O</em>(<em>k</em>      <sup>2</sup>      <em>T</em>). We conclude the proof by recalling that there are <span class="inline-equation"><span class="tex">$|\Gamma |=O(\frac{\log \delta }{\epsilon })$</span>      </span> values of <em>&#x03B2;</em>.</p>     </div>     <p>     <strong>High Probability Statements.</strong> Lemma&#x00A0;<a class="enc" href="#enc4">5.3</a> implies that any sequence of <em>T</em> insert/delete operations on a data structure has expected cost <em>O</em>(<em>k</em>     <sup>2</sup>     <em>T</em>). We next show that with high probability, the cost is also <em>O</em>(<em>k</em>     <sup>2</sup>     <em>T</em>). As seen in the proof of Lemma&#x00A0;<a class="enc" href="#enc4">5.3</a>, our analysis is based on the randomness used in line&#x00A0;6of Algorithm&#x00A0;2 .</p>     <p>Suppose <em>N</em> is an upper bound on the number of points stored by the data structure at any time. For <em>n</em> &#x2264; <em>N</em>, consider the random variable <em>&#x03B6;<sub>n</sub>     </em> that takes value <em>n</em> with probability <span class="inline-equation"><span class="tex">$\frac{1}{n}$</span>     </span> and value 0 with probability <span class="inline-equation"><span class="tex">$1 - \frac{1}{n}$</span>     </span>. As in the proof of the well-known Chernoff bound, we analyze the moment generating function of <em>&#x03B6;<sub>n</sub>     </em> to prove measure concentration results.</p>     <div class="lemma" id="enc6">     <Label>Lemma 5.5 (Moment Generating Function of &#x03B6;).</Label>     <p> For <em>n</em> &#x2264; <em>N</em> and <span class="inline-equation"><span class="tex">$0 \le \theta \le \frac{1}{N}$</span>      </span>, <span class="inline-equation"><span class="tex">$\mathbb {E}[e^{\theta \zeta _n}] \le e^{\theta + \frac{3}{4} \cdot \theta ^2 N}$</span>      </span>.</p>     </div>     <div class="proof" id="proof5">     <Label>Proof.</Label>     <p> For <span class="inline-equation"><span class="tex">$0 \le \theta \le \frac{1}{N}$</span>      </span>, we have <div class="table-responsive" id="eq1">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} \mathbb {E}[e^{\theta \zeta _n}] &#x0026; = &#x0026; (1 - \frac{1}{n}) + \frac{1}{n} \cdot e^{\theta n} \end{eqnarray} </span>        <br/>        <span class="equation-number">(1)</span>       </div>      </div>      <div class="table-responsive" id="eq2">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} &#x0026; \le &#x0026; \exp (\frac{e^{\theta n} - 1}{n}) \end{eqnarray} </span>        <br/>        <span class="equation-number">(2)</span>       </div>      </div>      <div class="table-responsive" id="eq3">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} &#x0026; \le &#x0026; e^{\theta + \frac{3}{4} \cdot \theta ^2 n} \end{eqnarray} </span>        <br/>        <span class="equation-number">(3)</span>       </div>      </div>      <div class="table-responsive" id="eq4">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} &#x0026; \le &#x0026; e^{\theta + \frac{3}{4} \cdot \theta ^2 N}, \end{eqnarray} </span>        <br/>        <span class="equation-number">(4)</span>       </div>      </div> where (<a class="eqn" href="#eq2">2</a>) comes from 1 + <em>x</em> &#x2264; <em>e<sup>x</sup>      </em>, and (<a class="eqn" href="#eq3">3</a>) comes from the inequality <span class="inline-equation"><span class="tex">$e^x \le 1 + x + \frac{3}{4} \cdot x^2$</span>      </span> for 0 &#x2264; <em>x</em> &#x2264; 1.</p>     </div>     <div class="lemma" id="enc7">     <Label>Lemma 5.6.</Label>     <p> Suppose <em>N</em> is an upper bound on the number of points stored by the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>      </span> at any time, and fix any <em>i</em> &#x2208; [<em>k</em>]. For any <span class="inline-equation"><span class="tex">$0 \le \theta \le \frac{1}{k N}$</span>      </span> and <span class="inline-equation"><span class="tex">$T \in \mathbb {N}$</span>      </span>, we have <span class="inline-equation"><span class="tex">$\mathbb {E}[e^{\theta \sum _{t \in [T]}Z^t_i}] \le e^{T(\theta k + \frac{3}{4} \cdot \theta ^2 k^2 N)}$</span>      </span>.</p>     </div>     <div class="proof" id="proof6">     <Label>Proof.</Label>     <p> We prove the statement by induction on <em>T</em>. The base case <em>T</em> = 0 holds trivially. We next consider step&#x00A0;<em>T</em> + 1. Let <span class="inline-equation"><span class="tex">$\mathcal {F}$</span>      </span> be the sigma-algebra generated by the random variables&#x00A0;<span class="inline-equation"><span class="tex">$(Z^t_i: t \in [T])$</span>      </span> together with the indicator variables <span class="inline-equation"><span class="tex">$(I^t_j: j \in [k], t \in [T])$</span>      </span>, where <span class="inline-equation"><span class="tex">$I^t_j = 1$</span>      </span>      <em>iff</em> a new center <em>c<sub>j</sub>      </em> is picked in step&#x00A0;<em>t</em>.</p>     <p>We next define a random object <em>U</em>      <sub>       <em>T</em> + 1</sub>. At the beginning of step&#x00A0;<em>T</em> + 1, consider the center <em>c<sub>i</sub>      </em> and the moment when it was picked in some previous step. Observe that <em>c<sub>i</sub>      </em> was picked in line&#x00A0;6 of some invocation of Algorithm&#x00A0;2 . Define <em>U</em>      <sub>       <em>T</em> + 1</sub> to be the set&#x00A0;<em>U</em> in line&#x00A0;6 at that moment. Observe that some points in <em>U</em>      <sub>       <em>T</em> + 1</sub> could be deleted from the time <em>c<sub>i</sub>      </em> was picked as the center to the beginning of step&#x00A0;<em>T</em> + 1. Denote <span class="inline-equation"><span class="tex">$\widehat{U} \subseteq U_{T+1}$</span>      </span> as the points that still remain at the beginning of step&#x00A0;<em>T</em> + 1.</p>     <p>Similar to the proof of Lemma&#x00A0;<a class="enc" href="#enc4">5.3</a>, we show that conditioning on the history (<span class="inline-equation"><span class="tex">$\mathcal {F}$</span>      </span> and <em>U</em>      <sub>       <em>T</em> + 1</sub>), the random variable <span class="inline-equation"><span class="tex">$Z^{T+1}_i$</span>      </span> is stochastically dominated<a class="fn" href="#fn3" id="foot-fn3"><sup>1</sup></a> by <span class="inline-equation"><span class="tex">$k \cdot \zeta _{|\widehat{U}|}$</span>      </span>; note that this trivially holds if the (<em>T</em> + 1)-st operation is an insertion, because <span class="inline-equation"><span class="tex">$Z^{T+1}_i$</span>      </span> equals 0 in this case.</p>     <p>We next argue why this is also true when the (<em>T</em> + 1)-st operation is a deletion. Observe that conditioning on the history <span class="inline-equation"><span class="tex">$\mathcal {F}$</span>      </span> and <em>U</em>      <sub>       <em>T</em> + 1</sub>, we know exactly in which step the current center&#x00A0;<em>c<sub>i</sub>      </em> was picked, namely, the largest <em>i</em> &#x2264; <em>t</em> such that <span class="inline-equation"><span class="tex">$I^t_i = 1$</span>      </span>. Therefore, conditioning on <span class="inline-equation"><span class="tex">$\mathcal {F}$</span>      </span> and <em>U</em>      <sub>       <em>T</em> + 1</sub>, the current center&#x00A0;<em>c<sub>i</sub>      </em> is a uniformly random point in <span class="inline-equation"><span class="tex">$\widehat{U}$</span>      </span>. Therefore, it follows that the center&#x00A0;<em>c<sub>i</sub>      </em> is deleted in step&#x00A0;<em>T</em> + 1 with probability at most <span class="inline-equation"><span class="tex">$\frac{1}{\widehat{U}}$</span>      </span>, which incurs a cost of <span class="inline-equation"><span class="tex">$k \cdot |\widehat{U}|$</span>      </span> when this event happens. Hence, conditioning on <span class="inline-equation"><span class="tex">$\mathcal {F}$</span>      </span> and <em>U</em>      <sub>       <em>T</em> + 1</sub>, <span class="inline-equation"><span class="tex">$Z^{T+1}_i$</span>      </span> is stochastically dominated by <span class="inline-equation"><span class="tex">$k \cdot \zeta _{|\widehat{U}|}$</span>      </span>, as required. Hence, from Lemma&#x00A0;<a class="enc" href="#enc6">5.5</a>, we have for <span class="inline-equation"><span class="tex">$0 \le \theta \le \frac{1}{N}$</span>      </span>, <span class="inline-equation"><span class="tex">$\mathbb {E}[e^{\theta Z^{T+1}_i} | \mathcal {F}, U_{T+1}] \le e^{\theta k + \frac{3}{4} \cdot \theta ^2 k^2 N}$</span>      </span>.</p>     <p>Finally, the inductive step is completed by considering the following conditional expectation: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray*} \mathbb {E}[e^{\theta \sum _{t \in [T+1]}Z^t_i} | \mathcal {F}, U_{T+1}] &#x0026; = &#x0026; e^{\theta \sum _{t \in [T]} Z^t_i} \cdot \mathbb {E}[e^{\theta Z^{T+1}_i} | \mathcal {F}, U_{T+1}] \nonumber \\ &#x0026; \le &#x0026; e^{\theta \sum _{t \in [T]} Z^t_i} \cdot e^{\theta k + \frac{3}{4} \cdot \theta ^2 k^2 N}. \nonumber\end{eqnarray*} </span>        <br/>       </div>      </div> Then, taking expectation and using the induction hypothesis completes the induction proof.</p>     </div>     <div class="lemma" id="enc8">     <Label>Lemma 5.7.</Label>     <p> Suppose <em>N</em> is an upper bound on the number of points stored by the data structure <span class="inline-equation"><span class="tex">$\mathcal {L}_\beta$</span>      </span> at any time, and fix any <em>i</em> &#x2208; [<em>k</em>]. For any <em>&#x03BB;</em> &#x2265; 2, <span class="inline-equation"><span class="tex">$\Pr [\sum _{t \in [T]}Z^t_i \ge \lambda k T] \le e^{- \Omega (\frac{\lambda T}{N})}$</span>      </span>.</p>     </div>     <div class="proof" id="proof7">     <Label>Proof.</Label>     <p> Using the standard method of moment generating function as in the proof of the Chernoff bound, we choose <span class="inline-equation"><span class="tex">$\theta = \frac{1}{kN}$</span>      </span> in the following: <div class="table-responsive" id="eq5">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} \Pr [\sum _{t \in [T]}Z^t_i \ge \lambda k T] &#x0026; = &#x0026; \Pr [e^{\theta \sum _{t \in [T]}Z^t_i} \ge e^{\theta \lambda k T}] \end{eqnarray} </span>        <br/>        <span class="equation-number">(5)</span>       </div>      </div>      <div class="table-responsive" id="eq6">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} &#x0026; \le &#x0026; \mathbb {E}[e^{\theta \sum _{t \in [T]}Z^t_i} ] \cdot e^{- \theta \lambda k T} \end{eqnarray} </span>        <br/>        <span class="equation-number">(6)</span>       </div>      </div>      <div class="table-responsive" id="eq7">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} &#x0026; \le &#x0026; e^{- \frac{(\lambda - \frac{7}{4})T}{N}}, \end{eqnarray} </span>        <br/>        <span class="equation-number">(7)</span>       </div>      </div>     </p>     <p>where (<a class="eqn" href="#eq6">6</a>) follows from Markov&#x0027;s inequality and (<a class="eqn" href="#eq7">7</a>) follows from Lemma&#x00A0;<a class="enc" href="#enc7">5.6</a> and the choice of <span class="inline-equation"><span class="tex">$\theta = \frac{1}{kN}$</span>      </span>.</p>     </div>     <p>The following theorem follows from Theorem&#x00A0;<a class="enc" href="#enc5">5.4</a> and Lemma&#x00A0;<a class="enc" href="#enc8">5.7</a>.</p>     <div class="theorem" id="enc9">     <Label>Theorem 5.8.</Label>     <p> Let <em>N</em> be an upper bound on the number of points at any point in time (|<em>X<sup>t</sup>      </em>| &#x2264; <em>N</em>, for all <em>t</em>). For any &#x03F5; > 0, for any sequence of <span class="inline-equation"><span class="tex">$T \in \mathbb {N}$</span>      </span> insert/delete operations the <SmallCap>FullyDynClust</SmallCap> algorithm maintains a (2 + &#x03F5;)-approximation solution for the <em>k</em>-center problem.</p>     <p>Moreover, the probability that the total time exceeds <span class="inline-equation"><span class="tex">$\Omega (\frac{\log \delta }{\epsilon } \cdot \lambda k^2 T)$</span>      </span> is at most <span class="inline-equation"><span class="tex">$\frac{\log \delta }{\epsilon } \cdot k e^{- \Omega (\frac{\lambda T}{N})}$</span>      </span>, for any <em>&#x03BB;</em> &#x2265; 2, under the fully dynamic model. The algorithm requires <span class="inline-equation"><span class="tex">$O(\frac{\log \delta }{\epsilon } \cdot N)$</span>      </span> space.</p>     </div>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Experiments</h2>     </div>    </header>    <p>All our experiments have been carried on a machine equipped with two Intel(R) Xeon(R) CPU E5-2660 v3 running at 2.60GHz and with 250Go of DDR3 SDRAM. The Twitter dataset we used as well as our implementation in C of our algorithm have been made publicly available&#x00A0;<a class="fn" href="#fn4" id="foot-fn4"><sup>2</sup></a>. Observe that for each tweet we retain only its timestamp and its GPS coordinates, due to privacy concerns. We evaluate the algorithms under a wide range of values for <em>k</em>, &#x03F5; and the length of the sliding window <em>W</em>. In all experiments the default values are <em>k</em> = 20 and &#x03F5; = 0.1, unless otherwise specified. For our randomized algorithm, each result is the average among ten runs, unless otherwise specified.</p>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Datasets</h3>     </div>     </header>     <p>We consider some datasets that are publicly available, as well as a dataset that we collected from Twitter.</p>     <p>     <strong>Twitter.</strong> We collected geotagged tweets by means of the Twitter API. We were able to collect 21M tweets between 9/09/2017 and 20/10/2017. Each tweet is associated with GPS coordinates, such as longitude and latitude, as well as, a timestamp.</p>     <p>     <strong>Flickr.</strong> The Yahoo Flickr Creative Commons 100 Million (YFCC100m) dataset&#x00A0;<a class="fn" href="#fn5" id="foot-fn5"><sup>3</sup></a> is a dataset containing the metadata of 100 Million picture posted on Flickr under the creative common licence between 2011 and 2015. Each picture is associated with GPS coordinate and a timestamp. We used the metadata of 47M pictures that possessed both a valid timestamp and GPS coordinate.</p>     <p>     <strong>Trajectories.</strong> This dataset contains trajectories performed by all the 442 taxis running in the city of Porto, in Portugal&#x00A0;<a class="fn" href="#fn6" id="foot-fn6"><sup>4</sup></a>. Each trajectory consists of a set of two-dimensional points, each one being associated with a timestamp.Trajectories are updated every 15 seconds, with most of the trajectories consisting of at most 500 points. The dataset contains 83M two-dimensional points, in total leading to 83M updates.</p>     <p>Table&#x00A0;<a class="tbl" href="#tab2">2</a> summarizes the statistics of the datasets considered in our experimental evaluation.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Dataset statistics.</span>     </div>     <table class="table">     <thead>       <tr>        <td style="text-align:center;">Name</td>        <td style="text-align:center;">Source</td>        <td style="text-align:center;">Type</td>        <td style="text-align:center;">#Updates</td>       </tr>     </thead>       <tbody>       <tr>        <td style="text-align:center;">Twitter</td>        <td style="text-align:center;">twitter.com</td>        <td style="text-align:center;">2D points</td>        <td style="text-align:center;">42M</td>       </tr>       <tr>        <td style="text-align:center;">Flickr</td>        <td style="text-align:center;">yfcc100m.appspot.com</td>        <td style="text-align:center;">2D points</td>        <td style="text-align:center;">96M</td>       </tr>       <tr>        <td style="text-align:center;">Porto taxi trajectories</td>        <td style="text-align:center;">archive.ics.uci.edu</td>        <td style="text-align:center;">Trajectories</td>        <td style="text-align:center;">83M</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Distance Measures</h3>     </div>     </header>     <p>Depending on the dataset at hand, we consider different distance measures. In the case of Twitter and Flickr, we compute the great circle distance between two GPS coordinates&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. For the trajectories we use the symmetric Hausdorff distance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>], which is defined as follows. Let <em>P</em> and <em>Q</em> be two sets of points in the Euclidean space. <span class="inline-equation"><span class="tex">$H(P,Q) :=\max \limits _{p\in P} \min \limits _{q \in Q}d(p,q)$</span>     </span> where <em>d</em> is the Euclidean distance. The symmetric Hausdorff distance between two trajectories <em>P</em> and <em>Q</em> is defined as <span class="inline-equation"><span class="tex">$\widehat{H}(P,Q)=\max (H(P,Q),H(Q,P))$</span>     </span>. All the distance measures considered in our experiments are metrics.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Dynamic Model of Computation</h3>     </div>     </header>     <p>In the case of Twitter and Filckr, we evaluated the algorithms under the sliding window model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. In this case, if a point is inserted at time <em>t</em>, it will be removed at time <em>t</em> + <em>W</em>, where <em>W</em> is the duration of the sliding window. As a result, at each point in time <em>t</em>, new points might be added to the current dataset in which case any point with timestamp <em>t</em> + <em>W</em> will be removed (if any).</p>     <p>We then focus on the task of maintaining a clustering of all trajectories available up to any given point. At any point in time, either a new point is added to one of the available trajectories at that time or a new trajectory is created. Since trajectories might be updated in any arbitrary order, the sliding window model turns out to be over-simplistic in this setting. Therefore the approach in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] cannot be used.</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.4</span> Flickr and Twitter</h3>     </div>     </header>     <p>We evaluate the algorithms under a wide range of values for <em>k</em>, &#x03F5; and the length of the sliding window <em>W</em>. In all experiments the default values are <em>k</em> = 20, <em>eps</em> = 0.1. For Twitter, the length of the sliding window is two hours, while for Flickr it is six hours. Both those values result in a sliding window containing &#x00A0;60K points.</p>     <p>We start by evaluating the accuracy of the two algorithms. In particular, we measure the ratio between the maximum radius of the clustering produced by each algorithm and a lower bound on the optimum radius. At any point in time, we compute such a lower bound as follows. Let <em>&#x03B2;</em> be largest such that the set <em>U<sub>&#x03B2;</sub>     </em> computed by our algorithm is not the empty set. Our lower bound is computed as half the minimum pairwise distance between the <em>k</em> + 1 points (cluster centers) in <em>S<sub>&#x03B2;</sub>     </em>. We observe that in practice it is important to have small maximum radius, in that, this might lead to more &#x201C;meaningful&#x201D; clusters.</p>     <p>Figure&#x00A0;<a class="fig" href="#fig1">1</a> (left) measures the approximation ratio as a function of <em>k</em>, while Figure&#x00A0;<a class="fig" href="#fig1">1</a> (right) measures the approximation ratio as a function of &#x03F5; in Twitter. For any given value of <em>k</em> and &#x03F5; we report the median, the first and third quartiles, as well as maximum and minimum approximation ratio obtained by the algorithm when executed on the whole dataset. The two algorithms have been tested for the same value of <em>k</em> and &#x03F5;, however, some additional space between the corresponding values in the plot has been introduced to improve the presentation.</p>     <p>We can observe that our algorithm (FD) consistently delivers better approximation ratio then the algorithm presented in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] (<SmallCap>SW</SmallCap>). In particular, we can observe that while the approximation ratio of our algorithm is always close to 2 when &#x03F5; = 0.1, it can be as large as 6 for <SmallCap>SW</SmallCap> with its median being as large as 4.5. This is expected, as our algorithm comes with stronger guarantees on the maximum radius. We can also see that on average FD performs better than its worst-case analysis suggests.</p>     <p>This is even more apparent in Flickr (Figure&#x00A0;<a class="fig" href="#fig3">3</a>), where our algorithm produces results being closer to an optimum solution. In particular the median for the case when <em>k</em> = 100 and &#x03F5; = 0.1 ((Figure&#x00A0;<a class="fig" href="#fig3">3</a>) (left)) is close to 1.5, while in a few cases a near-optimal solution is computed. We can also observe that the maximum radius for <span class="inline-equation"><span class="tex">$\rm\small {SW}$</span>     </span> drops as <em>k</em> increases, while it improves again when <em>k</em> is large enough (Figure&#x00A0;<a class="fig" href="#fig3">3</a>) (left)). This might be due to the way the deletion of a center is handled in <span class="inline-equation"><span class="tex">$\rm\small{SW}$</span>     </span>. If a center is removed, the cluster is not reconstructed as in our case but a new point (called <em>orphan</em>) might be elected as representative of the cluster. Although this might be more efficient in practice, it might affect the quality of the results, in that, orphans might not be as good representatives of the clusters as the original centers. As a result, points might be added to the &#x201C;wrong&#x201D; clusters. As <em>k</em> increases, it becomes more likely that there is at least one &#x201C;bad&#x201D; orphan, which explains the drop in quality of the results. For even larger values of <em>k</em>, the task of clustering becomes &#x201C;easier&#x201D; with the easiest case being when <em>k</em> approaches the total number of points.</p>     <p>Figure&#x00A0;<a class="fig" href="#fig1">1</a> (right) and Figure&#x00A0;<a class="fig" href="#fig3">3</a> (right) show the approximation ratio as a function of &#x03F5; for Twitter and Flickr, respectively. Those figures confirm that our algorithm produces better results in practice. Moreover, we can see that the approximation ratio worsens as &#x03F5; increases, which is expected.</p>     <p>We then move to evaluate the average running time of the two algorithms, which is defined as the total running time of the algorithms divided by the total number of update operations. The expected running time of our algorithm has been studied in Theorem&#x00A0;<a class="enc" href="#enc5">5.4</a>. Figure&#x00A0;<a class="fig" href="#fig2">2</a> shows the average running time of the two algorithms as a function of <em>k</em> (left) and as a function of &#x03F5; (right) in Twitter, while Figure&#x00A0;<a class="fig" href="#fig4">4</a> shows the corresponding plots for Flickr. We can see that <SmallCap>SW</SmallCap> is consistently more efficient than our algorithm, which is expected. However, both algorithm are very efficient with the average running time being at most 5*10<sup>&#x2212; 4</sup> seconds and as small as 2*10<sup>&#x2212; 5</sup> seconds in some cases. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Ratio between the maximum radius and a lower bound on the optimum radius as a function of <em>k</em> (left) and &#x03F5; (right) on Twitter. The two algorithms have been tested for the same value of <em>k</em> and &#x03F5;, however, some additional space between the corresponding values in the plot has been introduced to improve the presentation.</span>      </div>     </figure>     <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Running time of the algorithms in second as a function of <em>k</em> (left) and &#x03F5; (right) on Twitter.</span>      </div>     </figure>     <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Ratio between the maximum radius and a lower bound on the optimum radius as a function of <em>k</em> (left) and &#x03F5; (right) on Flickr. The two algorithms have been tested for the same value of <em>k</em> and &#x03F5;, however, some additional space between the corresponding values in the plot has been introduced to improve the presentation.</span>      </div>     </figure>     <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Running time of the algorithms in second as a function of <em>k</em> (left) and &#x03F5; (right) on Flickr.</span>      </div>     </figure>     </p>    </section>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.5</span> Trajectory Data and Multicore Implementation</h3>     </div>     </header>     <p>In this section, we consider the task of clustering trajectory data. We consider the scenario where at each point in time, either a new point is added to one of the existing trajectories or a new trajectory consisting of one single point is created. Each update can therefore be seen as either an insertion of a new trajectory or a deletion of an existing trajectory followed by an insertion of the updated trajectory. Since trajectories can be modified in any arbitrary order, the sliding window model appears to be simplistic in this setting. Therefore, the algorithm in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] cannot be used. The distance between any two trajectories is computed as the symmetric Hausdorff distance (see Section&#x00A0;<a class="sec" href="#sec-17">6.2</a>), which is a metric.</p>     <p>Computing the distance between two trajectories requires <em>&#x03A9;</em>(<em>r</em>) operations, where <em>r</em> is the maximum number of points in the two trajectories. In our dataset, most trajectory contain up to 500 points with the largest one containing nearly 4000 points. This makes the problem of clustering trajectory data more computationally extensive than in the case of two-dimensional euclidean space. Therefore, in order to be able to conduct an extensive evaluation of the algorithm, we consider a multicore implementation. Since the clusterings <span class="inline-equation"><span class="tex">$\mathcal {C}_{\beta }$</span>     </span>&#x2019;s can be processed independently, a multicore implementation is straightforward: the <span class="inline-equation"><span class="tex">$\mathcal {C}_{\beta }$</span>     </span>&#x2019;s are partitioned across the threads, with each thread taking care of all deletions and insertions in the clusterings associated to such a thread. All our experiments discussed in this section use 30 threads.</p>     <p>We first evaluate the running time of our algorithm. Figure&#x00A0;<a class="fig" href="#fig5">5</a> shows the histogram for the running time of each update operation (in seconds). In particular, for each running time value, the number of update operations requiring that running time is reported. We can see that approximately 80M operations require less than 4*10<sup>&#x2212; 3</sup> seconds, while the maximum running time per operation is 0.38 seconds. Figure&#x00A0;<a class="fig" href="#fig6">6</a> shows the average running time as a function of the number of update operations. At any point in time, the average running time is computed as the total running time divided by the total number of operations up to that point. The running time is measured in seconds. We can see that the average running time is always less than one millisecond, while it becomes stable after approximately 18 million update operations. This is consistent with Theorem&#x00A0;<a class="enc" href="#enc9">5.8</a>, which roughly speaking states that the probability of significantly deviating from the expected running time becomes very low when the number of update operations is large.</p>     <p>As for the approximation ratio, using the default value of <em>k</em> and &#x03F5;, we obtain a median of 2.05, while the first and third quartile are respectively equal to 2.03 and 2.13. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Histogram of the running time of each operation during a single run on trajectories of our algorithm.</span>      </div>     </figure>     <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Evolution of the average running time per operation during a single run.</span>      </div>     </figure>     </p>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.6</span> Concentration around the Expectation</h3>     </div>     </header>     <p>Our last experiment aims to study the concentration around the expected running time (i.e. Theorem&#x00A0;<a class="enc" href="#enc9">5.8</a>) from an experimental point of view. We consider 100 runs of our algorithm on the Twitter dataset using the default values and we show the histogram of the average running time in Figure&#x00A0;<a class="fig" href="#fig7">7</a>. We can see that almost all runs require 5.8*10<sup>&#x2212; 4</sup> seconds on average, while in very few cases the average running time is more than 12*10<sup>&#x2212; 4</sup> seconds, which is consistent with Theorem&#x00A0;<a class="enc" href="#enc9">5.8</a>. <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186124/images/www2018-133-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">concentration of measure experiments.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion and Future Work</h2>     </div>    </header>    <p>In our work, we developed a (2 + &#x03F5;)-approximation algorithm for <em>k</em>-center clustering, under a fully dynamic adversarial model of computation. In such a model, points can be inserted or deleted arbitrarily, provided that the adversary does not have access to the random choices of our algorithm. Our theoretical analysis, shows that the expected amortized cost of our algorithm is poly-logarithmic (in some cases of interest) while we show concentration around its expected value. Our work is the first work with strong theoretical guarantees for fully dynamic <em>k</em>-center clustering, to the best of our knowledge.</p>    <p>We then conducted an extensive evaluation of our algorithm on dynamic data from Twitter, Flickr, as well as trajectory data. Our experiments show that our algorithm produces clustering solutions with smaller maximum radius in comparison with state-of-the-art approaches, although this comes at the price of slightly worse average running time and more space.</p>    <p>For future work, we are planning to study other data mining and machine learning algorithms under the fully dynamic model of computation.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">R.&#x00A0;P.&#x00A0;L. A.&#x00A0;Epasto, S.&#x00A0;Lattanzi. Ego-splitting framework: from non-overalapping to overalapping clusters. In <em>      <em>KDD 2017</em>     </em>.</li>     <li id="BibPLXBIB0002" label="[2]">D.&#x00A0;Arthur and S.&#x00A0;Vassilvitskii. k-means++: the advantages of careful seeding. In <em>      <em>Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007</em>     </em>, pages 1027&#x2013;1035, 2007.</li>     <li id="BibPLXBIB0003" label="[3]">B.&#x00A0;Bahmani, B.&#x00A0;Moseley, A.&#x00A0;Vattani, R.&#x00A0;Kumar, and S.&#x00A0;Vassilvitskii. Scalable k-means++. <em>      <em>PVLDB</em>     </em>, 5(7): 622&#x2013;633, 2012.</li>     <li id="BibPLXBIB0004" label="[4]">M.&#x00A0;Charikar, C.&#x00A0;Chekuri, T.&#x00A0;Feder, and R.&#x00A0;Motwani. Incremental clustering and dynamic information retrieval. In <em>      <em>Proceedings of the twenty-ninth annual ACM symposium on Theory of computing</em>     </em>, pages 626&#x2013;635. ACM, 1997.</li>     <li id="BibPLXBIB0005" label="[5]">M.&#x00A0;Charikar, L.&#x00A0;O&#x0027;Callaghan, and R.&#x00A0;Panigrahy. Better streaming algorithms for clustering problems. In <em>      <em>Proceedings of the 35th Annual ACM Symposium on Theory of Computing, June 9-11, 2003, San Diego, CA, USA</em>     </em>, pages 30&#x2013;39, 2003.</li>     <li id="BibPLXBIB0006" label="[6]">V.&#x00A0;Cohen-Addad, C.&#x00A0;Schwiegelshohn, and C.&#x00A0;Sohler. Diameter and k-center in sliding windows. In <em>      <em>43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy</em>     </em>, pages 19:1&#x2013;19:12, 2016.</li>     <li id="BibPLXBIB0007" label="[7]">A.&#x00A0;Epasto, S.&#x00A0;Lattanzi, and M.&#x00A0;Sozio. Efficient densest subgraph computation in evolving graphs. In <em>      <em>Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015</em>     </em>, pages 300&#x2013;310, 2015.</li>     <li id="BibPLXBIB0008" label="[8]">S.&#x00A0;L. F.&#x00A0;Chierichetti, R.&#x00A0;Kumar and S.&#x00A0;Vassilvitskii. Fair clustering through fairlets. In <em>      <em>NIPS</em>     </em>, 2017.</li>     <li id="BibPLXBIB0009" label="[9]">T.&#x00A0;F. Gonzalez. Clustering to minimize the maximum intercluster distance. <em>      <em>Theoretical Computer Science</em>     </em>, 38:293&#x2013;306, 1985.</li>     <li id="BibPLXBIB0010" label="[10]">S.&#x00A0;Guha, N.&#x00A0;Mishra, R.&#x00A0;Motwani, and L.&#x00A0;O&#x0027;Callaghan. Clustering data streams. In <em>      <em>41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA</em>     </em>, pages 359&#x2013;366, 2000.</li>     <li id="BibPLXBIB0011" label="[11]">S.&#x00A0;Gupta, R.&#x00A0;Kumar, K.&#x00A0;Lu, B.&#x00A0;Moseley, and S.&#x00A0;Vassilvitskii. Local search methods for k-means with outliers. <em>      <em>PVLDB</em>     </em>, 10(7): 757&#x2013;768, 2017.</li>     <li id="BibPLXBIB0012" label="[12]">S.&#x00A0;Lattanzi and S.&#x00A0;Vassilvitskii. Consistent k-clustering. In <em>      <em>ICML</em>     </em>, 2017.</li>     <li id="BibPLXBIB0013" label="[13]">M.&#x00A0;D. M. H. S.&#x00A0;L. M.&#x00A0;Bateni, S.&#x00A0;Behnezhad and V.&#x00A0;Mirrokni. On distributed hierarchical clustering. In <em>      <em>NIPS 2017</em>     </em>.</li>     <li id="BibPLXBIB0014" label="[14]">R.&#x00A0;Matthew&#x00A0;Mccutchen and S.&#x00A0;Khuller. Streaming algorithms for k-center clustering with outliers and with anonymity. <em>APPROX &#x2019;08 / RANDOM &#x2019;08</em>, pages 165&#x2013;178, 2008.</li>     <li id="BibPLXBIB0015" label="[15]">H.&#x00A0;Steinhaus. <em>      <em>Mathematical Snapshots</em>     </em>. 3rd ed.<City>New York,</City>1999.</li>     <li id="BibPLXBIB0016" label="[16]">A.&#x00A0;A. Taha and A.&#x00A0;Hanbury. An efficient algorithm for calculating the exact hausdorff distance. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>, 2015.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>This research was partially supported by a grant from the PROCORE France-Hong Kong Joint Research Scheme sponsored by the Research Grants Council of Hong Kong and the Consulate General of France in Hong Kong under the project F-HKU702/16.</p>   <p id="fn2">This research was partially supported by the Hong Kong RGC under the grants 17217716.</p>   <p id="fn2a">This research was partially supported by French National Agency (ANR) under project FIELDS (ANR-15-CE23- 0006).</p>   <p id="fn3"><a href="#foot-fn3"><sup>1</sup></a>A random variable <em>Z</em> is stochastically dominated by a random variable <em>Y</em> if for all real <em>x</em>, <span class="inline-equation"><span class="tex">$\Pr [Z \ge x] \le \Pr [Y \ge x]$</span>    </span>.</p>   <p id="fn4"><a href="#foot-fn4"><sup>2</sup></a><a class="link-inline force-break"     href="https://github.com/fe6Bc5R4JvLkFkSeExHM/k-center">https://github.com/fe6Bc5R4JvLkFkSeExHM/k-center</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>3</sup></a><a class="link-inline force-break" href="http://yfcc100m.appspot.com/">http://yfcc100m.appspot.com/</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>4</sup></a><a class="link-inline force-break"     href="https://archive.ics.uci.edu/ml/datasets/Taxi+Service+Trajectory+-+Prediction+Challenge,+ECML+PKDD+2015">https://archive.ics.uci.edu/ml/datasets/Taxi+Service+Trajectory+-+Prediction+Challenge,+ECML+PKDD+2015</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186124">https://doi.org/10.1145/3178876.3186124</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
