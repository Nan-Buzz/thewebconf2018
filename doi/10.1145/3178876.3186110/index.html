<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>A Correlation Clustering Framework for Community Detection</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186110"/></head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186110'>https://doi.org/10.1145/3178876.3186110</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186110'>https://w3id.org/oa/10.1145/3178876.3186110</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">A Correlation Clustering Framework for Community Detection</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Nate</span>     <span class="surName">Veldt</span>,     Mathematics Department, Purdue University, West Lafayette, Indiana, <a href="mailto:lveldt@purdue.edu">lveldt@purdue.edu</a>    </div>    <div class="author">     <span class="givenName">David F.</span>     <span class="surName">Gleich</span>,     Computer Science Department, Purdue University, West Lafayette, Indiana, <a href="mailto:dgleich@purdue.edu">dgleich@purdue.edu</a>    </div>    <div class="author">     <span class="givenName">Anthony</span>     <span class="surName">Wirth</span>,     School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia, <a href="mailto:awirth@unimelb.edu.au">awirth@unimelb.edu.au</a>    </div>                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186110" target="_blank">https://doi.org/10.1145/3178876.3186110</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Graph clustering, or community detection, is the task of identifying groups of closely related objects in a large network. In this paper we introduce a new community-detection framework called <SmallCap>LambdaCC</SmallCap> that is based on a specially weighted version of correlation clustering. A key component in our methodology is a clustering resolution parameter,&#x00A0;<em>&#x03BB;</em>, which implicitly controls the size and structure of clusters formed by our framework. We show that, by increasing this parameter, our objective effectively interpolates between two different strategies in graph clustering: finding a sparse cut and forming dense subgraphs. Our methodology unifies and generalizes a number of other important clustering quality functions including modularity, sparsest cut, and cluster deletion, and places them all within the context of an optimization problem that has been well studied from the perspective of approximation algorithms. Our approach is particularly relevant in the regime of finding dense clusters, as it leads to a 2-approximation for the cluster deletion problem. We use our approach to cluster several graphs, including large collaboration networks and social networks.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Mathematics of computing </strong>&#x2192; <strong>Graph algorithms;</strong> <strong>Approximation algorithms;</strong> <em>Mathematical optimization;</em></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>graph clustering; community detection; network analysis; correlation clustering; cluster deletion; sparsest cut</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Nate Veldt, David F. Gleich, and Anthony Wirth. 2018. A Correlation Clustering Framework for Community Detection. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France.</em> ACM, New York, NY, USA, 10 Pages. <a href="https://doi.org/10.1145/3178876.3186110" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186110</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Identifying groups of related entities in a network is a ubiquitous task across scientific disciplines. This task is often called graph clustering, or community detection, and can be used to find similar proteins in a protein-interaction network, group related organisms in a food web, identify communities in a social network, and classify web documents, among numerous other applications.</p>    <p>Defining the right notion of a &#x201C;good&#x201D; community in a graph is an important precursor to developing successful algorithms for graph clustering. In general, a good clustering is one in which nodes inside clusters are more densely connected to each other than to the rest of the graph. However, no consensus exists as to the best way to determine the quality of network clusterings, and recent results show there cannot be such a consensus for the multiple possible reasons people may cluster data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. Common objective functions studied by theoretical computer scientists include normalized cut, sparsest cut, conductance, and edge expansion, all of which measure some version of the cut-to-size ratio for a single cluster in a graph. Other standards of clustering quality put a greater emphasis on the internal density of clusters, such as the cluster deletion objective, which seeks to partition a graph into completely connected sets of nodes (cliques) by removing the fewest edges possible.</p>    <p>Arguably the most widely used multi-cluster objective for community detection is modularity, introduced by Newman and Girvan&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. Modularity measures the difference between the true number of edges inside the clusters of a given partitioning (&#x201C;inner edges&#x201D;) minus the <em>expected</em> number of inner edges, where expectation is calculated with respect to a specific random graph model.</p>    <p>There are a limited number of results which have begun to unify distinct clustering measures by introducing objective functions that are closely related to modularity and depend on a tunable clustering resolution parameter&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. Reichardt and Bornholdt developed an approach based on finding the minimum-energy state of an infinite-range Potts spin glass. The resulting Hamiltonian function they study is viewed as a clustering objective with a resolution parameter,&#x00A0;<em>&#x03B3;</em>, which can be used as a heuristic for detecting overlapping and hierarchical community structure in a network. When&#x00A0;<em>&#x03B3;</em> = 1, the authors prove an equivalence between minimizing the Hamiltonian and finding the maximum modularity partitioning of a network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. Later, Delvenne et al. introduced a measure called the <em>stability</em> of a clustering, which generalizes modularity and also is related to the normalized cut objective and Fiedler&#x0027;s spectral clustering method for certain values of an input parameter&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>].</p>    <p>The inherent difficulty in obtaining clusterings that are provably close to the optimal solution puts these objective functions at a disadvantage. Although both the <em>stability</em> and the Hamiltonian-Potts objectives provide useful interpretations for community detection, there are no approximation guarantees for either: all current algorithms are heuristics. Furthermore, it is known that maximizing modularity itself is not only NP-hard, but is also NP-hard to approximate to within any constant factor&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>].</p>    <p>    <em>Our Contributions</em>. In this paper, we introduce a new clustering framework based on a specially weighted version of correlation clustering&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. Our partitioning objective for signed networks lies &#x201C;between&#x201D; the family of&#x00A0; &#x00B1; 1 complete instances and the most general correlation clustering instances. Our framework comes with several novel theoretical properties and leads to many connections between clustering objectives that were previously not seen to be related. In summary, we provide:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">A novel framework <SmallCap>LambdaCC</SmallCap> for community detection that is related to modularity and the Hamiltonian, but is more amenable to approximation results.<br/></li>    <li id="list2" label="&#x2022;">A proof that our framework interpolates between the sparsest cut objective and the cluster deletion problem, as we increase a single resolution parameter,&#x00A0;<em>&#x03BB;</em>.<br/></li>    <li id="list3" label="&#x2022;">Several successful algorithms for optimizing our new objective function in both theory and practice, including a 2-approximation for cluster deletion, which improves upon the previous best approximation factor of 3.<br/></li>    <li id="list4" label="&#x2022;">A demonstration of our methods in a number of clustering applications, including social network analysis and mining cliques in collaboration networks.<br/></li>    </ul>   </section>   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Background and Related Work</h2>    </div>    </header>    <p>Let&#x00A0;<em>G</em> be an undirected and unweighted graph on&#x00A0;<em>n</em> nodes&#x00A0;<em>V</em>, with&#x00A0;<em>m</em> edges&#x00A0;<em>E</em>. For all <em>v</em> &#x2208; <em>V</em>, let&#x00A0;<em>d<sub>v</sub>    </em> be node&#x00A0;<em>v</em>&#x2019;s degree. Given <em>S</em>&#x2286;<em>V</em>, let <span class="inline-equation"><span class="tex">$\bar{S} = V\backslash S$</span>    </span> be the complement of&#x00A0;<em>S</em> and <span class="inline-equation"><span class="tex">$\operatorname{vol}(S) = \sum _{v\in S}d_v$</span>    </span> be its volume. For every two disjoint sets of vertices <em>S</em>, <em>T</em>&#x2286;<em>V</em>, <span class="inline-equation"><span class="tex">$\operatorname{cut}(S,T)$</span>    </span> indicates the number of edges between&#x00A0;<em>S</em> and&#x00A0;<em>T</em>. If <span class="inline-equation"><span class="tex">$T = \bar{S}$</span>    </span>, we write <span class="inline-equation"><span class="tex">$\operatorname{cut}(S) = \operatorname{cut}(S,\bar{S})$</span>    </span>. Let&#x00A0;<em>E<sub>S</sub>    </em> denote the interior edge set of&#x00A0;<em>S</em>. The edge density of a cluster is <span class="inline-equation"><span class="tex">$\operatorname{density}(S) = |E_S|/ {|S| \atopwithdelims ()2 }$</span>    </span>, the ratio between the number of edges to the number of pairs of nodes in&#x00A0;<em>S</em>. By convention, the density of a single node is 1. We now present background and related work that is foundational to our results, including definitions for several common clustering objectives.</p>    <section id="sec-6">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Correlation Clustering</h3>     </div>    </header>    <p>An instance of correlation clustering is given by a signed graph where every pair of nodes&#x00A0;<em>i</em> and&#x00A0;<em>j</em> possesses two non-negative weights,&#x00A0;<span class="inline-equation"><span class="tex">$w_{ij}^+$</span>     </span> and&#x00A0;<span class="inline-equation"><span class="tex">$w_{ij}^-$</span>     </span>, to indicate how similar and how dissimilar&#x00A0;<em>i</em> and&#x00A0;<em>j</em> are, respectively. Typically only one of these weights is nonzero for each pair <em>i</em>, <em>j</em>. The objective can be expressed as an integer linear program (ILP): <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{array}{ll}\text{minimize} &#x0026; \sum _{i{\lt}j} w_{ij}^+ x_{ij} + w_{ij}^- (1-x_{ij})\\ \text{subject to}&#x0026; x_{ij} \le x_{ik} + x_{jk} \text{ for all $i,j,k$} \\ &#x0026; x_{ij} \in \lbrace 0,1\rbrace \text{ for all $i,j$.} \end{array} \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> In the above formulation,&#x00A0;<em>x<sub>ij</sub>     </em> represents &#x201C;distance&#x201D;: <em>x<sub>ij</sub>     </em> = 0 indicates that nodes&#x00A0;<em>i</em> and&#x00A0;<em>j</em> are clustered together, while <em>x<sub>ij</sub>     </em> = 1 indicates they are separated. Including triangle inequality constraints ensures the output of the above ILP defines a valid clustering of the nodes. This objective counts the total <em>weight</em> of disagreements between the signed weights in the graph and a given clustering of its nodes. The disagreement (or &#x201C;mistake&#x201D;) weight of a pair&#x00A0;<em>i</em>, <em>j</em> is&#x00A0;<span class="inline-equation"><span class="tex">$w_{ij}^-$</span>     </span> if the nodes are clustered together, but&#x00A0;<span class="inline-equation"><span class="tex">$w_{ij}^+$</span>     </span> if they are separated. We can equivalently define the agreement weight to be <span class="inline-equation"><span class="tex">$w_{ij}^+$</span>     </span> if <em>i</em>, <em>j</em> are clustered together, but&#x00A0;<span class="inline-equation"><span class="tex">$w_{ij}^-$</span>     </span> if they are separated. The optimal clusterings for maximizing agreements and minimizing disagreements are identical, but it is harder to approximate the latter.</p>    <p>Correlation clustering was introduced by Bansal et al., who proved the problem is NP-complete&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. They gave a polynomial-time approximation scheme for the maximization version and a constant-factor approximation for minimizing disagreements in&#x00A0; &#x00B1; 1-weighted graphs. Subsequently, Charikar et al. gave a factor 4-approximation for minimizing disagreements and proved APX-hardness of this variant. They also described an <em>O</em>(log&#x2009;<em>n</em>) approximation for minimization in general weighted graphs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>], proved independently by two different groups, who showed that minimizing disagreements is equivalent to minimum multicut&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>].</p>    <p>The problem has also been studied for the case where edges carry both positive and negative weights, satisfying probability constraints: for all pairs&#x00A0;<em>i</em>, <em>j</em>, <span class="inline-equation"><span class="tex">$w_{ij}^+ + w_{ij}^- = 1$</span>     </span>. Ailon et al. gave a&#x00A0;2.5-approximation for this version of the problem based on an LP-relaxation, and additionally developed a very fast algorithm, called <SmallCap>Pivot</SmallCap>, that in expectation gives a 3-approximation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. Currently the best-known approximation factor for correlation clustering on&#x00A0; &#x00B1; 1 instances is slightly smaller than&#x00A0;2.06, obtained by a careful rounding of the canonical LP relaxation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>].</p>    </section>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Modularity and the Hamiltonian</h3>     </div>    </header>    <p>One very popular measure of clustering quality is modularity, introduced in its most basic form by Newman and Girvan&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]. We more closely follow the presentation of modularity given by Newman&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. The modularity&#x00A0;<em>Q</em> of an underlying clustering is: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\textstyle Q(x) = \frac{1}{2m} \sum _{i \ne j} \left(A_{ij} - P_{ij} \right) (1-x_{ij})\,}, \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <em>A<sub>ij</sub>     </em> = 1 if nodes&#x00A0;<em>i</em> and&#x00A0;<em>j</em> are adjacent, and zero otherwise, and&#x00A0;<em>x<sub>ij</sub>     </em> is again the binary variable indicating &#x201C;distance&#x201D; between&#x00A0;<em>i</em> and&#x00A0;<em>j</em> in the corresponding clustering. The value&#x00A0;<em>P<sub>ij</sub>     </em> represents the probability of an edge existing between&#x00A0;<em>i</em> and&#x00A0;<em>j</em> in a specific random graph model. The intent of this measure is to reward clusterings in which the actual number of edges inside a cluster is greater than the <em>expected</em> number of edges in the cluster, as determined by the choice for&#x00A0;<em>P<sub>ij</sub>     </em>. Although there are many options, it is standard in the literature to set <em>P<sub>ij</sub>     </em> = <em>d<sub>i</sub>d<sub>j</sub>     </em>/(2<em>m</em>), since this preserves both the degree distribution and the expected number of edges between the original graph and null model.</p>    <p>By slightly editing the modularity function, we obtain the Hamiltonian objective of Reichardt and Bornholdt&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\textstyle \mathcal {H}(x) = - \sum _{i\ne j} \left(A_{ij} - \gamma P_{ij} \right)(1-x_{ij})\,}. \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> The primary difference between this and modularity is the inclusion of a clustering resolution parameter&#x00A0;<em>&#x03B3;</em>. If we fix <em>&#x03B3;</em> = 1, minimizing&#x00A0;(<a class="eqn" href="#eq3">3</a>) is equivalent to maximizing modularity. When varied, this parameter controls how much a clustering is penalized for putting two non-adjacent nodes together or separating adjacent nodes.</p>    <p>The Hamiltonian objective is in turn closely related to the <em>stability</em> of a clustering as defined by Delvenne et al., another generalization of modularity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. Roughly speaking, the stability of a partition measures the likelihood that a random walker, beginning at a node and following outgoing edges uniformly at random, will end up in the cluster it started in after a random walk of length&#x00A0;<em>t</em>. This&#x00A0;<em>t</em> serves as a resolution parameter, since the walker will tend to &#x201C;wander&#x201D; farther when&#x00A0;<em>t</em> is increased, leading to the formation of larger clusters when the stability is maximized. Delvenne et al. showed that objective&#x00A0;(<a class="eqn" href="#eq3">3</a>) is equivalent to a linearized version of the stability measure for a specific range of time steps&#x00A0;<em>t</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>].</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Sparsest Cut and Normalized Cut</h3>     </div>    </header>    <p>One measure of cluster quality in an unsigned network&#x00A0;<em>G</em> is the sparsest cut score, defined for a set&#x00A0;<em>S</em>&#x2286;<em>V</em> to be <span class="inline-equation"><span class="tex">$\phi (S) = \operatorname{cut}(S)/|S| + \operatorname{cut}(S)/|\bar{S}| = n \cdot \operatorname{cut}(S)/ (|S||\bar{S}|)$</span>     </span>. Smaller values for&#x00A0;<em>&#x03D5;</em>(<em>S</em>) are desirable, since they indicate that&#x00A0;<em>S</em>, in spite of its size, is only loosely connected to the rest of the graph. This measure differs by at most a factor of two from the related edge expansion measure: <span class="inline-equation"><span class="tex">$\operatorname{cut}(S)/(\min \lbrace |S|, |\bar{S}| \rbrace)$</span>     </span>. If we replace |<em>S</em>| with <span class="inline-equation"><span class="tex">$\operatorname{vol}(S)$</span>     </span> in these two objectives, we obtain the normalized cut and the conductance measure respectively. In our work we focus on a multiplicative scaling of the sparsest cut objective that we call the <em>scaled sparsest cut</em>: <span class="inline-equation"><span class="tex">$\psi (S) = \phi (S)/n = {\operatorname{cut}(S)}/(|S| |\bar{S}|)$</span>     </span>, which is identical to sparsest cut in terms of multiplicative approximations. The best known approximation for finding the minimum sparsest cut of a graph is an <span class="inline-equation"><span class="tex">$O(\sqrt {\log n})$</span>     </span>-approximation algorithm due to Arora et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>].</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Cluster Deletion</h3>     </div>    </header>    <p>Cluster deletion is the problem of finding a minimum number of edges in&#x00A0;<em>G</em> to be deleted in order to convert&#x00A0;<em>G</em> into a disjoint set of cliques. This problem was first studied by Ben-Dor et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], later formalized in the work of Natanzon et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>], who proved it is NP-hard, and Shamir et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>], who showed it is APX-hard. The latter studied the problem in conjunction with other related <em>edge-modification</em> problems, including cluster completion and cluster editing. Numerous fixed parameter tractability results are known for cluster deletion [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>], as well many results regarding special graphs for which the problem can be solved in polynomial time [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]. Dessmark et al. proved that recursively finding maximum cliques will return a clustering with a cluster deletion score within a factor 2 of optimal&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>], though in general this procedure is NP-hard.</p>    </section>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Theoretical Results</h2>    </div>    </header>    <p>Our novel clustering framework takes an unsigned graph <em>G</em> = (<em>V</em>, <em>E</em>) and converts it into a signed graph&#x00A0;<em>G</em>&#x2032; = (<em>V</em>, <em>E</em>    <sup>+</sup>, <em>E</em>    <sup>&#x2212;</sup>) on the same set of nodes,&#x00A0;<em>V</em>, for a fixed clustering resolution parameter <em>&#x03BB;</em> &#x2208; (0, 1). Partitioning&#x00A0;<em>G</em>&#x2032; with respect to the correlation clustering objective will then induce a clustering on&#x00A0;<em>G</em>. To construct the signed graph, we first introduce a node weight&#x00A0;<em>w<sub>v</sub>    </em> for each <em>v</em> &#x2208; <em>V</em>. If (<em>i</em>, <em>j</em>) &#x2208; <em>E</em>, we place a positive edge between nodes&#x00A0;<em>i</em> and&#x00A0;<em>j</em> in&#x00A0;<em>G</em>&#x2032;, with weight (1 &#x2212; <em>&#x03BB;w<sub>i</sub>w<sub>j</sub>    </em>). For (<em>i</em>, <em>j</em>)&#x2209;<em>E</em>, we place a negative edge between&#x00A0;<em>i</em> and&#x00A0;<em>j</em> in&#x00A0;<em>G</em>&#x2032;, with weight <em>&#x03BB;w<sub>i</sub>w<sub>j</sub>    </em>. We consider two different choices for node weights&#x00A0;<em>w<sub>v</sub>    </em>: setting <em>w<sub>v</sub>    </em> = 1 for all <em>v</em> (<em>standard</em>) or choosing <em>w<sub>v</sub>    </em> = <em>d<sub>v</sub>    </em> (<em>degree-weighted</em>). In Figure&#x00A0;<a class="fig" href="#fig1">1</a> we illustrate the process of converting&#x00A0;<em>G</em> into the <SmallCap>LambdaCC</SmallCap> signed graph,&#x00A0;<em>G</em>&#x2032;. The goal of <SmallCap>LambdaCC</SmallCap> is to find the clustering that minimizes disagreements in <em>G</em>&#x2032;, or equivalently minimizes the following objective function expressed in terms of edges and non-edges in <em>G</em>: <div class="table-responsive" id="eq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \lambda \mathcal {CC}(x) = \sum _{ {{\scriptstyle {\begin{array}{*10c}(i,j) \in E\end{array}}}}} (1 - \lambda w_i w_j) x_{ij} + \sum _{ {{\scriptstyle {\begin{array}{*10c}(i,j) \notin E\end{array}}}}} \lambda w_i w_j (1-x_{ij}) \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>    </div> where <em>x</em> = (<em>x<sub>ij</sub>    </em>) represents the binary distances for the clustering. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186110/images/www2018-119-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">We convert a toy graph (left) into a signed graph for standard (middle) and degree-weighted (right) <SmallCap>LambdaCC</SmallCap>. Dashed red lines indicate negative edges.</span>     </div>    </figure>    </p>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Connection to Modularity</h3>     </div>    </header>    <p>Despite a significant difference in approach and interpretation, the clustering that minimizes disagreements is the same clustering that minimizes the Hamiltonian objective&#x00A0;(<a class="eqn" href="#eq3">3</a>), for a certain choice of parameters. To see this, we introduce node adjacency values <em>A<sub>ij</sub>     </em> in objective&#x00A0;(<a class="eqn" href="#eq4">4</a>) and perform a few steps of algebra: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \lambda \mathcal {CC}(x) &#x0026;= \sum _{(i,j) \in E} (A_{ij} - \lambda w_i w_j)x_{ij} - \sum _{(i,j) \notin E} (A_{ij} - \lambda w_i w_j)(1-x_{ij}) \\ &#x0026;= \sum _{(i,j) \in E} (1 - \lambda w_i w_j) - \sum _{i{\lt}j} (A_{ij} - \lambda w_i w_j)(1-x_{ij})\,.\end{align*} </span>       <br/>      </div>     </div> Choosing <em>P<sub>ij</sub>     </em> = <em>w<sub>i</sub>w<sub>j</sub>     </em>/(2<em>m</em>) and <em>&#x03B3;</em> = 2<em>m&#x03BB;</em>, we see that: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \lambda \mathcal {CC}(x) = {\textstyle \sum _{(i,j) \in E} (1 - \lambda w_i w_j) + \mathcal {H}(x)/2 \, , } \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where the first term is just a constant. This theorem follows:</p>    <div class="theorem" id="enc1">     <Label>Theorem 3.1.</Label>     <p> Minimizing disagreements for the <SmallCap>LambdaCC</SmallCap> objective is equivalent to minimizing&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {H}(x)$</span>      </span>.</p>    </div>    <p>The choice&#x00A0;<em>P<sub>ij</sub>     </em> = <em>w<sub>i</sub>w<sub>j</sub>     </em>/(2<em>m</em>) is reminiscent of the graph null model most commonly used for modularity and the Hamiltonian. This best highlights the similarity between these objectives and degree-weighted <SmallCap>LambdaCC</SmallCap>.</p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Standard <SmallCap>LambdaCC</SmallCap>      </h3>     </div>    </header>    <p>While degree-weighted <SmallCap>LambdaCC</SmallCap> is more closely related to modularity and the Hamiltonian, standard <SmallCap>LambdaCC</SmallCap> (setting <em>w<sub>v</sub>     </em> = 1 for every <em>v</em> &#x2208; <em>V</em>) leads to strong connections between the sparsest cut objective and cluster deletion. This version corresponds to solving a correlation clustering problem where all positive edges have equal weight,&#x00A0;(1 &#x2212; <em>&#x03BB;</em>), while all negative edges have equal weight,&#x00A0;<em>&#x03BB;</em>. The objective function for minimizing disagreements is <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \min \sum _{(i,j)\in E^+} (1-\lambda) x_{ij} + \sum _{(i,j) \in E^-} \lambda (1-x_{ij})\,, \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where we include the same constraints as in ILP&#x00A0;(<a class="eqn" href="#eq1">1</a>). This is a strict generalization of the unit-weight correlation clustering problem&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] (<em>&#x03BB;</em> = 1/2) indicating the problem in general is NP-hard (though it admits several approximation algorithms). If&#x00A0;<em>&#x03BB;</em> is&#x00A0;0 or&#x00A0;1, the problem is trivial to solve: put all nodes in one cluster or put each node in a singleton cluster, respectively. By selecting values for&#x00A0;<em>&#x03BB;</em> other than&#x00A0;0, 1/2, or&#x00A0;1, we uncover subtler connections between identifying sparse cuts and finding dense subgraphs in the network.</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Connection to Sparsest Cut</h3>     </div>    </header>    <p>Given&#x00A0;<em>G</em> and&#x00A0;<em>&#x03BB;</em>, the weight of positive-edge mistakes in the standard <SmallCap>LambdaCC</SmallCap> objective made by a two-clustering <span class="inline-equation"><span class="tex">$\mathcal {C} = \lbrace S, \bar{S}\rbrace$</span>     </span> equals the weight of edges crossing the cut: <span class="inline-equation"><span class="tex">$(1-\lambda)\operatorname{cut}(S)$</span>     </span>. To compute the weight of negative-edge mistakes, we take the weight of all negative edges in the entire network, <span class="inline-equation"><span class="tex">$\lambda \left({n \atopwithdelims ()2} - |E| \right)$</span>     </span>, and then subtract the weight of negative edges between <em>S</em> and <span class="inline-equation"><span class="tex">$\bar{S}$</span>     </span>: <span class="inline-equation"><span class="tex">$\lambda \left(|S| |\bar{S}| - \operatorname{cut}(S) \right)$</span>     </span>. Adding together all terms, we find that the <SmallCap>LambdaCC</SmallCap> objective for this clustering is <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {\textstyle \operatorname{cut}(S,\bar{S}) - \lambda |S| |\bar{S}| + \lambda {n \atopwithdelims ()2} -\lambda |E|\,}. \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> Note that if we minimize&#x00A0;(<a class="eqn" href="#eq7">7</a>) over all 2-clusterings, we solve the decision version of the minimum scaled sparsest cut problem: a few steps of algebra confirm that there is some set <em>S</em>&#x2286;<em>V</em> with <span class="inline-equation"><span class="tex">$\psi (S) = \operatorname{cut}(S)/(|S| |\bar{S}|) {\lt} \lambda$</span>     </span> if and only if (<a class="eqn" href="#eq7">7</a>) is less than <span class="inline-equation"><span class="tex">$\lambda {n \atopwithdelims ()2} -\lambda |E|$</span>     </span>.</p>    <p>In a similar way we can show that objective&#x00A0;(<a class="eqn" href="#eq6">6</a>) is equivalent to <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \min \,\, \frac{1}{2}\sum _{i=1}^k \operatorname{cut}(S_i) - \frac{\lambda }{2} \sum _{i=1}^k |S_i| |\bar{S_i}| + \lambda {n \atopwithdelims ()2} -\lambda |E|\,, \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> where we minimize over all clusterings of&#x00A0;<em>G</em> (note that the number of clusters <em>k</em> is determined automatically by optimizing the objective). In this case, optimally solving objective&#x00A0;(<a class="eqn" href="#eq8">8</a>) will tell us whether we can find a clustering <span class="inline-equation"><span class="tex">$\mathcal {C} = \lbrace S_1, S_2, \dots , S_k\rbrace$</span>     </span> such that <span class="inline-equation"><span class="tex">${\sum _{i=1}^k \operatorname{cut}(S_i,\bar{S_i})} /\left({ \sum _{j=1}^k |S_j| |\bar{S_j}|} \right){\lt} \lambda$</span>     </span>. Hence <SmallCap>LambdaCC</SmallCap> can be viewed as a multi-cluster generalization of the decision version of minimum sparsest cut. We now prove an even deeper connection between sparsest cut and <SmallCap>LambdaCC</SmallCap>. Using degree-weighted <SmallCap>LambdaCC</SmallCap> yields an analogous result for normalized cut.</p>    <div class="theorem" id="enc2">     <Label>Theorem 3.2.</Label>     <p>Let&#x00A0;<em>&#x03BB;</em>      <sup>*</sup> be the minimum scaled sparsest cut value for graph&#x00A0;<em>G</em>.</p>     <p>      <ol class="list-no-style">       <li id="list5" label="(a)">For all <em>&#x03BB;</em> > <em>&#x03BB;</em>       <sup>*</sup>, optimal solution&#x00A0;(<a class="eqn" href="#eq8">8</a>) partitions&#x00A0;<em>G</em> into two or more clusters, each of which has scaled sparsest cut&#x00A0; &#x2264; <em>&#x03BB;</em>. There exists some&#x00A0;<em>&#x03BB;</em>&#x2032; > <em>&#x03BB;</em>       <sup>*</sup> such that the optimal clustering for <SmallCap>LambdaCC</SmallCap> is the minimum sparsest cut partition.<br/></li>       <li id="list6" label="(b)">For&#x00A0;<em>&#x03BB;</em> &#x2264; <em>&#x03BB;</em>       <sup>*</sup>, it is optimal to place all nodes into a single cluster.<br/></li>      </ol>     </p>    </div>    <div class="proof" id="proof1">     <Label>Proof.</Label>     <p>      <strong>Statement (a)</strong> Let&#x00A0;<em>S</em>      <sup>*</sup> be some subset of&#x00A0;<em>V</em> that induces a sparsest cut, i.e., <span class="inline-equation"><span class="tex">$\psi (S^*) = {\operatorname{cut}(S^*)}/ ({|S^*| | \bar{S}^*|}) = \lambda ^*$</span>      </span>. The <SmallCap>LambdaCC</SmallCap> objective corresponding to&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {C} = \lbrace S^*, \bar{S}^* \rbrace$</span>      </span> is <div class="table-responsive" id="eq9">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \operatorname{cut}(S^*) - \lambda |S^*| | \bar{S}^*| + \lambda {n \atopwithdelims ()2} -\lambda |E|\,. \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>       </div>      </div> When minimizing objective&#x00A0;(<a class="eqn" href="#eq8">8</a>), we can always obtain a score of <span class="inline-equation"><span class="tex">$\lambda {n \atopwithdelims ()2} -\lambda |E|$</span>      </span> by placing all nodes into a single cluster. Note however that the the score of clustering <span class="inline-equation"><span class="tex">$\lbrace S^*, \bar{S}^*\rbrace$</span>      </span> in expression&#x00A0;(<a class="eqn" href="#eq9">9</a>) is strictly less than <span class="inline-equation"><span class="tex">$\lambda {n \atopwithdelims ()2} -\lambda |E|$</span>      </span> for all <em>&#x03BB;</em> > <em>&#x03BB;</em>      <sup>*</sup>. Even if <span class="inline-equation"><span class="tex">$\lbrace S^*, \bar{S}^*\rbrace$</span>      </span> is not optimal, this means that when <em>&#x03BB;</em> > <em>&#x03BB;</em>      <sup>*</sup>, we can do strictly better than placing all nodes into one cluster. In this case let <span class="inline-equation"><span class="tex">$\mathcal {C}^*$</span>      </span> be the optimal <SmallCap>LambdaCC</SmallCap> clustering and consider two of its clusters:&#x00A0;<em>S<sub>i</sub>      </em> and&#x00A0;<em>S<sub>j</sub>      </em>. The weight of disagreements between&#x00A0;<em>S<sub>i</sub>      </em> and&#x00A0;<em>S<sub>j</sub>      </em> is equal to the number of positive edges between them times the weight of a positive edge: <span class="inline-equation"><span class="tex">$(1-\lambda)\operatorname{cut}(S_i,S_j)$</span>      </span>. Should we form a new clustering by merging&#x00A0;<em>S<sub>i</sub>      </em> and&#x00A0;<em>S<sub>j</sub>      </em>, these positive disagreements will disappear; in turn, we would introduce <span class="inline-equation"><span class="tex">$\lambda |S_i||S_j| - \lambda \operatorname{cut}(S_i,S_j)$</span>      </span> new mistakes, being negative edges between the clusters. Because we assumed&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {C}^*$</span>      </span> is optimal, we know that we cannot decrease the objective by merging two of the clusters, implying that <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{gather*} (1-\lambda)\operatorname{cut}(S_i,S_j) - \left(\lambda |S_i||S_j| - \lambda \operatorname{cut}(S_i,S_j)\right)\\ = \operatorname{cut}(S_i,S_j) - \lambda |S_i||S_j| \le 0\,.\end{gather*} </span>       <br/>       </div>      </div> Given this, we fix an arbitrary cluster&#x00A0;<em>S<sub>i</sub>      </em> and perform a sum over all other clusters to see that <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ {\textstyle \sum _{j \ne i}} \operatorname{cut}(S_i,S_j) - {\textstyle \sum _{j \ne i}} \lambda |S_i||S_j| \le 0\, \] </span>       <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \Rightarrow \operatorname{cut}(S_i, \bar{S_i}) - \lambda |S_i||\bar{S_i}| \le 0 \Rightarrow {\operatorname{cut}(S_i, \bar{S_i})}/\left({|S_i||\bar{S_i}|}\right) \le \lambda \,, \] </span>       <br/>       </div>      </div> proving the desired upper bound on scaled sparsest cut.</p>     <p>Since&#x00A0;<em>G</em> is a finite graph, there are a finite number of scaled sparsest cut scores that can be induced by a subset of&#x00A0;<em>V</em>. Let&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{\lambda }$</span>      </span> be the second-smallest scaled sparsest cut score achieved, so <span class="inline-equation"><span class="tex">$\tilde{\lambda } {\gt} \lambda ^*$</span>      </span>. If we set <span class="inline-equation"><span class="tex">$\lambda ^{\prime } = (\lambda ^* + \tilde{\lambda })/2$</span>      </span>, then the optimal <SmallCap>LambdaCC</SmallCap> clustering produces at least two clusters, since <em>&#x03BB;</em>&#x2032; > <em>&#x03BB;</em>      <sup>*</sup>, and each cluster has scaled sparsest cut at most <span class="inline-equation"><span class="tex">$\lambda ^{\prime } {\lt} \tilde{\lambda }$</span>      </span>. By our selection of&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{\lambda }$</span>      </span>, all clusters returned must have scaled sparsest cut exactly equal to&#x00A0;<em>&#x03BB;</em>      <sup>*</sup>, which is only possible if the clustering returned has two clusters. Hence this clustering is a minimum sparsest cut partition of the network.</p>     <p>      <strong>Statement (b)</strong> If <em>&#x03BB;</em> < <em>&#x03BB;</em>      <sup>*</sup>, forming a single cluster must be optimal, otherwise we could invoke Statement&#x00A0;(a) to assert the existence of some nontrivial cluster with scaled sparsest cut less than or equal to <em>&#x03BB;</em> < <em>&#x03BB;</em>      <sup>*</sup>, contradicting the minimality of&#x00A0;<em>&#x03BB;</em>      <sup>*</sup>. If <em>&#x03BB;</em> = <em>&#x03BB;</em>      <sup>*</sup>, forming a single cluster or using the clustering <span class="inline-equation"><span class="tex">$\mathcal {C} = \lbrace S^*, \bar{S}^* \rbrace$</span>      </span> yield the same objective score, which is again optimal for the same reason.</p>    </div>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Connection to Cluster Deletion</h3>     </div>    </header>    <p>For large&#x00A0;<em>&#x03BB;</em>, our problem becomes more similar to cluster deletion. We can reduce any cluster deletion problem to correlation clustering by taking the input graph&#x00A0;<em>G</em> and introducing a negative edge of weight&#x00A0;&#x201C;&#x221E;&#x201D; between every pair of non-adjacent nodes. This guarantees that optimally solving correlation clustering will yield clusters that all correspond to cliques in&#x00A0;<em>G</em>. Furthermore, the weight of disagreements will be the number of edges in&#x00A0;<em>G</em> that are cut, i.e., the cluster deletion score. We can obtain a generalization of cluster deletion by instead choosing the weight of each negative edge to be <em>&#x03B1;</em> < &#x221E;. The corresponding objective is <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \sum _{(i,j)\in E^+} x_{ij} + \sum _{(i,j) \in E^-} \alpha (1-x_{ij})\,. \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> If we substitute <em>&#x03B1;</em> = <em>&#x03BB;</em>/(1 &#x2212; <em>&#x03BB;</em>) we see this differs from objective&#x00A0;(<a class="eqn" href="#eq6">6</a>) only by a multiplicative constant, and is therefore equivalent in terms of approximation. When <em>&#x03B1;</em> > 1, putting dissimilar nodes together will be more expensive than cutting positive edges, so we would expect that the clustering which optimizes the <SmallCap>LambdaCC</SmallCap> objective will separate&#x00A0;<em>G</em> into dense clusters that are &#x201C;nearly&#x201D; cliques. We formalize this with a simple theorem and corollary.</p>    <div class="theorem" id="enc3">     <Label>Theorem 3.3.</Label>     <p> If&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {C}$</span>      </span> minimizes the <SmallCap>LambdaCC</SmallCap> objective for the unsigned network <em>G</em> = (<em>V</em>, <em>E</em>), then the edge density of every cluster in&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {C}$</span>      </span> is at least&#x00A0;<em>&#x03BB;</em>.</p>    </div>    <div class="proof" id="proof2">     <Label>Proof.</Label>     <p> Take a cluster&#x00A0;<span class="inline-equation"><span class="tex">$S \in \mathcal {C}$</span>      </span> and consider what would happen if we broke apart&#x00A0;<em>S</em> so that each of its nodes were instead placed into its own singleton cluster. This means we are now making mistakes at every positive edge previously in <em>S</em>, which increases the weight of disagreements by&#x00A0;(1 &#x2212; <em>&#x03BB;</em>)|<em>E<sub>S</sub>      </em>|. On the other hand, there are no longer negative mistakes between nodes in&#x00A0;<em>S</em>, so the <SmallCap>LambdaCC</SmallCap> objective would simultaneously decrease by <span class="inline-equation"><span class="tex">$\lambda \left({|S| \atopwithdelims ()2} - |E_S| \right)$</span>      </span>. The total change in the objective made by pulverizing&#x00A0;<em>S</em> is <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ {\textstyle (1-\lambda)|E_S| - \lambda \left({|S| \atopwithdelims ()2} - |E_S| \right) = |E_S| - \lambda {|S| \atopwithdelims ()2}\, }, \] </span>       <br/>       </div>      </div> which must be nonnegative, since&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {C}$</span>      </span> is optimal, so&#x00A0;<span class="inline-equation"><span class="tex">$|E_S| - \lambda {|S| \atopwithdelims ()2} \ge 0$</span>      </span>, which implies <span class="inline-equation"><span class="tex">$\text{density}(S) ={|E_S|}/{ {|S| \atopwithdelims ()2} } \ge \lambda$</span>      </span>.</p>    </div>    <div class="corollary" id="enc4">     <Label>Corollary 3.4.</Label>     <p> Let&#x00A0;<em>G</em> have&#x00A0;<em>m</em> edges. For every <em>&#x03BB;</em> > <em>m</em>/(<em>m</em> + 1), optimizing <SmallCap>LambdaCC</SmallCap> is equivalent to optimizing cluster deletion.</p>    </div>    <div class="proof" id="proof3">     <Label>Proof.</Label>     <p> All output clusters must have density at least <em>m</em>/(<em>m</em> + 1), which is only possible if the density is actually&#x00A0;1, since&#x00A0;<em>m</em> is the total number of edges in the graph. Therefore all clusters are cliques and the <SmallCap>LambdaCC</SmallCap> and cluster deletion objectives differ only by a multiplicative constant (1 &#x2212; <em>&#x03BB;</em>).</p>    </div>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Equivalences and Approximations</h3>     </div>    </header>    <p>We summarize the equivalence relationships between <SmallCap>LambdaCC</SmallCap> and other objectives in Figure&#x00A0;<a class="fig" href="#fig2">2</a>. Accompanying this, Table&#x00A0;<a class="tbl" href="#tab1">1</a> outlines the best-known approximation results both for maximizing agreements and minimizing disagreements for the standard <SmallCap>LambdaCC</SmallCap> signed graph. For degree-weighted <SmallCap>LambdaCC</SmallCap>, the best-known approximation factors for all&#x00A0;<em>&#x03BB;</em> are <em>O</em>(log&#x2009;<em>n</em>)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] for minimizing disagreements, and&#x00A0;0.7666 for maximizing agreements&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]. Thus, <SmallCap>LambdaCC</SmallCap> is more amenable to approximation than modularity (and relatives) because of additive constants. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186110/images/www2018-119-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">       <SmallCap>LambdaCC</SmallCap> is equivalent to several other objectives for specific values of <em>&#x03BB;</em> &#x2208; (0, 1).Values&#x00A0;<em>&#x03BB;</em>       <sup>*</sup> and&#x00A0;<em>&#x03C1;</em>       <sup>*</sup> are not known a priori, but can be obtained by solving <SmallCap>LambdaCC</SmallCap> for increasingly smaller values of <em>&#x03BB;</em>.</span>      </div>     </figure>    </p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">The best approximation factors known for standard <SmallCap>LambdaCC</SmallCap>, for <em>&#x03BB;</em> &#x2208; (0, 1), both for minimizing disagreements and maximizing agreements. We contribute two constant-factor approximations for minimizing disagreements when&#x00A0;<em>&#x03BB;</em> > 1/2.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;"/>       <th style="text-align:left;">        <em>&#x03BB;</em> &#x2208; (0, 1/2)</th>       <th style="text-align:left;">        <em>&#x03BB;</em> = 1/2</th>       <th style="text-align:left;">        <em>&#x03BB;</em> &#x2208; (1/2, 1)</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Max-Ag.</td>       <td style="text-align:left;">0.7666&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0037">37</a>]</td>       <td style="text-align:left;">PTAS&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0004">4</a>]</td>       <td style="text-align:left;">0.7666&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0037">37</a>]</td>       </tr>       <tr>       <td style="text-align:left;">Min-Dis.</td>       <td style="text-align:left;">        <em>O</em>(log&#x2009;<em>n</em>)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0020">20</a>]</td>       <td style="text-align:left;">2.06&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0013">13</a>]</td>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$3 \left(2: \lambda {\gt} \frac{m}{m+1} \right)$</span>        </span>       </td>       </tr>      </tbody>     </table>    </div> 				 <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186110/images/www2018-119-img1.jpg" class="img-responsive" alt="" longdesc=""/>    </section>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Algorithms</h2>    </div>    </header>    <p>We present several new algorithms, tailored specifically to our <SmallCap>LambdaCC</SmallCap> framework; some come with approximation guarantees, some are designed for efficiency.<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> Our first two methods rely on a key theorem of van Zuylen and Williamson, which can be used to prove approximation results for different cases of correlation clustering using pivoting algorithms, which operate by selecting a pivot node <em>k</em>, clustering it with all its positive neighbors, and recursing on the rest of the graph. If pivots are chosen uniformly at random this corresponds to the <SmallCap>Pivot</SmallCap> algorithm of Ailon et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. We state the theorem here for completeness, with minor changes to match our notation and presentation:</p>    <p>    <div class="theorem" id="enc5">     <Label>Theorem 4.1.</Label>     <p> (Theorem&#x00A0;3.1 in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0038">38</a>]) Let <em>G</em> = (<em>V</em>, <em>W</em>      <sup>+</sup>, <em>W</em>      <sup>&#x2212;</sup>) be a signed, weighted graph where each pair of nodes (<em>i</em>, <em>j</em>) has positive and negative weights <span class="inline-equation"><span class="tex">$w_{ij}^+ \in W^+$</span>      </span> and <span class="inline-equation"><span class="tex">$w_{ij}^- \in W^-$</span>      </span>. Given a set of LP costs {<em>c<sub>ij</sub>      </em>: <em>i</em> &#x2208; <em>V</em>, <em>j</em> &#x2208; <em>V</em>, <em>i</em> &#x2260; <em>j</em>}, and an unweighted graph <span class="inline-equation"><span class="tex">$\tilde{G} = (V, F^+, F^-)$</span>      </span>     </p>     <p>satisfying the following assumptions:</p>     <p>      <ol class="list-no-style">       <li id="list7" label="(i)"><span class="inline-equation"><span class="tex">$w_{ij}^- \le \alpha c_{ij}$</span>       </span> for all (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>       <sup>+</sup> and<br/>       <span class="inline-equation"><span class="tex">$w_{ij}^+ \le \alpha c_{ij}$</span>       </span> for all (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>       <sup>&#x2212;</sup>,<br/></li>       <li id="list8" label="(ii)"><span class="inline-equation"><span class="tex">$w_{ij}^+ + w_{jk}^+ + w_{ik}^- \le \alpha \left(c_{ij} + c_{jk} + c_{ik} \right)$</span>       </span>       <br/>for every bad triangle in <span class="inline-equation"><span class="tex">$\tilde{G}$</span>       </span>: (<em>i</em>, <em>j</em>), (<em>j</em>, <em>k</em>) &#x2208; <em>F</em>       <sup>+</sup>, (<em>i</em>, <em>k</em>) &#x2208; <em>F</em>       <sup>&#x2212;</sup>,<br/></li>      </ol>     </p>     <p>then applying <SmallCap>Pivot</SmallCap> on <span class="inline-equation"><span class="tex">$\tilde{G}$</span>      </span> will return a solution that costs <em>&#x03B1;</em>&#x2211;<sub>       <em>i</em> < <em>j</em>      </sub>      <em>c<sub>ij</sub>      </em> in expectation.</p>    </div>    </p>    <p>A full proof is given by van Zuylen and Williamson&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], who also include a strategy for deterministically choosing pivots to achieve the same approximation.</p>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> 3-Approximation for <SmallCap>LambdaCC</SmallCap>      </h3>     </div>    </header>    <p>In order to apply Theorem&#x00A0;<a class="enc" href="#enc5">4.1</a> we must compute the LP relaxation&#x00A0;(<a class="eqn" href="#eq6">6</a>) for <SmallCap>LambdaCC</SmallCap> (where <em>c<sub>ij</sub>     </em> is the cost for edge (<em>i</em>, <em>j</em>)) to obtain distances&#x00A0;<em>x<sub>ij</sub>     </em>, which we then round into an unweighted graph&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{G}$</span>     </span>. If we can construct&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{G}$</span>     </span> to satisfy the assumptions of the theorem, all that remains is to apply <SmallCap>Pivot</SmallCap> to yield the desired approximation results. Pseudocode for our first method is displayed in Algorithm&#x00A0;1, which we call <SmallCap>threeLP</SmallCap> since it satisfies the following approximation guarantee:</p>    <div class="theorem" id="enc6">     <Label>Theorem 4.2.</Label>     <p> Algorithm <SmallCap>threeLP</SmallCap> satisfies Theorem&#x00A0;<a class="enc" href="#enc5">4.1</a> with <em>&#x03B1;</em> = 3 for standard <SmallCap>LambdaCC</SmallCap> when <em>&#x03BB;</em> > 1/2.</p>    </div>    <div class="proof" id="proof4">     <Label>Proof.</Label>     <p> We begin by stating the correspondence between the notation of Theorem&#x00A0;<a class="enc" href="#enc5">4.1</a> and the edge weights and LP costs for <SmallCap>LambdaCC</SmallCap>. The graph <em>G</em>&#x2032; = (<em>V</em>, <em>E</em>      <sup>+</sup>, <em>E</em>      <sup>&#x2212;</sup>) is made up of positive edges of weight (1 &#x2212; <em>&#x03BB;</em>) and negative edges with weight <em>&#x03BB;</em>, so <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;c_{ij} = (1-\lambda)x_{ij} \text{ and } (w_{ij}^+, w_{ij}^-) = (1-\lambda , 0) \text{ if $(i,j) \in E^+$} \\ &#x0026;c_{ij} = \lambda (1-x_{ij}) \text{ and } (w_{ij}^+, w_{ij}^-) = (0, \lambda) \text{ if $(i,j) \in E^-$}.\end{align*} </span>       <br/>       </div>      </div> By construction, if (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>&#x2212;</sup> then <em>x<sub>ij</sub>      </em> < 1/3, otherwise (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>+</sup> and we know <em>x<sub>ij</sub>      </em> &#x2265; 1/3. The first two inequalities we need to check for Theorem&#x00A0;<a class="enc" href="#enc5">4.1</a> are <div class="table-responsive" id="eq11">       <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026;\text{$w_{ij}^- \le \alpha c_{ij}$ for all $(i,j) \in F^+$} \end{align} </span>       <br/>       <span class="equation-number">(11)</span>       </div>      </div>      <div class="table-responsive" id="eq12">       <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026;\text{$w_{ij}^+ \le \alpha c_{ij}$ for all $(i,j) \in F^-$} \end{align} </span>       <br/>       <span class="equation-number">(12)</span>       </div>      </div> where <em>&#x03B1;</em> = 3. If (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>+</sup>&#x2229;<em>E</em>      <sup>+</sup>, then <span class="inline-equation"><span class="tex">$w_{ij}^- = 0$</span>      </span> and inequality&#x00A0;(<a class="eqn" href="#eq11">11</a>) is trivial since the left hand side is zero. Similarly, inequality&#x00A0;(<a class="eqn" href="#eq12">12</a>) is trivial if (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>&#x2212;</sup>&#x2229;<em>E</em>      <sup>&#x2212;</sup>. Assume then that (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>+</sup>&#x2229;<em>E</em>      <sup>&#x2212;</sup>. Then <span class="inline-equation"><span class="tex">$w_{ij}^- = \lambda$</span>      </span> and <em>c<sub>ij</sub>      </em> = <em>&#x03BB;</em>(1 &#x2212; <em>x<sub>ij</sub>      </em>), and we know <em>x<sub>ij</sub>      </em> < 1/3&#x21D2;(1 &#x2212; <em>x<sub>ij</sub>      </em>) > 2/3. Therefore: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ w_{ij}^- = \lambda {\lt} 3\lambda \left(2/3\right) {\lt} 3\lambda (1-x_{ij})= \alpha c_{ij}. \] </span>       <br/>       </div>      </div> On the other hand, if (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>&#x2212;</sup>&#x2229;<em>E</em>      <sup>+</sup>, then <span class="inline-equation"><span class="tex">$w_{ij}^+ = (1-\lambda)$</span>      </span>, <em>c<sub>ij</sub>      </em> = (1 &#x2212; <em>&#x03BB;</em>)<em>x<sub>ij</sub>      </em>, and <em>x<sub>ij</sub>      </em> &#x2265; 1/3, so we see: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ w_{ij}^+ = (1-\lambda) = 3(1-\lambda)\left(1/3 \right) \le 3 (1-\lambda) x_{ij} = \alpha c_{ij}. \] </span>       <br/>       </div>      </div> This concludes the proof for inequalities&#x00A0;(<a class="eqn" href="#eq11">11</a>) and&#x00A0;(<a class="eqn" href="#eq12">12</a>). Next we consider a triplet of nodes {<em>i</em>, <em>j</em>, <em>k</em>} where (<em>i</em>, <em>j</em>) &#x2208; <em>F</em>      <sup>+</sup>, (<em>j</em>, <em>k</em>) &#x2208; <em>F</em>      <sup>+</sup> but (<em>i</em>, <em>k</em>) &#x2208; <em>F</em>      <sup>&#x2212;</sup>. This is called a <em>bad triangle</em> since we will have to violate at least one of these edges when clustering&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{G}$</span>      </span>. We must prove that: <div class="table-responsive" id="eq13">       <div class="display-equation">       <span class="tex mytex">\begin{equation} w_{ij}^+ + w_{jk}^+ + w_{ik}^- \le 3 \left(c_{ij} + c_{jk} + c_{ik} \right), \end{equation} </span>       <br/>       <span class="equation-number">(13)</span>       </div>      </div> which is somewhat tedious to show. The variables in&#x00A0;(<a class="eqn" href="#eq13">13</a>) are highly dependent on the types of edges shared among nodes {<em>i</em>, <em>j</em>, <em>k</em>} in the original signed graph <em>G</em>&#x2032;; there are two possibilities for each edge for a total of eight cases. We give a proof here for the first case. <em>Case 1: Assume (<em>i</em>, <em>j</em>) &#x2208; <em>E</em>       <sup>+</sup>, (<em>j</em>, <em>k</em>) &#x2208; <em>E</em>       <sup>+</sup>, and (<em>i</em>, <em>k</em>) &#x2208; <em>E</em>       <sup>&#x2212;</sup>.</em>. For this case, we note that (<em>c<sub>ij</sub>      </em>, <em>c<sub>jk</sub>      </em>, <em>c<sub>ik</sub>      </em>) = ((1 &#x2212; <em>&#x03BB;</em>)<em>x<sub>ij</sub>      </em>, (1 &#x2212; <em>&#x03BB;</em>)<em>x<sub>jk</sub>      </em>, <em>&#x03BB;</em>(1 &#x2212; <em>x<sub>ik</sub>      </em>)) and <span class="inline-equation"><span class="tex">$(w_{ij}^+,w_{jk}^+,w_{ik}^-) = (1-\lambda , 1-\lambda , \lambda)$</span>      </span>. By our construction of <em>F</em>      <sup>+</sup> and <em>F</em>      <sup>&#x2212;</sup> we know that <em>x<sub>ij</sub>      </em> < 1/3, <em>x<sub>jk</sub>      </em> < 1/3, and by the triangle inequality we have <em>x<sub>ik</sub>      </em> &#x2264; <em>x<sub>ij</sub>      </em> + <em>x<sub>jk</sub>      </em> < 2/3. Combining these facts: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} 3(c_{ij} &#x0026;+ c_{jk} + c_{ik}) = 3 \left((1-\lambda)(x_{ij} + x_{jk}) + \lambda (1-x_{ik}) \right) \\ &#x0026;\ge 3 \left((1-\lambda)x_{ik} + \lambda (1-x_{ik}) \right) = 3\left((1-2 \lambda) x_{ik} + \lambda \right) \\ &#x0026;{\gt} 3\left((1-2\lambda) 2/3 + \lambda \right) = 2-\lambda = w_{ij}^+ + w_{jk}^+ + w_{ik}^-.\end{align*} </span>       <br/>       </div>      </div> We rely above on the fact that (1 &#x2212; 2<em>&#x03BB;</em>) < 0, which restricts our proof to cases where <em>&#x03BB;</em> > 1/2. Due to space constraints we defer the proof of the other seven cases to the full version of the paper&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0039">39</a>].</p>    </div>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> 2-Approximation for Cluster Deletion</h3>     </div>    </header>    <p>In order to obtain an approximation algorithm for cluster deletion we alter <SmallCap>threeLP</SmallCap> in two ways:</p>    <ul class="list-no-style">     <li id="list9" label="&#x2022;">Begin by solving the LP relaxation of cluster deletion: <div class="table-responsive" id="Xeq1">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{array}{ll}\text{minimize} &#x0026; \sum _{(i,j) \in E^+} x_{ij} \\ \text{subject to}&#x0026; x_{ij} \le x_{ik} + x_{jk} \text{ for all $i,j,k$} \\ &#x0026; x_{ij} \in [0,1] \text{ for all $(i,j) \in E^+$} \\ &#x0026; x_{ij} = 1 \text{ for all $(i,j) \in E^-$}\end{array} \end{equation} </span>       <br/>       <span class="equation-number">(14)</span>       </div>      </div>      <br/></li>     <li id="list10" label="&#x2022;">Define <em>F</em>      <sup>+</sup> = {(<em>i</em>, <em>j</em>): <em>x<sub>ij</sub>      </em> < 1/2}, <em>F</em>      <sup>&#x2212;</sup> = {(<em>i</em>, <em>j</em>): <em>x<sub>ij</sub>      </em> &#x2265; 1/2}.<br/></li>    </ul>    <p>As before, we then run <SmallCap>Pivot</SmallCap> on <span class="inline-equation"><span class="tex">$\tilde{G} = (V,F^+,F^-)$</span>     </span>. We name the resulting procedure <SmallCap>twoCD</SmallCap>, and prove the following result:</p>    <div class="theorem" id="enc7">     <Label>Theorem 4.3.</Label>     <p> Algorithm <SmallCap>twoCD</SmallCap> returns a 2-approximation for cluster deletion.</p>    </div>    <div class="proof" id="proof5">     <Label>Proof.</Label>     <p> First observe that no negative edge mistakes are made by performing <SmallCap>Pivot</SmallCap> on <span class="inline-equation"><span class="tex">$\tilde{G}$</span>      </span>: if <em>k</em> is the pivot and <em>i</em>, <em>j</em> are two positive neighbors of <em>k</em> in <span class="inline-equation"><span class="tex">$\tilde{G}$</span>      </span>, then <em>x<sub>ik</sub>      </em> < 1/2, <em>x<sub>jk</sub>      </em> < 1/2, and <em>x<sub>ij</sub>      </em> &#x2264; <em>x<sub>ik</sub>      </em> + <em>x<sub>jk</sub>      </em> < 1. Since all distances are less than one, all nodes must share positive edges in <em>G</em>&#x2032;, because <em>x<sub>ij</sub>      </em> = 1 for any (<em>i</em>, <em>j</em>) &#x2208; <em>E</em>      <sup>&#x2212;</sup>. The rest of the proof follows from showing that the newly constructed graph <span class="inline-equation"><span class="tex">$\tilde{G}$</span>      </span> satisfies the assumptions of Theorem&#x00A0;<a class="enc" href="#enc5">4.1</a> when <em>&#x03B1;</em> = 2. For the sake of space we defer details to the full version of the paper&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0039">39</a>].</p>    </div>    <p>This result is particularly interesting given that no constant-factor approximation for cluster deletion has been explicitly presented in previous literature.<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> Until now, the best approximations for correlation clustering were stronger than any result known for cluster deletion; our result indicates that the latter problem is perhaps the easier of the two to approximate.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Scalable Heuristic Algorithms</h3>     </div>    </header>    <p>As a counterpart to the previous approximation-driven approaches, we provide fast algorithms for <SmallCap>LambdaCC</SmallCap> based on greedy local heuristics. The first of these is <SmallCap>GrowCluster</SmallCap>, which iteratively selects an unclustered node uniformly at random and forms a cluster around it by greedily aggregating adjacent nodes, until there is no more improvement to the <SmallCap>LambdaCC</SmallCap> objective.</p>    <p>A variant of this, called <SmallCap>GrowClique</SmallCap>, is specifically designed for cluster deletion. It monotonically improves the <SmallCap>LambdaCC</SmallCap> objective, but differs in that at each iteration it uniformly at random selects&#x00A0;<em>k</em> unclustered nodes, and greedily grows cliques around each of these seeds. The resulting cliques may overlap: at each iteration we select only the largest of such cliques.</p>    <p>Finally, since the <SmallCap>LambdaCC</SmallCap> and Hamiltonian objectives are equivalent, we can use previously developed algorithms and software for modularity-like objectives with a resolution parameter. In particular we employ adaptations of the <em>Louvain</em> method, an algorithm developed by Blondel et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. It iteratively visits each node in the graph and moves it to an adjacent cluster, if such a move gives a locally maximum increase in the modularity score. This continues until no move increases modularity, at which point the clusters are aggregated into super-nodes and the entire process is repeated on the aggregated network. By adapting the original Louvain method to make greedy local moves based on the <SmallCap>LambdaCC</SmallCap> objective, rather than modularity, we obtain a scalable algorithm that is known to provide good approximations for a related objective, and additionally adapts well to changes in our parameter&#x00A0;<em>&#x03BB;</em>. We refer to this as <SmallCap>Lambda-Louvain</SmallCap>. Both standard and degree-weighted versions of the algorithm can be achieved by employing existing generalized Louvain algorithms (e.g., the GenLouvain algorithm of Jeub et al. <a class="link-inline force-break" href="http://netwiki.amath.unc.edu/GenLouvain/">http://netwiki.amath.unc.edu/GenLouvain/</a>). Our heuristic algorithms satisfy the following guarantee:</p>    <div class="theorem" id="enc8">     <Label>Theorem 4.4.</Label>     <p> For every&#x00A0;<em>&#x03BB;</em>, both the algorithms <SmallCap>GrowCluster</SmallCap> and <SmallCap>Lambda-Louvain</SmallCap> either place all nodes in one cluster or they produce clusters that have scaled sparsest cut bounded above by&#x00A0;<em>&#x03BB;</em>.</p>    </div>    <p>An analogous result for normalized cut holds when the algorithms greedily optimize degree-weighted <SmallCap>LambdaCC</SmallCap>. We provide a detailed proof in the full version of the paper&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>].</p>    </section>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>    </div>    </header>    <p>We begin by comparing our new methods against existing correlation clustering algorithms on several small networks. This shows our algorithms for <SmallCap>LambdaCC</SmallCap> are superior to common alternatives. We then study how well-known graph partitioning algorithms implicitly optimize the <SmallCap>LambdaCC</SmallCap> objective for various&#x00A0;<em>&#x03BB;</em>. In subsequent experiments, we apply our methods to clique detection in collaboration and gene networks, and to social network analysis.</p>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span>       <SmallCap>LambdaCC</SmallCap> on Small Networks</h3>     </div>    </header>    <p>In our first experiment, we show that <SmallCap>Lambda-Louvain</SmallCap> is the best general-purpose correlation clustering method for minimizing the <SmallCap>LambdaCC</SmallCap> objective. We test this on four small networks: Karate&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>], Les Mis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], Polbooks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>], and Football&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. Figure&#x00A0;<a class="fig" href="#fig3">3</a> shows the performance of our algorithms, <SmallCap>Pivot</SmallCap>, and&#x00A0;<SmallCap>ICM</SmallCap>, for a range of&#x00A0;<em>&#x03BB;</em> values. <SmallCap>Pivot</SmallCap> is the fast algorithm of Ailon et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], which selects a uniform random node and clusters its neighbors with it. <SmallCap>ICM</SmallCap> is the energy-minimization heuristic algorithm of Bagon and Galun&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186110/images/www2018-119-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">We optimize the standard <SmallCap>LambdaCC</SmallCap> objective with five correlation clustering algorithms on four small networks. The&#x00A0;<em>y</em>-axis reports the ratio between each algorithm&#x0027;s score and the lower bound on the optimal objective determined by solving the LP relaxation.<SmallCap>Lambda-Louvain</SmallCap> (black) and <SmallCap>GrowCluster</SmallCap> (purple) perform well for all&#x00A0;<em>&#x03BB;</em>, in addition to being the most scalable algorithms. In each plot, a dashed vertical line indicates the optimal scaled sparsest cut value,&#x00A0;<em>&#x03BB;</em>       <sup>*</sup>, for that network.</span>      </div>     </figure>    </p>    <p>We find that&#x00A0;<SmallCap>threeLP</SmallCap> gives much better than a 3-approximation in practice. <SmallCap>Pivot</SmallCap> is much faster, but performs poorly for&#x00A0;<em>&#x03BB;</em> close to&#x00A0;0 or&#x00A0;1. <SmallCap>ICM</SmallCap> is also much quicker than solving the LP relaxation, but is still limited in scalability, as it is intended for correlation clustering problems where most edge weights are&#x00A0;0 (not the case for <SmallCap>LambdaCC</SmallCap>). On the other hand, <SmallCap>GrowCluster</SmallCap> and <SmallCap>Lambda-Louvain</SmallCap> are scalable and give good approximations for all input networks and values of&#x00A0;<em>&#x03BB;</em>.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Standard Clustering Algorithms</h3>     </div>    </header>    <p>Many existing clustering algorithms implicitly optimize different parameter regimes of the <SmallCap>LambdaCC</SmallCap> objective. We show this by running several clustering algorithms on a 1000-node synthetic graph generated from the BTER model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>]. We do the same on the largest component (4158 nodes) of the ca-GrQc collaboration network from the <em>arXiv</em> e-print website. We then compute <SmallCap>LambdaCC</SmallCap> objective scores for each algorithm for a range of&#x00A0;<em>&#x03BB;</em> values. We first cluster each graph using Graclus&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] (forming two clusters), Infomap&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>], and Louvain&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. To form dense clusters, we also partition the networks by recursively extracting the maximum clique (called <SmallCap>RMC</SmallCap>), and by recursively extracting the maximum quasi-clique (<SmallCap>RMQC</SmallCap>), i.e., the largest set of nodes with inner edge density bounded below by some&#x00A0;<em>&#x03C1;</em> < 1 (here we use&#x00A0;<em>&#x03C1;</em> = 0.6). The last two procedures must solve an NP-hard objective at each step, but for reasonably sized graphs there is available clique and quasi-clique detection software&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>].</p>    <p>After each algorithm has produced a single clustering of the unsigned network, we evaluate how the&#x00A0;<SmallCap>LambdaCC</SmallCap> objective score of that clustering changes as we vary <em>&#x03BB;</em>. This allows us to observe whether an algorithm is effective in approximating the&#x00A0;<SmallCap>LambdaCC</SmallCap> objective for a certain range of&#x00A0;<em>&#x03BB;</em> values. For comparison we run <SmallCap>Lambda-Louvain</SmallCap> for each different value of&#x00A0;<em>&#x03BB;</em>. Figure&#x00A0;<a class="fig" href="#fig4">4</a> reports the ratio between the <SmallCap>LambdaCC</SmallCap> objective score of each clustering and the LP-relaxation lower bound. These plots illustrate that our framework and <SmallCap>Lambda-Louvain</SmallCap> effectively interpolate between several well-established strategies in graph partitioning, and can serve as a good proxy for any clustering task for which any one of these algorithms is known to be effective. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186110/images/www2018-119-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">We illustrate the performance of well-known clustering algorithms in approximating the <SmallCap>LambdaCC</SmallCap> objective on (a) one synthetic and (b) and one real-world graph. The bowl-shaped curves indicate that each algorithm implicitly optimizes the <SmallCap>LambdaCC</SmallCap> objective in a different parameter regime. The <em>y</em>-axis reports the ratio between each clustering&#x0027;s objective score and the LP-relaxation lower bound. <SmallCap>Lambda-Louvain</SmallCap> effectively interpolates between all the clustering strategies seen here.</span>      </div>     </figure>    </p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Cluster Deletion in Large Collaboration Networks</h3>     </div>    </header>    <p>The connection between <SmallCap>LambdaCC</SmallCap> and cluster deletion provides a new approach for enumerating groups in large networks. Here we evaluate <SmallCap>GrowClique</SmallCap> for cluster deletion and use it to cluster two large collaboration networks, one formed from a snapshot of the author-paper DBLP dataset in&#x00A0;2007, and the other generated using actor-movie information from the NotreDame actors dataset&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. The original data in both cases is a bipartite network indicating which <em>players</em> (i.e., authors or actors) have parts in different <em>projects</em> (papers or movies respectively). We transform each bipartite network into a graph in which nodes are players and edges represent collaboration on a project.</p>    <p>At each iteration <SmallCap>GrowClique</SmallCap> grows&#x00A0;500 (possibly overlapping) cliques from random seeds and selects the largest to be included in the final output. We compare against <SmallCap>RMC</SmallCap>, an expensive method which provably returns a 2-approximation to the optimal cluster deletion objective&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>]. We also design <SmallCap>ProjectClique</SmallCap>, a method that looks at the original bipartite network and recursively identifies the project associated with the largest number of players not yet assigned to a cluster. These players form a clique in the collaboration network, so <SmallCap>ProjectClique</SmallCap> clusters them together, then repeats the procedure on remaining nodes.</p>    <p>Table&#x00A0;<a class="tbl" href="#tab2">2</a> shows that <SmallCap>GrowClique</SmallCap> outperforms <SmallCap>ProjectClique</SmallCap> in both cases, and slightly outperforms <SmallCap>RMC</SmallCap> on the actor network. Our method is therefore competitive against two algorithms that in some sense have an unfair advantage over it: <SmallCap>ProjectClique</SmallCap> employs knowledge not available to <SmallCap>GrowClique</SmallCap> regarding the original bipartite dataset, and <SmallCap>RMC</SmallCap> performs very well mainly because it solves an NP-hard problem at each step.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Cluster deletion scores for <SmallCap>GrowClique</SmallCap> (GC), <SmallCap>ProjectClique</SmallCap> (PC) and <SmallCap>RMC</SmallCap> on two collaboration networks. <SmallCap>GrowClique</SmallCap> is unaware of the underlying player-project network, and does not solve an NP-hard objective at each iteration, yet returns very good results. Best score for each dataset is <em>emphasized</em>.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Graph</th>       <th style="text-align:center;">Nodes</th>       <th style="text-align:center;">Edges</th>       <th style="text-align:center;">        <SmallCap>GC</SmallCap>       </th>       <th>        <SmallCap>PC</SmallCap>       </th>       <th>        <SmallCap>RMC</SmallCap>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Actors</td>       <td style="text-align:center;">341,185</td>       <td style="text-align:center;">10,643,420</td>       <td style="text-align:center;">        <em>8,085,286</em>       </td>       <td>8,086,715</td>       <td>8,087,241</td>       </tr>       <tr>       <td style="text-align:left;">DBLP</td>       <td style="text-align:center;">526,303</td>       <td style="text-align:center;">1,616,814</td>       <td style="text-align:center;">945,489</td>       <td>946,295</td>       <td>        <em>944,087</em>       </td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Clustering Yeast Genes</h3>     </div>    </header>    <p>The study of cluster deletion and cluster editing (equivalent to &#x00B1; 1-correlation clustering, or to <em>&#x03BB;</em> = 1/2) was originally motivated by applications to clustering genes using expression patterns&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]. Standard <SmallCap>LambdaCC</SmallCap> is a natural framework for this, since it generalizes both objectives and interpolates between them as&#x00A0;<em>&#x03BB;</em> ranges from&#x00A0;1/2 to&#x00A0;<em>m</em>/(<em>m</em> + 1). We cluster genes of the <em>Saccharomyces cerevisiae</em> yeast organism using microarray expression data collected by Kemmeren et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. With the&#x00A0;200 expression values from the dataset, we compute correlation coefficients between all pairs of genes. We threshold these at&#x00A0;0.9 to obtain a small graph of&#x00A0;131 nodes corresponding to unique genes, which we cluster with <SmallCap>twoCD</SmallCap>. For this cluster deletion experiment, our algorithm returns the optimal solution: solving the LP-relaxation returns a solution that is in fact integral. We validate each clique of size at least three returned by <SmallCap>twoCD</SmallCap> against known gene-association data from the Saccharomyces Genome Database (SGD) and the String Consortium Database (see Table&#x00A0;<a class="tbl" href="#tab3">3</a>). With one exception, these cliques match groups of genes that are known to be strongly associated, according to at least one validation database. The exception is a cluster with four genes (YHR093W, YIL171W, YDR490C, and YOR225W), three of which, according to the SGD are not known to be associated with any Gene Ontology term. We conjecture that this may indicate a relationship between genes not previously known to be related.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">We list cliques of size&#x00A0; &#x2265; 3 in the optimal clustering (found by <SmallCap>twoCD</SmallCap>) of a network of&#x00A0;131 yeast genes. We validate each cluster using the SGD GO slim mapper tool, which identifies any GO term (function, process, or component of the organism) for a given gene. We list one GO term shared by all genes in the cluster, if one exists. The Term&#x00A0;<span class="inline-equation"><span class="tex">$\%$</span>       </span> column reports the percentage of all genes in the organism associated with this term. A low percentage indicates a cluster of genes that share a GO term that is not widely shared among other genes. The final column shows the minimum String association score between every pair of genes in the cluster, a number between&#x00A0;0 and&#x00A0;1000 (higher is better). Any non-zero score is a strong indication of gene association, as the majority of String scores between genes of <em>S.&#x00A0;cerevisiae</em> are zero. All clusters, except the third, either have a high minimum String score or are all associated with a specific GO term.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Clique #</th>       <th style="text-align:left;">Size</th>       <th style="text-align:left;">Shared GO term</th>       <th style="text-align:right;">Term <span class="inline-equation"><span class="tex">$\%$</span>        </span>       </th>       <th style="text-align:center;">String</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">1</td>       <td style="text-align:left;">6</td>       <td style="text-align:left;">nucleus</td>       <td style="text-align:right;">34.3</td>       <td style="text-align:center;">0</td>       </tr>       <tr>       <td style="text-align:center;">2</td>       <td style="text-align:left;">4</td>       <td style="text-align:left;">nucleus</td>       <td style="text-align:right;">34.3</td>       <td style="text-align:center;">202</td>       </tr>       <tr>       <td style="text-align:center;">3</td>       <td style="text-align:left;">4</td>       <td style="text-align:left;">N/A</td>       <td style="text-align:right;">-</td>       <td style="text-align:center;">0</td>       </tr>       <tr>       <td style="text-align:center;">4</td>       <td style="text-align:left;">4</td>       <td style="text-align:left;">vitamin metabolic process</td>       <td style="text-align:right;">0.7</td>       <td style="text-align:center;">980</td>       </tr>       <tr>       <td style="text-align:center;">5</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">cytoplasm</td>       <td style="text-align:right;">67.0</td>       <td style="text-align:center;">990</td>       </tr>       <tr>       <td style="text-align:center;">6</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">cytoplasm</td>       <td style="text-align:right;">67.0</td>       <td style="text-align:center;">998</td>       </tr>       <tr>       <td style="text-align:center;">7</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">N/A</td>       <td style="text-align:right;">-</td>       <td style="text-align:center;">962</td>       </tr>       <tr>       <td style="text-align:center;">8</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">cytoplasm</td>       <td style="text-align:right;">67.0</td>       <td style="text-align:center;">996</td>       </tr>       <tr>       <td style="text-align:center;">9</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">N/A</td>       <td style="text-align:right;">-</td>       <td style="text-align:center;">973</td>       </tr>       <tr>       <td style="text-align:center;">10</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">transposition</td>       <td style="text-align:right;">1.7</td>       <td style="text-align:center;">0</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-25">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.5</span> Social Network Analysis with <SmallCap>LambdaCC</SmallCap>      </h3>     </div>    </header>    <p>Clustering a social network using a range of resolution parameters can reveal valuable insights about how links are formed in the network. Here we examine several graphs from the Facebook100 dataset, each of which represents the induced subgraph of the Facebook network corresponding to a US university at some point in&#x00A0;2005. The networks come with anonymized meta-data, reporting attributes such as major and graduation year for each node. While meta-data attributes are not expected to correspond to ground-truth communities in the network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], we do expect them to play a role in how friendship links and communities are formed. In this experiment we illustrate strong correlations between the link structure of the networks and the dorm, graduation year, and student/faculty status meta-data attributes. We also see how these correlations are revealed, to different degrees, depending on our choice of&#x00A0;<em>&#x03BB;</em>.</p>    <p>Given a Facebook subgraph with&#x00A0;<em>n</em> nodes, we cluster it with degree-weighted <SmallCap>Lambda-Louvain</SmallCap> for a range of&#x00A0;<em>&#x03BB;</em> values between 0.005/<em>n</em> and 0.25/<em>n</em>. In this clustering, we refer to two nodes in the same cluster as an <em>interior pair</em>. We measure how well a meta-data attribute&#x00A0;<em>M</em> correlates with the clustering by calculating the proportion of interior pairs that share the same value for&#x00A0;<em>M</em>. This value, denoted by&#x00A0;<em>P</em>(<em>M</em>), can also be interpreted as the probability of selecting an interior pair uniformly at random and finding that they agree on attribute&#x00A0;<em>M</em>. To determine whether the probability is meaningful, we compare it against a null probability <span class="inline-equation"><span class="tex">$P(\tilde{M})$</span>     </span>: the probability that a random interior pair agree at a <em>fake</em> meta-data attribute&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{M}$</span>     </span>. We assign to each node a value for the fake attribute&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{M}$</span>     </span> by performing a random permutation on the vector storing values for true attribute&#x00A0;<em>M</em>. In this way, we can compare each true attribute&#x00A0;<em>M</em> against a fake attribute&#x00A0;<span class="inline-equation"><span class="tex">$\tilde{M}$</span>     </span> that has the same exact proportion of nodes with each attribute value, but does not impart any true information regarding each node.</p>    <p>In Figure&#x00A0;<a class="fig" href="#fig5">5</a> we plot results for each of the three attributes <em>M</em> &#x2208; {<em>dorm</em>,&#x2009;&#x2009;<em>s/f</em>&#x2009;(student/faculty)} on four Facebook networks, as&#x00A0;<em>&#x03BB;</em> is varied. In all cases, we see significant differences between&#x00A0;<em>P</em>(<em>M</em>) and&#x00A0;<span class="inline-equation"><span class="tex">$P({\tilde{M}})$</span>     </span>. In general,&#x00A0;<em>P</em>(and <em>P</em>(<em>s/f</em>) reach a peak at small values of&#x00A0;<em>&#x03BB;</em> when clusters are large, whereas <em>P</em>(<em>dorm</em>) is highest when&#x00A0;<em>&#x03BB;</em> is large and clusters are small. This indicates that the first two attributes are more highly correlated with large sparse communities in the network, whereas sharing a dorm is more correlated with smaller, denser communities. Caltech, a small residential university, is an exception to these trends and exhibits a much stronger correlation with the dorm attribute, even for very small&#x00A0;<em>&#x03BB;</em>. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186110/images/www2018-119-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">On four university Facebook graphs, we illustrate that the dorm (red), graduation year (green), and student/faculty (S/F) status (blue) meta-data attributes all correlate highly with the clustering found by <SmallCap>Lambda-Louvain</SmallCap> for each&#x00A0;<em>&#x03BB;</em>. Above the <em>x</em>-axis we show the number of clusters formed, which increases with&#x00A0;<em>&#x03BB;</em>. The <em>y</em>-axis reports the probability that two nodes sharing a cluster also share an attribute value. Each attribute curve is compared against a null probability, shown as a dashed line of the same color.The large gaps between each attribute curve and its null probability indicate that the link structure of each network is highly correlated with these attributes. In general, probabilities for <em>ands/fstatusarehighestforsmall</em>~, <em>whereasdorm</em>       <em>hasahighercorrelationwithsmaller</em>, <em>densercommunitiesinthenetwork</em>.<em>Caltechisanexceptiontothegeneraltrend</em>; <em>seethemaintextfordiscussion</em>.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Discussion</h2>    </div>    </header>    <p>We have introduced a new clustering framework that unifies several other commonly-used objectives and offers many attractive theoretical properties. We prove that our objective function interpolates between the sparsest cut objective and the cluster deletion problem, as we vary a single input parameter,&#x00A0;<em>&#x03BB;</em>. We give a 3-approximation algorithm for our objective when <em>&#x03BB;</em> &#x2265; 1/2, and a related method which improves the best approximation factor for cluster deletion from 3 to 2. We also give scalable procedures for greedily improving our objective, which are successful in a wide variety of clustering applications. These methods are easily modified to add must-cluster and cannot-cluster constraints, which makes them amenable to many applications. In future work, we will continue exploring approximations when&#x00A0;<em>&#x03BB;</em> < 1/2.</p>   </section>   <section id="sec-27">    <header>    <div class="title-info">     <h2>Acknowledgements</h2>    </div>    </header>    <p>This work was supported by several funding agencies: Nate Veldt and David Gleich are supported by NSF award IIS-154648, David Gleich is additionally supported by NSF awards CCF-1149756 and CCF-093937 as well as the DARPA Simplex program and the Sloan Foundation. Anthony Wirth is supported by the Australian Research Council. We thank Flavio Chierichetti for several helpful conversations and also thank the anonymous reviewers for several helpful suggestions for improving our work, in particular for mentioning connections to the work of van Zuylen and Williamson&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], which led to significantly improved approximation results.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Nir Ailon, Moses Charikar, and Alantha Newman. 2008. Aggregating inconsistent information: ranking and clustering. <em>      <em>Journal of the ACM (JACM)</em>     </em>55, 5 (2008), 23.</li>    <li id="BibPLXBIB0002" label="[2]">Sanjeev Arora, Satish Rao, and Umesh Vazirani. 2009. Expander flows, geometric embeddings and graph partitioning. <em>      <em>Journal of the ACM (JACM)</em>     </em>56, 2 (2009).</li>    <li id="BibPLXBIB0003" label="[3]">Shai Bagon and Meirav Galun. 2011. Large Scale Correlation Clustering Optimization. <em>      <em>arXiv</em>     </em>cs.CV(2011), 1112.2903.</li>    <li id="BibPLXBIB0004" label="[4]">Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation Clustering. <em>      <em>Machine Learning</em>     </em>56(2004), 89&#x2013;113.</li>    <li id="BibPLXBIB0005" label="[5]">Albert-L&#x00E1;szl&#x00F3; Barab&#x00E1;si and R&#x00E9;ka Albert. 1999. Emergence of Scaling in Random Networks. <em>      <em>Science</em>     </em>286, 5439 (1999), 509&#x2013;512. <a href="https://doi.org/10.1126/science.286.5439.509" target="_blank">https://doi.org/10.1126/science.286.5439.509</a></li>    <li id="BibPLXBIB0006" label="[6]">Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. 1999. Clustering gene expression patterns. <em>      <em>Journal of computational biology</em>     </em>6, 3-4 (1999), 281&#x2013;297.</li>    <li id="BibPLXBIB0007" label="[7]">Vincent&#x00A0;D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. <em>      <em>Journal of Statistical Mechanics: Theory and Experiment</em>     </em>2008, 10(2008), P10008. <a href="http://stacks.iop.org/1742-5468/2008/i=10/a=P10008" target="_blank">http://stacks.iop.org/1742-5468/2008/i=10/a=P10008</a></li>    <li id="BibPLXBIB0008" label="[8]">Sebastian B&#x00F6;cker and Peter Damaschke. 2011. Even faster parameterized cluster deletion and cluster editing. <em>      <em>Inform. Process. Lett.</em>     </em>111, 14 (2011), 717 &#x2013; 721. <a href="https://doi.org/10.1016/j.ipl.2011.05.003" target="_blank">https://doi.org/10.1016/j.ipl.2011.05.003</a></li>    <li id="BibPLXBIB0009" label="[9]">Ludvig Bohlin, Daniel Edler, Andrea Lancichinetti, and Martin Rosvall. 2014. Community detection and visualization of networks with the map equation framework. In <em>      <em>Measuring Scholarly Impact</em>     </em>. Springer, 3&#x2013;34.</li>    <li id="BibPLXBIB0010" label="[10]">Flavia Bonomo, Guillermo Duran, Amedeo Napoli, and Mario Valencia-Pabon. 2015. A one-to-one correspondence between potential solutions of the cluster deletion problem and the minimum sum coloring problem, and its application to P4-sparse graphs. <em>      <em>Inform. Process. Lett.</em>     </em>115 (2015), 600&#x2013;603.</li>    <li id="BibPLXBIB0011" label="[11]">Flavia Bonomo, Guillermo Duran, and Mario Valencia-Pabon. 2015. Complexity of the cluster deletion problem on subclasses of chordal graphs. <em>      <em>Theoretical Computer Science</em>     </em>600 (2015), 59&#x2013;69.</li>    <li id="BibPLXBIB0012" label="[12]">Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. 2005. Clustering with qualitative information. <em>      <em>J. Comput. System Sci.</em>     </em>71, 3 (2005), 360 &#x2013; 383. Learning Theory 2003.</li>    <li id="BibPLXBIB0013" label="[13]">Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. 2015. Near optimal LP rounding algorithm for correlation clustering on complete and complete <em>k</em>-partite graphs. In <em>      <em>Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing</em>     </em>. ACM, 219&#x2013;228.</li>    <li id="BibPLXBIB0014" label="[14]">Peter Damaschke. 2009. <em>      <em>Bounded-Degree Techniques Accelerate Some Parameterized Graph Algorithms</em>     </em>. Springer Berlin Heidelberg, Berlin, Heidelberg, 98&#x2013;109. <a href="https://doi.org/10.1007/978-3-642-11269-0_8" target="_blank">https://doi.org/10.1007/978-3-642-11269-0_8</a></li>    <li id="BibPLXBIB0015" label="[15]">J.-C. Delvenne, Sophia&#x00A0;N Yaliraki, and Mauricio Barahona. 2010. Stability of graph communities across time scales. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>107, 29(2010), 12755&#x2013;12760.</li>    <li id="BibPLXBIB0016" label="[16]">Erik&#x00A0;D. Demaine and Nicole Immorlica. 2003. Correlation Clustering with Partial Information. In <em>      <em>Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques: 6th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems, APPROX 2003 and 7th International Workshop on Randomization and Approximation Techniques in Computer Science, RANDOM 2003, Princeton, NJ, USA, August 24-26, 2003. Proceedings</em>     </em>, Sanjeev Arora, Klaus Jansen, Jos&#x00E9; D.&#x00A0;P. Rolim, and Amit Sahai (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1&#x2013;13. 978-3-540-45198-3</li>    <li id="BibPLXBIB0017" label="[17]">Anders Dessmark, Jesper Jansson, Andrezej Lingas, Eva-Marta Lundell, and Mia Person. 2007. On the Approximability of Maximum and Minimum Edge Clique Partition Problems. <em>      <em>International Journal of Foundations of Computer Science</em>     </em>18, 02(2007), 217&#x2013;226.</li>    <li id="BibPLXBIB0018" label="[18]">Inderjit&#x00A0;S Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cuts without eigenvectors a multilevel approach. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>29, 11(2007).</li>    <li id="BibPLXBIB0019" label="[19]">Thang&#x00A0;N Dinh, Xiang Li, and My&#x00A0;T Thai. 2015. Network clustering via maximizing modularity: Approximation algorithms and theoretical limits. In <em>      <em>Proceedings of the 2015 IEEE International Conference on Data Mining (ICDM)</em>     </em>. IEEE, 101&#x2013;110.</li>    <li id="BibPLXBIB0020" label="[20]">Dotan Emanuel and Amos Fiat. 2003. Correlation Clustering &#x2013; Minimizing Disagreements on Arbitrary Weighted Graphs. In <em>      <em>Algorithms - ESA 2003: 11th Annual European Symposium, Budapest, Hungary, September 16-19, 2003. Proceedings</em>     </em>, Giuseppe Di&#x00A0;Battistaand Uri Zwick (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 208&#x2013;220. <a href="https://doi.org/10.1007/978-3-540-39658-1_21" target="_blank">https://doi.org/10.1007/978-3-540-39658-1_21</a></li>    <li id="BibPLXBIB0021" label="[21]">Yong Gao, Donovan&#x00A0;R Hare, and James Nastos. 2013. The cluster deletion problem for cographs. <em>      <em>Discrete Mathematics</em>     </em>313, 23 (2013), 2763&#x2013;2771.</li>    <li id="BibPLXBIB0022" label="[22]">Michelle Girvan and Mark&#x00A0;EJ Newman. 2002. Community structure in social and biological networks. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>99, 12 (2002), 7821&#x2013;7826.</li>    <li id="BibPLXBIB0023" label="[23]">Jens Gramm, Jiong Guo, Falk H&#x00FC;ffner, and Rolf Niedermeier. 2003. Graph-Modeled Data Clustering: Fixed-Parameter Algorithms for Clique Generation. In <em>      <em>Algorithms and Complexity: 5th Italian Conference, CIAC 2003, Rome, Italy, May 28&#x2013;30, 2003. Proceedings</em>     </em>, Rossella Petreschi, Giuseppe Persiano, and Riccardo Silvestri (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 108&#x2013;119. <a href="https://doi.org/10.1007/3-540-44849-7_17" target="_blank">https://doi.org/10.1007/3-540-44849-7_17</a></li>    <li id="BibPLXBIB0024" label="[24]">Jens Gramm, Jiong Guo, Falk H&#x00FC;ffner, and Rolf Niedermeier. 2004. Automated Generation of Search Tree Algorithms for Hard Graph Modification Problems. <em>      <em>Algorithmica</em>     </em>39, 4 (2004), 321&#x2013;347. <a href="https://doi.org/10.1007/s00453-004-1090-5" target="_blank">https://doi.org/10.1007/s00453-004-1090-5</a></li>    <li id="BibPLXBIB0025" label="[25]">Patrick Kemmeren, Katrin Sameith, Loes&#x00A0;AL van&#x00A0;de Pasch, Joris&#x00A0;J Benschop, Tineke&#x00A0;L Lenstra, Thanasis Margaritis, Eoghan O&#x0027;Duibhir, Eva Apweiler, Sake van Wageningen, Cheuk&#x00A0;W Ko, <em>et al.</em> 2014. Large-scale genetic perturbations reveal regulatory networks and an abundance of gene-specific repressors. <em>      <em>Cell</em>     </em>157, 3 (2014), 740&#x2013;752.</li>    <li id="BibPLXBIB0026" label="[26]">D.&#x00A0;E. Knuth. 1993. <em>      <em>The Stanford GraphBase: A Platform for Combinatorial Computing</em>     </em>. Addison-Wesley, Reading, MA.</li>    <li id="BibPLXBIB0027" label="[27]">V. Krebs. 2004. Books about US Politics. (2004). <a href="http://networkdata.ics.uci.edu/data.php?d=polbooks" target="_blank">http://networkdata.ics.uci.edu/data.php?d=polbooks</a> Hosted at UCI Data Repository.</li>    <li id="BibPLXBIB0028" label="[28]">Guimei Liu and Limsoon Wong. 2008. Effective Pruning Techniques for Mining Quasi-Cliques. In <em>      <em>Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2008, Antwerp, Belgium, September 15-19, 2008, Proceedings, Part II</em>     </em>, Walter Daelemans, Bart Goethals, and Katharina Morik (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 33&#x2013;49. <a href="https://doi.org/10.1007/978-3-540-87481-2_3" target="_blank">https://doi.org/10.1007/978-3-540-87481-2_3</a></li>    <li id="BibPLXBIB0029" label="[29]">Assaf Natanzon, Ron Shamir, and Roded Sharan. 1999. Complexity classification of some edge modification problems. In <em>      <em>International Workshop on Graph-Theoretic Concepts in Computer Science</em>     </em>. Springer, 65&#x2013;77.</li>    <li id="BibPLXBIB0030" label="[30]">Mark&#x00A0;EJ Newman. 2006. Finding community structure in networks using the eigenvectors of matrices. <em>      <em>Physical review E</em>     </em>74, 3 (2006), 036104.</li>    <li id="BibPLXBIB0031" label="[31]">Mark&#x00A0;EJ Newman and Michelle Girvan. 2004. Finding and evaluating community structure in networks. <em>      <em>Physical review E</em>     </em>69, 026113 (2004).</li>    <li id="BibPLXBIB0032" label="[32]">Leto Peel, Daniel&#x00A0;B. Larremore, and Aaron Clauset. 2017. The ground truth about metadata and community detection in networks. <em>      <em>Science Advances</em>     </em>3, 5 (2017). <a href="https://doi.org/10.1126/sciadv.1602548" target="_blank">https://doi.org/10.1126/sciadv.1602548</a> arXiv: <a href="http://advances.sciencemag.org/content/3/5/e1602548.full.pdf" target="_blank">http://advances.sciencemag.org/content/3/5/e1602548.full.pdf</a></li>    <li id="BibPLXBIB0033" label="[33]">J&#x00F6;rg Reichardt and Stefan Bornholdt. 2006. Statistical mechanics of community detection. <em>      <em>Physical Review E</em>     </em>74, 016110 (2006).</li>    <li id="BibPLXBIB0034" label="[34]">Ryan&#x00A0;A. Rossi, David&#x00A0;F. Gleich, and Assefaw&#x00A0;H. Gebremedhin. 2015. Parallel Maximum Clique Algorithms with Applications to Network Analysis. <em>      <em>SIAM Journal on Scientific Computing</em>     </em>37, 5 (2015), C589&#x2013;C616. <a href="https://doi.org/10.1137/14100018X" target="_blank">https://doi.org/10.1137/14100018X</a></li>    <li id="BibPLXBIB0035" label="[35]">C. Seshadhri, Tamara&#x00A0;G. Kolda, and Ali Pinar. 2012. Community Structure and Scale-free Collections of Erd&#x0151;s-R&#x00E9;nyi Graphs. <em>      <em>Physical Review&#x00A0;E</em>     </em>85, 5, Article 056109 (May 2012). <a href="https://doi.org/10.1103/PhysRevE.85.056109" target="_blank">https://doi.org/10.1103/PhysRevE.85.056109</a></li>    <li id="BibPLXBIB0036" label="[36]">Ron Shamir, Roded Sharan, and Dekel Tsur. 2004. Cluster graph modification problems. <em>      <em>Discrete Applied Mathematics</em>     </em>144 (2004), 173&#x2013;182.</li>    <li id="BibPLXBIB0037" label="[37]">Chaitanya Swamy. 2004. Correlation clustering: maximizing agreements via semidefinite programming. In <em>      <em>Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em>     </em>. Society for Industrial and Applied Mathematics, 526&#x2013;527.</li>    <li id="BibPLXBIB0038" label="[38]">Anke van Zuylen and David&#x00A0;P. Williamson. 2009. Deterministic Pivoting Algorithms for Constrained Ranking and Clustering Problems. <em>      <em>Mathematics of Operations Research</em>     </em>34, 3 (2009), 594&#x2013;620. <a href="https://doi.org/10.1287/moor.1090.0385" target="_blank">https://doi.org/10.1287/moor.1090.0385</a> arXiv: <a href="https://doi.org/10.1287/moor.1090.0385" target="_blank">https://doi.org/10.1287/moor.1090.0385</a></li>    <li id="BibPLXBIB0039" label="[39]">Nate Veldt, David Gleich, and Tony Wirth. 2017. Unifying Sparsest Cut, Cluster Deletion, and Modularity Clustering Objectives with Correlation Clustering. <em>      <em>arXiv</em>     </em>cs.DS(2017). <a href="https://arxiv.org/abs/1712.05825" target="_blank">https://arxiv.org/abs/1712.05825</a></li>    <li id="BibPLXBIB0040" label="[40]">W.W. Zachary. 1977. An information flow model for conflict and fission in small groups. <em>      <em>Journal of Anthropological Research</em>     </em>33 (1977), 452&#x2013;473.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>For the initial submission of our work we presented a 5-approximation for <SmallCap>LambdaCC</SmallCap> when <em>&#x03BB;</em> > 1/2 and a 4-approximation for cluster deletion by altering the approach of Charikar et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0012">12</a>]. Here we show improved approximations that we developed based on a helpful suggestion from an anonymous reviewer. We include the original approximation results in the full version of our paper&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0039">39</a>].</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>The results of van Zuylen and Williamson for constrained correlation clustering can be used to obtain a 3-approximation for cluster deletion (Theorem 4.2 in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0038">38</a>]), though cluster deletion is not mentioned explicitly in their work.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW' 18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.</p> 			<p>ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186110">https://doi.org/10.1145/3178876.3186110</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
