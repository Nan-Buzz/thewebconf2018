<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Inferring Missing Categorical Information in Noisy and Sparse Web Markup</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186028"/></head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186028'>https://doi.org/10.1145/3178876.3186028</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186028'>https://w3id.org/oa/10.1145/3178876.3186028</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Inferring Missing Categorical Information in Noisy and Sparse Web Markup</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Nicolas</span>     <span class="surName">Tempelmeier</span>,     L3S Research Center, Leibniz Universit&#x00E4;t Hannover, Hannover, Germany, <a href="mailto:tempelmeier@L3S.de">tempelmeier@L3S.de</a>    </div>    <div class="author">     <span class="givenName">Elena</span>     <span class="surName">Demidova</span>,     L3S Research Center, Leibniz Universit&#x00E4;t Hannover, Hannover, Germany, <a href="mailto:demidova@L3S.de">demidova@L3S.de</a>    </div>    <div class="author">     <span class="givenName">Stefan</span>     <span class="surName">Dietze</span>,     L3S Research Center, Leibniz Universit&#x00E4;t Hannover, Hannover, Germany, <a href="mailto:dietze@L3S.de">dietze@L3S.de</a>    </div>                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186028" target="_blank">https://doi.org/10.1145/3178876.3186028</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Embedded markup of Web pages has seen widespread adoption throughout the past years driven by standards such as RDFa and Microdata and initiatives such as schema.org, where recent studies show an adoption by 39% of all Web pages already in 2016. While this constitutes an important information source for tasks such as Web search, Web page classification or knowledge graph augmentation, individual markup nodes are usually sparsely described and often lack essential information. For instance, from 26 million nodes describing events within the Common Crawl in 2016, 59% of nodes provide less than six statements and only 257,000 nodes (0.96%) are typed with more specific event subtypes. Nevertheless, given the scale and diversity of Web markup data, nodes that provide missing information can be obtained from the Web in large quantities, in particular for categorical properties. Such data constitutes potential training data for inferring missing information to significantly augment sparsely described nodes. In this work, we introduce a supervised approach for inferring missing categorical properties in Web markup. Our experiments, conducted on properties of events and movies, show a performance of 79% and 83% F1 score correspondingly, significantly outperforming existing baselines.</small>    </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Nicolas Tempelmeier, Elena Demidova, Stefan Dietze. 2018. Inferring Missing Categorical Information in Noisy and Sparse Web Markup. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA 10 pages. <a href="https://doi.org/10.1145/3178876.3186028" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186028</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Web search in general and the interpretation of Web documents in particular are increasingly being supported through semi-structured, entity-centric knowledge. For instance, publicly available knowledge graphs (KGs) such as Freebase&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] or YAGO&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] as well as proprietary KGs used by Google or Microsoft [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] are key ingredients when interpreting search queries as well as Web documents. More recently, Web markup facilitated through standards such as RDFa<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, Microdata<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> and Microformats<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> has become prevalent on the Web, driven by initiatives such as <em>schema.org</em>, a joint effort led by Google, Yahoo!, Bing and Yandex.</p>    <p>For instance, the Web Data Commons (WDC) project [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] that releases markup extracted from the Common Crawl<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>, found that in 2016 39% out of 3.18 billion HTML pages from over 34 million pay-level-domains (plds) contain some form of embedded markup, resulting in a corpus of 44.24 billion RDF quadruples<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>. There is an upward trend of Web markup adoption, where the proportion of pages containing markup increased from 5.76% to 39% between 2010 and 2016.</p>    <p>To this extent, markup data provides an unprecedented and growing source of explicit entity annotations to be used when interpreting and retrieving Web documents, to complement annotations otherwise obtainable through traditional information extraction pipelines, or to train information extraction methods. In addition, while traditional KGs capture large amounts of factual knowledge, they still are incomplete, i.e. coverage and completeness vary heavily across different types or domains. In particular, there is a large percentage of less popular (long-tail) entities and properties that are usually insufficiently represented [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. In this context, markup also provides essential input when incrementally augmenting and maintaining KGs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>], in particular when attempting to complement information about long-tail properties and entities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>].</p>    <p>The specific characteristics of statements extracted from embedded Web markup pose particular challenges&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>]. Whereas coreferences are very frequent (for instance, in the WDC 2013 corpus, 18,000 entity descriptions of type <em>schema.org:Product</em> are returned for the query <tt>&#x2018;Iphone 6&#x2019;</tt>), these are not linked through explicit statements. In contrast to traditional densely connected RDF graphs, markup statements mostly consist of isolated nodes and small subgraphs, each usually made up of small sets of statements per entity description. In addition, extracted RDF markup statements are highly redundant and are often limited to a small set of highly popular predicates, such as <em>schema.org:name</em>, complemented by a long tail of less frequent statements. Moreover, data extracted from markup contains a wide variety of errors&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>], ranging from typos to the frequent misuse of vocabulary terms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. Hence, individual markup extracted from a particular Web document or crawl usually contains very limited or unreliable information about a particular entity. According to our analysis, out of 26 million annotated events in the WDC 2016 corpus, less than 257,000 (0.96%) indicate a more specific event subtype and 59% nodes provide less than six statements. This strongly limits the meaningfulness of Web markup, in particular for entities that cannot be mapped to a representation in an existing knowledge graph.</p>    <p>In this work, we introduce an approach to automatically infer missing categorical information for particular entities obtained from Web markup. Building on the Web-scale availability of markup, and hence, the abundance of potential training data for the task, we introduce a supervised method to efficiently infer missing categorical information from existing entity markup describing coreferring or similar entities. Our experiments address the inference of entity (sub-)types, as well as inference of arbitrary non-hierarchical predicates, such as movie genres. We demonstrate superior performance compared to both naive baselines as well as specialised state-of-the-art methods for type inference and achieve F1 scores of 79% and 83% in two experimental tasks.</p>   </section>   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Motivation and Problem Definition</h2>    </div>    </header>    <section id="sec-6">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Motivation</h3>     </div>    </header>    <p>Microdata and RDFa markup are used to embed semi-structured data about entities within Websites. While being leveraged to facilitate interpretation and retrieval of Websites by most major search engines, markup data is also used to maintain and augment knowledge graphs, where additional applications include Google Rich Snippets, Pinterest Rich Pins and search features for Apple Siri [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>]. By today, Web markup data is available at an unprecedentedly large scale, which can be exemplarily observed on the <em>Web Data Commons</em> (WDC)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] corpus. WDC<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> offers a large-scale corpus of RDF quadruples extracted from the <em>Common Crawl</em><a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>. The crawl of October 2016 contains 3.18 &#x00B7; 10<sup>9</sup> URLs of which 39% exhibit markup from which over 4.4 &#x00B7; 10<sup>10</sup> triples were extracted. In contrast, the crawl of November 2015 contains ~1.77 &#x00B7; 10<sup>9</sup> URLs of which only 31% exhibit markup, resulting in only about 2.4 &#x00B7; 10<sup>10</sup> extracted triples<a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a>. <em>Schema.org</em> is a joint initiative from major search engines such as Bing, Google, Yahoo! and Yandex that provides a joint vocabulary and is the most commonly deployed vocabulary on the Web [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. In the following we abbreviate the prefix of the <em>schema.org</em> vocabulary by <em>s:</em>, e.g. <em>s:Movie</em>. We refer to the WDC corpus from October 2016 as the WDC 2016 corpus.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Number of quadruples per node for specific types in WDC 2016.</span>     </div>     <table class="table">      <thead>       <tr>       <th rowspan="2" style="text-align:center; vertical-align:middle;">Type</th>       <th rowspan="2" style="text-align:center; vertical-align:middle;">Total No. Quadruples</th>       <th rowspan="2" style="text-align:center; vertical-align:middle;">Total No. Nodes</th>       <th colspan="4" style="text-align:center;">Quadruples<hr/></th>       <th colspan="4" style="text-align:center;">Distinct Properties<hr/></th>       </tr>       <tr>       <th style="text-align:right;">Min.</th>       <th style="text-align:right;">Max</th>       <th style="text-align:right;">Avg.</th>       <th style="text-align:right;">Median</th>       <th style="text-align:right;">Min.</th>       <th style="text-align:right;">Max.</th>       <th style="text-align:right;">Avg.</th>       <th style="text-align:right;">Median</th>       <th/>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:right;">        <em>s:Event</em>       </td>       <td style="text-align:right;">1.58 &#x00B7; 10<sup>8</sup>       </td>       <td style="text-align:right;">2.66 &#x00B7; 10<sup>7</sup>       </td>       <td style="text-align:right;">1</td>       <td style="text-align:right;">2889</td>       <td style="text-align:right;">5.55</td>       <td style="text-align:right;">5</td>       <td style="text-align:right;">1</td>       <td style="text-align:right;">32</td>       <td style="text-align:right;">5.31</td>       <td style="text-align:right;">5</td>       </tr>       <tr>       <td style="text-align:right;">        <em>s:Movie</em>       </td>       <td style="text-align:right;">1.25 &#x00B7; 10<sup>8</sup>       </td>       <td style="text-align:right;">1.62 &#x00B7; 10<sup>7</sup>       </td>       <td style="text-align:right;">1</td>       <td style="text-align:right;">4547</td>       <td style="text-align:right;">7.71</td>       <td style="text-align:right;">6</td>       <td style="text-align:right;">1</td>       <td style="text-align:right;">26</td>       <td style="text-align:right;">5.77</td>       <td style="text-align:right;">6</td>       </tr>      </tbody>     </table>    </div>    <p>While Web markup constitutes an unprecedented source of semi-structured knowledge, markup is usually sparse and highly redundant, consisting of vast amounts of coreferences and (near) duplicate statements [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. Individual entities extracted from Web markup usually are sparsely described, such that only a fraction of the properties foreseen by <em>schema.org</em> for a specific type is provided, often only providing a label and a type for a specific node. Table <a class="tbl" href="#tab1">1</a> provides an overview of the number of quadruples per single node for specific types (<em>s:Event</em>, <em>s:Movie</em>) in the WDC 2016 corpus. The property distribution follows a power law, where a small set of terms is very prevalent, yet the majority of properties is hardly used across the Web. Figure <a class="fig" href="#fig1">1</a> shows the top-20 most frequently used properties of movies, highlighting that certain properties occur very often (e.g. <em>s:actor</em>) while others are provided rarely, such as <em>s:productionCompany</em>.</p>    <p>Sparsity is exacerbated by the lack of connectivity of markup data, where controlled vocabularies, taxonomies, and essentially, links among nodes are hardly present. Previous studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>] on a specific markup subset find that, out of a set of 46 million quadruples involving transversal, i.e. non-hierarchical properties, approximately 97% actually refer to literals rather than URIs, that is object nodes. These findings underline that markup data largely consists of rather isolated nodes, which are linked through common schema terms (as provided by <em>schema.org</em>) at best, but commonly lack relations at the instance level. In particular for categorical information, such as movie genres or product categories, this poses a crucial challenge when it comes to interpreting such information. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186028/images/www2018-37-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Top-20 most frequent properties for the type <em>s:Movie</em> in WDC 2016. The second entry of <em>s:actor</em> is caused by erroneous annotations in Web markup.</span>      </div>     </figure>    </p>    <p>A particular instantiation of the aforementioned problem is the use of unspecific types. Figure <a class="fig" href="#fig2">2</a> illustrates the number of instances of events annotated with respective event subtypes. Note that assignment of multiple types is theoretically possible, but rarely used in practice (i.e. less than 0.1% of events have multiple types). Apparently, most of the instances are assigned the generic type <em>s:Event</em>, while only 0.96% of nodes use more specific types like <em>s:TheaterEvent</em> or <em>s:Festival</em>, hindering data interpretation. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186028/images/www2018-37-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Number of occurrences of schema.org event types in WDC 2016 (Y-axis is logarithmic).</span>      </div>     </figure>    </p>    <p>Whereas individual markup nodes are usually sparsely annotated, markup as a whole provides a rich source of data, where in particular for categorical, i.e. discrete, properties a wide variety of instances can be drawn from the long tail. For instance, referring to Figure <a class="fig" href="#fig2">2</a>, while only 0.96% of all event nodes are typed with a meaningful subtype, this still corresponds to a set of 257,000 nodes available as training data to build supervised models to classify the remaining 26 million insufficiently typed events. Hence, we follow the intuition that markup data can significantly benefit from supervised approaches, which learn categorial or discretised properties as a means to infer missing categorical information for sparsely annotated nodes, i.e. to enrich markup entities. Overall, augmentation of sparse Web markup nodes can contribute to the improvement of the interpretability of the markup, the enrichment of knowledge graphs, and hence, to the effectiveness of the applications using the markup. This includes search and Web page classification, where in particular categorical and type information is essential to correctly interpret resources.</p>    </section>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Problem Definition</h3>     </div>    </header>    <p>This work aims at inferring missing categorical information in data sourced from Web markup. For a given corpus of Websites <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathcal {Q}_\mathcal {C}$</span>     </span> denotes the set of <em>RDF quadruples</em> of the form (<em>s</em>, <em>p</em>, <em>o</em>, <em>u</em>) extracted from the corpus, where <em>s</em>, <em>p</em>, <em>o</em> represent an RDF triple, i.e. a statement, of the form subject, predicate and object and <em>u</em> represents the URL of the Web document, from which the triple has been extracted.</p>    <p>A <em>vocabulary V</em> consists of a set of <em>types T</em> and <em>properties P</em>. A particular property <em>p<sub>i</sub>     </em> &#x2208; <em>P</em> has a declared domain <em>d</em>(<em>p<sub>i</sub>     </em>) that defines the set of expected types <em>T<sub>i</sub>     </em>&#x2286;<em>T</em> a subject involved in the same triple with <em>p<sub>i</sub>     </em> is meant to be an instance of. The range <em>r</em>(<em>p<sub>i</sub>     </em>) of a property <em>p<sub>i</sub>     </em> defines the expected types an object involved in the same triple as <em>p<sub>i</sub>     </em> is meant to be an instance of.</p>    <p>For instance, within the <em>schema.org</em> vocabulary, the domain of the property <em>translator</em><a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a> is defined as instances of type <em>Event</em><a class="fn" href="#fn10" id="foot-fn10"><sup>10</sup></a> and <em>CreativeWork</em><a class="fn" href="#fn11" id="foot-fn11"><sup>11</sup></a>, while the declared range is defined as instances of type <em>Organization</em><a class="fn" href="#fn12" id="foot-fn12"><sup>12</sup></a> and <em>Person</em><a class="fn" href="#fn13" id="foot-fn13"><sup>13</sup></a>.</p>    <div class="definition" id="enc1">     <Label>Definition 2.1.</Label>     <p> Given a vocabulary <em>V</em>, a set of quadruples <span class="inline-equation"><span class="tex">$\mathcal {Q}_\mathcal {C}$</span>      </span>, for a particular node representing a subject <span class="inline-equation"><span class="tex">$s_i \in \mathcal {Q}_\mathcal {C}$</span>      </span>, this work aims at predicting quadruples <em>q</em> = (<em>s<sub>i</sub>      </em>, <em>p<sub>i</sub>      </em>, <em>o<sub>i</sub>      </em>, <em>u<sub>i</sub>      </em>) which are: (a) not present in the markup corpus (<span class="inline-equation"><span class="tex">$q \notin \mathcal {Q}_\mathcal {C}$</span>      </span>), (b) valid according to the definition of vocabulary <em>V</em>, and (c) a valid statement about subject <em>s<sub>i</sub>      </em> in the context of <em>u<sub>i</sub>      </em>.</p>    </div>    <p>The last requirement of the aforementioned definition is experimentally evaluated according to a ground truth <em>G</em>, where an example is described in Section <a class="sec" href="#sec-13">4.1</a>.</p>    <p>Note that our work focuses on <em>categorical</em> properties, i.e. we consider properties where the corresponding range <em>r</em>(<em>p</em>) is <em>finite</em>.</p>    <p>For instance, consider the following markup triple, extracted from the URL <a class="link-inline force-break"      href="http://www.imdb.com/title/tt0109830/?ref_=tt_trv_cnn">http://www.imdb.com/title/tt0109830/?ref_=tt_trv_cnn</a> describing the movie &#x201D;Forrest Gump&#x201D;: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \left[ \begin{array}{l|l}s: &#x0026; {\it \_:nodea73846c741abe988abf1c682f1fe26e7}\\ p: &#x0026; {\it rdf:type}\\ o: &#x0026; {\it s:Movie}\\ \end{array} \right] \] </span>       <br/>      </div>     </div> For the specific subtask of predicting movie genres (Section <a class="sec" href="#sec-12">4</a>), we aim at predicting the quadruple involving the following triple (URL omitted) stating the genre of the movie: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \left[ \begin{array}{l|l}s: &#x0026; {\it \_:nodea73846c741abe988abf1c682f1fe26e7}\\ p: &#x0026; {\it s:genre}\\ o: &#x0026; ^{\prime \prime }Drama^{\prime \prime }\\ \end{array} \right] \] </span>       <br/>      </div>     </div>    </p>    </section>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Approach</h2>    </div>    </header>    <p>The characteristics of the data at hand suggest that, for most subjects <em>s<sub>i</sub>    </em> which are to be augmented, e.g. the movie mentioned in the previous example, sufficient training data can be obtained (Section <a class="sec" href="#sec-5">2</a>). That means, we anticipate that a sufficient number of entity descriptions (instances) exist, which share the same missing categorical property <em>p<sub>i</sub>    </em>, e.g. a movie genre in the example above. Thus, we approach the inference problem as a supervised classification problem, where nodes which share the sought after property <em>p<sub>i</sub>    </em> are used as training data to build a model for the prediction of respective statements. This section describes our approach, namely the steps taken for data cleansing, feature extraction and building classification models.</p>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Data Cleansing</h3>     </div>    </header>    <p>Based on studies on common errors on deployed microdata [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], we applied the following heuristics proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], to improve the quality of the dataset by fixing the following errors:</p>    <p>     <strong>Wrong namespaces</strong>: Many terms that deviate from the correct <em>schema.org</em> namespace can be corrected by adding missing slashes, changing <em>https://</em> to <em>http://</em>, removing additional substrings between <em>http://</em> and <em>schema.org</em> and fixing capitalisation errors.</p>    <p>     <strong>Undefined properties and types</strong>: The use of wrong capitalisation of property and type names leads to the presence of undefined terms in markup data. We corrected the capitalisation by using the capitalisation defined by the <em>schema.org</em> vocabulary.</p>    <p>Applying these heuristics aids the feature extraction and classification steps described below by providing a larger amount of training data as well as by improving feature quality.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Feature Extraction</h3>     </div>    </header>    <p>This section describes the considered features for our task and the applied feature extraction.</p>    <p>     <em>      <strong>pld/tld</strong>     </em>: Based on the assumption that many Web domains are specialised on particular topics, e.g. concerts or documentary films, we employ domain-based features. The intuition is that any particular <em>pay-level-domain</em> (pld) and/or <em>top-level-domain</em> (tld) usually correlates with particular categorical properties, such as the types of covered events. Thus, for each node, we extract the pld and the tld from the URL of the Web page. For instance, taking into account the task of predicting event subtypes, consider the quadruple: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \left[ \begin{array}{l|l}s:&#x0026; {\it \_:node396540c21b6fa0388c7293ebe216583}\\ p:&#x0026; {\it rdf:type}\\ o:&#x0026; {\it s:Event}\\ u: &#x0026; {\it {\lt}http://www.touristlink.com/india/cat/events.html{\gt}}\\ \end{array} \right] \] </span>       <br/>      </div>     </div> From this quadruple we extract the pld &#x201D;touristlink.com&#x201D; and the tld &#x201D;.com&#x201D; from <em>u</em> and use these as features to predict the subtype &#x201D;<em>s:MusicEvent</em>&#x201D; of the described event. The plds and tlds are mapped into feature space via <em>1-hot-encoding</em><a class="fn" href="#fn14" id="foot-fn14"><sup>14</sup></a>, resulting in one dimension for each pld and each tld.</p>    <p>     <em>      <strong>node-vocab</strong>     </em>: The intuition behind this feature is that there is a correlation between the used vocabulary terms and the specific classes we aim to predict. For example, a composer (<em>s:composer</em>) is more likely to be provided for a music event (<em>s:MusicEvent</em>) than for a sports event. Following this intuition, Paulheim et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] proposed an approach for entity type prediction using vocabulary term correlations. To this extent, they made use of the outgoing and incoming statements of the node <em>n</em> for type prediction of <em>n</em> in knowledge graphs (i.e. statements that have <em>n</em> either in the subject or the object position, respectively). In case of Web markup, it may not be feasible to determine all incoming statements for a given subject at Web scale. Therefore, in this work we make use of the outgoing statements only and use these statements to predict categorical properties of the entity described through the node <em>n</em>. More specifically, for all quadruples <em>Q<sub>n</sub>     </em> involving subject <em>n</em>, we extract all <em>schema.org</em> terms used as predicate. For each node <em>n</em>, we compute a frequency vector, where each dimension corresponds to a vocabulary term <em>t<sub>i</sub>     </em> and each value is the normalised number of times <em>t<sub>i</sub>     </em> occurs in a quadruple with <em>n</em> as a subject. The frequencies are normalised using the <em>l</em>     <sup>2</sup> (euclidean) norm.</p>    <div class="example" id="enc2">     <Label>Example 3.1.</Label>     <p> For the node <em>s</em> and URL <em>u</em>      <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ \left[ \begin{array}{l|l}\rm s: &#x0026; {\it \_:node3957c770b4f7c0bd1a17805dd8ca406}\\ {\rm u:} &#x0026; {\it {\lt}https://gdssummits.com/nghealthcare/us/{\gt}}\\ \end{array} \right] \] </span>       <br/>       </div>      </div> the following tuples are present: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;\left[ \begin{array}{l|l}\rm p: &#x0026; {\it rdf:type}\\ {\rm o:} &#x0026; {\it {\lt}http://schema.org/BusinessEvent{\gt}}\\\end{array} \right]\\ &#x0026;\left[ \begin{array}{l|l}\rm p: &#x0026; {\it s:Event/name}\\ {\rm o:} &#x0026; {\it ^{\prime \prime }NG Healthcare Summit US^{\prime \prime }@en}\\\end{array} \right]\\ &#x0026;\left[ \begin{array}{l|l}\rm p: &#x0026; {\it s:Event/location}\\ {\rm o:} &#x0026; {\it ^{\prime \prime }Omni Barton Creek Resort \&#x0026; Spa, Austin, Texas^{\prime \prime }@en}\\\end{array} \right]\end{align*} </span>       <br/>       </div>      </div>     </p>     <p>These tuples result in the following node-vocab: {<em>rdf:type</em>:1,</p>     <p>      <em>s:Event/name</em>:1, <em>s:Event/location</em>:1}.</p>    </div>    <p>Note that we concatenated the predicate and the type used as the domain of the predicate. This way we ensure that: (a) types as well as terms are considered and (b) the connection between a predicate and its observed domain is preserved. The latter appears useful, considering that <em>schema.org</em> terms are used in a variety of contexts, often in ways other than recommended by the vocabulary definition, e.g. by violating domain and range definitions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>].</p>    <p>     <em>      <strong>page-vocab</strong>     </em>: The vocabulary used on a Web page within which a subject appears intuitively correlates with categorical classes associated with nodes on the respective page. For instance, Websites discussing music albums are more likely to also contain music events rather than sports events. To take this context into account, we consider all <em>schema.org</em> vocabulary terms that appear as predicates on the same Web page as the node under consideration as a feature. Similar to the node-vocab, we create a frequency vector normalised using the <em>l</em>     <sup>2</sup> (euclidean) norm.</p>    <div class="example" id="enc3">     <Label>Example 3.2.</Label>     <p> Assume that in addition to the quadruples in Example&#x00A0;<a class="enc" href="#enc2">3.1</a>, the following triples are present on the same Web page: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;\left[ \begin{array}{l|l}\rm s &#x0026; {\it \_:nodea9ff152514bcfb63c2714bc1336b2b3} \\ {\rm p:} &#x0026; {\it s:Organization/url}\\ {\rm o:} &#x0026; {\it {\lt}http://www.gdsinternational.com{\gt}}\\\end{array} \right]\\ &#x0026;\left[ \begin{array}{l|l}\rm s &#x0026; {\it \_:node4ccbf7f34c95f14168f5fdb47b73ab} \\ {\rm p:} &#x0026; {\it rdf:type}\\ {\rm o:} &#x0026; {\it s:BusinessEvent}\\\end{array} \right]\end{align*} </span>       <br/>       </div>      </div> Then the terms from these quadruples are added to the node-vocab to form the page-vocab: {<em>rdf:type</em>:2, <em>s:Event/name</em>:1, <em>s:Event/location</em>:1, <em>s:Organization/url</em>:1}.</p>    </div>    <p>After computing the individual features, all features are concatenated to form a single feature vector. Finally, the feature vectors are normalised, i.e. the mean is removed and the features are scaled to unit variance. The feature vectors serve as input for supervised machine learning approaches that are detailed in Section <a class="sec" href="#sec-11">3.3</a>.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Classification Models</h3>     </div>    </header>    <p>We compare the use of the following classifiers:</p>    <p>     <strong>Na&#x00EF;ve Bayes</strong>: A Gaussian Na&#x00EF;ve Bayes classifier that assumes that the likelihood of the features follows a Gaussian distribution. Since the features are normalised (i.e. may have negative values), a multinomial Na&#x00EF;ve Bayes can not be applied. Na&#x00EF;ve Bayes classifiers are known to be adoptable to many classification tasks.</p>    <p>     <strong>Decision Tree</strong>: A classifier that successively divides the feature space to maximise a given metric (e.g. Gini Impurity, Information Gain). Decision Trees are able to identify discriminative features within high-dimensional data.</p>    <p>     <strong>Random Forest</strong>: A classifier that utilises an ensemble of uncorrelated decision trees. Random Forests can utilise a large amount of training data that is likely to be found in Web crawls.</p>    <p>     <strong>SVM</strong>: A Support Vector Machine with a linear kernel. SVMs have been applied to a large variety of classification problems.</p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Evaluation Setup</h2>    </div>    </header>    <p>While our approach is independent of the respective categorical information to be inferred, we conducted an evaluation in two specific tasks: (1) predicting subtypes of <em>s:Event</em> instances, and (2) predicting genres (<em>s:genre</em>) of <em>s:Movie</em> instances.</p>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>    </header>    <p>Training and test datasets were extracted from the Web Data Commons dataset of October 2016.</p>    <p>     <strong>Event Classification</strong>: This task deals with the prediction of event subtypes. <em>Schema.org</em> distinguishes between 19 different event subtypes, such as <em>s:BusinessEvent</em> or <em>s:SportsEvent</em>. Given a generic event, the goal of this task is to predict the correct subtype of the event, i.e. to predict the object of the <em>rdf:type</em> statement.</p>    <p>     <strong>Movie Genre Classification</strong>: <em>Schema.org</em> allows annotation of movie genres via the <em>s:genre</em> property. The goal of this classification task is to predict statements describing the <em>s:genre</em> of respective movies. Since it is possible to assign multiple genres to a single movie by defining multiple <em>s:genre</em> properties, the classification of movie genres is a <em>multi-label problem</em>, i.e. a single movie entity can belong to multiple genre classes. We address this multi-label problem by extracting individual datasets for each genre upon which a binary classifier for each genre is trained. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186028/images/www2018-37-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">tld/pld-distribution of <em>s:VisualArtsEvent</em>s. Y-axis is logarithmic.</span>      </div>     </figure>    </p>    <section id="sec-14">     <p><em>4.1.1 Balancing and Sampling.</em> We extracted quadruples that exhibit the respective property of interest by selecting quadruples which describe nodes of <em>rdf:type s:Event</em> (<em>s:Movie</em>) and are annotated with a more specific event subtype in the case of events and the <em>s:genre</em> predicate for movies. This results into a single <em>Events</em> dataset (containing instances of all considered subtypes) and an individual dataset for each movie genre. As illustrated in Figure <a class="fig" href="#fig2">2</a>, the class distribution is uneven.</p>     <p>To obtain a balanced dataset that is sufficiently large for training of a machine learning algorithm, we applied the following steps. For <em>Events</em>, we picked the top-7 classes with the highest number of instances. We introduced an additional class containing all events not included in the top-7 classes. The classes were balanced by limiting the size of all classes to <em>c<sub>e</sub>      </em>, which is the size of the smallest class. For <em>Movies</em>, we extracted 7 individual datasets corresponding to the top-7 most frequent movie genres. Each individual genre dataset includes all instances of the particular genre as well as all the remaining instances, which are labeled as &#x201D;Other&#x201D;. The size of each genre datasets is limited to <em>c<sub>m</sub>      </em>, which is the size of the smallest class among all 7 datasets.</p>     <p>We employed two different sampling strategies:</p>     <p>1) <em>Stratified Random Sampling</em> simply chooses <em>c<sub>e</sub>      </em> (<em>c<sub>m</sub>      </em>) instances of each class at random from the whole dataset.</p>     <p>2) <em>pld-Aware Sampling</em>: Figure <a class="fig" href="#fig3">3</a> depicts the pld distribution of <em>s:VisualArtsEvent</em>s. The distribution follows a power law, such that a small set of plds provides the majority of events. Random sampling may result in dropping some of the plds with fewer events and overfitting towards the patterns exhibited by very prominent plds. Therefore, we employ a sampling approach that ensures representation of long-tail entities in the sample. To this extent, we calculate a <em>fair share</em> in the sample by dividing the number of instances by the numbers of plds. We add all instances from plds that have fewer instances than the <em>fair share</em>. This process is repeated with recalculating the <em>fair share</em> with respect to the number of missing instances until the dataset contains <em>c<sub>e</sub>      </em> (<em>c<sub>m</sub>      </em>) instances of each class, where <em>c<sub>e</sub>      </em> (<em>c<sub>m</sub>      </em>) is the number of instances of the smallest class in the case of events (movies). If all remaining plds contain more instances than the <em>fair share</em>, each pld contributes the <em>fair share</em> to the final sample.</p>     <p>After the sampling, we split each resulting dataset in an individual training and test set (80% / 20% of the instances).</p>    </section>    <section id="sec-15">     <p><em>4.1.2 Labeling &#x0026; Ground Truth.</em> We follow a dataset-specific strategy to obtain class labels, i.e. a ground truth for training and testing. For assigning event types, we rely on the event subtypes defined within the <em>schema.org</em> type hierarchy. The class labels for events are thus explicitly given by the <em>rdf:type</em>-statements.</p>     <p>With respect to the prediction of movie genres, no controlled vocabulary is used consistently, whereas literals are used widely. Therefore, we map the literals to a unified genre taxonomy. We make use of the 22 genres defined by the International Movie Database (IMDB)<a class="fn" href="#fn15" id="foot-fn15"><sup>15</sup></a>. To obtain the class labels, we check for string containment of the IMDB genre names in the literal values of the <em>s:genre</em> properties. If a genre name is a substring of the aforementioned property the genre is assigned as class label to the respective instance. Note that it is possible for one instance to exhibit multiple labels since multiple genre names may be substrings of a single <em>s:genre</em> property and, in addition, single instances may have multiple <em>s:genre</em> properties. Intuitively, this process leads to reasonable class labels for the majority of instances, such that a sufficiently large amount of correctly labeled training data can be obtained. Yet, we also anticipate a certain amount of noise. The cleansed and labeled datasets are made publicly available<a class="fn" href="#fn16" id="foot-fn16"><sup>16</sup></a>.</p>     <p>Table <a class="tbl" href="#tab2">2</a> provides an overview of the size of the extracted datasets as well as the amount of included plds. The event datasets are denoted by <em>Events</em> and contain the following classes: <em>PublicationEvent, MusicEvent, ScreeningEvent, ComedyEvent, TheaterEvent, EducationEvent, VisualArtsEvent, Other</em>. For movie genres, the genre-specific datasets are denoted by the first three letters of the respective genre as follows <em>{Drama, Comedy, Action, Thriller, Romance, Documentary, Adventure}</em> = {<em>Dra, Com, Act, Thr, Rom, Doc, Adv</em>}. <em>Movies</em> refers to average values for all genres. The sampling method is denoted by the subscript, where <em>s</em> represents <em>stratified random sampling</em> and <em>p pld-aware sampling</em>.</p>     <div class="table-responsive" id="tab2">      <div class="table-caption">       <span class="table-number">Table 2:</span>       <span class="table-title">Overview of the dataset size and contained plds. Movie genres are abbreviated by their first three letters. An own dataset for each genre is extracted since each genre is treated as a binary classification problem.</span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:left;">         Dataset        </th>        <th style="text-align:right;">         Size        </th>        <th style="text-align:right;">         Distinct plds        </th>        <th style="text-align:right;">         Avg. Instances/pld        </th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:left;">         <em>Events<sub>s</sub>         </em>        </td>        <td style="text-align:right;">67,744</td>        <td style="text-align:right;">1,482</td>        <td style="text-align:right;">45.71</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Events<sub>p</sub>         </em>        </td>        <td style="text-align:right;">67,744</td>        <td style="text-align:right;">2,064</td>        <td style="text-align:right;">32.82</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Dra<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">360</td>        <td style="text-align:right;">663.97</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Dra<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Com<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">342</td>        <td style="text-align:right;">698.92</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Com<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Act<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">361</td>        <td style="text-align:right;">662.13</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Act<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Thr<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">342</td>        <td style="text-align:right;">698.92</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Thr<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Rom<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">347</td>        <td style="text-align:right;">688.85</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Rom<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Doc<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">337</td>        <td style="text-align:right;">709.29</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Doc<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Adv<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">340</td>        <td style="text-align:right;">703.03</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Adv<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Movies<sub>s</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">347</td>        <td style="text-align:right;">689.30</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Movies<sub>p</sub>         </em>        </td>        <td style="text-align:right;">239,030</td>        <td style="text-align:right;">476</td>        <td style="text-align:right;">502.16</td>       </tr>       </tbody>      </table>     </div>    </section>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Metrics</h3>     </div>    </header>    <p>To evaluate the performance of the different classifiers, we compute the following metrics:</p>    <p>     <strong>Precision</strong>: The fraction of the correctly classified instances among the instances assigned to one class.</p>    <p>     <strong>Recall</strong>: The fraction of the correctly assigned instances among all instances of the class.</p>    <p>     <strong>F1 score</strong>: The harmonic mean of recall and precision. This work considers the F1 score to be the most relevant metric since it reflects both recall and precision.</p>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Baselines</h3>     </div>    </header>    <p>We compare our approach to the following baselines:</p>    <p>     <strong>RANDOM</strong>: This baseline chooses a class at random.</p>    <p>     <strong>SD-TYPE</strong>: This baseline leverages conditional probabilities to infer the subject types using the <em>SD-Type</em> approach [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>]. The probabilities are based on the incoming and outgoing statements of a particular node. Since <em>SD-Type</em> was not originally designed to be applied to Web markup, we adapted it by only considering outgoing statements. This is motivated by the fact that a complete set of incoming statements can not be obtained for Web markup, where links might (but are unlikely to) originate from any Web page.</p>    <p>     <strong>KG-B</strong>: This baseline employs a knowledge graph to obtain class labels. The <em>s:name</em> of a subject is used as input for <em>DBpedia Spotlight</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] to obtain candidate entities from <em>DBpedia</em> (<em>dbp</em>). If the markup is annotated in one of the 12 languages supported by Spotlight<a class="fn" href="#fn17" id="foot-fn17"><sup>17</sup></a>, the corresponding Spotlight model is used. For all other cases we employ the English Spotlight model. Labels obtained from DBpedia may be different from labels found in Web markup (e.g. the genre of the movie &#x201D;Forrest Gump&#x201D; is stated to be <em>Drama</em> and <em>Comedy</em> in DBpedia, but marked as <em>Drama</em> and <em>Romance</em> on <em>imdb.com</em>).</p>    <p>In order to avoid noisy and costly matching process, we address this issue by considering all candidates with a confidence of at least 0.5 as true positives as long as the matching entity shows the correct type (<em>dbp:Event or s:Event</em> respectively <em>dbp:Movie</em> or <em>s:Movie</em>), independent of whether or not the entity actually shows the expected categorical property.</p>    <p>If no candidate with a suitable type is found, the instance is assigned to the &#x201D;Other&#x201D;-class. Note that this simplification significantly boosts the performance of this otherwise naive baseline, yet serves the purpose of illustrating the lack of sufficient coverage (Section <a class="sec" href="#sec-18">5</a>).</p>    </section>   </section>   <section id="sec-18">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Results</h2>    </div>    </header>    <p>This section presents the results on the classification performance, the influence of the sampling methods and the individual features.</p>    <div class="table-responsive" id="tab3">    <div class="table-caption">     <span class="table-number">Table 3:</span>     <span class="table-title">Macro averages for precision, recall, and F1 score [%] over all datasets.</span>    </div>    <table class="table">     <thead>      <tr>       <th rowspan="2" style="text-align:bottom; vertical-align:middle;">       Classifier       </th>       <th colspan="3" style="text-align:center;">       <em>Events<sub>s</sub>       </em>       <hr/>       </th>       <th colspan="3" style="text-align:center;">       <em>Events<sub>p</sub>       </em>       <hr/>       </th>       <th colspan="3" style="text-align:center;">       <em>Movies<sub>s</sub>       </em>       <hr/>       </th>       <th colspan="3" style="text-align:center;">       <em>Movies<sub>p</sub>       </em>       <hr/>       </th>      </tr>      <tr>       <th style="text-align:right;">Precision</th>       <th style="text-align:right;">Recall</th>       <th style="text-align:right;">F1</th>       <th style="text-align:right;">Precision</th>       <th style="text-align:right;">Recall</th>       <th style="text-align:right;">F1</th>       <th style="text-align:right;">Precision</th>       <th style="text-align:right;">Recall</th>       <th style="text-align:right;">F1</th>       <th style="text-align:right;">Precision</th>       <th style="text-align:right;">Recall</th>       <th style="text-align:right;">F1</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">       <SmallCap>Random</SmallCap>       </td>       <td style="text-align:right;">12.72</td>       <td style="text-align:right;">12.71</td>       <td style="text-align:right;">12.71</td>       <td style="text-align:right;">12.81</td>       <td style="text-align:right;">12.82</td>       <td style="text-align:right;">12.81</td>       <td style="text-align:right;">50.00</td>       <td style="text-align:right;">50.00</td>       <td>50.00</td>       <td>49.87</td>       <td>49.87</td>       <td>49.86</td>      </tr>      <tr>       <td style="text-align:left;">       <SmallCap>SD-Type</SmallCap>       </td>       <td style="text-align:right;">58.35</td>       <td style="text-align:right;">49.56</td>       <td style="text-align:right;">40.98</td>       <td style="text-align:right;">58.71</td>       <td style="text-align:right;">62.83</td>       <td style="text-align:right;">56.99</td>       <td style="text-align:right;">61.67</td>       <td style="text-align:right;">58.92</td>       <td>56.36</td>       <td>68.34</td>       <td>67.77</td>       <td>67.62</td>      </tr>      <tr>       <td style="text-align:left;">       <SmallCap>KG-B</SmallCap>       </td>       <td style="text-align:right;">39.06</td>       <td style="text-align:right;">12.59</td>       <td style="text-align:right;">02.96</td>       <td style="text-align:right;">39.06</td>       <td style="text-align:right;">12.59</td>       <td style="text-align:right;">02.96</td>       <td style="text-align:right;">       <strong>76.52</strong>       </td>       <td style="text-align:right;">55.70</td>       <td>44.82</td>       <td>76.94</td>       <td>57.16</td>       <td>47.42</td>      </tr>      <tr>       <td style="text-align:left;">       <SmallCap>Na&#x00EF;ve Bayes</SmallCap>       </td>       <td style="text-align:right;">       <strong>86.04</strong>       </td>       <td style="text-align:right;">44.24</td>       <td style="text-align:right;">40.04</td>       <td style="text-align:right;">       <strong>84.06</strong>       </td>       <td style="text-align:right;">50.51</td>       <td style="text-align:right;">47.78</td>       <td style="text-align:right;">69.06</td>       <td style="text-align:right;">50.29</td>       <td>33.98</td>       <td>61.55</td>       <td>50.39</td>       <td>34.19</td>      </tr>      <tr>       <td style="text-align:left;">       <SmallCap>Decision Tree</SmallCap>       </td>       <td style="text-align:right;">70.60</td>       <td style="text-align:right;">70.26</td>       <td style="text-align:right;">70.15</td>       <td style="text-align:right;">78.70</td>       <td style="text-align:right;">77.78</td>       <td style="text-align:right;">77.25</td>       <td style="text-align:right;">72.95</td>       <td style="text-align:right;">72.88</td>       <td>72.85</td>       <td>82.01</td>       <td>81.89</td>       <td>81.86</td>      </tr>      <tr>       <td style="text-align:left;">       <SmallCap>Random Forest</SmallCap>       </td>       <td style="text-align:right;">73.34</td>       <td style="text-align:right;">       <strong>72.46</strong>       </td>       <td style="text-align:right;">       <strong>71.67</strong>       </td>       <td style="text-align:right;">80.75</td>       <td style="text-align:right;">       <strong>79.71</strong>       </td>       <td style="text-align:right;">       <strong>79.59</strong>       </td>       <td style="text-align:right;">74.62</td>       <td style="text-align:right;">       <strong>74.49</strong>       </td>       <td>       <strong>74.46</strong>       </td>       <td>       <strong>83.27</strong>       </td>       <td>       <strong>83.16</strong>       </td>       <td>       <strong>83.14</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">       <SmallCap>SVM</SmallCap>       </td>       <td style="text-align:right;">75.51</td>       <td style="text-align:right;">70.10</td>       <td style="text-align:right;">67.64</td>       <td style="text-align:right;">81.45</td>       <td style="text-align:right;">78.67</td>       <td style="text-align:right;">77.34</td>       <td style="text-align:right;">72.84</td>       <td style="text-align:right;">72.42</td>       <td>72.27</td>       <td>81.75</td>       <td>81.37</td>       <td>81.27</td>      </tr>     </tbody>    </table>    </div>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Classification Performance</h3>     </div>    </header>    <p>Table <a class="tbl" href="#tab3">3</a> summarises the overall results of the baselines (<SmallCap>Random</SmallCap>, <SmallCap>SD-Type</SmallCap>, <SmallCap>KG-B</SmallCap>) as well as our proposed classification models (<SmallCap>Na&#x00EF;ve Bayes</SmallCap>, <SmallCap>Decision Tree</SmallCap>, <SmallCap>Random Forest</SmallCap>, <SmallCap>SVM</SmallCap>). For both tasks, we report the macro averages of the results with respect to precision, recall and F1 scores for both <em>stratified random sampling</em> and <em>pld-aware sampling</em>. We observe that, for <em>Movies</em>, <SmallCap>Random Forest</SmallCap>, closely followed by <SmallCap>Decision Tree</SmallCap>, performs best across all evaluation metrics, except for precision/<em>Movies<sub>s</sub>     </em>, where it is slightly outperformed by <SmallCap>KG-B</SmallCap>. This is caused by the underlying assumption of the <SmallCap>KG-B</SmallCap> baseline that any entity match is considered as successful information inference, which unfairly boosts the baseline performance, in particular for popular entities. For <em>Events</em>, <SmallCap>Random Forest</SmallCap> shows the highest Recall and F1, closely followed by <SmallCap>Decision Tree</SmallCap>, whereas highest precision is achieved by <SmallCap>Na&#x00EF;ve Bayes</SmallCap> in this case. The use of a single Decision Tree already results in relatively high F1 scores, e.g. 81.86% for <em>Movies<sub>p</sub>     </em>. Considering a <SmallCap>Random Forest</SmallCap> as an ensemble of Decision Trees, we conclude that additional trees only slightly improve the outcome (F1 of 83.14%). The <SmallCap>SD-Type</SmallCap> baseline achieves F1 scores of 56.99% for <em>Events</em>. This significant difference in performance between the baseline and our approach reflects the fundamental difference between knowledge graphs and data sourced from markup and the need to consider features beyond the structural connections of entity descriptions when dealing with markup data. For both <em>Events</em> and <em>Movies</em>, <SmallCap>KG-B</SmallCap> assigns the vast majority of the instances to the &#x201D;Other&#x201D;-class, resulting in high recall and low precision for the aforementioned class. Due to the design of the baseline, all classes different from &#x201D;Other&#x201D; exhibit 100% precision but very low recall, which ultimately results in low F1 scores after computing the macro average across classes.</p>    <p>For <em>Movies</em>, Table <a class="tbl" href="#tab3">3</a> reports the average scores of the individual genre-specific classifiers. It is worth to mention that the boundary of the classes (genres) might be fuzzy, e.g. it could be hard to differentiate a movie of genre &#x201D;Thriller&#x201D; from a movie of genre &#x201D;Action&#x201D;. Since the classification of each genre is formulated as a binary classification problem, the <SmallCap>random</SmallCap>-baseline performance is close to 50% for all classes. The highest F1 score achieved by <SmallCap>SD-Type</SmallCap> is 67.62%, indicating that the subject properties used by this baseline might not be sufficient to classify movie genres precisely. Overall performance of the <SmallCap>KG-B</SmallCap> baseline is better in this task, driven by higher recall for instances of type movie, which are better represented in knowledge bases. Similar to our observations in the event classification task, <SmallCap>Random Forest</SmallCap> performs best, closely followed by <SmallCap>Decision Tree</SmallCap>. The F1 score of 83.14% for <SmallCap>Random Forest</SmallCap> significantly outperforms the baselines (paired t-test with <em>p</em> < 0.01) when comparing <SmallCap>Random Forest</SmallCap> against the baselines in all configurations.</p>    <p>Overall <SmallCap>Random Forest</SmallCap> classification using the features proposed in this paper clearly outperforms the baselines in both tasks.</p>    <section id="sec-20">     <p><em>5.1.1 Classification Hyperparameter.</em> For each classifier used with an exception of the Na&#x00EF;ve Bayes classifier, we determine the parameters that maximise the F1 score by employing the random search algorithm proposed by Bergstra and Bengio [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>]. The Na&#x00EF;ve Bayes classifier does not exhibit parameters that could be optimised. Table <a class="tbl" href="#tab4">4</a> gives an overview of the parameters that were considered during the optimisation, whereas Table <a class="tbl" href="#tab5">5</a> summarises the hyper-parameters that were determined using random search. All previously shown performance results were obtained using the specified hyper-parameters.</p>     <div class="table-responsive" id="tab4">      <div class="table-caption">       <span class="table-number">Table 4:</span>       <span class="table-title">Hyperparameters considered for optimisation.</span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:left;">         Classifier        </th>        <th style="text-align:left;">         Parameter        </th>        <th style="text-align:left;">         Range        </th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:left;">         <SmallCap>Decision Tree</SmallCap>        </td>        <td style="text-align:left;">Criterion</td>        <td style="text-align:left;">Gini Impurity,</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;"/>        <td style="text-align:left;">Information Gain</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">Min.Impurity Decrease</td>        <td style="text-align:left;">[0,1]</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td/>        <td/>       </tr>       <tr>        <td style="text-align:left;">         <SmallCap>Random Forest</SmallCap>        </td>        <td style="text-align:left;">Criterion</td>        <td style="text-align:left;">Gini Impurity,</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;"/>        <td style="text-align:left;">Information Gain</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">Min.Impurity Decrease</td>        <td style="text-align:left;">[0,1]</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">No. Estimators</td>        <td style="text-align:left;">[5,20]</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td/>        <td/>       </tr>       <tr>        <td style="text-align:left;">         <SmallCap>SVM</SmallCap>        </td>        <td style="text-align:left;">Penalty</td>        <td style="text-align:left;">[0,5]</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">Stopping Tolerance</td>        <td style="text-align:left;">[0,10<sup>&#x2212; 3</sup>]</td>       </tr>       </tbody>      </table>     </div>     <div class="table-responsive" id="tab5">      <div class="table-caption">       <span class="table-number">Table 5:</span>       <span class="table-title">Summary of classifier hyperparameters determined with random search for the following parameters: Crit: Criterion, Imp: Min. Impurity Decrease, No: No. Estimators, Pen: Penalty, Tol: Stopping Tolerance.</span>      </div>      <table class="table">       <thead>       <tr>        <th rowspan="2" style="text-align:left; vertical-align:middle;">         Dataset        </th>        <th colspan="2" style="text-align:center;">         <SmallCap>Decision Tree</SmallCap>         <hr/>        </th>        <th colspan="3" style="text-align:center;">         <SmallCap>Random Forest</SmallCap>         <hr/>        </th>        <th colspan="2" style="text-align:center;">         <SmallCap>SVM</SmallCap>         <hr/>        </th>       </tr>       <tr>        <th style="text-align:left;">         Crit        </th>        <th style="text-align:left;">         Imp        </th>        <th style="text-align:left;">         Crit        </th>        <th style="text-align:left;">         Imp        </th>        <th style="text-align:left;">         No        </th>        <th style="text-align:left;">         Pen        </th>        <th style="text-align:center;">         Tol        </th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:left;">         <em>Events<sub>s</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.192</td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.892</td>        <td style="text-align:left;">13</td>        <td style="text-align:left;">3.53</td>        <td style="text-align:center;">0.0043</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Events<sub>p</sub>         </em>        </td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.527</td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.892</td>        <td style="text-align:left;">13</td>        <td style="text-align:left;">1.88</td>        <td style="text-align:center;">0.0098</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Dra<sub>s</sub>         </em>        </td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.360</td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.938</td>        <td style="text-align:left;">16</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Dra<sub>p</sub>         </em>        </td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.360</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.414</td>        <td style="text-align:left;">18</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Com<sub>s</sub>         </em>        </td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.360</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.414</td>        <td style="text-align:left;">18</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Com<sub>p</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.608</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.160</td>        <td style="text-align:left;">20</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Act<sub>s</sub>         </em>        </td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.360</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.160</td>        <td style="text-align:left;">20</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Act<sub>p</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.558</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.160</td>        <td style="text-align:left;">20</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Thr<sub>s</sub>         </em>        </td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.360</td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.482</td>        <td style="text-align:left;">16</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Thr<sub>p</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.608</td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.482</td>        <td style="text-align:left;">13</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Rom<sub>s</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.608</td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.482</td>        <td style="text-align:left;">13</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Rom<sub>p</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.287</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.160</td>        <td style="text-align:left;">20</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Doc<sub>s</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.192</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.160</td>        <td style="text-align:left;">20</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Doc<sub>p</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.099</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.068</td>        <td style="text-align:left;">16</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Adv<sub>s</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.287</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.160</td>        <td style="text-align:left;">20</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       <tr>        <td style="text-align:left;">         <em>Adv<sub>p</sub>         </em>        </td>        <td style="text-align:left;">ent.</td>        <td style="text-align:left;">0.287</td>        <td style="text-align:left;">gini</td>        <td style="text-align:left;">0.068</td>        <td style="text-align:left;">16</td>        <td style="text-align:left;">0.66</td>        <td style="text-align:center;">0.0037</td>       </tr>       </tbody>      </table>     </div>    </section>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Influence of Sampling Methods</h3>     </div>    </header>    <p>In this section, we discuss the influence of the different sampling methods. Since the <SmallCap>Random Forest</SmallCap> classifier achieves the best results, we investigate the effects of sampling methods on our <SmallCap>Random Forest</SmallCap> configuration. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186028/images/www2018-37-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">F1 scores macro averages [%] for the <SmallCap>Random Forest</SmallCap> classifier with respect to dataset and sampling method.</span>      </div>     </figure>    </p>    <p>Figure <a class="fig" href="#fig4">4</a> shows the F1 scores with respect to the sampling method for <em>Events</em> and the individual <em>Movies</em> genre datasets. The use of <em>pld-aware sampling</em> yields up to 17% percentage points better results than the use of <em>stratified random sampling</em>.</p>    <p>We observe that the use of a more diverse training set (i.e. a dataset including more data from long-tail domains e.g. obtained through the <em>pld-aware sampling</em>) has a significant and beneficial effect on the classification outcome (paired t-test with <em>p</em> < 0.03).</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Influence of Features</h3>     </div>    </header>    <p>In this section, we discuss the influence of the proposed features. We focus on the best performing classifier (<SmallCap>Random Forest</SmallCap>) while investigating the effects of varying the feature set.</p>    <div class="table-responsive" id="tab6">     <div class="table-caption">      <span class="table-number">Table 6:</span>      <span class="table-title">Random Forest F1 scores macro averages [%] for different feature combinations (<em>Events</em> datasets).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        Features       </th>       <th style="text-align:left;">        <em>Events</em><sub><em>s</em></sub>       </th>       <th style="text-align:left;">       <em>Events</em><sub><em>p</em></sub>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <em>tld/pld</em>       </td>       <td style="text-align:left;">65.30</td>       <td style="text-align:left;">76.29</td>       </tr>       <tr>       <td style="text-align:left;">        <em>page-vocab</em>       </td>       <td style="text-align:left;">62.57</td>       <td style="text-align:left;">79.8</td>       </tr>       <tr>       <td style="text-align:left;">        <em>node-vocab</em>       </td>       <td style="text-align:left;">60.66</td>       <td style="text-align:left;">68.09</td>       </tr>       <tr>       <td style="text-align:left;">        <em>tld/pld,page-vocab</em>       </td>       <td style="text-align:left;">71.01</td>       <td style="text-align:left;">80.03</td>       </tr>       <tr>       <td style="text-align:left;">        <em>tld/pld,node-vocab</em>       </td>       <td style="text-align:left;">65.38</td>       <td style="text-align:left;">77.65</td>       </tr>       <tr>       <td style="text-align:left;">        <em>page-vocab,node-vocab</em>       </td>       <td style="text-align:left;">71.70</td>       <td style="text-align:left;">80.27</td>       </tr>       <tr>       <td style="text-align:left;">        <em>tld/pld,page-vocab,node-vocab</em>       </td>       <td style="text-align:left;">71.67</td>       <td style="text-align:left;">79.59</td>       </tr>      </tbody>     </table>    </div>    <div class="table-responsive" id="tab7">     <div class="table-caption">      <span class="table-number">Table 7:</span>      <span class="table-title">Random Forest F1 scores macro averages [%] for different feature combinations (<em>Movies</em> datasets).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        Features       </th>       <th style="text-align:left;">        <em>Movies</em><sub><em>s</em></sub>       </th>       <th style="text-align:left;">        <em>Movies</em><sub><em>p</em></sub>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <em>tld/pld</em>       </td>       <td style="text-align:left;">66.32</td>       <td style="text-align:left;">80.56</td>       </tr>       <tr>       <td style="text-align:left;">        <em>page-vocab</em>       </td>       <td style="text-align:left;">72.27</td>       <td style="text-align:left;">82.14</td>       </tr>       <tr>       <td style="text-align:left;">        <em>node-vocab</em>       </td>       <td style="text-align:left;">73.96</td>       <td style="text-align:left;">82.18</td>       </tr>       <tr>       <td style="text-align:left;">        <em>tld/pld,page-vocab</em>       </td>       <td style="text-align:left;">72.59</td>       <td style="text-align:left;">82.68</td>       </tr>       <tr>       <td style="text-align:left;">        <em>tld/pld,node-vocab</em>       </td>       <td style="text-align:left;">74.28</td>       <td style="text-align:left;">82.94</td>       </tr>       <tr>       <td style="text-align:left;">        <em>page-vocab,node-vocab</em>       </td>       <td style="text-align:left;">74.33</td>       <td style="text-align:left;">82.59</td>       </tr>       <tr>       <td style="text-align:left;">        <em>tld/pld,page-vocab,node-vocab</em>       </td>       <td style="text-align:left;">74.46</td>       <td style="text-align:left;">83.14</td>       </tr>      </tbody>     </table>    </div>    <p>Table <a class="tbl" href="#tab6">6</a> presents the F1 scores obtained through <SmallCap>Random Forest</SmallCap> on the <em>Events</em> dataset with respect to different feature combinations. Our results indicate that the influence of features varies strongly dependent on the respective types and classes. This seems intuitive, given that some classes might be more specifically characterised by certain features, such as a set of plds.</p>    <p>The <em>tld/pld</em> features alone result in a reasonable performance for <em>Events</em> but not for <em>Movies</em>. This indicates that the source of the markup node is stronger correlated with its actual type or category for <em>Events</em> than for <em>Movies</em>. This seems intuitive, given that event-centred Websites tend to be more focused on certain event types than movie-centred Websites are focused on particular genres. However, these observations are likely to vary strongly dependent on the actual classification task. In contrast, the <em>node-vocab</em> alone is not sufficient to determine the event subtype with high F1 score. This observation corresponds to the insufficient performance of the <em>SD-Type</em> baseline.</p>    <p>The combination of <em>tld/pld</em> and <em>node-vocab</em> results only in a slight improvement of the results for <em>Events<sub>p</sub>     </em>. A dependence between the two features seems intuitive as pages extracted from the same pld are likely to be maintained by the same organisation and thus typically use the same set of <em>schema.org</em> terms. For instance, an event database is likely to assign the same set of properties to each event resulting in a characteristic <em>node-vocabulary</em> for the events of a single pld. Since the <em>page-vocab</em> considers the terms that occur on the whole page, the number of considered terms is higher, which results in better chances to find usage of the same terms on other Web pages. This is reflected by the fact that both combinations of <em>tld/pld, page-vocab</em> and <em>page-vocab, node-vocab</em> lead to an improvement while the performance of <em>tld/pld, node-vocab</em> is roughly the same as <em>tld/pld</em> only. The combination of all three features yields in a slight decrease of the F1 score compared to <em>tld/pld, page-vocab</em> only, indicating once more that the information contributed by <em>node-vocab</em> is already provided by <em>tld/pld</em>.</p>    <p>Table <a class="tbl" href="#tab7">7</a> shows average F1 scores using the <SmallCap>Random Forest</SmallCap> classifier on the <em>Movies</em> datasets. In contrast to the <em>Events</em> datasets we can achieve relatively good performance by employing only the <em>node-vocab</em> feature. Another difference is that we can observe a slightly larger margin between the exclusive use of <em>node-vocab</em> and <em>tld/pld</em>. This indicates that markup of movies of certain genres tend to exhibit the same <em>schema.org</em> terms. Any combination of two or more features results in similar outcomes (with approximatly 1 percentage point difference).</p>    <p>In both domains we can see a substantial difference in the performance with respect to the sampling methods for all feature combinations. <em>pld-aware sampling</em> consistently achieves higher F1 scores than <em>stratified random sampling</em>, leading to the conclusion that individual features and feature combinations benefit from <em>pld-aware sampling</em>.</p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Discussion</h3>     </div>    </header>    <p>Our experiments illustrated that traditional knowledge graph completion approaches that are not specifically designed for Web markup data may not be directly applicable to this kind of data, mainly due to the sparsity of individuals and the lack of connectivity in Web markup. Moreover, we observed that it is not sufficient to consider only node-specific features such as <em>node-vocab</em> to infer missing categorical information in Web markup. In contrast, contextual features such as <em>tld/pld</em> and <em>page-vocab</em> provide important information to infer missing statements.</p>    <p>In particular, our experiments demonstrated that contextual features such as <em>tld/pld</em> and <em>page-vocab</em> are discriminative for both tasks under consideration. These features are effective because many Websites focus on a particular topic, e.g. theater or music events. We observed that the <em>page-vocab</em> feature is especially useful in both tasks, as it describes the context of the particular node in a more specific way. Whereas the use of the <em>tld/pld</em> feature can naturally only be applied to instances from known plds, i.e. plds that are contained in the training data, performance drops are expected when classifying data from unknown plds. However, our results indicate that features representative for certain kinds of plds, such as <em>page-vocab</em>, can serve as potent substitute able to efficiently classify markup from unknown sources.</p>    <p>Limitations arise from the focus on two particular tasks only. We anticipate variation in performance of particular features when applying this approach to other kinds of categorical information. Similarly, considering that our ground truth has been constructed by relying on markup nodes where the sought-after information was present already on the Web, one might argue that this constraint has led to a bias towards markup nodes of generally higher quality. Additional experiments on an unconstrained and randomly selected ground truth will investigate this assumption further as part of future work.</p>    </section>   </section>   <section id="sec-24">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Related Work</h2>    </div>    </header>    <p>In this section we discuss related work in the areas of knowledge graph completion and schema inference for traditional knowledge graphs along with works focused directly on Web markup.</p>    <p>Existing approaches to knowledge graph completion and dataset profiling including its applications to schema inference have been summarised in recent survey articles [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. These approaches include in particular entity type inference, relation prediction and relation validation. In the context of KG completion, entity type inference is most commonly addressed as a multi-class prediction problem. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] makes use of properties and conditional probabilities to infer entity types, building the baseline for our approach. Schemex is an approach to extract and index schema information from Linked Open Data (LOD) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. In YAGO+F instance-based matching enables to enrich Freebase entities with YAGO concepts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] made use of Schemex to analyse schema information of LOD and found that properties provide information about subject types. In our work, we use properties as features for inferring missing categorical information in general. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] predicts relations between two nodes by leveraging random walk inference methods using sub-graphs to improve the path ranking algorithm (PRA), initially proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] also builds on PRA and extends it to a multi-task learning approach.</p>    <p>All of the works discussed above have been applied to traditional KGs such as DBpedia, NELL and YAGO. In contrast, in this work we aim at inferring information on the Web markup data. Web markup is distinguished from the aforementioned knowledge graphs by specific characteristics, i.e. annotations are often very sparse or noisy, vocabularies are not used correctly in many cases and the overall RDF graph is connected very loosely [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. For these reasons, existing KG completion methods are not likely to perform well on Web markup. For instance, KG completion approaches based on graph topology (e.g. relation prediction discussed above) rely on the presence of relations, which are not widely available in markup.</p>    <p>Various approaches employ embeddings for KG completion in traditional knowledge graphs. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] conducted a survey on KG embeddings for applications such as link prediction, entity classification and triple classification. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] makes use of embeddings and rules. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] propose the <em>TransR</em> model that builds separate entity and relation embeddings to compute the plausibility of missing triples. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] predicts relations between entities by employing neural tensor networks. Embeddings techniques have not yet been applied to Web markup yet lend themselves as direction for future research.</p>    <p>Several recent studies focused on analysing the characteristics, evolution and coverage of markup [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] and on addressing specific tasks in the context of Web markup. Meusel et al. proposed heuristics that can be employed to fix common errors in Web markup [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. In this work, we applied the heuristics proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] for pre-processing and data cleansing. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] provide pipelines for data fusion and entity summarisation on Web markup, involving heuristics, clustering and supervised approaches for entity matching and classification of markup statements. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] builds on these works by utilising fused markup data to augment existing knowledge bases, showing the complementarity of markup data and its potential to significantly complement information from traditional reference KGs.</p>    <p>While these works demonstrate the use of markup data, they suffer from the sparsity of individual nodes. The inference approach proposed in our work can augment markup nodes and is likely to boost the performance on both fusion as well as KG augmentation tasks. In particular, considering the impact of the use of controlled vocabularies on data reuse [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>], we anticipate that inference of crucial categorical information can facilitate reuse of markup data.</p>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion &#x0026; Future Work</h2>    </div>    </header>    <p>In this work, we addressed the problem of interpreting noisy and sparse Web markup by proposing an approach for automatically inferring categorical information for particular entities, thereby augmenting sparse markup nodes with information, which often is essential when interpreting markup and the corresponding Web pages. We leveraged the large amount of publicly available data as training data for a supervised machine learning approach. We employed Web markup specific features such as <em>tld/pld</em>, <em>node vocabulary</em> and <em>page vocabulary</em> and conducted an extensive evaluation of different classification algorithms, sampling methods and feature sets. Our proposed configuration outperforms existing baselines significantly, with <SmallCap>Random Forest</SmallCap> providing the most consistent performance across classes and datasets.</p>    <p>By applying our approach to the problem of inferring event types and movie genres, we demonstrated that supervised inference can uncover entity-centric categorical information, which is essential when interpreting markup or Websites in general. Potential applications include knowledge base augmentation from Web markup [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], Website classification or Web search in general. Considering the still limited experiments and the limitations of our dataset (Section <a class="sec" href="#sec-23">5.4</a>), future work will include the conduction of additional experiments, involving more diverse datasets and tasks. With respect to the latter, current experiments are being conducted on discretised information rather than properties, which are apriori categorical. In addition, we aim at investigating the impact of our approach when being included into data fusion and KG augmentation pipelines, such as [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>].</p>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>Acknowledgments</h2>    </div>    </header>    <p>This work was partially funded by the European Commission (&#x201D;AFEL&#x201D; project, grant ID 687916) and the BMBF (&#x201D;Data4UrbanMobility&#x201D; project, grant ID 02K15A040).</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Mohamed Ben&#x00A0;Ellefi, Zohra Bellahsene, Breslin John, Elena Demidova, Stefan Dietze, Julian Szymanski, and Konstantin Todorov. 2017. RDF Dataset Profiling - a Survey of Features, Methods, Vocabularies and Applications. <em>      <em>Semantic Web Journal</em>     </em>(2017). to appear.</li>    <li id="BibPLXBIB0002" label="[2]">James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-parameter Optimization. <em>      <em>J. Mach. Learn. Res.</em>     </em>13 (Feb. 2012), 281&#x2013;305.</li>    <li id="BibPLXBIB0003" label="[3]">Christian Bizer, Kai Eckert, Robert Meusel, Hannes M&#x00FC;hleisen, Michael Schuhmacher, and Johanna V&#x00F6;lker. 2013. <em>      <em>Deployment of RDFa, Microdata, and Microformats on the Web &#x2013; A Quantitative Analysis</em>     </em>. Springer Berlin Heidelberg, 17&#x2013;32.</li>    <li id="BibPLXBIB0004" label="[4]">Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. In <em>      <em>Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</em>     </em>(<em>      <em>SIGMOD &#x2019;08</em>     </em>). ACM, 1247&#x2013;1250.</li>    <li id="BibPLXBIB0005" label="[5]">Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo&#x00A0;N. Mendes. 2013. Improving Efficiency and Accuracy in Multilingual Entity Extraction. In <em>      <em>Proceedings of the 9th International Conference on Semantic Systems</em>     </em>(<em>      <em>I-SEMANTICS &#x2019;13</em>     </em>). ACM, 121&#x2013;124.</li>    <li id="BibPLXBIB0006" label="[6]">Elena Demidova, Iryna Oelze, and Wolfgang Nejdl. 2013. Aligning Freebase with the YAGO Ontology. In <em>      <em>Proceedings of the 22Nd ACM International Conference on Information &#x0026; Knowledge Management</em>     </em> (<em>      <em>CIKM &#x2019;13</em>     </em>). ACM, 579&#x2013;588.</li>    <li id="BibPLXBIB0007" label="[7]">Stefan Dietze, Davide Taibi, Ran Yu, Phil Barker, and Mathieu d&#x0027;Aquin. 2017. Analysing and Improving Embedded Markup of Learning Resources on the Web. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web Companion</em>     </em>(<em>      <em>WWW &#x2019;17 Companion</em>     </em>). International World Wide Web Conferences Steering Committee, 283&#x2013;292.</li>    <li id="BibPLXBIB0008" label="[8]">Kemele&#x00A0;M. Endris, Jos&#x00E9;&#x00A0;M. Gim&#x00E9;nez-Garc&#x00ED;a, Harsh Thakkar, Elena Demidova, Antoine Zimmermann, Christoph Lange, and Elena Simperl. 2017. Dataset Reuse: An Analysis of References in Community Discussions, Publications and Data. In <em>      <em>Proceedings of the Ninth International Conference on Knowledge Capture (K-CAP 2017)</em>     </em>.</li>    <li id="BibPLXBIB0009" label="[9]">Matt Gardner and Tom&#x00A0;M. Mitchell. 2015. Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction. In <em>      <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015</em>     </em>. 1488&#x2013;1498.</li>    <li id="BibPLXBIB0010" label="[10]">Thomas Gottron, Malte Knauf, Stefan Scheglmann, and Ansgar Scherp. 2013. A Systematic Investigation of Explicit and Implicit Schema Information on the Linked Open Data Cloud. In <em>      <em>Proceedings of the ESWC 2013</em>     </em>. Springer Berlin Heidelberg, 228&#x2013;242.</li>    <li id="BibPLXBIB0011" label="[11]">R.&#x00A0;V. Guha, Dan Brickley, and Steve Macbeth. 2016. Schema.Org: Evolution of Structured Data on the Web. <em>      <em>Commun. ACM</em>     </em>59, 2 (Jan. 2016), 44&#x2013;51.</li>    <li id="BibPLXBIB0012" label="[12]">Mathias Konrath, Thomas Gottron, Steffen Staab, and Ansgar Scherp. 2012. SchemEX - Efficient Construction of a Data Catalogue by Stream-based Indexing of Linked Data. <em>      <em>Web Semant.</em>     </em>16 (Nov. 2012), 52&#x2013;58.</li>    <li id="BibPLXBIB0013" label="[13]">Ni Lao and William&#x00A0;W. Cohen. 2010. Relational Retrieval Using a Combination of Path-constrained Random Walks. <em>      <em>Mach. Learn.</em>     </em>81, 1 (Oct. 2010), 53&#x2013;67.</li>    <li id="BibPLXBIB0014" label="[14]">Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation Embeddings for Knowledge Graph Completion. In <em>      <em>Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</em>     </em>(<em>      <em>AAAI&#x2019;15</em>     </em>). AAAI Press, 2181&#x2013;2187.</li>    <li id="BibPLXBIB0015" label="[15]">Robert Meusel and Heiko Paulheim. 2015. Heuristics for Fixing Common Errors in Deployed Schema.Org Microdata. In <em>      <em>Proceedings of the 12th European Semantic Web Conference on The Semantic Web</em>     </em>. Springer-Verlag New York, Inc., 152&#x2013;168.</li>    <li id="BibPLXBIB0016" label="[16]">Robert Meusel, Petar Petrovski, and Christian Bizer. 2014. The WebDataCommons Microdata, RDFa and Microformat Dataset Series. In <em>      <em>Proceedings of the 13th International Semantic Web Conference - Part I</em>     </em>(<em>      <em>ISWC &#x2019;14</em>     </em>). Springer-Verlag New York, Inc., 277&#x2013;292.</li>    <li id="BibPLXBIB0017" label="[17]">Robert Meusel, Dominique Ritze, and Heiko Paulheim. 2016. Towards More Accurate Statistical Profiling of Deployed schema.org Microdata. <em>      <em>ACM Journal of Data and Information Quality</em>     </em>8, 1 (2016).</li>    <li id="BibPLXBIB0018" label="[18]">Heiko Paulheim. 2016. Knowledge graph refinement: A survey of approaches and evaluation methods. <em>      <em>Semantic Web</em>     </em>Preprint(2016), 1&#x2013;20.</li>    <li id="BibPLXBIB0019" label="[19]">Heiko Paulheim and Christian Bizer. 2013. Type Inference on Noisy RDF Data. In <em>      <em>Proceedings of the 12th International Semantic Web Conference - Part I</em>     </em> (<em>      <em>ISWC &#x2019;13</em>     </em>). Springer-Verlag New York, Inc., 510&#x2013;525.</li>    <li id="BibPLXBIB0020" label="[20]">Pracheta Sahoo, Ujwal Gadiraju, Ran Yu, Sriparna Saha, and Stefan Dietze. 2016. Analysing Structured Scholarly Data Embedded in Web Pages. (April 2016).</li>    <li id="BibPLXBIB0021" label="[21]">Amit Singhal. 2012. Introducing the Knowledge Graph: things, not strings. Official Google Blog. (May 2012). <a class="link-inline force-break"      href="https://googleblog.blogspot.de/2012/05/introducing-knowledge-graph-things-not.html"      target="_blank">https://googleblog.blogspot.de/2012/05/introducing-knowledge-graph-things-not.html</a>accessed on 01/20/2018.</li>    <li id="BibPLXBIB0022" label="[22]">Richard Socher, Danqi Chen, Christopher&#x00A0;D Manning, and Andrew Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In <em>      <em>Advances in Neural Information Processing Systems 26</em>     </em>. Curran Associates, Inc., 926&#x2013;934.</li>    <li id="BibPLXBIB0023" label="[23]">Fabian&#x00A0;M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In <em>      <em>Proceedings of the 16th International Conference on World Wide Web</em>     </em> (<em>      <em>WWW &#x2019;07</em>     </em>). ACM, 697&#x2013;706.</li>    <li id="BibPLXBIB0024" label="[24]">Davide Taibi and Stefan Dietze. 2016. Towards embedded markup of learning resources on the Web: a quantitative Analysis of LRMI Terms Usage. In <em>      <em>Proceedings of the WWW Companion 2016</em>     </em>.</li>    <li id="BibPLXBIB0025" label="[25]">Quan Wang, Jing Liu, Yuanfei Luo, Bin Wang, and Chin-Yew Lin. 2016. Knowledge Base Completion via Coupled Path Ranking. In <em>      <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</em>     </em>. ACL - Association for Computational Linguistics, 1308&#x2013;1318.</li>    <li id="BibPLXBIB0026" label="[26]">Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge Graph Embedding: A Survey of Approaches and Applications. <em>      <em>IEEE Trans. Knowl. Data Eng.</em>     </em>29, 12 (2017), 2724&#x2013;2743.</li>    <li id="BibPLXBIB0027" label="[27]">Quan Wang, Bin Wang, and Li Guo. 2015. Knowledge Base Completion Using eddings and Rules. In <em>      <em>Proceedings of the 24th International Conference on Artificial Intelligence</em>     </em> (<em>      <em>IJCAI&#x2019;15</em>     </em>). AAAI Press, 1859&#x2013;1865.</li>    <li id="BibPLXBIB0028" label="[28]">Ran Yu, Besnik Fetahu, Ujwal Gadiraju, and Stefan Dietze. 2016. A Survey on Challenges in Web Markup Data for Entity Retrieval. In <em>      <em>Proceedings of the ISWC 2016 Posters &#x0026; Demonstrations Track</em>     </em>.</li>    <li id="BibPLXBIB0029" label="[29]">Ran Yu, Ujwal Gadiraju, Besnik Fetahu, and Stefan Dietze. 2017. FuseM: Query-Centric Data Fusion on Structured Web Markup. In <em>      <em>Proceedings of the IEEE 33rd International Conference on Data Engineering (ICDE), 2017</em>     </em>. IEEE Computer Society, 179&#x2013;182.</li>    <li id="BibPLXBIB0030" label="[30]">Ran Yu, Ujwal Gadiraju, Besnik Fetahu, Oliver Lehmberg, Dominique Ritze, and Stefan Dietze. 2017. KnowMore - Knowledge Base Augmentation with Structured Web Markup. <em>      <em>Semantic Web Journal, IOS Press</em>     </em>(2017).</li>    <li id="BibPLXBIB0031" label="[31]">Ran Yu, Ujwal Gadiraju, Xiaofei Zhu, Besnik Fetahu, and Stefan Dietze. 2016. Towards Entity Summarisation on Structured Web Markup. <em>      <em>The Semantic Web: ESWC 2016 Satellite Events,</em>     </em> (June 2016).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>NOTES</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>RDFa W3C recommendation: <a class="link-inline force-break" href="http://www.w3.org/TR/xhtml-rdfa-primer/">http://www.w3.org/TR/xhtml-rdfa-primer/</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://www.w3.org/TR/microdata/">https://www.w3.org/TR/microdata/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="http://microformats.org">http://microformats.org</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break" href="http://commoncrawl.org/">http://commoncrawl.org/</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break"    href="http://webdatacommons.org/structureddata/2016-10/stats/stats.html">http://webdatacommons.org/structureddata/2016-10/stats/stats.html</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class="link-inline force-break" href="http://webdatacommons.org/">http://webdatacommons.org/</a>   </p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break" href="http://commoncrawl.org/">http://commoncrawl.org/</a>   </p>   <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>Detailed numbers can be found at <a class="link-inline force-break"    href="http://webdatacommons.org/structureddata/index.html">http://webdatacommons.org/structureddata/index.html</a>   </p>   <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a><a class="link-inline force-break" href="http://schema.org/translator">http://schema.org/translator</a>   </p>   <p id="fn10"><a href="#foot-fn10"><sup>10</sup></a><a class="link-inline force-break" href="http://schema.org/Event">http://schema.org/Event</a>   </p>   <p id="fn11"><a href="#foot-fn11"><sup>11</sup></a><a class="link-inline force-break" href="http://schema.org/CreativeWork">http://schema.org/CreativeWork</a>   </p>   <p id="fn12"><a href="#foot-fn12"><sup>12</sup></a><a class="link-inline force-break" href="http://schema.org/Organization">http://schema.org/Organization</a>   </p>   <p id="fn13"><a href="#foot-fn13"><sup>13</sup></a><a class="link-inline force-break" href="http://schema.org/Person">http://schema.org/Person</a>   </p>   <p id="fn14"><a href="#foot-fn14"><sup>14</sup></a>For a brief description of 1-hot-encoding see: <a class="link-inline force-break"    href="http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features">http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features</a>   </p>   <p id="fn15"><a href="#foot-fn15"><sup>15</sup></a><a class="link-inline force-break" href="http://www.imdb.com/genre/">http://www.imdb.com/genre/</a>   </p>   <p id="fn16"><a href="#foot-fn16"><sup>16</sup></a>The datasets can be found at <a class="link-inline force-break" href="http://markup.l3s.de">http://markup.l3s.de</a>. </p>   <p id="fn17"><a href="#foot-fn17"><sup>17</sup></a><a class="link-inline force-break" href="http://www.dbpedia-spotlight.org/faq">http://www.dbpedia-spotlight.org/faq</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY&#x00A0;4.0 License.<br/>ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186028">https://doi.org/10.1145/3178876.3186028</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
