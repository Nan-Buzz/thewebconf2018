<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>HARE: An Engine for Enhancing Answer Completeness of
  SPARQL Queries via Crowdsourcing</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186241'>https://doi.org/10.1145/3184558.3186241</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186241'>https://w3id.org/oa/10.1145/3184558.3186241</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">HARE: An Engine for Enhancing
          Answer Completeness of SPARQL Queries via
          Crowdsourcing</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Maribel</span> <span class=
          "surName">Acosta</span> Karlsruhe Institute of
          Technology, <a href=
          "mailto:maribel.acosta@kit.edu">maribel.acosta@kit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Elena</span> <span class=
          "surName">Simperl</span> University of Southampton,
          <a href=
          "mailto:e.simperl@soton.ac.uk">e.simperl@soton.ac.uk</a>
        </div>
        <div class="author">
          <span class="givenName">Fabian</span> <span class=
          "surName">Flöck</span> GESIS - Leibniz Institute for the
          Social Sciences, <a href=
          "mailto:fabian.floeck@gesis.org">fabian.floeck@gesis.org</a>
        </div>
        <div class="author">
          <span class="givenName">Maria-Esther</span> <span class=
          "surName">Vidal</span> TIB Information Center /
          Universidad Simon Bolivar, <a href=
          "mailto:maria.vidal@tib.eu">maria.vidal@tib.eu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186241"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186241</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We propose HARE, a SPARQL query engine that
        encompasses human-machine query processing to augment the
        completeness of query answers. We empirically assessed the
        effectiveness of HARE on 50 SPARQL queries over DBpedia.
        Experimental results clearly show that our solution
        accurately enhances answer completeness.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Crowdsourcing;</strong> <em>Data cleaning;</em>
        <em>World Wide Web;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>SPARQL; Crowdsourcing;
          Completeness; Query Execution; RDF</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Maribel Acosta, Elena Simperl, Fabian Flöck, and
          Maria-Esther Vidal. 2018. HARE: An Engine for Enhancing
          Answer Completeness of SPARQL Queries via Crowdsourcing.
          In <em>WWW '18 Companion: The 2018 Web Conference
          Companion,</em> <em>April 23–27, 2018 (WWW ’18
          Companion),</em> <em>Lyon, France. ACM, New York, NY,
          USA</em> 5 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186241" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186241</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <figure id="fig1">
      <img src=
      "../../../data/deliveryimages.acm.org/10.1145/3190000/3186241/images/www18companion-94-fig1.jpg"
      class="img-responsive" alt="Figure 1" longdesc="" />
      <div class="figure-caption">
        <span class="figure-number">Figure 1:</span> <span class=
        "figure-title">Motivating example. (a) Portion of DBpedia
        for movies and producers. Missing values in the RDF graph
        are highlighted. (b) SPARQL query executed against DBpedia.
        Portions of the query (highlighted) affected by missing
        values are crowdsourced. (c) Crowd answers are mapped into
        RDF to augment the result of queries.</span>
      </div>
    </figure>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>As in traditional semi-structured data models, RDF allows
      for creating datasets that result from integrating multiple,
      and typically heterogenous and unstructured data sources. In
      RDF datasets, triples represent positive statements, under
      the open world assumption. These key aspects of RDF data
      impose fundamental challenges on deciding data completeness:
      it is unknown a priori whether a value is actually missing in
      the dataset, thus negatively impacting completeness of tasks
      of Linked Data consumption and query processing. To
      illustrate, consider the motivating example depicted in
      Figure&nbsp;<a class="fig" href="#fig1">1</a>. The SPARQL
      query (cf. Figure&nbsp;1b) executed over
      DBpedia&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>] selects movies, including their
      producers, that have been filmed by Universal Pictures. The
      query execution returns no producers for 239 out of the 1,461
      movies filmed by Universal Pictures. An inspection of the
      query results reveals that DBpedia has no producers for
      dbr:Tower_Heist (cf. Figure&nbsp;1a). However, manually
      checking external sources, this movie has in fact 3
      producers. With cases like this being a common occurrence in
      RDF datasets, further techniques are needed to improve query
      answer completeness.</p>
      <p><strong>Problem Statement.</strong>We tackle the problem
      of automatically identifying and completing portions of a
      SPARQL query evaluated over an RDF dataset that yields
      incomplete results.</p>
      <p><strong>Related Work.</strong>The Database and Semantic
      Web communities have extensively studied methods for assuring
      data quality in traditional databases&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>] as well as on web
      data&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0002">2</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>]. Despite
      all these developments, common sense knowledge acquired from
      humans may be required for improving effectiveness of
      automatic methods of data quality assessment&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>]. In the
      context of data management, crowdsourcing has been used to
      design advanced query processing systems that combine human
      and computational intelligence&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0009">9</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>]. The Database community
      has proposed several human/computer query processing
      architectures for relational data. In approaches such as
      CrowdDB&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>], Deco&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0014">14</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>], Qurk&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>], and
      CrowdOp&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>], existing microtask platforms are
      embedded in query processing systems. These systems provide
      declarative languages tailored to facilitate an adaptive
      design of hybrid query execution pipelines. Albeit effective
      for relational databases, such approaches are less feasible
      for a Linked Data scenario, which is confronted with
      autonomous and heterogeneous RDF datasets.</p>
      <p><strong>Motivation and Research Questions.</strong>Due to
      the semi-structured nature of RDF and the assumptions of the
      RDF model (e.g., the open world assumption), the results of
      crowd-based relational solutions cannot be directly applied
      to the problem of SPARQL query processing. Therefore, we
      investigate the following research questions:</p>
      <ol class="list-no-style">
        <li id="list1" label="label=">Can answers of SPARQL queries
        be completed via hybrid computation without incurring
        additional complexity in query evaluation?<br /></li>
        <li id="list2" label="lbbel=">Can answer completeness of
        SPARQL queries be augmented via microtasks?<br /></li>
        <li id="list3" label="lcbel=">What is the impact of
        exploiting the semantics of RDF resources on crowd
        effectiveness and efficiency when solving missing
        values?<br /></li>
      </ol>
      <p><strong>Methodology and Contributions.</strong>We propose
      HARE&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>], a hybrid
      SPARQL query engine. HARE integrates crowd knowledge into
      query processing to enhance query answer completeness. We
      formally demonstrate the theoretical properties of HARE. In
      addition, we empirically assess the performance of HARE while
      evaluating 50 SPARQL queries against DBpedia (version 2014).
      The novel contributions of this work can be summarized as
      follows&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>]:</p>
      <ul class="list-no-style">
        <li id="list4" label="•">An RDF completeness model that
        exploits the topology of RDF graphs to estimate the answer
        completeness of SPARQL queries where triple patterns have
        variables in the subject, predicate, or object
        position.<br /></li>
        <li id="list5" label="•">A formal definition of the
        operations carried out by the proposed microtask
        manager.<br /></li>
        <li id="list6" label="•">A fuzzy set semantics of the
        SPARQL query language; we formally prove the complexity of
        computing the solution of SPARQL queries under the proposed
        semantics.<br /></li>
        <li id="list7" label="•">A query engine able to evaluate
        SPARQL queries respecting the proposed SPARQL fuzzy set
        semantics.<br /></li>
        <li id="list8" label="•">An extensive empirical evaluation
        that demonstrates the impact of HARE on the effectiveness
        of the crowd.<br /></li>
      </ul>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Our Approach:
          HARE</h2>
        </div>
      </header>
      <p>We propose HARE, a query engine that automatically
      identifies portions of a SPARQL query that might yield
      incomplete results and resolves them via crowdsourcing. The
      input of the engine is a SPARQL query <em>Q</em> and a
      quality threshold <em>τ</em> ∈ [0.0; 1.0]. As part of HARE,
      we devise an RDF completeness model that estimates missing
      values in sub-graphs of a dataset, exploiting the topology of
      the RDF graph and the knowledge collected from the crowd.</p>
      <p>The query engine takes into consideration <em>τ</em>, the
      completeness model, and RDF triples collected from the crowd.
      Potential missing values are passed to the microtask manager,
      which contacts the crowd to complete the missing values in
      the dataset. The HARE engine efficiently combines results
      retrieved from the dataset with human input to produce the
      final results for <em>Q</em>.</p>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> RDF
            Completeness Model</h3>
          </div>
        </header>
        <p>We propose a model to estimate the completeness of
        portions of RDF datasets. The intuition behind our model is
        to capture the number of different subjects, predicates,
        and objects in RDF triples, i.e., the multiplicity of RDF
        resources. To estimate the completeness of a given
        resource, the model compares its multiplicity with the
        multiplicity of other resources that belong to the same
        classes in the dataset. Then, by comparing the topology of
        other resources, the model can estimate whether a value is
        missing for a resource.</p>
        <p>For example, consider the RDF graph from Figure&nbsp;1a
        . The movie dbr:Tower_Heist contains no values for the
        predicate dbp:producer. Nonetheless, other resources of the
        class schema.org:Movie (e.g, dbr:The_Interpreter) have
        multiplicity 3 for dbp:producer. Based on this information,
        and considering all the resources of the class, the
        completeness model can estimate that the movie
        dbr:Tower_Heist is incomplete with respect to this
        predicate.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span>
            Representation of the Crowd Knowledge</h3>
          </div>
        </header>
        <p>RDF triples allow for representing positive facts,
        nonetheless, considering negative knowledge is crucial to
        model the local closed world assumption. Moreover, using
        human knowledge effectively demands the representation of
        negative or even unknown statements: in some cases, human
        contributors might assert that a statement cannot hold or
        that they do not know the answer to a question. Therefore,
        in HARE, the knowledge from the crowd <em>CKB</em> is
        captured in three knowledge bases:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ CKB=(CKB^+, CKB^-, CKB^\sim)
            \]</span><br />
          </div>
        </div><em>CKB</em> <sup>+</sup> comprises RDF triples that
        should belong to the dataset (positive facts). <em>CKB</em>
        <sup>−</sup> contains triples that should not exist in the
        dataset (negative facts). <em>CKB</em> <sup>∼</sup> lists
        all associations that the crowd could not confirm or deny
        (unknown facts). The crowd knowledge bases are modeled as
        fuzzy sets, i.e., triples are annotated with a membership
        degree that represents the confidence of the crowd.
        <p></p>
        <p>The representation of crowd knowledge as <em>CKB</em>
        <sup>+</sup>, <em>CKB</em> <sup>−</sup>, and <em>CKB</em>
        <sup>∼</sup> allows for modeling contradictions or
        unknownness in crowd answers. Contradiction about the
        existence of a value arises when members of the crowd
        assert that the value exists and does not exist. In HARE,
        contradictions can be detected by comparing the triples in
        <em>CKB</em> <sup>+</sup> and <em>CKB</em> <sup>−</sup>.
        Unknownness indicates that the crowd does not know whether
        a value exists. Statements for which the crowd has declared
        to be unknowledgeable about are stored in <em>CKB</em>
        <sup>∼</sup>.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Query
            Engine</h3>
          </div>
        </header>
        <p>The HARE query engine combines information from RDF
        datasets, the crowd knowledge bases containing curated
        results of prior crowdsourced tasks, and the crowd itself.
        Because RDF data collected from the crowd knowledge bases
        and the crowd is not necessarily precise, our approach uses
        fuzzy RDF to capture multiple degrees of vagueness and
        imprecision when combining data from crowd-based sources.
        In&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>], we propose a fuzzy set semantics
        of SPARQL queries such that correct data from RDF datasets
        and vague data from crowd knowledge bases can be merged
        during SPARQL query processing. Furthermore,
        in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
        we demonstrated the theorems that derived the following
        corollary.</p>
        <div class="corollary" id="enc1">
          <label>Corollary 2.1.</label>
          <p>The complexity of computing the mapping set of a
          SPARQL query under fuzzy set semantics is the same as
          when it is computed under set semantics.</p>
        </div>
        <p>For the HARE query engine, we propose an efficient
        algorithm that executes BGPs of SPARQL queries under fuzzy
        set semantics. During query execution, the algorithm
        combines data from an RDF dataset <em>D</em> and a crowd
        knowledge base <em>CKB</em> that contains fuzzy sets of RDF
        data. In HARE, all triples in <em>D</em> are assumed to
        have membership degree equal to 1.0, since they are assumed
        to be correct. Furthermore, the query engine considers the
        completeness model and knowledge captured from the crowd.
        When the evaluation of a triple pattern leads to incomplete
        answers, the query engine verifies if the crowd can provide
        the missing mappings. HARE aims at crowdsourcing triple
        patterns where the crowd exhibits: i) high confidence
        values in positive or negative facts, or ii) high levels of
        contradiction but low unknownness.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186241/images/www18companion-94-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Microtask generated for
            crowdsourcing the triple pattern
            (dbr:Carotid_artery_dissection, dbp:icd, ?icd).</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.4</span> Microtask
            Manager</h3>
          </div>
        </header>
        <p>This component creates human tasks from triple patterns
        and collects the crowd answers. The microtask manager is
        composed of the user interface generator and the microtask
        executor.</p>
        <p>The user interface generator receives as input the
        triple patterns to be crowdsourced. This component is able
        to generate interfaces for triple patterns with at most one
        variable. In addition, this component exploits the
        semantics of RDF resources in triple patterns to build rich
        human-readable interfaces to RDF data. The human-readable
        information is obtained by dereferencing URIs in the triple
        pattern. The user interface generator displays the values
        (if available) of different properties of RDF resources
        (cf. Figure&nbsp;<a class="fig" href="#fig2">2</a>). In
        this way, HARE exploits the semantic descriptions of
        resources and includes further properties in the
        microtasks. The more properties to describe the resources
        are included in the microtasks, the less ambiguous the task
        is. Providing details like these in microtasks has also
        proven to assist the crowd in providing right
        answers&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0005">5</a>].</p>
        <p>The microtask executor submits the human tasks created
        by the user interface generator to the crowdsourcing
        platform. Answers provided by the crowd in each task are
        retrieved by the microtask executor and processed in order
        to update the crowd knowledge bases <em>CKB</em>
        accordingly.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186241/images/www18companion-94-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Effectiveness of the RDF
            completeness model.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experimental
          Study</h2>
        </div>
      </header>
      <p><strong>Query Benchmark.</strong>We designed a benchmark
      of 50 queries<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a> by analyzing triple patterns
      answerable by the DBpedia dataset (version 2014). The
      benchmark includes five categories with 10 queries each to
      study the crowd behavior across different domains: Sports,
      Music, Life Sciences, Movies, and History.</p>
      <p><strong>Gold Standard.</strong>We built a gold standard
      <em>D</em> <sup>*</sup> of missing answers by removing
      portions of the dataset. Depending on the query, the gold
      standard contains between 8% and 97% of the query answer.</p>
      <p><strong>Implementation.</strong> HARE is implemented in
      Python 2.7.6. and we use CrowdFlower&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>] as the crowdsourcing
      platform. We implemented two variants of our approach which
      generate different microtasks: <strong>HARE</strong> that
      exploits the semantics of resources as described in
      Section&nbsp;<a class="sec" href="#sec-10">2.4</a>, and
      <strong>HARE-BL</strong> is a baseline approach that simply
      substitutes URIs with labels in the microtasks.</p>
      <p><strong>Crowdsourcing Configurations.</strong>We asked
      workers to solve a maximum of 4 RDF triples per task. The
      monetary reward was 0.07 US dollars per task. We collected at
      least 3 answers per task.</p>
      <p>In this study, we submitted 1,004 triple patterns to the
      crowd with HARE and HARE-BL. In total, we collected 3,163
      crowd answers.</p>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> HARE
            Crowdsourcing Capabilities</h3>
          </div>
        </header>
        <p>We measure the number of triple patterns that are
        crowdsourced when executing the benchmark queries with HARE
        for different values of the threshold <em>τ</em>.
        Figure&nbsp;3a shows that the number of crowdsourced triple
        patterns differs per knowledge domain. In certain domains
        (such as History and Movies) the benchmark queries produce
        a large amount of results with respect to queries from
        other domains. Figure&nbsp;3a further indicates that in
        domains where queries produce large amount of results, HARE
        – based on the estimations of the completeness model – also
        crowdsources a large number of triple patterns. These
        results illustrate how the value of <em>τ</em> impacts the
        number of crowdsourced triple patterns: the higher the
        value of <em>τ</em> the lower is the requested completeness
        of the answer.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186241/images/www18companion-94-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Recall: Query answer
            completeness (y-axis) obtained with DBpedia (Dataset)
            and our approaches HARE and HARE-BL per query (x-axis).
            HARE consistently outperforms the other approaches in
            all benchmark queries.</span>
          </div>
        </figure>
        <p></p>
        <p>We then measure the percentage of crowdsourced triple
        patterns with respect to the size of intermediate results
        (Figure&nbsp;3b). For <em>τ</em> ≥ 0.50, the completeness
        model is able to prune the number of tasks submitted to the
        crowd. In particular, in domains where the benchmark
        queries produce a large number of intermediate results –
        such as History and Movies –, the completeness model
        reduces the number of crowdsourced triple patterns
        considerably (for <em>τ</em> = 0.75).</p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span>
            Completeness of Query Answers</h3>
          </div>
        </header>
        <p>To measure the effectiveness of our solution, we compute
        the proportion of completeness (PC) per query. PC
        corresponds to the ratio of answers produced by HARE to the
        answers when the same query is executed only against the
        dataset. Figure&nbsp;5a depicts the PC values achieved by
        HARE per knowledge domain. In all domains, the minimum PC
        values are higher than 1.0 indicating that HARE always
        increased the number of answers in all SPARQL queries. It
        is important to highlight that PC values are affected by
        the completeness of the dataset. This is the case, for
        instance, in DBpedia in the domains of Life Sciences and
        Movies, which exhibit high completeness. Therefore, the PC
        values achieved in these domains are not as high as for
        other knowledge domains in DBpedia.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186241/images/www18companion-94-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">HARE effectiveness.</span>
          </div>
        </figure>
        <p></p>
        <p>In addition, we report on the recall values obtained
        with HARE, HARE-BL, and when the queries are executed
        directly over the dataset (cf. Figure&nbsp;<a class="fig"
        href="#fig4">4</a>). We can observe that the recall when
        querying DBpedia varies among queries and knowledge
        domains. This indicates that completeness in DBpedia is
        heterogenous among different sub-graphs, in this case,
        represented by different knowledge domains. These results
        support the importance of taking into consideration the
        local completeness of resources. Furthermore, HARE and
        HARE-BL are able to improve on recall, which suggests that
        our RDF completeness model is able to capture the skewed
        distribution of values in real-world datasets. Lastly, HARE
        outperforms the other approaches suggesting that
        semantically enriched interfaces enable the crowd to
        provide correct answers.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Quality of
            Crowd Answers</h3>
          </div>
        </header>
        <p>In this study, we compute precision and recall of the
        triples retrieved from the crowd with respect to the gold
        standard <em>D</em> <sup>*</sup>.</p>
        <p>Figure&nbsp;<a class="fig" href="#fig5">5b</a> reports
        on the aggregated results of precision and recall of crowd
        answers obtained with HARE. It can be observed that
        precision values fluctuate over the knowledge domains. The
        lowest performance in terms of precision is obtained in the
        Music domain, where the median is 0.55. Still, the high
        value of the third quartile in the Music domain indicates
        that most of the precision values range from 0.55 to 0.90.
        Overall, the median precision values of HARE in the other
        domains are greater than 0.93. In turn, recall values are
        consistently high with median equal to 1.0.</p>
        <p>HARE microtasks assisted the crowd in reaching perfect
        precision and recall scores in 30 out of 50 SPARQL queries.
        These experiments confirm that exploiting the semantics of
        RDF resources allows the crowd to effectively solve missing
        RDF values which, in turn, enhances the answer completeness
        of SPARQL queries.</p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Conclusions and
          Outlook</h2>
        </div>
      </header>
      <p>We presented HARE, a SPARQL query engine that relies on an
      RDF completeness model and knowledge collected from the crowd
      to complete missing values. We conducted an experimental
      study using 50 queries over DBpedia. Based on the empirical
      results, we can answer the research questions formulated in
      Section&nbsp;<a class="sec" href="#sec-5">1</a>.</p>
      <p><strong>Answer to RQ1.</strong>We formally demonstrate
      that HARE solves the problem of identifying and evaluating
      SPARQL sub-queries that yield missing values in polynomial
      time.</p>
      <p><strong>Answer to RQ2.</strong>The crowd reached via
      microtasks resolves missing values in RDF graphs with high
      precision and recall which, in turn, increases the
      completeness of SPARQL queries. Nonetheless, there are
      predicates for which the crowd does not perform well.</p>
      <p><strong>Answer to RQ3.</strong>We compare the crowd
      behavior when using microtasks built with and without
      semantics. We observe a significant difference in crowd
      performance in microtasks with and without semantics. Our
      results confirm that semantically enriched microtasks
      increase the quality and efficiency of crowd answers.</p>
      <p>Future work could study other models to accurately capture
      crowd answer reliability. Another important research
      direction is to study the impact of – instead of triple-based
      – more complex microtasks against the crowd in terms of
      accuracy and cost.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">2017. CrowdFlower.
        (2017). <a class="link-inline force-break" href=
        "http://crowdflower.com" target=
        "_blank">http://crowdflower.com</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">Ziawasch Abedjan, Toni
        Grütze, Anja Jentzsch, and Felix Naumann. 2014. Profiling
        and mining RDF data with ProLOD++. In <em><em>IEEE 30th
        International Conference on Data Engineering, Chicago, ICDE
        2014, IL, USA, March 31 - April 4, 2014</em></em> .
        1198–1201.</li>
        <li id="BibPLXBIB0003" label="[3]">Maribel Acosta, Elena
        Simperl, Fabian Flöck, and Maria-Esther Vidal. 2015. HARE:
        A Hybrid SPARQL Engine to Enhance Query Answers via
        Crowdsourcing. In <em><em>Proceedings of the 8th
        International Conference on Knowledge Capture, K-CAP 2015,
        Palisades, NY, USA, October 7-10, 2015</em></em> .
        11:1–11:8.</li>
        <li id="BibPLXBIB0004" label="[4]">Maribel Acosta, Elena
        Simperl, Fabian Flöck, and Maria-Esther Vidal. 2017.
        Enhancing answer completeness of SPARQL queries via
        crowdsourcing. <em><em>Web Semantics: Science, Services and
        Agents on the World Wide Web</em></em> (2017). <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1016/j.websem.2017.07.001" target=
        "_blank">https://doi.org/10.1016/j.websem.2017.07.001</a>
        </li>
        <li id="BibPLXBIB0005" label="[5]">Maribel Acosta, Amrapali
        Zaveri, Elena Simperl, Dimitris Kontokostas, Sören Auer,
        and Jens Lehmann. 2013. Crowdsourcing Linked Data Quality
        Assessment. In <em><em>ISWC</em></em> . 260–276.</li>
        <li id="BibPLXBIB0006" label="[6]">Xu Chu, John Morcos,
        Ihab&nbsp;F. Ilyas, Mourad Ouzzani, Paolo Papotti, Nan
        Tang, and Yin Ye. 2015. KATARA: A Data Cleaning System
        Powered by Knowledge Bases and Crowdsourcing. In
        <em><em>SIGMOD</em></em> . 1247–1261. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2723372.2749431" target="_blank">
          https://doi.org/10.1145/2723372.2749431</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Xin Dong, Evgeniy
        Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin
        Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014.
        Knowledge vault: a web-scale approach to probabilistic
        knowledge fusion. In <em><em>The 20th ACM SIGKDD
        International Conference on Knowledge Discovery and Data
        Mining, KDD ’14, New York, NY, USA - August 24 - 27,
        2014</em></em> . 601–610.</li>
        <li id="BibPLXBIB0008" label="[8]">Ju Fan, Meihui Zhang,
        Stanley Kok, Meiyu Lu, and Beng&nbsp;Chin Ooi. 2015.
        CrowdOp: Query Optimization for Declarative Crowdsourcing
        Systems. <em><em>IEEE Trans. Knowl. Data Eng.</em></em> 27,
        8 (2015), 2078–2092.</li>
        <li id="BibPLXBIB0009" label="[9]">M. Franklin, D.
        Kossmann, T. Kraska, S. Ramesh, and R. Xin. 2011. CrowdDB:
        answering queries with crowdsourcing. In
        <em><em>SIGMOD</em></em> . 61–72.</li>
        <li id="BibPLXBIB0010" label="[10]">Jens Lehmann, Robert
        Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas,
        Pablo&nbsp;N Mendes, Sebastian Hellmann, Mohamed Morsey,
        Patrick van Kleef, Sören Auer, and others. 2014. DBpedia–A
        large-scale, multilingual knowledge base extracted from
        Wikipedia. <em><em>Semantic Web</em></em> (2014).</li>
        <li id="BibPLXBIB0011" label="[11]">Adam Marcus,
        David&nbsp;R. Karger, Samuel Madden, Rob Miller, and
        Sewoong Oh. 2012. Counting with the Crowd.
        <em><em>PVLDB</em></em> 6, 2 (2012), 109–120.</li>
        <li id="BibPLXBIB0012" label="[12]">Adam Marcus, Eugene Wu,
        Samuel Madden, and Robert&nbsp;C. Miller. 2011.
        Crowdsourced Databases: Query Processing with People. In
        <em><em>CIDR</em></em> . 211–214.</li>
        <li id="BibPLXBIB0013" label="[13]">Felix Naumann. 2002.
        <em><em>Quality-Driven Query Answering for Integrated
        Information Systems</em></em> . <em>Lecture Notes in
        Computer Science</em>, Vol.&nbsp;2261. Springer.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1007/3-540-45921-9" target=
          "_blank">https://doi.org/10.1007/3-540-45921-9</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">H. Park, R. Pang,
        A.&nbsp;G. Parameswaran, H. Garcia-Molina, N. Polyzotis,
        and J. Widom. 2012. Deco: A System for Declarative
        Crowdsourcing. <em><em>PVLDB</em></em> 5, 12 (2012),
        1990–1993.</li>
        <li id="BibPLXBIB0015" label="[15]">Hyunjung Park and
        Jennifer Widom. 2013. Query Optimization over Crowdsourced
        Data. <em><em>PVLDB</em></em> 6, 10 (2013), 781–792.</li>
        <li id="BibPLXBIB0016" label="[16]">Hyunjung Park and
        Jennifer Widom. 2014. CrowdFill: collecting structured data
        from the crowd. In <em><em>SIGMOD</em></em> . 577–588.</li>
        <li id="BibPLXBIB0017" label="[17]">Amrapali Zaveri, Anisa
        Rula, Andrea Maurino, Ricardo Pietrobon, Jens Lehmann, and
        Sören Auer. 2016. Quality assessment for Linked Data: A
        Survey. <em><em>Semantic Web</em></em> 7, 1 (2016),
        63–93.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://sites.google.com/site/hareengine">https://sites.google.com/site/hareengine</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186241">https://doi.org/10.1145/3184558.3186241</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
