<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Towards Annotating Relational Data on the Webwith Language Models</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186029'>https://doi.org/10.1145/3178876.3186029</a> 
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186029'>https://w3id.org/oa/10.1145/3178876.3186029</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Towards Annotating Relational Data on the Webwith Language Models</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Matteo</span>     <span class="surName">Cannaviccio</span>,     Roma Tre University, Rome, Italy, <a href="mailto:cannaviccio@uniroma3.it">cannaviccio@uniroma3.it</a>    </div>    <div class="author">     <span class="givenName">Denilson</span>     <span class="surName">Barbosa</span>,     University of Alberta, Edmonton, AB, Canada, <a href="mailto:denilson@ualberta.ca">denilson@ualberta.ca</a>    </div>    <div class="author">     <span class="givenName">Paolo</span>     <span class="surName">Merialdo</span>,     Roma Tre University, Rome, Italy, <a href="mailto:paolo.merialdo@uniroma3.it">paolo.merialdo@uniroma3.it</a>    </div>                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186029" target="_blank">https://doi.org/10.1145/3178876.3186029</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Tables and structured lists on Web pages are a potential source of valuable information, and several methods have been proposed to annotate them with semantics that can be leveraged for search, question answering and information extraction. This paper is concerned with the specific problem of finding and ranking relations from a given Knowledge Graph (KG) that hold over pairs of entities juxtaposed in a table or structured list. The state-of-the-art for this task is to attempt to link the entities mentioned in the table cells to objects in the KG and rank the relations that hold for those linked objects. As a result, these methods are hampered by the incompleteness and uneven coverage in even the best knowledge graphs available today. The alternative described here does not require entity linking, relying instead on ranking relations using generative language models derived from Web-scale corpora. As such, it can produce quality results even when the entities in the table are missing in the KG. The experimental validation, designed to expose the challenges posed by KG incompleteness, shows that our approach is robust and effective in practice.</small>    </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Matteo Cannaviccio, Denilson Barbosa, and Paolo Merialdo. 2018. Towards Annotating Relational Data on the Webwith Language Models. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186029" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186029</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>The Web is a vast source of intrinsically relational knowledge expressed in hundreds of millions of tables and many more structured lists within billions of documents. Web-scale table corpora have found many applications, including search and question answering&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>], knowledge graph construction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>], schema understanding and auto-complete&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>], to name a few. However, unlike with documents in which information is encoded in text amenable to natural language understanding tools, the facts and relationships encoded in tables are <em>implicit</em>, and therefore hard to extract automatically. The various approaches for <em>understanding</em> Web tables amount to two main tasks: (1) identifying the <em>type</em> of each column in a table, and (2) identifying the <em>relationship</em> between pairs of columns in the table. As a motivating example, Fig.&#x00A0;<a class="fig" href="#fig1">1</a> shows a snippet of a prototypical table (from Wikipedia in this case) that can be easily extracted and parsed with tools like Google tables or an automatic wrapper induced from a set DOM-trees. It is clear to a human looking at the table that the second column has <em>actors</em> who played in the <em>movies</em> in the first column. With little effort (e.g., after reading the article with that table) a human can infer that the third column has <em>countries</em> that were <em>filming locations</em> of the <em>movies</em> in the first column. Yet, extracting those types and relationships is out of reach of text-based Information Extraction tools that rely on linguistic patterns used to encode knowledge&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Web table with actors and filming locations of James Bond movies.</span>     </div>    </figure>    </p>    <p>The first methods for Web table understanding&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] were strictly lexical, using frequently occurring keywords and phrases as annotations, and are primarily useful for keyword-based table search. The prevalent approach, however, is to leverage existing Web-scale Knowledge Graphs (KGs) for <em>semantic</em> Web table understanding, whereby one annotates columns with <em>classes</em> of entities and pairs of columns with <em>relationships</em> from the KG ontology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]. To do so, these methods attempt to disambiguate the entities in the table by <em>linking</em> them to objects in the KG. If this can be done, one can immediately annotate each table column with the ontology type covering the entities in the column. Next, the relationship between a pair of columns can be inferred <em>ranking</em> KG relations based on their <em>coverage</em> of (entity pairs in the rows) of the table.</p>    <p>Although highly intuitive, the approach outlined above is hampered by the fact that even the best existing KGs are notoriously incomplete&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], missing many entities (not only obscure tail entities) as well as many relations among the entities. More precisely, KG incompleteness introduces two problems. First, if the Web table contains mostly entities that are not in the KG or cannot be easily linked, no table annotations are possible. It is worth mentioning that state-of-the-art entity linking methods rely on textual features (e.g., keyphrases) which are hardly available in the context of Web table understanding. Second, the KG coverage for specific relations is often biased, which can lead to unexpected results even if the entities can be correctly linked as illustrated next.</p>    <p>The example of Fig.&#x00A0;<a class="fig" href="#fig1">1</a> was chosen systematically to illustrate the problems caused by KG incompleteness. We picked a table with a column whose cells would: (1) be difficult to link to Freebase objects, and (2) participate in a partially populated relation of interest. In our example, the names of the movies are hard to distinguish from their respective <em>soundtrack</em> albums, even with sophisticated string matching methods (e.g.,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>]). Moreover, although we can easily disambiguate the countries in the table, and although Freebase has a dedicated relation for filming locations of movies, the coverage of that relation is heavily biased towards recent movies, lacking the filming location of most Bond movies. Freebase is not as incomplete, however, in the music domain. In fact, it contains all countries were the soundtracks of the Bond movies were released. As a result, one would be biased towards annotating the relationship between the first and third columns of Fig.&#x00A0;<a class="fig" href="#fig1">1</a> with the predicate for the region where an album is released, which would be incorrect in this case. (In passing, at the time of writing, both YAGO and DBpedia also lack the filming locations of most Bond movies.) <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Overview of our approach.</span>     </div>    </figure>    </p>    <section id="sec-5">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.1</span> Problem Statement</h3>     </div>    </header>    <p>For convenience, we adopt the Freebase notation and terminology.</p>    <p>As customary, we model a <strong>knowledge graph</strong> as a labeled, directed multi-graph <em>KG</em> = (<em>N</em>, <em>E</em>, <em>L</em>) where <em>N</em>, <em>E</em>, and <em>L</em> are sets of nodes, edges, and labels. Nodes can be entities, represented using unique identifiers called &#x201C;m-ids&#x201D; (e.g., <tt>m/03_gd</tt>), text literals in quotes (e.g., <tt>&#x201D;James Cameron&#x201D;</tt>), or types denoted as paths in an ontology (e.g., <tt>film/director</tt>). Labels in <em>L</em> define relation names (e.g., <tt>film/director/film</tt>). Edges in <em>E</em>&#x2286;<em>N</em> &#x00D7; <em>L</em> &#x00D7; <em>N</em> are called statements or triples and can be used to assign types to entities (e.g., &#x27E8;<tt>m/03_gd</tt>, <tt>type/object/type</tt>, <tt>film/director</tt>&#x27E9;); to describe entities (e.g., &#x27E8;<tt>m/03_gd</tt>, <tt>type/object/name</tt>, <tt>&#x201D;James Cameron&#x201D;</tt>&#x27E9;) or to relate pairs of entities (e.g., &#x27E8;<tt>m/03_gd</tt>, <tt>film/director/film</tt>, <tt>m/0dr_4</tt>&#x27E9;). We assume the KG ontology specifies types for the domain and range of every relation.</p>    <p>This paper seeks to describe and evaluate a principled and effective way of predicting KG relations that hold over columns of Web tables. (Note that doing so allows one to annotate the columns themselves with the domain and range types from the KG ontology for those relations.) Predicting which relation(s) hold for pairs of entities amounts to <em>ranking</em> all KG relations for those entities followed by either thresholding or taking the top-k relations in the ranking. Thus, in this paper we focus on, and evaluate relation rankings instead of predictions. Without loss of generality, we assume the input to be a set of entity pairs, as one can always convert a multi-column table or a nested list into one or more sets of pairs. Also, we assume that all pairs in the set are similarly related; or, in other words, the input is not random. More precisely:</p>    <div class="definition" id="enc1">     <Label>Definition 1.</Label>     <p> Given a set <em>I</em> = {&#x27E8;<em>s</em>      <sub>1</sub>, <em>o</em>      <sub>1</sub>&#x27E9;, &#x2026;, &#x27E8;<em>s<sub>n</sub>      </em>, <em>o<sub>n</sub>      </em>&#x27E9;} of subject-object entity pairs, and a list <em>L</em> of relation names from a KG, produce a ranked list <em>r</em>      <sub>1</sub>, &#x2026;, <em>r<sub>k</sub>      </em> of <em>k</em> relations from <em>L</em> that hold over pairs in <em>I</em>, sorted by decreasing relevance.</p>    </div>    <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig3.svg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Generative language models for two relations between works of art and countries.</span>     </div>    </figure>    <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig4.svg" class="img-responsive" alt="Figure 4"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Web search results for &#x27E8;<tt>&#x201D;Dr.&#x00A0;No&#x201D;</tt>,<tt>&#x201D;Jamaica&#x201D;</tt>&#x27E9;.</span>     </div>    </figure>    </section>    <section id="sec-6">    <header>     <div class="title-info">      <h3>       <span class="section-number">1.2</span>       <strong>Overview of Our Approach</strong>      </h3>     </div>    </header>    <p>Fig.&#x00A0;<a class="fig" href="#fig2">2</a> illustrates our method, which is heavily inspired by the established Language Models (LMs) for IR approach&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>], in which the goal is to model each document separately and score documents w.r.t. queries based on how likely the corresponding models are to generate the query. Keeping with this analogy, the KG relations in our setting play the role of the &#x201C;documents&#x201D; and the pairs of entities play the role of the &#x201C;queries&#x201D;.</p>    <p>     <strong>Models of KG Relations</strong>. We model a KG relation distributionally using the <em>phrases</em> that are used to express it. We learn such models from the approximately 500M texts in English of the ClueWeb09 corpus, leveraging Google&#x0027;s FACC1 annotation corpus assigning m-ids of Freebase entities to the <em>mentions</em> in those texts where these entities appear. Following the state-of-the-art in open relation extraction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>], we gather phrases appearing <em>between</em> m-ids in the corpus and filter out phrases that describe ontological relations (e.g., &#x201C;is a&#x201D; and variants) and phrases that do not conform to known patterns defined at the level of parts-of-speech&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>] tags. The details of the construction of LMs are given in Sec.&#x00A0;<a class="sec" href="#sec-8">3</a>.</p>    <p>     <strong>Relation Ranking</strong>. To rank KG relations for entity pair &#x27E8;<em>s<sub>i</sub>     </em>, <em>o<sub>i</sub>     </em>&#x27E9;, we perform a Web search with those entities and extract <em>relational phrases</em> connecting the entities in the result of the Web search. Then, we score the KG relations based on how likely their corresponding models are to generate the set of phrases extracted from the Web search. Continuing with our running example, Fig.&#x00A0;3a shows some of the phrases of the LM associated with the relation associating albums to the countries they have been released, while Fig.&#x00A0;3b shows phrases for the relation for filming locations of movies. Fig.&#x00A0;<a class="fig" href="#fig4">4</a> shows some sentences from the Web search with entities &#x27E8;<tt>&#x201D;Dr.&#x00A0;No&#x201D;</tt>,<tt>&#x201D;Jamaica&#x201D;</tt>&#x27E9;. In this case, the relation about movie locations will rank higher than the relation about album releases.</p>    <p>Two models for KG relation scoring are considered here (see Sec.&#x00A0;<a class="sec" href="#sec-15">4.2</a>): (1) <SmallCap>PLM</SmallCap>, the &#x201C;standard&#x201D; <em>conjunctive</em> approach of maximizing the likelihood of all terms in the query, and (2) <SmallCap>Noisy OR</SmallCap>, a <em>disjunctive</em> approach where the ranking may be biased towards a small number of highly relevant phrases. Finally, note that when the input consists of multiple entity pairs, we need a way of finding aggregate scores for the relations relative to all such pairs. Again, two ways of doing so are discussed and evaluated here (Sec.&#x00A0;<a class="sec" href="#sec-16">4.3</a>): (<em>a</em>) a <em>global</em> strategy that combines all sentences of all pairs into a single <em>global</em> query used to rank the LMs (once), and (<em>b</em>) a <em>local</em> strategy where we produce a ranking for each pair, merging them to arrive at a final prediction.</p>    <p>     <strong>Evaluation</strong>. To the best of our knowledge, there are no benchmarks concerned with KG incompleteness and how they affect table understanding. Therefore, we designed two experiments to illustrate how our method can overcome this problem. In the first we use a synthetic benchmark with facts that are both true and known to be missing from Freebase, DBpedia, and YAGO (Sec.&#x00A0;<a class="sec" href="#sec-17">5</a>), following our previous work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]. In the second we experiment with tables from Wikipedia for which a state-of-the-art web table annotation tool&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] fails to produce any output (Sec.&#x00A0;<a class="sec" href="#sec-24">5.7</a>).</p>    <p>     <strong>Contribution</strong>. We describe an effective method for ranking KG relations that applies to pairs of named entities that is less susceptible to KG incompleteness than the current state-of-the-art. Departing from previous work, our method does <em>not</em> require that the entities are linked to, or even exist in the KG. Instead, our method works whenever a Web search returns phrases describing how the entities are related. Our method is general: it is not specific to any KG, corpus or natural language, and the Web search can be replaced by a search on any large corpus. Finally, perhaps the biggest advantage of our method is that it predicts relations based on <em>corpus</em> statistics which are <em>independent</em> of, and not biased by the KG coverage. Our experimental evaluation, which has been conducted on publicly available datasets, indicates that our method can correctly predict relationships for in-KG and for out-of-KG entities, where state-of-the-art approaches fail, and thus can significantly contribute to improve previous work in this area.</p>    </section>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>Language Models have found many uses in Information Retrieval beyond document ranking&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>]. A state-of-the-art entity search method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] is based on &#x201C;entity LMs&#x201D; that harness entity categories (i.e., semantic types) for ranking and filtering answers based on a desired <em>type</em> (e.g., movies, albums, etc.). Other applications include searching and ranking over RDF-structured Linked Data and knowledge graphs with queries that combine keywords and entity examples&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] or interpret so-called telegraphic text queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] on the underlying structured data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. LMs have also been found useful in ranking the results of exact, relaxed and keyword-augmented graph-pattern queries over RDF graphs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>], which has applications in translating natural language questions into SPARQL queries over KGs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], among others. We use LMs for relation prediction.</p>    <p>Our work borrows from relation prediction methods that exploit the duality between KG relations and phrases occurring in text (e.g.,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]), except that we employ strict filters to remove non-relational and ontological patterns&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. While we use LMs for prediction and achieve good results, other ranking models from Information Retrieval and/or other relation prediction models from the field of Information Extraction could be used for the same purpose. We leave as future work investigating other scoring models and how they fare in our setting.</p>    <p>We are motivated by the problem of understanding Web tables, widely recognized as a valuable knowledge source on the Web&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. The first solution&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] is meant for search, annotating columns with keywords from an &#x201C;is-a&#x201D; database and relationships between columns with <em>keyphrases</em> frequently occurring with the entities in the table. On the other hand, by annotating pairs of columns with KG relations that hold over the respective entities, our method annotates the tables with semantic information. With our method, the columns can be annotated with the <em>expected</em> (semantic) types for the relations as per the KG schema.</p>    <p>Recent work on Web table understanding links entities in <em>table cells</em> to KG objects and pairs of columns with KG relations that hold over them&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]. An earlier work in this area&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] learns a probabilistic graphical model that collectively annotates cells with entity identifiers, columns with KG types and pairs of columns with KG relations, maximizing the joint probability of the assignment. Another idea is to model each row of the table as a set of (possibly multi-valued) attributes describing a single entity in the KG&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]; each row of the table is then matched with entities in DBpedia, taking into account the table headers and how well they match classes in the DBpedia ontology. Unlike these works, our method does not require linking entities to KG objects and, thus, should be less susceptible to KG incompleteness. We validate this hypothesis experimentally in two ways. First, we use a synthetic benchmark with facts that are both true and known to be missing from Freebase, DBpedia, and YAGO (Sec.&#x00A0;<a class="sec" href="#sec-17">5</a>), obtained from&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. Second, we experiment with tables from Wikipedia for which a state-of-the-art web table annotation tool&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] fails to produce any output (Sec.&#x00A0;<a class="sec" href="#sec-24">5.7</a>). Our evaluation confirms our hypothesis and suggests our method can be <em>used in conjunction</em> with previous work to lead to better Web table understanding tools.</p>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Building Language Models</h2>    </div>    </header>    <p>Achieving accurate results in our setting requires LMs derived from a large corpus of phrases that are relational, grammatical, and frequent (so that they are likely to match evidence gathered at prediction time). Thus, we use the English subset of ClueWeb09 and the 5 billion annotations provided by Google&#x0027;s FACC1 corpus,<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> indicating which text spans contain mentions of entities known to Freebase, identified through their m-ids.</p>    <p>Replacing actual mentions to named entities in the text by their corresponding m-ids, we arrive at content such as the following:</p>    <p>    <tt>/m/06mr6</tt> famously starred as <tt>/m/06k5xq</tt> besides <tt>/m/0clpml</tt>    </p>    <p>Since in Freebase the entities <tt>/m/06mr6</tt> (actor Sir Sean Connery) and <tt>/m/06k5xq</tt> (fictional character Robin Hood) are related through relation <tt>film/actor/.../character</tt>, we add the phrase &#x201C;famously starred as&#x201D; to the language model of that relation (and also to the models of all other relations between these entities).</p>    <p>In a nutshell, building LMs boils down to: for every pair of m-ids that belong to a relation, extracting all phrases connecting those m-ids from the corpus, filtering uninformative phrases and aggregating the counts accordingly. Next, we explain the filtering steps we perform to increase the quality of our language models.</p>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Filtering Phrases</h3>     </div>    </header>    <p>Our goal is to predict relations between <em>pairs of entities</em> such as family and romantic relationships between people, employment relationships between people and organizations, and business relationships among organizations. In order to keep our LMs highly focused we discard generic and uninformative phrases with the help of lightweight natural language processing tools.</p>    <p>     <strong>Filtering Uninformative Phrases</strong>. Not all phrases connecting entities are useful for relation prediction. For example, in the sentence above, the phrase &#x201C;famously starred as&#x201D; describes an actual relation between the surrounding entities while the phrase &#x201C;besides&#x201D; between <tt>/m/06k5xq</tt> (Robin Hood) and <tt>/m/0clpml</tt> (James Bond) does not. To filter out such noise, we parse the sentences containing pairs of m-ids and check if the phrases connecting the entities conform to known grammatical patterns that describe binary relations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>], discarding those that do not. This step eliminates the vast majority of the phrases but ensures our language models are grammatical and predictive.</p>    <p>     <strong>Placeholder Generalization</strong>. We often can (and should) generalize the phrases that reveal the same relation but differ in some detail. For example, phrases &#x201C;starred in the 3rd movie of&#x201D; and &#x201C;starred in the first movie of&#x201D; express the same relation, and are generalized into &#x201C;starred in the ORD movie of&#x201D;, where &#x201C;ORD&#x201D; stands for any ordinal number. We apply similar generalizations to instances of other common generic types such dates, distances and numbers. Fig.&#x00A0;<a class="fig" href="#fig5">5</a> summarizes the placeholders used with the relative frequency (i.e. number of phrases).</p>    <p>     <strong>Further Filtering</strong>. We are not interested in ontological relations that describe class membership of entities, such as that Sir Sean Connery is an actor and that James Bond is a fictional character. Other prominent ontological relations concern the ethnicity of people, the business segment of organizations, etc. Thus, we discard phrases that are variants of the &#x201C;is a&#x201D; pattern, often used in these cases (e.g., &#x201C;is a British actor&#x201D; or &#x201C;was an American activist&#x201D;).</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Phrase Statistics</h3>     </div>    </header>    <p>We were able to find 19M distinct pairs of m-ids that are connected by a (filtered) relational phrase in the ClueWeb09 corpus. Only 1.4M of these pairs (8%) belong to one of the approximately 5K Freebase relations. In total, these 1.4M pairs are related through 2.36M distinct phrases in the corpus. Although we found some fairly long phrases, the majority of them are relatively short (4.3 tokens per phrase on average). As expected, we observed that the distribution of phrases by frequency in the corpus follows a power-law.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Specializing LMs Based on Type</h3>     </div>    </header>    <p>The Freebase ontology specifies the <em>expected</em> types of the entities that can participate in any given relation. For example, relation <tt>/film/film/subject</tt>, that describes the subject of a movie, has domain <tt>/film/film</tt> and range <tt>/film/film_subject</tt>. Although somewhat informative, these types are fairly generic. For example, the subjects of biographical movies are people (and thus instances of <tt>/people/person</tt>), while the subjects of documentaries can be organizations or locations. Note however, that the LMs for these different kinds of movies are likely to be very different: the relational phrase &#x201C;is the biography of&#x201D; is appropriate for movies whose subject are people, while &#x201C;portrays the founding of&#x201D; is suitable for movies about organizations. In order to account for such nuances, we partition the phrases associated with each relation based on generic entity types that can be inferred automatically by typical NER systems<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, and are available in Freebase as <tt>/people/person</tt>, <tt>/organization/organization</tt>, and <tt>/location/location</tt>. A catch-all &#x201C;misc&#x201D; type is used for all the other entities. This results in each FB relation having up to 16 different LMs, one for each possible combination of types. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig5.svg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Statistics after the filtering process.</span>      </div>     </figure>    </p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Model Statistics</h3>     </div>    </header>    <p>In the end, of the 4819 relations in Freebase, we are able to build models for 2739. For the purposes of the evaluations reported here, we experimented only with those relations for which we could find at least 200 distinct phrases. This corresponds to 500 relations and 1934 different models (on average 3.78 LMs per relation, based on the combination of NER types). Fig.&#x00A0;<a class="fig" href="#fig5">5</a> summarizes statistics about the number of language models we derived from ClueWeb09, and those we use in our experimentation.</p>    </section>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Relation Ranking</h2>    </div>    </header>    <p>This section gives details the steps involved in relation ranking.</p>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Gathering Evidence from the Web</h3>     </div>    </header>    <p>Given an entity pair &#x27E8;<em>s</em>     <sub>1</sub>, <em>o</em>     <sub>1</sub>&#x27E9;, we perform a Web search looking for sentences mentioning both entities in the given order. For the purposes of this paper, we collected sentences from <em>snippets</em> returned by Google, saving time and bandwidth. The actual scoring of relations is done on relational phrases, extracted from the snippets, that match those used to build the language models. That is, we process the snippets in the same way described in Sec.&#x00A0;<a class="sec" href="#sec-9">3.1</a>. If multiple relational phrases are found in the same snippet our system uses all of them. Also, we attempt to minimize the effects of geo-localization and personalization in Google searches by periodically obtaining a new IP address via a Private Virtual Network service. Of course, our system is not restricted to Google. In fact, a local index of a large Web crawl (e.g., ClueWeb or the Web commons crawl) could be used instead of Google for this step.</p>    <p>We compare two strategies for matching phrases: exact matching (equality) and shallow approximate matching, which works as follows. Given a span of text we find candidate phrases using n-grams at character level (3-grams) and then we score them using a Fuzzy Jaccard Similarity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] that takes into account fuzzy matchings between the individual words<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. The approximate match comes with a clear precision and recall trade-off: it introduces noise but results in more phrases being matched. For example, the text &#x201C;filmed entirely in&#x201D; in a Web search snippet could match phrases &#x201C;was filmed in&#x201D; and &#x201C;were filmed by&#x201D;, belonging to different LMs. We investigate this trade-off and confirm in the experimental evaluation that approximate matching generally improves the quality of the rankings.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Ranking for Individual Entity Pairs</h3>     </div>    </header>    <p>As mentioned in Sec.&#x00A0;<a class="sec" href="#sec-6">1.2</a>, we take an IR approach to rank KG relations based on their relevance for an entity pair. To recap the notation and avoid confusion, each &#x201C;document&#x201D; <em>D</em> corresponds to a KG relation and a &#x201C;query&#x201D; <em>Q</em> corresponds to relational phrases connecting entities obtained through a Web search. Given an entity pair &#x27E8;<em>s<sub>i</sub>     </em>, <em>o<sub>i</sub>     </em>&#x27E9;, we denote by <em>a</em>(&#x27E8;<em>s<sub>i</sub>     </em>, <em>o<sub>i</sub>     </em>&#x27E9;) the ranking of all documents for the query resulting from that pair, computed according to a <em>score</em>(&#x00B7;, &#x00B7;) function (explained below).</p>    <p>     <strong>Query Likelihood Scoring</strong>. The <em>query likelihood</em> retrieval model assumes that the query terms are samples drawn from a LM derived from a document. Formally, given query <em>Q</em> and document <em>D</em>, from which a model <em>&#x03B8;<sub>D</sub>     </em> is derived, we rank the documents by decreasing score, defined as: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \mathit {score}(Q,D) = P(Q|\theta _D). \] </span>       <br/>      </div>     </div> Many approaches have been used for estimating <em>P</em>(<em>Q</em>|<em>&#x03B8;<sub>D</sub>     </em>). We start with the robust multinomial language model, which assumes that terms are generated independently, and avoid overfitting with interpolation (i.e., Jelinek-Mercer smoothing)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]. More precisely, let <em>C</em> be a document containing all the phrases and <em>S</em> the set of phrases. Then:</p>    <p>     <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathit {score}(Q,D) = \displaystyle \prod _{p\in S} P(p|\theta _D)^{c(p,Q)} \end{eqnarray} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} P(p|\theta _D) = \lambda \, P(p|D) + (1-\lambda)\,P(p|C)\end{eqnarray} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>     <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} P(p|D) = \frac{c(p,D)}{|D|}\quad \text{and}\quad P(p|C) = \frac{c(p,C)}{|C|} \end{eqnarray} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>    </p>    <p>Above, <em>c</em>(<em>p</em>, &#x00B7;) denotes the frequency of phrase <em>p</em> in the query <em>Q</em>, document <em>D</em> or corpus <em>C</em>. We call this ranking approach <SmallCap>PLM</SmallCap>, for <em>Phrase Language Model</em> in the experimental evaluation.</p>    <p>We set <em>&#x03BB;</em> = 0.9 experimentally.</p>    <p>     <strong>Disjunctive Gate Scoring</strong>. The query likelihood approach uses a <em>conjunctive gate</em> to combine evidence from multiple phrases while predicting the likelihood of a relation: a model that cannot generate most phrases in the query is unlikely to rank high. In our setting, this is often an overkill. For example, one can be reasonably certain that the relation <tt>/film/film/featured_film_locations</tt> holds for a pair of entities connected by the phrase &#x201C;was filmed in&#x201D;. An implicit assumption here is that the frequency of the phrases used to build the LMs and the queries is a good proxy for how reliable they are. While this seems reasonable at Web scale, one could take the trustworthiness of the sources&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] into account, e.g., via re-ranking or by a priori filtering.</p>    <p>To allow for more permissive predictions we calculate the score of each relation interpolating its prior and its posterior probability conditioned by every single phrase in the query. We aggregate the posterior of each phrase using a &#x201C;noisy-OR&#x201D; gate&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathit {score}(Q,D) = \beta \, P(D|p_{1},...,p_{Q}) + (1-\beta)\, P(D) \end{eqnarray} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} P(D|p_{1},...,p_{Q}) = 1 - \prod _{p\in Q} \left(1 - P(D|p)\right) \end{eqnarray} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>     <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} P(D|p) = \frac{c(p,D)}{\sum _{l\in L}c(p,D_l)} \end{eqnarray} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div>    </p>    <p>A &#x201C;noisy-OR&#x201D; gate combines evidence differently from the standard <SmallCap>PLM</SmallCap>: a relation scores high if the query contains <em>any</em> of the high frequency phrases associated with that relation. We call this ranking approach <SmallCap>Noisy OR</SmallCap> in the experimental evaluation.</p>    <p>As <em>&#x03BB;</em> for the standard model, <em>&#x03B2;</em> is a coefficient to control the amount of smoothing. We set it experimentally to <em>&#x03B2;</em> = 0.8.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Ranking for Multiple Pairs</h3>     </div>    </header>    <p>We now move on to the general form of the problem which applies to a set of entity pairs &#x27E8;<em>s</em>     <sub>1</sub>, <em>o</em>     <sub>1</sub>&#x27E9;, &#x2026;, &#x27E8;<em>s<sub>n</sub>     </em>, <em>o<sub>n</sub>     </em>&#x27E9;, e.g., coming from different rows of the same table, and in which we need to rank KG relations in some aggregate form. We consider two ways of producing an aggregate ranking. The first is a <em>global</em> approach where we merge the results of the Web searches for each individual pair into a single query, used to rank all KG relations, while the second is a <em>local</em> aggregation approach, where we score KG relations by merging the individual rankings obtained for each pair separately.</p>    <p>     <strong>Global Aggregation</strong>. In this aggregation approach we build a query <em>Q</em>&#x2032; containing all phrases in each <em>Q<sub>i</sub>     </em> derived from entity pair &#x27E8;<em>s<sub>i</sub>     </em>, <em>o<sub>i</sub>     </em>&#x27E9;, with the appropriate phrase counts, and obtain a single ranking using Eq.&#x00A0;<a class="eqn" href="#eq1">1</a> or Eq.&#x00A0;<a class="eqn" href="#eq4">4</a> to produce the final answer, depending on the scoring model used.</p>    <p>     <strong>Local Aggregation</strong>. In this approach we first rank all relations for each of the entity pairs, and combine these rankings to score the relations. Let <em>a<sub>i</sub>     </em> = <em>a</em>(&#x27E8;<em>s<sub>i</sub>     </em>, <em>o<sub>i</sub>     </em>&#x27E9;) be the ranking obtained for entity pair &#x27E8;<em>s<sub>i</sub>     </em>, <em>o<sub>i</sub>     </em>&#x27E9;, computed according to Eq.&#x00A0;<a class="eqn" href="#eq1">1</a> or Eq.&#x00A0;<a class="eqn" href="#eq4">4</a> depending on the scoring model: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} a_1 = a(\langle s_1,o_1\rangle) = r^{a_1}_1, r^{a_1}_2, \ldots , r^{a_1}_k\\ {} \vdots\\a_n = a(\langle s_n,o_n\rangle) = r^{a_n}_1, r^{a_n}_2, \ldots , r^{a_n}_j\end{align*} </span>       <br/>      </div>     </div>    </p>    <p>The <em>local</em> aggregated score of a relation is its mean (inverse) rank across all individual rankings: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathit {agg_{score}}(r) = \frac{1}{n} \displaystyle \sum _{a_i} \frac{1}{\mathit {rank}(r, a_i)} \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>    </p>    <p>In this way, a relation that ranks high for a large number of pairs will have a higher score and rank high for the set of entity pairs.</p>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>    </div>    </header>    <p>We now report on an experimental evaluation of the LM-based relation ranking approach to show it does not suffer from the KG incompleteness problem and, thus, can be a viable alternative to Entity Linking (EL) approaches. To do that, we experiment first on two corpora of facts involving pairs of in-KG entities, comprising 9 relations from the person domain (Tab.&#x00A0;<a class="tbl" href="#tab1">1</a> shows the relations). The first corpus, called <SmallCap>LectorFacts</SmallCap>, consists of facts <em>known to be missing</em> from DBpedia and Freebase. The second corpus, called <SmallCap>KGFacts</SmallCap>, comprises the same relations as <SmallCap>LectorFacts</SmallCap>, but with facts that are present in both DBpedia and Freebase. With these two corpora, we can simulate the scenarios where EL methods should work (<SmallCap>KGFacts</SmallCap>) and the scenario where they would not (<SmallCap>LectorFacts</SmallCap>). We thoroughly evaluate our system both with individual pairs of entities (Sec.&#x00A0;<a class="sec" href="#sec-19">5.2</a>) and also with sets of pairs (Sec.&#x00A0;<a class="sec" href="#sec-20">5.3</a>) as input, under a variety of scenarios. Then, to further illustrate the KG incompleteness issue, we evaluate our approach on pairs of columns from Wikipedia tables mixing in-KG and out-of-KG entities but for which a state-of-the-art EL method, T2K Match&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], fails to identify the correct relations (Sec.&#x00A0;<a class="sec" href="#sec-24">5.7</a>).</p>    <p>For the experiments reported here, we trained our method was to predict 500 different Freebase relations (recall Sec.&#x00A0;<a class="sec" href="#sec-12">3.4</a>) and used Google for the Web search step.</p>    <p>    <strong>Statistics about</strong>     <SmallCap>     <strong>LectorFacts</strong>    </SmallCap>    <strong>and</strong>     <SmallCap>     <strong>KGFacts</strong>    </SmallCap>. To the best of our knowledge, no previous benchmark for Web table understanding considers the case where KG incompleteness prevents an EL approach to predict a good relation even when all entities are in the KG. Therefore, we rely on the only corpus of facts <em>known to be missing</em> from mainstream and publicly available KGs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>, and call it <SmallCap>LectorFacts</SmallCap> here. We restrict our evaluation to the 9 non-ontological relations in the original corpus<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>, and use 50 facts (i.e., entity-pairs) from each relation. For the sake of comparison, we created a similar benchmark, <SmallCap>KGFacts</SmallCap>, by randomly picking 50 entity pairs from each of the relations in <SmallCap>LectorFacts</SmallCap>, among those pairs that appear in both DBpedia and Freebase. In a sense, these two benchmarks complement each other: facts from <SmallCap>KGFacts</SmallCap> concern prominent pairs of entities and generate more hits on a Web search. As shown in Tab.&#x00A0;<a class="tbl" href="#tab1">1</a>, on average, we are able to obtain twice as many hits (the &#x201C;sent.&#x201D; column in the table) on that corpus compared to <SmallCap>LectorFacts</SmallCap>. The two columns under &#x201C;phrase&#x201D; in the table show the (average) number of relational phrases (i.e., the ones used to build the LMs) that <em>match</em> the sentences returned by the Web search. These phrases are used (as queries) to predict the relations in our approach. We experiment with exact and approximate matching of sentences and relational phrases. For clarity, all results reported use exact matching, except those in Sec.&#x00A0;<a class="sec" href="#sec-21">5.4</a>.</p>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Metrics</h3>     </div>    </header>    <p>Given a set <em>I</em> = {&#x27E8;<em>s</em>     <sub>1</sub>, <em>o</em>     <sub>1</sub>&#x27E9;, &#x2026;, &#x27E8;<em>s<sub>n</sub>     </em>, <em>o<sub>n</sub>     </em>&#x27E9;} of subject-object entity pairs we have only one correct answer (the pairs are labeled with a single relation). In order to evaluate the ranking produced by the system we use the reciprocal rank&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] that corresponds to the multiplicative inverse of the rank of the correct relation. More precisely, let <em>a</em>(<em>I</em>) = <em>r</em>     <sub>1</sub>, &#x2026;, <em>r<sub>k</sub>     </em> be the response of a prediction for the input pairs <em>I</em> in which relations are given in decreasing order of relevance, and let <em>truth</em>(<em>I</em>) be the ground truth relation for <em>I</em>, the <em>reciprocal<sub>rank</sub>     </em>(<em>a</em>, <em>I</em>) is:</p>    <p>     <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{align}\mathit{reciprocal_{rank}}({a}, {I}) = \sum_{i=1}^k \frac{\mathbb{1}(\mathit{truth}({I}) = a[i])}{i}\end{align} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>    </p>    <p>where 1(&#x00B7;) is the indicator function (returns 1 if the condition appearing as its argument holds, 0 otherwise) and <em>a</em>[<em>i</em>] is the relation in position <em>i</em> in the ranking. Note that the metric implicitly takes recall into account. Indeed if the system does not predict any ranking or the correct relation is not present the reciprocal rank is 0. Finally, we use the Mean Reciprocal Ranking (MRR)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] to evaluate the results of multiple input sets <span class="inline-equation"><span class="tex">$\mathcal {I}=I_{1}, ..., I_{n}$</span>     </span>: <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathit {MRR}(\mathcal {I}) = \frac{1}{\vert \mathcal {I} \vert }\sum _{I_{j} \in \mathcal {I}} \mathit {reciprocal_{rank}}(a,I_{j}) \end{align} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>    </p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Mean number of sentences retrieved from the Web search and corresponding number of matching phrases obtained with the exact and the approximate method. Each relation consists of 50 entity pairs.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Relation</th>       <th style="text-align:center;" colspan="3">        <SmallCap>         <strong>LectorFacts</strong>        </SmallCap>        <hr/>       </th>       <th style="text-align:center;" colspan="3">        <SmallCap>         <strong>KGFacts</strong>        </SmallCap>        <hr/>       </th>       </tr>       <tr>       <th style="text-align:left;"/>       <th style="text-align:right;">sent.</th>       <th style="text-align:center;" colspan="2">phrase<hr/>       </th>       <th style="text-align:right;">sent.</th>       <th style="text-align:center;" colspan="2">phrase<hr/>       </th>       </tr>       <tr>       <th style="text-align:left;"/>       <th style="text-align:right;"/>       <th style="text-align:right;">ex.</th>       <th style="text-align:right;">ap.</th>       <th style="text-align:right;"/>       <th style="text-align:right;">ex.</th>       <th>ap.</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <tt>people/person/parents</tt>       </td>       <td style="text-align:right;">39.5</td>       <td style="text-align:right;">2.8</td>       <td style="text-align:right;">4.9</td>       <td style="text-align:right;">93.5</td>       <td style="text-align:right;">8.6</td>       <td>16.6</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>people/person/education</tt>       </td>       <td style="text-align:right;">22.3</td>       <td style="text-align:right;">1.3</td>       <td style="text-align:right;">2.2</td>       <td style="text-align:right;">113.3</td>       <td style="text-align:right;">8.6</td>       <td>16.8</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>sports/pro_athlete/teams</tt>       </td>       <td style="text-align:right;">61.4</td>       <td style="text-align:right;">3.5</td>       <td style="text-align:right;">6.3</td>       <td style="text-align:right;">131.4</td>       <td style="text-align:right;">17.3</td>       <td>29.9</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>people/person/place_of_death</tt>       </td>       <td style="text-align:right;">55.1</td>       <td style="text-align:right;">1.5</td>       <td style="text-align:right;">3.3</td>       <td style="text-align:right;">126.3</td>       <td style="text-align:right;">10.9</td>       <td>21.5</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>government/politician/party</tt>       </td>       <td style="text-align:right;">37.7</td>       <td style="text-align:right;">1.6</td>       <td style="text-align:right;">2.5</td>       <td style="text-align:right;">102.8</td>       <td style="text-align:right;">11.0</td>       <td>21.9</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>people/person/place_of_birth</tt>       </td>       <td style="text-align:right;">54.2</td>       <td style="text-align:right;">2.5</td>       <td style="text-align:right;">4.4</td>       <td style="text-align:right;">119.9</td>       <td style="text-align:right;">9.7</td>       <td>18.5</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>award/award_winner/awards_won</tt>       </td>       <td style="text-align:right;">58.1</td>       <td style="text-align:right;">2.7</td>       <td style="text-align:right;">6.1</td>       <td style="text-align:right;">94.2</td>       <td style="text-align:right;">8.2</td>       <td>15.0</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>people/person/spouse</tt>       </td>       <td style="text-align:right;">49.0</td>       <td style="text-align:right;">2.7</td>       <td style="text-align:right;">5.5</td>       <td style="text-align:right;">84.2</td>       <td style="text-align:right;">8.5</td>       <td>15.4</td>       </tr>       <tr>       <td style="text-align:left;">        <tt>people/person/children</tt>       </td>       <td style="text-align:right;">43.2</td>       <td style="text-align:right;">2.0</td>       <td style="text-align:right;">4.1</td>       <td style="text-align:right;">82.0</td>       <td style="text-align:right;">7.1</td>       <td>14.2</td>       </tr>       <tr>       <td style="text-align:left;">Mean</td>       <td style="text-align:right;">46.7</td>       <td style="text-align:right;">2.3</td>       <td style="text-align:right;">4.4</td>       <td style="text-align:right;">105.3</td>       <td style="text-align:right;">10.0</td>       <td>18.9</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> MRR on Individual Entity Pairs</h3>     </div>    </header>    <p>Fig.&#x00A0;<a class="fig" href="#fig6">6</a> shows the <span class="inline-equation"><span class="tex">$\mathit {MRR}(\mathcal {I})$</span>     </span>, per relation, obtained on predicting the relation for each of the 50 pairs individually. As expected, relation prediction on <SmallCap>KGFacts</SmallCap> is easier than on <SmallCap>LectorFacts</SmallCap> regardless of the ranking model. Quantitatively speaking, <SmallCap>Noisy OR</SmallCap> was 20% more effective than <SmallCap>PLM</SmallCap> on the <SmallCap>KGFacts</SmallCap> corpus (MRR of 0.64 and 0.53, respectively), and 31% more effective on <SmallCap>LectorFacts</SmallCap> (MRR of 0.46 and 0.35, respectively). This can be explained by the larger number of relational phrases that can be obtained with pairs of entities in <SmallCap>KGFacts</SmallCap> (recall Tab.&#x00A0;<a class="tbl" href="#tab1">1</a>). Looking at the MRR results across relations, one can see that some relations are harder to predict than others. This is explained by the ambiguity of the phrases in some language models. For example, &#x201C;is the daughter of&#x201D; is almost exclusively used in <tt>people/person/parents</tt>. Similarly, descriptive phrases like &#x201C;won the&#x201D; and &#x201C;was awarded the&#x201D; are strongly associated with <tt>award/award_winner/awards_won</tt>. Other relations are expressed through generic phrases that can only be interpreted in context, such as &#x201C;leader of the&#x201D;, &#x201C;led the&#x201D; or &#x201C;left the&#x201D; which appear in the models of <tt>.../person/employment</tt>, <tt>.../politician/party</tt> or <tt>.../pro_athlete/teams</tt>.</p>    <p>It should be noted that the difficulties of dealing with ambiguity are more pronounced because the system evaluated here is configured to predict 500 different relations from all Freebase domains. Much better results are to be expected if one learns models for domain-specific relations. <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">MRR on individual entity pairs.</span>      </div>     </figure>    </p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> MRR on Multiple Pairs</h3>     </div>    </header>    <p>Fig.&#x00A0;<a class="fig" href="#fig7">7</a> shows the MRR for each combination of: corpus (<SmallCap>LectorFacts</SmallCap> and <SmallCap>KGFacts</SmallCap>), scoring model (<SmallCap>Noisy OR</SmallCap> and <SmallCap>PLM</SmallCap>), and method for combining evidence (local aggregation and global query), when considering multiple entity pairs. We vary the size of the input set from 3 to 20 pairs, and each plot is the average of 10 different samples. We cap the size to 20 pairs as previous work has reported that this is the average number of rows found in real Web Tables&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. Several general observations are possible from the figure: (1) using multiple pairs for relation ranking has a significant positive impact on MRR; (2) aggregating pairwise rankings is more robust, regardless of scoring model; and (3) <SmallCap>Noisy OR</SmallCap> generally outperforms the standard language model <SmallCap>PLM</SmallCap>. Next, we discuss these findings in more detail. <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">MRR on sets of entity pairs.</span>      </div>     </figure>    </p>    <p>     <strong>More Entity Pairs is Better</strong>.</p>    <p>Unsurprisingly, the more entity pairs we use, the higher the MRR. To quantify this, the highest MRR achieved with individual pairs was 0.64, obtained with the <SmallCap>Noisy OR</SmallCap> on <SmallCap>KGFacts</SmallCap>. Testing the same model on the same corpus with local aggregation, we achieve MRR of 0.84 using 3 pairs and 0.98 using 20 pairs, corresponding to improvements of 31% and 53%, respectively. Looking at <SmallCap>LectorFacts</SmallCap> with <SmallCap>Noisy OR</SmallCap> and local aggregation, even more pronounced gains are evident: 43% with 3 pairs and 89% with 20 pairs. The highest MRR are obtained with 20 entity pairs and the <SmallCap>Noisy OR</SmallCap> model: 0.98 on <SmallCap>KGFacts</SmallCap> with local aggregation, and 0.91 on <SmallCap>LectorFacts</SmallCap> with global queries. We conjecture on this discrepancy below. These results underscore the high potential of our method for Web table understanding, where predictions are made on multiple entity pairs, and not just one.</p>    <p>To see how multiple pairs help relation scoring, suppose the input is a table with soccer players and the teams they played for. If we are given just the pair &#x27E8;<tt>&#x201D;Luis Enrique&#x201D;</tt>,<tt>&#x201D;FC Barcelona&#x201D;</tt>&#x27E9;, we will not be able to discern whether <tt>.../pro_athlete/teams</tt> or <tt>soccer/.../manager</tt> should rank higher as both relations apply to that pair. However, if the input also contains another pair like &#x27E8;<tt>&#x201D;Neymar Jr.&#x201D;</tt>,<tt>&#x201D;Paris St Germain&#x201D;</tt>&#x27E9;, we will be much more likely to predict <tt>.../pro_athlete/teams</tt>. In other words, the ambiguity of the input reduces with more pairs, as expected.</p>    <p>     <strong>Local Aggregation is More Robust</strong>. When it comes to the local aggregation of multiple pairwise predictions versus a single prediction using a global query formed by grouping all sentences from the Web searches, there seems to be a dependence on the actual number of phrases that are used: with a small number of phrases (e.g., as in <SmallCap>LectorFacts</SmallCap>), it is better to use the global approach while with many phrases it is much better to use the local aggregation. Using the AUC of the plots in Fig.&#x00A0;<a class="fig" href="#fig7">7</a> as a proxy, we can quantify this argument: for <SmallCap>LectorFacts</SmallCap>, the global approach is superior by 4% (Fig.&#x00A0;<a class="fig" href="#fig7">7</a>(a) and (c)) while for <SmallCap>KGFacts</SmallCap> the local aggregation is better by 7% (Fig.&#x00A0;<a class="fig" href="#fig7">7</a>(b) and (d)). As a matter of fact, the global aggregation method gets worse on <SmallCap>KGFacts</SmallCap> as more entity pairs are used. This happens because with more pairs there are more (and more diverse) sentences, which leads to the global query to lose focus. For example, looking at some entity pairs in relation <tt>people/person/education</tt> we can find phrases such as &#x201C;who became president of&#x201D;, &#x201C;is the co founder of&#x201D; or &#x201C;played football at&#x201D; that are associated (sometimes very strongly) with many relations. The problem is more evident with the <SmallCap>Noisy OR</SmallCap> model as it does not take phrase frequency into account and thus does not have any way of weighing importance of the phrases in the query. To verify this hypothesis, we pruned all but the 5 most frequent phrases in the queries used in Fig.&#x00A0;<a class="fig" href="#fig7">7</a>(d), and scored these pruned queries with <SmallCap>Noisy OR</SmallCap>, resulting in a 5% improvement.</p>    <p>     <SmallCap>      <strong>Noisy OR</strong>     </SmallCap>     <strong>is Better</strong>. A comparison of the AUC of the plots in Fig.&#x00A0;<a class="fig" href="#fig7">7</a> reveals that scoring with <SmallCap>Noisy OR</SmallCap> is generally superior to that with <SmallCap>PLM</SmallCap>. In quantitative terms, the relative improvements can be as high as 24% (see Fig.&#x00A0;<a class="fig" href="#fig7">7</a>(a)) and considering all the configurations <SmallCap>Noisy OR</SmallCap> obtains a relative gains of 20% over <SmallCap>PLM</SmallCap>. In conclusion, a <SmallCap>Noisy OR</SmallCap> scoring model is very robust if applied with a local aggregation approach independently from the popularity of the entities involved.</p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Matching Phrases Approximately</h3>     </div>    </header>    <p>All results shown so far were obtained by <em>exactly</em> matching sentences from the Web search against relational phrases in the LMs (to form the queries used for relation ranking). Because exact matching leads to low hit counts (sometimes, even empty queries), we experimented with alternative ways of approximately matching sentences to relational phrases, settling for the scheme described in Sec.&#x00A0;<a class="sec" href="#sec-14">4.1</a>. Fig.&#x00A0;<a class="fig" href="#fig8">8</a> shows that approximate matching (dashed line) generally improves on exact matching (solid line). For clarity, we show the results with local aggregation only. In quantitative terms, comparing the AUC of the plots, we observe the biggest improvement (19%) for the <SmallCap>PLM</SmallCap> scoring. This improvement can be attributed to the fact that approximate matching effectively doubles the number of hits as shown in Tab.&#x00A0;<a class="tbl" href="#tab1">1</a> (see the <em>phrase</em> columns). <figure id="fig8">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186029/images/www2018-38-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">Approximate (dashed lines) versus exact (solid lines) matching on multiple entity pairs with local aggregation.</span>      </div>     </figure>    </p>    <p>It is worth noting that approximate matching closes the gap in MRR between <SmallCap>KGFacts</SmallCap> and <SmallCap>LectorFacts</SmallCap> considerably. Comparing the AUC between plots, the best setting for <SmallCap>KGFacts</SmallCap> (<SmallCap>Noisy OR</SmallCap> with local aggregation and exact phrase matching) is only 6% superior to the best setting for <SmallCap>LectorFacts</SmallCap> (<SmallCap>Noisy OR</SmallCap> with local aggregation and approximate phrase matching).</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.5</span> Pruning LMs</h3>     </div>    </header>    <p>While our concern in this work is effectiveness, we report on a brief experiment on the natural trade-off between the LM sizes, inference time and accuracy. Tab.&#x00A0;<a class="tbl" href="#tab2">2</a> shows the MRR and the inference time of the two models on samples of 20 pairs from <SmallCap>LectorFacts</SmallCap> for different values of the minimum <em>support</em> (F in the table) required for a phrase to be used in any language model<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a>. Unsurprisingly, increasing the minimum support leads to fewer and smaller models, and, consequently: a drop in MRR, and a drop in inference time. For the <SmallCap>Noisy OR</SmallCap> scoring model, however, the trade-off is quite favorable: with F =100 there is no drop in MRR while the inference time is cut in half. This is easily explained by the power-law distribution of phrases in the ClueWeb09 corpus (Sec.&#x00A0;<a class="sec" href="#sec-10">3.2</a>) and the fact <SmallCap>Noisy OR</SmallCap> relies on phrases that are <em>strongly</em> associated with a relation, which inevitably ignores phrases with low support.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">MRR and inference time versus minimum phrase <em>support</em> (F).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:right;">F</th>       <th style="text-align:center;" colspan="2">        <SmallCap>PLM</SmallCap>        <hr/>       </th>       <th style="text-align:center;"/>       <th style="text-align:center;" colspan="2">        <SmallCap>Noisy OR</SmallCap>        <hr/>       </th>       </tr>       <tr>       <th style="text-align:right;"/>       <th style="text-align:center;">MRR</th>       <th style="text-align:center;">time (ms)</th>       <th style="text-align:center;"/>       <th style="text-align:center;">MRR</th>       <th style="text-align:center;">time (ms)</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:right;">1</td>       <td style="text-align:right;">0.71</td>       <td style="text-align:center;">230.0 , 11.1</td>       <td style="text-align:center;"/>       <td style="text-align:right;">0.88</td>       <td style="text-align:center;">78.5 , 11.0</td>       </tr>       <tr>       <td style="text-align:right;">10</td>       <td style="text-align:right;">0.70</td>       <td style="text-align:center;">180.5 , &#x00A0;3.5</td>       <td style="text-align:center;"/>       <td style="text-align:right;">0.87</td>       <td style="text-align:center;">78.1 , &#x00A0;7.1</td>       </tr>       <tr>       <td style="text-align:right;">100</td>       <td style="text-align:right;">0.55</td>       <td style="text-align:center;">178.1 , &#x00A0;4.6</td>       <td style="text-align:center;"/>       <td style="text-align:right;">0.88</td>       <td style="text-align:center;">44.1 , &#x00A0;2.0</td>       </tr>       <tr>       <td style="text-align:right;">1000</td>       <td style="text-align:right;">0.14</td>       <td style="text-align:center;">167.4 ,&#x00A0;3.5</td>       <td style="text-align:center;"/>       <td style="text-align:right;">0.85</td>       <td style="text-align:center;">39.1 , &#x00A0;1.3</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.6</span> Towards Practical Tools</h3>     </div>    </header>    <p>The observations in the experiments above indicate that the LM-based relation ranking can perform well enough to be useful in practice. Moreover, they indicate that a configuration of <SmallCap>Noisy OR</SmallCap> with local aggregation and approximate sentence matching to be very robust. One possible tuning would be to use the global query approach if the number of sentences returned by the Web search is below an application specific threshold.</p>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.7</span> Towards Web Table Understanding</h3>     </div>    </header>    <p>We now report on a preliminary experiment to show that relation prediction with LMs can be a viable alternative to Entity Linking (EL) for Web Table Understanding. While EL methods fail on tables with entity pairs from <SmallCap>LectorFacts</SmallCap>, even though they concern in-KG entities, the issue is not artificially introduced by that benchmark. To show this, we collected actual Wikipedia tables containing in-KG and out-of-KG entities and facts from different domains and for which even the state-of-the-art EL method T2K Match&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] fails. Since an independent benchmark for this task is lacking, we manually extracted 80 pairs of columns from 65 different Wikipedia tables, for which T2K Match was unable to assign a relation. These pairs cover a range of topics and relations beyond <SmallCap>LectorFacts</SmallCap>, including geography, sports, actors and directors of movies and TV shows, among others.</p>    <p>The average number of entity pairs in this test was 31. The average number of sentences retrieved by the Web search is 39. We tested our method with these 80 pairs and manually evaluated the results to determine if the predicted relations were appropriate. <a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>    </p>    <p>We evaluate the results using precision@1 (1 if the first relation in the output ranking is correct, 0 otherwise) of each pair. Tab.&#x00A0;<a class="tbl" href="#tab3">3</a> shows the number of entity pairs for which the top ranked relation was judged appropriate, for different configurations. The best results (52 relations out of 80) are obtained with approximate matching (A), using the <SmallCap>Noisy OR</SmallCap> score model (N) and locally aggregating the multiple pairs (L), which is also the best configuration on <SmallCap>LectorFacts</SmallCap>. Developing a proper benchmark for this task is non-trivial and important future work for this area.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Relations correctly predicted (out of 80) on Web Table Understanding for different combinations of phrase matching (A-approximate, E-exact), model (N-<SmallCap>Noisy OR</SmallCap>, P-<SmallCap>PLM</SmallCap>) and aggregation approach (L-local, G-global).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">        <strong>EPG</strong>       </th>       <th style="text-align:center;">        <strong>EPL</strong>       </th>       <th style="text-align:center;">        <strong>ENG</strong>       </th>       <th style="text-align:center;">        <strong>ENL</strong>       </th>       <th style="text-align:center;">        <strong>APG</strong>       </th>       <th style="text-align:center;">        <strong>APL</strong>       </th>       <th style="text-align:center;">        <strong>ANG</strong>       </th>       <th>        <strong>ANL</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">35</td>       <td style="text-align:center;">38</td>       <td style="text-align:center;">43</td>       <td style="text-align:center;">49</td>       <td style="text-align:center;">32</td>       <td style="text-align:center;">41</td>       <td style="text-align:center;">28</td>       <td>52</td>       </tr>      </tbody>     </table>    </div>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>This paper studied the problem of predicting relations from a Knowledge Graph that hold over relational data found on the Web, such as tables, structured lists, CSV/TSV files, among others. The methods described here fill a gap left by text-based Information Extraction tools and overcome the main obstacle of table understanding methods relying on linking entities to objects in the graph.</p>    <p>Our approach borrows from Information Retrieval and Information Extraction work and uses generative language models to represent the relations in a KG with relational phrases, extracted from a Web-scale corpus of independently annotated text using the state-of-the-art in Open Information Extraction to filter out noise. Thus, relation prediction is done by ranking the respective models based on how likely they are to generate the phrases on the Web that connect the entities given as input. The paper evaluates different approaches for model ranking and for aggregating evidence to predict relations for a set of entity pairs. Further evaluations with other standard ranking approaches are left for future work.</p>    <p>Finally, the paper reports on an experimental evaluation designed to stress the issues caused by KG incompleteness, a limiting factor of the most related previous work. The encouraging results indicate the method overcome the KG incompleteness issue and that its accuracy is high enough to suggest it can be successfully applied on tasks such as KG construction and augmentation.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-26">    <header>    <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>    </div>    </header>    <p>This work was supported in part by grants from the Natural Sciences and Engineering Research Council of Canada. D. Barbosa was a visiting researcher at the Max Planck Institute for Informatics, Saarbrucken, Germany during the development of parts of this work. The authors thank Y. A. Sekhavat, who contributed to our preliminary work.</p>   </section>   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from Large Plain-text Collections. In <em>      <em>Proceedings of the Fifth ACM Conference on Digital Libraries</em>     </em>(DL &#x2019;00). ACM, 85&#x2013;94. 1-58113-231-X</li>    <li id="BibPLXBIB0002" label="[2]">Krisztian Balog, Marc Bron, and Maarten De&#x00A0;Rijke. 2011. Query Modeling for Entity Search Based on Terms, Categories, and Examples. <em>      <em>ACM Trans. Inf. Syst.</em>     </em>29, 4 (2011), 22:1&#x2013;22:31.</li>    <li id="BibPLXBIB0003" label="[3]">Danushka&#x00A0;Tarupathi Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2010. Relational Duality: Unsupervised Extraction of Semantic Relations Between Entities on the Web. In <em>      <em>Proceedings of the 19th International Conference on World Wide Web</em>     </em>(WWW 2010). ACM, 151&#x2013;160. 978-1-60558-799-8</li>    <li id="BibPLXBIB0004" label="[4]">Marc Bron, Krisztian Balog, and Maarten de Rijke. 2013. Example Based Entity Search in the Web of Data. In <em>      <em>Proceedings of the 35th European Conference on Advances in Information Retrieval</em>     </em>(ECIR&#x2019;13). Springer-Verlag, 392&#x2013;403. 978-3-642-36972-8</li>    <li id="BibPLXBIB0005" label="[5]">Michael&#x00A0;J. Cafarella, Alon&#x00A0;Y. Halevy, Daisy&#x00A0;Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: exploring the power of tables on the web. <em>      <em>PVLDB</em>     </em>1, 1 (2008), 538&#x2013;549.</li>    <li id="BibPLXBIB0006" label="[6]">Matteo Cannaviccio, Denilson Barbosa, and Paolo Merialdo. 2016. Accurate Fact Harvesting from Natural Language Text in Wikipedia with Lector. In <em>      <em>Proceedings of the 19th International Workshop on Web and Databases</em>     </em>(WebDB &#x2019;16). ACM, 9:1&#x2013;9:6. 978-1-4503-4310-7</li>    <li id="BibPLXBIB0007" label="[7]">Filipe de S&#x00E1;&#x00A0;Mesquita, Jordan Schmidek, and Denilson Barbosa. 2013. Effectiveness and Efficiency of Open Relation Extraction. In <em>      <em>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>     </em>. 447&#x2013;457.</li>    <li id="BibPLXBIB0008" label="[8]">Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion. In <em>      <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 601&#x2013;610.</li>    <li id="BibPLXBIB0009" label="[9]">Doug Downey, Stefan Schoenmackers, and Oren Etzioni. 2007. Sparse Information Extraction: Unsupervised Language Models to the Rescue. In <em>      <em>ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</em>     </em>.</li>    <li id="BibPLXBIB0010" label="[10]">Shady Elbassuoni, Maya Ramanath, Ralf Schenkel, Marcin Sydow, and Gerhard Weikum. 2009. Language-model-based Ranking for Queries on RDF-graphs. In <em>      <em>Proceedings of the 18th ACM Conference on Information and Knowledge Management</em>     </em>(CIKM &#x2019;09). ACM, 977&#x2013;986. 978-1-60558-512-3</li>    <li id="BibPLXBIB0011" label="[11]">Jenny&#x00A0;Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In <em>      <em>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</em>     </em>(ACL&#x2019;05). Association for Computational Linguistics, 363&#x2013;370.</li>    <li id="BibPLXBIB0012" label="[12]">Yusra Ibrahim, Mirek Riedewald, and Gerhard Weikum. 2016. Making Sense of Entities and Quantities in Web Tables. In <em>      <em>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</em>     </em>(CIKM &#x2019;16). ACM, 1703&#x2013;1712. 978-1-4503-4073-1</li>    <li id="BibPLXBIB0013" label="[13]">Mandar Joshi, Uma Sawant, and Soumen Chakrabarti. 2014. Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries. In <em>      <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014</em>     </em>. 1104&#x2013;1114.</li>    <li id="BibPLXBIB0014" label="[14]">John Lafferty and Chengxiang Zhai. 2017. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. <em>      <em>SIGIR Forum</em>     </em>51, 2, 251&#x2013;259. 0163-5840</li>    <li id="BibPLXBIB0015" label="[15]">Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and Searching Web Tables Using Entities, Types and Relationships. <em>      <em>PVLDB</em>     </em>3, 1 (2010), 1338&#x2013;1347.</li>    <li id="BibPLXBIB0016" label="[16]">Christopher&#x00A0;D. Manning, Prabhakar Raghavan, and Hinrich Sch&#x00FC;tze. 2008. <em>      <em>Introduction to information retrieval</em>     </em>. Cambridge University Press. 978-0-521-86571-5</li>    <li id="BibPLXBIB0017" label="[17]">Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open Language Learning for Information Extraction. In <em>      <em>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</em>     </em>. 523&#x2013;534.</li>    <li id="BibPLXBIB0018" label="[18]">Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant Supervision for Relation Extraction with an Incomplete Knowledge Base. In <em>      <em>Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA</em>     </em>. 777&#x2013;782.</li>    <li id="BibPLXBIB0019" label="[19]">Emir Mu&#x00F1;oz, Aidan Hogan, and Alessandra Mileo. 2014. Using linked data to mine RDF from wikipedia&#x0027;s tables. In <em>      <em>Proceedings of the 7th ACM international conference on Web search and data mining</em>     </em>. ACM, 533&#x2013;542.</li>    <li id="BibPLXBIB0020" label="[20]">Ndapandula Nakashole, Gerhard Weikum, and Fabian&#x00A0;M. Suchanek. 2012. PATTY: A Taxonomy of Relational Patterns with Semantic Types. In <em>      <em>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</em>     </em>. 1135&#x2013;1145.</li>    <li id="BibPLXBIB0021" label="[21]">Judea Pearl. 1989. <em>      <em>Probabilistic reasoning in intelligent systems - networks of plausible inference</em>     </em>. Morgan Kaufmann.</li>    <li id="BibPLXBIB0022" label="[22]">Jay&#x00A0;M. Ponte and W.&#x00A0;Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In <em>      <em>SIGIR &#x2019;98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>. 275&#x2013;281.</li>    <li id="BibPLXBIB0023" label="[23]">Jeffrey Pound, Ihab&#x00A0;F. Ilyas, and Grant&#x00A0;E. Weddell. 2010. QUICK: Expressive and Flexible Search over Knowledge Bases and Text Collections. <em>      <em>PVLDB</em>     </em>3, 2 (2010), 1573&#x2013;1576.</li>    <li id="BibPLXBIB0024" label="[24]">Dominique Ritze and Christian Bizer. 2017. Matching Web Tables To DBpedia - A Feature Utility Study. In <em>      <em>Proceedings of the 20th International Conference on Extending Database Technology, EDBT 2017, Venice, Italy, March 21-24, 2017.</em>     </em>210&#x2013;221.</li>    <li id="BibPLXBIB0025" label="[25]">Dominique Ritze, Oliver Lehmberg, and Christian Bizer. 2015. Matching HTML Tables to DBpedia. In <em>      <em>Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics</em>     </em>(WIMS &#x2019;15). ACM, Article 10, 10:1&#x2013;10:6&#x00A0;pages. 978-1-4503-3293-4</li>    <li id="BibPLXBIB0026" label="[26]">Dominique Ritze, Oliver Lehmberg, Yaser Oulabi, and Christian Bizer. 2016. Profiling the Potential of Web Tables for Augmenting Cross-domain Knowledge Bases. In <em>      <em>Proceedings of the 25th International Conference on World Wide Web</em>     </em>. 251&#x2013;261.</li>    <li id="BibPLXBIB0027" label="[27]">Yoones&#x00A0;A. Sekhavat, Francesco&#x00A0;Di Paolo, Denilson Barbosa, and Paolo Merialdo. 2014. Knowledge Base Augmentation using Tabular Data. In <em>      <em>Proceedings of the Workshop on Linked Data on the Web co-located with the 23rd International World Wide Web Conference (WWW 2014), Seoul, Korea, April 8, 2014.</em>     </em></li>    <li id="BibPLXBIB0028" label="[28]">Petros Venetis, Alon&#x00A0;Y. Halevy, Jayant Madhavan, Marius Pasca, Warren Shen, Fei Wu, Gengxin Miao, and Chung Wu. 2011. Recovering Semantics of Tables on the Web. <em>      <em>PVLDB</em>     </em>4, 9 (2011), 528&#x2013;538.</li>    <li id="BibPLXBIB0029" label="[29]">Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Surajit Chaudhuri. 2012. Targeted Disambiguation of Ad-hoc, Homogeneous Sets of Named Entities. In <em>      <em>Proceedings of the 21st International Conference on World Wide Web</em>     </em>(WWW &#x2019;12). ACM, 719&#x2013;728. 978-1-4503-1229-5</li>    <li id="BibPLXBIB0030" label="[30]">Jiannan Wang, Guoliang Li, and Jianhua Feng. 2014. Extending String Similarity Join to Tolerant Fuzzy Token Matching. <em>      <em>ACM Trans. Database Syst.</em>     </em>39, 1, Article 7 (Jan. 2014), 7:1&#x2013;7:45&#x00A0;pages. 0362-5915</li>    <li id="BibPLXBIB0031" label="[31]">Yue Wang and Yeye He. 2017. Synthesizing mapping relationships using table corpus. In <em>      <em>Proceedings of the 2017 ACM International Conference on Management of Data</em>     </em>. ACM, 1117&#x2013;1132.</li>    <li id="BibPLXBIB0032" label="[32]">Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge Base Completion via Search-based Question Answering. In <em>      <em>Proceedings of the 23rd International Conference on World Wide Web</em>     </em>(WWW &#x2019;14). ACM, 515&#x2013;526. 978-1-4503-2744-2</li>    <li id="BibPLXBIB0033" label="[33]">Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction. In <em>      <em>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>     </em>. 1366&#x2013;1371.</li>    <li id="BibPLXBIB0034" label="[34]">Mohamed Yahya, Denilson Barbosa, Klaus Berberich, Qiuyue Wang, and Gerhard Weikum. 2016. Relationship Queries on Extended Knowledge Graphs. In <em>      <em>Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</em>     </em>(WSDM &#x2019;16). ACM, 605&#x2013;614. 978-1-4503-3716-8</li>    <li id="BibPLXBIB0035" label="[35]">Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural Language Questions for the Web of Data. In <em>      <em>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</em>     </em>(EMNLP-CoNLL &#x2019;12). Association for Computational Linguistics, 379&#x2013;390.</li>    <li id="BibPLXBIB0036" label="[36]">ChengXiang Zhai. 2008. <em>      <em>Statistical Language Models for Information Retrieval</em>     </em>. Morgan &#x0026; Claypool Publishers.</li>    <li id="BibPLXBIB0037" label="[37]">Ziqi Zhang. 2014. Towards Efficient and Effective Semantic Table Interpretation. In <em>      <em>Proceedings of the 13th International Semantic Web Conference - Part I</em>     </em>(ISWC &#x2019;14). Springer-Verlag New York, Inc., 487&#x2013;502. 978-3-319-11963-2</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://lemurproject.org/clueweb09">http://lemurproject.org/clueweb09</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC) as defined by Stanford NER &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0011">11</a>].</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>We use Jaro-Winkler similarity with a threshold of 0.9.</p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Available at:&#x00A0;<a class="link-inline force-break"    href="http://downloads.dbpedia.org/2016-04/ext/lector_facts/">http://downloads.dbpedia.org/2016-04/ext/lector_facts/</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>    <tt>people/person/nationality</tt>, <tt>people/person/religion</tt> and <tt>people/person/ethnicity</tt> are ontological relations and thus ignored.</p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>For clarity, all other results reported here are on LMs with F =1.</p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a>All data used for this experiment, including the top-3 relations produced by each of our methods, can be found at&#x00A0;<a class="link-inline force-break"    href="https://ln.sync.com/dl/b7e5a9f40#rfyp47tn-irz5dvhn-bwmn58kr-cetrhxmj">https://ln.sync.com/dl/b7e5a9f40#rfyp47tn-irz5dvhn-bwmn58kr-cetrhxmj</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186029">https://doi.org/10.1145/3178876.3186029</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
