<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Learning to Recognize Musical Genre from Audio</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>    <link rel="cite-as" href="https://doi.org/10.1145/3184558.3192310"/>
</head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3192310'>https://doi.org/10.1145/3184558.3192310</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3192310'>https://w3id.org/oa/10.1145/3184558.3192310</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Learning to Recognize Musical Genre from Audio</span>      <br/>      <span class="subTitle">       <SubTitle>Challenge Overview</SubTitle>      </span>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author"><a href="https://orcid.org/0000-0002-6028-9024" ref="author"><span class="givenName">Micha&#x00EB;l</span>      <span class="surName">Defferrard,</span></a>     EPFL, Lausanne, Switzerland, <a href="mailto:michael.defferrard@epfl.ch">michael.defferrard@epfl.ch</a>     </div>     <div class="author">     <span class="givenName">Sharada P.</span>      <span class="surName">Mohanty</span>,     EPFL, Lausanne, Switzerland, <a href="mailto:sharada.mohanty@epfl.ch">sharada.mohanty@epfl.ch</a>     </div>     <div class="author">     <span class="givenName">Sean F.</span>      <span class="surName">Carroll</span>,     EPFL, Lausanne, Switzerland, <a href="mailto:sean.carroll@epfl.ch">sean.carroll@epfl.ch</a>     </div>     <div class="author">     <span class="givenName">Marcel</span>      <span class="surName">Salath&#x00E9;</span>,     EPFL, Lausanne, Switzerland, <a href="mailto:marcel.salathe@epfl.ch">marcel.salathe@epfl.ch</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3192310" target="_blank">https://doi.org/10.1145/3184558.3192310</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>We here summarize our experience running a challenge with open data for musical genre recognition. Those notes motivate the task and the challenge design, show some statistics about the submissions, and present the results.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Music retrieval;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <em>Supervised learning;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Music Information Retrieval (MIR); ML Challenge; Open Data</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Micha&#x00EB;l Defferrard, Sharada P. Mohanty, Sean F. Carroll, and Marcel Salath&#x00E9;. 2018. Learning to Recognize Musical Genre from Audio: Challenge Overview. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 2 Pages. <a href="https://doi.org/10.1145/3184558.3192310" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3192310</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Like never before, the web has become a place for sharing creative work &#x2014; such as music &#x2014; among a global community of artists and art lovers. While music and music collections predate the web, the web has enabled much larger scale collections. Whereas people used to own a handful of vinyl records or CDs, they nowadays have instant access to the whole of published musical content via online platforms such as Spotify, iTunes, Youtube, FMA, Jamendo, Bandcamp, etc. Such dramatic increases in the size of music collections has created two challenges: (i) the need to automatically organize a collection (as users and publishers cannot manage them manually anymore), and (ii) the need to automatically recommend new songs to a user knowing their listening habits. An underlying task in both those challenges is to be able to group songs in semantic categories.</p>    <p>Music genres are categories that have arisen through a complex interplay of cultures, artists, and market forces to characterize similarities between compositions and organize music collections. Yet the boundaries between genres still remain fuzzy, making the problem of music genre recognition (MGR) a nontrivial task&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. While its utility has been debated, mostly because of its ambiguity and cultural definition, it is widely used and understood by end-users who find it useful to discuss musical categories&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>].</p>    <p>The task of this challenge, one of the four Web Conference&#x0027;s challenges, was to recognize the musical genre of a piece of music of which only a recording is available. Genres are broad, e.g. <em>pop</em> or <em>rock</em>, and each song only has one target genre. Other metadata, e.g. the song title or artist name, were not to be used for the prediction.</p>    <p>The data for this challenge comes from the recently published FMA dataset&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>], a dump of the Free Music Archive (FMA).<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> The dataset is a collection of 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> The Challenge</h2>     </div>    </header>    <p>To avoid overfitting and cheating, we organized the challenge in two rounds. The final ranking was based on results from the second round. The training data for both rounds consisted of the FMA medium subset, which is composed of 25,000 clips of 30 seconds, categorized in 16 genres. The categorization is unbalanced with 21 to 7,103 clips per genre. As the data is public, we collected new test data for the second round to prevent access to the test set.</p>    <p>In the first round, participants were provided a test set of 30,000 clips of 30 seconds each and had to upload the predicted genre for each of these clips. The platform readily evaluated those predictions and ranked the participants upon each submission. A subset of these clips were sampled from the FMA large dataset, while ensuring that none overlaps with any clip provided in the training set. The other subset was sampled from songs in the FMA full dataset which are not present in the medium subset.</p>    <p>For the second round, the participants had to provide their models as git repositories which contained the prediction code and the trained model along with an executive summary of their approach. Docker containers were built out of those repositories.<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> We then ran them against a new unseen test set which was sampled from new contributions to the Free Music Archive.</p>    <p>Both rounds used the same evaluation metric. The primary score was the mean log loss and the secondary score was the mean F1 score. The mean log loss is defined by <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} L = - \frac{1}{N} \sum _{n=1}^N \sum _{c=1}^{C} y_{nc} \ln (p_{nc}), \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div> where <em>N</em> = 35000 is the number of examples in the test set, <em>C</em> = 16 is the number of genres, <em>y<sub>nc</sub>     </em> is a binary value indicating if the <em>n</em>-th instance belongs to the <em>c</em>-th label, <em>p<sub>nc</sub>     </em> is the probability according to a submission that the <em>n</em>-th instance belongs to the <em>c</em>-th label, and <span class="inline-equation"><span class="tex">$\ln$</span>     </span> is the natural logarithm. The mean F1 score is given by <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} F_1 = \frac{2}{C} \sum _{c=1}^{C} \frac{p^c r^c}{p^c + r^c}, \hspace{10.0pt} p^c = \frac{tp^c}{tp^c + fp^c}, \hspace{10.0pt} r^c = \frac{tp^c}{tp^c + fn^c}, \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>     </div> where <em>p<sup>c</sup>     </em> an <em>r<sup>c</sup>     </em> are the precision and recall for class <em>c</em>, and <em>tp<sup>c</sup>     </em>, <em>fp<sup>c</sup>     </em>, <em>fn<sup>c</sup>     </em> refers to the number of true positives, false positives, and false negatives.</p>    <p>The challenge was hosted on crowdAI, a public platform for open challenges. Instructions on how to participate, access to training and test data, graded submissions, and the leaderboard were available on the challenge page.<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> Moreover, we developed a starter kit with code to handle the data and make a submission.<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> It also featured some examples and a baseline. Finally, participants were encouraged to review the FMA paper&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] for a detailed description of the data as well as the GitHub repository for Jupyter notebooks showing how to use the data, explore it, and train baseline models.<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>    </p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Results</h2>     </div>    </header>       <p>At the end of the first round, we had engaged a total of 246 participants who either made a submission, downloaded the datasets, or contributed to the forums. From those 246 participants, 38 made at least one submission, with some of the top participants making as many as 110 submissions. A total of 671 submissions were made in the first round. Of these, 77 were invalid and 576 were successfully graded. From those 576 submissions, 364 had a score better than the baseline provided by the organizers. Figure&#x00A0;<a class="fig" href="#fig1">1</a> shows the distribution of F1 scores and log losses. The current best solution has an F1 score of 0.909 and a log loss of 0.330. Figure&#x00A0;<a class="fig" href="#fig2">2</a> shows how the participants progressed through the first round.</p> <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3192310/images/www18companion-428-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Joint distribution of the F1 score and the log loss (if smaller than 5) of all submissions. The red point represents the baseline prepared by the organizers.</span>     </div>    </figure>    <p>Moreover, we reviewed and accepted two papers. In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], the authors compared the following approaches: (i) ConvNet on spectrograms, and (ii) deep neural net, (iii) ExtraTrees, and (iv) XGBoost on higher-level features extracted by Essentia. They found that ensemble methods outperformed neural networks, with XGBoost performing best. In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>], the authors argued that genres are subjective and noisy labels, whereas artists are more objective labels. As an artist is commonly part a subset of genres, and that sets of artists can be seen as exemplars for genres, they hypothesized that musical characteristics which identify an artist may also be key features of certain genres. As such, they proposed to train a multi-task neural network to jointly predict artist group and genre. Results showed that features learned for artist recognition were indeed useful for MGR. They thus achieved transfer learning, though keeping the main MGR task as one of the multiple subtasks was crucial. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3192310/images/www18companion-428-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Score progression of all participants through the first round of the challenge. Each line represents an active participant. The dotted line represents the baseline.</span>     </div>     </figure>    </p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Conclusion</h2>     </div>    </header>    <p>The outcomes of the challenge were multiple. First, the accepted papers presented new perspectives and introduced new methods. Then, all the participants to the second round had to share their code as open-source. We hope that those implementations will be useful to the community, for example to serve as baselines, to be scrutinized, or to be improved upon. Finally, the challenge introduced participants to the new FMA dataset and was an opportunity for them to get familiar with it.</p>    <p>That challenge was part of a wider effort to promote open evaluation in machine learning for music data, of which the release of the open FMA dataset was the first step&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. The goal of this initiative is to establish a reference benchmark based on open data. MIR research has historically suffered from the lack of publicly available benchmark datasets, which stem from the commercial interest in music by record labels, and therefore imposed rigid copyright. The FMA&#x0027;s solution was to aim for tracks which license permits redistribution. All data and code produced during the project and challenge are released under the CC BY 4.0 and MIT licenses.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Micha&#x00EB;l Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. 2017. FMA: A Dataset for Music Analysis. In <em>      <em>ISMIR</em>     </em>.</li>     <li id="BibPLXBIB0002" label="[2]">Jaehun Kim, Minz Won, Xavier Serra, and Cynthia C.&#x00A0;S. Liem. 2018. Transfer Learning of Artist Group Factors to Musical Genre Classification. In <em>      <em>WWW</em>     </em>.</li>     <li id="BibPLXBIB0003" label="[3]">C McKay and I Fujinaga. 2006. Musical genre classification: Is it worth pursuing and how can it be improved?. In <em>      <em>ISMIR</em>     </em>.</li>     <li id="BibPLXBIB0004" label="[4]">Benjamin Murauer and G&#x00FC;nther Specht. 2018. Detecting Music Genre Using Extreme Gradient Boosting. In <em>      <em>WWW</em>     </em>.</li>     <li id="BibPLXBIB0005" label="[5]">N Scaringella, G Zoia, and D Mlynek. 2006. Automatic genre classification of music content: a survey. <em>      <em>IEEE Signal Processing Magazine</em>     </em>(2006).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://freemusicarchive.org">https://freemusicarchive.org</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://github.com/jupyter/repo2docker">https://github.com/jupyter/repo2docker</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break"     href="https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre">https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break"     href="https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit">https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>Code and data available at <a class="link-inline force-break" href="https://github.com/mdeff/fma.">https://github.com/mdeff/fma.</a>. The <tt>rc1</tt> version was used.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18 Companion, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/> ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3192310">https://doi.org/10.1145/3184558.3192310</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
