<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Cold Start Thread Recommendation as Extreme Multi-label Classification</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3184558.3191659'>https://doi.org/10.1145/3184558.3191659</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191659'>https://w3id.org/oa/10.1145/3184558.3191659</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Cold Start Thread Recommendation as Extreme Multi-label Classification</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Kishaloy</span>      <span class="surName">Halder</span>,     School of Computing, National University of Singapore, <a href="mailto:kishaloy@comp.nus.edu.sg">kishaloy@comp.nus.edu.sg</a>     </div>     <div class="author">     <span class="givenName">Lahari</span>      <span class="surName">Poddar</span>,     School of Computing, National University of Singapore, <a href="mailto:lahari@comp.nus.edu.sg">lahari@comp.nus.edu.sg</a>     </div>     <div class="author">     <span class="givenName">Min-Yen</span>      <span class="surName">Kan</span>,     School of Computing, National University of Singapore, <a href="mailto:kanmy@comp.nus.edu.sg">kanmy@comp.nus.edu.sg</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191659" target="_blank">https://doi.org/10.1145/3184558.3191659</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>In public online discussion forums, the large user base and frequent posts can create challenges for recommending threads to users. Importantly, traditional recommender systems, based on collaborative filtering, are not capable of handling <em>never-seen-before</em> items (threads). We can view this task as a form of Extreme Multi-label Classification (XMLC), where for a newly-posted thread, we predict the set of users (labels) who will want to respond to it. Selecting a subset of users from the set of all users in the community poses significant challenges due to scalability, and sparsity. We propose a neural network architecture to solve this <em>new</em> thread recommendation task. Our architecture uses stacked bi-directional Gated Recurrent Units (GRU) for text encoding along with cluster sensitive attention for exploiting correlations among the large label space. Experimental evaluation with four datasets from different domains show that our model outperforms both the state-of-the-art recommendation systems as well as other XMLC approaches for this task in terms of MRR, Recall, and NDCG.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Recommendation System; Cold Start; Extreme Multi-Label Classification; Neural Network; Discussion Forum</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Kishaloy Halder, Lahari Poddar, and Min-Yen Kan. 2018. Cold Start Thread Recommendation as Extreme Multi-label Classification. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA 8 Pages. <a href="https://doi.org/10.1145/3184558.3191659" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191659</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Online Forums have become an important social media platform across many domains such as health<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, education<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, technical question answering<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>, e-commerce, government policy making and so on. Discussion in these forums are usually in the form of <em>threads</em>. One starts a thread by posting a question or asking others for opinions on a certain topic. Community members then participate in the thread by replying with their knowledge and opinions on the topic.</p>    <p>These forums are continuously growing as new threads are created frequently. While this enables users to ask questions to a large community, ensuring that the members find questions relevant to their interests, is key to getting them answered. This is a challenging matching problem, due to the huge number of threads, and the large number of active members in the community. Recommendation systems can help bridge this gap by suggesting users with relevant interests and expertise for a discussion thread.</p>    <p>We build a system that recommends incoming threads to relevant users for participation. Recommendation systems mostly use past interaction history of a user or item to solve the matching problem. Even though this strategy can model users, given the threads they responded to in the past, it will fail on new threads. They have no interaction history to facilitate predictions &#x2013; this is a form of cold start. For such a thread, the system needs to use its textual content in order to find potentially interested users.</p>    <p>We view this cold start thread recommendation from a different perspective &#x2013; as one of supervised eXtreme Multi-Label Classification (XMLC). XMLC has been applied for text classification in domains where a document can have multiple tags among several thousands of possible tags (e.g., Wikipedia page categorization or product categorization in e-commerce). Recently deep learning approaches have been proposed for this area for better text understanding and handling the large label space efficiently [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>].</p>    <p>We propose a novel neural network architecture for this recommendation task. Inspired by the success of Recurrent Neural Networks (RNNs) on a range of natural language processing tasks, we apply stacked bidirectional RNN for encoding the raw textual content of a post. We consider the multi-label prediction task as multiple, individual binary classifications where the correlation among labels (i.e., users) is exploited by the model.</p>    <p>We hypothesize that users can be subdivided into clusters in a latent space depending on their interests. Users belonging to the same cluster are likely to have similar preferences and vice versa. In the literature, we find similar observations in different contexts around recommendation systems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. Inspired from this, we introduce a novel, cluster-sensitive attention (CSA) mechanism. It allows a post text to be encoded differently for different clusters using cluster-specific attention weights. This lets the network focus on parts of the text that might be more important for the set of clustered users while predicting their participation interest. Assuming similarity of preferences among users, and learning text encoding per cluster (as opposed to every individual user), helps us in addressing the scalability of the extreme multi-label task by reducing the parameter space. Additionally, it also helps in alleviating the sparsity issue, as the limited amount of evidence per user could easily lead to overfitting in such complex model architecture, otherwise. From our results over multiple datasets, we find that our CSA-based XMLC model outperforms standard content-based recommendation algorithms as well as state-of-the-art XMLC models significantly.</p>    <p>To the best of our knowledge, this is the first attempt at solving the cold start recommendation problem from the extreme multi-label classification perspective. To summarize, our contributions are the following:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">We formulate the well-known cold start recommendation problem as an Extreme Multi-Label Classification task.<br/></li>     <li id="list2" label="&#x2022;">We propose a neural architecture using a novel cluster sensitive attention mechanism to cater to the varying interests of users.<br/></li>     <li id="list3" label="&#x2022;">We show the effectiveness and generalization ability of our approach through a set of carefully-designed experiments, over multiple datasets. Additionally, we validate our problem formulation by comparing our model with traditional recommendation algorithms.<br/></li>    </ul>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Background</h2>     </div>    </header>    <p>We first describe the cold start problem commonly found in recommendation systems, and thereafter describe the approaches for extreme multi-label classification. We then conclude this section by connecting these two parts in a formal problem statement.</p>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Cold Start Recommendation Problem</h3>     </div>     </header>     <p>The two primary elements in a recommendation scenario are users and items. The user&#x2013;item interaction forms a bipartite graph (Figure&#x00A0;<a class="fig" href="#fig1">1</a> a) where a directed edge from a user to an item represents that the user has interacted in some way with the item (e.g &#x2018;like&#x2019;, &#x2018;comment&#x2019;, &#x2018;retweet&#x2019; etc). The corresponding interaction matrix is shown in Figure <a class="fig" href="#fig1">1</a> b. In the widely used latent factor models, the user and item are represented in a low-dimensional (<em>D</em>) space - user is denoted by a latent vector <span class="inline-equation"><span class="tex">$\mathbf {u}_i \in \mathbb {R}^D$</span>     </span>, and item by <span class="inline-equation"><span class="tex">$\mathbf {v}_j \in \mathbb {R}^D$</span>     </span>. The prediction <em>r<sub>ij</sub>     </em> is formed by an inner product of these two vectors, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} r_{ij} = \mathbf {u}_i^{T}\mathbf {v}_j\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>In non-negative matrix factorization (NMF) based approaches, the latent vectors are initialized randomly and can be learned using a regularized squared error loss in terms of <strong>u</strong>     <sub>      <em>i</em>     </sub> and <strong>v</strong>     <sub>      <em>j</em>     </sub>, where <em>i</em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>U</em>}&#x2009;and&#x2009;<em>j</em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>V</em>}; <em>U</em>, and <em>V</em> are the number of users and items respectively.</p>     <p>Let&#x0027;s consider the dynamics when a newly-created item &#x2018;4&#x2019; is introduced. In the interaction matrix, since the column for item &#x2018;4&#x2019; is entirely unobserved, it is also referred as <em>out-of-matrix</em> item recommendation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. As no ground truth value of <em>r<sub>ij</sub>     </em> for <em>j</em> = 4 is available, the model will <em>not</em> be able to learn the correct representation of <strong>v</strong>     <sub>      <em>j</em> = 4</sub>, giving rise to cold start problem. This is a significant limitation of an NMF based recommender system in a forum context where new threads are posted quite frequently needing user participation. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191659/images/www18companion-398-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Illustration of the cold start problem. (a) Edges represent user interactions. Item 4 has no interaction. (b) In interaction matrix: &#x2018;1&#x2019; &#x21D2; interaction, &#x2018;0&#x2019; &#x21D2; no interaction.</span>      </div>     </figure>     </p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Extreme Multi-label Classification</h3>     </div>     </header>     <p>Extreme multi-label classification (XMLC) refers to the task of assigning each item its most relevant subset of labels from an extremely large collection of class labels. The fundamental difference between multi-label classification and traditional binary or multi-class classification task is that in multi-class classification only one among the possible labels applies to an item, whereas in multi-label classification the labels can be correlated with each other or have a subsuming relationship, and multiple labels can apply for an item (e.g., &#x2018;politics&#x2019; and &#x2018;White House&#x2019; for news articles, &#x2018;electronics&#x2019;, &#x2018;Samsung&#x2019; and &#x2018;smartphone&#x2019; for products, &#x2018;Eiffel tower&#x2019; and &#x2018;vacation 2017&#x2019; for an image).</p>     <p>In this setting, an instance can be considered as a pair (<strong>x</strong>, <strong>y</strong>) where <strong>x</strong> is the feature vector for an item, and <strong>y</strong> is the label vector i.e., <strong>y</strong> &#x2208; {0, 1}<sup>      <em>L</em>     </sup>, <em>L</em> is the number of labels. Given <em>n</em> such training instances, a classifier is trained which can predict the label vector for an unseen test item. Since the label space <em>L</em> can be extremely large, it suffers from scalability, and sparsity issues. Properly exploiting the correlation among labels can help in alleviating them.</p>     <p>     <strong>Problem Statement:</strong>We approach the cold-start thread recommendation problem from an XMLC task perspective, where given a <em>new</em> thread, using only its textual features we try to predict the set of interested users. We formalize the problem statement as,</p>     <p>Given a piece of text <em>t</em> &#x2208; <em>T</em>, find a mapping <em>f</em>: <em>T</em> &#x2192; {0, 1}<sup>      <em>U</em>     </sup> where <em>T</em> is the set of all items. <em>f</em> would give us a probability score for each of the <em>U</em> labels given <em>t</em>, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ f(t) = P(r_i = 1|t) \] </span>       <br/>      </div>     </div>     </p>     <p>where <em>i</em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>U</em>}, and <em>r<sub>i</sub>     </em> is the label corresponding to <em>i</em>     <sup>th</sup> user.</p>    </section>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Proposed Method</h2>     </div>    </header>    <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191659/images/www18companion-398-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 2:</span>     <span class="figure-title">Overall model architecture.</span>     </div>    </figure>    <p>We propose a neural network architecture (Figure&#x00A0;<a class="fig" href="#fig2">2</a>) to predict the subset of users interested in a new thread from the extremely large set of users in the forum community. As a newly-created thread has only a single post, we use the terms <em>thread</em> and <em>post</em> interchangeably.</p>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Text Encoding</h3>     </div>     </header>     <p>The network takes as input a post text <em>p</em> consisting of a sequence of words (<em>w</em>     <sub>1</sub>, <em>w</em>     <sub>2</sub>, &#x2026;, <em>w<sub>n</sub>     </em>).</p>     <p>We first embed each word in a lower-dimensional space so that a post is now represented as a sequence of word vectors {<strong>q<sub>1</sub>, q<sub>2</sub>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>q<sub>n</sub>     </strong>} where <span class="inline-equation"><span class="tex">$\mathbf {q_i} \in \mathbb {R}^d$</span>     </span>. We initialize the word vectors using pre-trained GloVe embeddings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] but tune it during training to capture domain specific semantics.</p>     <p>The post is then encoded using bi-directional RNNs. The input to the bi-directional RNN is the embedded word sequence of a post {<strong>q<sub>1</sub>     </strong>, <strong>q<sub>2</sub>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>q<sub>n</sub>     </strong>} and the output is a sequence of vectors <strong>h</strong>     <sup>      <em>p</em>     </sup> = {<strong>h<sub>1</sub>     </strong>, <strong>h<sub>2</sub>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>h<sub>n</sub>     </strong>} where <span class="inline-equation"><span class="tex">$\mathbf {h_i} \in \mathbb {R}^g$</span>     </span> denotes the encoded representation of the post.</p>     <p>An RNN reads the sequence of word vectors {<strong>q<sub>1</sub>, q<sub>2</sub>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>q<sub>n</sub>     </strong>} from left to right in the forward pass and creates a sequence of hidden states <span class="inline-equation"><span class="tex">$\lbrace \mathbf {h_1^f}, \mathbf {h_2^f}, \cdots , \mathbf {h_n^f}\rbrace$</span>     </span>, where <span class="inline-equation"><span class="tex">$\mathbf {h_i^f}$</span>     </span> is computed as: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {h_i^f} = RNN(\mathbf {q_i}, \mathbf {h_{i-1}^f}) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where RNN is a function. Due to vanishing (and conversely, exploding) gradients, the basic RNN cannot learn long-distance temporal dependencies with gradient-based optimization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. To deal with this, extensions to the basic RNN have been proposed that incorporate a memory unit to remember long term dependencies. We use one such variant named Gated Recurrent Unit (GRU) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>] instead of the basic RNN in our model.</p>     <p>In the backward pass, a GRU reads the input sequence in reverse order and returns a sequence of hidden states <span class="inline-equation"><span class="tex">$\lbrace \mathbf {h_n^b}, \mathbf {h_{n-1}^b}, \cdots , \mathbf {h_{1}^b}\rbrace$</span>     </span>. The forward and backward hidden states are then concatenated to create the encoded hidden state of a word <span class="inline-equation"><span class="tex">$\mathbf {h_i} = [\mathbf {h_i^f};\mathbf {h_i^b}]$</span>     </span> considering all its surrounding words.</p>     <p>We use a stack of such bi-directional GRUs where the output of a GRU layer is fed as input to the GRU at next level. This increases the expressive power of the network by capturing higher-level feature interactions between different words. The output sequence from the final bi-directional GRU layer is the representation of the post text <strong>h</strong>     <sup>      <em>p</em>     </sup>. In our experiments, we have used a stack of two bi-directional GRUs. We also experimented with adding more layers, but that did not lead to much improvements in our results.</p>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Cluster Sensitive Attention</h3>     </div>     </header>     <Quote>     <p>      <em>I have been recommended to undergo tracheotomy and put in a PEG. I am wondering how many days I&#x0027;ll have to stay in the hospital? Will I have a hard time adjusting afterwards? Does the hose need to be connected while transferring? Will the equipments take up a lot of room? How do you call for help? I am unable to talk or move. What type of tube would you suggest? I have been a member of the ALS community for some time now. It is nice to read the way some people think and face ALS, it gives me courage.</em>     </p>     </Quote>     <p>The above is an illustrative post (synthetically modified for anonymity) in an ALS forum. The patient is about to undergo a surgical procedure (tracheotomy) and has queries regarding the procedure, recovery time and the after effects. Furthermore, since the procedure creates a hole in the neck to provide an air passage to the windpipe, it disrupts the normal eating and speaking abilities of a person. The patient therefore has additional questions regarding the best feeding tubes and ways of communicating with others. Given its complexity and detailed information need, individual users are unlikely to be able to answer all parts of it. Instead, we envision that users with different backgrounds and experience could address specific parts; e.g., someone having experience with a PEG (Percutaneous Endoscopic Gastrostomy) could answer the queries regarding it, while someone else could help clear the user&#x0027;s concerns regarding the procedure and recovery. Put succinctly, different users may be interested in disparate parts of a (new) post.</p>     <p>This motivates us to build a component in our network that can help focus on parts of a post for different users. To achieve this, we need an <em>attention</em> mechanism that can give different weights to words of the post and generate an encoded text representation using the weighted words, thus focusing on important parts.</p>     <p>Given the encoded text representation of a post <em>p</em>, as <strong>h</strong>     <sup>      <em>p</em>     </sup> = {<strong>h<sub>1</sub>     </strong>, <strong>h<sub>2</sub>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>h<sub>n</sub>     </strong>} from the bi-directional GRU component, the attention mechanism [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] weights each of the hidden states of the words i.e. <strong>h<sub>i</sub>     </strong>. For each <strong>h<sub>i</sub>     </strong>, we compute a weight <em>a<sub>i</sub>     </em> for its corresponding word <em>w<sub>i</sub>     </em> and get an attention vector <strong>a</strong> = {<em>a</em>     <sub>1</sub>, <em>a</em>     <sub>2</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>a<sub>n</sub>     </em>} as: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{align} a_i &#x0026;= \frac{\text{exp}(e_i)}{\sum _{j=1}^n \text{exp}(e_j)} , \text{ where} \end{align} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>     <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{align} e_i &#x0026;= tanh(\mathbf {W_i \dot{h}_i} + b_i) \end{align} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <strong>W<sub>i</sub>     </strong> is a weight matrix of dimension 1 &#x00D7; <em>g</em>, and <em>b<sub>i</sub>     </em> is the bias term. The text representation with attention is then computed as: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {c} = \sum _{i}^n a_i \mathbf {h_i} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>     </p>     <p>Note that a single attention layer is insufficient, since the attention weights(<strong>a</strong>) should not be general but should instead be dependent on different users&#x2019; interests. Na&#x00EF;vely, to achieve per-user attention, we need <em>U</em> such attentions. This will significantly expand the number of parameters to be estimated to an extremely large value (<em>U</em> &#x00D7; <em>n</em> &#x00D7; <em>g</em>), which is infeasible to train due to the scalability issue. Additionally, in most datasets not enough data-points are available for all users, to reliably learn the individual attention vectors.</p>     <p>We assume that, since the forums are topical, the users can be softly clustered in a finite number of clusters depending on their interests. The number of clusters <em>k</em>, would be much smaller than <em>U</em> (i.e. <em>k</em> &#x226A; <em>U</em>). Therefore, instead of learning <em>U</em> different attention vectors, we only need to learn <em>k</em> such vectors. This reduces the parameter space hugely. We call this as cluster sensitive attention mechanism. From the same hidden text representation <strong>h</strong>     <sup>      <em>p</em>     </sup>, we learn <em>k</em> different attention weight vectors <strong>a<sup>1</sup>     </strong>, <strong>a<sup>2</sup>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>a<sup>k</sup>     </strong>. Thereafter, by using the different attention weights on <strong>h</strong>     <sup>      <em>p</em>     </sup>, we get a cluster sensitive encoding of the post text <em>p</em> (<strong>C</strong>     <sup>      <em>p</em>     </sup> = <strong>c<sub>1</sub>     </strong>, <strong>c<sub>2</sub>     </strong>, &#x22C5;&#x22C5;&#x22C5;, <strong>c<sub>k</sub>     </strong>).</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Multi-label Prediction</h3>     </div>     </header>     <p>For a post <em>p</em>, we concatenate the <em>k</em> text encodings and feed through a fully connected layer with <em>U</em> output neurons. For each of the output neuron (corresponding to each user), the fully connected layer learns the weights for its <em>k</em> inputs (corresponding to the different text encodings). <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {z}_p = tanh(\mathbf {W}.\mathbf {C}^p + \mathbf {b}) \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where <strong>W</strong> and <strong>b</strong> are weight and bias matrices respectively and <em>tanh</em> is an element-wise non-linear activation function. The output of this feed-forward layer <span class="inline-equation"><span class="tex">$\mathbf {z}_p \in \mathbb {R}^U$</span>     </span> is then passed through a <em>sigmoid</em> activation function to scale each of its element value in the range [0,1]. The model is trained using binary cross-entropy as the loss function which is defined as, <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathcal {L} = - \frac{1}{T} \sum _{i=1}^T \sum _{j=1}^U \big (y_{ij}.\log (\sigma (z_{ij})) + (1-y_{ij})\log (1-\sigma (z_{ij}))\big) \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <em>&#x03C3;</em> denotes the sigmoid function <span class="inline-equation"><span class="tex">$\sigma (x) = \frac{1}{1+e^{-x}}$</span>     </span>, <em>z<sub>ij</sub>     </em> is <em>j</em>     <sup>th</sup> element in <strong>z</strong>     <sub>      <em>i</em>     </sub>, and <em>y<sub>ij</sub>     </em> is the ground truth value for <em>j</em>     <sup>th</sup> user (label) and <em>i</em>     <sup>th</sup> post. Our network is end-to-end trainable and is optimized with Adam optimizer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>].</p>    </section>   </section>   <section id="sec-12">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>To evaluate the generalization ability of our model, we experimented with multiple datasets from different domains involving users, and some form of textual items in a recommendation scenario. We also present a comparison with some of the well-known content based recommendation systems, as well as the state-of-the-art XMLC approaches to show its effectiveness.</p>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Dataset</h3>     </div>     </header>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Dataset Statistics.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Dataset</th>        <th style="text-align:left;">#users</th>        <th colspan="2" style="text-align:center;">#threads<hr/>        </th>        <th colspan="2" style="text-align:center;">Avg #word in thread<hr/>        </th>        <th colspan="2" style="text-align:center;">Avg #user per thread<hr/>        </th>        <th style="text-align:left;">Sparsity</th>       </tr> 						</thead> 						<tbody>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;"/>        <td style="text-align:left;">train</td>        <td style="text-align:left;">test</td>        <td style="text-align:left;">train</td>        <td style="text-align:left;">test</td>        <td style="text-align:left;">train</td>        <td style="text-align:left;">test</td>        <td style="text-align:left;"/>       </tr>       <tr>        <td style="text-align:left;">1. Epilepsy</td>        <td style="text-align:left;">1506</td>        <td style="text-align:left;">1644</td>        <td style="text-align:left;">412</td>        <td style="text-align:left;">147</td>        <td style="text-align:left;">168</td>        <td style="text-align:left;">7.39</td>        <td style="text-align:left;">9.29</td>        <td style="text-align:left;">99.49%</td>       </tr>       <tr>        <td style="text-align:left;">2. ALS</td>        <td style="text-align:left;">3182</td>        <td style="text-align:left;">6466</td>        <td style="text-align:left;">1617</td>        <td style="text-align:left;">148</td>        <td style="text-align:left;">135</td>        <td style="text-align:left;">9.85</td>        <td style="text-align:left;">9.75</td>        <td style="text-align:left;">99.69%</td>       </tr>       <tr>        <td style="text-align:left;">3. Fibromyalgia</td>        <td style="text-align:left;">5669</td>        <td style="text-align:left;">8576</td>        <td style="text-align:left;">2144</td>        <td style="text-align:left;">203</td>        <td style="text-align:left;">233</td>        <td style="text-align:left;">9.02</td>        <td style="text-align:left;">9.14</td>        <td style="text-align:left;">99.84%</td>       </tr>       <tr>        <td style="text-align:left;">4. Stackoverflow</td>        <td style="text-align:left;">69,631</td>        <td style="text-align:left;">20,137</td>        <td style="text-align:left;">5035</td>        <td style="text-align:left;">93</td>        <td style="text-align:left;">99</td>        <td style="text-align:left;">6.81</td>        <td style="text-align:left;">7.29</td>        <td style="text-align:left;">99.99%</td>       </tr>      </tbody>     </table>     </div>     <p>We used the following datasets in our experiments.</p>     <ul class="list-no-style">     <li id="list4" label="&#x2022;"><strong>[1-3] Health Forum:</strong> a popular online health discussion forum website where users can post a thread asking something related to their disease. Other relevant users reply in the threads to share their experiences with it. The website consists of subforums for different diseases. We used three subforum datasets i.e., &#x2018;Epilepsy,&#x2019; &#x2018;ALS&#x2019;, and &#x2018;Fibromyalgia&#x2019; for the experiments. We removed threads which have replies from lesser than 4 users or greater than 100 to get rid of extremely off-topic or survey threads.<br/></li>     <li id="list5" label="&#x2022;"><strong>[4] Stackoverflow:</strong> is a CQA website for programming related questions. We obtained a data-dump from kaggle<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>. We have used all the questions posted during 2008 &#x2212; 2010 to form the dataset. We have removed all the code snippets (encapsulated within the tags &#x2018;&#x3C;code&#x3E;&#x3C;code&#x3E;&#x2019;) from the question texts.<br/></li>     </ul>     <p>The dataset statistics are presented in Table <a class="tbl" href="#tab1">1</a>. We observe that the number of labels (i.e., users) is quite large in the stackoverflow dataset. This leads to extremely high sparsity (99.99%) as well. We will describe in Section <a class="sec" href="#sec-17">4.5</a> how this affects the recommendation accuracy compared to the others.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Metrics</h3>     </div>     </header>     <p>In our setting, the label set is huge with very high sparsity. Therefore we do not use overall accuracy as our evaluation metric and only aim to evaluate the positive instances i.e. the users who actually participated in a thread. To ensure participation, the ranking quality of the recommended list of users should be evaluated and commonly used metrics for such evaluation include the Mean Reciprocal Rank, precision at top M, Normalized Discounted Cumulated Gains at top M, and Recall at top M. Even though <em>precision at top M</em> is usually used for evaluating XMLC methods, it is not appropriate in our case. This is due to the fact that the labels are implicit user feedback. A negative instance could imply that the user is actually not interested in the thread but could also imply that the user had not seen it (and could have been interested in).</p>     <p>We use the following three metrics to evaluate the recommendation quality of the competing methods</p>     <ul class="list-no-style">     <li id="list6" label="&#x2022;"><strong>Mean Reciprocal Rank (MRR)</strong> indicates the position of the first relevant user in the ranked list. This measures the ability of a system in identifying an interested user at the top of the ranking. Let <em>r<sub>t</sub>      </em> be the rank of the highest ranking relevant user for a test thread <em>t</em>. MRR is just the reciprocal rank, averaged over all threads in test set, <em>n</em>: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} MRR=\frac{1}{n} \sum _{t=1}^{n}\frac{1}{r_{t}}\end{equation*} </span>        <br/>       </div>      </div>      <br/></li>     <li id="list7" label="&#x2022;"><strong>Recall@M</strong> considers how many top-<em>M</em> users actually interacted with the thread (higher is better). Recall for the entire system is computed as the average recall value for all threads in test data.<br/></li>     <li id="list8" label="&#x2022;"><strong>Normalized Discounted Cumulative Gain (NDCG@M)</strong>is well suited for evaluation of recommendation system, as it rewards relevant results ranked higher in the returned list more heavily than those ranked lower. NDCG@M for a thread <em>t</em> is computed as: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} NDCG_{t}=Z_{t}\sum _{j=1}^{M}\frac{2^{r(j)}-1}{log(1+j)}\end{equation*} </span>        <br/>       </div>      </div> where <em>Z<sub>i</sub>      </em> is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each <em>r</em>(<em>j</em>) is an integer relevance level (for our case, <em>r</em>(<em>j</em>) = 1 and <em>r</em>(<em>j</em>) = 0 for relevant and irrelevant recommendations, respectively) of result returned at the rank <em>j</em> &#x2208; {1, &#x22C5;&#x22C5;&#x22C5;, <em>k</em>}. Then, for each <em>M</em> value, <em>NDCG<sub>t</sub>      </em> is averaged over all (<em>n</em>) threads in the test set to get the overall NDCG@M.<br/></li>     </ul>     <p>In our evaluation, we experiment with <em>M</em> = {5, 10, 30, 50, 100} to determine the quality of recommendation at different thresholds of the ranked list.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Baselines</h3>     </div>     </header>     <p>We compare our model with the following competing methods:</p>     <p>     <strong>CVAE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>] :</strong> was proposed to tackle cold start problem using a Bayesian generative model. It reportedly outperforms many state-of-the-art recommendation systems by considering both rating and textual content using deep learning.</p>     <p>     <strong>CTR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0027">27</a>] :</strong> takes LDA [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] discovered topic distributions as input along with the user-item interaction matrix. This has proven to be a very solid baseline for cold start problem and we use it as a representative of traditional recommendation algorithms.</p>     <p>     <strong>CNN-Kim [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>]:</strong> constructs a document vector with its constituent word embeddings, and then convolutional filters are applied to this feature maps. The features pass through a max-over-time pooling layer to construct the document representation. For prediction, the document representation is fed to a fully-connected layer with <em>L</em> softmax outputs, corresponding to the <em>L</em> labels.</p>     <p>     <strong>XML-CNN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0018">18</a>]:</strong> introduces some advancements over CNN-Kim. It adopts a dynamic max pooling scheme, a bottleneck layer and a loss function more suitable for multi-label prediction. It has reportedly outperformed many traditional XMLC models over several datasets.</p>     <p>     <strong>BiGRU-2:</strong> is a baseline implemented by us which uses a stack of two Bidirectional GRU layers for text representation. This is essentially equivalent to our model without the CSA component.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Experimental Settings</h3>     </div>     </header>     <p>Pre-processing for CTR is done as per the recommendations in the paper. We remove all the stopwords and compute tf-idf scores for all the words in all the documents in the training set and retain the top 8000 words to form the vocabulary. Thereafter LDA is run with 100 topics and LDA discovered document-, and word-topic distributions are provided to CTR. For CVAE we used the implementation provided by the authors<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>.</p>     <p>For the CNN based models (CNN-Kim and XML-CNN), we used rectified linear units as activation functions, and one-dimensional convolutional filters with window sizes of 2, 4, 8. The number of feature maps for each convolutional filter was 128. For XML-CNN the dropout rate was <em>p</em> = 0.5, and hidden units of the bottleneck layer was 512 as suggested by the authors [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>].</p>     <p>For the baseline BiGRU-2 and the proposed model, we set the number of neurons for the GRUs to 128, and number of clusters (<em>k</em>) to 100. A dropout layer with 0.3 dropout rate is used after the fully connected layer. To deal with the highly imbalanced class distribution, we use normalized class weights to weigh the sparse positive training examples more. All the deep learning models are implemented using Keras library<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> with Theano<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a> as the backend.</p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Results</h3>     </div>     </header>     <p>Table <a class="tbl" href="#tab2">2</a>, <a class="tbl" href="#tab3">3</a>, and <a class="tbl" href="#tab4">4</a> show the performance of different methods on the four datasets in terms of MRR, Recall@<em>M</em>, and NDCG@<em>M</em> respectively.</p>     <p>Firstly, we note that all the XMLC models outperform the widely used off-the-shelf recommendation algorithms comfortably in most cases. However the same does not hold true for the off-the-shelf text classifier as CNN-Kim&#x0027;s scores are not always better. This empirical proof works as a validation of our approach of posing cold start recommendation problem as an XMLC task.</p>     <p>Moreover, we observe that our model outperforms the baselines consistently in all datasets. We achieve a relative performance gain of <span class="inline-equation"><span class="tex">$4.5\%-21.7\%$</span>     </span> (depending on the dataset) in terms of MRR compared to current state-of-the-art for XMLC i.e., XML-CNN.</p>     <p>We find the performance of the models to be consistent in terms of both Recall, and nDCG@<em>M</em>. From the NDCG scores we conclude that our model is able to correctly identify interested users and places them near the top of the list in most cases. For <em>M</em> = 100, we achieve a relative performance gain of <span class="inline-equation"><span class="tex">$7.79\%-16.19\%$</span>     </span> in terms of NDCG compared to XML-CNN. We observe similar trends in case of recall, with a relative performance gain of <span class="inline-equation"><span class="tex">$3.23\%-15.39\%$</span>     </span>. We would like to mention that in our setting, the recall values at larger <em>M</em> values are equally important as the lower ones &#x2013; quite unlike the traditional case, where a recommended list of items are presented to every user. Since it is infeasible for a user to look through more than the first 5 &#x2212; 10 items, the objective is to have better recall, and NDCG scores for small <em>M</em> (e.g., 5 &#x2212; 10). However for a new item, we are trying to identify the set of interested users who would be notified individually. Typically the recommendation engines try to notify as many interested users as possible to ensure sufficient user-engagement. For this reason, we argue that our model would be more appropriate as it consistently achieves higher recall, and NDCG scores for large <em>M</em> values compared to the state-of-the-art for XMLC. Although CVAE uses both rating and textual content, we observe that it struggles to provide accurate recommendation in our scenario. It was reported to outperform other methods when it has seen the test item <em>at least</em> once [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. However in our case, the test item is <em>never</em> seen during training &#x2013; we believe this makes it challenging for CVAE to perform well.</p>     <p>In absolute terms, the performance of all the competing methods degrade drastically in case of the stackoverflow dataset because of extremely high sparsity (99.99%) and huge label space (&#x223C; 70<em>K</em>). However relatively speaking, our model fairs well compared to the others with better (in most cases) or very close scores (in few cases) in terms of all the metrics.</p>     <p>     <strong>Ablation Study:</strong> The choice of baselines allows us to do two ablation studies. Firstly, we observe that BiGRU encoding of text works much better compared to XML-CNN which uses CNN to encode the text. We believe that the long sequential nature of posts is better captured with a recurrent network rather than fixed length convolution filters. Finally, recall that the BiGRU-2 is primarily our model without the CSA component. This allows to us to an ablation study between our model variants with/without it. We observe that the attention mechanism achieves a relative performance improvement of upto 6.33% over the BiGRU-2 model in MRR, 3.40% in Recall@100, and 4.67% in NDCG@100 respectively. Moreover, the attention mechanism consistently scores better than BiGRU-2 for larger values of <em>M</em>. This study quantitatively validates the hypothesis of having the CSA component in our model.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Comparison of Mean Reciprocal Rank (MRR) of different methods for the four datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Dataset</th>        <th colspan="6" style="text-align:center;">Methods        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;">CVAE</th>        <th style="text-align:center;">CTR</th>        <th style="text-align:center;">CNN-Kim</th>        <th style="text-align:center;">XML-CNN</th>        <th style="text-align:center;">BiGRU-2</th>        <th style="text-align:center;">Our Model</th>       </tr> 						</thead> 						<tbody>       <tr>        <td style="text-align:left;">1.Epilepsy</td>        <td style="text-align:center;">0.159</td>        <td style="text-align:center;">0.443</td>        <td style="text-align:center;">0.536</td>        <td style="text-align:center;">0.551</td>        <td style="text-align:center;">0.631</td>        <td style="text-align:center;">        <strong>0.671</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">2.ALS</td>        <td style="text-align:center;">0.201</td>        <td style="text-align:center;">0.275</td>        <td style="text-align:center;">0.270</td>        <td style="text-align:center;">0.293</td>        <td style="text-align:center;">0.297</td>        <td style="text-align:center;">        <strong>0.306</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">3.Fibromyalgia</td>        <td style="text-align:center;">0.304</td>        <td style="text-align:center;">0.435</td>        <td style="text-align:center;">0.669</td>        <td style="text-align:center;">0.668</td>        <td style="text-align:center;">0.740</td>        <td style="text-align:center;">        <strong>0.773</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">4.Stackoverflow</td>        <td style="text-align:center;">0.003</td>        <td style="text-align:center;">0.032</td>        <td style="text-align:center;">0.025</td>        <td style="text-align:center;">0.029</td>        <td style="text-align:center;">0.047</td>        <td style="text-align:center;">        <strong>0.050</strong>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Comparison of Recall@M of different methods across four datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Dataset</th>        <th style="text-align:center;">Metric</th>        <th colspan="6" style="text-align:center;">Method        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">CVAE</th>        <th style="text-align:center;">CTR</th>        <th style="text-align:center;">CNN-Kim</th>        <th style="text-align:center;">XML-CNN</th>        <th style="text-align:center;">BiGRU-2</th>        <th style="text-align:center;">Our Model</th>       </tr> 						</thead> 						<tbody>       <tr>        <td style="text-align:left;">1. Epilepsy</td>        <td style="text-align:center;">recall@5</td>        <td style="text-align:center;">3.69</td>        <td style="text-align:center;">17.46</td>        <td style="text-align:center;">17.23</td>        <td style="text-align:center;">        <strong>22.76</strong>        </td>        <td style="text-align:center;">22.64</td>        <td style="text-align:center;">22.65</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@10</td>        <td style="text-align:center;">7.22</td>        <td style="text-align:center;">27.67</td>        <td style="text-align:center;">22.93</td>        <td style="text-align:center;">        <strong>34.67</strong>        </td>        <td style="text-align:center;">29.22</td>        <td style="text-align:center;">29.26</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@30</td>        <td style="text-align:center;">21.14</td>        <td style="text-align:center;">43.83</td>        <td style="text-align:center;">44.63</td>        <td style="text-align:center;">49.08</td>        <td style="text-align:center;">50.99</td>        <td style="text-align:center;">        <strong>51.21</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@50</td>        <td style="text-align:center;">29.62</td>        <td style="text-align:center;">50.86</td>        <td style="text-align:center;">52.45</td>        <td style="text-align:center;">53.69</td>        <td style="text-align:center;">59.47</td>        <td style="text-align:center;">        <strong>59.80</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@100</td>        <td style="text-align:center;">42.44</td>        <td style="text-align:center;">59.93</td>        <td style="text-align:center;">65.77</td>        <td style="text-align:center;">63.67</td>        <td style="text-align:center;">68.23</td>        <td style="text-align:center;">        <strong>69.37</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">2. ALS</td>        <td style="text-align:center;">recall@5</td>        <td style="text-align:center;">4.17</td>        <td style="text-align:center;">7.05</td>        <td style="text-align:center;">6.19</td>        <td style="text-align:center;">6.51</td>        <td style="text-align:center;">7.63</td>        <td style="text-align:center;">        <strong>9.23</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@10</td>        <td style="text-align:center;">7.07</td>        <td style="text-align:center;">12.08</td>        <td style="text-align:center;">10.09</td>        <td style="text-align:center;">11.44</td>        <td style="text-align:center;">        <strong>14.65</strong>        </td>        <td style="text-align:center;">13.89</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@30</td>        <td style="text-align:center;">17.04</td>        <td style="text-align:center;">25.00</td>        <td style="text-align:center;">22.15</td>        <td style="text-align:center;">23.56</td>        <td style="text-align:center;">30.18</td>        <td style="text-align:center;">        <strong>31.84</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@50</td>        <td style="text-align:center;">24.07</td>        <td style="text-align:center;">32.46</td>        <td style="text-align:center;">31.27</td>        <td style="text-align:center;">30.61</td>        <td style="text-align:center;">36.32</td>        <td style="text-align:center;">        <strong>36.55</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@100</td>        <td style="text-align:center;">35.77</td>        <td style="text-align:center;">44.14</td>        <td style="text-align:center;">43.82</td>        <td style="text-align:center;">43.14</td>        <td style="text-align:center;">48.14</td>        <td style="text-align:center;">        <strong>49.78</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">3. Fibromyalgia</td>        <td style="text-align:center;">recall@5</td>        <td style="text-align:center;">8.24</td>        <td style="text-align:center;">14.58</td>        <td style="text-align:center;">23.01</td>        <td style="text-align:center;">22.11</td>        <td style="text-align:center;">25.63</td>        <td style="text-align:center;">        <strong>25.97</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@10</td>        <td style="text-align:center;">14.93</td>        <td style="text-align:center;">27.18</td>        <td style="text-align:center;">34.77</td>        <td style="text-align:center;">33.88</td>        <td style="text-align:center;">35.18</td>        <td style="text-align:center;">        <strong>37.38</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@30</td>        <td style="text-align:center;">32.83</td>        <td style="text-align:center;">54.39</td>        <td style="text-align:center;">58.04</td>        <td style="text-align:center;">61.83</td>        <td style="text-align:center;">62.39</td>        <td style="text-align:center;">        <strong>63.06</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@50</td>        <td style="text-align:center;">42.43</td>        <td style="text-align:center;">63.91</td>        <td style="text-align:center;">67.83</td>        <td style="text-align:center;">68.92</td>        <td style="text-align:center;">69.17</td>        <td style="text-align:center;">        <strong>72.04</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@100</td>        <td style="text-align:center;">55.02</td>        <td style="text-align:center;">72.31</td>        <td style="text-align:center;">76.37</td>        <td style="text-align:center;">75.74</td>        <td style="text-align:center;">77.98</td>        <td style="text-align:center;">        <strong>78.19</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">4. Stackoverflow</td>        <td style="text-align:center;">recall@5</td>        <td style="text-align:center;">0.02</td>        <td style="text-align:center;">0.59</td>        <td style="text-align:center;">0.46</td>        <td style="text-align:center;">0.51</td>        <td style="text-align:center;">0.66</td>        <td style="text-align:center;">        <strong>0.86</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@10</td>        <td style="text-align:center;">0.06</td>        <td style="text-align:center;">1.14</td>        <td style="text-align:center;">0.73</td>        <td style="text-align:center;">0.97</td>        <td style="text-align:center;">1.15</td>        <td style="text-align:center;">        <strong>1.30</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@30</td>        <td style="text-align:center;">0.16</td>        <td style="text-align:center;">2.73</td>        <td style="text-align:center;">1.84</td>        <td style="text-align:center;">2.42</td>        <td style="text-align:center;">        <strong>2.94</strong>        </td>        <td style="text-align:center;">2.80</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@50</td>        <td style="text-align:center;">0.31</td>        <td style="text-align:center;">4.02</td>        <td style="text-align:center;">2.74</td>        <td style="text-align:center;">3.43</td>        <td style="text-align:center;">4.03</td>        <td style="text-align:center;">        <strong>4.11</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">recall@100</td>        <td style="text-align:center;">0.69</td>        <td style="text-align:center;">        <strong>6.36</strong>        </td>        <td style="text-align:center;">4.43</td>        <td style="text-align:center;">5.35</td>        <td style="text-align:center;">6.09</td>        <td style="text-align:center;">6.33</td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Comparison of NDCG@M of different methods across four datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Dataset</th>        <th style="text-align:center;">Metric</th>        <th colspan="6" style="text-align:center;">Method        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">CVAE</th>        <th style="text-align:center;">CTR</th>        <th style="text-align:center;">CNN-Kim</th>        <th style="text-align:center;">XML-CNN</th>        <th style="text-align:center;">BiGRU-2</th>        <th style="text-align:center;">Our Model</th>       </tr> 						</thead> 						<tbody>       <tr>        <td style="text-align:left;">1. Epilepsy</td>        <td style="text-align:center;">NDCG@5</td>        <td style="text-align:center;">3.80</td>        <td style="text-align:center;">19.44</td>        <td style="text-align:center;">19.52</td>        <td style="text-align:center;">25.80</td>        <td style="text-align:center;">27.80</td>        <td style="text-align:center;">        <strong>29.52</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@10</td>        <td style="text-align:center;">5.95</td>        <td style="text-align:center;">25.80</td>        <td style="text-align:center;">24.26</td>        <td style="text-align:center;">33.08</td>        <td style="text-align:center;">31.96</td>        <td style="text-align:center;">        <strong>33.72</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@30</td>        <td style="text-align:center;">12.26</td>        <td style="text-align:center;">33.38</td>        <td style="text-align:center;">34.10</td>        <td style="text-align:center;">39.91</td>        <td style="text-align:center;">41.78</td>        <td style="text-align:center;">        <strong>43.91</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@50</td>        <td style="text-align:center;">15.38</td>        <td style="text-align:center;">36.02</td>        <td style="text-align:center;">38.49</td>        <td style="text-align:center;">41.70</td>        <td style="text-align:center;">45.14</td>        <td style="text-align:center;">        <strong>47.01</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@100</td>        <td style="text-align:center;">19.50</td>        <td style="text-align:center;">38.97</td>        <td style="text-align:center;">42.18</td>        <td style="text-align:center;">44.88</td>        <td style="text-align:center;">47.93</td>        <td style="text-align:center;">        <strong>50.17</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">2. ALS</td>        <td style="text-align:center;">NDCG@5</td>        <td style="text-align:center;">5.28</td>        <td style="text-align:center;">8.59</td>        <td style="text-align:center;">8.02</td>        <td style="text-align:center;">8.21</td>        <td style="text-align:center;">9.16</td>        <td style="text-align:center;">        <strong>10.24</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@10</td>        <td style="text-align:center;">7.26</td>        <td style="text-align:center;">11.94</td>        <td style="text-align:center;">10.62</td>        <td style="text-align:center;">11.41</td>        <td style="text-align:center;">        <strong>13.71</strong>        </td>        <td style="text-align:center;">13.42</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@30</td>        <td style="text-align:center;">12.12</td>        <td style="text-align:center;">18.18</td>        <td style="text-align:center;">16.38</td>        <td style="text-align:center;">17.40</td>        <td style="text-align:center;">21.05</td>        <td style="text-align:center;">        <strong>22.49</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@50</td>        <td style="text-align:center;">14.90</td>        <td style="text-align:center;">21.08</td>        <td style="text-align:center;">19.88</td>        <td style="text-align:center;">20.13</td>        <td style="text-align:center;">23.54</td>        <td style="text-align:center;">        <strong>23.86</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@100</td>        <td style="text-align:center;">18.90</td>        <td style="text-align:center;">25.00</td>        <td style="text-align:center;">24.14</td>        <td style="text-align:center;">24.39</td>        <td style="text-align:center;">27.53</td>        <td style="text-align:center;">        <strong>28.34</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">3. Fibromyalgia</td>        <td style="text-align:center;">NDCG@5</td>        <td style="text-align:center;">10.29</td>        <td style="text-align:center;">17.27</td>        <td style="text-align:center;">28.97</td>        <td style="text-align:center;">28.57</td>        <td style="text-align:center;">32.38</td>        <td style="text-align:center;">        <strong>33.71</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@10</td>        <td style="text-align:center;">14.72</td>        <td style="text-align:center;">25.43</td>        <td style="text-align:center;">33.67</td>        <td style="text-align:center;">36.32</td>        <td style="text-align:center;">38.44</td>        <td style="text-align:center;">        <strong>41.05</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@30</td>        <td style="text-align:center;">23.53</td>        <td style="text-align:center;">38.82</td>        <td style="text-align:center;">48.23</td>        <td style="text-align:center;">50.19</td>        <td style="text-align:center;">51.09</td>        <td style="text-align:center;">        <strong>54.03</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@50</td>        <td style="text-align:center;">27.29</td>        <td style="text-align:center;">42.50</td>        <td style="text-align:center;">52.04</td>        <td style="text-align:center;">52.98</td>        <td style="text-align:center;">54.53</td>        <td style="text-align:center;">        <strong>57.36</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@100</td>        <td style="text-align:center;">31.46</td>        <td style="text-align:center;">45.36</td>        <td style="text-align:center;">54.95</td>        <td style="text-align:center;">55.32</td>        <td style="text-align:center;">57.52</td>        <td style="text-align:center;">        <strong>59.63</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">4. Stackoverflow</td>        <td style="text-align:center;">NDCG@5</td>        <td style="text-align:center;">0.02</td>        <td style="text-align:center;">0.64</td>        <td style="text-align:center;">0.54</td>        <td style="text-align:center;">0.59</td>        <td style="text-align:center;">1.01</td>        <td style="text-align:center;">        <strong>1.22</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@10</td>        <td style="text-align:center;">0.04</td>        <td style="text-align:center;">0.98</td>        <td style="text-align:center;">0.70</td>        <td style="text-align:center;">0.87</td>        <td style="text-align:center;">1.31</td>        <td style="text-align:center;">        <strong>1.48</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@30</td>        <td style="text-align:center;">0.09</td>        <td style="text-align:center;">1.68</td>        <td style="text-align:center;">1.19</td>        <td style="text-align:center;">1.52</td>        <td style="text-align:center;">2.09</td>        <td style="text-align:center;">        <strong>2.12</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@50</td>        <td style="text-align:center;">0.14</td>        <td style="text-align:center;">2.14</td>        <td style="text-align:center;">1.51</td>        <td style="text-align:center;">1.88</td>        <td style="text-align:center;">2.47</td>        <td style="text-align:center;">        <strong>2.59</strong>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">NDCG@100</td>        <td style="text-align:center;">0.26</td>        <td style="text-align:center;">2.86</td>        <td style="text-align:center;">2.03</td>        <td style="text-align:center;">2.46</td>        <td style="text-align:center;">3.11</td>        <td style="text-align:center;">        <strong>3.27</strong>        </td>       </tr>      </tbody>     </table>     </div>    </section>   </section>   <section id="sec-18">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Related Work</h2>     </div>    </header>    <p>     <strong>Recommendation System:</strong> Collaborative Filtering (CF) based approaches have been the most popular recommendation systems in the past decade. CF based approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] suffer when the data is sparse (i.e. not much interaction history available for a user or item) and do not work for cold start (i.e. no interaction history available for a new item or user).</p>    <p>In order to make recommendations for a new item, in absence of any interaction history, the recommender system needs to make use of additional information such as item content or metadata. Similar to our setting, the authors in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] tackle the problem of recommending incoming news articles for users to comment. However, they do not use the whole article content but only use the tags associated with a document. Collaborative Topic Regression (CTR) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] proposes an elegant method of using the textual content for recommendation. It is a probabilistic graphical model that integrates a topic model, latent Dirichlet allocation (LDA) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] for modeling contents of a document, and uses the LDA-discovered topics while doing the regression later with probabilistic matrix factorization (PMF) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>]. Afterwards in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] the authors extend CTR by incorporating explicitly mentioned user interests in order to handle cold start recommendation for new users as well.</p>    <p>Some recent works [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] have explored deep learning models for recommendation based on item content. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] the authors use CNNs to model the acoustic signals present in a music video in order to predict the latent factors to be used by a CF model to make recommendation. Similar to CTR, Collaborative Deep Learning (CDL) has been proposed that uses stacked denoising autoencoders (SDAE) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] for representation learning of the textual content, and collaborative filtering for the rating matrix. To eliminate the bag-of-words assumption of CDL, Collaborative Recurrent Autoencoder (CRAE) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] is proposed to model the sequence information in item content. Instead of using a denoising autoencoder, CVAE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] uses a Bayesian generative approach for the content representation and reportedly outperforms the other methods. Recently, for cold-start recommendation, dropout is applied to input mini-batches, for training Deep Neural Networks to generalize for a missing input [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>].</p>    <p>     <strong>Extreme Multi-label Classification:</strong> Embedding based approaches have proved to be popular for handling the extreme multi-label learning problem by reducing the effective number of labels. Generally, they assume that the label matrix is low-rank, and project label vectors into a lower dimensional subspace. Hence, instead of predicting the original high-dimensional label vector for each instance, they reliably train for prediction of embedded label vectors, and then employ a decompression algorithm to map the embedded label vectors back to the original label space. Various compression and decompression techniques have been proposed in the literature to achieve this [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>].</p>    <p>In order to avoid the loss of information during the compression phase of the embedding based approaches, tree-based methods have been proposed that try to partition the label space similar to a decision tree. It recursively partitions the huge label space in subtrees until only a few labels are left at each leaf node. A base classifier at each leaf node then focuses on only the active labels in the node. The LPSR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] method focuses on learning a hierarchy over a base classifier or ranker starting with a base multi-label classifier for the entire label set - this becomes computationally expensive to train if a discriminative classifier (e.g. SVM) is used. Instead of using a base classifier, MLRF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] uses an ensemble of randomized trees with a modified Gini index for partitioning the nodes. In FastXML [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] an NDCG-based objective is used at each node of the hierarchy for optimization.</p>    <p>Despite the success of deep learning in many fields, it has not been explored much for XMLC tasks. Recently, a CNN based approach (XML-CNN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]) has been proposed, which uses convolutional layers for text representation and a feed forward layer acting as a bottleneck layer for scalability. This has been shown to outperform both embedding based and tree based approaches for XMLC, therefore we chose this method for comparison in our experiments.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Rahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. 2013. Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages. In <em>      <em>Proc. of WWW.</em></em> ACM, 13&#x2013;24.</li>     <li id="BibPLXBIB0002" label="[2]">Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. <em>      <em>arXiv preprint arXiv:1409.0473</em>     </em>(2014).</li>     <li id="BibPLXBIB0003" label="[3]">Krishnakumar Balasubramanian and Guy Lebanon. 2012. The landmark selection method for multiple output prediction. In <em>      <em>Proc. of ICML.</em></em> Omnipress, 283&#x2013;290.</li>     <li id="BibPLXBIB0004" label="[4]">Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. <em>      <em>IEEE transactions on neural networks</em>     </em>5, 2 (1994), 157&#x2013;166.</li>     <li id="BibPLXBIB0005" label="[5]">Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. 2015. Sparse local embeddings for extreme multi-label classification. In <em>      <em>Proc. of NIPS.</em></em> 730&#x2013;738.</li>     <li id="BibPLXBIB0006" label="[6]">David&#x00A0;M Blei, Andrew&#x00A0;Y Ng, and Michael&#x00A0;I Jordan. 2003. Latent dirichlet allocation. <em>      <em>Journal of machine Learning research</em>     </em>3, Jan (2003), 993&#x2013;1022.</li>     <li id="BibPLXBIB0007" label="[7]">Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. <em>      <em>Proc. of NIPS Deep Learning and Representation Learning Workshop</em>     </em> (2014).</li>     <li id="BibPLXBIB0008" label="[8]">Moustapha&#x00A0;M Cisse, Nicolas Usunier, Thierry Artieres, and Patrick Gallinari. 2013. Robust bloom filters for large multilabel classification tasks. In <em>      <em>Proc. of NIPS.</em></em> 1851&#x2013;1859.</li>     <li id="BibPLXBIB0009" label="[9]">Kishaloy Halder, Min-Yen Kan, and Kazunari Sugiyama. 2017. Health Forum Thread Recommendation Using an Interest Aware Topic Model. In <em>      <em>Proc. of CIKM.</em></em> ACM, 1589&#x2013;1598.</li>     <li id="BibPLXBIB0010" label="[10]">Daniel&#x00A0;J Hsu, Sham&#x00A0;M Kakade, John Langford, and Tong Zhang. 2009. Multi-label prediction via compressed sensing. In <em>      <em>Proc. of NIPS.</em></em> 772&#x2013;780.</li>     <li id="BibPLXBIB0011" label="[11]">Ashish Kapoor, Raajay Viswanathan, and Prateek Jain. 2012. Multilabel classification using bayesian compressed sensing. In <em>      <em>Proc. of NIPS.</em></em> 2645&#x2013;2653.</li>     <li id="BibPLXBIB0012" label="[12]">Yoon Kim. 2014. Convolutional neural networks for sentence classification. In <em>      <em>Proc. of EMNLP.</em></em></li>     <li id="BibPLXBIB0013" label="[13]">Diederik&#x00A0;P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. <em>      <em>CoRR</em>     </em>abs/1412.6980(2014).</li>     <li id="BibPLXBIB0014" label="[14]">Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In <em>      <em>Proc. of SIGKDD.</em></em> ACM, 426&#x2013;434.</li>     <li id="BibPLXBIB0015" label="[15]">Daniel&#x00A0;D Lee and H&#x00A0;Sebastian Seung. 2001. Algorithms for non-negative matrix factorization. In <em>      <em>Proc. of NIPS.</em></em> 556&#x2013;562.</li>     <li id="BibPLXBIB0016" label="[16]">Xiaopeng Li and James She. 2017. Collaborative variational autoencoder for recommender systems. In <em>      <em>Proc. of SIGKDD.</em></em> ACM, 305&#x2013;314.</li>     <li id="BibPLXBIB0017" label="[17]">Zhouhan Lin, Minwei Feng, Cicero Nogueira&#x00A0;dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. <em>      <em>ICLR</em>     </em> (2017).</li>     <li id="BibPLXBIB0018" label="[18]">Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. 2017. Deep Learning for Extreme Multi-label Text Classification. In <em>      <em>Proc. of SIGIR.</em></em> ACM, 115&#x2013;124.</li>     <li id="BibPLXBIB0019" label="[19]">Andriy Mnih and Ruslan&#x00A0;R Salakhutdinov. 2008. Probabilistic matrix factorization. In <em>      <em>Proc. of NIPS.</em></em> 1257&#x2013;1264.</li>     <li id="BibPLXBIB0020" label="[20]">Jeffrey Pennington, Richard Socher, and Christopher&#x00A0;D Manning. 2014. Glove: Global vectors for word representation.. In <em>      <em>Proc. of EMNLP.</em></em></li>     <li id="BibPLXBIB0021" label="[21]">Yashoteja Prabhu and Manik Varma. 2014. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. In <em>      <em>Proc. of SIGKDD.</em></em> ACM, 263&#x2013;272.</li>     <li id="BibPLXBIB0022" label="[22]">Ruslan Salakhutdinov and Andriy Mnih. 2008. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In <em>      <em>Proc. of ICML.</em></em> 880&#x2013;887.</li>     <li id="BibPLXBIB0023" label="[23]">Erez Shmueli, Amit Kagian, Yehuda Koren, and Ronny Lempel. 2012. Care to Comment?: Recommendations for Commenting on News Stories. In <em>      <em>Proc. of WWW.</em></em> ACM, 429&#x2013;438.</li>     <li id="BibPLXBIB0024" label="[24]">Aaron Van&#x00A0;den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In <em>      <em>Proc. of NIPS.</em></em> 2643&#x2013;2651.</li>     <li id="BibPLXBIB0025" label="[25]">Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. <em>      <em>Journal of Machine Learning Research</em>     </em>11, Dec (2010), 3371&#x2013;3408.</li>     <li id="BibPLXBIB0026" label="[26]">Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Addressing Cold Start in Recommender Systems. In <em>      <em>Proc. of NIPS.</em></em> 4964&#x2013;4973.</li>     <li id="BibPLXBIB0027" label="[27]">Chong Wang and David&#x00A0;M Blei. 2011. Collaborative topic modeling for recommending scientific articles. In <em>      <em>Proc of SIGKDD.</em></em> ACM, 448&#x2013;456.</li>     <li id="BibPLXBIB0028" label="[28]">Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative Deep Learning for Recommender Systems. In <em>      <em>Proc. of SIGKDD.</em></em> ACM, 1235&#x2013;1244.</li>     <li id="BibPLXBIB0029" label="[29]">Hao Wang, SHI Xingjian, and Dit-Yan Yeung. 2016. Collaborative recurrent autoencoder: recommend while learning to fill in the blanks. In <em>      <em>Proc. of NIPS.</em></em> 415&#x2013;423.</li>     <li id="BibPLXBIB0030" label="[30]">Xinxi Wang and Ye Wang. 2014. Improving Content-based and Hybrid Music Recommendation Using Deep Learning. In <em>      <em>Proceedings of the 22Nd ACM International Conference on Multimedia.</em></em> 627&#x2013;636.</li>     <li id="BibPLXBIB0031" label="[31]">Jason Weston, Ameesh Makadia, and Hector Yee. 2013. Label partitioning for sublinear ranking. In <em>      <em>Proc. of ICML.</em></em> 181&#x2013;189.</li>     <li id="BibPLXBIB0032" label="[32]">Bin Xu, Jiajun Bu, Chun Chen, and Deng Cai. 2012. An Exploration of Improving Collaborative Recommender Systems via User-item Subgroups. In <em>      <em>Proc. of WWW.</em></em> ACM, 21&#x2013;30.</li>     <li id="BibPLXBIB0033" label="[33]">Yi Zhang and Jeff Schneider. 2011. Multi-label output codes using canonical correlation analysis. In <em>      <em>Proc. of AISTATS.</em></em> 873&#x2013;882.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://www.healthboards.com/boards/">https://www.healthboards.com/boards/</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://www.coursera.org">https://www.coursera.org</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://www.stackoverflow.com">https://www.stackoverflow.com</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break"     href="https://www.kaggle.com/stackoverflow/stacksample/data">https://www.kaggle.com/stackoverflow/stacksample/data</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break"     href="https://github.com/eelxpeng/CollaborativeVAE">https://github.com/eelxpeng/CollaborativeVAE</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class="link-inline force-break" href="https://keras.io/">https://keras.io/</a>   </p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break" href="https://github.com/Theano/Theano">https://github.com/Theano/Theano</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191659">https://doi.org/10.1145/3184558.3191659</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
