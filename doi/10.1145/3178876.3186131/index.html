<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Geographical Feature Extraction for Entities in Location-based Social Networks</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Geographical Feature Extraction for Entities in Location-based Social Networks</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Daizong</span>      <span class="surName">Ding</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, China     </div>     <div class="author">     <span class="givenName">Mi</span>      <span class="surName">Zhang</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, China     </div>     <div class="author">     <span class="givenName">Xudong</span>      <span class="surName">Pan</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, China     </div>     <div class="author">     <span class="givenName">Duocai</span>      <span class="surName">Wu</span>,     Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, China     </div>     <div class="author">     <span class="givenName">Pearl</span>      <span class="surName">Pu</span>,     Human Computer Interaction Group, School of Computer and Communication Sciences, Swiss Federal Institude of Technology (EFPL), Switzerland, <a href="mailto:17110240010,mi_zhang,14302010024,14302010040@fudan.edu.cn">17110240010,mi_zhang,14302010024,14302010040@fudan.edu.cn</a>, <a href="mailto:pearl.pu@efpl.ch">pearl.pu@efpl.ch</a>     </div>                         </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186131" target="_blank">https://doi.org/10.1145/3178876.3186131</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Location-based embedding is a fundamental problem to solve in location-based social network (LBSN). In this paper, we propose a geographical convolutional neural tensor network (GeoCNTN) as a generic embedding model. GeoCNTN first takes the raw location data and extracts from it a well-conditioned representation by our proposed Geo-CMeans algorithm. We then use a convolutional neural network (CNN) and an embedding structure to extract individual latent structural patterns from the preprocessed data. Finally, we apply a neural tensor network (NTN) to craft the implicitly related features we have obtained into a unified geographical feature.</small>     </p>     <p>     <small>The advantages of our GeoCNTN mainly come from its novel neural network structure, which intrinsically offers a mechanism to extract latent structural features from the geographical data, as well as its wide applicability in various LBSN-related tasks. From two case studies, i.e. link prediction and entity classification in user-group LBSN, we evaluate the embedding efficacy of our model. Results show that GeoCNTN significantly performs better on at least two tasks, with improvement by 9% w.r.t. NDCG and 11% w.r.t. F1 score respectively, using the Meetup-USA dataset.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Social and professional topics </strong>&#x2192; <strong>Geographic characteristics;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Location-based Social Networks</small>, </span>     <span class="keyword">      <small> Feature Embedding</small>, </span>     <span class="keyword">      <small> Deep Learning</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Daizong Ding, Mi Zhang, Xudong Pan, Duocai Wu, and Pearl Pu. 2018. Geographical Feature Extraction for Entities in Location-based Social Networks. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186131" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186131</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Location based social network (LBSN) continues to gain popularity. In 2017, there are over 55 million monthly active users and over 12 billion<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> cumulative check-ins in Foursquare<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> alone. Yet there is still more room for these networks grow. LBSNs provide users with a multitude of information and services around their current locations, thus enriching their off-line daily lives. For example, Yelp provides restaurant advice based on a user&#x0027;s daily routine as well as his/her friends&#x2019; check-ins. Examples of LBSN include Meetup, Yelp, and Facebook<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. In such networks, various kinds of heterogeneous <em>entites</em>, i.e. users, groups, events, etc., interact at a given location in near-real time (Fig. <a class="fig" href="#fig1">1</a>). For this study, we take Meetup, a popular off-line group meeting facilitator website, as an illustrative example where users seemingly prefer to participate in meet-up groups not far away from their current residence. This behavioral pattern resonates with Tobler&#x0027;s first law of geography [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] that says near things are more related than distant things.</p>    <p>Traditional tasks in social networks, such as link prediction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>] and entity classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], are still the focal points in LBSN, while the difference lies in the additional geographical information brought by its innate setting. How to extract useful location-based features from the original geographical information largely determines the final performance of a specific learning model. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Location-based social networks.</span>     </div>     </figure>    </p>    <p>From our perspective, the following illustrative examples will present two indispensable aspects of a good geographical feature:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">Users living in Shanghai are considered to be more similar to each other than those living in Beijing, e.g., their preferences for spicy or sweet food. Inspired by Tobler&#x0027;s law, methods such as those presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>] exploit such kind of similarity via the assumption that entities with intersecting regions of activities should have more similar features. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] takes advantage of the geographical neighborhood characteristics for location recommendation.<br/></li>     <li id="list2" label="&#x2022;">Users exhibit different location patterns even though they all live in Shanghai. As is shown on the right part of Fig. <a class="fig" href="#fig1">1</a>, user A on the left has a more concentrated location pattern locally than user B&#x0027;s on the right. It seems A prefers to stay home most of the time while B enjoys hanging out in diverse locations. It inspires us to obtain more expressive features via considering the characteristics in their location patterns.<br/></li>    </ul>    <p>In the former example, geographical distance still has a considerable impact on entity&#x0027;s behavior in LBSN. We refer to such a factor as a <em>global</em> factor. Moreover, the latter case illustrates that the difference inside location patterns locally may also play an essential role in determining entity&#x0027;s behavior, which is referred to as a <em>local</em> factor in this work. To our knowledge, most of the previous works such as [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>] are basically focused on exploiting the global geographical information, while, from our perspective, it is critical to combine the local information from location patterns with the global information as a complement.</p>    <p>A similar consideration has been introduced independently in several fields such as image processing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] and time series analysis [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>]. To the best of our knowledge, methods for combining global and local geographical information have not ever been proposed for location-based social networks, probably due to the difficulty of obtaining a well-conditioned representation as input from the raw location data, which is inherently sparse.</p>    <p>In this paper, we present a location-based embedding model called GeoCNTN, which allows us to obtain a unified feature from raw locations for each entity by crafting both global and local geographical information.</p>    <p>A brief description of our proposed method is presented here. Given a set of locations defined by their longitude and latitude for each entity, we first apply our fuzzy-clustering method (Geo-CMeans) on the curved surface of the earth. With fuzziness, we are able to represent <em>areas</em> where local patterns can be revealed without <em>fractures</em>. We then divide the global geographical coordinate system into grids, obtain the indices of grids where cluster centers are located, and reformulate each grid ID into a corresponding one-hot vector (Fig. <a class="fig" href="#fig3">3</a>b) as a global representation of locations. Simultaneously, matrices that represent the occurrences of locations in the gridded domain of each truncated cluster (Fig. <a class="fig" href="#fig3">3</a>c) are obtained as the corresponding representation for each local pattern. We then apply a 3-layer CNN for feature extraction from local patterns, which would otherwise suffer from extreme sparsity without our preprocessing procedures, and project global representations into a vector form as a global feature via embedding the geographical grids into vector space. In the final step, we combine the global and local features into a unified form with a neural tensor network (NTN) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>], which is well-known for its capability to capture higher order correlations among several implicitly related inputs.</p>    <p>Our contributions are three-fold:</p>    <ul class="list-no-style">     <li id="list3" label="&#x2022;">We propose a neural network based embedding structure called GeoCNTN, which adopts embedding table [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] and a convolutional neural network for extracting features from global and local geographical information, respectively. With application of neural tensor network, our model are able to craft the global and local features together, capturing their innate correlations.<br/></li>     <li id="list4" label="&#x2022;">We develop a curvature-sensitive fuzzy clustering method called Geo-CMeans to obtain well-defined global and local representations from the original geographical data.<br/></li>     <li id="list5" label="&#x2022;">Our generic model has a strong applicability for a wide range of LBSN-related tasks.<br/></li>    </ul>    <p>As a validation for our feature extraction model, we apply our model to both link prediction and entity classification tasks in LBSN. Compared with the state-of-art, our model can perform much better with a net improvement by 9% w.r.t NDCG in the former task and by 11% w.r.t F1 score in the latter one. We have also validated the interpretablility of the obtained features via a series of visualizations.</p>    <p>The rest of the paper is organized as follows. Section 2 describes the related work. In Section 3, we present our approach for obtaining well-conditioned representations of location patterns with our geographical fuzzy-clustering method, Geo-CMeans. In Section 4, we propose our GeoCNTN model for obtaining location-based embedding from global and local representations that we have obtained in Section 3. In Section 5, we present several applications with our generic model and conduct a series of experiments and visualizations. Section 6 concludes our work.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Discovering location patterns in LBSN.</h3>     </div>     </header>     <p>Location information is innate in LBSN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>]. However, existing models often simplify a location with an integer ID rather than explicitly exploiting the geographical information, e.g., common places between entities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>]. These methods are unable to analyze the influence of geographical relatedness, which means they in essence could not recognize location pattern for entities.</p>     <p>In fact, location pattern recognition has been a popular topic in recent years. Existing methods can be classified into three categories. The first kind is based on rules <span class="inline-equation"><span class="tex">$\grave{a}$</span>     </span> priori. Methods such as extracting different statistical features in LBSN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], e.g., entropy of places, model user&#x0027;s movement patterns based on the assumption that users may stay around their home and work places [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. This kind of methods can only fit specific datasets that satisfy their prior assumptions, making them relatively weak for generalization. The second kind of methods is based on modeling location patterns with a specific probabilistic distribution on pairwise distances [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] or a two-dimensional Gaussian for locations with coordinate representations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. These methods may fail to extract personalized visiting pattern of their entities in LBSN because of a common probabilistic modeling for each user. The third kind argues that patterns of each user can be different [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>] and should not be modeled as a common distribution, which leads to the application of the kernel density estimator method (KDE) to model the distribution of pairwise distance with a much more powerful extension into 2-dimension [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>]. However, this family of methods may be overly complex and over-fitting can be a problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>].</p>     <p>As for the clustering of location patterns on a map, K-Means has been proposed to partition user&#x0027;s locations into several clusters [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. However, as pointed out by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>], hard-clustering method like K-Means may be sensitive to noise points. On the contrary, density-based clustering (fuzzy clustering) can remove noise points by filtering out low probability points under some thresholds. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] proposes to use multi-center Gaussian model to cluster locations into different areas with soft boundaries. A fuzzy clustering algorithm that is able to find center and radius of clusters for users was proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. However these methods did not take the curvature of the earth surface into account, which may led to the imprecision problem with a large geographical range setting.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Neural Network Architecture</h3>     </div>     </header>     <p>Recently, neural networks have achieved significant success in media processing tasks, such as image labeling, speech recognition [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>], as well as obtaining useful embeddings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. Furthermore, the use of convolutional neural network has been shown to perform much better than most traditional methods in several tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>].</p>     <p>A previously proposed architecture [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] is able to combine global and local information in time series analysis with a CNN for local pattern and long short term memory for global tendency. However, as far as we know, similar consideration has not been attempted for location pattern analysis, probably due to a lack of well-conditioned representations of both global and local location information. For example, a rough preprocessing may bring extremely sparse representations, which is unlikely for CNN to extract any useful features [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. More specially, several recent studies have already found that neural network structures are very effective in discovering 1-dimensional location pattern. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] uses recurrent neural network to exploit temporal information of check-ins. However, to our knowledge, work using neural network architecture for extracting feature for entities in LBSN directly from 2-dimensional geographical information has not yet been proposed, unlike those with a similar consideration found in natural language processing (NLP) and image processing.</p>    </section>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Data Representation</h2>     </div>    </header>    <p>As pointed out by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>], geographical distributions of locations in LBSN tend to have the following characteristics: <strong>(1)</strong> Visited locations of a certain entity tend to cluster in several implicit centers. <strong>(2)</strong> Locations away from any of these cluster centers can be considered as noises.</p>    <p>From our perspective, the <em>noise</em> locations could be considered negligible in our context, not only because of their relatively loose connections to each cluster, but also their lack of representativeness w.r.t. the local location distribution patterns. Intuitively, these singular points highly likely exist at the margin of clusters. Such an approximation thus has rather slight effects on our analysis of global and local location distribution patterns for a given entity.</p>    <p>Based on these prior observations, we present an auxiliary concept called <em>Area</em> in LBSN, which plays an indispensable role throughout the discussion of our proposed model.</p>    <p>     <div class="definition" id="enc1">     <Label>Definition 3.1.</Label>     <p> An <em>Area</em> is a truncated cluster representing the locations of a given entity in LBSN.</p>     </div>    </p>    <p>For a better understanding our <em>area</em> concept, consider a businessman who regularly travels to several cities. Although he makes abrupt visits while he is away, these visits may have little contributions to his global mobility distributions nor his local patterns in the city where he resides. We may thus consider each of these cities for business or every-day life an <em>area</em> related with him, except his unexpected adventures.</p>    <p>As a by-product of our definition, the concept of an <em>area</em> spontaneously separates information contained in the original set of locations into global and local information. Based on that separation, well-formed representations can be constructed rather naturally by globally <em>positioning</em> and locally <em>zooming</em>, which is the focal point of 3.2, 3.3.</p>    <p>From now on, the primary problem lies in how to detect a number of <em>areas</em> of a given entity without introducing unnecessary fractures of local patterns, while at the same time, filtering out negligible noise locations. As a strong candidate solution for fuzzy clustering on a curved plane, we develop a new algorithm called Geo-CMeans on the surface of the earth introduced by a large geographical range setting. Before an overall discussion of our method, we define the nomenclature. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Geo-CMeans algorithm on curved plane.</span>     </div>     </figure>     <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Obtain global and local data representation of an <em>area</em>.</span>     </div>     </figure>    </p>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Notations</h3>     </div>     </header>     <p>In the context of location-based social networks, the set of all entities is denoted as <em>E</em>. An entity <em>e</em> &#x2208; <em>E</em> in a general social network should be augmented with additional geographical information, denoted by a set of visited locations <span class="inline-equation"><span class="tex">$L_e = \lbrace L_e^{i}\rbrace _{i=1}^{n_e}$</span>     </span>, where <span class="inline-equation"><span class="tex">$L_e^{i} \doteq (\lambda _{e}^{i}, \phi _{e}^{i})$</span>     </span> denotes the longitude and latitude of a certain visited location. Moreover, let [N] denote the set <span class="inline-equation"><span class="tex">$\lbrace 1 \ldots N\rbrace$</span>     </span>, <em>C</em> denote the number of clusters, and a representation with footnote <em>e</em> means it belongs to entity <em>e</em>. <span class="inline-equation"><span class="tex">$A_e^{k}$</span>     </span> denotes a fuzzy cluster with <span class="inline-equation"><span class="tex">$p_{A_e^{k}}$</span>     </span> as its induced probabilistic distribution function (p.d.f). <span class="inline-equation"><span class="tex">$\tilde{A}_e^{k}$</span>     </span> is a truncated cluster from <span class="inline-equation"><span class="tex">$A_e^{k}$</span>     </span>, as we will call it <em>area</em> later. <span class="inline-equation"><span class="tex">$\tilde{I}_e^{k}$</span>     </span> denotes the one-hot vector obtained from <em>area</em>      <span class="inline-equation"><span class="tex">$\tilde{A}_e^{k}$</span>     </span>, while <span class="inline-equation"><span class="tex">$P_e^{k}$</span>     </span> is corresponding local pattern in matrix form. <em>GF<sub>e</sub>     </em> and <em>LF<sub>e</sub>     </em> respectively denote the global and local feature, while <em>F<sub>e</sub>     </em> is a unified feature learned by our model.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Geo-CMeans Clustering for Area Detection</h3>     </div>     </header>     <p>In this work, in order to detect the set of areas <span class="inline-equation"><span class="tex">$\lbrace \tilde{A}_e^{k}\rbrace _{k=1}^{C}$</span>     </span> for an entity <em>e</em>, we introduce a geographical fuzzy C-Means algorithm (Geo-CMeans) on the basis of the classical fuzzy clustering method C-Means [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>]. As we know, a hard clustering methods, such as K-Means, tends to give rigid boundaries among a set of points, which may make otherwise whole visiting pattern <em>fractured</em> in our context. An illustrative example occurs when we apply K-Means with a large k to a small number of concentrated locations. It will then partition the pattern into several pieces, which makes the position of the center lost. With a fuzzy clustering method, we can innately avoid this potential problem.</p>     <p>From another aspect, distance between points plays a critical role for any clustering methods. As the underlying geographical range gets larger, the geographical distance between locations cannot be calculated with an original Euclidean distance formula. Instead, we propose to use a common approximation of the intrinsic distance formula (Eq. <a class="eqn" href="#eq2">3</a>) over the earth. An illustrative result of our algorithm can be found in Fig. <a class="fig" href="#fig2">2</a>.</p>     <p>For a certain entity <em>e</em>, its location set is <em>L<sub>e</sub>     </em> containing <em>n<sub>e</sub>     </em> points, and a generic location <em>x<sub>i</sub>     </em> = (<em>&#x03BB;<sub>i</sub>     </em>, <em>&#x03D5;<sub>i</sub>     </em>). Suppose we want to find <em>C</em> areas for this entity, where each area has its center defined as <em>&#x03BC;<sub>j</sub>     </em> = (<em>&#x03BB;<sub>j</sub>     </em>, <em>&#x03D5;<sub>j</sub>     </em>), an extended loss function relative to Fuzzy C-Means can be formulated as</p>     <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} 					 \min_{p, \mu} \sum_{i=1}^{n_e}\sum_{j=1}^{C} p_{ij}^{\epsilon}\cdot d(x_i, \mu_j),\text{ } s.t.\text{ }\sum_{j=1}^{C}p_{ij}=1 \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div>     <p>where <span class="inline-equation"><span class="tex">$p_{ij}$</span></span> represents the probability that point <span class="inline-equation"><span class="tex">$x_i$</span></span> belongs to area <em>j</em>, <span class="inline-equation"><span class="tex">$\mu_j$</span></span> the center of area <span class="inline-equation"><span class="tex">$j$</span></span> , and <span class="inline-equation"><span class="tex">$\epsilon\in(1, +\infty)$</span></span> (used for preventing degenerate solutions during optimization). To solve this optimization problem, we adopt Lagrange multiplier method with the Lagrangian defined as     <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation}\mathcal{L} = \sum_{i=1}^{n_e}\sum_{j=1}^{C} p_{ij}^{\epsilon}\cdot d(x_i, \mu_j) + \sum_{i=1}^{n_e} \eta_i (\sum_{j=1}^{C}p_{ij} - 1) \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>where <span class="inline-equation"><span class="tex">$\eta_i$</span></span> is the Lagrange multiplier. With an approximate intrinsic distance over sphere <span class="inline-equation"><span class="tex">$d(x_i, x_j)$</span></span> defined as     <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} d(x_i, x_j) = R\text{ }\sqrt []{\big [(\phi _i-\phi _j)^2 + (\lambda _i - \lambda _j)^2\cdot \cos ^2(\frac{\phi _i + \phi _j}{2}) \big ]} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where R is a positive constant independent of the choice of <em>i</em>, <em>j</em>, we are able to use the alternative direction multiply method (ADMM) to minimize <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>     </span>.</p>     <p>After steps of algebraic manipulation, we obtain the updating rules in closed forms for <em>&#x03BB;</em>, <em>p</em> as <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p_{ij} = \Big [\sum _{k=1}^{C}\big (\frac{d(x_i, c_j)}{d(x_i, c_k)}\big)^{\frac{1}{m-1}} \Big ]^{-1} , \lambda _i = \frac{\sum _{j=1}^{C}p_{ij}^{\epsilon }\frac{R}{d(x_i, c_j)}\cdot \lambda _j}{\sum _{j=1}^{C}p_{ij}^{\epsilon }\frac{R}{d(x_i, c_j)}} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> while for <em>&#x03D5;<sub>j</sub>     </em>, stationary point condition is written as <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} h(\phi _j^{o}) = \sum _{i=1}^{n_e}p_{ij}^{\epsilon }\frac{R}{2d(x_i, c_j)}\cdot \big [2(\phi _i - \phi _j^{o}) + \frac{1}{2}\sin (\phi _i + \phi _j^{o}) \big ] = 0 \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>     </p>     <p>Without an explicit closed form, we apply the Newton method to solve this optimization problem with the second-order derivative written as <div class="table-responsive" id="Xeq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} h^{\prime }(\phi _j^{o})=\sum _{i=1}^{n_e}p_{ij}^{\epsilon }\frac{R}{2d(x_i, c_j)}\cdot \big [-2+\frac{1}{2}\cos (\phi _i + \phi _j^{o}) \big ] \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div>     </p>     <p>As a final comment, in order to accelerate the convergence of optimization, we set the initial value of <span class="inline-equation"><span class="tex">$\phi _j^{o}$</span>     </span> as the average of {<em>&#x03D5;<sub>i</sub>     </em>} from the observations {<em>x<sub>i</sub>     </em>} as <span class="inline-equation"><span class="tex">$\tilde{\phi }_j^{o} = 1 / n_e \sum _{k=1}^{n_e}\phi _k$</span>     </span>. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Convolutional Tensor Network for crafting global and local geographical information.</span>      </div>     </figure>     <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig5.jpg" class="img-responsive" alt="" longdesc=""/>     </figure>     </p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Positioning for Global Grid Representation</h3>     </div>     </header>     <p>Each cluster <span class="inline-equation"><span class="tex">$\lbrace A_e^{k}\rbrace _{k=1}^{C}$</span>     </span> we have obtained (Fig. <a class="fig" href="#fig2">2</a>) can thus be represented by each cluster center in the geographical maps, denoted by <span class="inline-equation"><span class="tex">$\lbrace \mu _e^{k}\rbrace _{k=1}^{C}$</span>     </span> with a set of induced probability density function (p.d.f.) <span class="inline-equation"><span class="tex">$\lbrace p_{A_e^k}: \mathcal {E}\rightarrow [0,1]\rbrace _{k=1}^{C}$</span>     </span>, where <span class="inline-equation"><span class="tex">$p_{A_e^k}(L_{ei})$</span>     </span> is the probability that location <em>L<sub>ei</sub>     </em> belongs to <span class="inline-equation"><span class="tex">$A_e^k$</span>     </span> obtained from Geo-CMeans (Fig. <a class="fig" href="#fig3">3</a>a). Fig. <a class="fig" href="#fig3">3</a> a serves as a 2-dimensional projection of a specific cluster in Fig. <a class="fig" href="#fig2">2</a>, for the sake of better visualization.</p>     <p>With the consideration of the intensively large number of entities and thus clusters in our problem setting, it is intractable if we attempt to identify each one with a unique ID, which may otherwise cause overfitting during learning. A solution lies in the discretization of the whole geographical map into a two-dimensional grid system [<em>N</em>] &#x00D7; [<em>N</em>], the grid of which is uniquely identified, say with natural number set [<em>N</em>     <sup>2</sup>], as demonstrated in <a class="fig" href="#fig3">3</a>b. We then assign to each area the ID <span class="inline-equation"><span class="tex">$I_e^k\in [N^2]$</span>     </span> of the grid where its cluster center lies and obtain the corresponding one-hot representations with length <em>N</em>     <sup>2</sup> denoted by <span class="inline-equation"><span class="tex">$\tilde{I}_e = \lbrace \tilde{I}_e^{k}\rbrace _{k=1}^{C}$</span>     </span>. The set <span class="inline-equation"><span class="tex">$\tilde{I}_e$</span>     </span> will be further used as the input of the embedding layer of our model in Section 4 (Fig. <a class="fig" href="#fig4">4</a>).</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Zooming for Local Pattern Representation</h3>     </div>     </header>     <p>In addition to the <em>positioning</em> practice we apply for the global information representation, we devise a sub-procedure called <em>zooming</em> for constructing a well-conditioned representation of local location distribution patterns in <span class="inline-equation"><span class="tex">$A_e^{k}$</span>     </span>, which will be discussed as follows.</p>     <p>Although the domain of each fuzzy cluster component <span class="inline-equation"><span class="tex">$A_e^k$</span>     </span> is the total <span class="inline-equation"><span class="tex">$\mathcal {E}$</span>     </span>, the density of distribution is featured by an extreme sparsity at the margin as shown in Fig. <a class="fig" href="#fig2">2</a>      <span class="inline-equation"><span class="tex">$\&#x0026;$</span>     </span> <a class="fig" href="#fig3">3</a>a. It inspires us to truncate a cluster with a certain probability threshold <em>q</em> &#x2208; (0, 1). For each location <span class="inline-equation"><span class="tex">$L_{e}^{i} \in A_e^k$</span>     </span>, we exclude it from the belonging component if <span class="inline-equation"><span class="tex">$p_{A_e^k}(L_{ej}^k) {\lt} q$</span>     </span>. After such a procedure is carried over all these components, we obtain the set of truncated cluster components as <span class="inline-equation"><span class="tex">$\lbrace \tilde{A}_e^k\rbrace _{k=1}^{C}$</span>     </span>, i.e. the set of areas for entity <em>e</em>.</p>     <p>In fact, we could interpret the <span class="inline-equation"><span class="tex">$\tilde{A}_e^k$</span>     </span> that we obtained as a zoomed-in view of the original <span class="inline-equation"><span class="tex">$A_e^k$</span>     </span>, which ignores the negligible part of the domain. <span class="inline-equation"><span class="tex">$\tilde{A}_e^k$</span>     </span> magnifies the view around the cluster center, which is exactly the representation of an area on the global level of view. We define the radius of each area as <span class="inline-equation"><span class="tex">$r_e^{k} = max_{L_e^{i} \in \tilde{A}_e^k}\text{ }d(L_e^i, \mu _e^k)$</span>     </span>. The outcome of the procedure above is illustrated in Fig. <a class="fig" href="#fig3">3</a>a.</p>     <p>We finally discretize the domain of each truncated components again as an integer-valued two-dimensional grid system <span class="inline-equation"><span class="tex">$[M]\times [M]\rightarrow \mathbb {N}$</span>     </span>, where <em>P<sub>e</sub>     </em>(<em>i</em>, <em>j</em>) gives the count of <span class="inline-equation"><span class="tex">$L_e^i$</span>     </span> in <span class="inline-equation"><span class="tex">$\tilde{A}_e^k$</span>     </span>, as shown in Fig. <a class="fig" href="#fig3">3</a>c. The center of each grid system <span class="inline-equation"><span class="tex">$P_e^k$</span>     </span> is aligned with the center of the corresponding area, while the geographical range represented by such a grid system is <span class="inline-equation"><span class="tex">$2r_e^k\times {2r_e^k}$</span>     </span>, which corresponds to the common knowledge that the rescaling is independent of a local pattern. The set <span class="inline-equation"><span class="tex">$\lbrace P_e^k\rbrace _{k=1}^{C}$</span>     </span> will serve as the input of CNN component in our model (Fig. <a class="fig" href="#fig4">4</a>). The procedure discussed in this section as a whole is depicted in Algorithm 1.</p>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Location-based Embedding Model</h2>     </div>    </header>    <p>After constructing a well-conditioned representation of the original set of locations <em>L<sub>e</sub>     </em> as a set of global one-hot vectors <span class="inline-equation"><span class="tex">$\lbrace \tilde{I}_e^k\rbrace _{k=1}^{C}$</span>     </span> and the corresponding relatively dense local pattern matrices <span class="inline-equation"><span class="tex">$\lbrace P_e^k\rbrace _{k=1}^{C}$</span>     </span>, in this section we propose a neural network based structure (Fig <a class="fig" href="#fig4">4</a>) to merge these heterogeneous geographical representations of various shapes, scalings, and even interpretations into a unified vector-valued feature for entity <em>e</em> in a LBSN. As far as we know, our model is the first neural net based structure for capturing the complex entanglement between global and local geographical information.</p>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Vector-Valued Embedding for Global Feature Modeling</h3>     </div>     </header>     <p>First, let us consider the problem of embedding a set of global one-hot vectors <span class="inline-equation"><span class="tex">$\lbrace \tilde{I}_{e}^k\rbrace ^{C}_{k=1}$</span>     </span> into a vector <em>GF<sub>e</sub>     </em> representing the entity <em>e</em>&#x2019;s global feature. In the literature of modern embedding methods, a generic architecture proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] has a successful application in NLP for word embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>]. Within our context, we were inspired by the representation compression power of such an efficient embedding structure. With the help of such embedding, we could compress the obtained one-hot representations by learning a distributed embedding for each global grid. The technical details are briefly explained as follows.</p>     <p>In the first layer we exploit the classical method proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] that introduces an embedding table to map the one-hot representation <span class="inline-equation"><span class="tex">$\tilde{I}_e^k\in {\tilde{I}_e}$</span>     </span> into a vector, which has an intuitive interpretation as the vector-valued embedding of a global geographical grid. For each entity <em>e</em>, we obtain <em>C</em> vectors as its global embedding of the grids. By the subsequent projection layer, <em>C</em> grid embeddings will be combined into a single vector <em>GF<sub>e</sub>     </em> using average pooling, which serves as an extracted global feature as shown in the lower part of Fig. <a class="fig" href="#fig4">4</a>.</p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Convolutional Neural Network for Local Pattern Feature Extraction</h3>     </div>     </header>     <p>Local patterns of different entities over a set of geographically related regions vary a lot [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]. Even the same entity could have totally divergent local patterns within distinct areas. A real-world pattern is shown in Fig. <a class="fig" href="#fig7">5</a>. <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig6.jpg" class="img-responsive" alt="" longdesc=""/>     </figure>     <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig7.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Various local patterns of users from Meetup-USA.</span>      </div>     </figure>     </p>     <p>As far as we know, few previous works have successfully applied CNN model to geographical pattern extraction, since a rough preprocessing of geographical features may cause the otherwise powerful convolution operator suffer from the extreme sparsity of the global counter matrix for each entity. Thanks for our locally <em>zooming method</em> (Sec. 3.4) applied during the representation construction stage, we are able to alleviate the sparseness and obtain a CNN-friendly relatively dense representation of local patterns.</p>     <p>Technically, we choose a classical 3-layer CNN architecture for local feature extraction, as depicted in the upper part of Fig. <a class="fig" href="#fig4">4</a>. The input of our CNN architecture is the <em>C</em> matrices <span class="inline-equation"><span class="tex">$\lbrace P_s^k\rbrace _{k=1}^{C}$</span>     </span>, representing local patterns for each area <span class="inline-equation"><span class="tex">$\tilde{A}_u^k$</span>     </span>, as shown in Fig. <a class="fig" href="#fig3">3</a>c. Such an input matrix is called a channel, which carries distinct views of information, as suggested by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>]. With a similar consideration, we stack <em>C</em> local patterns into a multi-channel input, actively embracing the diversity of local patterns in distinct areas. A multi-channel input is then forwarded into a convolutional layer, a pooling layer, and finally a projection layer for producing a local feature <em>LF<sub>e</sub>     </em> for entity <em>e</em>. The feature extraction power of CNN for local visiting patterns will be validated in the following sections via self-comparison experiments.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Combining Global and Local Features with Neural Tensor Network</h3>     </div>     </header>     <p>After obtaining the global feature <em>GF<sub>e</sub>     </em> and local feature <em>LF<sub>e</sub>     </em> for a given entity <em>e</em>, we further our discussion on merging these two kinds of implicitly related feature into a unified form. Previous models such as [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>] adopt a naive method by concatenating these vectors into a longer one, which, from our perspective, may fail to model some deeper correlations between features residing on distinct layers of abstraction.</p>     <p>In fact, a better neural architecture for this subtask is neural tensor network (NTN) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], which replaces the traditional structure of a standard linear layer with input on the concatenated vector with a tensor directly operating on several input features. This architecture has the power to explicitly model the interaction of features among the vector-valued cascading dimensions. <figure id="fig8">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig8.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Neural tensor network for a better unifying of global and local features.</span>      </div>     </figure>     </p>     <p>A formal description of NTN with our notations for global feature <em>GF<sub>e</sub>     </em> and local feature <em>LF<sub>e</sub>     </em> can be given as follows (a schematic diagram with <em>L</em> = 3 in Fig. <a class="fig" href="#fig8">6</a>) <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} F_e = \tanh \Bigg (GF_e^T \cdot W^{1:L} \cdot LF_e + V \cdot \Big [{\begin{array}{*10c}GF_e\\LF_e \end{array}}\Big ] + b\Bigg) \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> where L represents the extra dimension of the tensor W, as compared to an original weight matrix, with <em>V</em> term maintaining a degenerate form as a linear layer and <em>b</em> the bias with dimension <em>L</em>. We will see the advantage of NTN over the original linear structure in our self-comparison experiments as well.</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Optimization</h3>     </div>     </header>     <p>The output of our GeoCNTN model can thus be considered as a unified geographical vector-valued feature <em>F<sub>e</sub>     </em> for each entity <em>e</em>. In order to refine the features obtained from our model, we introduce a generic form of loss function as follows <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} L(\theta) = L(X|F,\theta) + \alpha \Vert \theta \Vert ^2 \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> where <em>L</em>(<em>X</em>|<em>F</em>, <em>&#x03B8;</em>) is the log likelihood function of given set of features <em>F</em>, with parameters <em>&#x03B8;</em> and observations <em>X</em>, and the latter term serves for regularization in case of overfitting. Specifically, <em>&#x03B8;</em> = {<em>&#x03B8;<sub>c</sub>     </em>, <em>&#x03B8;<sub>e</sub>     </em>, <em>&#x03B8;<sub>n</sub>     </em>} are parameters of our model: <em>&#x03B8;<sub>c</sub>     </em> for CNN, <em>&#x03B8;<sub>e</sub>     </em> for global grid embedding structure and <em>&#x03B8;<sub>n</sub>     </em> = {<em>W</em>, <em>V</em>, <em>b</em>} denotes the parameters of NTN.</p>     <p>Then we minimize the loss function in Eq. <a class="eqn" href="#eq4">8</a> in order to obtain an optimal <em>&#x03B8;</em>     <sup>*</sup>. Specifically we use gradient based method for training, and We adopt Adam algorithm to conduct the learning process [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>].</p>     <p>Our GeoCNTN is thus a widely applicable model used with distinct form of observations in a wide range of tasks. Typical real-world case studies in LBSN will be presented in Section 5 with corresponding choices for the generic loss in Eq. <a class="eqn" href="#eq4">8</a>. As a summary of our model with the devised preprocessing method, the reader can refer to Algorithm <a class="fig" href="#fig6">2</a>.</p>    </section>   </section>   <section id="sec-20">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Applications and Empirical Results</h2>     </div>    </header>    <p>In this section, we conduct two typical case studies to validate our location-based embedding model. The former uses heterogeneous social links and the latter experiment uses entities&#x2019; implicit classification as observations. We compare our model with the state-of-art ones in both tasks. Finally we visualize the features learned by our models for each entity in LBSN, the result shows that our model not only perform well in prediction tasks but also give interpreted features, unifying both global and local geographical information after the supervised learning with observations.</p>    <p>We conduct our experiments on one NVIDIA GTX-1080. As a near-optimal setting of model&#x0027;s hyper-parameter during a large number of experiments, we choose the learning rate as 0.005, regularization parameter as 0.01, grid configuration for the map as 100 &#x00D7; 100, that for local pattern as 10 &#x00D7; 10, cluster count to 3 for our Geo-CMeans fuzzy clustering algorithm. In CNN, kernel size is 3 &#x00D7; 3, together with 2 &#x00D7; 2 max pooling and the projection layer as a fully connected layer.</p>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Case Study: Link Prediction</h3>     </div>     </header>     <p>We first use links between heterogeneous entities in a user-group LBSN as observations, specifically we set <em>E</em> = {<em>U</em>, <em>G</em>}, where <em>U</em> represents the set of users and <em>G</em> the set of groups. Then the objective loss function in Eq. <a class="eqn" href="#eq4">8</a> can be written as <div class="table-responsive" id="Xeq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} L(X|F, \theta) = -\sum _{u\in U}\sum _{g \in G}\big [ x_{ug}\log {\tilde{x}_{ug}}+(1-x_{ug})\log {(1 - \tilde{x}_{ug})} \big ] \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> where <em>x<sub>ug</sub>     </em> equals 1 if there is an observed link between <em>u</em> and <em>g</em>, 0 means not, with <span class="inline-equation"><span class="tex">$\tilde{x}_{ug}=\sigma (F_u^T\cdot F_g)$</span>     </span> the prediction as a probability. Such a task is well defined since intuitively entities with more similar geographical features may have higher similarity in LBSN.</p>     <section id="sec-22">     <p><em>5.1.1 Experiment Settings.</em> In this section we present our detailed experiment settings as follows,</p>     <p>      <strong>Datasets Metrics</strong> We validate our model on two real-world Meetup datasets containing a set of locations for each entity, described by longitude-latitude pairs, together with observed links between heterogeneous entities. We choose two datasets, i.e. Meetup-USA and Meetup-Europe, with the detailed statistics shown in Table <a class="tbl" href="#tab1">1</a>. We divide the training set and validation set with the ratio of 9:1. In terms of metrics, we adopt normalized discounted cumulative gain for first n items (NDCG@n) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>], receiver operating characteristic (ROC) curve [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0010">10</a>] and its corresponding area under curve (AUC) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0049">49</a>] for the performance evaluation. Specifically, we choose NDCG@10 in evaluation.</p>     <div class="table-responsive" id="tab1">      <div class="table-caption">       <span class="table-number">Table 1:</span>       <span class="table-title">Statistics of Datasets for Experiments.</span>      </div>      <table class="table"> 					 <thead>        <tr>        <th style="text-align:center;">InformationDatasets</th>        <th style="text-align:center;">Meetup-USA</th>        <th style="text-align:center;">Meetup-Europe</th>        </tr> 					 </thead>       <tbody>        <tr>        <td style="text-align:center;"/>        <td/>        <td/>        </tr>        <tr>        <td style="text-align:center;">Users</td>        <td style="text-align:center;">35255</td>        <td style="text-align:center;">21004</td>        </tr>        <tr>        <td style="text-align:center;">Groups</td>        <td style="text-align:center;">12860</td>        <td style="text-align:center;">2521</td>        </tr>        <tr>        <td style="text-align:center;">Heterogeneous links</td>        <td style="text-align:center;">174354</td>        <td style="text-align:center;">178414</td>        </tr>        <tr>        <td style="text-align:center;">Longitude range</td>        <td style="text-align:center;">(&#x2212; 125&#x00B0;, &#x2212;65&#x00B0;)</td>        <td style="text-align:center;">(&#x2212; 25&#x00B0;, 66&#x00B0;)</td>        </tr>        <tr>        <td style="text-align:center;">Latitude range</td>        <td style="text-align:center;">(23&#x00B0;, 55&#x00B0;)</td>        <td style="text-align:center;">(36&#x00B0;, 82&#x00B0;)</td>        </tr>        <tr>        <td style="text-align:center;">Average location # per user</td>        <td style="text-align:center;">13.6</td>        <td style="text-align:center;">12.3</td>        </tr>        <tr>        <td style="text-align:center;">Average location # per group</td>        <td style="text-align:center;">26.7</td>        <td style="text-align:center;">20.3</td>        </tr>        <tr>        <td style="text-align:center;">Linkage density</td>        <td style="text-align:center;">7.691 &#x00D7; 10<sup>&#x2212; 4</sup>        </td>        <td style="text-align:center;">6.739 &#x00D7; 10<sup>&#x2212; 3</sup>        </td>        </tr>       </tbody>      </table>     </div>     <figure id="fig9">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig9.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">The ROC curves and NDCG@10 results of different methods on the two datasets.</span>      </div>     </figure>     <p>      <strong>Comparison Methods</strong> We compare GeoCNTN&#x0027;s performance with several state-of-art models in predicting heterogeneous links between users and groups, i.e. two content based methods (A)(B), two collaborative methods (C)(D) and two content-based collaborative methods (E)(F), together with three self-comparison models (G)-(I). A brief review is as follows, <strong>(A) Adamic/Adar</strong>: A metric for calculating similarity between two entities based on their common neighbors in graph [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0034">34</a>]. Here we grid the map and count ratio of common grid as similarity. <strong>(B) Random Walk with Restart (RWR)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>]: It uses geographical similarity and social similarity between users and conduct predicting links between users by a Random Walk with Restart algorithm. <strong>(C) Probabilistic Matrix Factorization (PMF)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0030">30</a>]: A probabilistic model based on matrix factorization to predict the rating between heterogeneous entities. <strong>(D) Probabilistic Matrix Factorization with Social Regularization (PMFSR)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0025">25</a>]: It suggests that users have similar social connection should have similar latent factors, which outperform PMF in link prediction. Here, we assume that users in common groups share similar latent factors. <strong>(E) Pairwise Tag-enhanced and feature-based Matrix factorization for group recommendation (PTA)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0047">47</a>]: It adds similarity-based features between user and group to matrix factorization to conduct group recommendation. Here, we use the minimized distance, common grids of user and group as similarity. <strong>(F) Geographical Matrix Factorization (GeoMF) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0020">20</a>]</strong>: It uses kernel density estimator (KDE) to model user&#x0027;s distribution on global geographical grids and add this feature to matrix factorization to conduct point-of-interest (POI) recommendation for users. Here, we regard group&#x0027;s location set as POI&#x0027;s location. <strong>(G) rawCNN</strong>: Directly application of a CNN of the same architecture to the location counting matrix obtained from the global geographical grids and regard the output as the feature vector. <strong>(H) GeoCNTN-no-local</strong>: We remove the CNN component by injecting random white noise as local pattern <strong>(I) GeoCNTN-no-tensor</strong>: We replace the NTN component with a simply vector concatenation.</p>     </section>     <section id="sec-23">     <p><em>5.1.2 Performance.</em> The NDCG@10 and AUC curves are shown in Table <a class="tbl" href="#tab2">2</a>, Roc curves and NDCG@n are depicted as Fig. <a class="fig" href="#fig9">7</a>. Compared to the state-of-art models, our GeoCNTN brings a huge improvement by 9% in NDCG@10 for both dataset, compared with the best baseline GeoMF.</p>     <p>First comparing two content-based models Adamic/Adar and RWR, we can see that by the additional observations, RWR improved 4% in NDCG, which means it may not be satisfying if we only use location features to predict heterogeneous links in LBSN. For a collaborative method such as PMF and PMFSR, which only uses supervised heterogeneous links for learning and prediction, it performs better than RWR without using any geographical information. Then if we add rough preprocessed geographical information into MF as PTA, we can see that it achieves 7% improvement in NDCG than PMFSR, which states the fact that geographical information is useful in predicting social links in LBSN. By taking advantage of geographical information in a more sophisticated way, we can see that GeoMF made 10% improvement in NDCG than PTA. We infer that 2-dimensional geographical data carries important information in LBSN. AUC states the same result. <figure id="fig10">       <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186131/images/www2018-140-fig10.jpg" class="img-responsive" alt="Figure 8"        longdesc=""/>       <div class="figure-caption">        <span class="figure-number">Figure 8:</span>        <span class="figure-title">Visualization of original locations (a) and features (b)(c) via t-SNE, where the color of each scatter point annotates the geographical cluster in (a) it comes from.</span>       </div>      </figure>     </p>     <div class="table-responsive" id="tab2">      <div class="table-caption">       <span class="table-number">Table 2:</span>       <span class="table-title">AUC and NDCG@10 of comparison methods.</span>      </div>      <table class="table"> 					 <thead>        <tr>        <th style="text-align:center;">MethodsResults</th>        <th colspan="2" style="text-align:center;">Meetup-USA<hr/>        </th>        <th colspan="2" style="text-align:center;">Meetup-Europe<hr/>        </th>        </tr>        <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">AUC</th>        <th style="text-align:center;">NDCG@10</th>        <th>AUC</th>        <th>NDCG@10</th>        </tr> 					 </thead>       <tbody>        <tr>        <td style="text-align:center;">Adamic/Adar</td>        <td style="text-align:center;">0.747</td>        <td style="text-align:center;">0.291</td>        <td>0.691</td>        <td>0.344</td>        </tr>        <tr>        <td style="text-align:center;">RWR</td>        <td style="text-align:center;">0.805</td>        <td style="text-align:center;">0.333</td>        <td>0.793</td>        <td>0.386</td>        </tr>        <tr>        <td style="text-align:center;">PMF</td>        <td style="text-align:center;">0.871</td>        <td style="text-align:center;">0.445</td>        <td>0.859</td>        <td>0.496</td>        </tr>        <tr>        <td style="text-align:center;">PMFSR</td>        <td style="text-align:center;">0.885</td>        <td style="text-align:center;">0.456</td>        <td>0.875</td>        <td>0.517</td>        </tr>        <tr>        <td style="text-align:center;">PTA</td>        <td style="text-align:center;">0.906</td>        <td style="text-align:center;">0.524</td>        <td>0.896</td>        <td>0.531</td>        </tr>        <tr>        <td style="text-align:center;">GeoMF</td>        <td style="text-align:center;">0.933</td>        <td style="text-align:center;">0.625</td>        <td>0.915</td>        <td>0.565</td>        </tr>        <tr>        <td style="text-align:center;">rawCNN</td>        <td style="text-align:center;">0.497</td>        <td style="text-align:center;">0.011</td>        <td>0.496</td>        <td>0.021</td>        </tr>        <tr>        <td style="text-align:center;">GeoCNTN-no-local</td>        <td style="text-align:center;">0.864</td>        <td style="text-align:center;">0.435</td>        <td>0.854</td>        <td>0.433</td>        </tr>        <tr>        <td style="text-align:center;">GeoCNTN-no-tensor</td>        <td style="text-align:center;">0.951</td>        <td style="text-align:center;">0.664</td>        <td>0.923</td>        <td>0.630</td>        </tr>        <tr>        <td style="text-align:center;">GeoCNTN</td>        <td style="text-align:center;">         <strong>0.962</strong>        </td>        <td style="text-align:center;">         <strong>0.718</strong>        </td>        <td>         <strong>0.937</strong>        </td>        <td>         <strong>0.660</strong>        </td>        </tr>       </tbody>      </table>     </div>     <p>Finally our model shows a remarkable advantage over these baselines. Unlike GeoMF, which neglects the local location distribution pattern inside global grids as a conprimise to the complexity of KDE, our model can exploit extra geographical information on a finer level. Especially, we made 9% improvement in NDCG@10, where as <em>n</em> gets larger, the result gets better, which means our model can indeed make less errors in ranking negative-prone links on top of the list. To do further study on the benefits brought by our delicate crafting of global and local information, we conduct several self-comparison models as follows.</p>     </section>     <section id="sec-24">     <p><em>5.1.3 GeoCNTN Parts Study.</em> As depicted in Table&#x00A0;<a class="tbl" href="#tab2">2</a>, the comparison between rawCNN and GeoCNTN states the fact that CNN cannot even extract any information from the extreme sparse counting matrix obtained without our locally <em>zooming</em> strategy. Furthermore, if we remove the CNN part from our model, the result is similar to PMF because the embedding of global grids only contains latent information rather than geographical information. Finally, we study the effect of replacing the NTN with a naive concatenation, which brought poorer performance compared with our model by about 5% in NDCG. The result shows that the tensor layer indeed discovers the innate correlations between global information and local pattern.</p>     </section>    </section>    <section id="sec-25">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Case study: Entity Classification</h3>     </div>     </header>     <p>Then we study another task, i.e., use entities&#x2019; location information to predict their classification. In LBSN, each entity always has a list of tags, e.g., shopping, reading, traveling etc. Entities with irrelevant tags may have different location pattern intuitively. Based on such an observation, we define the loss function as <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} L(X|F, \theta) = -\sum _{e\in E}\big [ CEH(X_e, \tilde{X}_e) \big ] \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> where <em>CEH</em> denotes the cross entropy error and <span class="inline-equation"><span class="tex">$\tilde{X}_e=softmax(F_e)$</span>     </span> the predicted probability for each class.</p>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Experiment results (F1 score) for entity classification (Case Study 2) on Meetup-USA.</span>     </div>     <table class="table"> 				 <thead>       <tr>        <th style="text-align:center;">MethodsEntities</th>        <th style="text-align:center;">User</th>        <th style="text-align:center;">Group</th>       </tr> 				 </thead>      <tbody>       <tr>        <td style="text-align:center;"/>        <td/>        <td/>       </tr>       <tr>        <td style="text-align:center;">Place Count</td>        <td>0.172</td>        <td>0.142</td>       </tr>       <tr>        <td style="text-align:center;">KDE</td>        <td>0.201</td>        <td>0.168</td>       </tr>       <tr>        <td style="text-align:center;">GeoCNTN</td>        <td>        <strong>0.317</strong>        </td>        <td>        <strong>0.267</strong>        </td>       </tr>      </tbody>     </table>     </div>     <section id="sec-26">     <p><em>5.2.1 Experiment Settings.</em> Here we target on two specific tasks, user classification and group classification in the Meetup-USA dataset. In this experiment, we treat the tags of the users/groups as their class labels. The set of users has 5372 tags in total and for groups, there are 5045 tags annotated on them. For each entity, the average tag number is 6. Considering the large number of tags, in order to reduce the class number, we adopt the method proposed by word2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0027">27</a>]. With this method, tags are at first embedded into vectors and then clustered into 20 clusters with K-Means based on their semantic distance. These 20 clusters finally serve as our target classes. Moreover, in terms of the baseline methods, since collaborative filtering methods such as PMF could not be directly applied to solve this problem, we adopt the <strong>(A) place count</strong> and <strong>(B) kernel density estimator (KDE)</strong> as two baselines, where the former one counts the locations in global grids for each entity and the latter applies KDE on these grids, similar to GeoMF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>]. We then train a multi-label support vector machine to predict the class the entity belongs to. To measure the performance, we use F1 score [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0011">11</a>] as our metric.</p>     </section>     <section id="sec-27">     <p><em>5.2.2 Performance.</em> As shown in Table <a class="tbl" href="#tab3">3</a>, we can see our model outperforms the best baseline 11% in F1 score. Comparing the baseline methods, we find that although they are both based on the counts on grids, for KDE has more continuous features, it makes a 16% improvement. However, their F1 scores are still low for their lack of information from local patterns, which may lead to the situation that these trained models would give a global class to each grid, e.g., shopping for New York City. GeoCNTN also uses the information from observed local patterns, which may give a much more accurate prediction. Hence it is not surprising that it outperforms the classical feature extraction methods.</p>     <p>Combined with the first case study, our GeoCNTN indeed has the flexibility and extensibility to be applied in a wide range of LBSN-related tasks. Via combining the global and local geographical information in a clever way, it should bring a noticeable improvement with a large probability.</p>     </section>    </section>    <section id="sec-28">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Visualization of Extracted Features</h3>     </div>     </header>     <p>As a further validation of our GeoCNTN&#x0027;s strength for unifying global and local geographical information with the supervision of observations in LBSN, we conduct a series of visualization experiments with the users&#x2019; features we have obtained in the first case study. With the aid of t-SNE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] (an auxiliary method that projects the high-dimensional data to a two-dimensional plane), we plot the the latent factors of users from the PMFSR and the unified features {<em>F<sub>e</sub>     </em>}<sub>      <em>e</em> &#x2208; <em>E</em>     </sub> learned by GeoCNTN respectively in <a class="fig" href="#fig10">8</a>b and <a class="fig" href="#fig10">8</a>c, together with the users&#x2019; original locations in <a class="fig" href="#fig10">8</a>a.</p>     <p>As shown in Fig. <a class="fig" href="#fig10">8</a>, it is hard to explain pure latent features (Fig. <a class="fig" href="#fig10">8</a>b). However, our refined geographical feature is highly interpretable (Fig. <a class="fig" href="#fig10">8</a>c). The first observed property is that our model roughly preserves the geographical clusters. We say <em>roughly</em> because users (see the features in Fig. <a class="fig" href="#fig10">8</a>a) are divided into three clusters in (c). In order to find out the reasons, we sampled two users A and C, as plotted in Fig. <a class="fig" href="#fig10">8</a>c. We found out that they have no common groups, in contrast to 9 common groups with user B, which means that although A and C are geographically closer, the lack of common social interactions makes their features more distant according to our model. What is more interesting, A and D are much closer w.r.t our features than A and B because they have 1 common group. We further study the relatedness between A and B in Fig. <a class="fig" href="#fig10">8</a>c. When we inspected the local patterns from the preprocessed dataset (Fig. <a class="fig" href="#fig7">5</a>a &#x0026; <a class="fig" href="#fig7">5</a>b), soon it became clear that they have totally different local patterns. Whereas the local patterns of A are relatively concentrated, those for B are scattered. This explains why we can infer that although they are close together in geography and share several existing social links, they are relatively separated in our feature diagram.</p>     <p>To sum up, our GeoCNTN has indeed extracted interpretable features by unifying both global and local geographical information, refined with observed information in LBSN.</p>    </section>   </section>   <section id="sec-29">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> conclusion and Future work</h2>     </div>    </header>    <p>In this paper, we proposed to study the problem of geographical feature extraction for entities in LBSN and described a novel location-based embedding model with neural network structure called GeoCNTN. It has three major contributions. First, we proposed a curvature-sensitive fuzzy clustering algorithm (Geo-CMeans) to obtain well-defined global and local representations from the original geographical data. We then proposed a neural network based learning model using convolutional neural network and an embedding structure for local pattern and global geographical feature extraction respectively. Finally, we applied a neural tensor network in order to discover higher-order correlations between global and local features. With a 9% improvement in NDCG for link prediction and a 11% improvement in F1 score for entity classification, we have indeed validated the embedding efficacy of our model. Furthermore, we validated the interpretability of the embeddings learned by our model with a series of visualization tasks.</p>    <p>Our work demonstrates how to use deep neural networks to extract geographical features for entities in LBSN. We propose several future directions of this work. First, it may be fruitful to extend the proposed location-based embedding structure towards unsupervised learning as shown in variational autoencoder (VAE) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. This can potentially alleviate the rare observation problem in a given LBSN. Another probable direction is to exploit and combine other geographical contexts, such as trajectories and temporal information of entities, with our current GeoCNTN for better embedding performance. Finally, it is interesting to connect the study with machine learning theories to further understand why GeoCNTN can perform better than baseline models in various LBSN-related tasks.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-30">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>The authors would also like to thank the anonymous referees for their valuable comments and helpful suggestions. This work is funded in part by the National Program on Key Basic Research (NO. 2015CB358800) and R&#x0026;D Program of STCSM under Grant No.17JC1420200.</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Daniel Ashbrook and Thad Starner. 2003. Using GPS to learn significant locations and predict movement across multiple users. <em>      <em>Personal and Ubiquitous computing</em>     </em>7, 5 (2003), 275&#x2013;286.</li>     <li id="BibPLXBIB0002" label="[2]">Hakan Bagci and Pinar Karagoz. 2016. Context-aware friend recommendation for location based social networks using random walk. In <em>Proceedings of the 25th international conference companion on world wide web</em>. 531&#x2013;536.</li>     <li id="BibPLXBIB0003" label="[3]">Yoshua Bengio, R&#x00E9;jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. <em>      <em>Journal of machine learning research</em>     </em>3, Feb (2003), 1137&#x2013;1155.</li>     <li id="BibPLXBIB0004" label="[4]">Chen Cheng, Haiqin Yang, Irwin King, and Michael&#x00A0;R Lyu. 2012. Fused Matrix Factorization with Geographical and Social Influence in Location-Based Social Networks. In <em>      <em>Aaai</em></em>, Vol.&#x00A0;12. 17&#x2013;23.</li>     <li id="BibPLXBIB0005" label="[5]">Eunjoon Cho, Seth&#x00A0;A Myers, and Jure Leskovec. 2011. Friendship and mobility: user movement in location-based social networks. In <em>      <em>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. 1082&#x2013;1090.</li>     <li id="BibPLXBIB0006" label="[6]">Justin Cranshaw, Eran Toch, Jason Hong, Aniket Kittur, and Norman Sadeh. 2010. Bridging the gap between physical location and online social networks. In <em>      <em>ACM International Conference on Ubiquitous Computing</em></em>. 119&#x2013;128.</li>     <li id="BibPLXBIB0007" label="[7]">Bal&#x00E1;zs&#x00A0;Csan&#x00E1;d Cs&#x00E1;ji. 2001. Approximation with artificial neural networks. <em>      <em>Faculty of Sciences, Etvs Lornd University, Hungary</em>     </em>24 (2001), 48.</li>     <li id="BibPLXBIB0008" label="[8]">J.&#x00A0;C. Dunn. 1974. A fuzzy relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters. <em>      <em>Journal of Cybernetics</em>     </em>3, 3 (1974), 32&#x2013;57.</li>     <li id="BibPLXBIB0009" label="[9]">Martin Ester, Hans&#x00A0;Peter Kriegel, and Xiaowei Xu. 1996. A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise. In <em>      <em>International Conference on Knowledge Discovery and Data Mining</em></em>. 226&#x2013;231.</li>     <li id="BibPLXBIB0010" label="[10]">Tom Fawcett. 2006. An introduction to ROC analysis. <em>      <em>Pattern Recognition Letters</em>     </em>27, 8 (2006), 861 &#x2013; 874. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.patrec.2005.10.010"      target="_blank">https://doi.org/10.1016/j.patrec.2005.10.010</a> ROC Analysis in Pattern Recognition.</li>     <li id="BibPLXBIB0011" label="[11]">William&#x00A0;B Frakes and Ricardo Baeza-Yates. 1992. Information retrieval: data structures and algorithms. (1992).</li>     <li id="BibPLXBIB0012" label="[12]">Kalervo J&#x00E4;rvelin and Jaana Kek&#x00E4;l&#x00E4;inen. 2002. Cumulated gain-based evaluation of IR techniques. <em>      <em>ACM Transactions on Information Systems (TOIS)</em>     </em>20, 4 (2002), 422&#x2013;446.</li>     <li id="BibPLXBIB0013" label="[13]">Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional architecture for fast feature embedding. In <em>      <em>Proceedings of the 22nd ACM international conference on Multimedia</em></em>. 675&#x2013;678.</li>     <li id="BibPLXBIB0014" label="[14]">Donghyun Kim, Chanyoung Park, Jinoh Oh, Sungyoung Lee, and Hwanjo Yu. 2016. Convolutional matrix factorization for document context-aware recommendation. In <em>      <em>Proceedings of the 10th ACM Conference on Recommender Systems</em></em>. 233&#x2013;240.</li>     <li id="BibPLXBIB0015" label="[15]">Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <em>      <em>arXiv preprint arXiv:1412.6980</em>     </em>(2014).</li>     <li id="BibPLXBIB0016" label="[16]">Diederik&#x00A0;P Kingma and Max Welling. 2013. Auto-encoding variational bayes. <em>      <em>arXiv preprint arXiv:1312.6114</em>     </em>(2013).</li>     <li id="BibPLXBIB0017" label="[17]">Alex Krizhevsky, Ilya Sutskever, and Geoffrey&#x00A0;E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In <em>      <em>Advances in neural information processing systems</em></em>. 1097&#x2013;1105.</li>     <li id="BibPLXBIB0018" label="[18]">Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. <em>      <em>Nature</em>     </em>521, 7553 (2015), 436&#x2013;444.</li>     <li id="BibPLXBIB0019" label="[19]">Yann LeCun, L&#x00E9;on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. <em>      <em>Proc. IEEE</em>     </em>86, 11 (1998), 2278&#x2013;2324.</li>     <li id="BibPLXBIB0020" label="[20]">Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong Rui. 2014. GeoMF: joint geographical modeling and matrix factorization for point-of-interest recommendation. In <em>      <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. ACM, 831&#x2013;840.</li>     <li id="BibPLXBIB0021" label="[21]">Tao Lin, Tian Guo, and Karl Aberer. 2017. Hybrid Neural Networks for Learning the Trend in Time Series. (2017).</li>     <li id="BibPLXBIB0022" label="[22]">Bin Liu, Hui Xiong, Spiros Papadimitriou, Yanjie Fu, and Zijun Yao. 2015. A general geographical probabilistic factor model for point of interest recommendation. <em>      <em>IEEE Transactions on Knowledge and Data Engineering</em>     </em>27, 5(2015), 1167&#x2013;1179.</li>     <li id="BibPLXBIB0023" label="[23]">Yong Liu, Wei Wei, Aixin Sun, and Chunyan Miao. [n. d.]. Exploiting geographical neighborhood characteristics for location recommendation. In <em>      <em>CIKM&#x2019;14</em></em>. 739&#x2013;748.</li>     <li id="BibPLXBIB0024" label="[24]">Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional networks for semantic segmentation. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em></em>. 3431&#x2013;3440.</li>     <li id="BibPLXBIB0025" label="[25]">Hao Ma, Dengyong Zhou, Chao Liu, Michael&#x00A0;R Lyu, and Irwin King. 2011. Recommender systems with social regularization. In <em>      <em>Proceedings of the fourth ACM international conference on Web search and data mining</em></em>. 287&#x2013;296.</li>     <li id="BibPLXBIB0026" label="[26]">Laurens van&#x00A0;der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. <em>      <em>Journal of Machine Learning Research</em>     </em>9, Nov (2008), 2579&#x2013;2605.</li>     <li id="BibPLXBIB0027" label="[27]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. <em>      <em>arXiv preprint arXiv:1301.3781</em>     </em>(2013).</li>     <li id="BibPLXBIB0028" label="[28]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&#x00A0;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em>      <em>Advances in neural information processing systems</em></em>. 3111&#x2013;3119.</li>     <li id="BibPLXBIB0029" label="[29]">Boriana&#x00A0;L. Milenova and Marcos&#x00A0;M. Campos. 2002. O-Cluster: scalable clustering of large high dimensional data sets. In <em>      <em>IEEE International Conference on Data Mining, 2002. ICDM 2003. Proceedings</em></em>. 290&#x2013;297.</li>     <li id="BibPLXBIB0030" label="[30]">Andriy Mnih and Ruslan&#x00A0;R Salakhutdinov. 2008. Probabilistic matrix factorization. In <em>      <em>Advances in neural information processing systems</em></em>. 1257&#x2013;1264.</li>     <li id="BibPLXBIB0031" label="[31]">Yafeng Ren, Yue Zhang, Meishan Zhang, and Donghong Ji. 2016. Context-Sensitive Twitter Sentiment Classification Using Neural Network. In <em>      <em>AAAI</em></em>. 215&#x2013;221.</li>     <li id="BibPLXBIB0032" label="[32]">Luca Rossi and Mirco Musolesi. 2014. It&#x0027;s the way you check-in: identifying users in location-based social networks. In <em>      <em>Proceedings of the second ACM conference on Online social networks</em></em>. ACM, 215&#x2013;226.</li>     <li id="BibPLXBIB0033" label="[33]">Maja Rudolph, Francisco Ruiz, Stephan Mandt, and David Blei. 2016. Exponential family embeddings. In <em>      <em>Advances in Neural Information Processing Systems</em></em>. 478&#x2013;486.</li>     <li id="BibPLXBIB0034" label="[34]">Salvatore Scellato, Anastasios Noulas, and Cecilia Mascolo. [n. d.]. Exploiting place features in link prediction on location-based social networks. In <em>      <em>SIGKDD&#x2019;11</em></em>. 1046&#x2013;1054.</li>     <li id="BibPLXBIB0035" label="[35]">Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In <em>      <em>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em>. 373&#x2013;382.</li>     <li id="BibPLXBIB0036" label="[36]">Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. <em>      <em>arXiv preprint arXiv:1409.1556</em>     </em>(2014).</li>     <li id="BibPLXBIB0037" label="[37]">Richard Socher, Danqi Chen, Christopher&#x00A0;D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In <em>      <em>Advances in neural information processing systems</em></em>. 926&#x2013;934.</li>     <li id="BibPLXBIB0038" label="[38]">Waldo&#x00A0;R Tobler. 1970. A computer movie simulating urban growth in the Detroit region. <em>      <em>Economic geography</em>     </em>46, sup1 (1970), 234&#x2013;240.</li>     <li id="BibPLXBIB0039" label="[39]">Cheng Yang, Maosong Sun, Wayne&#x00A0;Xin Zhao, Zhiyuan Liu, and Edward&#x00A0;Y Chang. 2017. A Neural Network Approach to Jointly Modeling Social Networks and Mobile Trajectories. <em>      <em>ACM Transactions on Information Systems (TOIS)</em>     </em>35, 4 (2017), 36.</li>     <li id="BibPLXBIB0040" label="[40]">Mao Ye, Peifeng Yin, Wang-Chien Lee, and Dik-Lun Lee. 2011. Exploiting geographical influence for collaborative point-of-interest recommendation. In <em>      <em>Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</em></em>. 325&#x2013;334.</li>     <li id="BibPLXBIB0041" label="[41]">Josh Jia-Ching Ying, Eric Hsueh-Chan Lu, Wang-Chien Lee, Tz-Chiao Weng, and Vincent&#x00A0;S Tseng. 2010. Mining user similarity from semantic trajectories. In <em>      <em>Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Location Based Social Networks</em></em>. 19&#x2013;26.</li>     <li id="BibPLXBIB0042" label="[42]">Yonghong Yu and Xingguo Chen. 2015. A survey of point-of-interest recommendation in location-based social networks. In <em>      <em>Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence</em></em>, Vol. 130.</li>     <li id="BibPLXBIB0043" label="[43]">Quan Yuan, Gao Cong, Zongyang Ma, Aixin Sun, and Nadia&#x00A0;Magnenat Thalmann. [n. d.]. Time-aware point-of-interest recommendation. In <em>      <em>NIPS&#x2019;13</em></em>. 363&#x2013;372.</li>     <li id="BibPLXBIB0044" label="[44]">Jiawei Zhang and S&#x00A0;Yu Philip. 2014. Link prediction across heterogeneous social networks: A survey. <em>      <em>SOCIAL NETWORKS</em>     </em> (2014).</li>     <li id="BibPLXBIB0045" label="[45]">Jia-Dong Zhang and Chi-Yin Chow. 2015. CoRe: Exploiting the personalized influence of two-dimensional geographic coordinates for location recommendations. <em>      <em>Information Sciences</em>     </em>293(2015), 163&#x2013;181.</li>     <li id="BibPLXBIB0046" label="[46]">Jia-Dong Zhang, Chi-Yin Chow, and Yanhua Li. 2015. iGeoRec: A personalized and efficient geographical location recommendation framework. <em>      <em>IEEE Transactions on Services Computing</em>     </em>8, 5 (2015), 701&#x2013;714.</li>     <li id="BibPLXBIB0047" label="[47]">Wei Zhang, Jianyong Wang, and Wei Feng. 2013. Combining latent factor model with location features for event-based group recommendation. In <em>      <em>Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. 910&#x2013;918.</li>     <li id="BibPLXBIB0048" label="[48]">Yang Zhang and Jun Pang. 2015. Distance and Friendship: A Distance-Based Model for Link Prediction in Social Networks. (2015).</li>     <li id="BibPLXBIB0049" label="[49]">Kelly&#x00A0;H Zou, A&#x00A0;James O&#x0027;Malley, and Laura Mauri. 2007. Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models. <em>      <em>Circulation</em>     </em>115, 5 (2007), 654&#x2013;657.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://www.statista.com">https://www.statista.com</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://foursquare.com">https://foursquare.com</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://www.meetup.com">https://www.meetup.com</a>&#x00A0;&#x00A0;&#x00A0;<a class="link-inline force-break" href="https://www.yelp.com">https://www.yelp.com</a>&#x00A0;&#x00A0;&#x00A0;<a class="link-inline force-break" href="https://www.facebook.com">https://www.facebook.com</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186131">https://doi.org/10.1145/3178876.3186131</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
