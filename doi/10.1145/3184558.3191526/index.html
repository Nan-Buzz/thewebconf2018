<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Continuous-Time Dynamic Network Embeddings</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> <link rel="cite-as" href="https://doi.org/10.1145/3184558.3191526"/></head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191526'>https://doi.org/10.1145/3184558.3191526</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191526'>https://w3id.org/oa/10.1145/3184558.3191526</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Continuous-Time Dynamic Network Embeddings</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Giang Hoang</span>     <span class="surName">Nguyen</span>     Worcester Polytechnic Institute, <a href="mailto:ghnguyen@wpi.edu">ghnguyen@wpi.edu</a>    </div>    <div class="author">     <span class="givenName">John Boaz</span>     <span class="surName">Lee</span>     Worcester Polytechnic Institute, <a href="mailto:jtlee@wpi.edu">jtlee@wpi.edu</a>    </div>    <div class="author">     <span class="givenName">Ryan A.</span>     <span class="surName">Rossi</span>     Adobe Research, <a href="mailto:rrossi@adobe.com">rrossi@adobe.com</a>    </div>    <div class="author">     <span class="givenName">Nesreen K.</span>     <span class="surName">Ahmed</span>     Intel Labs, <a href="mailto:nesreen.k.ahmed@intel.com">nesreen.k.ahmed@intel.com</a>    </div>    <div class="author">     <span class="givenName">Eunyee</span>     <span class="surName">Koh</span>     Adobe Research, <a href="mailto:eunyee@adobe.com">eunyee@adobe.com</a>    </div>    <div class="author">     <span class="givenName">Sungchul</span>     <span class="surName">Kim</span>     Adobe Research, <a href="mailto:sukim@adobe.com">sukim@adobe.com</a>    </div>                            </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3191526" target="_blank">https://doi.org/10.1145/3184558.3191526</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Networks evolve continuously over time with the addition, deletion, and changing of links and nodes. Although many networks contain this type of temporal information, the majority of research in network representation learning has focused on static snapshots of the graph and has largely ignored the temporal dynamics of the network. In this work, we describe a general framework for incorporating temporal information into network embedding methods. The framework gives rise to methods for learning time-respecting embeddings from continuous-time dynamic networks. Overall, the experiments demonstrate the effectiveness of the proposed framework and dynamic network embedding approach as it achieves an average gain of 11.9% across all methods and graphs. The results indicate that modeling temporal dependencies in graphs is important for learning appropriate and meaningful network representations.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Artificial intelligence;</strong> <strong>Machine learning;</strong> <strong>Logical and relational learning;</strong> &#x2022;<strong> Mathematics of computing </strong>&#x2192; <strong>Graph theory;</strong> <strong>Graph algorithms;</strong> &#x2022;<strong> Theory of computation </strong>&#x2192; <strong>Streaming, sublinear and near linear time algorithms;</strong> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Data mining;</strong></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Dynamic network embeddings</small>, </span>     <span class="keyword">      <small> temporal node embeddings</small>, </span>     <span class="keyword">      <small> dynamic networks</small>, </span>     <span class="keyword">      <small> network representation learning</small>, </span>     <span class="keyword">      <small> temporal random walks</small>, </span>     <span class="keyword">      <small> continuous-time dynamic network</small>, </span>     <span class="keyword">      <small> graph stream</small>, </span>     <span class="keyword">      <small> feature learning</small>, </span>     <span class="keyword">      <small> temporal networks</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Giang Hoang Nguyen, John Boaz Lee, Ryan A. Rossi, Nesreen K. Ahmed, Eunyee Koh, and Sungchul Kim. 2018. Continuous-Time Dynamic Network Embeddings. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 9 Pages. <a href="https://doi.org/10.1145/3184558.3191526" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3191526</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>In recent years, network (graph/relational) data has grown at a tremendous rate; it is present in domains such as the Internet and the world-wide web &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>], scientific citation and collaboration &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>], epidemiology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>], communication analysis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0060">60</a>], metabolism&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0066">66</a>], ecosystems&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], bioinformatics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>], fraud and terrorist analysis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>], and many others. The links in these network data may represent citations, friendships, associations, metabolic functions, communications, co-locations, shared mechanisms, or many other explicit or implicit relationships.</p>    <p>The majority of these real-world networks are naturally dynamic&#x2014;evolving over time with the addition, deletion, and changing of nodes and links. The temporal information in networks is known to be important to accurately model, predict, and understand network data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0067">67</a>]. Despite the importance of these dynamics, the majority of previous work has ignored the temporal information in network data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0052">52</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0061">61</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0065">65</a>].</p>    <p>In this work, we address the problem of learning an appropriate network representation from <em>continuous-time dynamic networks</em> for improving the accuracy of predictive models. We describe a general framework for incorporating temporal dependencies into network embedding methods. The framework serves as a basis for incorporating temporal dependencies into existing node embedding and deep graph models based on random walks. The result is a more appropriate time-dependent network representation that captures the important temporal properties of the continuous-time dynamic network. Hence, the framework allows existing embedding methods to be easily adapted for learning more appropriate network representations from continuous-time dynamic networks by ensuring time is respected during the learning and therefore reduces noise by avoiding spurious or impossible sequences of events.</p>    <p>The proposed approach learns a more appropriate network representation from continuous-time dynamic networks that captures the important temporal dependencies of the network at the finest most natural granularity (<em>e.g.</em>, at a time scale of seconds or milliseconds). This is in contrast to representing the dynamic network as a sequence of static snapshot graphs where each static snapshot graph represents all edges that occur between a user-specified discrete-time interval (<em>e.g.</em>, day or week)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0057">57</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0059">59</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0063">63</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0064">64</a>]. This can be seen as a very coarse and noisy approximation of the actual continuous-time dynamic network. Besides the loss of information, there are many other issues such as selecting an appropriate aggregation granularity which is known to be an important and challenging problem in itself that can lead to poor predictive performance or misleading results. In addition, our approach naturally supports learning in <em>graph streams</em> where edges arrive continuously over time (<em>e.g.</em>, every second/millisecond)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] and therefore can be used for a variety of applications requring real-time performance&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0053">53</a>].</p>    <p>We demonstrate the effectiveness of the proposed framework and generalized dynamic network embedding method for temporal link prediction in several real-world networks from a variety of application domains. Overall, the proposed method achieves an average gain of 11.9% across all methods and graphs. The results indicate that modeling temporal dependencies in graphs is important for learning appropriate and meaningful network representations. In addition, any existing embedding method or deep graph model that uses random walks can benefit from the proposed framework (<em>e.g.</em>,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0052">52</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>]) as it serves as a basis for incorporating important temporal dependencies into existing methods. Methods generalized by the framework are able to learn more meaningful and accurate time-dependent network embeddings that capture important properties from continuous-time dynamic networks.</p>    <p>Previous embedding methods and deep graph models that use random walks search over the space of random walks <span class="inline-equation"><span class="tex">$\mathbb {S}$</span>    </span> on <em>G</em>, whereas the proposed approach learns temporal embeddings by searching over the space <span class="inline-equation"><span class="tex">$\mathbb {S}_{T}$</span>    </span> of temporal random walks that obey time. Informally, a <em>temporal walk S<sub>t</sub>    </em> from node <span class="inline-equation"><span class="tex">$v_{i_{1}}$</span>    </span> to node <span class="inline-equation"><span class="tex">$v_{i_{L+1}}$</span>    </span> is defined as a sequence of edges <span class="inline-equation"><span class="tex">$\lbrace (v_{i_{1}}, v_{i_{2}}, t_{i_{1}})$</span>    </span>, <span class="inline-equation"><span class="tex">$(v_{i_{2}},v_{i_{3}}, t_{i_{2}}), \ldots , (v_{i_{L}},$</span>    </span>     <span class="inline-equation"><span class="tex">$v_{i_{L+1}}, t_{i_{L}})\rbrace$</span>    </span> such that <span class="inline-equation"><span class="tex">$t_{i_{1}} \le t_{i_{2}} \le \ldots \le t_{i_{L}}$</span>    </span>. A temporal walk represents a <em>temporally valid</em> sequence of edges traversed in increasing order of edge times and therefore respect time. For instance, suppose each edge represents a contact (<em>e.g.</em>, email, phone call, proximity) between two entities, then a temporal random walk represents a feasible route for a piece of information through the dynamic network. It is straightforward to see that existing methods that ignore time learn embeddings from a set of random walks of which the vast majority of them capture sequences of events that are invalid when considering time. In other words, many of the random walks used by these methods to derive embeddings for nodes are not actually possible when time is respected. For instance, suppose we have two emails <em>e<sub>i</sub>    </em> = (<em>v</em>    <sub>1</sub>, <em>v</em>    <sub>2</sub>) from <em>v</em>    <sub>1</sub> to <em>v</em>    <sub>2</sub> and <em>e<sub>j</sub>    </em> = (<em>v</em>    <sub>2</sub>, <em>v</em>    <sub>3</sub>) from <em>v</em>    <sub>2</sub> to <em>v</em>    <sub>3</sub>; and let <span class="inline-equation"><span class="tex">$\mathcal {T}(v_1,v_2)$</span>    </span> be the time of an email <em>e<sub>i</sub>    </em> = (<em>v</em>    <sub>1</sub>, <em>v</em>    <sub>2</sub>). If <span class="inline-equation"><span class="tex">$\mathcal {T}(v_1,v_2) {\lt} \mathcal {T}(v_2,v_3)$</span>    </span> then the message <em>e<sub>j</sub>    </em> = (<em>v</em>    <sub>2</sub>, <em>v</em>    <sub>3</sub>) may reflect the information received from the email communication <em>e<sub>i</sub>    </em> = (<em>v</em>    <sub>1</sub>, <em>v</em>    <sub>2</sub>). However, if <span class="inline-equation"><span class="tex">$\mathcal {T}(v_1,v_2) {\gt} \mathcal {T}(v_2,v_3)$</span>    </span> then the message <em>e<sub>j</sub>    </em> = (<em>v</em>    <sub>2</sub>, <em>v</em>    <sub>3</sub>) cannot contain any information communicated in the email <em>e<sub>i</sub>    </em> = (<em>v</em>    <sub>1</sub>, <em>v</em>    <sub>2</sub>). This is just one simple example illustrating the importance of modeling the actual sequence of events (email communications). Embedding methods that ignore time are prone to many issues such as learning inappropriate node embeddings that do not accurately capture the dynamics in the network such as the real-world interactions or flow of information among nodes. See Figure&#x00A0;<a class="fig" href="#fig1">1</a> for another example of information loss that occurs when time is not respected (in existing methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0052">52</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0061">61</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0065">65</a>]).</p>    <p>The proposed approach has the following desired properties:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;"><strong>General &#x0026; Unifying Framework</strong>: We present a general framework for incorporating temporal dependencies in node embedding and deep graph models that leverage random walks.<br/></li>    <li id="list2" label="&#x2022;"><strong>Continuous-Time Dynamic Networks</strong>: Learns a time-dependent network representation for continuous-time dynamic networks. The approach avoids the issues and loss in information that arise from creating a sequence of discrete snapshot graphs from the continuous-time representation of the graph.<br/></li>    <li id="list3" label="&#x2022;"><strong>Effectiveness</strong>: The proposed approach is shown to be effective for learning dynamic network representations. We achieve an average gain in AUC of 11.9% across all methods and graphs from various application domains.<br/></li>    </ul>    <figure id="fig1">    <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191526/images/www18companion-265-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Dynamic network. Edges are labeled by time. Observe that <em>v</em>      <sub>4</sub>, <em>v</em>      <sub>1</sub>, <em>v</em>      <sub>2</sub> is not a valid temporal walk since <em>v</em>      <sub>1</sub>, <em>v</em>      <sub>2</sub> exists in the past with respect to <em>v</em>      <sub>4</sub>, <em>v</em>      <sub>1</sub>.</span>    </div>    </figure>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Framework</h2>    </div>    </header>    <p>This section describes the general framework for learning time-dependent embeddings from continuous-time dynamic networks.</p>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Temporal Model</h3>     </div>    </header>    <p>In this work, instead of modeling the dynamic network as a sequence of discrete snapshot graphs defined as <em>G</em>     <sub>1</sub>, &#x2026;, <em>G<sub>T</sub>     </em> where <em>G<sub>i</sub>     </em> = (<em>V</em>, <em>E<sub>t</sub>     </em>) and <em>E<sub>t</sub>     </em> are the edges active between the timespan [<em>t</em>     <sub>      <em>i</em> &#x2212; 1</sub>, <em>t<sub>i</sub>     </em>], we model the temporal interactions as a <em>continuous-time dynamic network</em> (CTDN) defined formally as:</p>    <div class="definition" id="enc1">     <Label>Definition 2.1 (Continuous-Time Dynamic Network).</Label>     <p> Given a graph <span class="inline-equation"><span class="tex">$G=(V,E_T,\mathcal {T})$</span>      </span>, let <em>V</em> be a set of vertices, and <span class="inline-equation"><span class="tex">$E_T \subseteq V \times V \times \mathbb {R}^{+}$</span>      </span> be the set of temporal edges between vertices in <em>V</em>, and <span class="inline-equation"><span class="tex">$\mathcal {T} : E \rightarrow \mathbb {R}^{+}$</span>      </span> is a function that maps each edge to a corresponding timestamp. At the finest granularity, each edge <em>e<sub>i</sub>      </em> = (<em>u</em>, <em>v</em>, <em>t</em>) &#x2208; <em>E<sub>T</sub>      </em> may be assigned a unique time <span class="inline-equation"><span class="tex">$t \in \mathbb {R}^{+}$</span>      </span>.</p>    </div>    <p>In continuous-time dynamic networks (<em>i.e.</em>, temporal networks<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>), events denoted by edges occur over a time span <span class="inline-equation"><span class="tex">$\mathcal {T} \subseteq \mathbb {T}$</span>     </span> where <span class="inline-equation"><span class="tex">$\mathbb {T}$</span>     </span> is the temporal domain. For continuous-time systems <span class="inline-equation"><span class="tex">$\mathbb {T}=\mathbb {R}^{+}$</span>     </span>. In such networks, a <em>valid</em> walk is denoted by a sequence of nodes connected by edges with non-decreasing timestamps. In other words, if each edge captures the time of contact between two entities, then a (valid temporal) walk may represent a feasible route for a piece of information. More formally,</p>    <div class="definition" id="enc2">     <Label>Definition 2.2 (Temporal Walk).</Label>     <p> A temporal walk from <em>v</em>      <sub>1</sub> to <em>v<sub>k</sub>      </em> in <em>G</em> is a sequence of vertices &#x27E8;<em>v</em>      <sub>1</sub>, <em>v</em>      <sub>2</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>v<sub>k</sub>      </em>&#x27E9; such that &#x27E8;<em>v<sub>i</sub>      </em>, <em>v</em>      <sub>       <em>i</em> + 1</sub>&#x27E9; &#x2208; <em>E<sub>T</sub>      </em> for 1 &#x2264; <em>i</em> < <em>k</em>, and <span class="inline-equation"><span class="tex">$\mathcal {T}(v_i, v_{i+1}) \le \mathcal {T}(v_{i+1}, v_{i+2})$</span>      </span> for 1 &#x2264; <em>i</em> < (<em>k</em> &#x2212; 1). For two arbitrary vertices <em>u</em>, <em>v</em> &#x2208; <em>V</em>, we say that <em>u</em> is <em>temporally connected</em> to <em>v</em> if there exists a temporal walk from <em>u</em> to <em>v</em>.</p>    </div>    <p>The definition of temporal walk echoes the standard definition of a walk in static graphs but with an additional constraint that requires the walk to respect time, that is, edges must be traversed in increasing order of edge times. As such, temporal walks are naturally asymmetric.</p>    <p>We now define the problem of learning time-respecting embeddings for continuous-time dynamic networks as follows: Given a temporal network <span class="inline-equation"><span class="tex">$G=(V,E_T,\mathcal {T})$</span>     </span>, the goal is to learn a function <span class="inline-equation"><span class="tex">$f : V \rightarrow \mathbb {R}^{D}$</span>     </span> that maps nodes in <em>G</em> to <em>D</em>-dimensional <em>time-dependent feature representations</em> suitable for a down-stream machine learning task such as temporal link prediction. The proposed continuous-time dynamic network embedding framework has two main interchangeable components that allow the user to <em>temporally bias</em> the learning of time-dependent network representations. We now describe each component in Section&#x00A0;<a class="sec" href="#sec-10">2.2</a> and&#x00A0;<a class="sec" href="#sec-13">2.3</a>.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Initial Temporal Edge Selection</h3>     </div>    </header>    <p>Given a time-continuous dynamic network <span class="inline-equation"><span class="tex">$G=(V,E_T,\mathcal {T})$</span>     </span>, how can we select a node to begin a temporal random walk? Observe that most existing methods that ignore time simply perform a fixed number of random walks from each node in the graph. However, recall that a temporal walk from <em>v</em>     <sub>1</sub> to <em>v<sub>k</sub>     </em> in <em>G</em> is a sequence of vertices &#x27E8;<em>v</em>     <sub>1</sub>, <em>v</em>     <sub>2</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>v<sub>k</sub>     </em>&#x27E9; such that &#x27E8;<em>v<sub>i</sub>     </em>, <em>v</em>     <sub>      <em>i</em> + 1</sub>&#x27E9; &#x2208; <em>E<sub>T</sub>     </em> for 1 &#x2264; <em>i</em> < <em>k</em>, and <span class="inline-equation"><span class="tex">$\mathcal {T}(v_i, v_{i+1}) \le \mathcal {T}(v_{i+1}, v_{i+2})$</span>     </span> for 1 &#x2264; <em>i</em> < (<em>k</em> &#x2212; 1). Notice that in addition to a node <em>v</em>, a <em>temporal random walk</em> requires a starting time <em>t</em>. In time-continuous dynamic networks (Definition&#x00A0;<a class="enc" href="#enc1">2.1</a>), every edge <em>e<sub>i</sub>     </em> = (<em>v</em>, <em>u</em>) &#x2208; <em>E<sub>T</sub>     </em> is associated with a time <span class="inline-equation"><span class="tex">$t = \mathcal {T}(e_i)=\mathcal {T}(v,u)$</span>     </span>. Therefore, we can either sample an initial time <em>t</em>     <sub>*</sub> from a uniform or weighted distribution <span class="inline-equation"><span class="tex">$\mathbb {F}_{s}$</span>     </span> and find the edge <em>e<sub>i</sub>     </em> closest to time <em>t</em>     <sub>*</sub> or select an initial edge <em>e<sub>i</sub>     </em> = (<em>v</em>, <em>w</em>) along with its associated time <span class="inline-equation"><span class="tex">$t_{*} = \mathcal {T}(e_i)$</span>     </span> by sampling from an arbitrary (uniform or weighted) distribution <span class="inline-equation"><span class="tex">$\mathbb {F}_{s}$</span>     </span>. The choice of where to begin the temporal random walk is used to our advantage as a way to <em>temporally bias</em> the temporal random walks and therefore improve predictive performance when the time-dependent embeddings are used on a downstream time-series regression or classification task. This is a important and fundamental difference between the proposed dynamic network embedding framework that uses temporal random walks and existing methods that use random walks on static graphs.</p>    <p>In general, each temporal walk starts from a temporal edge <em>e<sub>i</sub>     </em> &#x2208; <em>E<sub>T</sub>     </em> at time <span class="inline-equation"><span class="tex">$t=\mathcal {T}$</span>     </span> sampled from a distribution <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span>. The distribution used to select the initial temporal edge can either be uniform in which case there is no bias or the selection can be temporally biased using an arbitrary weighted (non-uniform) distribution for <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span>. For instance, to learn node embeddings for the temporal link prediction task, we may want to begin more temporal walks from edges closer to the current time point as the events/relationships in the distant past may be less predictive or indicative of the state of the system now. Selecting the initial temporal edge in an unbiased fashion is discussed in Section&#x00A0;<a class="sec" href="#sec-11">2.2.1</a> whereas strategies that temporally bias the selection of the initial edge are discussed in Section&#x00A0;<a class="sec" href="#sec-12">2.2.2</a>.</p>    <section id="sec-11">     <p><em>2.2.1 Unbiased.</em> In the case of initial edge selection, each edge <em>e<sub>i</sub>      </em> = (<em>v</em>, <em>u</em>, <em>t</em>) &#x2208; <em>E<sub>T</sub>      </em> has the same probability of being selected: <div class="table-responsive" id="eq1">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr (e) = 1 / |E_T| \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>       </div>      </div> This corresponds to sampling the initial temporal edge using a uniform distribution.</p>    </section>    <section id="sec-12">     <p><em>2.2.2 Biased.</em> We describe two techniques to temporally bias the selection of the initial edge that determines the start of the temporal random walk. In particular, we sample the initial temporal edge by sampling a temporally weighted distribution based on exponential and linear functions. However, the continuous-time dynamic network embedding framework is flexible and can easily support other temporally weighted distributions for selecting the initial temporal edge.</p>     <p>      <strong>Exponential:</strong>We can also bias initial edge selection using an exponential distribution, in which case each edge <em>e</em> &#x2208; <em>E<sub>T</sub>      </em> is assigned the probability: <div class="table-responsive" id="eq2">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr (e) = \frac{\exp \big [ \mathcal {T}(e)-t_{min}]}{\sum _{e^\prime \in E_T} \, \exp \big [ \mathcal {T}(e^\prime)-t_{min}]} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>       </div>      </div> where <em>t<sub>min</sub>      </em> is the minimum time associated with an edge in the dynamic graph. This defines a distribution that heavily favors edges appearing later in time.</p>     <p>      <strong>Linear:</strong>When the time difference between two time-wise consecutive edges is large, it can sometimes be helpful to map the edges to discrete time steps. Let <span class="inline-equation"><span class="tex">$\eta : E_T \rightarrow \mathbb {Z}^{+}$</span>      </span> be a function that sorts (in ascending order by time) the edges in the dataset. In other words <em>&#x03B7;</em> maps each edge to an index with <em>&#x03B7;</em>(<em>e</em>) = 1 for the earliest edge <em>e</em>. In this case, each edge <em>e</em> &#x2208; <em>&#x03B7;</em>(<em>E<sub>T</sub>      </em>) will be assigned the probability: <div class="table-responsive" id="eq3">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr (e) = \frac{\eta (e)}{\sum _{e^\prime \in E_T} \eta (e^\prime)} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>       </div>      </div>     </p>    </section>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Temporal Random Walk</h3>     </div>    </header>    <p>After selecting the initial edge <em>e<sub>i</sub>     </em> = (<em>u</em>, <em>v</em>, <em>t</em>) at time <em>t</em> to begin the temporal random walk (Section&#x00A0;<a class="sec" href="#sec-10">2.2</a>), how can we perform a temporal random walk starting from that edge? We define the set of temporal neighbors of a node <em>v</em> at time <em>t</em> as follows:</p>    <div class="definition" id="enc3">     <Label>Definition 2.3 (Temporal Neighborhood).</Label>     <p> The set of temporal neighbors of a node <em>v</em> at time <em>t</em> denoted as <em>&#x0393;<sub>t</sub>      </em>(<em>v</em>) are: <div class="table-responsive" id="eq4">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Gamma _t(v) = \big \lbrace (w, t^\prime) \,\, | \,\, e=(v,w, t^\prime) \in E_T \, \wedge \mathcal {T}(e) {\gt} t \big \rbrace \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>       </div>      </div>     </p>    </div>    <p>Note that it is possible for the same neighbor <em>w</em> to appear in <em>&#x0393;<sub>t</sub>     </em>(<em>v</em>) multiple times since multiple temporal edges can exist between the same pair of nodes &#x2013; for instance, two individuals may exchange multiple email messages over the course of time. The next node in a temporal random walk can then be chosen from the set <em>&#x0393;<sub>t</sub>     </em>(<em>v</em>). Here we use a second distribution <span class="inline-equation"><span class="tex">$\mathbb {F}_\Gamma$</span>     </span> to <em>temporally bias</em> the neighbor selection. Again, this distribution can either be uniform, in which case no bias is applied, or more intuitively biased to consider time. For instance, we may want to bias the sampling strategy towards walks that exhibit smaller &#x201C;in-between&#x201D; time for consecutive edges. That is, for each consecutive pair of edges (<em>u</em>, <em>v</em>, <em>t</em>), and (<em>v</em>, <em>w</em>, <em>t</em> + <em>k</em>) in the random walk, we want <em>k</em> to be small. For temporal link prediction on a dynamic social network, restricting the &#x201C;in-between&#x201D; time allows us to sample walks that do not group friends from different time periods together. As an example, if <em>k</em> is small we are likely to sample the random walk sequence (<em>v</em>     <sub>1</sub>, <em>v</em>     <sub>2</sub>, <em>t</em>), (<em>v</em>     <sub>2</sub>, <em>v</em>     <sub>3</sub>, <em>t</em> + <em>k</em>) which makes sense as <em>v</em>     <sub>1</sub> and <em>v</em>     <sub>3</sub> are more likely to know each other since <em>v</em>     <sub>2</sub> has interacted with them both recently. On the other hand, if <em>k</em> is large we are unlikely to sample the sequence. This helps to separate people that <em>v</em>     <sub>2</sub> interacted with during very different time periods (<em>e.g.</em> high-school and graduate school) as they are less likely to know each other.</p>    <section id="sec-14">     <p><em>2.3.1 Unbiased.</em> For unbiased temporal neighbor selection, given an arbitrary edge <em>e</em> = (<em>u</em>, <em>v</em>, <em>t</em>), each temporal neighbor <em>w</em> &#x2208; <em>&#x0393;<sub>t</sub>      </em>(<em>v</em>) of node <em>v</em> at time <em>t</em> has the following probability of being selected: <div class="table-responsive" id="eq5">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr (w) = 1 / |\Gamma _t(v)| \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>       </div>      </div>     </p>    </section>    <section id="sec-15">     <p><em>2.3.2 Biased.</em> We describe two techniques to bias the temporal random walks by sampling the next node in a temporal walk via temporally weighted distribution based on exponential and linear functions. However, the continuous-time dynamic network embedding framework is flexible and can easily be used with other application or domain-dependent <em>temporal bias functions</em>.</p>     <p>      <strong>Exponential:</strong>When exponential decay is used, we formulate the probability as follows. Given an arbitrary edge <em>e</em> = (<em>u</em>, <em>v</em>, <em>t</em>), each temporal neighbor <em>w</em> &#x2208; <em>&#x0393;<sub>t</sub>      </em>(<em>v</em>) has the following probability of being selected: <div class="table-responsive" id="eq6">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr (w) = \frac{\exp \!\big [ \tau (w) - \tau (v)\big ]}{\sum _{w^\prime \in \Gamma _t(v)} \exp \!\big [ \tau (w^\prime) - \tau (v) \big ]} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>       </div>      </div> Note that we abuse the notation slightly here and use <em>&#x03C4;</em> to mean the mapping to the corresponding time. This is similar to the exponentially decaying probability of consecutive contacts observed in the spread of computer viruses and worms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>].</p>     <p>      <strong>Linear:</strong>Here, we define <span class="inline-equation"><span class="tex">$\delta : V \times \mathbb {R}^{+} \rightarrow \mathbb {Z}^{+}$</span>      </span> as a function which sorts temporal neighbors in descending order time-wise. The probability of each temporal neighbor neighbor <em>w</em> &#x2208; <em>&#x0393;<sub>t</sub>      </em>(<em>v</em>) of node <em>v</em> at time <em>t</em> is then defined as: <div class="table-responsive" id="eq7">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \Pr (w) = \frac{\delta (w)}{\sum _{w^\prime \in \Gamma _t(v)} \delta (w^\prime)} \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>       </div>      </div> This distribution biases the selection towards edges that are closer in time to the current node.</p>    </section>    <section id="sec-16">     <p><em>2.3.3 Temporal context windows.</em> Since temporal walks preserve time, it is possible for a walk to run out of <em>temporally valid</em> edges to traverse. Therefore, we do not impose a strict length on the sampled temporal walks. Instead, we simply require each temporal walk to have a minimum length <em>&#x03C9;</em> (in this work, <em>&#x03C9;</em> is equivalent to the context window size for skip-gram [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0044">44</a>]). A maximum length <em>L</em> can be provided to accommodate longer walks. Hence, when generating a set of temporal walks, any temporal walk <span class="inline-equation"><span class="tex">$\mathcal {S}_{t_i}$</span>      </span> with length <span class="inline-equation"><span class="tex">$\omega \le |\mathcal {S}_{t_i}| \le L$</span>      </span> is considered valid. Given a set of temporal random walks <span class="inline-equation"><span class="tex">$\lbrace \mathcal {S}_{t_1}, \mathcal {S}_{t_2}, \cdots , \mathcal {S}_{t_k}\rbrace$</span>      </span>, we define a temporal context window count <em>&#x03B2;</em> as the total number of context windows of size <em>&#x03C9;</em> that can be derived from the set of temporal random walks. Formally, this can be written as: <div class="table-responsive" id="eq8">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \beta \, = \sum _{i=1}^{k} |\mathcal {S}_{t_i}| - \omega + 1 \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>       </div>      </div> When sampling a set of temporal walks, we typically set <em>&#x03B2;</em> to be a multiple of <em>N</em> = |<em>V</em>|.</p>    </section>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Learning Time-preserving Embeddings</h3>     </div>    </header>    <p>Given a temporal walk <span class="inline-equation"><span class="tex">$\mathcal {S}_{t}$</span>     </span>, we can now formulate the task of learning time-preserving node embeddings in a CTDN as the optimization problem: <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{align} \max _{f} \; \log \Pr \big (\,W_T = \lbrace v_{i-\omega },\cdots ,v_{i+\omega } \rbrace \setminus v_i \;|\; f(v_i) \big) \end{align} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$f : V \rightarrow \mathbb {R}^{D}$</span>     </span> is the node embedding function, <em>&#x03C9;</em> is the context window size for optimization, and <em>W<sub>T</sub>     </em> = {<em>v</em>     <sub>      <em>i</em> &#x2212; <em>&#x03C9;</em>     </sub>, &#x22C5;&#x22C5;&#x22C5;, <em>v</em>     <sub>      <em>i</em> + <em>&#x03C9;</em>     </sub>} such that <span class="inline-equation"><span class="tex">$\mathcal {T}(v_{i-\omega },v_{i-\omega +1}) {\lt} \cdots {\lt} \mathcal {T}(v_{i+\omega -1},v_{i+\omega })$</span>     </span> is an arbitrary temporal context window <em>W<sub>T</sub>     </em>&#x2286;<em>S<sub>t</sub>     </em>. We assume conditional independence between the nodes of a temporal context window when observed with respect to the source node <em>v<sub>i</sub>     </em>. That is: <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{align} \Pr \big (\,W_T | f(v_i) \big) = \prod _{v_{i+k} \in W_T} \Pr \big (v_{i+k} | f(v_i) \big) \end{align} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> Given a graph <em>G</em>, let <span class="inline-equation"><span class="tex">$\mathbb {S}$</span>     </span> be the space of all possible random walks on <em>G</em> and let <span class="inline-equation"><span class="tex">$\mathbb {S}_{T}$</span>     </span> be the space of all temporal random walks on <em>G</em>. It is straightforward to see that the space of temporal random walks <span class="inline-equation"><span class="tex">$\mathbb {S}_{T}$</span>     </span> is contained within <span class="inline-equation"><span class="tex">$\mathbb {S}$</span>     </span>, and <span class="inline-equation"><span class="tex">$\mathbb {S}_{T}$</span>     </span> represents only a tiny fraction of possible random walks in <span class="inline-equation"><span class="tex">$\mathbb {S}$</span>     </span>. Existing methods sample a set of random walks <span class="inline-equation"><span class="tex">$\mathcal {S}$</span>     </span> from <span class="inline-equation"><span class="tex">$\mathbb {S}$</span>     </span> whereas this work focuses on sampling a set of <em>temporal random walks</em>     <span class="inline-equation"><span class="tex">$\mathcal {S}_t$</span>     </span> from <span class="inline-equation"><span class="tex">$\mathbb {S}_{T} \subseteq \mathbb {S}$</span>     </span>. In general, the probability of an existing method sampling a temporal random walk from <span class="inline-equation"><span class="tex">$\mathbb {S}$</span>     </span> by chance is very small and the vast majority of random walks sampled by these methods represent sequences of events between nodes that are invalid (not possible) when time is respected. For instance, suppose each edge represents an interaction/event (<em>e.g.</em>, email, phone call, spatial proximity) between two people, then a temporal random walk may represent a feasible route for a piece of information through the dynamic network or a temporally valid pathway for the spread of an infectious disease.</p>    <p>We summarize the procedure to learn time-preserving embeddings for CTDNs in Algorithm&#x00A0;. Our procedure in Algorithm&#x00A0; generalizes the Skip-Gram architecture to continuous-time dynamic networks. However, the framework can easily be used for other deep graph models that leverage random walks (<em>e.g.</em>, &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]) as the temporal walks can serve as input vectors for neural networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191526/images/www18companion-265-fig2.jpg" class="img-responsive" alt=""       longdesc=""/>     </figure>     <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191526/images/www18companion-265-fig3.jpg" class="img-responsive" alt=""       longdesc=""/>     </figure>    </p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.5</span> Hyperparameters</h3>     </div>    </header>    <p>While other methods have a lot of hyperparameters that require tuning such as node2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], the proposed framework has only a single hyperparameter that requires tuning.</p>    <section id="sec-19">     <p><em>2.5.1 Arbitrary length walk.</em> In our work, we allow temporal walks to have arbitrary lengths which we simply restrict to be between the range [<em>&#x03C9;</em>, <em>L</em>]. We argue that arbitrary-sized walks between <em>&#x03C9;</em> and <em>L</em> allow more accurate representations of node behaviors. For instance, a walk starting at <em>u</em> can return to <em>u</em> after traversing <em>L</em> edges, showing a closed community. On the other hand, another walk starting from <em>v</em> can end immediately at minimum length <em>&#x03C9;</em> without ever going back to <em>v</em>. These are two distant cases that would be misrepresented if a fixed random walk length is imposed. Regarding the sensitivity of <em>&#x03C9;</em> and <em>L</em>, they do not affect the overall performance by a large margin for graphs used in our experiments. However, for much larger graphs, these values could be more data dependent and may be modified by the user.</p>    </section>    <section id="sec-20">     <p><em>2.5.2 Exponential base.</em> Suppose the exponential function is used to bias the temporal random walk (Eq.&#x00A0;<a class="eqn" href="#eq6">6</a>) or bias the selection of the initial edge to begin the temporal walk (Eq.&#x00A0;<a class="eqn" href="#eq2">2</a>), then we allow the user to choose the base <em>b</em> of the exponential function for the exponential distribution. In the case of initial temporal edge selection (Eq.&#x00A0;<a class="eqn" href="#eq6">6</a>), a large base <em>b</em> would cause the function to grow rapidly. Notice that if the observed temporal interactions (<em>e.g.</em> edges) in the dynamic network span a large time period, the probability of choosing one of the recent edges may be much larger than the probability to choose all other edges resulting in sampled walks that are skewed too much towards recent edges. <figure id="fig4">       <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191526/images/www18companion-265-fig4.jpg" class="img-responsive" alt="Figure 4"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Frequency of <em>temporal random walks</em> by length (ia-contact).</span>       </div>      </figure>     </p>     <div class="table-responsive" id="tab1">      <div class="table-caption">       <span class="table-number">Table 1:</span>       <span class="table-title">AUC scores for Temporal Link Prediction.</span>      </div>      <table class="table">       <tbody>       <tr>        <td style="text-align:right;">         <SmallCap>Data</SmallCap>        </td>        <td style="text-align:center;">         <strong>DeepWalk</strong>        </td>        <td style="text-align:center;">         <strong>Node2Vec</strong>        </td>        <td style="text-align:center;">         <strong>LINE</strong>        </td>        <td style="text-align:center;">         <strong>CTDNE</strong>        </td>        <td style="text-align:right;">         <strong>TemporalWalk</strong>-Overall</td>        <td style="text-align:right;">         <strong>Gain</strong>        </td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>ia-contact</SmallCap>        </td>        <td style="text-align:center;">0.845</td>        <td style="text-align:center;">0.874</td>        <td style="text-align:center;">0.736</td>        <td style="text-align:center;">         <strong>0.913</strong>        </td>        <td style="text-align:right;">         <strong>0.915</strong>        </td>        <td style="text-align:right;">10.369</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>ia-hypertext09</SmallCap>        </td>        <td style="text-align:center;">0.620</td>        <td style="text-align:center;">0.641</td>        <td style="text-align:center;">0.621</td>        <td style="text-align:center;">         <strong>0.671</strong>        </td>        <td style="text-align:right;">         <strong>0.675</strong>        </td>        <td style="text-align:right;">6.508</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>ia-enron-employees</SmallCap>        </td>        <td style="text-align:center;">0.719</td>        <td style="text-align:center;">0.759</td>        <td style="text-align:center;">0.550</td>        <td style="text-align:center;">         <strong>0.777</strong>        </td>        <td style="text-align:right;">         <strong>0.782</strong>        </td>        <td style="text-align:right;">12.999</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>ia-radoslaw-email</SmallCap>        </td>        <td style="text-align:center;">0.734</td>        <td style="text-align:center;">0.741</td>        <td style="text-align:center;">0.615</td>        <td style="text-align:center;">         <strong>0.811</strong>        </td>        <td style="text-align:right;">         <strong>0.818</strong>        </td>        <td style="text-align:right;">14.833</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>ia-email-eu</SmallCap>        </td>        <td style="text-align:center;">0.820</td>        <td style="text-align:center;">0.860</td>        <td style="text-align:center;">0.650</td>        <td style="text-align:center;">         <strong>0.890</strong>        </td>        <td style="text-align:right;">         <strong>0.900</strong>        </td>        <td style="text-align:right;">12.734</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>fb-forum</SmallCap>        </td>        <td style="text-align:center;">0.670</td>        <td style="text-align:center;">0.790</td>        <td style="text-align:center;">0.640</td>        <td style="text-align:center;">         <strong>0.826</strong>        </td>        <td style="text-align:right;">         <strong>0.840</strong>        </td>        <td style="text-align:right;">15.254</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>soc-bitcoinA</SmallCap>        </td>        <td style="text-align:center;">0.840</td>        <td style="text-align:center;">0.870</td>        <td style="text-align:center;">0.670</td>        <td style="text-align:center;">         <strong>0.891</strong>        </td>        <td style="text-align:right;">         <strong>0.890</strong>        </td>        <td style="text-align:right;">10.962</td>       </tr>       <tr>        <td style="text-align:right;">         <SmallCap>soc-wiki-elec</SmallCap>        </td>        <td style="text-align:center;">0.820</td>        <td style="text-align:center;">0.840</td>        <td style="text-align:center;">0.620</td>        <td style="text-align:center;">         <strong>0.857</strong>        </td>        <td style="text-align:right;">         <strong>0.860</strong>        </td>        <td style="text-align:right;">11.319</td>       </tr>       </tbody>      </table>     </div>    </section>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.6</span> Model variants</h3>     </div>    </header>    <p>The proposed <em>continuous-time dynamic network embedding</em> (<SmallCap>CTDNE</SmallCap>) framework has two main interchangeable components that give rise to a variety of useful models. In this section, we discuss a few of the variants we investigate in Section&#x00A0;<a class="sec" href="#sec-22">3</a>. Recall that we use a distribution <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span> to select the starting edge <em>e</em>     <sub>*</sub> of a temporal random walk (Section&#x00A0;<a class="sec" href="#sec-10">2.2</a>). Furthermore, we use another distribution <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>     </span> to bias the selection of each subsequent edge in a temporal random walk (Section&#x00A0;<a class="sec" href="#sec-13">2.3</a>). Thus, different distributions <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>     </span> can be used to bias the random walk sampling strategy. In particular, we investigated three different approaches to sample (1) the starting temporal edge <em>e</em>     <sub>*</sub> via <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span>, and (2) each subsequent edge in a temporal random walk via <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>     </span>. This gives rise to nine different model variants by taking all possible combinations of unbiased and biased distributions discussed in Section&#x00A0;<a class="sec" href="#sec-10">2.2</a> and Section&#x00A0;<a class="sec" href="#sec-13">2.3</a>.</p>    </section>   </section>   <section id="sec-22">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Experiments</h2>    </div>    </header>    <p>The experiments are designed to investigate the effectiveness of the proposed <em>continuous-time dynamic network embedding</em> (CTDNE) framework using a wide range of temporal graphs with different structural and temporal characteristics from a variety of different application domains. A summary of the dynamic networks used for evaluation and their statistics are provided in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. All networks are continuous-time dynamic networks with <span class="inline-equation"><span class="tex">$\mathbb {T} = \mathbb {R}^{+}$</span>    </span>. For these dynamic networks, the time scale of the edges is at the level of seconds or milliseconds, <em>i.e.</em>, the edge timestamps record the time an edge occurred at the level of seconds or milliseconds (finest granularity given as input). Our approach uses the finest time scale given as input. All data was obtained from NetworkRepository&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0058">58</a>].</p>    <p>In particular, we evaluate the performance of the proposed framework on the temporal link prediction task. To generate a set of labeled examples for link prediction, we first sort the edges in each graph by time (ascending) and use the first 75% for representation learning. The remaining 25% are considered as positive links and we sample an equal number of negative edges randomly. We perform link prediction on this labeled data <span class="inline-equation"><span class="tex">$\mathcal {X}$</span>    </span> of positive and negative edges.</p>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Experimental setup</h3>     </div>    </header>    <p>We evaluate the framework presented in Section&#x00A0;<a class="sec" href="#sec-8">2</a> for learning dynamic network representations against the following baseline methods: node2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], DeepWalk&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0052">52</a>], and LINE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0065">65</a>]. For node2vec, we use the same hyperparameters (<em>D</em> = 128, <em>R</em> = 10, <em>L</em> = 80, <em>&#x03C9;</em> = 10) and grid search over <em>p</em>, <em>q</em> &#x2208; {0.25, 0.50, 1, 2, 4} as mentioned in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. We use the same hyperparameters for DeepWalk but with <em>p</em> = <em>q</em> = 1 as it is a special case of node2vec. As for our method, we use <em>&#x03C9;</em> = 10, <em>L</em> = 80, <em>D</em> = 128. For LINE, we also use <em>D</em> = 128 with 2nd-order-proximity and number of samples <em>T</em> = 60 million.</p>    <p>After the embeddings are learned for each node, we follow the methodology of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] and compute the feature vector for an edge by combining the learned embedding vectors of the corresponding nodes using one of the following operations: <em>ops</em> &#x2208; {weighted-L1, weighted-L2, average, hadamard}.</p>    <p>Recall that for each dataset, we generate a labeled dataset <span class="inline-equation"><span class="tex">$\mathcal {X}$</span>     </span> for link prediction. We use logistic regression (LR) with hold-out validation of 25% on this dataset. Experiments are repeated for 10 random seed initializations and the average performance is reported. Unless otherwise mentioned, we use AUC to evaluate the models.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Dynamic network data and statistics. Let |<em>E<sub>T</sub>       </em>| = number of <em>temporal edges</em>; <span class="inline-equation"><span class="tex">$\bar{d}$</span>       </span> = average temporal node degree; and <em>d</em>       <sub>max&#x2009;</sub> = max temporal node degree.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:right;"/>       <td style="text-align:left;"/>       <td style="text-align:left;"/>       <td style="text-align:left;"/>       <td style="text-align:left;"/>       <td style="text-align:left;"/>       <td style="text-align:left;">        <strong>Temporal</strong>       </td>       <td style="text-align:left;">        <strong>Timespan</strong>       </td>       <td style="text-align:left;">        <strong>Timespan</strong>       </td>       <td style="text-align:left;">        <strong>Timespan</strong>       </td>       <td style="text-align:center;">        <strong>Timespan</strong>       </td>       <td style="text-align:left;">        <strong>Timespan</strong>       </td>       <td/>       </tr>       <tr>       <td style="text-align:right;">        <strong>Dynamic Network</strong>       </td>       <td style="text-align:left;">|<em>V</em>|</td>       <td style="text-align:left;">|<em>E<sub>u</sub>        </em>|</td>       <td style="text-align:left;">|<em>E<sub>T</sub>        </em>|</td>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\bar{d}$</span>        </span>       </td>       <td style="text-align:left;">        <em>d</em>        <sub>max&#x2009;</sub>       </td>       <td style="text-align:left;">        <strong>Granularity</strong>       </td>       <td style="text-align:left;">        <strong>sec.</strong>       </td>       <td style="text-align:left;">        <strong>min.</strong>       </td>       <td style="text-align:left;">        <strong>hours</strong>       </td>       <td style="text-align:center;">        <strong>(days)</strong>       </td>       <td style="text-align:left;">        <strong>years</strong>       </td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>ia-contact</SmallCap>       </td>       <td style="text-align:left;">274</td>       <td style="text-align:left;">2.8K</td>       <td style="text-align:left;">28.2K</td>       <td style="text-align:left;">206.2</td>       <td style="text-align:left;">2092</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">343361</td>       <td style="text-align:left;">5722.68</td>       <td style="text-align:left;">95.38</td>       <td style="text-align:center;">3.97</td>       <td style="text-align:left;">0.01</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>ia-contacts-hypertext09</SmallCap>       </td>       <td style="text-align:left;">113</td>       <td style="text-align:left;">2.4K</td>       <td style="text-align:left;">20.8K</td>       <td style="text-align:left;">368.5</td>       <td style="text-align:left;">1483</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">212340</td>       <td style="text-align:left;">3539.00</td>       <td style="text-align:left;">58.98</td>       <td style="text-align:center;">2.46</td>       <td style="text-align:left;">0.01</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>ia-enron-employees</SmallCap>       </td>       <td style="text-align:left;">151</td>       <td style="text-align:left;">2.2K</td>       <td style="text-align:left;">50.5K</td>       <td style="text-align:left;">669.8</td>       <td style="text-align:left;">5177</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">98284399</td>       <td style="text-align:left;">1638073.32</td>       <td style="text-align:left;">27301.22</td>       <td style="text-align:center;">1137.55</td>       <td style="text-align:left;">3.12</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>ia-radoslaw-email</SmallCap>       </td>       <td style="text-align:left;">167</td>       <td style="text-align:left;">5.7K</td>       <td style="text-align:left;">82.9K</td>       <td style="text-align:left;">993.1</td>       <td style="text-align:left;">9053</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">23430482</td>       <td style="text-align:left;">390508.03</td>       <td style="text-align:left;">6508.47</td>       <td style="text-align:center;">271.19</td>       <td style="text-align:left;">0.74</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>ia-email-Eu</SmallCap>       </td>       <td style="text-align:left;">986</td>       <td style="text-align:left;">24.9K</td>       <td style="text-align:left;">332.3K</td>       <td style="text-align:left;">674.1</td>       <td style="text-align:left;">10571</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">69459254</td>       <td style="text-align:left;">1157654.23</td>       <td style="text-align:left;">19294.24</td>       <td style="text-align:center;">803.93</td>       <td style="text-align:left;">2.20</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>fb-forum</SmallCap>       </td>       <td style="text-align:left;">899</td>       <td style="text-align:left;">7K</td>       <td style="text-align:left;">33.7K</td>       <td style="text-align:left;">75.0</td>       <td style="text-align:left;">1841</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">14212105</td>       <td style="text-align:left;">236868.42</td>       <td style="text-align:left;">3947.81</td>       <td style="text-align:center;">164.49</td>       <td style="text-align:left;">0.45</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>soc-sign-bitcoinA</SmallCap>       </td>       <td style="text-align:left;">3.7K</td>       <td style="text-align:left;">24.1K</td>       <td style="text-align:left;">24.1K</td>       <td style="text-align:left;">12.8</td>       <td style="text-align:left;">888</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">164246400</td>       <td style="text-align:left;">2737440.00</td>       <td style="text-align:left;">45624.00</td>       <td style="text-align:center;">1901.00</td>       <td style="text-align:left;">5.21</td>       <td style="text-align:left;"/>       </tr>       <tr>       <td style="text-align:right;">        <SmallCap>soc-wiki-elec</SmallCap>       </td>       <td style="text-align:left;">7.1K</td>       <td style="text-align:left;">103.7K</td>       <td style="text-align:left;">107K</td>       <td style="text-align:left;">30.1</td>       <td style="text-align:left;">1346</td>       <td style="text-align:left;">seconds</td>       <td style="text-align:left;">119088240</td>       <td style="text-align:left;">1984804.00</td>       <td style="text-align:left;">33080.07</td>       <td style="text-align:center;">1378.34</td>       <td style="text-align:left;">3.78</td>       <td style="text-align:left;"/>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Comparison</h3>     </div>    </header>    <p>For fair comparison we set <em>D</em> to the same value for all compared methods. Furthermore, we ensure the same amount of information is used for learning by all baseline methods. In particular, the number of temporal context windows <em>&#x03B2;</em> used is <div class="table-responsive" id="eq11">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \beta = R \times N \times (L - \omega + 1) \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div>    </p>    <p>where <em>R</em> denotes the number of walks for each node and <em>L</em> is the length of a random walk required by the baseline methods.</p>    <p>Table&#x00A0;<a class="tbl" href="#tab1">1</a> shows the performance of all the compared methods on the temporal link prediction task. For this experiment, we use the simplest variant from the proposed framework and did not apply any <em>additional bias</em> to the selection strategy. In other words, both <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>     </span> are set to the uniform distribution. We note, however, that since our temporal walks are time-obeying (by Definition&#x00A0;<a class="enc" href="#enc2">2.2</a>), the sampling is already biased towards edges that appear later in time as the random walk traversal does not go back in time. Here we see that the proposed approach performs consistently better than DeepWalk, node2vec, and LINE. This is an indication that important information is lost when temporal information is ignored. Strikingly, our model does not leverage the bias introduced by node2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], and yet it still outperforms this model by a significant margin. We could have generalized node2vec in a similar manner using the proposed framework from Section&#x00A0;<a class="sec" href="#sec-8">2</a>. Obviously, we can expect to achieve even better predictive performance from the embeddings derived using the continuous-time node2vec generalization.</p>    <p>The last column of Table&#x00A0;<a class="tbl" href="#tab1">1</a> provides the mean gain in AUC averaged over all embedding methods for each dynamic network. In all cases, the proposed approach significantly outperforms the other embedding methods across all dynamic networks. Notably, we achieve an overall gain in AUC of 11.9% across all embedding methods and graphs. These results indicate that modeling and incorporating the temporal dependencies in graphs is important for learning appropriate and meaningful network representations.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">AUC scores for different variants that were tested. Note that <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>       </span> is the distribution used for initial edge sampling and <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>       </span> is the distribution for temporal neighbor sampling. Reference to formulation in paper in parentheses.</span>     </div>     <table class="table">      <tbody>       <tr>       <td colspan="2" style="text-align:center;">        <strong>Variant</strong>        <hr/>       </td>       <td/>       <td/>       <td/>       <td/>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>        </span>       </td>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>        </span>       </td>       <td style="text-align:center;">        <SmallCap>contact</SmallCap>       </td>       <td style="text-align:left;">        <SmallCap>hyper</SmallCap>       </td>       <td style="text-align:left;">        <SmallCap>enron</SmallCap>       </td>       <td style="text-align:left;">        <SmallCap>rado</SmallCap>       </td>       </tr>       <tr>       <td style="text-align:left;">Unif (Eq.&#x00A0;<a class="eqn" href="#eq1">1</a>)</td>       <td style="text-align:left;">Unif (Eq.&#x00A0;<a class="eqn" href="#eq5">5</a>)</td>       <td style="text-align:left;">0.913</td>       <td style="text-align:left;">0.671</td>       <td style="text-align:left;">0.777</td>       <td style="text-align:left;">0.811</td>       </tr>       <tr>       <td style="text-align:left;">Unif (Eq.&#x00A0;<a class="eqn" href="#eq1">1</a>)</td>       <td style="text-align:left;">Lin (Eq.&#x00A0;<a class="eqn" href="#eq7">7</a>)</td>       <td style="text-align:left;">0.903</td>       <td style="text-align:left;">0.665</td>       <td style="text-align:left;">0.769</td>       <td style="text-align:left;">0.797</td>       </tr>       <tr>       <td style="text-align:left;">Lin (Eq.&#x00A0;<a class="eqn" href="#eq3">3</a>)</td>       <td style="text-align:left;">Unif (Eq.&#x00A0;<a class="eqn" href="#eq5">5</a>)</td>       <td style="text-align:left;">        <strong>0.915</strong>       </td>       <td style="text-align:left;">        <strong>0.675</strong>       </td>       <td style="text-align:left;">0.773</td>       <td style="text-align:left;">        <strong>0.818</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">Lin (Eq.&#x00A0;<a class="eqn" href="#eq3">3</a>)</td>       <td style="text-align:left;">Lin (Eq.&#x00A0;<a class="eqn" href="#eq7">7</a>)</td>       <td style="text-align:left;">0.903</td>       <td style="text-align:left;">0.667</td>       <td style="text-align:left;">        <strong>0.782</strong>       </td>       <td style="text-align:left;">0.806</td>       </tr>      </tbody>     </table>    </div>    <p>It is also worth noting that many other approaches that leverage random walks can also be generalized using the proposed framework&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0056">56</a>], as well as any future state-of-the-art embedding method. We also find that for many data sets, using a biased distribution (either linear or exponential) does indeed improve predictive performance in terms of AUC when compared to the uniform distribution. For others however, there is no noticeable gain in performance. This can likely be attributed to the fact that most of the dynamic networks investigated have a short time span (more than 3 years at most). Table&#x00A0;<a class="tbl" href="#tab3">3</a> provides results for a few other variants from the framework. In particular, Table&#x00A0;<a class="tbl" href="#tab3">3</a> shows the difference of applying a biased distribution to the initial edge selection strategy <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span> as well as the temporal neighbor selection <span class="inline-equation"><span class="tex">$\mathbb {F}_{\Gamma }$</span>     </span> for the temporal random walk. Interestingly, using a biased distribution for <span class="inline-equation"><span class="tex">$\mathbb {F}_s$</span>     </span> seems to improve more on the tested datasets. However, for <SmallCap>ia-enron-employees</SmallCap>, the best result can be observed when both distributions are biased.</p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Related work</h2>    </div>    </header>    <p>The node embedding problem has received considerable attention from the research community in recent years. In this problem, a low-dimensional encoding is learned for each node in a graph. The goal is to learn encodings (<em>i.e.</em> embeddings) that capture key properties about each node such as their position in the graph, or their local neighborhood structure. Since nodes that share similar properties are grouped close to each other in the embedding space, one can easily use the learned embeddings for tasks such as ranking&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>], community detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>], link prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>], and node classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0057">57</a>].</p>    <p>Many of the techniques that were initially proposed for solving the node embedding problem were based on graph factorization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>]. More recently, the skip-gram model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>] was introduced in the natural language processing domain to learn continuous vector representations for words. Inspired by skip-gram&#x0027;s success in language modeling, various methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0052">52</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0065">65</a>] have been proposed to learn node embeddings using skip-gram by treating a graph as a &#x201C;document.&#x201D; Two of the more notable methods, DeepWalk&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0052">52</a>] and node2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>], use random walks to sample an ordered sequence of nodes from a graph. The skip-gram model can then be applied to these sequences to learn node embeddings.</p>    <p>Researchers have also tackled the problem of node embedding in more complex graphs, including attributed networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], and heterogeneous networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. However, the majority of the work in the area still fail to consider graphs that evolve over time (<em>i.e.</em> temporal graphs). Notably, the framework described in this work can be used to generalize these methods for learning more appropriate time-dependent embeddings since they are based on random walks.</p>    <p>A few work have begun to explore the problem of node embedding in temporal networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. However, our work differs from previous work on several key points and is, in particular, of a more general nature. While previous work have mostly been based on using discrete snapshots of temporal networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], we propose a framework for incorporating temporal dependencies into node embeddings based on temporal random walks on a continuous representation of the temporal network. Furthermore, this work introduces a general framework that can serve as a basis for generalizing other random walk based deep learning (<em>e.g.</em>,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>]) and embedding methods (<em>e.g.</em>,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>]) for learning more appropriate time-dependent embeddings from temporal networks. In contrast, most other work has simply introduced new approaches for temporal networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] and therefore are significantly different than the problem focused on in this work which is a general framework that can be leveraged by other non-temporal approaches to lift them for modeling time-dependent networks.</p>    <p>Random walks on graphs have been studied for decades&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. The theory underlying random walks and their connection to eigenvalues and other fundamental properties of graphs are well-understood&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. Our work is also related to uniform and non-uniform random walks on graphs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. Random walks are at the heart of many important applications such as ranking&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>], community detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0054">54</a>], recommendation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], link prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>], influence modeling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], search engines&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], image segmentation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], routing in wireless sensor networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0062">62</a>], and time-series forecasting&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. These applications and techniques may also benefit from the notion of temporal random walks.</p>    <p>More recently, there has been significant research in developing network analysis and machine learning methods for modeling temporal networks. Temporal networks have been the focus of recent research including node classification in temporal networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0057">57</a>], temporal link prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], dynamic community detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], and dynamic mixed-membership role models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0059">59</a>], anomaly detection in dynamic networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0055">55</a>], influence modeling and online advertisement&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>], finding important entities in dynamic networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0049">49</a>] temporal network centrality and measures&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. For a survey on temporal network analysis, see&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>].</p>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusion</h2>    </div>    </header>    <p>We have described a general framework for incorporating temporal information into network embedding methods. The framework provides a basis for generalizing existing random walk-based embedding methods for learning dynamic (time-dependent) network embeddings from continuous-time dynamic networks. The result is a more appropriate time-dependent network representation that captures the important temporal properties of the continuous-time dynamic network. We demonstrated the effectiveness of the proposed framework and generalized dynamic network embedding method for temporal link prediction in several real-world networks. Overall, the proposed method achieves an average gain of 11.9% across all methods and graphs. Our results indicate that modeling and incorporating the temporal dependencies in graphs is important for learning appropriate and meaningful network representations. Future work will investigate using the continious-time dynamic network framework to generalize heterogeneous network embedding methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] and attributed network embedding methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], among other approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0056">56</a>].</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Charu Aggarwal and Karthik Subbian. 2014. Evolutionary network analysis: A survey. <em>      <em>CSUR</em>     </em>47, 1 (2014), 10.</li>    <li id="BibPLXBIB0002" label="[2]">Charu&#x00A0;C Aggarwal, Yao Li, Philip&#x00A0;S Yu, and Ruoming Jin. 2010. On dense pattern mining in graph streams. <em>      <em>VLDB</em>     </em>3, 1-2 (2010), 975&#x2013;984.</li>    <li id="BibPLXBIB0003" label="[3]">Charu&#x00A0;C Aggarwal, Yuchen Zhao, and S&#x00A0;Yu Philip. 2011. Outlier detection in graph streams. In <em>      <em>ICDE</em>     </em>. IEEE, 399&#x2013;409.</li>    <li id="BibPLXBIB0004" label="[4]">Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander&#x00A0;S. Smola. 2013. Distributed large-scale natural graph factorization. In <em>      <em>WWW</em>     </em>. 37&#x2013;48.</li>    <li id="BibPLXBIB0005" label="[5]">Nesreen&#x00A0;K. Ahmed, Nick Duffield, Theodore&#x00A0;L. Willke, and Ryan&#x00A0;A. Rossi. 2017. On Sampling from Massive Graph Streams. In <em>      <em>VLDB</em>     </em>. 1430&#x2013;1441.</li>    <li id="BibPLXBIB0006" label="[6]">Nesreen&#x00A0;Kamel Ahmed and Ryan&#x00A0;Anthony Rossi. 2015. Interactive Visual Graph Analytics on the Web.. In <em>      <em>ICWSM</em>     </em>. 566&#x2013;569.</li>    <li id="BibPLXBIB0007" label="[7]">Nesreen&#x00A0;K. Ahmed, Ryan&#x00A0;A. Rossi, Rong Zhou, John&#x00A0;Boaz Lee, Xiangnan Kong, Theodore&#x00A0;L. Willke, and Hoda Eldardiry. 2017. Inductive Representation Learning in Large Attributed Graphs. In <em>      <em>WiML NIPS</em>     </em>.</li>    <li id="BibPLXBIB0008" label="[8]">R. Albert, H. Jeong, and A.L. Barab&#x00E1;si. 1999. Internet: Diameter of the world-wide web. <em>      <em>Nature</em>     </em>401, 6749 (1999), 130&#x2013;131.</li>    <li id="BibPLXBIB0009" label="[9]">Mikhail Belkin and Partha Niyogi. 2002. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. <em>      <em>Neural Computation</em>     </em>15(2002), 1373&#x2013;1396.</li>    <li id="BibPLXBIB0010" label="[10]">Toine Bogers. 2010. Movie recommendation using random walks over the contextual graph. In <em>      <em>Context-Aware Recommender Systems</em>     </em>.</li>    <li id="BibPLXBIB0011" label="[11]">A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Wiener. 2000. Graph structure in the web. <em>      <em>Computer Networks</em>     </em>33, 1-6 (2000), 309&#x2013;320.</li>    <li id="BibPLXBIB0012" label="[12]">Zhuhua Cai, Dionysios Logothetis, and Georgos Siganos. 2012. Facilitating real-time graph mining. In <em>      <em>Proceedings of the Fourth International Workshop on Cloud Data Management</em>     </em>. 8.</li>    <li id="BibPLXBIB0013" label="[13]">J. Camacho, R. Guimer&#x00E0;, and L.A. Nunes&#x00A0;Amaral. 2002. Robust patterns in food web structure. <em>      <em>Physical Review Letters</em>     </em>88, 22 (2002), 228102: 1&#x2013;4.</li>    <li id="BibPLXBIB0014" label="[14]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning graph representations with global structural information. In <em>      <em>CIKM</em>     </em>. ACM, 891&#x2013;900.</li>    <li id="BibPLXBIB0015" label="[15]">Sandro Cavallari, Vincent&#x00A0;W Zheng, Hongyun Cai, Kevin Chen-Chuan Chang, and Erik Cambria. 2017. Learning community embedding with community detection and node embedding on graphs. In <em>      <em>CIKM</em>     </em>. 377&#x2013;386.</li>    <li id="BibPLXBIB0016" label="[16]">R&#x00E9;my Cazabet and Fr&#x00E9;d&#x00E9;ric Amblard. 2014. Dynamic community detection. In <em>      <em>ESNAM</em>     </em>. Springer, 404&#x2013;414.</li>    <li id="BibPLXBIB0017" label="[17]">Fan Chung. 2007. Random walks and local cuts in graphs. <em>      <em>Linear Algebra and its applications</em>     </em>423, 1 (2007), 22&#x2013;32.</li>    <li id="BibPLXBIB0018" label="[18]">Yuxiao Dong, Nitesh&#x00A0;V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks. In <em>      <em>SIGKDD</em>     </em>. 135&#x2013;144.</li>    <li id="BibPLXBIB0019" label="[19]">Daniel&#x00A0;M Dunlavy, Tamara&#x00A0;G Kolda, and Evrim Acar. 2011. Temporal link prediction using matrix and tensor factorizations. <em>      <em>TKDD</em>     </em>5, 2 (2011), 10.</li>    <li id="BibPLXBIB0020" label="[20]">J.A. Dunne, R.J. Williams, and N.D. Martinez. 2002. Food-web structure and network theory: The role of connectance and size. <em>      <em>Proceedings of the National Academy of Sciences of the United States of America</em>     </em>99, 20 (2002), 12917.</li>    <li id="BibPLXBIB0021" label="[21]">M. Faloutsos, P. Faloutsos, and C. Faloutsos. 1999. On power-law relationships of the internet topology. In <em>      <em>Proceedings of the ACM SIGCOMM International Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication</em>     </em>. 251&#x2013;262.</li>    <li id="BibPLXBIB0022" label="[22]">Wenjie Fu, Le Song, and Eric&#x00A0;P Xing. 2009. Dynamic mixed membership blockmodel for evolving networks. In <em>      <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>     </em>. 329&#x2013;336.</li>    <li id="BibPLXBIB0023" label="[23]">David&#x00A0;F. Gleich and Ryan&#x00A0;A. Rossi. 2014. A Dynamical System for PageRank with Time-Dependent Teleportation. <em>      <em>Internet Mathematics</em>     </em>(2014), 188&#x2013;217.</li>    <li id="BibPLXBIB0024" label="[24]">A. Goyal, F. Bonchi, and L.V.S. Lakshmanan. 2010. Learning influence probabilities in social networks. In <em>      <em>WSDM</em>     </em>. ACM, 241&#x2013;250.</li>    <li id="BibPLXBIB0025" label="[25]">Leo Grady. 2006. Random walks for image segmentation. <em>      <em>TPAMI</em>     </em>28, 11 (2006), 1768&#x2013;1783.</li>    <li id="BibPLXBIB0026" label="[26]">Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In <em>      <em>SIGKDD</em>     </em>. 855&#x2013;864.</li>    <li id="BibPLXBIB0027" label="[27]">Sudipto Guha and Andrew McGregor. 2012. Graph synopses, sketches, and streams: A survey. <em>      <em>VLDB</em>     </em>5, 12 (2012), 2030&#x2013;2031.</li>    <li id="BibPLXBIB0028" label="[28]">Ryohei Hisano. 2016. Semi-supervised Graph Embedding Approach to Dynamic Link Prediction. <em>      <em>arXiv preprint arXiv:1610.04351</em>     </em>(2016).</li>    <li id="BibPLXBIB0029" label="[29]">P. Holme and J. Saram&#x00E4;ki. 2012. Temporal networks. <em>      <em>Physics Reports</em>     </em> (2012).</li>    <li id="BibPLXBIB0030" label="[30]">Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates. 2006. Modeling the spread of influence on the blogosphere. In <em>      <em>WWW</em>     </em>. 22&#x2013;26.</li>    <li id="BibPLXBIB0031" label="[31]">H. Jeong, S.P. Mason, A.L. Barabasi, and Z.N. Oltvai. 2001. Lethality and centrality in protein networks. <em>      <em>arXiv preprint cond-mat/0105306</em>     </em>(2001).</li>    <li id="BibPLXBIB0032" label="[32]">H. Jeong, B. Tombor, R. Albert, Z.N. Oltvai, and A.L. Barab&#x00E1;si. 2000. The large-scale organization of metabolic networks. <em>      <em>Nature</em>     </em>407, 6804 (2000), 651&#x2013;654.</li>    <li id="BibPLXBIB0033" label="[33]">Nitin Kamra, Umang Gupta, and Yan Liu. 2017. Deep Generative Dual Memory Network for Continual Learning. <em>      <em>arXiv preprint, arXiv:1710.10368</em>     </em>(2017).</li>    <li id="BibPLXBIB0034" label="[34]">A. Kleczkowski and B.T. Grenfell. 1999. Mean-field-type equations for spread of epidemics: The small world model. <em>      <em>Physica A: Statistical Mechanics and its Applications</em>     </em>274, 1-2(1999), 355&#x2013;360.</li>    <li id="BibPLXBIB0035" label="[35]">V.E. Krebs. 2002. Mapping networks of terrorist cells. <em>      <em>Connections</em>     </em>24, 3 (2002), 43&#x2013;52.</li>    <li id="BibPLXBIB0036" label="[36]">Jean-Louis Lassez, Ryan Rossi, and Kumar Jeev. 2008. Ranking Links on the Web: Search and Surf Engines. In <em>      <em>IEA/AIE</em>     </em>. 199&#x2013;208.</li>    <li id="BibPLXBIB0037" label="[37]">John&#x00A0;Boaz Lee, Ryan Rossi, and Xiangnan Kong. 2017. Deep Graph Attention Model. In <em>      <em>arXiv:1709.06075</em>     </em>.</li>    <li id="BibPLXBIB0038" label="[38]">Lizi Liao, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua. 2017. Attributed Social Network Embedding. <em>      <em>arXiv preprint arXiv:1705.04969</em>     </em>(2017).</li>    <li id="BibPLXBIB0039" label="[39]">W. Liu and L. L&#x00FC;. 2010. Link prediction based on local random walk. <em>      <em>Europhysics Letters</em>     </em>89(2010), 58007.</li>    <li id="BibPLXBIB0040" label="[40]">L&#x00E1;szl&#x00F3; Lov&#x00E1;sz. 1993. Random walks on graphs. <em>      <em>Combinatorics</em>     </em>2(1993), 1&#x2013;46.</li>    <li id="BibPLXBIB0041" label="[41]">S. Maslov and K. Sneppen. 2002. Specificity and stability in topology of protein networks. <em>      <em>Science</em>     </em>296, 5569 (2002), 910&#x2013;913.</li>    <li id="BibPLXBIB0042" label="[42]">R.M. May and A.L. Lloyd. 2001. Infection dynamics on scale-free networks. <em>      <em>Physical Review E</em>     </em>64, 6 (2001), 66112.</li>    <li id="BibPLXBIB0043" label="[43]">A. McGovern, L. Friedland, M. Hay, B. Gallagher, A. Fast, J. Neville, and D. Jensen. 2003. Exploiting Relational Structure to Understand Publication Patterns in High-Energy Physics. <em>      <em>SIGKDD Explorations</em>     </em>5, 2 (2003), 165&#x2013;172.</li>    <li id="BibPLXBIB0044" label="[44]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In <em>      <em>ICLR Workshop</em>     </em>. 10.</li>    <li id="BibPLXBIB0045" label="[45]">C. Moore and M.E.J. Newman. 2000. Epidemics and percolation in small-world networks. <em>      <em>Physical Review E</em>     </em>61, 5 (2000), 5678&#x2013;5682.</li>    <li id="BibPLXBIB0046" label="[46]">J. Neville, O. Simsek, D. Jensen, J. Komoroske, K. Palmer, and H. Goldberg. 2005. Using relational knowledge discovery to prevent securities fraud. In <em>      <em>Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining</em>     </em>. 449&#x2013;458.</li>    <li id="BibPLXBIB0047" label="[47]">M.E.J. Newman. 2001. The structure of scientific collaboration networks. <em>      <em>Proceedings of the National Academy of Sciences</em>     </em>98, 2 (2001), 404&#x2013;409.</li>    <li id="BibPLXBIB0048" label="[48]">Andrew&#x00A0;Y Ng, Michael&#x00A0;I Jordan, and Yair Weiss. 2002. On spectral clustering: Analysis and an algorithm. In <em>      <em>NIPS</em>     </em>. 849&#x2013;856.</li>    <li id="BibPLXBIB0049" label="[49]">J. O&#x0027;Madadhain, J. Hutchins, and P. Smyth. 2005. Prediction and ranking algorithms for event-based network data. <em>      <em>SIGKDD Explorations</em>     </em>7, 2 (2005), 30.</li>    <li id="BibPLXBIB0050" label="[50]">L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. PageRank citation ranking: Bringing order to the web. <em>      <em>Stanford Tech. Report</em>     </em>(1998).</li>    <li id="BibPLXBIB0051" label="[51]">R. Pastor-Satorras and A. Vespignani. 2001. Epidemic spreading in scale-free networks. <em>      <em>Physical Review Letters</em>     </em>86, 14 (2001), 3200&#x2013;3203.</li>    <li id="BibPLXBIB0052" label="[52]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In <em>      <em>SIGKDD</em>     </em>. 701&#x2013;710.</li>    <li id="BibPLXBIB0053" label="[53]">Robert Pienta, James Abello, Minsuk Kahng, and Duen&#x00A0;Horng Chau. 2015. Scalable graph exploration and visualization: Sensemaking challenges and opportunities. In <em>      <em>BigComp</em>     </em>. 271&#x2013;278.</li>    <li id="BibPLXBIB0054" label="[54]">Pascal Pons and Matthieu Latapy. 2006. Computing communities in large networks using random walks.<em>      <em>J. Graph Alg. Appl.</em>     </em>10, 2 (2006), 191&#x2013;218.</li>    <li id="BibPLXBIB0055" label="[55]">Stephen Ranshous, Shitian Shen, Danai Koutra, Steve Harenberg, Christos Faloutsos, and Nagiza&#x00A0;F Samatova. 2015. Anomaly detection in dynamic networks: a survey. <em>      <em>Wiley Interdisc. Rev.: Comp. Stat.</em>     </em>7, 3 (2015), 223&#x2013;247.</li>    <li id="BibPLXBIB0056" label="[56]">Leonardo&#x00A0;F.R. Ribeiro, Pedro&#x00A0;H.P. Saverese, and Daniel&#x00A0;R. Figueiredo. 2017. Struc2Vec: Learning Node Representations from Structural Identity. In <em>      <em>SIGKDD</em>     </em>. 385&#x2013;394.</li>    <li id="BibPLXBIB0057" label="[57]">Ryan Rossi and Jennifer Neville. 2012. Time-evolving Relational Classification and Ensemble Methods. In <em>      <em>PAKDD</em>     </em>. 13.</li>    <li id="BibPLXBIB0058" label="[58]">Ryan&#x00A0;A. Rossi and Nesreen&#x00A0;K. Ahmed. 2015. The Network Data Repository with Interactive Graph Analytics and Visualization. In <em>      <em>AAAI</em>     </em>. 4292&#x2013;4293. <a class="link-inline force-break" href="http://networkrepository.com"      target="_blank">http://networkrepository.com</a></li>    <li id="BibPLXBIB0059" label="[59]">Ryan&#x00A0;A. Rossi, Brian Gallagher, Jennifer Neville, and Keith Henderson. 2013. Modeling Dynamic Behavior in Large Evolving Graphs. In <em>      <em>WSDM</em>     </em>. ACM, 667&#x2013;676.</li>    <li id="BibPLXBIB0060" label="[60]">Ryan&#x00A0;A. Rossi and Jennifer Neville. 2010. Modeling the Evolution of Discussion Topics and Communication to Improve Relational Classification. In <em>      <em>SIGKDD SOMA</em>     </em>. 89&#x2013;97.</li>    <li id="BibPLXBIB0061" label="[61]">Ryan&#x00A0;A. Rossi, Rong Zhou, and Nesreen&#x00A0;K. Ahmed. 2017. Deep Feature Learning for Graphs. In <em>      <em>arXiv:1704.08829</em>     </em>.</li>    <li id="BibPLXBIB0062" label="[62]">Sergio&#x00A0;D Servetto and Guillermo Barrenechea. 2002. Constrained random walks on random graphs: routing algorithms for large scale wireless sensor networks. In <em>      <em>Wireless Sensor Networks &#x0026; App.</em>     </em>12&#x2013;21.</li>    <li id="BibPLXBIB0063" label="[63]">Sucheta Soundarajan, Acar Tamersoy, Elias&#x00A0;B Khalil, Tina Eliassi-Rad, Duen&#x00A0;Horng Chau, Brian Gallagher, and Kevin Roundy. 2016. Generating graph snapshots from streaming edge data. In <em>      <em>Proceedings of the 25th International Conference Companion on World Wide Web</em>     </em>. 109&#x2013;110.</li>    <li id="BibPLXBIB0064" label="[64]">Jimeng Sun, Christos Faloutsos, Spiros Papadimitriou, and Philip&#x00A0;S Yu. 2007. GraphScope: parameter-free mining of large time-evolving graphs. In <em>      <em>SIGKDD</em>     </em>. 687&#x2013;696.</li>    <li id="BibPLXBIB0065" label="[65]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. In <em>      <em>WWW</em>     </em>. 1067&#x2013;1077.</li>    <li id="BibPLXBIB0066" label="[66]">A. Wagner and D.A. Fell. 2001. The small world inside large metabolic networks. <em>      <em>Proceedings of the Royal Society of London. Series B: Biological Sciences</em>     </em>268, 1478(2001), 1803&#x2013;1810.</li>    <li id="BibPLXBIB0067" label="[67]">D.J. Watts and S.H. Strogatz. 1998. Collective dynamics of small-world networks. <em>      <em>Nature</em>     </em>393, 6684 (1998), 440&#x2013;442.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>The terms temporal network and continuous-time dynamic network are used interchangeably.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191526">https://doi.org/10.1145/3184558.3191526</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
