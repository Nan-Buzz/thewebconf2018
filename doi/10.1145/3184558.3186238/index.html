<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Joint Label Inference in Networks Extended Abstract</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186238'>https://doi.org/10.1145/3184558.3186238</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186238'>https://w3id.org/oa/10.1145/3184558.3186238</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Joint Label Inference in Networks Extended Abstract</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Deepayan</span>     <span class="surName">Chakrabarti</span>,     University of Texas, Austin, <a href="mailto:deepay@utexas.edu">deepay@utexas.edu</a>    </div>    <div class="author">     <span class="givenName">Stanislav</span>     <span class="surName">Funiak</span>,     Cerebras Systems, <a href="mailto:stano@cerebras.net">stano@cerebras.net</a>    </div>    <div class="author">     <span class="givenName">Jonathan</span>     <span class="surName">Chang</span>,     Health Coda, <a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Sofus A.</span>     <span class="surName">Macskassy</span>,     Branch Metrics, <a href="mailto:sofmac@branch.io">sofmac@branch.io</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3186238" target="_blank">https://doi.org/10.1145/3184558.3186238</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label <em>types</em> and each label type has a large number of possible labels. Existing approaches such as Label Propagation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0027">27</a>] fail to consider interactions between the label types. Our proposed method, called <SmallCap>EdgeExplain</SmallCap>, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0004">4</a>], <SmallCap>EdgeExplain</SmallCap> outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Semi-supervised learningsettings;</strong></small> </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, and Sofus A. Macskassy. 2018. Joint Label Inference in Networks Extended Abstract. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href="https://doi.org/10.1145/3184558.3186238" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3186238</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Consider the problem of inferring multiple fields such as the hometowns, current cities, and employers of users of a social network, where users often only partially fill in their profile, if at all. Here, each user is associated with one label of each <em>label type</em> (such as hometown, employer, etc.), and the set of possible labels for each type is very high-dimensional. By predicting the profile fields, the social network can make better friend recommendations or show more relevant content. Consequently, accurate predictions can greatly improve the user experience. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186238/images/www18companion-91-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">       <em>An example graph of <em>u</em> and her friends:</em> The hometown friends of <em>u</em> coincidentally contain a subset with current city <em>C</em>&#x2032;. This swamps the group from <em>u</em>&#x2019;s actual current city <em>C</em>, causing label propagation to infer <em>C</em>&#x2032; for <em>u</em>. However, our proposed model (called <SmallCap>EdgeExplain</SmallCap>) correctly explains all friendships by setting the hometown to be <em>H</em> and current city to be <em>C</em>.</span>     </div>    </figure>    </p>    <p>Such graph-based semi-supervised learning problems have been widely studied. A standard method of label inference is label propagation and its variants&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>], which try to set the label probabilities of nodes so that friends have similar probabilities. However, label propagation assumes only a single category of relationships. It therefore fails to address the complexity of edge formation in networks, where nodes have different reasons to link to each other (see Figure&#x00A0;<a class="fig" href="#fig1">1</a> for an example). Statistical relational learning methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] build classifiers based on both node attributes and links, but [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] show that the performance of these methods is comparable to label propagation. Other powerful models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] are difficult to scale. Graph structure has been modeled using latent classes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] and latent variables&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], but with an emphasis on link prediction. More recent work on attribute inference in social networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] use variants of random walks and label propagation.</p>    <p>Our proposed method, named <SmallCap>EdgeExplain</SmallCap>, approaches the problem from a different viewpoint, using the following intuition: Two people form an edge in a social network because they share the same label for one or more label types (e.g., both went to the same college). Using this intuition, we can go beyond standard label propagation in the following way: instead of taking the graph as given, and modeling labels as items that propagate over this graph, we consider the labels as factors that can <em>explain</em> the observed graph structure. For example, the inferences for <em>u</em> made by label propagation leave <em>u</em>&#x2019;s edges from <em>C</em> completely unexplained. Our proposed method rectifies this, by trying to infer node labels such that for each edge <em>u</em> &#x223C; <em>v</em>, we can explain the existence of the edge in terms of a shared label &#x2014; <em>u</em> and <em>v</em> are friends from the same hometown, or college, or the like. While we are primarily interested in inferring labels, we note that the inferred reason for each edge can be useful by itself. For example, if a new node <em>u</em> joins a network and forms and edge with <em>v</em>, and we can infer that the reason is a shared college, we can recommend other college friends of <em>v</em> as possible new edges for <em>u</em>.</p>    <p>Our contributions are as follows.</p>    <p>    <em>Formulation:</em>We propose a probabilistic model, called <SmallCap>EdgeExplain</SmallCap>, for the social network given the labels of all nodes in the network. This model codifies the intuition of &#x201C;explaining link&#x201D; via shared labels. The model has two key properties. First, the presence or absence of a link is conditionally independent of all other nodes and edges given the labels of the two endpoints of the link. This enables distributed computation for inference, which is important for scaling to large networks. Second, labels corresponding to all the label types are jointly considered in the model.</p>    <p>    <em>Scalable inference:</em>We propose two approaches for inference under <SmallCap>EdgeExplain</SmallCap>: a relaxation labeling approach, and a variational method. Both are scalable iterative methods. The difference is that relaxation labeling works with a modified (&#x201C;relaxed&#x201D;) version of the probabilistic model, while the variational approach optimizes a lower bound of the likelihood of <SmallCap>EdgeExplain</SmallCap>. We present a comparison of these two approaches under our problem setting.</p>    <p>    <em>Analysis:</em>We present an analysis of the conditions that affect the accuracy of <SmallCap>EdgeExplain</SmallCap> relative to label propagation. In particular, we find conditions under which the inferences of label propagation (which looks at each label type independently) deviate maximally from those of <SmallCap>EdgeExplain</SmallCap> (which does joint inference).</p>    <p>    <em>Empirical evaluation:</em>We show that <SmallCap>EdgeExplain</SmallCap> is more accurate than competing baselines on several datasets, including a sample of the Facebook social network and a movie network. On Facebook, <SmallCap>EdgeExplain</SmallCap> achieves lifts of up to 120% for recall@1 and 60% for recall@3 over label propagation. Among the inference procedures for <SmallCap>EdgeExplain</SmallCap>, relaxation labeling works better than variational inference in most cases.</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> The <SmallCap>EdgeExplain</SmallCap> model</h2>    </div>    </header>    <p>Consider an undirected network <span class="inline-equation"><span class="tex">$\mathcal {G}=(V,E)$</span>    </span> with nodes <em>V</em> and edges <em>E</em>. Let <span class="inline-equation"><span class="tex">$\mathcal {T} =\lbrace t_1,\ldots ,t_{|\mathcal {T}|}\rbrace$</span>    </span> denote the set of label types. For each label type <span class="inline-equation"><span class="tex">$t\in \mathcal {T}$</span>    </span>, let <em>L</em>(<em>t</em>) denote the (high-dimensional) set of labels for that label type. Each node in the graph is associated with binary variables <em>S</em>    <sub>     <em>ut</em>&#x2113;</sub> &#x2208; {0, 1}, where <em>S</em>    <sub>     <em>ut</em>&#x2113;</sub> = 1 if node <em>u</em> &#x2208; <em>V</em> has label &#x2113; for label type <em>t</em>. Let <span class="inline-equation"><span class="tex">$S_V\subset \lbrace S_{ut\ell }\mid u\in V, t\in \mathcal {T}, \ell \in L(t)\rbrace$</span>    </span> denote the set of variables whose values are known (the &#x201C;visible&#x201D; variables). Let <em>S<sub>H</sub>    </em> denote the remaining variables (the &#x201C;hidden&#x201D; variables). Our goal is to infer the correct values of <em>S<sub>H</sub>    </em>, given <em>S<sub>V</sub>    </em> and <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>    </span>.</p>    <p>One simple approach is to apply Label Propagation separately for each label type. However, this is flawed, because it treats the label types as independent. For example, in the context of social networks, it implicitly assumes that friends tend to be similar in all respects (i.e., all label types). However, intuitively, each friendship tends to have a single reason: two people are friends because they share the same high school or college or current city, etc. Thus, friendships need to be modeled by considering all label types <em>jointly</em>.</p>    <p>We propose a probabilistic model (called <SmallCap>EdgeExplain</SmallCap>) for such networks. <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{align} P(S_V, S_H \mid \mathcal {G}) &#x0026;= \dfrac{1}{Z} \prod _{u\sim v} \underset{t\in \mathcal {T}}{\mbox{softmax}} (r(u, v, t))\end{align} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div>    <div class="table-responsive" id="eq2">     <div class="display-equation">      <span class="tex mytex">\begin{align} r(u, v, t) &#x0026; = \sum _{\ell \in L(t)} S_{ut\ell } S_{vt\ell } \end{align} </span>      <br/>      <span class="equation-number">(2)</span>     </div>    </div>    <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{align} \underset{t\in \mathcal {T}}{\mbox{softmax}}(r(u, v, t)) &#x0026;= \sigma \biggl (\alpha \sum _{t\in \mathcal {T}} r(u, v, t) + c\biggr), \end{align} </span>      <br/>      <span class="equation-number">(3)</span>     </div>    </div> where <em>Z</em> is a normalization constant, and <em>&#x03C3;</em>(<em>x</em>) = 1/(1 + <em>e</em>    <sup>&#x2212; <em>x</em>    </sup>) is the sigmoid function. Here, <em>r</em>(<em>u</em>, <em>v</em>, <em>t</em>) indicates whether a shared label type <em>t</em> is the <em>reason</em> underlying the edge <em>u</em> &#x223C; <em>v</em> (Eq.&#x00A0;<a class="eqn" href="#eq2">2</a>). Eq.&#x00A0;<a class="eqn" href="#eq1">1</a> is maximized if the softmax function achieves a high value for each edge <em>u</em> &#x223C; <em>v</em>, i.e., if each edge is &#x201C;explained.&#x201D; This is achieved if the sum <span class="inline-equation"><span class="tex">$\sum _{t\in \mathcal {T}} r(u, v, t)$</span>    </span> is relatively high, which in turn is satisfied if the product <em>S</em>    <sub>     <em>ut</em>&#x2113;</sub>    <em>S</em>    <sub>     <em>vt</em>&#x2113;</sub> is 1 for even one label &#x2113; &#x2014; in other words, when there exists any label &#x2113; that both <em>u</em> and <em>v</em> share.</p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Inference</h2>    </div>    </header>    <p>We consider two methods for parameter inference under <SmallCap>EdgeExplain</SmallCap>. The <em>relaxation labeling</em> approach, called <SmallCap>REL</SmallCap>, solves a relaxed optimization problem where the binary hidden variables <em>S</em>    <sub>     <em>ut</em>&#x2113;</sub> &#x2208; {0, 1} are replaced by real-valued variables <em>f</em>    <sub>     <em>ut</em>&#x2113;</sub> &#x2208; [0, 1]. The <em>variational</em> approach, called <SmallCap>VAR</SmallCap>, maximizes a lower bound on the likelihood of <SmallCap>EdgeExplain</SmallCap>. Both approaches can scale to large networks, and our work serves as a comparison of these two in the context of network inference.</p>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Relaxation Labeling</h3>     </div>    </header>    <p>Inference under <SmallCap>EdgeExplain</SmallCap> can be viewed as the problem of maximizing the model likelihood (Eqs.&#x00A0;<a class="eqn" href="#eq1">1</a>-<a class="eqn" href="#eq3">3</a>) over the hidden variables <em>S</em>     <sub>      <em>ut</em>&#x2113;</sub> &#x2208; {0, 1}. In the spirit of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>], we propose a relaxation in terms of a real-valued function <em>f</em>, with <em>f</em>     <sub>      <em>ut</em>&#x2113;</sub> &#x2208; [0, 1] representing the probability that <em>S</em>     <sub>      <em>ut</em>&#x2113;</sub> = 1, i.e., the probability that user <em>u</em> has label &#x2113; for label type <em>t</em>. This yields the following optimization: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathop{Maximize~}_{f} &#x0026; \sum _{u\sim v} \log \Bigl (\underset{t\in \mathcal {T}}{\mbox{softmax}} (r(u, v, t))\Bigr) \end{align} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>     <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mbox{where}\,\, r(u, v, t) &#x0026; = \sum _{\ell \in L(t)} f_{ut\ell } f_{vt\ell },\end{align} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>     <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{align} \sum _{\ell \in L(t)} f_{ut\ell } &#x0026;= 1 (\forall t\in \mathcal {T}), \quad f_{ut\ell } \ge 0. \end{align} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> The problem is not convex in <em>f</em>, but is convex in <span class="inline-equation"><span class="tex">$f_u = \lbrace f_{ut\ell } | t\in \mathcal {T}, \ell \in L(t) \rbrace$</span>     </span> if the distributions <em>f<sub>v</sub>     </em> are held fixed for all nodes <em>v</em> &#x2260; <em>u</em>. Hence, we propose an iterative algorithm to infer <em>f</em>. Given <em>f<sub>v</sub>     </em> for all <em>v</em> &#x2260; <em>u</em>, finding the optimal <em>f<sub>u</sub>     </em> corresponds to solving the following sub-problem: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \mathop{Maximize~}_{f_u} g(f_u) = \sum _{v\in \Gamma (u)} \log \Bigl (\underset{t\in \mathcal {T}}{\mbox{softmax}} (r(u, v, t))\Bigr), \] </span>       <br/>      </div>     </div> where the summation is only over the set <em>&#x0393;</em>(<em>u</em>) of the friends of <em>u</em>, and we again restrict <em>f<sub>u</sub>     </em> to be a set of <span class="inline-equation"><span class="tex">$|\mathcal {T}|$</span>     </span> probability distributions, one for each label type. We note that <em>g</em>(&#x00B7;) is convex and Lipschitz continuous with constant <em>L</em> = <em>&#x03B1;</em> &#x00B7; |<em>&#x0393;</em>(<em>u</em>)|, where |<em>&#x0393;</em>(<em>u</em>)| is the number of friends of <em>u</em>. Thus, the sub-problem can be optimally solved by projected gradient ascent. We iteratively solve such sub-problems, one for each node <em>u</em>, until we converge to a local optimum of Eq.&#x00A0;<a class="eqn" href="#eq4">4</a>.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Variational Inference</h3>     </div>    </header>    <p>From Eqs.&#x00A0;<a class="eqn" href="#eq1">1</a>-<a class="eqn" href="#eq3">3</a>, we have: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} P(S_V, S_H \mid \mathcal {G}) &#x0026;= \dfrac{1}{Z} \prod _{u\sim v} \sigma \biggl (\alpha \sum _{t\in \mathcal {T}} \sum _{\ell \in L(t)} S_{ut\ell }S_{vt\ell } + c\biggr).\end{align*} </span>       <br/>      </div>     </div> Given a fixed assignment to the visible variables <em>S<sub>V</sub>     </em> and any distribution <em>Q</em>(<em>S<sub>H</sub>     </em>) over the hidden variables, we have the inequality: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \ln P(S_V \mid \mathcal {G}) \ge -\sum _{S_H} Q(S_H) \ln \dfrac{Q(S_H)}{P(S_H,S_V \mid \mathcal {G})}. \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> We shall choose a fully factorized distribution for <em>Q</em>(<em>S<sub>H</sub>     </em>): <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ Q(S_H) = \prod _u \prod _{t\in \mathcal {T}} \prod _{\ell \in L(t)} \mu _{ut\ell }^{S_{ut\ell }}, \] </span>       <br/>      </div>     </div> where <em>&#x03BC;</em>     <sub>      <em>ut</em> &#x00B7;</sub> represents a multinomial distribution over all labels &#x2113; &#x2208; <em>L</em>(<em>t</em>) for label type <em>t</em> for user <em>u</em>; for notational convenience, we set <em>&#x03BC;</em>     <sub>      <em>ut</em>&#x2113;</sub> to 0 or 1 if the user&#x0027;s labels are known.</p>    <p>Define <em>&#x03B7;<sub>uvt</sub>     </em> = &#x2211;<sub>&#x2113; &#x2208; <em>L</em>(<em>t</em>)</sub>     <em>&#x03BC;</em>     <sub>      <em>ut</em>&#x2113;</sub>     <em>&#x03BC;</em>     <sub>      <em>vt</em>&#x2113;</sub>. Let <span class="inline-equation"><span class="tex">${\bf w} \in \lbrace 0, 1\rbrace ^{|\mathcal {T} |}$</span>     </span> represent a binary vector of length <span class="inline-equation"><span class="tex">$|\mathcal {T} |$</span>     </span>, with <em>w<sub>t</sub>     </em> being the <em>t<sup>th</sup>     </em> component and |<strong>w</strong>| the number of &#x201C;ones&#x201D;. Given the parameters <em>&#x03BC;</em>&#x2216;{<em>&#x03BC;</em>     <sub>      <em>ut</em> &#x00B7;</sub>}, the distribution <span class="inline-equation"><span class="tex">$\mu _{ut\ell }^*$</span>     </span> that maximizes the RHS of Eq.&#x00A0;<a class="eqn" href="#eq7">7</a> is given by: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;&#x0026; \mu _{ut\ell }^* &#x0026;\propto \exp \left\lbrace \sum _{\lbrace v\mid u\sim v\rbrace }\mu _{vt\ell } \sum _{\bf w} \phi _{uvt}({\bf w})\right\rbrace ,\nonumber \\\text{where}&#x0026;&#x0026; \phi _{uvt}({\bf w}) &#x0026;= \ln \left(1+e^{-(\alpha |{\bf w}| + c)}\right) (-1)^{w_t}\prod _{t^{\prime }\ne t} \kappa (w_{t^{\prime }}, \eta _{uvt^{\prime }})\nonumber \\\text{and}&#x0026;&#x0026; \kappa (w_{t^{\prime }}, \eta _{uvt^{\prime }}) &#x0026;= \eta _{uvt^{\prime }}^{w_{t^{\prime }}}(1-\eta _{uvt^{\prime }})^{1-w_{t^{\prime }}}.\nonumber\end{align*} </span>       <br/>      </div>     </div> We iterate over the nodes, applying this variational update, until convergence.</p>    </section>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Analysis</h2>    </div>    </header>    <p>To analyze <SmallCap>EdgeExplain</SmallCap>, we set up a simplified &#x201C;ego&#x201D; network <span class="inline-equation"><span class="tex">$\mathcal {G}$</span>    </span> consisting of a central node <em>u</em> surrounded by <em>N</em> friends <em>v</em>    <sub>1</sub>, &#x2026;, <em>v<sub>N</sub>    </em>. Let <span class="inline-equation"><span class="tex">$Y_u =\lbrace Y_u (t_1), \ldots , Y_u (t_{|\mathcal {T}|})\rbrace$</span>    </span> denote the labels of <em>u</em> for each of the <span class="inline-equation"><span class="tex">$|\mathcal {T}|$</span>    </span> label types. Similarly, let <em>Y<sub>i</sub>    </em> represent the vector of labels for node <em>v<sub>i</sub>    </em>. Let <span class="inline-equation"><span class="tex">$\pi (Y_u, Y_{v_1}, \ldots , Y_{v_N})$</span>    </span> denote the probability of observing these labels. Since <span class="inline-equation"><span class="tex">$\mathcal {G}_u$</span>    </span> is a tree rooted at <em>u</em>, the labels of the friends are conditionally independent given the labels of the ego: <span class="inline-equation"><span class="tex">$\pi (Y_u, Y_{v_1}, \ldots , Y_{v_N}) = \pi (Y_u)\cdot \prod \pi (Y_i\mid Y_u)$</span>    </span>.</p>    <p>We generate node labels as follows. First, the ego <em>u</em> selects her labels first according to a prior <em>&#x03C0;</em>(<em>Y<sub>u</sub>    </em>). Then, each friend <em>v<sub>i</sub>    </em> independently selects a &#x201C;reason&#x201D; for her friendship with <em>u</em> by selecting the label type <em>Z<sub>i</sub>    </em> that <em>v<sub>i</sub>    </em> shares with <em>u</em>. This shared label type is drawn according to a multinomial distribution <em>q</em>: <em>q</em>(<em>Z<sub>i</sub>    </em> = <em>t</em>) = <em>q<sub>t</sub>    </em>. Thus, <em>Y<sub>i</sub>    </em>(<em>Z<sub>i</sub>    </em>) = <em>Y<sub>u</sub>    </em>(<em>Z<sub>i</sub>    </em>). The remaining labels of <em>v<sub>i</sub>    </em> are drawn from <em>&#x03C0;</em>(<em>Y<sub>i</sub>    </em>&#x2223;<em>Y<sub>i</sub>    </em>(<em>Z<sub>i</sub>    </em>)). This construction of <em>Y<sub>u</sub>    </em> and <em>Y<sub>i</sub>    </em> ensures that there is a shared label for each edge. We now analyze the following inference problem: <em>Given the network </em>    <span class="inline-equation"><span class="tex">$\mathcal {G}_u$</span>    </span>    <em> and the labels <em>Y<sub>i</sub>     </em> of all friends <em>v<sub>i</sub>     </em>, predict the labels <em>Y<sub>u</sub>     </em> of the ego.</em>    </p>    <p>Clearly, <SmallCap>EdgeExplain</SmallCap> can choose the correct labels <em>Y<sub>u</sub>    </em> for the ego, since each friendship can explained by at least one shared label. LP fails if the ego has a label &#x2113; of type <em>t</em> (i.e., <em>Y<sub>u</sub>    </em>(<em>t</em>) = &#x2113;), but a different label &#x2113;&#x2032; &#x2260; &#x2113; of the same type <em>t</em> is shared by more friends; we will call this the event <em>&#x201C;LP fails via (<em>t</em>, &#x2113;, &#x2113;&#x2032;).&#x201D;</em> Define <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} p_{t,t^{\prime }} (\ell) &#x0026;\triangleq \pi (Y_i (t)=\ell \mid Y_u \mbox{ and } \lbrace Z_i =t^{\prime }\ne t\rbrace),\\\Delta _{t,\ell ,\ell ^{\prime }} &#x0026;\triangleq \sum _{t^{\prime }\ne t} q_{t^{\prime }} \left(p_{t,t^{\prime }} (\ell ^{\prime }) - p_{t,t^{\prime }} (\ell)\right) - q_{t}.\end{align*} </span>      <br/>     </div>    </div>    </p>    <p>    <div class="theorem" id="enc1">     <Label>Theorem 4.1.</Label>     <p> If <span class="inline-equation"><span class="tex">$\Delta _{t,\ell ,\ell ^{\prime }} {\gt}0$</span>      </span>, then for any small &#x03F5; such that <span class="inline-equation"><span class="tex">$0{\lt}\epsilon {\lt}\Delta _{t,\ell ,\ell ^{\prime }}$</span>      </span>, we have: <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\[ P(\text{LP fails via $(t, \ell , \ell ^{\prime })$}) \ge \sum _{\lbrace Y_u \mid Y_u (t)=\ell \rbrace } \pi (Y_u) \cdot \left(1 - \exp \lbrace -0.5N(\Delta _{t,\ell ,\ell ^{\prime }}-\epsilon)^2\rbrace \right). \] </span>       <br/>       </div>      </div>     </p>    </div>    </p>    <p>Thus, LP is likely to fail when <span class="inline-equation"><span class="tex">$\Delta _{t,\ell ,\ell ^{\prime }}$</span>    </span> is large. This happens when the following two conditions hold: (a)&#x00A0;label &#x2113; is somewhat less likely than &#x2113;&#x2032; in the entire population (so that <span class="inline-equation"><span class="tex">$p_{t,t^{\prime }} (\ell ^{\prime })-p_{t,t^{\prime }} (\ell){\gt}0$</span>    </span>), and (b)&#x00A0;friendships based on a shared label for label type <em>t</em> are rare (i.e., <em>q<sub>t</sub>    </em> is small and consequently <span class="inline-equation"><span class="tex">$q_{t^{\prime }}$</span>    </span> can be large).</p>    <p>Maximizing the lower bound of Thm.&#x00A0;<a class="enc" href="#enc1">4.1</a> gives us the conditions when LP is most likely to fail. When there are only two label types, and the labels for the ego and her friends follow the same marginal distribution, we can show the following.</p>    <p>    <div class="theorem" id="enc2">     <Label>Theorem 4.2.</Label>     <p> With <em>q<sub>t</sub>      </em> < 0.5, the lower bound in Theorem&#x00A0;<a class="enc" href="#enc1">4.1</a> under <SmallCap>TwoLabels</SmallCap> is maximized for <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{align*} \Delta _{t,\ell ,\ell ^{\prime }} &#x0026;= O\left(\sqrt {\dfrac{\log N}{N}}\right), &#x0026; p_{t,t^{\prime }} (\ell) &#x0026;= \dfrac{1-2q_{t}-\epsilon }{2(1-q_{t})} - O\left(\sqrt {\dfrac{\log N}{N}}\right),\end{align*} </span>       <br/>       </div>      </div>     </p>    </div>    </p>    <p>Theorem&#x00A0;<a class="enc" href="#enc2">4.2</a> demonstrates the link between the probability <span class="inline-equation"><span class="tex">$p_{t,t^{\prime }} (\ell)$</span>    </span> of a person having label &#x2113; and the probability <em>q<sub>t</sub>    </em> of forming a friendship based on a shared label of type <em>t</em>. If <span class="inline-equation"><span class="tex">$p_{t,t^{\prime }} (\ell)$</span>    </span> is too large, then it becomes very unlikely that another label &#x2113;&#x2032; can be shared by more friends than &#x2113;. Conversely, if <span class="inline-equation"><span class="tex">$p_{t,t^{\prime }} (\ell)$</span>    </span> is too small, the ego will rarely have label &#x2113;, so there will be fewer situations where LP fails. Setting <span class="inline-equation"><span class="tex">$p_{t,t^{\prime }} (\ell)\approx (1-2q_{t})/(2(1-q_{t}))$</span>    </span> achieves the optimal balance between these two.</p>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>    </div>    </header>    <p>We performed several experiments on simulated and real-world data to verify the accuracy of <SmallCap>EdgeExplain</SmallCap>. Here, we present some results based on two datasets: a snapshot of the Facebook social network, and a movie network.</p>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Evaluation on the Facebook network</h3>     </div>    </header>    <p>We performed a study on a previously collected subgraph of the Facebook social network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. <a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> This data set consists of a large number of users and their friendship edges, as well as the hometown, current city, high school, college, and employer for each user, whenever these fields are available and have their visibility set to public. The accuracy of label inference is measured via 5-fold cross-validation <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186238/images/www18companion-91-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">       <em>Accuracy of</em>        <SmallCap>EdgeExplain</SmallCap>       <em>:</em> Plots (a) and (b) show the lift of <SmallCap>EdgeExplain</SmallCap> over label propagation (LP). Increasing the number of friends <em>K</em> benefits <SmallCap>EdgeExplain</SmallCap> much more than label propagation for high school, college, and especially employer. Plots (c) and (d) compare the lift of <SmallCap>VAR</SmallCap> and <SmallCap>REL</SmallCap> over LP, for <em>K</em> = 100. <SmallCap>REL</SmallCap> is seen to outperform <SmallCap>VAR</SmallCap>, and both are better than LP.</span>      </div>     </figure>     <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186238/images/www18companion-91-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Precision@1 for various label inference methods on the movie network.</span>      </div>     </figure>    </p>    <p>Figure&#x00A0;<a class="fig" href="#fig2">2</a> shows the lift in recall achieved by <SmallCap>EdgeExplain</SmallCap> over LP. We observe similar performance of both methods for hometown and current city, but increasing improvements for high school, college, and employer. With fewer employer-based friendships, the prototypical example of Figure&#x00A0;<a class="fig" href="#fig1">1</a> would also occur frequently, with label propagation likely picking common employers of (say) hometown friends instead of the less common friendships based on the actual employer. By attempting to explain each friendship, <SmallCap>EdgeExplain</SmallCap> is able to infer the employer even under such difficult circumstances. This ability to perform well even for under-represented label types makes <SmallCap>EdgeExplain</SmallCap> particularly attractive. Plots (c) and (d) compare the two inference methods (<SmallCap>VAR</SmallCap> and <SmallCap>REL</SmallCap>) for <SmallCap>EdgeExplain</SmallCap>. While both outperform LP, <SmallCap>REL</SmallCap> is more accurate than <SmallCap>VAR</SmallCap>.</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Evaluation on a Movie Network</h3>     </div>    </header>    <p>We constructed a network of English-language movies, where two movies are connected by an edge if they have the same writer, or cinematographer, or production designer (they could share several of these label types). <a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> There are 23,921 movies and 189,828 edges. We compared LP against the inference methods for <SmallCap>EdgeExplain</SmallCap> (<SmallCap>REL</SmallCap>, <SmallCap>VAR</SmallCap>, and a combination of the two called <SmallCap>HYBRID</SmallCap>). <a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> We also tested three algorithms that have been recommended by prior work. <SmallCap>LINK</SmallCap>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] represents each node as a feature vector encoding the IDs of its neighboring nodes, and a standard classifier is used to predict labels from the feature vector. Given the size of the feature vector (23,921 binary features) and the size of the output labels (39,317 labels in total), we choose Naive Bayes as the classifier. <SmallCap>CN-SAN</SmallCap>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] assigns to each unlabeled node the most popular label of each label type among the node&#x0027;s neighbors, and iterates this process until convergence. <SmallCap>RWR-SAN</SmallCap>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] picks labels for each label type based on random walks with restarts on a combined graph that includes movie-movie and movie-label edges.</p>    <p>Figure&#x00A0;<a class="fig" href="#fig3">3</a> shows the precision@1 for different fractions of the network being labeled. All methods achieve higher accuracy for cinematographer and production designer, but lower accuracy for writer. This is because less than 11% of the edges have a shared-writer as the reason for that edge. All <SmallCap>EdgeExplain</SmallCap> inference procedures (<SmallCap>REL</SmallCap>, <SmallCap>VAR</SmallCap>, and <SmallCap>HYBRID</SmallCap>) are better than the competing methods for all label types. LP is best among the baseline methods; LINK performed poorly and is omitted from the plots.</p>    </section>   </section>   <section id="sec-14">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions</h2>    </div>    </header>    <p>We proposed the problem of jointly inferring multiple correlated label types in a large network and described the problems with existing single-label models. In common instantiations of this problem, edges are often created for a reason associated with a particular label type (e.g., in a social network, two users may link because they went to the same high school, but they did not go to the same college). We propose the <SmallCap>EdgeExplain</SmallCap> model which explicitly tries to &#x201C;explain&#x201D; the reason behind each edge in terms of at least one shared label between nodes. We presented two inference methods for <SmallCap>EdgeExplain</SmallCap>: a relaxation-labeling method and a variational approach, both of which lead to fast iterative inference that is equivalent in running time to basic label propagation. Our empirical evaluation on a large subset of the Facebook graph amply demonstrates the benefits of <SmallCap>EdgeExplain</SmallCap>, with significant improvements across a set of different label types.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Edoardo&#x00A0;M. Airoldi, David&#x00A0;M. Blei, Stephen&#x00A0;E. Fienberg, and Eric&#x00A0;P. Xing. 2008. Mixed membership stochastic blockmodels. <em>      <em>Journal of Machine Learning Research</em>     </em>9, Sep (2008), 1981&#x2013;2014.</li>    <li id="BibPLXBIB0002" label="[2]">Shumeet Baluja, Rohan Seth, D Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravichandran, and Mohamed Aly. 2008. Video suggestion and discovery for YouTube: Taking random walks through the view graph. In <em>      <em>Proceedings of the 17th International Conference on World Wide Web</em>     </em>. 895&#x2013;904.</li>    <li id="BibPLXBIB0003" label="[3]">Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. <em>      <em>Journal of Machine Learning Research</em>     </em>7, Nov (2006), 2399&#x2013;2434.</li>    <li id="BibPLXBIB0004" label="[4]">Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, and Sofus&#x00A0;A. Macskassy. 2014. Joint inference of multiple label types in large networks. In <em>      <em>Proceedings of the 31st International Conference on Machine Learning</em>     </em>. 874&#x2013;882.</li>    <li id="BibPLXBIB0005" label="[5]">D. Chakrabarti, S. Funiak, J. Chang, and S.&#x00A0;A. Macskassy. 2017. Joint Label Inference in Networks. <em>      <em>JMLR</em>     </em>18, 59 (2017).</li>    <li id="BibPLXBIB0006" label="[6]">Soumen Chakrabarti, Byron Dom, and Piotr Indyk. 1998. Enhanced hypertext categorization using hyperlinks. In <em>      <em>Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data</em>     </em>. 307&#x2013;318.</li>    <li id="BibPLXBIB0007" label="[7]">Yuxiao Dong, Yang Yang, Jie Tang, Yang Yang, and Nitesh&#x00A0;V. Chawla. 2014. Inferring User Demographics and Social Strategies in Mobile Social Networks. In <em>      <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. 15&#x2013;24.</li>    <li id="BibPLXBIB0008" label="[8]">Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer. 1999. Learning probabilistic relational models. In <em>      <em>Proceedings of the 16th International Joint Conference on Artificial Intelligence</em>     </em>. 1300&#x2013;1309.</li>    <li id="BibPLXBIB0009" label="[9]">Neil&#x00A0;Zhenqiang Gong, Ameet Talwalkar, Lester Mackey, Ling Huang, Eui Chul&#x00A0;Richard Shin, Emil Stefanov, Elaine&#x00A0;(Runting) Shi, and Dawn Song. 2014. Joint Link Prediction and Attribute Inference Using a Social-Attribute Network. <em>      <em>ACM Transactions on Intelligent Systems and Technology</em>     </em>5, 2, Article 27 (April 2014), 27:1&#x2013;27:20&#x00A0;pages.</li>    <li id="BibPLXBIB0010" label="[10]">Peter&#x00A0;D. Hoff, Adrian&#x00A0;E. Raftery, and Mark&#x00A0;S. Handcock. 2002. Latent space approaches to social network analysis. <em>      <em>J. Amer. Statist. Assoc.</em>     </em>97, 460 (2002), 1090&#x2013;1098.</li>    <li id="BibPLXBIB0011" label="[11]">Charles Kemp, Joshua&#x00A0;B. Tenenbaum, Thomas&#x00A0;L. Griffiths, Takeshi Yamada, and Naonori Ueda. 2006. Learning systems of concepts with an infinite relational model. In <em>      <em>Proceedings of the 21st National Conference on Artificial Intelligence</em>     </em>. 381&#x2013;388.</li>    <li id="BibPLXBIB0012" label="[12]">Daphne Koller and Avi Pfeffer. 1998. Probabilistic frame-based systems. In <em>      <em>Proceedings of the 15th National Conference on Artificial Intelligence</em>     </em>. 580&#x2013;587.</li>    <li id="BibPLXBIB0013" label="[13]">Qing Lu and Lise Getoor. 2003. Link-based classification. In <em>      <em>Proceedings of the 20th International Conference on Machine Learning</em>     </em>. 496&#x2013;503.</li>    <li id="BibPLXBIB0014" label="[14]">Sofus&#x00A0;Attila Macskassy and Foster Provost. 2003. A simple relational classifier. In <em>      <em>Proceedings of the Multi-Relational Data Mining Workshop at the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. 64&#x2013;76.</li>    <li id="BibPLXBIB0015" label="[15]">Sofus&#x00A0;A. Macskassy and Foster Provost. 2007. Classification in networked data: A toolkit and a univariate case study. <em>      <em>Journal of Machine Learning Research</em>     </em>8, May (2007), 935&#x2013;983.</li>    <li id="BibPLXBIB0016" label="[16]">Kurt&#x00A0;T. Miller, Thomas&#x00A0;L. Griffiths, and Michael&#x00A0;I. Jordan. 2009. Nonparametric latent feature models for link prediction. In <em>      <em>Advances in Neural Information Processing Systems 22</em>     </em>. 1276&#x2013;1284.</li>    <li id="BibPLXBIB0017" label="[17]">Jennifer Neville and David Jensen. 2007. Relational dependency networks. <em>      <em>Journal of Machine Learning Research</em>     </em>8, Mar (2007), 653&#x2013;692.</li>    <li id="BibPLXBIB0018" label="[18]">Krzysztof Nowicki and Tom A.&#x00A0;B. Snijders. 2001. Estimation and prediction for stochastic blockstructures. <em>      <em>J. Amer. Statist. Assoc.</em>     </em>96, 455 (2001), 1077&#x2013;1087.</li>    <li id="BibPLXBIB0019" label="[19]">Konstantina Palla, David&#x00A0;A. Knowles, and Zoubin Ghahramani. 2012. An infinite latent attribute model for network data. In <em>      <em>Proceedings of the 29th International Conference on Machine Learning</em>     </em>. 1607&#x2013;1614.</li>    <li id="BibPLXBIB0020" label="[20]">Partha&#x00A0;Pratim Talukdar and Koby Crammer. 2009. New regularized algorithms for transductive learning. In <em>      <em>Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases</em>     </em>. 442&#x2013;457.</li>    <li id="BibPLXBIB0021" label="[21]">Benjamin Taskar, Pieter Abbeel, and Daphne Koller. 2002. Discriminative probabilistic models for relational data. In <em>      <em>Proceedings of the 18th Conference Conference on Uncertainty in Artificial Intelligence</em>     </em>. 485&#x2013;492.</li>    <li id="BibPLXBIB0022" label="[22]">Zhao Xu, Volker Tresp, Kai Yu, and Hans-Peter Kriegel. 2006. Learning infinite hidden relational models. In <em>      <em>ICML Workshop on Open Problems in Statistical Relational Learning</em>     </em>.</li>    <li id="BibPLXBIB0023" label="[23]">Zhijun Yin, Manish Gupta, Tim Weninger, and Jiawei Han. 2010. A Unified Framework for Link Recommendation Using Random Walks. In <em>      <em>Proceedings of the 2010 International Conference on Advances in Social Networks Analysis and Mining</em>     </em>. 152&#x2013;159.</li>    <li id="BibPLXBIB0024" label="[24]">Elena Zheleva and Lise Getoor. 2009. To Join or Not to Join: The Illusion of Privacy in Social Networks with Mixed Public and Private User Profiles. In <em>      <em>Proceedings of the 18th International Conference on World Wide Web</em>     </em>. 531&#x2013;540.</li>    <li id="BibPLXBIB0025" label="[25]">Dengyong Zhou, Olivier Bousquet, Thomas&#x00A0;Navin Lal, Jason Weston, and Bernhard Sch&#x00F6;lkopf. 2003. Learning with local and global consistency. In <em>      <em>Advances in Neural Information Processing Systems 16</em>     </em>. 321&#x2013;328.</li>    <li id="BibPLXBIB0026" label="[26]">Xiaojin Zhu and Zoubin Ghahramani. 2002. <em>      <em>Learning from labeled and unlabeled data with label propagation</em>     </em>. Technical Report. Carnegie Mellon University.</li>    <li id="BibPLXBIB0027" label="[27]">Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In <em>      <em>Proceedings of the 20th International Conference on Machine Learning</em>     </em>. 912&#x2013;919.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p>Part of this work was done while the authors were employees at Facebook, Inc.</p>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>We worked on a snapshot of data, and there was no interaction with users or their experience on the site.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>We have also experimented with other label types such as director and composer. All results are qualitatively similar and exhibit the same trends.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>Details of <SmallCap>HYBRID</SmallCap> are available in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0005">5</a>].</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18 Companion, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186238">https://doi.org/10.1145/3184558.3186238</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
