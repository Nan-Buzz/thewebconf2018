<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>A Sparse Topic Model for Extracting Aspect-Specific Summaries from Online Reviews</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">A Sparse Topic Model for Extracting Aspect-Specific Summaries from Online Reviews</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author"><span class="givenName">Vineeth</span>      <span class="surName">Rakesh<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a></span>,     Arizona State University, P.O. Box 1212Tempe, AZ 85281, <a href="mailto:vrakesh@asu.edu">vrakesh@asu.edu</a>    </div>    <div class="author">     <span class="givenName">Weicong</span>     <span class="surName">Ding</span>,     Amazon, Seattle, WA, <a href="mailto:weicding@amazon.com">weicding@amazon.com</a>    </div>    <div class="author">     <span class="givenName">Aman</span>     <span class="surName">Ahuja</span>,     Virginia Tech, Arlington, VA, <a href="mailto:aahuja@vt.edu">aahuja@vt.edu</a>    </div>    <div class="author">     <span class="givenName">Nikhil</span>     <span class="surName">Rao</span>,     Amazon, San Francisco, CA, <a href="mailto:nikhilsr@amazon.com">nikhilsr@amazon.com</a>    </div>    <div class="author">     <span class="givenName">Yifan</span>     <span class="surName">Sun</span>,     Technicolor, Los Altos, CA, <a href="mailto:yifan.sun@technicolor.com">yifan.sun@technicolor.com</a>    </div>    <div class="author">     <span class="givenName">Chandan K.</span>     <span class="surName">Reddy</span>,     Virginia Tech, Arlington, VA, <a href="mailto:reddy@cs.vt.edu">reddy@cs.vt.edu</a>    </div>    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186069" target="_blank">https://doi.org/10.1145/3178876.3186069</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Online reviews have become an inevitable part of a consumer&#x0027;s decision making process, where the likelihood of purchase not only depends on the product&#x0027;s overall rating, but also on the description of its aspects. Therefore, e-commerce websites such as Amazon and Walmart constantly encourage users to write good quality reviews and categorically summarize different facets of the products. However, despite such attempts, it takes a significant effort to skim through thousands of reviews and look for answers that address the query of consumers. For example, a gamer might be interested in buying a monitor with fast refresh rates and support for Gsync and Freesync technologies, while a photographer might be interested in aspects such as color depth and accuracy. To address these challenges, in this paper, we propose a generative aspect summarization model called APSUM that is capable of providing fine-grained summaries of online reviews. To overcome the inherent problem of aspect sparsity, we impose dual constraints: (a) a spike-and-slab prior over the document-topic distribution and (b) a linguistic supervision over the word-topic distribution. Using a rigorous set of experiments, we show that the proposed model is capable of outperforming the state-of-the-art aspect summarization model over a variety of datasets and deliver intuitive fine-grained summaries that could simplify the purchase decisions of consumers.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Data mining;</strong> <strong>Information retrieval;</strong> <strong>Document topic models;</strong> <strong>Summarization;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Machine learning;</strong> <strong>Topic modeling;</strong></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Probabilistic Generative Models; Topic Models; Information Retrieval; Aspect Summarization.</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Vineeth Rakesh, Weicong Ding, Aman Ahuja, Nikhil Rao, Yifan Sun, and Chandan K. Reddy. 2018. A Sparse Topic Model for Extracting Aspect-Specific Summaries from Online Reviews. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186069" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186069</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Aspect-specific topic detection is an emerging field of research where the goal is to detect fine-grained topics from a large text corpus. For example, consider the set of reviews about the Dell Alienware 15 inch laptop shown in Figure <a class="fig" href="#fig1">1</a>. Despite being a popular model, it might not be suitable for every person since the perspective of users vary based on their requirements. For instance, a traveler might be interested in the <em>portability</em> aspect of the product, while a gamer might be interested in aspects such as <em>processor frequency, GPU</em> and <em>RAM</em> and might not give importance to the weight of the laptop. Similarly, a photographer might be interested in high-end displays with great <em>color accuracy</em> and <em>SRGB coverage</em>, while other consumers might simply look for <em>budget-friendly</em> laptop with little importance to such nitty-gritty details. In our example, the Alienware laptop is well acclaimed for its screen and color accuracy; nonetheless, it is not a portable machine. The machine also has great gaming specs, but it is not a budget-friendly laptop. With such varied strengths and weaknesses of the product, it is extremely tedious to manually browse through thousands of user reviews to selectively look for aspects that meet the user&#x0027;s requirements. This emphasizes the need for automated techniques that are capable of mining aspect-specific summaries from user reviews.</p>    <p>A brute force approach to obtain fine-grained aspects is to apply the conventional topic model such as LDA, obtain the topic clusters, and retrieve only those clusters that match the query words. Unfortunately, this technique yields poor results since aspects themselves are sub-topics within an article; hence, they can be extremely sparse. For instance, consider a set of articles about &#x201C;Global Warming&#x201D;. Let us assume that a person is interested in aspects that talk about the &#x201C;birth rate of polar bear cubs&#x201D;. Now, since global warming is a very broad topic that covers several other aspects related to the environment, the query of interest (i.e. <em>polar bear, cubs, birth</em>) is just a tiny fraction of a vast topic space. Unfortunately, the conventional topic model clusters words from a global perspective, where the query words might get mixed-up with other words from global topics. For example, one of the topic clusters for the query <em>polar bear</em> could be <em>smoke, pollution, bear, polar, north</em>, and <em>global</em>. It is quite obvious that such topic clusters do not provide any specific information about the polar bear or their cubs due to other <em>intruded</em> words. Another way of modifying the conventional topic model is to first hand-pick sentences that contain the query words and feed this subset of corpus to the model. Unsurprisingly, this method also has some serious shortcomings. First, it leads to severe sparsity of text, which hampers the performance of the LDA model. Second, by throwing away large chunks of text corpus, we lose valuable information about the query itself. Circling back to our example, the aspects <em>polar bear, cubs</em>, and <em>birth</em> can manifest in different forms such as babies, animals, mammals, creatures etc. Additionally, the description about these aspects need not confine to one single sentence; rather, they could be described over a series of sentences that need not exclusively contain the query words. Therefore, losing such valuable data will lead to incomplete word clusters that could provide very little information about the query. In summary, the classical problems associated with the LDA topic model such as (a) presence of intruded words in coherent word chains, (b) topics with very broad and generic meaning and (c) presence of random noisy words will be greatly amplified if the aforementioned approach is taken to extract fine-grained aspects. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186069/images/www2018-78-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Example of user reviews about the Dell Alienware 15 inch Laptop.</span>     </div>    </figure>    </p>    <p>To overcome these challenges, we propose an aspect summarization model called <strong>APSUM</strong> that mines fine-grained aspects for user queries by constricting the document and word topic space to create focused topics. Our goal is to design a model that captures the natural flow of a review writing process. Therefore, we start by asking the question &#x201C;How does a user write a review?&#x201D;. After observing several reviews from Amazon products and IMDB movie database, we found the following explanation to be a reasonable interpretation of a review writing process. First, a user picks an aspect of interest. Second, he thinks about a sentiment and other aspects relevant to the original aspect of interest. Third, he combines these aspects with other words to create a sentence. For example, consider the <em>review 2</em> in Figure <a class="fig" href="#fig1">1</a>; here, the user talks about an aspect called <em>screen</em> and its <em>anti-reflective</em> property (i.e., anti-reflective is a new aspect relevant to the screen) and uses the polarity (or sentiment) <em>love</em> to describe this aspect. This interpretation of the review writing process leads us to the following assumptions:</p>    <ol class="list-no-style">    <li id="list1" label="(1)"><strong>Assumption 1</strong>: <em>Every sentence is composed of a narrow range of aspects</em>. For instance, in <em>review 2</em>, we can clearly see that the sentence describes just a couple of aspects (a) the <em>screen</em> and (b) the <em>anti-reflective</em> property of the screen. Although there can be sentences with multiple aspects, we observed that a majority of them focused on a very narrow range of aspects.<br/></li>    <li id="list2" label="(2)"><strong>Assumption 2</strong>: <em>If we can detect new keywords relevant to the query aspect, these keywords can in turn be used to obtain additional aspects</em>. To understand this intuition, consider a scenario where a user wants to learn about the <em>screen</em> quality of a laptop. Now, if we can somehow detect that the word <em>4k</em> is an aspect relevant to the query screen from review 2 (Figure <a class="fig" href="#fig1">1</a>), this newly found aspect can then be used to mine other new aspects such as <em>SRGB</em> and <em>Adobe-RGB</em> from <em>review 4</em> since it contains the word <em>4k</em>. Consequently, we can cluster the words <em>screen, SRGB, 4K and Adobe-RGB</em> as potential aspect words relevant to the query <em>screen</em>.<br/></li>    </ol>    <p>The rest of this paper is organized as follows. We begin by introducing a simple aspect model called M-ASUM in Section <a class="sec" href="#sec-8">2</a> and then proceed to explain the proposed APSUM model and the generative process. In Section <a class="sec" href="#sec-10">3</a>, we explain the collapsed Gibbs sampling and derive the equation for learning the model parameters. The data collection methodology and the results of our experiments are discussed in Section <a class="sec" href="#sec-11">4</a>. Finally, we review the related works on aspect summarization in Section <a class="sec" href="#sec-19">5</a> and conclude our paper in Section <a class="sec" href="#sec-20">6</a>.</p>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Modeling Aspect Summaries</h2>    </div>    </header>    <p>We begin this section by introducing a simple model that is depicted in Figure <a class="fig" href="#fig2">2</a> (a). Unlike LDA, we do not sample a topic for every word <em>w</em>; instead, a single topic <em>z</em> is drawn for an entire sentence <em>d</em> for a document <em>M</em>. The rationale behind this formulation is to mimic our observation that the number of aspects in a sentence is extremely small. After drawing the topic, for every word <em>w</em>, we draw a variable <em>r</em>, which indicates whether a word is an aspect word or a background word. If the word <em>w</em> matches with the query <em>Q</em> or the opinion corpus <em>O</em> we set <em>r</em> to 1 since it is most likely an aspect word. Otherwise, if <em>w</em> is not found in <em>O</em> or <em>Q</em>, then we sample the relevance <em>r</em> from the binomial <em>&#x03BB;</em>. If <em>r</em> = 0, we sample the word from the background distribution <em>&#x03D5;<sup>B</sup>    </em>; if not, we sample from the word-topic distribution <em>&#x03D5;<sup>AZ</sup>    </em>. In this paper, we consider the sentiment words to be a part of the aspects and do not model them separately. This formulation closely resembles the aspect and sentiment unification model (ASUM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] without the sentiment component; hence, we term this model as the modified ASUM (or M-ASUM). Despite being simple, during our experiments, we found that this model was surprisingly good at detecting fine-grained aspects. However, it is not without its flaws. Such brute force approach of constraining the document topic space has severe effects on the smoothness of the word clusters. This is in some sense equivalent to setting the Dirichlet hyper-parameter to zero for achieving sparsity (which is not desirable). Therefore, we propose a sparse aspect summarization model called APSUM that leverages the strengths of M-ASUM, while simultaneously alleviating its weakness.</p>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Generative Process of APSUM</h3>     </div>    </header>    <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186069/images/www2018-78-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Graphical Structure of (a) the simple aspect model M-ASUM and (b) the proposed aspect summarization model APSUM.</span>     </div>    </figure>    <p>Figure <a class="fig" href="#fig2">2</a>(b) illustrates the plate notation of the APSUM model, which overcomes the shortcomings of M-ASUM using three key components: (a) a document aggregator module, (b) a spike-and-slab prior over the document-topic space and (c) a supervised conditioning over the word-topic hyperparameter. These components are explained in more detail below:</p>    <p>     <strong>Mitigating the aspect sparsity</strong>: In Section <a class="sec" href="#sec-7">1</a>, we mentioned that the reviews that correspond to a query could be extremely sparse. This essentially translates to the popular problem of the lack of word co-occurrence in short texts. Therefore, we introduce a variable <em>l</em> that acts as a <em>document aggregator</em> to overcome this issue. The generative process of the model begins by sampling <em>l</em> for each document. Now, when sampling a topic for a document <em>d</em>, we use the topic distribution of <em>l</em> instead of <em>d</em>.</p>    <p>     <strong>Constraining the document topic space</strong>: The spike and slab technique was originally introduced by Wang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] to control the navigation of topic mixtures and the word distribution in the probability simplex and later extended to include a weak smoother [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]. In our model (shown in Figure <a class="fig" href="#fig2">2</a>(b)), we incorporate this technique using the Bernoulli variable <em>c</em> which introduces the spike by turning-on (i.e., assigning it to 1) or turning-off (i.e., assigning it to 0) a particular topic <em>z</em>. The smoothing is then introduced by hyperparameters <em>&#x03B1;</em> and <em>&#x03B1;</em>&#x2032;. This ensures that the per-document (i.e., review) topic distribution <em>&#x03B8;</em> is highly concentrated over a narrow topic space, while simutaneously avoiding document-topic distribution to go ill-defined. So, after sampling <em>l</em> in the previous step, we sample the topic selector <em>c</em> for every topic <em>z</em> &#x2208; <em>K</em>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">List of notations used in this paper.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Symbol</strong>       </th>       <th style="text-align:left;">        <strong>Description</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <em>D</em> = {<em>d<sub>i</sub>        </em>}</td>       <td style="text-align:left;"> set of documents, <em>d<sub>i</sub>        </em> indicates a single document       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>V</em> = {<em>v<sub>j</sub>        </em>}</td>       <td style="text-align:left;"> set of words       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>r</em>       </td>       <td style="text-align:left;"> binary relevance variable, representing <em>r</em>=1 or <em>r</em>=0       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>C</em>       </td>       <td style="text-align:left;"> binary choice variable for <em>spiking</em> topic distributions       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>L</em> = {<em>l<sub>i</sub>        </em>}</td>       <td style="text-align:left;"> set of document aggregator       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>Z</em> = {<em>z<sub>i</sub>        </em>}</td>       <td style="text-align:left;"> set of latent topics       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>K</em>       </td>       <td style="text-align:left;"> number of topics       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>Q</em>, <em>O</em>       </td>       <td style="text-align:left;"> observed user query and opinion corpus respectively       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03B8;</em>       </td>       <td style="text-align:left;"> document-topic distribution       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03D5;<sup>AZ</sup>        </em>       </td>       <td style="text-align:left;"> aspect-topic distribution       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03D5;<sup>B</sup>        </em>       </td>       <td style="text-align:left;"> background word distribution       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03A9;</em>       </td>       <td style="text-align:left;"> document-aggregator distribution       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03A0;</em>       </td>       <td style="text-align:left;"> aggregator-topic assignment distribution       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03BB;</em>       </td>       <td style="text-align:left;"> word relevance distribution       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03B1;</em>, <em>&#x03B2;</em>, <em>&#x03B4;</em>, <em>&#x03C3;</em>       </td>       <td style="text-align:left;"> hyper-parameters of Dirichlet priors       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>&#x03B3;</em>, <em>&#x03B3;</em>&#x2032;, &#x03F5;, &#x03F5;&#x2032;</td>       <td colspan="4" style="text-align:left;"> hyper-parameters of Beta priors       </td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$n^{ZV}_{k,v^{a}}$</span>        </span>       </td>       <td colspan="4" style="text-align:left;"># words <em>v</em> assigned to topic <em>k</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$n^{V}_{v^{b}}$</span>        </span>       </td>       <td colspan="4" style="text-align:left;"># background words <em>v<sup>b</sup>        </em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$n^{LC}_{l,c}$</span>        </span>       </td>       <td colspan="4" style="text-align:left;"># times choice variable <em>c</em> is assigned to an aggreagator <em>l</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$n^{LD}_{l,d}$</span>        </span>       </td>       <td colspan="4" style="text-align:left;"># aggregator <em>l</em> assigned to a document <em>d</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$n^{ZL}_{z,l}$</span>        </span>       </td>       <td colspan="4" style="text-align:left;"># words assigned to topic <em>z</em> in aggregator <em>l</em>       </td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$n^{RV}_{r,w}$</span>        </span>       </td>       <td colspan="4" style="text-align:left;"># words <em>w</em> assigned to relevance <em>r</em> = {0, 1}       </td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Constraining the word topic space</strong>: For obtaining aspects that are focused on the user query, it is important to constrain not only the document-topic proportion, but also the word-topic proportion. To this end, we infuse some supervision into the model in the form of word correlation. In Figure <a class="fig" href="#fig2">2</a>(b), this component is shown as the observed variable <em>G</em>. The rationale behind this formulation is simple, if we know that the words <em>lens</em> and <em>zoom</em> are related, then <em>we can use this supervised information to relate the topic distribution of these two words</em>. Therefore, we introduce a <em>downstream</em> conditioning on the word-topic smoother <em>&#x03B2;</em> in the form of the variable <em>y</em>. In this way, the word-topic sparsity is naturally infused into the model, while simultaneously avoiding the problem of over-fitting. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>], the authors use a similar technique to incorporate supervision into the LDA topic model. The details of this supervision will be explained in the next section.</p>    <p>Continuing with our generative process, after drawing the topic <em>z</em>, for every word <em>w</em>, we draw a variable <em>r</em>, which indicates whether a word is an aspect word or a background word. If the word <em>w</em> matches with the query <em>Q</em> or the opinion corpus <em>O</em>, we set <em>r</em> as 1 since it is most likely to be an aspect word. Otherwise, if <em>w</em> is not found in <em>O</em> or <em>Q</em>, then we sample the relevance <em>r</em> from the multinomial <em>&#x03BB;</em>. If <em>r</em> = 0, we sample the word from the background word distribution <em>&#x03D5;<sup>B</sup>     </em>, if not we sample from the distribution <em>&#x03D5;<sup>AZ</sup>     </em>. This process is described in Algorithm 1 .</p>    </section>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Parameter Inference</h2>    </div>    </header>    <p>With the generative process of the APSUM model, we now derive the collapsed Gibbs sampler for parameter estimation. Recall the likelihood of our model is given by the following equation: <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} &#x0026;P(l,z,C,r,w | *) = \int P(l|\Omega) P(\Omega |\sigma) d\Omega \int P(C|\pi) P(\pi | \epsilon , \epsilon ^{\prime }) d\pi \\ &#x0026;\int P(r|Q, O, \lambda) P(\lambda |\gamma , \gamma ^{\prime }) d\lambda \int P(z|l, \theta) P(\theta |C, \alpha , \alpha ^{\prime }) d\theta \\ &#x0026; \int \int P(w|r, z, \phi ^{AZ}, \phi ^{B} | \beta) P(\phi ^{AZ}|\beta) P(\phi ^{B}|\beta) d\phi ^{AZ} d\phi ^{B} \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div> where * refers to the collection of all the hyper-parameters. We estimate the variables <em>C</em>, <em>l</em>, <em>r</em>, and <em>z</em> using the collapsed Gibbs sampling technique as follows.</p>    <p>We begin by sampling the topic selector <em>C</em>. The joint probability distribution of <em>&#x03C0;<sub>l</sub>    </em> and <em>C<sub>l</sub>    </em> is given by the following equation: <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} P(\pi _l, C_l | *) \propto \prod _z P(c_{l,z} | \pi _l) P(\pi _l | \epsilon , \epsilon ^{\prime }) \frac{I[B_l \in A_l] \Gamma (|A_l|\alpha + K\alpha ^{\prime })}{\Gamma (N_l + |A_l|\alpha + K\alpha ^{\prime })} \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>    </div> Here <em>A<sub>l</sub>    </em> = {<em>z</em>: ~<em>c</em>    <sub>     <em>l</em>, <em>z</em>    </sub> = 1, <em>z</em> = 1, &#x2026;, <em>K</em>} and <em>B<sub>l</sub>    </em> = {<em>z</em>: ~<em>N</em>    <sub>     <em>z</em>, <em>l</em>    </sub> > 0, <em>z</em> = 1, &#x2026;, <em>K</em>}. <em>I</em>(&#x00B7;) is the standard indicator function. By integrating out <em>&#x03C0;</em>, the binary variable <em>c</em>    <sub>     <em>l</em>, <em>z</em>    </sub> is obtained using the following equation: <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} P(c_{l,z} = 0 | *) \propto (n_{l,0}^{LC} + \epsilon ^{\prime }) \frac{I[B_l \in A_l] \Gamma (|A_l|\alpha + K\alpha ^{\prime })}{\Gamma (N_l + |A_l|\alpha + K\alpha ^{\prime })} \\ P(c_{l,z} = 1 | *) \propto (n_{l,1}^{LC} + \epsilon) \frac{I[B_l \in A_l] \Gamma (|A_l|\alpha + K\alpha ^{\prime })}{\Gamma (N_l + |A_l|\alpha + K\alpha ^{\prime })} \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>    </div>    </p>    <p>    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186069/images/www2018-78-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>Second, for each document <em>d</em>, we sample the aggregator <em>l<sub>d</sub>    </em>. However, from Figure <a class="fig" href="#fig2">2</a>(b), we see that <em>l</em> is influenced by the topic distribution <em>&#x03B8;</em>. To overcome this problem, when sampling <em>l</em>, we assume the topics <em>z</em> as a known variable. This results in the following expression: <div class="table-responsive" id="Xeq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} P(l_d = l | *) \propto \frac{n_{l,\lnot d}^{LD} + \sigma }{D - 1 + L \sigma } \frac{\prod _{z \in d} \prod _{j=1}^{N_{z,d}^{ZD}} (n_{z,l,\lnot d}^{ZL} + c_{l,z} \alpha + \alpha ^{\prime } + j -1)}{\prod _{i=1}^{N_{*,d}^{ZD}} (n_{z,l,\lnot d}^{ZL} + |A_l| \alpha + K \alpha ^{\prime } + i -1)} \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>    </div> After obtaining <em>l</em> for a document <em>d</em>, we then sample the topic <em>z</em>    <sub>     <em>d</em>, <em>n</em>    </sub> for each word <em>n</em> according to the following equation: <div class="table-responsive" id="Xeq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} P(z_{d,n} = k | Z^{\lnot (dn)}, w_{d,n} = v, *) \propto \\ (n_{k,l}^{ZL} + c_{l,k} \alpha + \alpha ^{\prime }) \frac{n_{k,v^a,\lnot (dn)}^{ZV} + \beta _{k,v^a}}{\sum _{r=1}^{V^a}(n_{k,r^a,\lnot (dn)}^{ZV} + \beta _{k,r^a})} \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(5)</span>     </div>    </div> Finally, for each word, the relevance <em>r</em>    <sub>     <em>d</em>, <em>n</em>    </sub> is sampled as follows: <div class="table-responsive" id="Xeq5">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{split} &#x0026; P(r_{d,n}=0 |R^{\lnot (dn)}, *) \propto (n_{0,v,\lnot (dn)}^{RV} + \gamma) \cdot \cfrac{n^{V}_{v^{b},\lnot {(dn)}} + \delta _{v^{b}}}{\sum _{v}^{V}(n^{V}_{v^{b}} + \delta _{v})} \\ &#x0026; P(r_{d,n}=1 |R^{\lnot (dn)}, *) \propto (n_{1,v,\lnot (dn)}^{RV} + \gamma ^{\prime }) \cdot \cfrac{n_{k,v^a,\lnot (dn)}^{ZV} + \beta _{k,v^a}}{\sum _{v}^{V}n_{k,v^a,\lnot (dn)}^{ZV} + \beta _{k,v^a}} \end{split} \end{equation} </span>      <br/>      <span class="equation-number">(6)</span>     </div>    </div>    </p>    <p>The above expression marks the end of our Gibbs sampling process and we proceed with the methodology of achieving the word-topic sparsity. It should be noted that the negative log-likelihood <em>p</em>(<em>w</em>|<em>z</em>, <em>&#x03B2;</em>) of APSUM remains similar to the LDA topic model and is defined as follows: <div class="table-responsive" id="eq2">     <div class="display-equation">      <span class="tex mytex">\begin{align} L_{\beta } &#x0026;= \sum _{z=1}^{K} [ log \Gamma (\beta _{z}+n^{k}_{z}) - log \Gamma (\beta _z)] \\ \nonumber &#x0026; + \sum _{z=1}^{K} \sum _{v}^{V} [ log \Gamma (\beta _{zv}) - log \Gamma (\beta _{zv}+n^{kv}_{zv})\end{align} </span>      <br/>      <span class="equation-number">(7)</span>     </div>    </div> Now, instead of using the symmetric prior <em>&#x03B2;</em>, we modify it using a topic dependent coefficient <em>y</em> (shown in Figure <a class="fig" href="#fig2">2</a>(b)) as follows: <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{align} log \, p(\beta) = \cfrac{-1}{2\lambda ^{2}} \Bigg [ \sum _{v,v^{\prime },z} G_{v,v^{\prime }} (y_{zv} - y_{zv^{\prime }})^{2} \Bigg ] \end{align} </span>      <br/>      <span class="equation-number">(8)</span>     </div>    </div> where <span class="inline-equation"><span class="tex">$G_{v,v^{\prime }}$</span>    </span> is the observed relationship between the words (i.e., <em>lens</em> and <em>zoom</em>). Using equations (<a class="eqn" href="#eq2">7</a>) and (<a class="eqn" href="#eq3">8</a>), the objective function is defined as follows: <div class="table-responsive" id="eq4">     <div class="display-equation">      <span class="tex mytex">\begin{align} \mathop{argmin}\limits _{y_{zv}} \, [ L_{\beta } - log p(\beta)] \end{align} </span>      <br/>      <span class="equation-number">(9)</span>     </div>    </div> From the above function, one can realize that by optimizing over <em>y</em>, we <em>dynamically change the prior <em>&#x03B2;</em> with the aid of the observed (or supervised) variable <em>G</em>    </em>.</p>    <p>    <strong>Construction of the linguistic graph</strong>     <em>G</em>: The observed variable <em>G</em> in Figure <a class="fig" href="#fig2">2</a>, is created using two kinds of supervision: (a) constructing a linguistic dependency graph using the Stanford NLP module [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] and (b) a simple entity relationship.</p>    <p>The dependency graph from the natural language processing domain provides the grammatical relationships between words to induce extraction rules. For example in the sentence &#x201C;Nikon D500 has a great lens&#x201D;, we can extract <em>lens</em> and <em>D500</em> as potential aspect words utilizing the <em>aspect-aspect</em> relationship (i.e., AA-Rel) induced by the following dependency structure: <em>lens</em> &#x2192; <strong>obj</strong> &#x2192; <em>has</em> &#x2190; <strong>subj</strong> &#x2190; <em>D</em>500. There are many such rules for aspect extraction, but in this paper, we restrict our explanation to this simple example since creating these rules is not the main focus of our work. Instead, we simply utilize existing studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and python NLP tools<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> to extract such potential aspect words. Entity extraction on the other hand is a simple process and we use the same python module to <em>extract just the entities from the review sentences</em> and treat them as potential aspect words.</p>    <div class="table-responsive" id="tab2">    <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Characteristics of the Review Dataset.</span>    </div>    <table class="table">     <thead>      <tr>       <th style="text-align:center;">Reviews</th>       <th style="text-align:center;">#Docs</th>       <th style="text-align:center;">Source</th>       <th style="text-align:center;">Queries</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:center;">Restr</td>       <td style="text-align:center;">254</td>       <td style="text-align:center;">SEMVAL</td>       <td style="text-align:center;">       <em>wine, sushi, service, pizza</em>       </td>      </tr>      <tr>       <td style="text-align:center;">Hobbit</td>       <td style="text-align:center;">1k</td>       <td style="text-align:center;">IMDB</td>       <td style="text-align:center;">       <em>jackson, smaug, legolas, dwarf</em>       </td>      </tr>      <tr>       <td style="text-align:center;">CivilWar</td>       <td style="text-align:center;">1.3k</td>       <td style="text-align:center;">IMDB</td>       <td style="text-align:center;">       <em>panther, spider-man, plot, fight</em>       </td>      </tr>      <tr>       <td style="text-align:center;">Camera</td>       <td style="text-align:center;">5k</td>       <td style="text-align:center;">Amazon</td>       <td style="text-align:center;">       <em>picture, lens, battery, autofocus</em>       </td>      </tr>      <tr>       <td style="text-align:center;">HomTh</td>       <td style="text-align:center;">5k</td>       <td style="text-align:center;">Amazon</td>       <td style="text-align:center;">       <em>wireless, woofer, pandora,vizio</em>       </td>      </tr>     </tbody>    </table>    </div>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>    </div>    </header>    <p>In this section, we perform a rigorous series of quantitative and qualitative experiments over various datasets and test cases to evaluate the proposed model. Usually the document-topic and word-topic hyperparameters in topic models are set to 0.1 and 0.01 respectively; therefore, we follow the same convention. The model-specific hyperparameters <em>&#x03B7;</em>, <em>&#x03C3;</em>, <em>&#x03BB;</em> are set to 0.1, <em>&#x03B2;</em> = 0.01, <em>&#x03B4;</em> = 0.001, and the weak smoother <em>&#x03BB;</em>&#x2032; = 0.00001. These values are decided based on trial and error method after performing some initial experiments and manually judging the quality of the aspects produced by the APSUM model. The number of topics are varied between 100 &#x2212; 150 and the aggregator variable <em>l</em> is also varied between 150 &#x2212; 250 depending on sparsity of the query. The iteration count is set to 250 and the optimization over the word smoothing coefficient <em>y</em> is performed after a burn-in period of 50 iterations. The implementation of M-ASUM and APSUM models can be downloaded from our Github repository<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>.</p>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets Used</h3>     </div>    </header>    <p>For our experiments, we obtained datasets from three different domains. The information about these datasets are detailed as follows:</p>    <ol class="list-no-style">     <li id="list3" label="(a)"><strong>Restaurant reviews from SEMVAL:</strong> Semantic Evaluation (SEMVAL) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0024">24</a>] is a popular workshop on evaluations of computational semantic analysis systems, which provides annotated datasets for various information retrieval problems. For our experiments, we use the restaurants review data from the <em>Sentiment track, Task 12</em> of SEMVAL 2015 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0022">22</a>].<br/></li>     <li id="list4" label="(b)"><strong>Movie reviews from IMDB:</strong> We used IMDB&#x0027;s python API<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a> to crawl movie reviews from the internet movie database. For this paper, we select the following movie reviews: (a) <em>Hobbit: The Desolation of Smaug</em>, and (b) <em>Captain America: Civil War.</em>      <br/></li>     <li id="list5" label="(c)"><strong>Product reviews from Amazon:</strong> The 50 domain online review dataset from Amazon is another text corpus that is popular with the information retrieval community [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>]. We evaluate our model on two product categories, namely <em>Camera</em> and <em>Home Theatre</em>.<br/></li>    </ol>    <p>For each dataset, we select four different queries to measure the performance of the APSUM model. The basic statistics and the queries for each dataset are summarized in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. We adopt the conventional pre-processing steps which includes tokenization, removing stop words, lemmatization and removing vocabularies with word count fewer than five words.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Performance comparison of ASUM in terms of the precision scores.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">        <strong>Dataset</strong>       </th>       <th style="text-align:center;">        <strong>Aspects</strong>       </th>       <th colspan="3" style="text-align:center;">        <strong>LDA</strong>       </th>       <th colspan="3" style="text-align:center;">        <strong>MG-LDA</strong>       </th>       <th colspan="3" style="text-align:center;">        <strong>M-ASUM</strong>       </th>       <th colspan="3" style="text-align:center;">        <strong>TTM</strong>       </th>       <th colspan="3" style="text-align:center;">        <strong>APSUM</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Restr</td>       <td style="text-align:center;"/>       <td style="text-align:center;">p@5</td>       <td style="text-align:center;">p@10</td>       <td style="text-align:center;">p@20</td>       <td style="text-align:center;">p@5</td>       <td style="text-align:center;">p@10</td>       <td style="text-align:center;">p@20</td>       <td style="text-align:center;">p@5</td>       <td style="text-align:center;">p@10</td>       <td style="text-align:center;">p@20</td>       <td style="text-align:center;">p@5</td>       <td style="text-align:center;">p@10</td>       <td style="text-align:center;">p@20</td>       <td style="text-align:center;">p@5</td>       <td style="text-align:center;">p@10</td>       <td style="text-align:center;">p@20</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">wine</td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">0.3</td>       <td style="text-align:center;">0.13</td>       <td style="text-align:center;">0.57</td>       <td style="text-align:center;">0.43</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.58</td>       <td style="text-align:center;">0.56</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.38</td>       <td style="text-align:center;">0.76</td>       <td style="text-align:center;">0.72</td>       <td style="text-align:center;">0.41</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">sushi</td>       <td style="text-align:center;">0.34</td>       <td style="text-align:center;">0.31</td>       <td style="text-align:center;">0.15</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.53</td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.35</td>       <td style="text-align:center;">0.57</td>       <td style="text-align:center;">0.52</td>       <td style="text-align:center;">0.29</td>       <td style="text-align:center;">0.63</td>       <td style="text-align:center;">0.57</td>       <td style="text-align:center;">0.3</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">service</td>       <td style="text-align:center;">0.29</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.1</td>       <td style="text-align:center;">0.59</td>       <td style="text-align:center;">0.43</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.52</td>       <td style="text-align:center;">0.49</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.53</td>       <td style="text-align:center;">0.33</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.63</td>       <td style="text-align:center;">0.31</td>       </tr>       <tr>       <td style="text-align:center;">Hobbit</td>       <td style="text-align:center;">jackson</td>       <td style="text-align:center;">0.46</td>       <td style="text-align:center;">0.35</td>       <td style="text-align:center;">0.19</td>       <td style="text-align:center;">0.49</td>       <td style="text-align:center;">0.48</td>       <td style="text-align:center;">0.19</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.42</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.56</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.46</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">smaug</td>       <td style="text-align:center;">0.51</td>       <td style="text-align:center;">0.45</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.63</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.73</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">0.31</td>       <td style="text-align:center;">0.76</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.42</td>       <td style="text-align:center;">0.88</td>       <td style="text-align:center;">0.83</td>       <td style="text-align:center;">0.49</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">sauron</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">0.2</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.59</td>       <td style="text-align:center;">0.39</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.35</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.4</td>       <td style="text-align:center;">0.79</td>       <td style="text-align:center;">0.77</td>       <td style="text-align:center;">0.43</td>       </tr>       <tr>       <td style="text-align:center;">Civil War</td>       <td style="text-align:center;">panther</td>       <td style="text-align:center;">0.56</td>       <td style="text-align:center;">0.44</td>       <td style="text-align:center;">0.24</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">0.51</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">0.55</td>       <td style="text-align:center;">0.38</td>       <td style="text-align:center;">0.81</td>       <td style="text-align:center;">0.77</td>       <td style="text-align:center;">0.45</td>       <td style="text-align:center;">0.85</td>       <td style="text-align:center;">0.79</td>       <td style="text-align:center;">0.46</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">Spider-man</td>       <td style="text-align:center;">0.48</td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">0.19</td>       <td style="text-align:center;">0.73</td>       <td style="text-align:center;">0.67</td>       <td style="text-align:center;">0.34</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.68</td>       <td style="text-align:center;">0.39</td>       <td style="text-align:center;">0.79</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.44</td>       <td style="text-align:center;">0.82</td>       <td style="text-align:center;">0.79</td>       <td style="text-align:center;">0.4</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">plot</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.37</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.52</td>       <td style="text-align:center;">0.26</td>       <td style="text-align:center;">0.68</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.31</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.38</td>       <td style="text-align:center;">0.73</td>       <td style="text-align:center;">0.67</td>       <td style="text-align:center;">0.43</td>       </tr>       <tr>       <td style="text-align:center;">Camera</td>       <td style="text-align:center;">picture</td>       <td style="text-align:center;">0.53</td>       <td style="text-align:center;">0.46</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.58</td>       <td style="text-align:center;">0.25</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.55</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.77</td>       <td style="text-align:center;">0.7</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.77</td>       <td style="text-align:center;">0.73</td>       <td style="text-align:center;">0.44</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">autofocus</td>       <td style="text-align:center;">0.25</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.11</td>       <td style="text-align:center;">0.58</td>       <td style="text-align:center;">0.48</td>       <td style="text-align:center;">0.28</td>       <td style="text-align:center;">0.6</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.35</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.59</td>       <td style="text-align:center;">0.35</td>       <td style="text-align:center;">0.7</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.38</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">lens</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.23</td>       <td style="text-align:center;">0.09</td>       <td style="text-align:center;">0.6</td>       <td style="text-align:center;">0.51</td>       <td style="text-align:center;">0.25</td>       <td style="text-align:center;">0.63</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.31</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.38</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.38</td>       </tr>       <tr>       <td style="text-align:center;">HomTh</td>       <td style="text-align:center;">wireless</td>       <td style="text-align:center;">0.19</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.1</td>       <td style="text-align:center;">0.58</td>       <td style="text-align:center;">0.46</td>       <td style="text-align:center;">0.22</td>       <td style="text-align:center;">0.51</td>       <td style="text-align:center;">0.45</td>       <td style="text-align:center;">0.3</td>       <td style="text-align:center;">0.58</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.32</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.55</td>       <td style="text-align:center;">0.4</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">woofer</td>       <td style="text-align:center;">0.26</td>       <td style="text-align:center;">0.21</td>       <td style="text-align:center;">0.12</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.53</td>       <td style="text-align:center;">0.31</td>       <td style="text-align:center;">0.67</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.32</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.4</td>       <td style="text-align:center;">0.73</td>       <td style="text-align:center;">0.64</td>       <td style="text-align:center;">0.41</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">pandora</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.15</td>       <td style="text-align:center;">0.08</td>       <td style="text-align:center;">0.51</td>       <td style="text-align:center;">0.47</td>       <td style="text-align:center;">0.29</td>       <td style="text-align:center;">0.49</td>       <td style="text-align:center;">0.42</td>       <td style="text-align:center;">0.28</td>       <td style="text-align:center;">0.49</td>       <td style="text-align:center;">0.35</td>       <td style="text-align:center;">0.25</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.41</td>       <td style="text-align:center;">0.28</td>       </tr>       <tr>       <td colspan="2" style="text-align:center;">average score       </td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">0.3</td>       <td style="text-align:center;">0.15</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.52</td>       <td style="text-align:center;">0.28</td>       <td style="text-align:center;">0.61</td>       <td style="text-align:center;">0.57</td>       <td style="text-align:center;">0.32</td>       <td style="text-align:center;">0.67</td>       <td style="text-align:center;">0.6</td>       <td style="text-align:center;">0.37</td>       <td style="text-align:center;">0.73</td>       <td style="text-align:center;">0.67</td>       <td style="text-align:center;">0.4</td>       </tr>       <tr>       <td colspan="2" style="text-align:center;">        <strong>APSUM performance gain</strong>       </td>       <td style="text-align:center;">        <strong>0.37</strong>       </td>       <td style="text-align:center;">        <strong>0.37</strong>       </td>       <td style="text-align:center;">        <strong>0.25</strong>       </td>       <td style="text-align:center;">        <strong>0.11</strong>       </td>       <td style="text-align:center;">        <strong>0.15</strong>       </td>       <td style="text-align:center;">        <strong>0.12</strong>       </td>       <td style="text-align:center;">        <strong>0.12</strong>       </td>       <td style="text-align:center;">        <strong>0.1</strong>       </td>       <td style="text-align:center;">        <strong>0.08</strong>       </td>       <td style="text-align:center;">        <strong>0.06</strong>       </td>       <td style="text-align:center;">        <strong>0.07</strong>       </td>       <td style="text-align:center;">        <strong>0.03</strong>       </td>       <td style="text-align:left;"/>       <td style="text-align:left;"/>       <td style="text-align:left;"/>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Comparison Methods</h3>     </div>    </header>    <p>We compare the performance of the proposed model with the following baseline methods:</p>    <ol class="list-no-style">     <li id="list6" label="(1)"><strong>LDA:</strong> Our first candidate for comparison is the classic Latent Dirichlet Allocation (LDA) model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>]. For every dataset, we run the LDA model by setting the hyperparameters <em>&#x03B1;</em> as 0.1 and <em>&#x03B2;</em> as 0.01 and the number of topics as 70. The resulting topic clusters are then manually evaluated to see whether the word clusters are relevant to the target aspect (or query).<br/></li>     <li id="list7" label="(2)"><strong>MG-LDA:</strong> Proposed by Titov et. al., the multi-grain model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0025">25</a>] is one of the popular works on detecting aspect-specific topics from online reviews where aspect granularity is achieved by modeling both global and location topic distribution. The MG-LDA uses four hyperparameters <em>&#x03B3;</em>, <em>&#x03B1;<sup>gl</sup>      </em>, <em>&#x03B1;<sup>loc</sup>      </em> and <em>&#x03B1;<sup>loc</sup>      </em>. In our experiments, all these parameters are set to 0.1 and number of topics as 100.<br/></li>     <li id="list8" label="(3)"><strong>M-ASUM:</strong> As mentioned in Section <a class="sec" href="#sec-8">2</a>, the simple aspect model (M-ASUM) proposed in this paper (Figure <a class="fig" href="#fig2">2</a>) is a variation of the <em>aspect sentiment unification model</em> (ASUM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>], where the topics are sampled for an entire sentence instead of the conventional per-word sampling. In our experiments, parameters <em>&#x03B3;</em>, <em>&#x03B4;</em> are set to 0.1, <em>&#x03B2;</em> as 0.001 and the number of topics as 100.<br/></li>     <li id="list9" label="(4)"><strong>Targeted-Topic Model (TTM):</strong> The TTM is the state-of-the-art model for aspect-based topic summarization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0027">27</a>]. Therefore, in this paper, we choose the TTM model as the prime candidate for comparison. The model has seven hyper-parameters which are set as follows: <em>&#x03B3;</em> = <em>&#x03B1;</em> = 1, <em>p</em> = <em>q</em> = 1, <em>&#x03B2;<sup>ir</sup>      </em> = <em>&#x03B4;</em> = 0.001 ,&#x03F5; = 1.0 &#x00D7; 10<sup>&#x2212; 7</sup> and number of topics as 10.<br/></li>    </ol>    <p>The parameters of the above baselines were chosen based on trial and error. However, we noticed that for most of the scenarios the models gave the best results with the default value that was set by the authors.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Evaluation Methodology</h3>     </div>    </header>    <p>     <strong>Judging the Topic Quality:</strong> Topic models are typically evaluated using popular methods such as perplexity or the likelihood of held-out data; nonetheless, researchers have shown that these automated methods of evaluation does not translate to the actual human interpretability of topics [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. Therefore, in our paper, we adopt the following techniques to judge the quality of topics produced by APSUM: (a) human judgment and (b) topic coherence. In order to perform the human judgment, for each domain, (i.e movie reviews, product and restaurant reviews) we selected three students who are experts in judging topics related to movies and three of our collaborators who are experts in judging product-related topics. The quality of the topics were decided based on the majority voting scheme.</p>    <section id="sec-15">     <p><em>4.3.1 Evaluation Metrics.</em> For our first evaluation, we use a normalized version of the precision metric that was proposed by Wang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0027">27</a>]. The precision score for a model <em>m</em> is defined as follows: <div class="table-responsive" id="eq5">       <div class="display-equation">       <span class="tex mytex">\begin{align} P_{m}@T = \cfrac{\sum _{z=k}^{K_{m}}\#Rel(Q_{z})}{\sum _{z=k}^{K_{u}}\#MaxRel(Q_{z})} \end{align} </span>       <br/>       <span class="equation-number">(10)</span>       </div>      </div> where <em>K<sub>m</sub>      </em> is the set of aspects (or topics) that matches the user&#x0027;s query of interest, <span class="inline-equation"><span class="tex">$\# Rel(Q_{z})$</span>      </span> is the number of words that are relevant to the aspect <em>z</em>, <em>K<sub>u</sub>      </em> is total set of unique aspects from all models that are relevant to the user&#x0027;s query, and <span class="inline-equation"><span class="tex">$\# MaxRel(Q_{z})$</span>      </span> is the maximum number of words that are relevant to the user query. It should be noted that this count is obtained from the model that provides the best aspect for the query.</p>     <p>The second evaluation measure is the topic coherence, which is defined as follows: <div class="table-responsive" id="eq6">       <div class="display-equation">       <span class="tex mytex">\begin{align} coherence(V) = \sum _{v_i,v_j} score(v_i,v_j,\epsilon) \end{align} </span>       <br/>       <span class="equation-number">(11)</span>       </div>      </div>     </p>     <p>where <em>V</em> is the vocabulary and &#x03F5; is the smoothing factor. The <em>score</em>(<em>v<sub>i</sub>      </em>, <em>v<sub>j</sub>      </em>, &#x03F5;) signifies the mutual information between two words and can take many different forms. The most poplar ones are the UMass measure [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>] and the UCI measure [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>]; in this paper, we choose the latter.</p>    </section>    <section id="sec-16">     <p><em>4.3.2 Quantitative Evaluation.</em> Table <a class="tbl" href="#tab3">3</a> summarizes the results of our experiments, which reveal several interesting outcomes. First, all four models show a clear improvement over the standard LDA; thereby, proving that the conventional topic model is not suitable for detecting fine-grained aspects due to its tendency to generate global topics. Second, it is also quite obvious that the APSUM outperforms every other model by producing better precision scores. When comparing with TTM, APSUM has a gain of 6-7% over top-5 and top-10 words, but this gain slides down to just 3% when considering top-20 words. The reason for such diminishing gains can be attributed to the composition of the data, where most reviews (especially, the product data) are extremely short, and cover a narrow range of aspects with very limited vocabulary. For example, in the restaurant dataset, only six or seven reviews mentioned something about <em>Sushi</em> and more importantly the description was limited to just 4-5 lines and over 70% of them had strong overlap of words related to aspects such as <em>starter, appetizer, tuna, asian and service.</em>     </p>     <p>Comparing the proposed model to MG-LDA and M-ASUM, we see a performance increase of upto 15% and unsurprisingly, the largest gain achieved by APSUM is over the standard LDA model with about 37% improvement over the top 10 ranked words. We also tried to increase the number of topics for both MG-LDA and M-ASUM from 50 to 100 to see whether there is an increase in the aspect quality. Although this definitely resulted in mining more aspects, the resultant topic space was too noisy and the human judgment became too tedious. Table <a class="tbl" href="#tab3">3</a> also shows another interesting trend where MG-LDA seems to perform better than M-ASUM for the top-5 words; however, for top-10 and top-20 words, this outcome is reversed, where the former outperforms the latter. As mentioned in the previous section, LDA had the tendency to constantly produce a large set of globally related topics that were incoherent with the aspect of interest. Due to space constraints we only show the results of three queries, but the average precision scores was calculated using the outcome of the fourth query. <figure id="fig3">       <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186069/images/www2018-78-fig3.jpg" class="img-responsive" alt="Figure 3"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Comparison of Topic Coherence.</span>       </div>      </figure>      <strong>Analyzing the topic coherence:</strong> The coherence scores of APSUM shown in Figure <a class="fig" href="#fig3">3</a> reveals some intriguing similarities between the precision scores obtained using human judgment. First, the overall performance of APSUM is significantly better than other models across all datasets and the best coherence score is achieved over the movie dataset due to its rich word co-occurrence information. However, one can also observe that TTM supersedes our model for lower topic counts (i.e., between 10-50). This is mainly due to the structure of the graphical model proposed by the authors of TTM. That being said, our model significantly beats TTM when the number of topics exceeds hundred. Second, MG-LDA and M-ASUM have very similar coherence scores and LDA trails behind all other models; thus establishing a surprising analogy with the precision scores shown in Table <a class="tbl" href="#tab3">3</a>. Due to space constraints, we exclude the results of the home theater reviews, but in our testing, the performance was very similar to the dataset on camera reviews.</p>     <p>      <strong>Effect of linguistic supervision:</strong> We conclude this section by illustrating the effect of supervision in Figure <a class="fig" href="#fig4">4</a>, where x-axis denotes the different supervised information, and y-axis is the precision at <em>K</em> (<em>p</em>@<em>K</em>), which is calculated using the same human judgment. The term <em>s-prior</em> signifies the unsupervised version of our model that uses symmetric priors for all hyperparameters and the terms <em>D-graph</em> and <em>Ent</em> denote the supervision using dependency graphs and entities, respectively (refer to Section <a class="sec" href="#sec-10">3</a>). The figure clearly shows that the linguistic supervision in-terms of the dependency graph provides a reasonable boost to the performance of APSUM to mine aspects that are better correlated. On the other hand, the entity-type supervision is not as good as the D-graph supervision due to its simplistic nature. It looks like simply detecting entities in sentences does not convey sufficient information about the aspects themselves. Obviously, apart from the supervised information, the topic count plays a significant role in determining the precision. Except for the restaurant review dataset, a topic count of 150 yields the best performance. This is because, in restaurant dataset, the number of documents (i.e. reviews) about a specific item is extremely sparse; therefore, increasing the number of topics simply introduces noise, thereby rendering the supervision moot. <figure id="fig4">       <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186069/images/www2018-78-fig4.jpg" class="img-responsive" alt="Figure 4"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Effect of linguistic supervision.</span>       </div>      </figure>     </p>    </section>    <section id="sec-17">     <header>      <div class="title-info">       <h4>       <span class="section-number">4.3.3</span> Qualitative Evaluation</h4>      </div>     </header>     <div class="table-responsive" id="tab4">      <div class="table-caption">       <span class="table-number">Table 4:</span>       <span class="table-title">Qualitative comparisons of the aspects produced by APSUM for the product review dataset from Amazon.</span>      </div>      <table class="table">       <thead>       <tr>        <th colspan="6" style="text-align:center;">         <strong>Domain: Camera, Query:Battery</strong>         <hr/>        </th>        <th colspan="6" style="text-align:center;">         <strong>Domain: Home Theater, Query:Wireless</strong>         <hr/>        </th>       </tr>       <tr>        <th colspan="3" style="text-align:center;">APSUM<hr/>        </th>        <th colspan="2" style="text-align:center;">TTM<hr/>        </th>        <th style="text-align:center;">M-ASUM</th>        <th colspan="2" style="text-align:center;">APSUM<hr/>        </th>        <th colspan="2" style="text-align:center;">TTM<hr/>        </th>        <th colspan="2" style="text-align:center;">M-ASUM<hr/>        </th>       </tr>       <tr>        <th style="text-align:center;">         <strong>Capacity</strong>        </th>        <th style="text-align:center;">         <strong>Type</strong>        </th>        <th style="text-align:center;">         <strong>Video</strong>        </th>        <th style="text-align:center;">         <strong>Generic</strong>        </th>        <th style="text-align:center;">         <strong>Size</strong>        </th>        <th style="text-align:center;">         <strong>Capacity</strong>        </th>        <th style="text-align:center;">         <strong>Speaker</strong>        </th>        <th style="text-align:center;">         <strong>Setup</strong>        </th>        <th style="text-align:center;">         <strong>Speaker</strong>        </th>        <th style="text-align:center;">         <strong>Setup</strong>        </th>        <th style="text-align:center;">         <strong>Router</strong>        </th>        <th style="text-align:center;">         <strong>Generic</strong>        </th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;">life</td>        <td style="text-align:center;">hour</td>        <td style="text-align:center;">extra</td>        <td style="text-align:center;">HTML]FE0000 canon</td>        <td style="text-align:center;">HTML]FE0000 picture</td>        <td style="text-align:center;">HTML]FE0000 picture</td>        <td style="text-align:center;">wireless</td>        <td style="text-align:center;">wall</td>        <td style="text-align:center;">wireless</td>        <td style="text-align:center;">subwoofer</td>        <td style="text-align:center;">wireless</td>        <td style="text-align:center;">HTML]FE0000 money</td>       </tr>       <tr>        <td style="text-align:center;">capacity</td>        <td style="text-align:center;">aa</td>        <td style="text-align:center;">video</td>        <td style="text-align:center;">card</td>        <td style="text-align:center;">video</td>        <td style="text-align:center;">big</td>        <td style="text-align:center;">subwoofer</td>        <td style="text-align:center;">setup</td>        <td style="text-align:center;">speaker</td>        <td style="text-align:center;">unit</td>        <td style="text-align:center;">subwoofer</td>        <td style="text-align:center;">HTML]FE0000 samsung</td>       </tr>       <tr>        <td style="text-align:center;">charge</td>        <td style="text-align:center;">pack</td>        <td style="text-align:center;">action</td>        <td style="text-align:center;">video</td>        <td style="text-align:center;">feel</td>        <td style="text-align:center;">aa</td>        <td style="text-align:center;">speaker</td>        <td style="text-align:center;">unit</td>        <td style="text-align:center;">rear</td>        <td style="text-align:center;">add</td>        <td style="text-align:center;">theater</td>        <td style="text-align:center;">wireless</td>       </tr>       <tr>        <td style="text-align:center;">average</td>        <td style="text-align:center;">lithium</td>        <td style="text-align:center;">shot</td>        <td style="text-align:center;">dslr</td>        <td style="text-align:center;">hand</td>        <td style="text-align:center;">action</td>        <td style="text-align:center;">powered</td>        <td style="text-align:center;">mount</td>        <td style="text-align:center;">good</td>        <td style="text-align:center;">angle</td>        <td style="text-align:center;">router</td>        <td style="text-align:center;">HTML]FE0000 plasma</td>       </tr>       <tr>        <td style="text-align:center;">screen</td>        <td style="text-align:center;">rechargable</td>        <td style="text-align:center;">HTML]FE0000 personal</td>        <td style="text-align:center;">memory</td>        <td style="text-align:center;">HTML]FE0000 nikon</td>        <td style="text-align:center;">power</td>        <td style="text-align:center;">sound</td>        <td style="text-align:center;">wireless</td>        <td style="text-align:center;">surround</td>        <td style="text-align:center;">arrangement</td>        <td style="text-align:center;">receiver</td>        <td style="text-align:center;">HTML]FE0000 star</td>       </tr>       <tr>        <td style="text-align:center;">spare</td>        <td style="text-align:center;">compartment</td>        <td style="text-align:center;">summer</td>        <td style="text-align:center;">HTML]FE0000 software</td>        <td style="text-align:center;">big</td>        <td style="text-align:center;">huge</td>        <td style="text-align:center;">great</td>        <td style="text-align:center;">bracket</td>        <td style="text-align:center;">HTML]FE0000 added</td>        <td style="text-align:center;">direction</td>        <td style="text-align:center;">capability</td>        <td style="text-align:center;">HTML]FE0000 inch</td>       </tr>       <tr>        <td style="text-align:center;">HTML]FE0000 removable</td>        <td style="text-align:center;">charger</td>        <td style="text-align:center;">roll</td>        <td style="text-align:center;">HTML]FE0000 decide</td>        <td style="text-align:center;">hold</td>        <td style="text-align:center;">HTML]FE0000 kodak</td>        <td style="text-align:center;">corner</td>        <td style="text-align:center;">cord</td>        <td style="text-align:center;">amazing</td>        <td style="text-align:center;">HTML]FE0000 deal</td>        <td style="text-align:center;">ghz</td>        <td style="text-align:center;">hdmi</td>       </tr>       <tr>        <td style="text-align:center;">minute</td>        <td style="text-align:center;">extra</td>        <td style="text-align:center;">shutter</td>        <td style="text-align:center;">HTML]FE0000 upgrade</td>        <td style="text-align:center;">easy</td>        <td style="text-align:center;">HTML]FE0000 folk</td>        <td style="text-align:center;">bass</td>        <td style="text-align:center;">router</td>        <td style="text-align:center;">apartment</td>        <td style="text-align:center;">HTML]FE0000 beautiful</td>        <td style="text-align:center;">internet</td>        <td style="text-align:center;">bass</td>       </tr>       <tr>        <td style="text-align:center;">advantage</td>        <td style="text-align:center;">HTML]FE0000 style</td>        <td style="text-align:center;">dvd</td>        <td style="text-align:center;">HTML]FE0000 return</td>        <td style="text-align:center;">film</td>        <td style="text-align:center;">medium</td>        <td style="text-align:center;">watt</td>        <td style="text-align:center;">HTML]FE0000 little</td>        <td style="text-align:center;">HTML]FE0000 authorized</td>        <td style="text-align:center;">HTML]FE0000 folk</td>        <td style="text-align:center;">HTML]FE0000 comcast</td>        <td style="text-align:center;">feature</td>       </tr>       </tbody>      </table>     </div>     <div class="table-responsive" id="tab5">      <div class="table-caption">       <span class="table-number">Table 5:</span>       <span class="table-title">Qualitative comparisons of the aspects produced by APSUM for the movie review dataset from IMDB.</span>      </div>      <table class="table">       <thead>       <tr>        <th colspan="6" style="text-align:center;">         <strong>Domain: Desolation of Smaug, Query:legolas</strong>         <hr/>        </th>        <th colspan="6" style="text-align:center;">         <strong>Domain: Captain America Civil War, Query: Fight</strong>         <hr/>        </th>       </tr>       <tr>        <th colspan="3" style="text-align:center;">APSUM<hr/>        </th>        <th colspan="2" style="text-align:center;">TTM<hr/>        </th>        <th style="text-align:center;">M-ASUM</th>        <th colspan="2" style="text-align:center;">APSUM<hr/>        </th>        <th colspan="2" style="text-align:center;">TTM<hr/>        </th>        <th colspan="2" style="text-align:center;">M-ASUM<hr/>        </th>       </tr>       <tr>        <th style="text-align:center;">         <strong>Love</strong>        </th>        <th style="text-align:center;">         <strong>Combat</strong>        </th>        <th style="text-align:center;">         <strong>Chase</strong>        </th>        <th style="text-align:center;">         <strong>Love</strong>        </th>        <th style="text-align:center;">         <strong>Generic</strong>        </th>        <th style="text-align:center;">         <strong>Love</strong>        </th>        <th style="text-align:center;">         <strong>Airport</strong>        </th>        <th style="text-align:center;">         <strong>Team</strong>        </th>        <th style="text-align:center;">         <strong>Airport</strong>        </th>        <th style="text-align:center;">         <strong>Action</strong>        </th>        <th style="text-align:center;">         <strong>Airport</strong>        </th>        <th style="text-align:center;">         <strong>Antman</strong>        </th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;">tauriel</td>        <td style="text-align:center;">legolas</td>        <td style="text-align:center;">dwarf</td>        <td style="text-align:center;">HTML]000000 legolas</td>        <td style="text-align:center;">HTML]343434 bloom</td>        <td style="text-align:center;">HTML]343434 legolas</td>        <td style="text-align:center;">scene</td>        <td style="text-align:center;">america</td>        <td style="text-align:center;">fight</td>        <td style="text-align:center;">good</td>        <td style="text-align:center;">fight</td>        <td style="text-align:center;">HTML]FE0000 stuff</td>       </tr>       <tr>        <td style="text-align:center;">legolas</td>        <td style="text-align:center;">orcs</td>        <td style="text-align:center;">barrel</td>        <td style="text-align:center;">elf</td>        <td style="text-align:center;">orlando</td>        <td style="text-align:center;">tauriel</td>        <td style="text-align:center;">action</td>        <td style="text-align:center;">captain</td>        <td style="text-align:center;">character</td>        <td style="text-align:center;">iron</td>        <td style="text-align:center;">scene</td>        <td style="text-align:center;">HTML]000000 antman</td>       </tr>       <tr>        <td style="text-align:center;">dwarf</td>        <td style="text-align:center;">sequence</td>        <td style="text-align:center;">orcs</td>        <td style="text-align:center;">love</td>        <td style="text-align:center;">elf</td>        <td style="text-align:center;">HTML]FE0000 horse</td>        <td style="text-align:center;">airport</td>        <td style="text-align:center;">cap</td>        <td style="text-align:center;">film</td>        <td style="text-align:center;">movie</td>        <td style="text-align:center;">airport</td>        <td style="text-align:center;">HTML]FE0000 masterpiece</td>       </tr>       <tr>        <td style="text-align:center;">kili</td>        <td style="text-align:center;">scene</td>        <td style="text-align:center;">river</td>        <td style="text-align:center;">kili</td>        <td style="text-align:center;">HTML]FE0000 film</td>        <td style="text-align:center;">HTML]FE0000 orcs</td>        <td style="text-align:center;">sequence</td>        <td style="text-align:center;">team</td>        <td style="text-align:center;">airport</td>        <td style="text-align:center;">cap</td>        <td style="text-align:center;">epic</td>        <td style="text-align:center;">HTML]000000 funny</td>       </tr>       <tr>        <td style="text-align:center;">elf</td>        <td style="text-align:center;">orc</td>        <td style="text-align:center;">HTML]333333 chase</td>        <td style="text-align:center;">scene</td>        <td style="text-align:center;">HTML]FE0000 book</td>        <td style="text-align:center;">female</td>        <td style="text-align:center;">fight</td>        <td style="text-align:center;">bucky</td>        <td style="text-align:center;">scene</td>        <td style="text-align:center;">bucky</td>        <td style="text-align:center;">end</td>        <td style="text-align:center;">HTML]000000 nerd</td>       </tr>       <tr>        <td style="text-align:center;">triangle</td>        <td style="text-align:center;">head</td>        <td style="text-align:center;">legolas</td>        <td style="text-align:center;">HTML]000000 triangle</td>        <td style="text-align:center;">HTML]FE0000 story</td>        <td style="text-align:center;">thranduil</td>        <td style="text-align:center;">choreography</td>        <td style="text-align:center;">tony</td>        <td style="text-align:center;">HTML]333333 final</td>        <td style="text-align:center;">soldier</td>        <td style="text-align:center;">bucky</td>        <td style="text-align:center;">HTML]000000 huge</td>       </tr>       <tr>        <td style="text-align:center;">HTML]343434 love</td>        <td style="text-align:center;">great</td>        <td style="text-align:center;">ride</td>        <td style="text-align:center;">HTML]FE0000 dragon</td>        <td style="text-align:center;">thranduil</td>        <td style="text-align:center;">HTML]343434 love</td>        <td style="text-align:center;">great</td>        <td style="text-align:center;">final</td>        <td style="text-align:center;">battle</td>        <td style="text-align:center;">HTML]FE0000 new</td>        <td style="text-align:center;">HTML]FE0000 decade</td>        <td style="text-align:center;">HTML]FE0000 beautiful</td>       </tr>       <tr>        <td style="text-align:center;">HTML]FE0000 line</td>        <td style="text-align:center;">combat</td>        <td style="text-align:center;">water</td>        <td style="text-align:center;">HTML]FE0000 return</td>        <td style="text-align:center;">action</td>        <td style="text-align:center;">HTML]343434 need</td>        <td style="text-align:center;">seat</td>        <td style="text-align:center;">ironman</td>        <td style="text-align:center;">emotional</td>        <td style="text-align:center;">HTML]FE0000 spiderman</td>        <td style="text-align:center;">HTML]FE0000 menace</td>        <td style="text-align:center;">HTML]FE0000 zack</td>       </tr>       <tr>        <td style="text-align:center;">relationship</td>        <td style="text-align:center;">HTML]000000 horse</td>        <td style="text-align:center;">HTML]FE0000 able</td>        <td style="text-align:center;">HTML]000000 good</td>        <td style="text-align:center;">HTML]FE0000 lotr</td>        <td style="text-align:center;">relationship</td>        <td style="text-align:center;">edge</td>        <td style="text-align:center;">HTML]FE0000 reason</td>        <td style="text-align:center;">HTML]333333 epic</td>        <td style="text-align:center;">HTML]FE0000 brother</td>        <td style="text-align:center;">HTML]333333 funny</td>        <td style="text-align:center;">HTML]FE0000 natasha</td>       </tr>       </tbody>      </table>     </div>     <p>In this section, we perform qualitative analysis of the proposed model by showing the actual aspect summaries and analyze their quality from a perspective of human understanding. Due to space constraints, it is not feasible to show the outcome of every model and query. Consequently, besides APSUM, we choose two other models that produced the best results in our quantitative evaluation, namely TTM and M-ASUM. Table <a class="tbl" href="#tab4">4</a> shows the aspects produced by these models over the queries, <em>battery</em> and <em>wireless</em>, where the words marked in red denote the <em>intruded</em> (or noisy) words. From the results, it is quite apparent that the aspects produced by APSUM are very focused on the target query and more importantly, the word clusters under each aspect are extremely coherent in conveying a unified theme. For instance, the aspect <em>capacity</em> is highly relevant to the query <em>Battery</em> and the words <em>life, charge, average, screen</em>, etc., signify certain attributes of this aspect. Similarly, most words associated with the queries <em>Speaker</em> and <em>Setup</em> are highly accurate in describing the characteristics of these aspects. The same cannot be said about TTM, where, for some cases the model performs extremely well, while for others it is too noisy and fails to convey a coherent theme. For example, the words <em>card, video, dslr</em> and <em>memory</em> appears to convey some meaning about the aspect on <em>memory cards</em>; however, the intruded words <em>canon, software, decide</em> etc., have very little correlation with this aspect. This makes this topic cluster extremely generic and difficult to interpret. Finally, the example also shows that the topics produced by M-ASUM are reasonably good. However, when compared to APSUM and TTM, it definitely tends to have more intruded (or irrelevant) words. In fact, in some cases such as the query about <em>Wireless</em>, there are many random words like <em>money, samsung, plasma</em>, etc.</p>     <p>We now analyze the aspects from the movie dataset. A key trait that separates the movie from the product dataset is the homogeneity of reviews. In other words, the reviews about cameras are not about a specific product; instead, it is a combination of varied product types that include compact cameras, DSLRs, and binoculars from different brands such as <em>Canon, Nikon,</em> and <em>Sony</em>. Consequently, the aspect summaries of this dataset are not necessarily focused on any particular merchandise. The movie dataset on the other hand, consists of reviews that talk about a <em>specific movie</em>; therefore, the aspect summaries are focused towards the characteristics of a movie. Table <a class="tbl" href="#tab5">5</a> demonstrates this outcome using the queries <em>Legolas</em> over the movie &#x201C;Hobbit: The Desolation of Smaug&#x201D; and the attribute <em>fight</em> over the movie &#x201C;Captain America Civil War&#x201D;.</p>     <p>The film potrays the character <em>Legolas</em> in three main scenes: (a) a scene that depicts Legolas to have possessive feelings over the relationship between the elf Tauriel, and the dwarf Kili. (b) a well acclaimed chase between the orcs, the elves and the dwarfs on a fast flowing river and (c) a fight sequence where Legolas chases an orc named Bolg. All three scenes are summarized under the aspects <em>Love</em>, <em>Combat</em> and <em>Chase</em>, respectively. The TTM model also produces some good aspect summaries, but the word features that describe these aspects are noisier when compared to APSUM. For instance, the aspect <em>love</em> has some intruded words such as <em>dragon</em> and <em>return</em>, which do not coincide with the main theme. Additionally, we also observe a very <em>generic</em> topic (i.e., global topic), which basically has all the popular words from the movie and such topics convey little to no meaning. Similar arguments can be made over the results of the query <em>fight</em>. The movie depicts an intense face-off between Iron-man and Captain America at the <em>airport</em>, which was acclaimed by many critiques. Table <a class="tbl" href="#tab5">5</a>, shows that both APSUM and TTM are very good in summarizing this aspect. Apart from this scene, APSUM also produces an aspect called <em>team</em>, which summarizes the main characters involved in the climax fight scene. Alternatively, TTM is able to reveal some sentiment words related to the <em>action</em> aspect, while M-ASUM simply produces a noisy topic cluster that seems to be related to the character <em>ant-man</em>. Readers should note that we tried to retrieve the same aspects from all models for fair comparison. However, this was not feasible since the topics produced by models greatly vary. For instance, in this example, TTM never produced a topic that was relevant to the <em>type</em> of <em>battery</em> (i.e., aa, lithium, etc.) while APSUM did not summarize any aspect related to the size. In summary, from these qualitative examples, it is clear that APSUM outperforms other baselines and the state-of-the-art aspect model. Although we are unable to show the results of all the queries, in our rigorous testing, we found that APSUM produced focused and human interpretable aspects even on sparse datasets due to three key components: (1) the document aggregator <em>l</em> that mitigates the problem of word co-occurrence, (2) the spike-and-slab prior constraining the document-topic space and (3) the downstream conditioning <em>y<sub>kv</sub>      </em> on the topic smoother <em>&#x03B2;</em> that was discussed in Section <a class="sec" href="#sec-8">2</a>.</p>    </section>    <section id="sec-18">     <header>      <div class="title-info">       <h4>       <span class="section-number">4.3.4</span> Visual Interface of Aspect Summaries</h4>      </div>     </header>     <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186069/images/www2018-78-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">A visual interface of aspect-specific topic summarization system.</span>      </div>     </figure>     <p>Although we performed rigorous evaluation of our model using various test cases, one might still ask the question, &#x201C;how can this model be useful to the end-user?&#x201D;. To answer this question, we provide a visual interface of our system in Figure <a class="fig" href="#fig5">5</a>. Different blocks of this interface are numbered in red color. In block 1, the user enters a URL of an item to scrape its reviews. In our example, the item is the movie &#x201C;Hobbit: The Desolation of Smaug&#x201D;. The interface then presents the user with different global aspects (block 2) obtained from the word-topic proportions <em>&#x03D5;<sup>AZ</sup>      </em> of the APSUM model (Figure <a class="fig" href="#fig2">2</a>). Now, let us assume the user clicks on the main aspect named &#x201C;Action&#x201D;. The interface then provides the sentiments and the original reviews about <em>action</em> in block 3. In addition to this, users can also view and click the sub-aspects related to the main aspect in block 4. For instance, if the user clicks on the aspect <em>barrel-river</em>, the interface then displays the sentiment proportions and the user reviews that are specific to this sub-aspect (i.e., block 5). Readers should note that although our approach does not exclusively model sentiments, we can still obtain the aspect-specific sentiment distribution by simply detecting the polarities of top words in the word-topic proportions. The sentences for a given aspect can be obtained by mapping the document-topic proportion <em>&#x03B8;</em> to <em>&#x03D5;<sup>AZ</sup>      </em>.</p>    </section>    </section>   </section>   <section id="sec-19">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Related Works</h2>    </div>    </header>    <p>Aspect-specific topic summarization of textual reviews is an emerging field of research. Therefore, there are very few research works that exclusively tackle this problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. That being said, the techniques used in formulating such models are closely related to sparse topic models that operate on microblogging data from Twittter, Tumblr, Friendfeed, etc. One of the earliest works on granularizing LDA to detect fine-grained aspects can be seen in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. In their work, the authors propose a multi-grain topic model called MG-LDA that extends the standard LDA model to generate global topics at a document-level and local topics on a sentence-level. Later works on aspect detection incorporate sentiment words as a part of their joint modeling framework [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], the authors provide a comprehensive summary of various aspect and opinion summarization models. In a recent work, Yang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] leverage metadata about reviews such as gender, location and age to propose a user-aware topic model that jointly models aspect and meta-data about the users and topics. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], the authors introduce a supervised topic model that utilizes the overall rating of reviews to treat the documents as a bag-of-opinion pair; where, each pair consists of an aspect and an opinion associated with that aspect.</p>    <p>Apart from topic models, another popular way of detecting aspects is to use linguistic techniques from the domain of natural language processing (NLP). The NLP techniques can be as simple as extracting aspects based on frequently occurring noun phrases (NP) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] to more comprehensive techniques that involve building dependency grammar structures. For instance, the authors of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] propose an aspect and entity extraction module that uses several grammar rules [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] to create dependency graphs between words. One of the popular works by Hu et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] provide feature-based summaries of customer reviews on products such as digital camera, cellular phones and Mp3 players. The popular workshop on semantic evaluation (SEMVAL) provides an exclusive track on aspect-based sentiment detection, where several researchers compile heuristic techniques to mine aspect and sentimens [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. More recently, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], the authors leverage the prior knowledge from several other product domains (e.g., reviews of products from electronic category) to extract aspects of the target product. A comprehensive summary of aspect-level topic and sentiment detection can be seen in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>].</p>    <p>Despite such recent works on aspect-specific topic summarization, there is still room for several improvements since detecting fine-grained topics from large textual corpus is still an open problem. The research that is closest to our work is the targeted topic model (TTM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] where the authors use the spike-and-slab prior over the word-topic space. However, for achieving topic sparsity, it is important to perform both upstream (i.e., the document-topic simplex) and downstream conditioning (i.e., the word-topic simplex). Additionally, our method allows to incorporate supervision in the form of human annotation, linguistic dependency graphs and other information from external document corpus to improve the quality of summarized aspects. The results of our model clearly reveal the effectiveness of this approach by producing superior performance over TTM.</p>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>In this paper, we proposed a generative topic model, called APSUM, that is capable of retrieving and summarizing fine-grained aspects from online reviews. To achieve aspect sparsity in the word distribution, we performed a joint modeling of three different components: (1) a sentence aggregator to overcome the sparsity of word co-occurrence, (2) a spike-and-slab prior to introduce sparsity in document-topic space, while avoiding over-fitting using a smoother and (3) a supervised conditioning over the hyperparameter <em>&#x03B2;</em> to infuse word-topic sparsity. Using extensive set of experiments, and a variety of datasets from different domains, we showed that our model outperformed all the baselines and the state-of-the-art aspect summarization model in both quantitative and qualitative evaluations.</p>   </section>   <section id="sec-21">    <header>    <div class="title-info">     <h2>Acknowledgements</h2>    </div>    </header>    <p>This work was supported in part by the National Science Foundation grants IIS-1619028, IIS-1707498 and IIS-1646881.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDonald, Tyler Neylon, George&#x00A0;A Reis, and Jeff Reynar. 2008. Building a sentiment summarizer for local service reviews. In <em>WWW workshop on NLP in the information explosion era</em>, Vol.&#x00A0;14. 339&#x2013;348.</li>    <li id="BibPLXBIB0002" label="[2]">David&#x00A0;M Blei, Andrew&#x00A0;Y Ng, and Michael&#x00A0;I Jordan. 2003. Latent dirichlet allocation. <em>Journal of machine Learning research</em> 3, Jan (2003), 993&#x2013;1022.</li>    <li id="BibPLXBIB0003" label="[3]">Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In <em>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</em>. Association for Computational Linguistics, 804&#x2013;812.</li>    <li id="BibPLXBIB0004" label="[4]">Jonathan Chang, Sean Gerrish, Chong Wang, Jordan&#x00A0;L Boyd-Graber, and David&#x00A0;M Blei. 2009. Reading tea leaves: How humans interpret topic models. In <em>Advances in neural information processing systems</em>. 288&#x2013;296.</li>    <li id="BibPLXBIB0005" label="[5]">Zhiyuan Chen and Bing Liu. 2014. Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data. In <em>Proceedings of the 31st International Conference on Machine Learning (ICML-14)</em>. 703&#x2013;711.</li>    <li id="BibPLXBIB0006" label="[6]">Orph&#x00E9;e De&#x00A0;Clercq, Marjan Van&#x00A0;de Kauter, Els Lefever, and V&#x00E9;ronique Hoste. 2015. Applying hybrid terminology extraction to aspect-based sentiment analysis. In <em>International Workshop on Semantic Evaluation (SemEval 2015)</em>. Association for Computational Linguistics, 719&#x2013;724.</li>    <li id="BibPLXBIB0007" label="[7]">Marie-Catherine De&#x00A0;Marneffe and Christopher&#x00A0;D Manning. 2008. <em>Stanford typed dependencies manual</em>. Technical Report. Technical report, Stanford University.</li>    <li id="BibPLXBIB0008" label="[8]">Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander&#x00A0;J Smola, Jing Jiang, and Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). In <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>. ACM, 193&#x2013;202.</li>    <li id="BibPLXBIB0009" label="[9]">Aitor Garc&#x0131;a-Pablos, Montse Cuadros, and German Rigau. 2015. V3: unsupervised aspect based sentiment analysis for SemEval-2015 Task 12. <em>SemEval-2015</em> (2015), 714.</li>    <li id="BibPLXBIB0010" label="[10]">Zhen Hai, Gao Cong, Kuiyu Chang, Peng Cheng, and Chunyan Miao. 2017. Analyzing Sentiments in One Go: A Supervised Joint Topic Modeling Approach. <em>IEEE Transactions on Knowledge and Data Engineering</em> 29, 6(2017), 1172&#x2013;1185.</li>    <li id="BibPLXBIB0011" label="[11]">Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In <em>Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</em>. ACM, 168&#x2013;177.</li>    <li id="BibPLXBIB0012" label="[12]">Yohan Jo and Alice&#x00A0;H Oh. 2011. Aspect and sentiment unification model for online review analysis. In <em>Proceedings of the fourth ACM international conference on Web search and data mining</em>. ACM, 815&#x2013;824.</li>    <li id="BibPLXBIB0013" label="[13]">Tianyi Lin, Wentao Tian, Qiaozhu Mei, and Hong Cheng. 2014. The dual-sparse topic model: mining focused topics and focused terms in short text. In <em>Proceedings of the 23rd international conference on World wide web</em>. ACM, 539&#x2013;550.</li>    <li id="BibPLXBIB0014" label="[14]">Qian Liu, Bing Liu, Yuanlin Zhang, Doo&#x00A0;Soon Kim, and Zhiqiang Gao. 2016. Improving Opinion Aspect Extraction Using Semantic Similarity and Aspect Associations.. In <em>AAAI</em>. 2986&#x2013;2992.</li>    <li id="BibPLXBIB0015" label="[15]">David Mimno, Hanna&#x00A0;M Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In <em>Proceedings of the Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, 262&#x2013;272.</li>    <li id="BibPLXBIB0016" label="[16]">Samaneh Moghaddam and Martin Ester. 2012. On the design of LDA models for aspect-based opinion mining. In <em>Proceedings of the 21st ACM international conference on Information and knowledge management</em>. ACM, 803&#x2013;812.</li>    <li id="BibPLXBIB0017" label="[17]">David Newman, Youn Noh, Edmund Talley, Sarvnaz Karimi, and Timothy Baldwin. 2010. Evaluating topic models for digital libraries. In <em>Proceedings of the 10th annual joint conference on Digital libraries</em>. ACM, 215&#x2013;224.</li>    <li id="BibPLXBIB0018" label="[18]">James Petterson, Wray Buntine, Shravan&#x00A0;M Narayanamurthy, Tib&#x00E9;rio&#x00A0;S Caetano, and Alex&#x00A0;J Smola. 2010. Word features for latent dirichlet allocation. In <em>Advances in Neural Information Processing Systems</em>. 1921&#x2013;1929.</li>    <li id="BibPLXBIB0019" label="[19]">Ana-Maria Popescu and Orena Etzioni. 2007. Extracting product features and opinions from reviews. In <em>Natural language processing and text mining</em>. Springer, 9&#x2013;28.</li>    <li id="BibPLXBIB0020" label="[20]">Soujanya Poria, Erik Cambria, Lun-Wei Ku, Chen Gui, and Alexander Gelbukh. 2014. A rule-based approach to aspect extraction from product reviews. In <em>Proceedings of the second workshop on natural language processing for social media (SocialNLP)</em>. 28&#x2013;37.</li>    <li id="BibPLXBIB0021" label="[21]">Md&#x00A0;Mustafizur Rahman and Hongning Wang. 2016. Hidden topic sentiment model. In <em>Proceedings of the 25th International Conference on World Wide Web</em>. 155&#x2013;165.</li>    <li id="BibPLXBIB0022" label="[22]">Jos&#x00E9; Saias. 2015. Sentiue: Target and aspect based sentiment analysis in semeval-2015 task 12. Association for Computational Linguistics.</li>    <li id="BibPLXBIB0023" label="[23]">Kim Schouten and Flavius Frasincar. 2016. Survey on aspect-level sentiment analysis. <em>IEEE Transactions on Knowledge and Data Engineering</em> 28, 3(2016), 813&#x2013;830.</li>    <li id="BibPLXBIB0024" label="[24]">SEMVAL. [n. d.]. International Workshop on Semantic Evaluation.http://alt.qcri.org/semeval2016/</li>    <li id="BibPLXBIB0025" label="[25]">Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In <em>Proceedings of the 17th international conference on World Wide Web</em>. ACM, 111&#x2013;120.</li>    <li id="BibPLXBIB0026" label="[26]">Chong Wang and David&#x00A0;M Blei. 2009. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process. In <em>Advances in neural information processing systems</em>. 1982&#x2013;1989.</li>    <li id="BibPLXBIB0027" label="[27]">Shuai Wang, Zhiyuan Chen, Geli Fei, Bing Liu, and Sherry Emery. 2016. Targeted Topic Modeling for Focused Analysis. In <em>Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</em>. ACM.</li>    <li id="BibPLXBIB0028" label="[28]">Shuai Wang, Zhiyuan Chen, and Bing Liu. 2016. Mining Aspect-Specific Opinion using a Holistic Lifelong Topic Model. In <em>Proceedings of the 25th International Conference on World Wide Web</em>. 167&#x2013;176.</li>    <li id="BibPLXBIB0029" label="[29]">Yao Wu and Martin Ester. 2015. Flame: A probabilistic model combining aspect based opinion mining and collaborative filtering. In <em>Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</em>. ACM, 199&#x2013;208.</li>    <li id="BibPLXBIB0030" label="[30]">Zaihan Yang, Alexander Kotov, Aravind Mohan, and Shiyong Lu. 2015. Parametric and non-parametric user-aware sentiment topic models. In <em>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. ACM, 413&#x2013;422.</li>    <li id="BibPLXBIB0031" label="[31]">Lei Zhang and Bing Liu. 2014. Aspect and entity extraction for opinion mining. In <em>Data mining and knowledge discovery for big data</em>. Springer, 1&#x2013;40.</li>    <li id="BibPLXBIB0032" label="[32]">Wayne&#x00A0;Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid. In <em>Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, 56&#x2013;65.</li>    <li id="BibPLXBIB0033" label="[33]">Yuan Zuo, Junjie Wu, Hui Zhang, Deqing Wang, Hao Lin, Fei Wang, and Ke Xu. 2015. Complementary Aspect-based Opinion Mining Across Asymmetric Collections. In <em>Data Mining (ICDM), 2015 IEEE International Conference on</em>. IEEE, 669&#x2013;678.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>This work was done when the author was an intern at Technicolor Labs, Los Altos, CA, USA.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://github.com/dasmith/stanford-corenlp-python">https://github.com/dasmith/stanford-corenlp-python</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://github.com/VRM1/WWW18">https://github.com/VRM1/WWW18</a></p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="https://pypi.python.org/pypi/imdbpie/">https://pypi.python.org/pypi/imdbpie/</a>   </p>   <div class="bibStrip">    <p>Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186069">https://doi.org/10.1145/3178876.3186069</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
