<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Aspect-Based Financial Sentiment Analysis using Deep Learning</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
<link rel="cite-as" href="https://doi.org/10.1145/3184558.3191827"/></head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191827'>https://doi.org/10.1145/3184558.3191827</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191827'>https://w3id.org/oa/10.1145/3184558.3191827</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Aspect-Based Financial Sentiment Analysis using Deep Learning</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Hitkul</span> <span class="surName">Jangid</span>, IIIT-Delhi, Delhi, India, <a href="mailto:hitkuljangid@gmail.com">hitkuljangid@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Shivangi</span> <span class="surName">Singhal</span>, IIIT-Delhi, Delhi, India, <a href="mailto:shivangis@iiitd.ac.in">shivangis@iiitd.ac.in</a>
        </div>
        <div class="author">
          <span class="givenName">Rajiv Ratn</span> <span class="surName">Shah</span>, IIIT-Delhi, Delhi, India, <a href="mailto:rajivratn@iiitd.ac.in">rajivratn@iiitd.ac.in</a>
        </div>
        <div class="author">
          <a href="https://orcid.org/1234-5678-9012" ref="author"><span class="givenName">Roger</span> <span class="surName">Zimmermann</span></a>, National University of Singapore, Singapore, Singapore, <a href="mailto:rogerz@comp.nus.edu.sg">rogerz@comp.nus.edu.sg</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191827" target="_blank">https://doi.org/10.1145/3184558.3191827</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Aspect based sentiment analysis aims to detect an aspect (i.e. features) in a given text and then perform sentiment analysis of the text with respect to that aspect. This paper aims to give a solution for the FiQA 2018 challenge subtask 1<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. We perform aspect-based sentiment analysis on the microblogs and headlines of financial domain. We use a multi-channel convolutional neural network for sentiment analysis and a recurrent neural network with bidirectional long short-term memory units to extract aspect from a given headline or microblog. Our proposed model produces a weighted average F1 score of 0.69 for the aspect extraction task and predicts sentiment intensity scores with a mean squared error of 0.112 on 10-fold cross validation. We believe that the developed system has direct applications in the financial domain.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Aspect-based sentiment analysis</small>,</span> <span class="keyword"><small>Deep learning</small>,</span> <span class="keyword"><small>Convolutional Neural Networks</small>,</span> <span class="keyword"><small>Recurrent neural networks</small>,</span> <span class="keyword"><small>Bidirectional LSTM</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Hitkul Jangid, Shivangi Singhal, Rajiv Ratn Shah, and Roger Zimmermann. 2018. Aspect-Based Financial Sentiment Analysis using Deep Learning. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018 (WWW ’18 Companion),</em> <em>Lyon, France. ACM, New York, NY, USA</em> 6 Pages. <a href="https://doi.org/10.1145/3184558.3191827" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3191827</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Micro-blogging today is an extremely widespread communication medium among Internet users. People use such medium to express their opinions on diverse facets of life every day. Such platforms contain a wealth of information that can be extracted and analyzed to solve various issues prevalent these days. A popular method applied by researchers today is to follow the path of sentiment analysis. It is a widely used natural language processing (NLP) technique that has shown rapid growth in recent years. It aims to identify sentiment of a given text. However, sentiment analysis is not always successful in reflecting the whole story. For example “This car looks beautiful, but does not handle very well.” comprises a positive sentiment towards the looks of the car but a negative sentiment towards its handling. This shows that a given sentence can have multiple aspects present in it and a different sentiment towards each of them.</p>
      <p>To address such difficulties, Aspect Based Sentiment Analysis (ABSA) was introduced in 2010[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>]. It aims to extract aspects from a given piece of text and its associated sentiment. There are several sub-tasks involved with ABSA among which identifying relevant entities and aspects and determining the corresponding sentiment/polarity are the most studied.</p>
      <p>The aim of this paper is to present our proposed solution to the sub-task 1 of the FiQA 2018 challenge. We are provided with a dataset that comprises of financial tweets and headlines. For each sentence, we are provided with relevant snippets in that sentence, the associated sentiment score and the aspect of each snippet. The goal of this challenge is to predict the aspects present in a given snippet and its sentiment score. Towards this objective, we design and implement a model that consists of two neural networks, one for the aspect extraction and other for the sentiment score prediction.</p>
      <p>Next, we present relevant work to the scope of this paper.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>Sentiment analysis is a sub-field of NLP that was introduced to analyse the sentiments of the text corpus. In early days, supervised learning using annotations that tag evaluative content was used to classify text into subjective or objective[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>]. Machine learning models like Naïve Bayes classifiers and support vector machines (SVMs) have also shown good performance in sentiment classificat-ion[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. In recent times, recurrent neural networks (RNN) and convolution neural networks(CNN) has shown state-of-the-art results for sentiment analysis[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]. Aspect based sentiment analysis was first proposed in 2010[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] and since then different approaches were introduced by researchers to solve this problem. Rule based approach have shown good results in aspect extraction[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] but eventually deep learning models out performed conventional models in this task too [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>].</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Methodology</h2>
        </div>
      </header>
      <p>In this section, we explain the methodology behind our framework. A sentence can have multiple targets. All the targets present in a sentence and snippet related to them are given. Task is to find the aspect present in each snippet and sentiment intensity score of sentence towards each target. Our system has two major parts, an aspect model and a sentiment model. Firstly each snippet is passed into an aspect model which recognizes the aspect present. Each target present in the sentence is used to calculate a enhanced word vector. These enhanced word vectors links sentiment score with targets. Finally, enhanced word vector is passed into the sentiment model to generate sentiment intensity score of a sentence towards the target. Figure <a class="fig" href="#fig1">1</a> represents a high level picture of our proposed framework.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191827/images/www18companion-404-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">High level framework of our model.</span>
        </div>
      </figure>
      <p>Next, we will explain the various components of our framework.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Aspect model</h3>
          </div>
        </header>
        <p>Recurrent neural networks has shown state-of-the-art performance with sentence classification and aspect extraction task[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]. Its ability to recognize long range patterns in input data makes it a favorable choice for NLP tasks. We use Bidirectional LSTM RNN for aspect extraction as they can analyze input in both forward and backward direction simultaneously.</p>
        <p>We represent sentences using word vectors because bag-of-word sentence representation is unable to capture the positional and semantic relationships between words. In order to prevent overfitting of our models, we use pre-trained word embeddings. Our system supports multiple word embeddings like Stanford GloVe[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>], Google-News-Word2Vec[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>], Godin[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>], FastText[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] and Keras<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> in-built embedding layer. Choice of word embedding is a hyperparameter. In word vector representation, each sentence is represented as a matrix <span class="inline-equation"><span class="tex">$\mathbb {R}^{n\times d}$</span></span> , where <em>n</em> is number of words in a sentence and <em>d</em> is the dimension of word embedding.</p>
        <p>Our RNN takes word vector as an input and outputs a probability distribution over 27 given aspect classes mentioned in Table <a class="tbl" href="#tab1">1</a>. The one with highest probability value is taken as the output aspect. Moreover, the given problem has only one aspect per snippet but this model can be easily extended for multiple aspects as shown by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]. For multiple aspects, instead of using class with highest probability as an aspect, every class having probability above a threshold <em>θ</em> is treated as an aspect. Here <em>θ</em> is a hyperparameter. We use Bayesian optimization[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] for finding best combination of hyperparameters. Experiment section includes exact range of hyperparameters used.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Sentiment Model</h3>
          </div>
        </header>
        <p>Multichannel convolutional neural network has shown state-of-the-art performance for sentiment classification task[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. However, all the deep learning models classify sentences into discrete classes e.g. positive, negative and neutral but for the challenge, we need to provide a sentiment intensity score in range [ − 1, 1]. Rule based models which uses a predefined intensity score of each word to calculate the sentiment score of entire sentence are proposed[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], but they fail in complex scenarios like a negative sentiment can be communicated using negation of positive words. Deep learning models are good at dealing with complex situation like these.</p>
        <p>We use a modified version of model proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. In the output layer of our CNN, we have only one neuron with sigmoid activation function. This model is then trained on a dataset of sentences and their sentiment intensity score. Enhanced word vector are given as input and output is the sentiment intensity score between the range [0, 1] which is scaled to [ − 1, 1] before reporting. In this model also, the choice of word embedding is a hyperparameter and each channel can have a different word embedding. Propagation of errors into word vectors is also kept as a hyperparameter. We use Bayesian optimization[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] for finding best combination of hyperparameters. Experiment section includes exact range of hyperparameters used.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Aspect based Sentiment Model</h3>
          </div>
        </header>
        <p>Sentiment score is calculated using enhanced word vector of the sentence to establish connection of the sentiment score with its target. We modify the technique proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] to rescale each word vector according to its relation with the given target. Instead of using constituency tree of the sentence as suggested, we use a dependency tree as they are designed for representing relationship between words in a sentence. Spacy<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a> and NetworkX<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a> are used for generating dependency tree. Now, enhanced word vector is calculated by using the given target, original word vectors and dependency tree. Due to the page limit, we do not go into the details of this technique. Interested readers can refer to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>].</p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experiment</h2>
        </div>
      </header>
      <p>In this section, we present our experiments. We give an overview of the training dataset, hyperparameter settings and results of our experiments.</p>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Dataset</h3>
          </div>
        </header>
        <p>The dataset used for training is available at FiQA 2018 website<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a>. Organizers of the task provided 435 annotated financial headlines and 675 annotated financial tweets as the training dataset. For each tweet/headline, targets and snippet relevant to the target were provided. Each target has sentiment score and an aspect. An example of data block for one sentence is shown below.</p>
        <p><img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191827/images/www18companion-404-img1.jpg" class="img-responsive" alt="" longdesc="" /></p>
        <p>The aspect of each snippet can have up to six levels but for this challenge, we are required to report aspects up to level 2 only. Level 1 has 4 unique aspects and level 2 has 27 unique aspects. Names and frequency of each aspect is shown in Table <a class="tbl" href="#tab1">1</a>. A huge class imbalance is present in the training data that can be observed in Table <a class="tbl" href="#tab1">1</a>. This makes predicting certain aspects hard.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Frequency distribution of different aspect in training dataset.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Level 1 Aspect training dataset</th>
                <th style="text-align:left;">Level 2 Aspect</th>
                <th style="text-align:left;">Frequency in</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="12" style="text-align:center;">Corporate</td>
                <td style="text-align:left;">Reputation</td>
                <td style="text-align:left;">10</td>
              </tr>
              <tr>
                <td style="text-align:left;">Company Communication</td>
                <td style="text-align:left;">8</td>
              </tr>
              <tr>
                <td style="text-align:left;">Appointment</td>
                <td style="text-align:left;">37</td>
              </tr>
              <tr>
                <td style="text-align:left;">Financial</td>
                <td style="text-align:left;">26</td>
              </tr>
              <tr>
                <td style="text-align:left;">Regulatory</td>
                <td style="text-align:left;">18</td>
              </tr>
              <tr>
                <td style="text-align:left;">Sales</td>
                <td style="text-align:left;">92</td>
              </tr>
              <tr>
                <td style="text-align:left;">M&amp;A</td>
                <td style="text-align:left;">76</td>
              </tr>
              <tr>
                <td style="text-align:left;">Legal</td>
                <td style="text-align:left;">28</td>
              </tr>
              <tr>
                <td style="text-align:left;">Dividend Policy</td>
                <td style="text-align:left;">26</td>
              </tr>
              <tr>
                <td style="text-align:left;">Risks</td>
                <td style="text-align:left;">57</td>
              </tr>
              <tr>
                <td style="text-align:left;">Rumors</td>
                <td style="text-align:left;">33</td>
              </tr>
              <tr>
                <td style="text-align:left;">Strategy</td>
                <td style="text-align:left;">49</td>
              </tr>
              <tr>
                <td rowspan="9" style="text-align:left;">Stock</td>
                <td style="text-align:left;">Options</td>
                <td style="text-align:left;">12</td>
              </tr>
              <tr>
                <td style="text-align:left;">IPO</td>
                <td style="text-align:left;">8</td>
              </tr>
              <tr>
                <td style="text-align:left;">Signal</td>
                <td style="text-align:left;">26</td>
              </tr>
              <tr>
                <td style="text-align:left;">Coverage</td>
                <td style="text-align:left;">45</td>
              </tr>
              <tr>
                <td style="text-align:left;">Fundamentals</td>
                <td style="text-align:left;">13</td>
              </tr>
              <tr>
                <td style="text-align:left;">Insider Activity</td>
                <td style="text-align:left;">5</td>
              </tr>
              <tr>
                <td style="text-align:left;">Price Action</td>
                <td style="text-align:left;">437</td>
              </tr>
              <tr>
                <td style="text-align:left;">Buyside</td>
                <td style="text-align:left;">5</td>
              </tr>
              <tr>
                <td style="text-align:left;">Technical Analysis</td>
                <td style="text-align:left;">98</td>
              </tr>
              <tr>
                <td rowspan="2" style="text-align:left;">Economy</td>
                <td style="text-align:left;">Trade</td>
                <td style="text-align:left;">2</td>
              </tr>
              <tr>
                <td style="text-align:left;">Central Banks</td>
                <td style="text-align:left;">5</td>
              </tr>
              <tr>
                <td rowspan="4" style="text-align:left;">Market</td>
                <td style="text-align:left;">Currency</td>
                <td style="text-align:left;">2</td>
              </tr>
              <tr>
                <td style="text-align:left;">Conditions</td>
                <td style="text-align:left;">3</td>
              </tr>
              <tr>
                <td style="text-align:left;">Market</td>
                <td style="text-align:left;">24</td>
              </tr>
              <tr>
                <td style="text-align:left;">Volatility</td>
                <td style="text-align:left;">11</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Sentiment scores are given in range [ − 1, 1]. Here − 1 is extremely negative and 1 is extremely positive. Figure <a class="fig" href="#fig2">2</a> shows frequency distribution of sentiment scores in the training dataset.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191827/images/www18companion-404-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Frequency distribution of sentiment scores.</span>
          </div>
        </figure>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Data Pre-processing</h3>
          </div>
        </header>
        <p>We performed standard text pre-processing steps for cleaning snippets such as removal of twitter usernames, stop words, punctuation, website links and # from all the #tags. Each snippet and sentence is padded so that they can match the length of longest snippet and sentence respectively. In result to this, the final length of snippets and sentences are 11 and 19 respectively. Moreover, the provided sentiment score is scaled to [0, 1] from [ − 1, 1] for training sentiment model.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Hyperparameter Settings</h3>
          </div>
        </header>
        <section id="sec-15">
          <p><em>4.3.1 Aspect Model.</em> A bidirectional LSTM RNN is used for aspect extraction. We use Bayesian optimization[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] for finding best combination of hyperparameters. Table <a class="tbl" href="#tab2">2</a> shows all the hyperparameters and their exact ranges used.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span> <span class="table-title">Hyperparameter ranges used for aspect model.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Hyperparameter</th>
                  <th style="text-align:left;">Range</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Learning rate</td>
                  <td style="text-align:left;">0.0001 to 0.1</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Dropout</td>
                  <td style="text-align:left;">0.1 to 0.5</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Number of LSTM units</td>
                  <td style="text-align:left;">400,500,600,700</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Word Embedding<br />
                  Google-News-Word2Vec, Keras Embedding layer</td>
                  <td style="text-align:left;">Fast Text, Godin, GloVe</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Train embedding layer flag</td>
                  <td style="text-align:left;">True, False</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Dimension of free embedding layer</td>
                  <td style="text-align:left;">100,300,400</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Batch size</td>
                  <td style="text-align:left;">16,32,64</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Epochs</td>
                  <td style="text-align:left;">10,50,70,150</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>After training 60 models with different combination of hyperparameters, model with the highest weighted average F1 score had hyperparameter values as shown in Table <a class="tbl" href="#tab3">3</a>. We use 10 fold cross validation while training to prevent overfitting on validation set. Figure <a class="fig" href="#fig3">3</a> shows weighted average F1 score of all 60 models,and the best one is marked with a red star.</p>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span> <span class="table-title">Hyperparameters of best aspect model.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Hyperparameter</th>
                  <th style="text-align:left;">Value</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Learning rate</td>
                  <td style="text-align:left;">0.00063</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Dropout</td>
                  <td style="text-align:left;">0.5</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Number of LSTM units</td>
                  <td style="text-align:left;">400</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Word Embedding</td>
                  <td style="text-align:left;">Google-News-Word2Vec</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Train embedding layer flag</td>
                  <td style="text-align:left;">False</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Dimension of free embedding layer</td>
                  <td style="text-align:left;">400</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Batch size</td>
                  <td style="text-align:left;">8</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Epochs</td>
                  <td style="text-align:left;">100</td>
                </tr>
              </tbody>
            </table>
          </div>
          <figure id="fig3">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191827/images/www18companion-404-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span> <span class="figure-title">Weighted average F1 scores of 60 aspect models, ★ denotes the best model.</span>
            </div>
          </figure>
        </section>
        <section id="sec-16">
          <p><em>4.3.2 Sentiment Model.</em> Multi-channel CNN is used for sentiment analysis. We use Bayesian optimization[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] for finding best combination of hyperparameters. Table <a class="tbl" href="#tab4">4</a> shows all the hyperparameters and their exact ranges used.</p>
          <div class="table-responsive" id="tab4">
            <div class="table-caption">
              <span class="table-number">Table 4:</span> <span class="table-title">Hyperparameter ranges used for sentiment model.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Hyperparameter</th>
                  <th style="text-align:left;">Range</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Learning rate</td>
                  <td style="text-align:left;">0.0001 to 0.1</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Dropout</td>
                  <td style="text-align:left;">0.4 to 0.9</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Number of neurons in hidden layer</td>
                  <td style="text-align:left;">100,200,300,400</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Number of filters</td>
                  <td style="text-align:left;">100,200,300,400</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Filter size</td>
                  <td style="text-align:left;">[1,2,3,4,5,6]</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Word Embedding<br />
                  Google-News-Word2Vec</td>
                  <td style="text-align:left;">GloVe, Godin,FastText</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Train embedding layer flag</td>
                  <td style="text-align:left;">True, False</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Batch size</td>
                  <td style="text-align:left;">8,16,32,64</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Epoch</td>
                  <td style="text-align:left;">10,20,30,50</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>After training 250 models with different combination of hyperparameters, model with the least mean squared error had hyperparameter values as shown in Table <a class="tbl" href="#tab5">5</a>. We use 10 fold cross validation while training to prevent overfitting on validation set. Figure <a class="fig" href="#fig4">4</a> and <a class="fig" href="#fig5">5</a> shows mean squared error and R squared score respectively of all 250 models. The best one is marked with a red star.</p>
          <div class="table-responsive" id="tab5">
            <div class="table-caption">
              <span class="table-number">Table 5:</span> <span class="table-title">Hyperparameters of best sentiment model.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Hyperparameter</th>
                  <th style="text-align:left;">Value</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Learning rate</td>
                  <td style="text-align:left;">0.0099</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Dropout</td>
                  <td style="text-align:left;">0.8676</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Number of neurons in hidden layer</td>
                  <td style="text-align:left;">400</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Number of filters</td>
                  <td style="text-align:left;">100</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Filter size<br />
                  Channel 3 = 2</td>
                  <td style="text-align:left;">Channel 1 = 2, Channel 2 = 3</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Word Embedding<br />
                  Channel 2 = Google-News-Word2Vec<br /></td>
                  <td style="text-align:left;">Channel 1 = Godin</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Train embedding layer flag</td>
                  <td style="text-align:left;">False</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Batch size</td>
                  <td style="text-align:left;">64</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Epochs</td>
                  <td style="text-align:left;">50</td>
                </tr>
              </tbody>
            </table>
          </div>
          <figure id="fig4">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191827/images/www18companion-404-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span> <span class="figure-title">Mean squared error of 250 sentiment models, ★ denotes the best model.</span>
            </div>
          </figure>
          <figure id="fig5">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191827/images/www18companion-404-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span> <span class="figure-title">R squared score of 250 sentiment models, ★ denotes the best model.</span>
            </div>
          </figure>
        </section>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Results and Discussions</h3>
          </div>
        </header>
        <p>Gold Standard dataset is not released, so we use 10 fold cross validation to evaluate our models. F1 scores are used to evaluate performance of aspect model. Due to the class imbalance present in aspect data, we decide to use weighted average F1 score as a performance measure because it keeps an account for class imbalances while calculating the score. We achieved a weighted average F1 score of 0.69.</p>
        <p>For measuring the performance of the sentiment model, we use mean squared error and R squared score. Our best sentiment model has R squared score of 0.288 and predicts sentiment scores with MSE of 0.112.</p>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion and Future Works</h2>
        </div>
      </header>
      <p>In this paper, we present a framework of deep learning models to do aspect-based sentiment analysis on financial tweets and headlines. Our model shows promising performance. We propose a novel approach to find sentiment intensity score of a sentence using deep learning model. In future, we aim to train these models on a larger dataset to capture information about the aspect classes that were not studied properly due to unavailability of enough training samples in the current dataset. Ensemble of deep learning models has shown state-of-the-art performance in recent times[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] and we would like to study the effects of ensemble on our models too. We also aim to evaluate the performance of these models on data of other domains.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with Subword Information. <em><em>arXiv preprint arXiv:1607.04606</em></em> (2016).</li>
        <li id="BibPLXBIB0002" label="[2]">Jasper&nbsp;Friedrichs Debanjan&nbsp;Mahata, Shubham&nbsp;Gupta. 2017. InfyNLP at SMM4H Task 2: Stacked Ensemble of Shallow Convolutional Neural Networks for Identifying Personal Medication Intake from Twitter. In <em><em>Proceedings of the Second Workshop on Social Media Mining for Health Applications (SMM4H). Health Language Processing Laboratory; 2017</em></em> , Vol.&nbsp;1996. 68–71.</li>
        <li id="BibPLXBIB0003" label="[3]">CJ&nbsp;Hutto&nbsp;Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In <em><em>Eighth International Conference on Weblogs and Social Media (ICWSM-14). Available at (20/04/16) <a class="link-inline force-break" href="http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf" target="_blank">http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf</a></em></em> .</li>
        <li id="BibPLXBIB0004" label="[4]">Fréderic Godin, Baptist Vandersmissen, Wesley De&nbsp;Neve, and Rik Van&nbsp;de Walle. 2015. Multimedia Lab @ ACL WNUT NER Shared Task: Named Entity Recognition for Twitter Microposts using Distributed Word Representations. In <em><em>Proceedings of the Workshop on Noisy User-generated Text</em></em> . 146–153.</li>
        <li id="BibPLXBIB0005" label="[5]">Soufian Jebbara and Philipp Cimiano. 2016. Aspect-Based Sentiment Analysis Using a Two-Step Neural Network Architecture. In <em><em>Semantic Web Challenges</em></em> , Harald Sack, Stefan Dietze, Anna Tordai, and Christoph Lange (Eds.). Springer International Publishing, Cham, 153–167.</li>
        <li id="BibPLXBIB0006" label="[6]">Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. <em><em>CoRR</em></em> abs/1408.5882(2014). arxiv:1408.5882 <a class="link-inline force-break" href="http://arxiv.org/abs/1408.5882" target="_blank">http://arxiv.org/abs/1408.5882</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&nbsp;S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In <em><em>Advances in Neural Information Processing Systems 26</em></em> , C.&nbsp;J.&nbsp;C. Burges, L.&nbsp;Bottou, M.&nbsp;Welling, Z.&nbsp;Ghahramani, and K.&nbsp;Q. Weinberger (Eds.). Curran Associates, Inc., 3111–3119. <a class="link-inline force-break" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs Up?: Sentiment Classification Using Machine Learning Techniques. In <em><em>Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10</em></em> (<em>EMNLP ’02</em>). Association for Computational Linguistics, Stroudsburg, PA, USA, 79–86. <a class="link-inline force-break" href="https://doi.org/10.3115/1118693.1118704" target="_blank">https://doi.org/10.3115/1118693.1118704</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">Jeffrey Pennington, Richard Socher, and Christopher&nbsp;D. Manning. 2014. GloVe: Global Vectors for Word Representation. In <em><em>Empirical Methods in Natural Language Processing (EMNLP)</em></em> . 1532–1543. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/D14-1162" target="_blank">http://www.aclweb.org/anthology/D14-1162</a>
        </li>
        <li id="BibPLXBIB0010" label="[10]">Soujanya Poria, Erik Cambria, Lun-Wei Ku, Chen Gui, and Alexander Gelbukh. 2014. A rule-based approach to aspect extraction from product reviews. In <em><em>Proceedings of the second workshop on natural language processing for social media (SocialNLP)</em></em> . 28–37.</li>
        <li id="BibPLXBIB0011" label="[11]">Rajiv&nbsp;Ratn Shah, Yi Yu, Akshay Verma, Suhua Tang, Anwar&nbsp;Dilawar Shaikh, and Roger Zimmermann. 2016. Leveraging multimodal information for event summarization and concept-level sentiment analysis. <em><em>Knowledge-Based Systems</em></em> 108 (2016), 102–109.</li>
        <li id="BibPLXBIB0012" label="[12]">Jasper Snoek, Hugo Larochelle, and Ryan&nbsp;P Adams. 2012. Practical bayesian optimization of machine learning algorithms. In <em><em>Advances in neural information processing systems</em></em> . 2951–2959.</li>
        <li id="BibPLXBIB0013" label="[13]">Tun&nbsp;Thura Thet, Jin-Cheon Na, and Christopher&nbsp;S.G. Khoo. 2010. Aspect-based Sentiment Analysis of Movie Reviews on Discussion Boards. <em><em>J. Inf. Sci.</em></em> 36, 6 (Dec. 2010), 823–848. <a class="link-inline force-break" href="https://doi.org/10.1177/0165551510388123" target="_blank">https://doi.org/10.1177/0165551510388123</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Bo Wang and Min Liu. [n. d.]. Deep learning for aspect-based sentiment analysis. ([n. d.]).</li>
        <li id="BibPLXBIB0015" label="[15]">Janyce&nbsp;M. Wiebe, Rebecca&nbsp;F. Bruce, and Thomas&nbsp;P. O'Hara. 1999. Development and Use of a Gold-standard Data Set for Subjectivity Classifications. In <em><em>Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics</em></em> (<em>ACL ’99</em>). Association for Computational Linguistics, Stroudsburg, PA, USA, 246–253. <a class="link-inline force-break" href="https://doi.org/10.3115/1034678.1034721" target="_blank">https://doi.org/10.3115/1034678.1034721</a>
        </li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://sites.google.com/view/fiqa/home">https://sites.google.com/view/fiqa/home</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://keras.io/layers/embeddings/">https://keras.io/layers/embeddings/</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://spacy.io/">https://spacy.io/</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="http://networkx.github.io/">http://networkx.github.io/</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class="link-inline force-break" href="https://sites.google.com/view/fiqa/home">https://sites.google.com/view/fiqa/home</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, Companion, April 23–27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191827">https://doi.org/10.1145/3184558.3191827</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
