<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Online Compact Convexified Factorization Machine</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Online Compact Convexified Factorization Machine</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Xiao</span>      <span class="surName">Lin</span><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a>,     Tsinghua University</div>     <div class="author">     <span class="givenName">Wenpeng</span>      <span class="surName">Zhang</span><a class="fn" href="#fn2" id="foot-fn2"><sup>&#x2020;</sup></a>,     Tsinghua University</div>     <div class="author">     <span class="givenName">Min</span>      <span class="surName">Zhang</span><a class="fn" href="#fn3" id="foot-fn3"><sup>&#x2021;</sup></a>,     Tsinghua University</div>     <div class="author">     <span class="givenName">Wenwu</span>      <span class="surName">Zhu</span><a class="fn" href="#fn4" id="foot-fn4"><sup>&#x00A7;</sup></a>,     Tsinghua University</div>     <div class="author">     <span class="givenName">Jian</span>      <span class="surName">Pei</span>,     JD.com and Simon Fraser University     </div>     <div class="author">     <span class="givenName">Peilin</span>      <span class="surName">Zhao</span>,     South China University of Technology     </div>     <div class="author">     <span class="givenName">Junzhou</span>      <span class="surName">Huang</span>,     Tencent AI Lab     </div>                                  </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186075" target="_blank">https://doi.org/10.1145/3178876.3186075</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Factorization Machine (FM) is a supervised learning approach with a powerful capability of feature engineering. It yields state-of-the-art performances in various batch learning tasks where all the training data is made available prior to the training. However, in real-world applications where the data arrives sequentially in a streaming manner, the high cost of re-training with batch learning algorithms has posed formidable challenges in the online learning scenario. The initial challenge is that no prior formulations of FM could directly fulfill the requirements in Online Convex Optimization (OCO) &#x2013; the paramount framework for online learning algorithm design. To address this aforementioned challenge, we invent a new convexification scheme leading to a Compact Convexified FM (CCFM) that seamlessly meets the requirements in OCO. However for learning Compact Convexified FM (CCFM) in the online learning settings, most existing algorithms suffer from expensive projection operations. To address this subsequent challenge, we follow the general projection-free algorithmic framework of Online Conditional Gradient and propose an Online Compact Convex Factorization Machine (OCCFM) algorithm that eschews the projection operation with efficient linear optimization steps. In support of the proposed OCCFM in terms of its theoretical foundation, we prove that the developed algorithm achieves a sub-linear regret bound. To evaluate the empirical performance of OCCFM, we conduct extensive experiments on 6 real-world datasets for online regression and online classification tasks. The experimental results show that OCCFM outperforms the state-of-art online learning methods for FM.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Xiao Lin, Wenpeng Zhang, Min Zhang, Wenwu Zhu, Jian Pei, Peilin Zhao, and Junzhou Huang. 2018. Online Compact Convexified Factorization Machine. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em>, 10 Pages. <a href="https://doi.org/10.1145/3178876.3186075" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186075</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Factorization Machine (FM)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] is a generic approach for supervised learning . It provides an efficient mechanism for feature engineering, capturing the first-order information of each input feature as well as the second-order pairwise feature interactions with a low-rank matrix in a factorized form. FM achieves state-of-the-art performances in various applications, including recommendation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>], computational advertising&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>], search ranking&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] and toxicogenomics prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], and so on. As such, FM has recently regained significant attention from researchers&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] due to its increasing popularity in industrial applications and data science competitions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>].</p>    <p>Despite of the overwhelming research on Factorization Machine, majority of the existing studies are conducted in the batch learning settings where all the training data is available before training. However, in many real-world scenarios, like online recommendation and online advertising&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>], the training data arrives sequentially in a streaming fashion. If the batch learning algorithms are applied in accordance with the streams in such scenarios, the models have to be re-trained each time new data arrives. Since these data streams usually arrive in large-scales and are changing constantly in a real-time manner, the incurred high re-training cost makes batch learning algorithms impractical in such settings. This creates an impending need for an efficient online learning algorithm for Factorization Machine. Moreover, it is expected that the online learning algorithm has a theoretical guarantee for its performance.</p>    <p>In the current paper, we aim to develop an ideal algorithm based on the paramount framework &#x2013; Online Convex Optimization (OCO) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] in the online learning settings. Unfortunately, extant formulations can not directly fulfill the two fundamental requirements demanded in OCO: (i) any instance of all the parameters should be represented as a single point from a convex compact decision set; and, (ii) the loss incurred by the prediction should be formulated as a convex function over the decision set. Indeed, in most existing formulations for FM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], the loss functions are non-convex with respect to the factorized feature interaction matrix, thus violating requirement (ii). Further, although some studies have proposed formulations for convex FM which rectify the non-convexity problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], they still treat the feature weight vector and the feature interaction matrix as separated parameters, thus violating requirement (i) in OCO.</p>    <p>To address these problems, we propose a new convexification scheme for FM. Specifically, we rewrite the global bias, the feature weight vector and the feature interaction matrix into a compact augmented symmetric matrix and restrict the augmented matrix with a nuclear norm bound, which is a convex surrogate of the low-rank constraint&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. Therefore the augmented matrices form a convex compact decision set which is essentially a symmetric bounded nuclear norm ball. Then we rewrite the prediction of FM into a convex linear function with respect to the augmented matrix, thus the loss incurred by the prediction is convex. Based on the convexification scheme, the resulting formulation of <em>Compact Convexified FM</em>&#x00A0;(CCFM) can seamlessly meet the aforementioned requirements of the OCO framework.</p>    <p>Yet, when we investigate various online learning algorithms for Compact Convexified Factorization Machine within the OCO framework, we find that most of existing online learning algorithms involve a projection step in every iteration. When the decision set is a bounded nuclear norm ball, the projection amounts to the computationally expensive Singular Value Decomposition (SVD), and consequently limits the applicability of most online learning algorithms. Notably, one exceptional algorithm is Online Conditional Gradient (OCG), which eschews the projection operation by a linear optimization step. When OCG is applied to the nuclear norm ball, the linear optimization amounts to the computation of maximal singular vectors of a matrix, which is much simpler. However, it remains theoretically unknown whether an algorithm similar to OCG still exists for the specific subset of a bounded nuclear norm ball.</p>    <p>In response, we propose an online learning algorithm for CCFM, which is named as <em>Online Compact Convexified Factorization Machine</em>&#x00A0;(OCCFM). We prove that when the decision set is a symmetric nuclear norm ball, the linear optimization needed for an OCG-alike algorithm still exsits, i.e. it amounts to the computation of the maximal singular vectors of a specific symmetric matrix. Based on this finding, we propose an OCG-alike online learning algorithm for OCCFM. As OCCFM is a variant of OCG, we prove that the theoretical analysis of OCG still fits for OCCFM, which achieves a sub-linear regret bound in order of <em>O</em>(<em>T</em>     <sup>3/4</sup>). Further, we conduct extensive experiments on real-world datasets to evaluate the empirical performance of OCCFM. As shown in the experimental results in both online regression and online classification tasks, OCCFM outperforms the state-of-art online learning algorithms in terms of both efficiency and prediction accuracy.</p>    <p>The contributions of this work are summarized as follows:</p>    <ol class="list-no-style">     <li id="list1" label="(1)">To the best of our knowledge, we are the first to propose the Online Compact Convexified Factorization Machine with theoretical guarantees, which is a new variant of FM in the online learning settings.<br/></li>     <li id="list2" label="(2)">The proposed formulation of CCFM is superior to prior research in that it seamlessly fits into the paramount online learning framework of Online Convex Optimization. Moreover, this formulation can be used in not only online settings, but also batch and stochastic settings.<br/></li>     <li id="list3" label="(3)">We propose a routine for the linear optimization on the decision set of CCFM based on the Online Conditional Gradient algorithm, leading to an OCG-alike online learning algorithm named as OCCFM. Moreover, this finding also applies to any other online learning task whose decision set is the symmetric bounded nuclear norm ball.<br/></li>     <li id="list4" label="(4)">We evaluate the performance of the proposed OCCFM on both online regression and online classification tasks, showing that OCCFM outperforms state-of-art online learning algorithms.<br/></li>    </ol>    <p>We believe our work sheds light on the Factorization Machine research, especially for online Factorization Machine. Due to the wide application of FM, our work is of both theoretical and practical significance.</p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>     </div>    </header>    <p>In this section, we first review more details of factorization machine, and then provide an introduction to the online convex optimization framework and the online conditional gradient algorithm, both of which we utilize to design our online learning algorithm for factorization machine.</p>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Factorization Machine</h3>     </div>     </header>     <p>Factorization Machine&#x00A0;(FM)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] is a general supervised learning approach working with any real-valued feature vector. The most important characteristic of FM is the capability of powerful feature engineering. In addition to the usual first-order information of each input feature, it captures the second-order pairwise feature interaction in modeling, which leads to stronger expressiveness than linear models.</p>     <p>Given an input feature vector <span class="inline-equation"><span class="tex">$ {x}\in \mathbb {R}^{d}$</span>     </span>, vanilla FM makes the prediction <span class="inline-equation"><span class="tex">$\hat{y}\in \mathbb {R}$</span>     </span> with the following formula: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \hat{y}({x},\omega _0, {\omega }, {V})=\omega _0+ {\omega }^{T} {x}+\sum _{i=1}^d\sum _{j=i+1}^d({VV}^T)_{ij}x_ix_j,\end{equation*} </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$\omega _0\in \mathbb {R}$</span>     </span> is the bias term, <span class="inline-equation"><span class="tex">$ {\omega }\in \mathbb {R}^{d}$</span>     </span> is the first-order feature weight vector and <span class="inline-equation"><span class="tex">$ {V}\in \mathbb {R}^{d\times k}$</span>     </span> is the second-order factorized feature interaction matrix; (VV<sup>      <em>T</em>     </sup>)<sub>      <em>ij</em>     </sub> is the entry in the <em>i</em>-th row and <em>j</em>-th column of matrix VV<sup>      <em>T</em>     </sup>, and <em>k</em> &#x226A; <em>d</em> is the hyper-parameter determining the rank of V. As shown in previous studies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], <span class="inline-equation"><span class="tex">$\hat{y}$</span>     </span> is non-convex with respect to V.</p>     <p>The non-convexity in vanilla FM will result in some problems in practice, such as local minima and instability of convergence. In order to overcome these problems, two formulations for convex FM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] have been proposed, where the feature interaction is directly modeled as <span class="inline-equation"><span class="tex">$ {Z}\in \mathbb {R}^{d\times d}$</span>     </span> rather than VV<sup>      <em>T</em>     </sup> in the factorized form which induces non-convexity. The matrix Z is then imposed upon a bounded nuclear norm constraint to maintain the low-rank property. In general, convex FM allows for more general modeling of feature interaction and is quite effective in practice. The difference between the two formulations is whether the diagonal entries of Z are utilized in prediction. For clarity, we refer to the formulation in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] as Convex FM&#x00A0;(1) and that in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] as Convex FM&#x00A0;(2).</p>     <p>Note that we propose a new convexification scheme for FM in this work, which is inherently different from the above two formulations for convex FM. The details of our convexification scheme and its comparison with convex FMs will be presented in Section&#x00A0;<a class="sec" href="#sec-14">3.1</a>.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Online Convex Optimization</h3>     </div>     </header>     <p>Online Convex Optimization&#x00A0;(OCO)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] is the paramount framework for designing online learning algorithms. It can be seen as a structured repeated game between a learner and an adversary. At each round <em>t</em> &#x2208; {1, 2, &#x22C5;&#x22C5;&#x22C5;, <em>T</em>}, the learner is required to generate a decision point x<sub>      <em>t</em>     </sub> from a convex compact set <span class="inline-equation"><span class="tex">$\mathcal {Q}\subseteq \mathbb {R}^{n}$</span>     </span>. Then the adversary replies the learner&#x0027;s decision with a convex loss function <span class="inline-equation"><span class="tex">$f_{t}:~\mathcal {Q}\rightarrow \mathbb {R}$</span>     </span> and the learner suffers the loss <em>f<sub>t</sub>     </em>(x<sub>      <em>t</em>     </sub>). The goal of the learner is to generate a sequence of decisions {x<sub>      <em>t</em>     </sub>|~<em>t</em> = 1, 2, &#x22C5;&#x22C5;&#x22C5;, <em>T</em>} so that the regret with respect to the best fixed decision in hindsight <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \mathbf {regret}_T=\sum _{t=1}^Tf_t({x}_t)-\min _{ {x}^{*}\in \mathcal {Q}}\sum _{t=1}^Tf_t({x}^{*})\end{equation*} </span>       <br/>       <span class="equation-number">(2.2)</span>      </div>     </div> is sub-linear in <em>T</em>, i.e. <span class="inline-equation"><span class="tex">$\lim \limits _{T\rightarrow \infty }\frac{1}{T}\mathbf {regret}_T=0$</span>     </span>. The sub-linearity implies that when <em>T</em> is large enough, the learner can perform as well as the best fixed decision in hindsight.</p>     <p>Based on the OCO framework, many online learning algorithms have been proposed and successfully applied in various applications &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]. The two most popular representatives of them are Online Gradient Descent&#x00A0;(OGD)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] and Follow-The-Regularized-Leader&#x00A0;(FTRL)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>].</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Online Conditional Gradient</h3>     </div>     </header>     <p>Online Conditional Gradient&#x00A0;(OCG)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>] is a projection-free online learning algorithm that eschews the possible computationally expensive projection operation needed in its counterparts, including OGD and FTRL. It enjoys great computational advantage over other online learning algorithms when the decision set is a bounded nuclear norm ball. As will be shown in Section&#x00A0;<a class="sec" href="#sec-13">3</a>, we utilize a similar decision set in our proposed convexification scheme, thus we use OCG as the cornerstone in our algorithm design. In the following, we introduce more details about it.</p>     <p>In practice, to ensure that the newly generated decision points lie inside the decision set of interest, most online learning algorithms invoke a projection operation in every iteration. For example, in OGD, when the gradient descent step generates an infeasible iterate that lies out of the decision set, we have to project it back to regain feasibility. In general, this kind of projection amounts to solving a quadratic convex program over the decision set and will not cause much problem. However, when the decision sets are of specific types, such as the bounded nuclear norm ball, the set of all semi-definite matrices and polytopes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>], it turns to amount to very expensive algebraic operations. To avoid these expensive operations, projection-free online learning algorithm OCG has been proposed. It is much more efficient since it eschews the projection operation by using a linear optimization step instead in every iteration. For example, when the decision set is a bounded nuclear norm ball, the projection operation needed in other online learning algorithms amounts to computing a full singular value decomposition&#x00A0;(SVD) of a matrix, while the linear optimization step in OCG amounts to only computing the maximal singular vectors, which is at least one order of magnitude simpler. Recently, a decentralized distributed variant of OCG algorithm has been proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>], which allows for high efficiency in handling large-scale machine learning problems.<sup>1</sup><a class="fn" href="#fn5" id="foot-fn5"><sup/></a>     </p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186075/images/www2018-84-img1.svg" class="img-responsive" alt="" longdesc=""/>     </p>    </section>   </section>   <section id="sec-13">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Online Compact Convexified Factorization Machine</h2>     </div>    </header>    <p>In this section, we turn to the development of our Online Compact Convexified Factorization Machine. As introduced before, OCO is the paramount framework for designing online learning algorithms. There are two fundamental requirements in it: (i) any instance of all the model parameters should be represented as a single point from a convex compact decision set; (ii) the loss incurred by the prediction should be formulated as a convex function over the decision set.</p>    <p>Vanilla FM can not directly fit into the OCO framework due to the following two reasons. First, vanilla FM contains both the unconstrained feature weight vector &#x03C9;&#x00A0;(<em>&#x03C9;</em>     <sub>0</sub> can be written into &#x03C9;) and the factorized feature interaction matrix V; they are separated components which can not be formulated as a single point from a decision set, making it difficult to adapt to OCO framework directly. Second, as the prediction <span class="inline-equation"><span class="tex">$\hat{y}$</span>     </span> in vanilla FM is non-convex with respect to V&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], when we plug it into the loss function <em>f<sub>t</sub>     </em> at each round <em>t</em>, the resulting loss incurred by the prediction is also non-convex with respect to V. Although some previous studies have proposed two formulations for convex FM, they cannot directly fit into the OCO framework either. Similar to vanilla FM, the two convex formulations treat the unconstrained feature weight vector &#x03C9; and the feature interaction matrix Z as separated components, which violates requirement (i).</p>    <p>In order to meet the requirements of the OCO framework, we first invent a new convexification scheme for FM which results in a formulation named as Compact Convexified FM&#x00A0;(CCFM). Then based on the new formulation, we design an online learning algorithm &#x2013; Online Compact Convexified Factorization Machine&#x00A0;(OCCFM), which is a new variant of the OCG algorithm tailored to our setting.</p>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Compact Convexified Factorization Machine</h3>     </div>     </header>     <p>The main idea of our proposed convexification scheme is to put all the parameters, including the bias term, the first-order feature weight vector and the second-order feature interaction matrix, into a single augmented matrix which is then enforced to be low-rank. As the low-rank property is not a convex constraint, we approximate it with a bounded nuclear norm constraint, which is a common practice in the machine learning community&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. The details of the scheme are given in the following.</p>     <p>Recall that in vanilla FM, <em>&#x03C9;</em>     <sub>0</sub>, &#x03C9; and V are referred to as the bias term, the linear feature weight vector and the factorized feature interaction matrix respectively. Due to the non-convexity of the prediction <span class="inline-equation"><span class="tex">$\hat{y}$</span>     </span> with respect to V, we adopt a symmetric matrix <span class="inline-equation"><span class="tex">$ {Z}\in \mathbb {R}^{d\times d}$</span>     </span> to model the pairwise feature interactions, which is a popular practice in designing convex variants of FM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. Then we rewrite the bias term <em>&#x03C9;</em>     <sub>0</sub>, the first-order feature weight vector &#x03C9; and the second-order feature interaction matrix Z into a single compact augmented matrix C: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right],\end{equation*} </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$ {Z}= {Z}^T\in \mathbb {R}^{d\times d},~ {\omega }\in \mathbb {R}^{d},~\omega _0\in \mathbb {R}$</span>     </span>.</p>     <p>In order to achieve a model with low complexity, we restrict the augmented matrix C to be low-rank. However, the constraint over the rank of a matrix is non-convex and does not fit into the OCO framework. A typical approximation of the rank of a matrix C is its nuclear norm &#x2016;C&#x2016;<sub>      <em>tr</em>     </sub> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]: <span class="inline-equation"><span class="tex">$\Vert {C}\Vert _{tr}=tr(\sqrt { {C^TC}})=\sum _{i=1}^d\sigma _{i}$</span>     </span>, where <em>&#x03C3;<sub>i</sub>     </em> is the <em>i</em>-th singular value of C. As the singular values are non-negative, the nuclear norm is essentially a convex surrogate of the rank of the matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. Therefore it is standard to consider a relaxation that replaces the rank constraint by the bounded nuclear norm &#x2016;C&#x2016;<sub>      <em>tr</em>     </sub> &#x2264; <em>&#x03B4;</em>.</p>     <p>Denote <span class="inline-equation"><span class="tex">$\mathcal {S}$</span>     </span> as the set of symmetric matrices: <span class="inline-equation"><span class="tex">$\mathcal {S}^{d\times d}=\lbrace {X}|~ {X}\in \mathbb {R}^{d\times d},~ {X}= {X}^T\rbrace$</span>     </span>, the resulting decision set of the augmented matrices C can be written as the following formulation: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \mathcal {K}=\lbrace {C}| {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right],\!\Vert {C}\Vert _{tr}\le \delta , {Z}\in \mathcal {S}^{d\times d},\! {\omega }\in \mathbb {R}^{d},\!\omega _0\in \mathbb {R}\rbrace .\end{equation*} </span>       <br/>      </div>     </div> As the set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>     </span> is bounded and closed, it is also compact. Next we prove that the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>     </span> is convex in Lemma <a class="enc" href="#enc1">1</a>.</p>     <div class="lemma" id="enc1">     <Label>lemma 1.</Label>     <p> The set <span class="inline-equation"><span class="tex">$\mathcal {K}=\lbrace {C}| {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right],~\Vert {C}\Vert _{tr}\le \delta ,~ {Z}\in \mathcal {S}^{d\times d},~ {\omega }\in \mathbb {R}^{d},~\omega _0\in \mathbb {R}\rbrace$</span>      </span> is convex.</p>     </div>     <div class="proof" id="proof1">     <Label>Proof.</Label>     <p> The proof is based on an important property of convex sets: if two sets <em>S</em>      <sub>1</sub> and <em>S</em>      <sub>2</sub> are convex, then their intersection <em>S</em> = <em>S</em>      <sub>1</sub>&#x2229;<em>S</em>      <sub>2</sub> is also convex [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>]. In our case, the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>      </span> is an intersection of <span class="inline-equation"><span class="tex">$\tilde{\mathcal {K}}$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>      </span>, where <span class="inline-equation"><span class="tex">$\tilde{\mathcal {K}}=\lbrace {C}| {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right],~ {Z}\in \mathcal {S}^{d\times d},~ {\omega }\in \mathbb {R}^{d},~\omega _0\in \mathbb {R}\rbrace$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {B}=\lbrace {C}|\Vert {C}\Vert _{tr}\le \delta ,~ {C}\in \mathbb {R}^{(d+1)\times (d+1)}\rbrace$</span>      </span>. To prove <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>      </span> is convex, we prove that both <span class="inline-equation"><span class="tex">$\tilde{\mathcal {K}}$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>      </span> are convex sets.</p>     <p>First, we prove that <span class="inline-equation"><span class="tex">$\tilde{\mathcal {K}}$</span>      </span> is a convex set. For any two points <span class="inline-equation"><span class="tex">$ {C}_1,\, {C}_2\in \tilde{\mathcal {K}}$</span>      </span> and any <em>&#x03B1;</em> &#x2208; [0, 1], we have <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{aligned} \alpha {C}_1+(1-\alpha) {C}_2 =~&#x0026;\alpha \left[ \begin{array}{ccc} {Z}_1 &#x0026; {\omega }_1 \\ {\omega }_1^T &#x0026; 2\omega _0 \end{array} \right] +(1-\alpha)\left[ \begin{array}{ccc} {Z}_2 &#x0026; {\omega }_2 \\ {\omega }_2^T &#x0026; 2\omega _0 \end{array} \right]\\ =~&#x0026;\left[ \begin{array}{ccc} {Z}_{\alpha } &#x0026; {\omega }_{\alpha } \\ {\omega }_{\alpha }^T &#x0026; 2\omega _0 \end{array} \right], \end{aligned}\end{equation*} </span>        <br/>       </div>      </div> where Z<sub>       <em>&#x03B1;</em>      </sub> = <em>&#x03B1;</em>Z<sub>1</sub> + (1 &#x2212; <em>&#x03B1;</em>)Z<sub>2</sub> and &#x03C9;<sub>       <em>&#x03B1;</em>      </sub> = <em>&#x03B1;</em>&#x03C9;<sub>1</sub> + (1 &#x2212; <em>&#x03B1;</em>)&#x03C9;<sub>2</sub>. By definition of <span class="inline-equation"><span class="tex">$\tilde{\mathcal {K}}$</span>      </span>, <span class="inline-equation"><span class="tex">$ {Z}_1= {Z}_1^T$</span>      </span>, <span class="inline-equation"><span class="tex">$ {Z}_2= {Z}_2^T$</span>      </span>, thus <span class="inline-equation"><span class="tex">$\alpha {Z}_1+(1-\alpha) {Z}_2=\alpha {Z}_1^T+(1-\alpha) {Z}_2^T$</span>      </span>. Following the property of transpose, <span class="inline-equation"><span class="tex">$\alpha {\omega }_1^T+(1-\alpha) {\omega }_2^T = (\alpha {\omega }_1+(1-\alpha) {\omega }_2)^T$</span>      </span>. Therefore we obtain <span class="inline-equation"><span class="tex">$\alpha {C}_1+(1-\alpha) {C}_2\in \tilde{\mathcal {K}}$</span>      </span>. By definition, <span class="inline-equation"><span class="tex">$\tilde{\mathcal {K}}$</span>      </span> is convex.</p>     <p>Second, it remains to prove that <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>      </span> is also a convex set. The bounded nuclear norm ball is a typical convex set of matrices [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>], which is widely adopted in the machine learning community [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0026">26</a>]. The detailed proof can be found in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>].</p>     <p>Following the convexity preserving property under the intersection of convex sets, we have <span class="inline-equation"><span class="tex">$\mathcal {K}=\tilde{\mathcal {K}}\cap \mathcal {B}$</span>      </span> is also convex.</p>     </div>     <p>As the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>     </span> consists of augmented matrices with bounded nuclear norm, by the properties of block matrix, we show that the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>     </span> is equivalent to a symmetric nuclear norm ball in Lemma <a class="enc" href="#enc2">2</a>.</p>     <div class="lemma" id="enc2">     <Label>lemma 2.</Label>     <p> The sets <span class="inline-equation"><span class="tex">$\mathcal {K}=\lbrace {C}| {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right],~\Vert {C}\Vert _{tr}\le \delta ,~ {Z}\in \mathcal {S}^{d\times d},~ {\omega }\in \mathbb {R}^{d}\rbrace$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {K}^{^{\prime }}=\lbrace {C}| {C}\in \mathcal {S}^{(d+1)\times (d+1)},\Vert {C}\Vert _{tr}\le \delta \rbrace$</span>      </span> are equivalent: <span class="inline-equation"><span class="tex">$\mathcal {K}=\mathcal {K}^{^{\prime }}$</span>      </span>.</p>     </div>     <p>The proof is straight-forward by writing an arbitrary symmetric matrix into a block form, the details are omitted here.</p>     <p>By choosing a point C from the compact decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>     </span>, the prediction <span class="inline-equation"><span class="tex">$\hat{y}({C})$</span>     </span> of an instance x is formulated as: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \hat{y}({C})=\frac{1}{2} {\hat{x}}^T {C\hat{x}},\end{equation*} </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {K},~ {\hat{x}}= \left[ \begin{array}{ccc} {x} \\ 1 \end{array} \right].$</span>     </span> Although the prediction function <span class="inline-equation"><span class="tex">$\hat{y}$</span>     </span> has a similar form with the feature interaction component <span class="inline-equation"><span class="tex">$\frac{1}{2} {x}^T {Z} {x}$</span>     </span> of vanilla FM, they are intrinsically different. Plugging the formulation of <span class="inline-equation"><span class="tex">$ {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right]$</span>     </span> and <span class="inline-equation"><span class="tex">$ {\hat{x}}= \left[ \begin{array}{ccc} {x} \\ 1 \end{array} \right]$</span>     </span> into the prediction function, we have <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \hat{y}({C})=\frac{1}{2} {\hat{x}}^T {C\hat{x}}=\omega _0+ {\omega }^T {x}+\frac{1}{2} {x}^T {Z} {x}.\end{equation*} </span>       <br/>      </div>     </div> Therefore the prediction function <span class="inline-equation"><span class="tex">$\hat{y}({C})$</span>     </span> contains all the three components in vanilla FM, including the global bias, the first-order feature weight and the second-order feature interactions. Most importantly, <span class="inline-equation"><span class="tex">$\hat{y}({C})$</span>     </span> is a convex function in C, as shown in Lemma. <a class="enc" href="#enc3">3</a>.</p>     <div class="lemma" id="enc3">     <Label>lemma 3.</Label>     <p> The prediction function <span class="inline-equation"><span class="tex">$\hat{y}({C})$</span>      </span> is a convex function of <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {K}$</span>      </span>, where <span class="inline-equation"><span class="tex">$\mathcal {K}=\lbrace {C}| {C}=\left[ \begin{array}{ccc} {Z} &#x0026; {\omega } \\ {\omega }^T &#x0026; 2\omega _0 \end{array} \right],~\Vert {C}\Vert _{tr}\le \delta ,~ {Z}\in \mathcal {S}^{d\times d},~ {\omega }\in \mathbb {R}^{d},~\omega _0\in \mathbb {R}\rbrace$</span>      </span>.</p>     </div>     <div class="proof" id="proof2">     <Label>Proof.</Label>     <p> By definition, <span class="inline-equation"><span class="tex">$\hat{y}({C})$</span>      </span> is a separable function: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \hat{y}({C})=g({Z})+h({\omega })+\omega _0,\end{equation*} </span>        <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">$g({Z})=\frac{1}{2} {x}^T {Z} {x},~h({\omega })= {\omega }^{T} {x}$</span>      </span>. By definition, <em>g</em>(Z) and <em>h</em>(&#x03C9;) are linear functions with respect with Z and &#x03C9; correspondingly, and thus are convex functions. Consider <span class="inline-equation"><span class="tex">$\forall \,\alpha \in [0,\,1],~\forall \, {C}_1=\left[ \begin{array}{ccc} {Z}_1 &#x0026; {\omega }_1 \\ {\omega }_1^T &#x0026; 2\omega _0 \end{array} \right],~ {C}_2=\left[ \begin{array}{ccc} {Z}_2 &#x0026; {\omega }_2 \\ {\omega }_2^T &#x0026; 2\omega _0 \end{array} \right]\in \mathcal {K}$</span>      </span>, according to the separable formulation of <span class="inline-equation"><span class="tex">$\hat{y}$</span>      </span>, we obtain <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \hat{y}(\alpha {C}_1+(1-\alpha) {C}_2) =~g(\alpha {Z}_1+(1-\alpha) {Z}_2)+h(\alpha {\omega }_1+(1-\alpha) {\omega }_2)+\omega _0.\end{equation*} </span>        <br/>       </div>      </div> By definition of the linear functions <em>g</em>(Z) and <em>h</em>(&#x03C9;) and the rearrangement of the formulation, we have <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{aligned} &#x0026;g(\alpha {Z}_1+(1-\alpha) {Z}_2)+h(\alpha {\omega }_1+(1-\alpha) {\omega }_2)+\omega _0\\ =~&#x0026;\alpha g({Z}_1)+(1-\alpha)g({Z}_2)+\alpha h({\omega }_1)+(1-\alpha)h({\omega }_2)+\omega _0\\ =~&#x0026;\alpha (g({Z}_1)+h({\omega }_1)+\omega _0)+(1-\alpha)(g({Z}_2)+h({\omega }_2)+\omega _0). \end{aligned}\end{equation*} </span>        <br/>       </div>      </div> By applying the separable formulation of <span class="inline-equation"><span class="tex">$\hat{y}$</span>      </span> again, we obtain <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \alpha (g({Z}_1)+h({\omega }_1)+\omega _0)+(1-\alpha)(g({Z}_2)+h({\omega }_2)+\omega _0) =~\alpha \hat{y}({C}_1)+(1-\alpha)\hat{y}({C}_2).\end{equation*} </span>        <br/>       </div>      </div> Therefore, <span class="inline-equation"><span class="tex">$\hat{y}(\alpha {C}_1+(1-\alpha) {C}_2)=\hat{y}(\alpha {C}_1+(1-\alpha) {C}_2)$</span>      </span>, by definition, <span class="inline-equation"><span class="tex">$\hat{y}({C})$</span>      </span> is a convex linear function with respect with <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {K}$</span>      </span>.</p>     </div>     <p>Next, we show that given the above prediction function <span class="inline-equation"><span class="tex">$\hat{y}({C}):\mathcal {K}\rightarrow \mathbb {R}$</span>     </span> and any convex loss function <span class="inline-equation"><span class="tex">$f(y):\mathbb {R}\rightarrow \mathbb {R}$</span>     </span>, the nested function <span class="inline-equation"><span class="tex">$f(\hat{y}({C}))=f({C})$</span>     </span> is also convex, as shown in Lemma <a class="enc" href="#enc4">4</a>:</p>     <div class="lemma" id="enc4">     <Label>lemma 4.</Label>     <p> Let <span class="inline-equation"><span class="tex">$\hat{y}({C})=\frac{1}{2} {\hat{x}}^T {C\hat{x}},~ {C}\in \mathcal {K},~ {\hat{x}}^T=[ {x}^T,1],~ {x}\in \mathbb {R}^d$</span>      </span>, and <span class="inline-equation"><span class="tex">$f(y):\mathbb {R}\rightarrow \mathbb {R}$</span>      </span> be arbitrary convex function, the nested function <span class="inline-equation"><span class="tex">$f(\hat{y}({C}))=f({C}):\mathcal {K}\rightarrow \mathbb {R}$</span>      </span> is also convex with respect to <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {K}$</span>      </span>.</p>     </div>     <div class="proof" id="proof3">     <Label>Proof.</Label>     <p> Let <span class="inline-equation"><span class="tex">$ {C}_1\in \mathcal {K}$</span>      </span> and <span class="inline-equation"><span class="tex">$ {C}_2\in \mathcal {K}$</span>      </span> be any two points in the set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>      </span>, and <em>f</em>(<em>y</em>) be an arbitrary convex function, by definition of <span class="inline-equation"><span class="tex">$\hat{y}$</span>      </span>, &#x2200;&#x2009;<em>&#x03B1;</em> &#x2208; [0, 1], we obtain <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} f(\hat{y}(\alpha {C}_1+(1-\alpha) {C}_2))= f(\alpha \hat{y}({C}_1)+(1-\alpha)\hat{y}({C}_2)).\end{equation*} </span>        <br/>       </div>      </div> By the convexity of <em>f</em>(<em>y</em>), we obtain <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} f(\alpha \hat{y}({C}_1)+(1-\alpha)\hat{y}({C}_2))\le \alpha f(\hat{y}({C}_1))+(1-\alpha)f(\hat{y}({C}_2)).\end{equation*} </span>        <br/>       </div>      </div> Therefore <em>f</em>(<em>&#x03B1;</em>C<sub>1</sub> + (1 &#x2212; <em>&#x03B1;</em>)C<sub>2</sub>) &#x2264; <em>&#x03B1;f</em>(C<sub>1</sub>) + (1 &#x2212; <em>&#x03B1;</em>)<em>f</em>(C<sub>2</sub>). By definition, the nested function <span class="inline-equation"><span class="tex">$f(\hat{y}({C}))=f({C})$</span>      </span> is also convex.</p>     </div>     <p>In summary, by introducing the new convexification scheme, we obtain a new formulation for FM, in which the decision set is convex compact and the nested loss function is convex. We refer to the resulting formulation as <strong>Compact Convexified FM&#x00A0;(CCFM)</strong>. The comparison between vanilla FM, convex FM and the proposed CCFM is illustrated in Table <a class="tbl" href="#tab1">1</a>.</p>        <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Comparison between different FM formulations<sup>2</sup>.      </span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        Properties        </th>        <th style="text-align:center;">        Convex        </th>        <th style="text-align:center;">        Compact        </th>        <th style="text-align:center;">        All-Pairs        </th>        <th style="text-align:center;">        Online        </th>       </tr>       </thead> 						<tbody> 						<tr>        <td style="text-align:center;">        <strong>FM</strong>        </td>        <td style="text-align:center;">\</td>        <td style="text-align:center;">\</td>        <td style="text-align:center;">\</td>        <td style="text-align:center;">\</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CFM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"          href="#BibPLXBIB0003">3</a>]</strong>        </td>        <td style="text-align:center;">&#x221A;</td>        <td style="text-align:center;">\</td>        <td style="text-align:center;">&#x221A;</td>        <td style="text-align:center;">\</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CFM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"          href="#BibPLXBIB0033">33</a>]</strong>        </td>        <td style="text-align:center;">&#x221A;</td>        <td style="text-align:center;">\</td>        <td style="text-align:center;">\</td>        <td style="text-align:center;">\</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CCFM</strong>        </td>        <td style="text-align:center;">&#x221A;</td>        <td style="text-align:center;">&#x221A;</td>        <td style="text-align:center;">&#x221A;</td>        <td style="text-align:center;">&#x221A;</td>       </tr>       <tr>        <td colspan="5" style="text-align:center;">        <sup>2</sup> In Table <a class="tbl" href="#tab1">1</a>, FM, CFM, CCFM refer to vanilla FM , Convex FM and Compact Convex FM respectively. The term &#x201D;Convex&#x201D; indicates whether the prediction function is convex; the term &#x201D;Compact&#x201D; indicates whether the feasible set of the formulation is compact; the term &#x201D;All-Pairs&#x201D; indicates whether all the pair-wise feature interactions are involved in the formulation; the term &#x201D;Online&#x201D; indicates whether the formulation fits the OCO framework for Online learning.<hr/>        </td>       </tr>      </tbody>     </table>     </div> <p>     <strong>Comparison between vanilla FM and CCFM.</strong>&#x00A0;&#x00A0;&#x00A0;The differences between vanilla FM and CCFM are three-folded: (i) the primal difference between vanilla FM and CCFM is the convexity of prediction function: the prediction function of vanilla FM is non-convex while it is convex in CCFM. (ii) vanilla FM factorizes the feature interaction matrix Z into VV<sup>      <em>T</em>     </sup>, thus restricting Z to be symmetric and positive semi-definite; while in CCFM, there is no specific restriction on Z except symmetry. (iii) vanilla FM only considers the interactions between distinct features: <em>x<sub>i</sub>x<sub>j</sub>     </em>, ~&#x2200;&#x2009;<em>i</em>,&#x2009;<em>j</em> &#x2208; {1, &#x2026;, <em>d</em>}, ~<em>i</em> &#x2260; <em>j</em>, while CCFM models the interactions between all possible feature pairs: <em>x<sub>i</sub>x<sub>j</sub>     </em>, ~&#x2200;&#x2009;<em>i</em>,&#x2009;<em>j</em> &#x2208; {1, &#x2026;, <em>d</em>}. Combining (ii) and (iii), we find that CCFM allows for general modeling of feature interactions, which improves its expressiveness.</p>     <p>     <strong>Comparison between convex FM and CCFM.</strong>&#x00A0;&#x00A0;&#x00A0;Although convex FM involves convex prediction functions, they are still inherently different from CCFM: (i) in convex FM, the first-order feature weight vector and second-order feature interaction matrix are separated, resulting in a non-compact formulation; but in CCFM, all the parameters are written into a compact augmented matrix. (ii) in CCFM, we restrict that the compact augmented matrix C is low-rank; while convex FM formulations only require the second-order matrix Z to be low-rank, leaving the first-order feature weight vector &#x03C9; unbounded. (iii) convex FM can not fit into the OCO framework easily due to its non-compact decision set while CCFM seamlessly fits the OCO framework.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Online Learning Algorithm for Compact Convexified Factorization Machine</h3>     </div>     </header>     <p>With the convexification scheme mentioned above, we have got a convex compact set and a convex loss function in the new formulation of Compact Convexified Factorization Machine (CCFM), which allows us to design the online learning algorithm following the OCO framework [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. As shown in the Preliminaries, the decision set is an important issue that affects the computational complexity of the projection step onto the set. In CCFM, the decision set is a subset of the bounded nuclear norm ball, where the projection step involves singular value decomposition and takes super-linear time via our best known methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. Although Online Gradient Descent (OGD) and Follow-The-Regularized-Leader (FTRL) are the two most classical online learning algorithms, they suffer from the expensive SVD operation in the projection step, thus do not work well on this specific decision set.</p>     <p>Meanwhile, the nuclear norm ball is a typical decision set where the expensive projection step can be replaced by the efficient linear optimization subroutine in the projection-free OCG algorithm. As the decision set of CCFM is a subset of the bounded nuclear norm ball, we first propose the online learning algorithm for CCFM (OCCFM), which is essentially an OCG variant. Then we provide the theoretical analysis for OCCFM in details.</p>     <section id="sec-16">     <p><em>3.2.1 Algorithm.</em> As introduced in the Preliminaries, it is a typical practice to apply OCG algorithm on the bounded nuclear norm ball in the OCO framework. This typical example is related to CCFM, but is also inherently different. On one hand, the decision set of CCFM is a subset of the bounded nuclear norm ball. Thus it is tempting to apply the subroutine of OCG over the bounded nuclear norm ball on the decision set of CCFM; on the other hand, the decision set of CCFM is also a subset of symmetric matrices, and it remains theoretically unknown whether the subroutine can be applied to the symmetric bounded nuclear norm ball. Therefore, it is a non-trivial problem to design an OCG-alike algorithm for the CCFM due to its specific decision set.</p>     <p>To avoid the expensive projection onto the decision set of CCFM, we follow the core idea of OCG to replace the projection step with a linear optimization problem over the decision set. What remains to be done is to design an efficient subroutine that solves the linear optimization over this set in low computational complexity. Recall the procedure of OCG in Algorithm 1, the validity of this subroutine depends on the following two important requirements:</p>     <ul class="list-no-style">      <li id="list5" label="&#x2022;">First, the subroutine should solve the linear optimization over the decision set with low computational complexity; in our case, the subroutine should generate the optimal solution <span class="inline-equation"><span class="tex">$ {\hat{C}}_t$</span>       </span> to the problem <span class="inline-equation"><span class="tex">$ {\hat{C}}_t=\mathrm{argmin}_{ {C}\in \mathcal {K}}\langle {C},\nabla F_t({C}_{t})\rangle$</span>       </span> with linear or lower complexity, where C<sub>        <em>t</em>       </sub> is the iterate of C at round <em>t</em>, <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span> is the decision set of CCFM, &#x2207;<em>F<sub>t</sub>       </em>(C<sub>        <em>t</em>       </sub>) is the sum of gradients of the loss function incurred till round <em>t</em>.<br/></li>      <li id="list6" label="&#x2022;">Second, the subroutine should be closed over the convex decision set; in our case, the augmented matrix C<sub>        <em>t</em>       </sub> should be inside the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span> throughout the algorithm: <span class="inline-equation"><span class="tex">$ {C}_{t+1}=(1-\gamma _{t}) {C}_{t}+\gamma _{t} {\hat{C}}_{t}\in \mathcal {K},~\forall \,t=1,\ldots ,T$</span>       </span>, where <span class="inline-equation"><span class="tex">$ {\hat{C}}_t$</span>       </span> is the output of the subroutine and <em>&#x03B3;<sub>t</sub>       </em> is the step-size in round <em>t</em>.<sup>3</sup><a class="fn" href="#fn6" id="foot-fn6"><sup/></a>       <br/></li>     </ul>     <p>      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186075/images/www2018-84-img2.svg" class="img-responsive" alt="" longdesc=""/>     </p>     <p>Considering these two requirements, we propose a subroutine of the linear optimization over the symmetric bounded nuclear norm ball, based on which we build the online learning algorithm for the Compact Convexified Factorization Machine. Specifically, we prove that in each round, the linear optimization over the decision set in CCFM is also equivalent to the computation of maximal singular vectors of a specific symmetric matrix. This subroutine can be solved in linear time via the power method, which validates its efficiency.</p>     <p>The procedure is detailed in Algorithm 2 where u and v are the left and right maximal singular vectors of &#x2212; &#x2207;<em>F<sub>t</sub>      </em>(C<sub>       <em>t</em>      </sub>) respectively. The projection step is replaced with the subroutine at line 5 &#x2212; 6 in the algorithm and the detailed analysis of the algorithm will be presented later.</p>     </section>     <section id="sec-17">     <p><em>3.2.2 Theoretical Analysis.</em> The Online Compact Convexified Factorization Machine (OCCFM) algorithm is essentially a variant of OCG algorithm, which preserves the similar process. First, we show that OCCFM satisfies the aforementioned two requirements, then we prove the regret bound of OCCFM following the OCO framework.</p>     <p>To prove that OCCFM satisfies the aforementioned two requirements, we present the theoretical guarantee in Theorem <a class="enc" href="#enc5">1</a> and Theorem <a class="enc" href="#enc6">2</a> respectively.</p>     <div class="theorem" id="enc5">      <Label>Theorem 1.</Label>      <p> In Algorithm 2, the subroutine of linear optimization amounts to the computation of maximal singular vectors: Given <span class="inline-equation"><span class="tex">$ {C}_t\in \mathcal {K}=\lbrace {C}|~\Vert {C}\Vert _{tr}\le \delta ,~ {C}\in \mathcal {S}^{(d+1)\times (d+1)}\rbrace$</span>       </span>, <span class="inline-equation"><span class="tex">$ {\hat{C}}_t=\mathop {\mathrm{argmin}}\limits _{ {C}\in \mathcal {K}}\langle {C},\nabla F_t({C}_{t})\rangle =\delta {u}_1 {v}_1^T$</span>       </span>, where u<sub>1</sub> and v<sub>1</sub> are the maximal singular vectors of &#x2212; &#x2207;<em>F<sub>t</sub>       </em>(C<sub>        <em>t</em>       </sub>).</p>     </div>     <div class="theorem" id="enc6">      <Label>Theorem 2.</Label>      <p> In Algorithm 2, the subroutine is closed over the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span>, i.e. the augmented matrix C<sub>        <em>t</em>       </sub> generated at each iteration <em>t</em> is inside the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span>: <span class="inline-equation"><span class="tex">$ {C}_t\in \mathcal {K}$</span>       </span>, &#x2200;&#x2009;<em>t</em> = 1, &#x2026;, <em>T</em>, where <span class="inline-equation"><span class="tex">$\mathcal {K}=\lbrace {C}|\Vert {C}\Vert _{tr}\le \delta ,~ {C}\in \mathcal {S}^{(d+1)\times (d+1)}\rbrace$</span>       </span>.</p>     </div>     <p>Before we proceed with the proof of Theorem <a class="enc" href="#enc5">1</a> and Theorem <a class="enc" href="#enc6">2</a>, we provide a brief introduction to the Singular Value Decomposition (SVD) of a matrix, which is frequently used in the proofs of the theorems. For any matrix <span class="inline-equation"><span class="tex">$ {C}\in \mathbb {R}^{m\times n}$</span>      </span>, the Singular Value Decomposition is a factorization of the form C = U&#x03A3;V<sup>       <em>T</em>      </sup>, where <span class="inline-equation"><span class="tex">$ {U}\in \mathbb {R}^{m\times K}$</span>      </span> and <span class="inline-equation"><span class="tex">$ {V}\in \mathbb {R}^{n\times K},~(K=\min \lbrace m,\,n\rbrace)$</span>      </span> are the orthogonal matrices, and <span class="inline-equation"><span class="tex">$ {\Sigma }\in \mathbb {R}^{K\times K}$</span>      </span> is a diagonal matrix. The diagonal entries <em>&#x03C3;<sub>i</sub>      </em> of &#x03A3; are non-negative real numbers and known as singular values of C. Conventionally, the singular values are in a permutation of non-increasing order: <em>&#x03C3;</em>      <sub>1</sub> &#x2265; <em>&#x03C3;</em>      <sub>2</sub> &#x2265;, &#x2026;, &#x2265;<em>&#x03C3;<sub>K</sub>      </em>.</p>     <p>Next, we prove Lemma <a class="enc" href="#enc7">5</a>, which is an important part for the proof of both theorems. In this lemma, we show that the outer product of the left and right maximal singular vectors of a symmetric matrix (as part of the subroutine in Algorithm 2) is also a symmetric matrix.</p>     <div class="lemma" id="enc7">      <Label>lemma 5.</Label>      <p> Let <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {S}^{d\times d}$</span>       </span> be an arbitrary real symmetric matrix, and C = U&#x03A3;V<sup>        <em>T</em>       </sup> be the singular value decomposition of C,</p>      <p>u<sub>1</sub>,&#x2009;v<sub>1</sub> &#x2208; <em>R<sup>d</sup>       </em> be the left and right maximal singular vectors of C respectively, then the matrix <span class="inline-equation"><span class="tex">$ {u}_1 {v}_1^T$</span>       </span> is also symmetric: <span class="inline-equation"><span class="tex">$ {u}_1 {v}_1^T\in \mathcal {S}^{d\times d}$</span>       </span>.</p>     </div>     <div class="proof" id="proof4">      <Label>Proof.</Label>      <p> The proof is based on the connection between the singular value decomposition and eigenvalue decomposition of the real symmetric matrix. Denote the singular value decomposition of <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {S}^{d\times d}$</span>       </span> as: <span class="inline-equation"><span class="tex">$ {C}= {U} {\Sigma } {V}^T=\sum _{i=1}^d\sigma _i {u}_i {v}_i^T$</span>       </span> and its eigenvalue decomposition as: <span class="inline-equation"><span class="tex">$ {C}= {Q} {\Lambda } {Q}^T=\sum _{j=1}^d\lambda _j {q}_j {q}_j^T$</span>       </span>.</p>      <p>First, we show that the eigenvalues of CC<sup>        <em>T</em>       </sup> are the square of the singular values of C: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} {D}= {C}^{T} {C}= {V} {\Sigma } {U}^T {U} {\Sigma }^T {V}^T= {V} {\Sigma } {\Sigma }^T {V}^T.\end{equation*} </span>        <br/>        </div>       </div> As V is an orthogonal matrix, V<em>&#x03A3;</em>&#x03A3;<sup>        <em>T</em>       </sup>V<sup>        <em>T</em>       </sup> is an eigenvalue decomposition of D, where &#x03A3;&#x03A3;<sup>        <em>T</em>       </sup> is the diagonal matrix of eigenvalues of D, and v<sub>        <em>i</em>       </sub>,&#x2009;&#x2200;&#x2009;<em>i</em> &#x2208; {1, &#x2026;, <em>d</em>} is the eigenvector of D.</p>      <p>Since C is symmetric, D = C<sup>        <em>T</em>       </sup>C = C<sup>2</sup>. For any eigenvalue <em>&#x03BB;</em> and eigenvector q of C: Cq = <em>&#x03BB;</em>q, we have <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} {D} {q}= {C}^T {C} {q}= {C}^T({C} {q})=\lambda {C}^T {q}=\lambda {C} {q}=\lambda ^2 {q}.\end{equation*} </span>        <br/>        </div>       </div> Thus the square of every eigenvalue <em>&#x03BB;</em> of C is also the eigenvalue <em>&#x03BB;</em>       <sup>2</sup> of D and every eigenvector q of C is also the eigenvector v of D.</p>      <p>By rearranging the eigenvalues of C so that <span class="inline-equation"><span class="tex">$|\hat{\lambda }_i|,~i=1,\ldots ,d$</span>       </span> is non-increasing, we have <span class="inline-equation"><span class="tex">$\sigma _i=|\hat{\lambda }_i|$</span>       </span>. By rewriting the eigenvalue decomposition C as <span class="inline-equation"><span class="tex">$\sum _{i=1}^d\hat{\lambda }_{i}\hat{ {q}}_i\hat{ {q}}_i^T=\sum _{i=1}^d|\hat{\lambda }_{i}|\hat{ {p}}_i\hat{ {q}}_i^T=\sum _{i=1}^d\sigma _i {u}_i {v}_i^T$</span>       </span>, where <span class="inline-equation"><span class="tex">$\hat{ {p}}_i=sgn(\hat{\lambda }_i)\hat{ {q}}_i$</span>       </span>, we have <span class="inline-equation"><span class="tex">$\hat{ {p}}_i= {u}_i,~\hat{ {q}}_i= {v}_i,~\forall \,i=1,\ldots ,d$</span>       </span>.</p>      <p>Therefore <span class="inline-equation"><span class="tex">$ {u}_i {v}_i^T=sgn(\lambda _i)\hat{ {q}}_i\hat{ {q}}_i^T,~\forall \,i=\lbrace 1,\ldots ,d\rbrace$</span>       </span> is a symmetric matrix.</p>     </div>     <p>With these preparations, we prove Theorem <a class="enc" href="#enc5">1</a>.</p>     <div class="proof" id="proof5">      <Label>Proof of Theorem 1</Label>      <p> First, recall that for any <span class="inline-equation"><span class="tex">$ {C}\in \mathcal {S}^{(d+1)\times (d+1)}$</span>       </span>, the SVD of C is <span class="inline-equation"><span class="tex">$ {C}= {U} {\Sigma } {V}=\sum _{i=1}^{d+1}\sigma _i {u}_i {v}_i^T$</span>       </span>. Following the conclusion in Lemma <a class="enc" href="#enc7">5</a>, we have |u<sub>        <em>i</em>       </sub>| = |v<sub>        <em>i</em>       </sub>|, ~&#x2200;&#x2009;<em>i</em> &#x2208; {1, &#x2026;, <em>d</em> + 1}, we rewrite the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span> with the SVD of C: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{aligned} \mathcal {K}=&#x0026;\lbrace {C}| {C}= {U} {\Sigma } {V}^T=\sum _{i=1}^{d+1}\sigma _i {u}_i {v}_i^T,~| {u}_i|=| {v}_i|,~\forall \,i,\\ &#x0026;~\forall \, {C}\in \mathcal {S}^{(d+1)\times (d+1)},~\sum _{i=1}^{d+1}\sigma _{i}\le \delta ,~ {U},\, {V}\in \mathbb {R}^{(d+1)\times (d+1)}\rbrace . \end{aligned}\end{equation*} </span>        <br/>        </div>       </div> Recall that in Algorithm 2, the points are generated from the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}:~ {C}_t,~ {C}_1\in \mathcal {K}$</span>       </span>, we have <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \nabla F_t({C}_t)=\frac{\eta }{2}\sum _{\tau =1}^{t-1}f_{\tau }({C}_{\tau }) {\hat{x}}_t {\hat{x}}_t^T+2({C}_t- {C}_1)\in \mathcal {S}^{(d+1)\times (d+1)}.\end{equation*} </span>        <br/>        </div>       </div> Denote &#x2212; &#x2207;<em>F<sub>t</sub>       </em>(C<sub>        <em>t</em>       </sub>) as <span class="inline-equation"><span class="tex">$ {H}_t\in \mathcal {S}^{(d+1)\times (d+1)}$</span>       </span> and the SVD of H<sub>        <em>t</em>       </sub> as <span class="inline-equation"><span class="tex">$ {H}_t=\sum _{i=1}^{d+1}\theta _i {\mu }_i {\nu }_i^T$</span>       </span>, where <em>&#x03B8;<sub>i</sub>       </em>, ~&#x2200;&#x2009;<em>i</em> &#x2208; {1, &#x2026;, <em>d</em> + 1} are the singular values of H<sub>        <em>t</em>       </sub> and &#x03BC;<sub>        <em>i</em>       </sub>, ~&#x03BD;<sub>        <em>i</em>       </sub> are the left and right singular vectors respectively, the subroutine in Algorithm 2 solves the following linear optimization problem: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{aligned} &#x0026;\min .~&#x0026;&#x0026;\langle {C},~\nabla F_t({C}_t)\rangle \\ &#x0026;s.t.~&#x0026;&#x0026; {C}\in \mathcal {S}^{(d+1)\times (d+1)},~\Vert {C}\Vert _{tr}\le \delta . \end{aligned}\end{equation*} </span>        <br/>        </div>       </div> The objective function can be rewritten as &#x27E8;C, &#x2207;<em>F<sub>t</sub>       </em>(C<sub>        <em>t</em>       </sub>)&#x27E9; = &#x27E8;C, &#x2212;H<sub>        <em>t</em>       </sub>&#x27E9;. and the linear optimization problem can be rewritten as: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \mathop {\mathrm{argmin}}\limits _{ {C}\in \mathcal {K}}\langle {C},\nabla F_t({C}_{t})\rangle =\mathop {\mathrm{argmax}}\limits _{ {C}\in \mathcal {K}}\langle \sum _{i=1}^{d+1}\sigma _i {u}_i {v}_i^T,~ {H}_t\rangle .\end{equation*} </span>        <br/>        </div>       </div> Using the invariance of trace of scalar, we have <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{aligned} \mathop {\mathrm{argmax}}\limits _{ {C}\in \mathcal {K}}\langle \sum _{i=1}^{d+1}\sigma _i {u}_i {v}_i^T,~ {H}_t\rangle =&#x0026;\mathop {\mathrm{argmax}}\limits _{ {C}\in \mathcal {K}}\sum _{i=1}^{d+1}\sigma _i {u}_i^T {H}_t {v}_{i}\\\le &#x0026;\mathop {\mathrm{argmax}}\limits _{ {C}\in \mathcal {K}}\sum _{i=1}^{d+1}\sigma _i\xi _i\theta _{\pi (i)},~\forall \,\xi _i\in \lbrace -1,1\rbrace . \end{aligned}\end{equation*} </span>        <br/>        </div>       </div> where <em>&#x03B8;</em>       <sub>        <em>&#x03C0;</em>(<em>i</em>)</sub>, &#x2200;&#x2009;<em>i</em> is a permutation of the singular values of matrix H<sub>        <em>t</em>       </sub>, and <em>&#x03C0;</em> is the arbitrary permutation over [<em>d</em> + 1]. The last inequality can be attained following Lemma <a class="enc" href="#enc8">6</a>.</p>      <p>Based on the non-negativity of singular values and the rearrangement inequality, we have: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \sum _{i=1}^{d+1}\sigma _i\xi _i\theta _{\pi (i)}\le ~\sum _{i=1}^{d+1}\sigma _i\theta _{\pi (i)}\le ~\sum _{i=1}^{d+1}\sigma _i\theta _i \le ~\sum _{i=1}^{d+1}\sigma _i\cdot \theta _1=~\Vert {C}\Vert _{tr}\theta _1.\end{equation*} </span>        <br/>        </div>       </div> where <em>&#x03B8;</em>       <sub>1</sub> is the maximal singular value of H<sub>        <em>t</em>       </sub> = &#x2207;<em>F<sub>t</sub>       </em>(C<sub>        <em>t</em>       </sub>).</p>      <p>Recall that <span class="inline-equation"><span class="tex">$ {H}_t=-\nabla F_t({C}_t)\in \mathcal {S}^{(d+1)\times (d+1)}$</span>       </span>, following Lemma <a class="enc" href="#enc7">5</a>, we have <span class="inline-equation"><span class="tex">$ {\mu }_i {\nu }_i^T\in \mathcal {S}^{(d+1)\times (d+1)},~\forall \,i$</span>       </span>. Therefore the equality can be attained when u<sub>        <em>i</em>       </sub> = &#x03BC;<sub>        <em>i</em>       </sub>, ~v<sub>        <em>i</em>       </sub> = &#x03BD;<sub>        <em>i</em>       </sub>, ~&#x2200;&#x2009;<em>i</em> &#x2208; {1, &#x2026;, <em>d</em> + 1}, and <em>&#x03C3;</em>       <sub>1</sub> = <em>&#x03B4;</em>, ~<em>&#x03C3;<sub>i</sub>       </em> = 0, ~&#x2200;&#x2009;<em>i</em> = 2, &#x22C5;&#x22C5;&#x22C5;, <em>d</em> + 1.</p>      <p>In this case, <span class="inline-equation"><span class="tex">$\hat{ {C}}_t=\sum _{i=1}^{d+1} {u}_i\sigma _i {v}_i^T=\delta {\mu }_1 {\nu }_1^T$</span>       </span>. Following Lemma <a class="enc" href="#enc7">5</a>, <span class="inline-equation"><span class="tex">$\hat{ {C}}_t=\delta {\mu }_1 {\nu }_1^T\in \mathcal {S}^{(d+1)\times (d+1)}$</span>       </span> and <span class="inline-equation"><span class="tex">$\Vert \hat{ {C}}_t\Vert _{tr}=\sum _{i=1}^d\sigma _i=\delta$</span>       </span>.</p>      <p>As a result, <span class="inline-equation"><span class="tex">$\sum _{i=1}^{d+1} {u}_i\sigma _i {v}_i^T\in \mathcal {K}$</span>       </span> and thus we have <span class="inline-equation"><span class="tex">$ {\hat{C}}=\delta {\mu }_1 {\nu }_1^T=\mathop {\mathrm{argmin}}\limits _{ {C}\in \mathcal {K}}\langle {C},\nabla F_t({C}_{t})\rangle$</span>       </span>, which indicates that the linear optimization over the decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span> amounts to the computation of the maximal singular vectors of the symmetric matrix H<sub>        <em>t</em>       </sub>.</p>     </div>     <div class="lemma" id="enc8">      <Label>lemma 6.</Label>      <p> Let <span class="inline-equation"><span class="tex">$ {A}\in \mathbb {R}^{n\times n}$</span>       </span> and <span class="inline-equation"><span class="tex">$g:~\mathbb {R}^n\rightarrow \mathbb {R}$</span>       </span> be a twice-differentiable convex function, and <em>&#x03C3;<sub>i</sub>       </em>(A), &#x2200;&#x2009;<em>i</em> be the singular values of matrix A. For any two orthonormal bases {u<sub>1</sub>, &#x2026;, u<sub>        <em>n</em>       </sub>} and {v<sub>1</sub>, &#x2026;, v<sub>        <em>n</em>       </sub>} of <span class="inline-equation"><span class="tex">$~\mathbb {R}^n$</span>       </span>, there exists a permutation <em>&#x03C0;</em> over [<em>n</em>] and <em>&#x03BE;</em>       <sub>1</sub>, ..., <em>&#x03BE;<sub>n</sub>       </em> &#x2208; { &#x2212; 1, ~1} such that: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \begin{aligned} &#x0026;g({u}_1^T {A} {v}_1, {u}_2^T {A} {v}_2,\ldots , {u}_n^T {A} {v}_n)\le \\ &#x0026;g(\xi _1\sigma _{\pi (1)}({A}),\xi _2\sigma _{\pi (2)}({A}),\ldots ,\xi _n\sigma _{\pi (n)}({A})). \end{aligned}\end{equation*} </span>        <br/>        </div>       </div>      </p>     </div>     <p>The proof can be found in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>] and the details are omitted here. Now we are ready to prove the Theorem <a class="enc" href="#enc6">2</a>.</p>     <div class="proof" id="proof6">      <Label>Proof Proof of Theorem 2</Label>      <p> The proof is conducted with the mathematical induction method.</p>      <p>When <em>t</em> = 1, recall the initialization of Algorithm 2, <span class="inline-equation"><span class="tex">$ {C}_1\in \mathcal {K}$</span>       </span>, thus the proposition stands when <em>t</em> = 1.</p>      <p>Now assuming the proposition stands for <em>t</em> > 1: <span class="inline-equation"><span class="tex">$ {C}_t\in \mathcal {K},~\forall \,t\in \lbrace 2,\ldots ,T\rbrace$</span>       </span>, we prove that <span class="inline-equation"><span class="tex">$ {C}_{T+1}\in \mathcal {K}$</span>       </span>. According to the assumption, the augmented matrix is inside the decision set: <span class="inline-equation"><span class="tex">$ {C}_T\in \mathcal {K}$</span>       </span>, thus &#x2016;C<sub>        <em>T</em>       </sub>&#x2016;<sub>        <em>tr</em>       </sub> &#x2264; <em>&#x03B4;</em> and <span class="inline-equation"><span class="tex">$ {C}_T\in \mathcal {S}^{(d+1)\times (d+1)}$</span>       </span>. Following the formulation of CCFM and Algorithm 2, we have <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} \nabla F_T({C}_T)=\frac{\eta }{2}\sum _{t=1}^{T}f_t({C}_t) {\hat{x}}_t {\hat{x}}_t^T+2({C}_T- {C}_1)\in \mathcal {S}^{(d+1)\times (d+1)},\end{equation*} </span>        <br/>        </div>       </div> Based on Lemma <a class="enc" href="#enc7">5</a> and Theorem <a class="enc" href="#enc5">1</a>, we have <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} {\hat{C}}_T=\mathop {\mathrm{argmin}}\limits _{ {C}\in \mathcal {K}}\langle {C},\nabla F_T({C}_{T})\rangle =\delta {u}_1 {v}_1^T\in \mathcal {S}^{(d+1)\times (d+1)},\end{equation*} </span>        <br/>        </div>       </div> where u<sub>1</sub> and v<sub>1</sub> are the maximal singular vectors of &#x2212; &#x2207;<em>F<sub>T</sub>       </em>(C<sub>        <em>T</em>       </sub>). Moreover <span class="inline-equation"><span class="tex">$\Vert {\hat{C}}_T\Vert _{tr}=\Vert \delta {u}_1 {v}^T_1\Vert _{tr}=\delta$</span>       </span>. Therefore <span class="inline-equation"><span class="tex">$ {\hat{C}}_T\in \mathcal {K}$</span>       </span>. By the convexity of decision set <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span>, we obtain <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} {C}_{T+1}=\gamma {C}_T+(1-\gamma) {\hat{C}_T}\in \mathcal {K}.\end{equation*} </span>        <br/>        </div>       </div> Therefore the induction stands when <em>t</em> = <em>T</em> + 1.</p>      <p>In summary, the induction stands for both <em>t</em> = 1 and <em>t</em> &#x2208; 2, &#x2026;, <em>T</em>, &#x2200;&#x2009;<em>T</em> > 1, which indicates <span class="inline-equation"><span class="tex">$ {C}_t\in \mathcal {K},~\forall \,t\in \mathbb {Z}^{+}$</span>       </span>.</p>     </div>     <p>With Theorem <a class="enc" href="#enc5">1</a> and Theorem <a class="enc" href="#enc6">2</a>, we prove that the aforementioned two requirements are satisfied. Thus the subroutine in Online Compact Convexified Factorization Machine is a valid conditional gradient step in OCG algorithm, making OCCFM a valid OCG variant. Following the theoretical analysis of OCG in the OCO framework, we prove that the regret of Algorithm 2 after <em>T</em> rounds is sub-linear in <em>T</em>, as shown in Theorem <a class="enc" href="#enc9">3</a>.</p>     <div class="theorem" id="enc9">      <Label>Theorem 3.</Label>      <p> The Online Compact Convexified Factorization Machine with parameters <span class="inline-equation"><span class="tex">$\eta =\frac{D}{4GT^{3/4}}$</span>       </span>, <span class="inline-equation"><span class="tex">$\gamma _t=\frac{1}{t^{1/2}}$</span>       </span>, attains the following guarantee<sup>4</sup><a class="fn" href="#fn7" id="foot-fn7"><sup/></a>: <div class="table-responsive">        <div class="display-equation">        <span class="tex mytex">\begin{equation*} regret_T=\sum _{t=1}^Tf_t({C}_t)-\min _{ {C}^{*}\in \mathcal {K}}\sum _{t=1}^Tf_t({C}^{*})\le \frac{25}{2}DGT^{3/4},\end{equation*} </span>        <br/>        </div>       </div> where <em>D</em>, <em>G</em> represent an upper bound on the diameter of <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span> and an upper bound on the norm of the sub-gradients of <em>f<sub>t</sub>       </em>(C) over <span class="inline-equation"><span class="tex">$\mathcal {K}$</span>       </span>, i.e. <span class="inline-equation"><span class="tex">$\Vert \nabla f({C})\Vert \le G,~\forall \, {C}\in \mathcal {K}$</span>       </span>.</p>     </div>     <p>The proof of this theorem largely follows from that in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>] with some variations. Due to the page limit, we omit it here.</p>     </section>    </section>   </section>   <section id="sec-18">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>In this section, we evaluate the performance of the proposed online compact convexified factorization machine on two popular machine learning tasks: online regression and online classification.</p>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Experimental Setup</h3>     </div>     </header>     <p>     <strong>Compared Methods</strong>&#x00A0;&#x00A0;&#x00A0;We compare the empirical performance of OCCFM with state-of-the-art variants of FM in the online learning settings. As previous studies on FM focus on batch learning settings, we construct the baseline methods by applying online learning approaches to the existing formulations of FM, which is a common experimental methodology in online learning research [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>],?.</p>     <p>In the experiments, the comparison between OCCFM and other methods is focused on the formulations and online learning algorithms respectively. First, we construct baseline methods to illustrate the comparison between different formulations. Two extant formulations of FM are most related to this study, i.e. the non-convex formulation of vanilla FM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] and the non-compact formulation of Convex FM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. We apply the state-of-art FTRL algorithm on vanilla FM, CFM and the proposed CCFM, which are denoted as FM-FTRL, CFM-FTRL and CCFM-FTRL respectively. Second, to illustrate the comparison between different online learning algorithms, we apply different online learning algorithms on CCFM. The most popular online learning algorithms are Online Gradient Descent (OGD) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] and Follow-The-Regularized-Leader (FTRL) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] which achieves the superior empirical performances in most cases. The constructed methods are denoted as CCFM-OGD, CCFM-FTRL respectively. To summarize, the compared methods are:</p>     <ul class="list-no-style">     <li id="list7" label="&#x2022;">FM-FTRL: vanilla FM with FTRL algorithm;<br/></li>     <li id="list8" label="&#x2022;">CFM-FTRL: Convex FM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0033">33</a>] with FTRL algorithm;<br/></li>     <li id="list9" label="&#x2022;">CCFM-OGD: Compact Convexified FM with OGD algorithm;<br/></li>     <li id="list10" label="&#x2022;">CCFM-FTRL: Compact Convexified FM with FTRL algorithm;<br/></li>     <li id="list11" label="&#x2022;">OCCFM: Online Compact Convexified FM.<br/></li>     </ul>     <p>     <strong>Datasets</strong>&#x00A0;&#x00A0;&#x00A0;We select different datasets for the tasks respectively. For the online regression tasks, we use the typical Movielens datasets, including Movielens-100K, Movielens-1M and Movielens-10M <sup>5</sup><a class="fn" href="#fn8" id="foot-fn8"><sup/></a>; for the online classification tasks, we select datasets from LibSVM <sup>6</sup><a class="fn" href="#fn9" id="foot-fn9"><sup/></a>, including IJCNN1, Spam and Epsilon . The statistics of the datasets are summarized in Table <a class="tbl" href="#tab2">2</a>.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Statistical Details of the Datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        Datasets        </th>        <th style="text-align:center;">        #Features        </th>        <th style="text-align:center;">        #Instances        </th>        <th style="text-align:center;">        Label        </th>       </thead> 						<tbody> 						</tr>       <tr>        <td style="text-align:center;">        <strong>Movielens-100K</strong>        </td>        <td style="text-align:center;">2,625</td>        <td style="text-align:center;">100,000</td>        <td style="text-align:center;">Numerical</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>Movielens-1M</strong>        </td>        <td style="text-align:center;">9,940</td>        <td style="text-align:center;">1,000,209</td>        <td style="text-align:center;">Numerical</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>Movielens-10M</strong>        </td>        <td style="text-align:center;">82,248</td>        <td style="text-align:center;">10,000,000</td>        <td style="text-align:center;">Numerical</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>IJCNN1</strong>        </td>        <td style="text-align:center;">22</td>        <td style="text-align:center;">141,691</td>        <td style="text-align:center;">Binary</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>Spam</strong>        </td>        <td style="text-align:center;">252</td>        <td style="text-align:center;">350,000</td>        <td style="text-align:center;">Binary</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>Epsilon</strong>        </td>        <td style="text-align:center;">2,000</td>        <td style="text-align:center;">100,000</td>        <td style="text-align:center;">Binary</td>       </tr>      </tbody>     </table>     </div>      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray*} &&\hspace{3pc}|U|\quad\qquad\qquad\qquad|I|\\ x^T_t &=& [\overbrace{0,0,\cdots\underbrace{, 1,} \cdots, 0,0}|\overbrace{0,0,\cdots\underbrace{, 1,} \cdots, 0,0}]\\ && \hspace{2pc}u_t\hbox{-th user}\qquad\qquad i_t\hbox{-th item}\end{eqnarray*} </span>       <br/>      </div>     <p>For each dataset, we conduct the experiments with five-fold cross-validation. The training instances are randomly permutated and fed one by one to the model sequentially. Upon the arrival of each instance, the model makes the prediction and then gets updated after the label is revealed. The experiment is conducted with 20 runs of different random permutations for the training data. The results are reported with the averaging performance.</p>     <p>     <strong>Evaluation Metrics</strong>&#x00A0;&#x00A0;&#x00A0;To evaluate the performances on both tasks properly, we select different metrics respectively: the Root Mean Square Error (RMSE) for the online regression tasks; and the Error Rate and AUC (Area Under Curve, which is known to be immune to the class imbalance problems) for the online classification tasks.</p>    </section>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Online Regression</h3>     </div>     </header>     <p>One typical application of online regression is rating prediction for recommendation, whose objective is to predict the rating given by a user to an item for more accurate recommendation. At each round, the model receives a pair of user ID and item ID sequentially and then predicts the value of the incoming rating correspondingly. Denote the instance arriving at round <em>t</em> as (<em>u<sub>t</sub>     </em>,&#x2009;<em>i<sub>t</sub>     </em>,&#x2009;<em>y<sub>t</sub>     </em>), where <em>u<sub>t</sub>     </em>, <em>i<sub>t</sub>     </em> and <em>y<sub>t</sub>     </em> represent the user ID, item ID and the rating given by user <em>u<sub>t</sub>     </em> to item <em>i<sub>t</sub>     </em>, the input feature vector <span class="inline-equation"><span class="tex">$ {x}_t^T$</span>     </span> is constructed like this: where |<em>U</em>| and |<em>I</em>| refer to the number of users and the number of items respectively. Upon the arrival of each instance, the model predicts the rating with <span class="inline-equation"><span class="tex">$\hat{y}_t=\frac{1}{2}\hat{ {x}}_t^T {C}_t\hat{ {x}}_t$</span>     </span>, where <span class="inline-equation"><span class="tex">$\hat{ {x}}_t=[ {x}_t^T,1]^T$</span>     </span>. The convex loss function incurred at each round is the squared loss: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} f_t({C}_t)=\Vert \hat{y}_t({C}_t)-y_t\Vert _2^2.\end{equation*} </span>       <br/>      </div>     </div> The nuclear norm bounds in the CCFM and CFM are set to 10, 10 and 20 for Movielens-100K, 1M and 10M respectively. For FM, the rank parameters are set to 10 on all the datasets. These hyper-parameters are selected with a grid search in cross-validation. For OGD and FTRL algorithms, the learning rate at round <em>t</em> is set to <span class="inline-equation"><span class="tex">$\frac{1}{\sqrt {t}}$</span>     </span> as what their corresponding theories suggest [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>]. In experiments on ML-1M and ML-10M, we randomly sample 1000 users and 1000 items for simplicity.</p>     <p>We list the RMSE of OCCFM and other compared methods in Table. <a class="tbl" href="#tab3">3</a>. From our observation, OCCFM achieves higher prediction accuracy (lowest RMSE) than the other online learning baselines. Since FM-FTRL, CFM-FTRL, CCFM-FTRL use the same online learning algorithm with different formulations of FM, the comparison between them illustrates the advantage of Compact Convexified FM. Meanwhile, CCFM-OGD, CCFM-FTRL and OCCFM adopt the same formulation of Compact Convexified FM, the comparison between them shows the effectiveness of the conditional gradient step in OCCFM algorithm.</p>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">RMSE on Movielens datasets in Online Rating Regression tasks for Recommendation<sup>7</sup>.      </span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>Algorithms</strong>        </th>        <th colspan="3" style="text-align:center;">        <strong>RMSE on Datasets</strong>        <hr/>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">        <strong>Movielens100K</strong>        </th>        <th style="text-align:center;">        <strong>Movielens1M</strong>        </th>        <th style="text-align:center;">        <strong>Movielens10M</strong>        </th>       </tr> 						</thead> 						<tbody> 						<tr>        <td style="text-align:center;">        <strong>FM-FTRL</strong>        </td>        <td style="text-align:center;">1.2781</td>        <td style="text-align:center;">1.1074</td>        <td style="text-align:center;">1.0237</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CFM-FTRL</strong>        </td>        <td style="text-align:center;">1.1036</td>        <td style="text-align:center;">1.0552</td>        <td style="text-align:center;">1.0078</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CCFM-OGD</strong>        </td>        <td style="text-align:center;">1.2721</td>        <td style="text-align:center;">1.0645</td>        <td style="text-align:center;">1.0291</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CCFM-FTRL</strong>        </td>        <td style="text-align:center;">1.0873&#x2020;</td>        <td style="text-align:center;">1.0441&#x2020;</td>        <td style="text-align:center;">0.9725&#x2020;</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>OCCFM</strong>        </td>        <td style="text-align:center;">        <strong>1.0359*</strong>        </td>        <td style="text-align:center;">        <strong>0.9702*</strong>        </td>        <td style="text-align:center;">        <strong>0.9441*</strong>        </td>       </tr>       <tr>        <td colspan="4" style="text-align:center;">        <sup>7</sup> The results with &#x2020; mark have passed the significance test with <em>p</em> < 0.01 compared with FM-FTRL and CFM-FTRL; the results with * mark have passed the significance test with <em>p</em> < 0.01 compared with the other algorithms<hr/>        </td>       </tr>      </tbody>     </table>     </div>     <p>We also measure the efficiency of CCFM-OGD, CCFM-FTRL and OCCFM on different datasets and see how fast the average losses decrease with the running time, which is shown in Fig. <a class="fig" href="#fig1">1</a>. From the results we can clearly observe that OCCFM runs significantly faster than CCFM-OGD and CCFM-FTRL, which illustrates the necessity and efficiency of using the linear optimization instead of the projection step.</p>     <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186075/images/www2018-84-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Comparison of the efficiency of CCFM-OGD, CCFM-FTRL and OCCFM on Movielens 100K and 1M.</span>      </div>     </figure>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Online Classification</h3>     </div>     </header>     <p>In this paper, we focus on the online binary classification task, which is related to applications like spam email detection and CTR prediction. The instances are denoted as (x<sub>      <em>t</em>     </sub>, ~<em>y<sub>t</sub>     </em>)&#x2009;&#x2200;&#x2009;<em>t</em>, where x<sub>      <em>t</em>     </sub> is the input feature vector and <em>y<sub>t</sub>     </em> &#x2208; { &#x2212; 1, ~ + 1} is the class label. At round <em>t</em>, the model predicts the label with <span class="inline-equation"><span class="tex">$sign(\hat{y}_t)=sign(\frac{1}{2}\hat{ {x}}_t^T {C}_t\hat{ {x}}_t)$</span>     </span>, where <span class="inline-equation"><span class="tex">$\hat{ {x}}_t=[ {x}_t^T,1]^T$</span>     </span>. The loss function is a logistic loss function with respect to C<sub>      <em>t</em>     </sub>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} f_t({C_t})=log(1+\frac{1}{exp(-{y}_t\cdot \hat{y}_t({C}_t))}).\end{equation*} </span>       <br/>      </div>     </div> The nuclear norm bounds for CCFM and CFM in this task are set to 300, 1000, and 200 for IJCNN1, Spam and Epsilon respectively; while the ranks for FM are set to 20, 50 and 50 accordingly. All the hyper-parameters are selected with a grid search and set to the values with the best performances. For OGD and all FTRL algorithms, the learning rate at round <em>t</em> is set to <span class="inline-equation"><span class="tex">$\frac{1}{\sqrt {t}}$</span>     </span> as the theories suggest [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>].</p>     <p>The comparison of the prediction accuracy between OCCFM and other baseline methods is presented in Table <a class="tbl" href="#tab4">4</a>. As shown in the table, OCCFM achieves the lowest error rates and the highest AUC values among all the online learning approaches, which reveals the advantage of OCCFM in prediction accuracy.</p>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Error Rate and AUC on IJCNN1, Spam and Epsilon datasets in Online Binary Classification Tasks.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>Algorithms</strong>        </th>        <th colspan="3" style="text-align:center;">        <strong>Error Rate on Datasets</strong>        <hr/>        </th>        <th colspan="3" style="text-align:center;">        <strong>AUC on Datasets</strong>        <hr/>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">        <strong>IJCNN</strong>        </th>        <th style="text-align:center;">        <strong>Spam</strong>        </th>        <th style="text-align:center;">        <strong>Epsilon</strong>        </th>        <th style="text-align:center;">        <strong>IJCNN</strong>        </th>        <th style="text-align:center;">        <strong>Spam</strong>        </th>        <th style="text-align:center;">        <strong>Epsilon</strong>        </th>       </tr>       </thead> 						<tbody> 						<tr>        <td style="text-align:center;">        <strong>FM-FTRL</strong>        </td>        <td style="text-align:center;">0.0663</td>        <td style="text-align:center;">0.0867</td>        <td style="text-align:center;">0.1773</td>        <td style="text-align:center;">0.9647</td>        <td style="text-align:center;">0.9070</td>        <td style="text-align:center;">0.8213</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CFM-FTRL</strong>        </td>        <td style="text-align:center;">0.0544</td>        <td style="text-align:center;">0.0598</td>        <td style="text-align:center;">0.1654</td>        <td style="text-align:center;">0.9736</td>        <td style="text-align:center;">0.9390</td>        <td style="text-align:center;">0.8319</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CCFM-OGD</strong>        </td>        <td style="text-align:center;">0.0911</td>        <td style="text-align:center;">0.1076</td>        <td style="text-align:center;">0.1742</td>        <td style="text-align:center;">0.9192</td>        <td style="text-align:center;">0.9239</td>        <td style="text-align:center;">0.8277</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>CCFM-FTRL</strong>        </td>        <td style="text-align:center;">0.0520&#x2020;</td>        <td style="text-align:center;">0.0588&#x2020;</td>        <td style="text-align:center;">0.1380&#x2020;</td>        <td style="text-align:center;">0.9757</td>        <td style="text-align:center;">0.9398</td>        <td style="text-align:center;">0.8668&#x2020;</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>OCCFM</strong>        </td>        <td style="text-align:center;">        <strong>0.0243*</strong>        </td>        <td style="text-align:center;">        <strong>0.0567*</strong>        </td>        <td style="text-align:center;">        <strong>0.1202*</strong>        </td>        <td style="text-align:center;">        <strong>0.9859*</strong>        </td>        <td style="text-align:center;">        <strong>0.9414*</strong>        </td>        <td style="text-align:center;">        <strong>0.8737*</strong>        </td>       </tr>      </tbody>     </table>     </div>     <p>We also compare the running time of OCCFM and other baseline methods and present the results in Fig <a class="fig" href="#fig2">2</a>. Similar with the observation in regression tasks, our proposed OCCFM runs significantly faster than CCFM-OGD and CCFM-FTRL.</p> <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186075/images/www2018-84-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Comparison of the efficiency of CCFM-OGD, CCFM-FTRL and OCCFM on Spam and Epsilon.</span>     </div>     </figure>    </section>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Related Work</h2>     </div>    </header>    <p>Our work is closely related to two topics in machine learning: factorization machine and online learning.    </p>    <section id="sec-23">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Factorization Machine</h3>     </div>     </header>     <p>The core of FM is to leverage feature interactions for feature augmentation. According to the order of feature interactions, the existing research can be categorized into two lines: the first category [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] focuses on second-order FM which models pair-wise feature interactions for feature engineering; while the second category [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] attempts to model the interactions between arbitrary number of features. One line of related research in the first category is Convex Factorization Machine [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], which looks for convex variants of FM. However, the formulations of Convex FM are not well organized into a compact form which we provide for Compact Convexified FM to meet the requirements of online learning settings. The most significant difference between OCCFM and previous research is that OCCFM is an online learning model while most existing variants are batch learning models. Some studies attempt to apply FM in online applications [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>]. But these studies do not fit in the online learning framework with theoretical guarantees while OCCFM provides a theoretically provable regret bound.</p>    </section>    <section id="sec-24">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Online Learning</h3>     </div>     </header>     <p>Online learning stands for a family of efficient and scalable machine learning algorithms [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. Unlike conventional batch learning algorithms, the online learning algorithms are built upon the assumption that the training instances arrive sequentially rather than being available prior to the learning task.</p>     <p>Many algorithms have been proposed for online learning, including the classical Perception algorithm and Passive-Aggressive (PA) algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]. In recent years, the design of many efficient online learning algorithms has been influenced by convex optimization tools [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. Some typical algorithms include Online Gradient Descent (OGD) and Follow-The-Regularized-Leader (FTRL) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. However, their further applicability is limited by the expensive projection operation required when additional norm constraints are added. Recently, the Online Conditional Gradient (OCG) algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] has regained a surge of research interest. It eschews the computational expensive projection operation thus is highly efficient in handling large-scale learning problems. Our proposed OCCFM model builds upon the OCG algorithm. For further details, please refer to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>].</p>    </section>   </section>   <section id="sec-25">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions</h2>     </div>    </header>    <p>In this paper, we propose an online variant of FM which works in online learning settings. First, we first invent a convexification Compact Convexified FM (CCFM) based on OCO such that it fulfills the two fundamental requirements within the OCO framework. Then, we propose an Online Compact Convex Factorization Machine (OCCFM) algorithm that eschews the projection operation with linear optimization step. In terms of theoretical support, we prove that the algorithm preserves a sub-linear regret, which indicates that our algorithm can perform as well as the best fixed algorithm in hindsight. Regarding the empirical performance of OCCFM, we conduct extensive experiments on real-world datasets. The experimental results in online regression and online classification tasks indicate that OCCFM outperforms the state-of-art online learning variants of FM.</p>   </section>   <section id="sec-26">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work is supported in part by Natural Science Foundation of China (Grant No. 61532011, 61672311), National Key Basic Research Program (2015CB358700), National Program on Key Basic Research Project No. 2015CB352300, National Natural Science Foundation of China Major Project No. U1611461, the NSERC Discovery Grant program, the Canada Research Chair program, the NSERC Strategic Grant program. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. 2008. Competing in the dark: An efficient algorithm for bandit linear optimization. In <em>      <em>In Proceedings of the 21st Annual Conference on Learning Theory (COLT)</em>     </em>.</li>     <li id="BibPLXBIB0002" label="[2]">Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, and Yuanzhi Li. 2017. Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls. <em>      <em>CoRR</em>     </em>abs/1708.02105(2017).</li>     <li id="BibPLXBIB0003" label="[3]">Mathieu Blondel, Akinori Fujino, and Naonori Ueda. 2015. Convex factorization machines. In <em>      <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>     </em>. Springer, 19&#x2013;35.</li>     <li id="BibPLXBIB0004" label="[4]">Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-order factorization machines. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 3351&#x2013;3359.</li>     <li id="BibPLXBIB0005" label="[5]">Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, and Naonori Ueda. 2016. Polynomial networks and factorization machines: new insights and efficient training algorithms. <em>      <em>international conference on machine learning</em>     </em> (2016), 850&#x2013;858.</li>     <li id="BibPLXBIB0006" label="[6]">Stephen Boyd and Lieven Vandenberghe. 2004. <em>      <em>Convex optimization</em>     </em>. Cambridge university press.</li>     <li id="BibPLXBIB0007" label="[7]">N. Cesa-Bianchi, A. Conconi, and C. Gentile. 2004. On the Generalization Ability of On-Line Learning Algorithms. <em>      <em>Information Theory IEEE Transactions on</em>     </em>50, 9 (2004), 2050&#x2013;2057.</li>     <li id="BibPLXBIB0008" label="[8]">Chen Cheng, Fen Xia, Tong Zhang, Irwin King, and Michael&#x00A0;R Lyu. 2014. Gradient boosting factorization machines. In <em>      <em>Proceedings of the 8th ACM Conference on Recommender systems</em>     </em>. ACM, 265&#x2013;272.</li>     <li id="BibPLXBIB0009" label="[9]">Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. <em>      <em>J. Mach. Learn. Res.</em>     </em>7 (Dec. 2006), 551&#x2013;585.</li>     <li id="BibPLXBIB0010" label="[10]">Peter DeMarzo, Ilan Kremer, and Yishay Mansour. 2006. Online Trading Algorithms and Robust Option Pricing. In <em>      <em>Proceedings of the Thirty-eighth Annual ACM Symposium on Theory of Computing</em>     </em>(STOC &#x2019;06). ACM, New York, NY, USA, 477&#x2013;486.</li>     <li id="BibPLXBIB0011" label="[11]">Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted Linear Classification. In <em>      <em>Proceedings of the 25th International Conference on Machine Learning</em>     </em>(ICML &#x2019;08). ACM, New York, NY, USA, 264&#x2013;271.</li>     <li id="BibPLXBIB0012" label="[12]">Elad Hazan <em>et al.</em> 2016. Introduction to online convex optimization. <em>      <em>Foundations and Trends&#x00AE; in Optimization</em>     </em>2, 3-4(2016), 157&#x2013;325.</li>     <li id="BibPLXBIB0013" label="[13]">Elad Hazan and Satyen Kale. 2012. Projection-free Online Learning. In <em>      <em>Proceedings of the 29th International Coference on International Conference on Machine Learning</em>     </em>(ICML&#x2019;12). Omnipress, USA, 1843&#x2013;1850.</li>     <li id="BibPLXBIB0014" label="[14]">Steven&#x00A0;CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2018. Online Learning: A Comprehensive Survey. <em>      <em>arXiv preprint arXiv:1802.02871</em>     </em>(2018).</li>     <li id="BibPLXBIB0015" label="[15]">Steven C.&#x00A0;H. Hoi, Jialei Wang, and Peilin Zhao. 2014. <em>      <em>LIBOL: a library for online learning algorithms</em>     </em>. JMLR. 495&#x2013;499.</li>     <li id="BibPLXBIB0016" label="[16]">Martin Jaggi. 2013. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization. In <em>      <em>Proceedings of the 30th International Conference on Machine Learning</em>     </em>(Proceedings of Machine Learning Research), Sanjoy Dasgupta and David McAllester (Eds.). Vol.&#x00A0;28. PMLR, Atlanta, Georgia, USA, 427&#x2013;435.</li>     <li id="BibPLXBIB0017" label="[17]">Yuchin Juan, Damien Lefortier, and Olivier Chapelle. 2017. Field-aware factorization machines in a real-world online advertising system. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web Companion</em>     </em>. International World Wide Web Conferences Steering Committee, 680&#x2013;688.</li>     <li id="BibPLXBIB0018" label="[18]">Xiangnan He&#x00A0;Hanwang Zhang Fei Wu Tat-Seng&#x00A0;Chua Jun&#x00A0;Xiao, Hao&#x00A0;Ye. 2017. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. In <em>      <em>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</em>     </em>. 3119&#x2013;3125.</li>     <li id="BibPLXBIB0019" label="[19]">Takuya Kitazawa. 2016. Incremental Factorization Machines for Persistently Cold-starting Online Item Recommendation. <em>      <em>arXiv preprint arXiv:1607.02858</em>     </em>(2016).</li>     <li id="BibPLXBIB0020" label="[20]">Chun-Ta Lu, Lifang He, Weixiang Shao, Bokai Cao, and Philip&#x00A0;S. Yu. 2017. Multilinear Factorization Machines for Multi-Task Multi-View Learning. In <em>      <em>Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</em>     </em>(WSDM &#x2019;17). ACM, New York, NY, USA, 701&#x2013;709.</li>     <li id="BibPLXBIB0021" label="[21]">H.&#x00A0;Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar&#x00A0;Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. 2013. Ad Click Prediction: a View from the Trenches. In <em>      <em>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</em>     </em>.</li>     <li id="BibPLXBIB0022" label="[22]">Trung&#x00A0;V. Nguyen, Alexandros Karatzoglou, and Linas Baltrunas. 2014. Gaussian Process Factorization Machines for Context-aware Recommendations. In <em>      <em>Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>     </em>(SIGIR &#x2019;14). ACM, New York, NY, USA, 63&#x2013;72.</li>     <li id="BibPLXBIB0023" label="[23]">Steffen Rendle. 2010. Factorization machines. In <em>      <em>Data Mining (ICDM), 2010 IEEE 10th International Conference on</em>     </em>. IEEE, 995&#x2013;1000.</li>     <li id="BibPLXBIB0024" label="[24]">Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Fast context-aware recommendations with factorization machines. In <em>      <em>Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</em>     </em>. ACM, 635&#x2013;644.</li>     <li id="BibPLXBIB0025" label="[25]">F Rosenblatt. 1958. The perceptron: a probabilistic model for information storage and organization in the brain. <em>      <em>Psychological Review</em>     </em>65, 6 (1958), 386.</li>     <li id="BibPLXBIB0026" label="[26]">Shai Shalev-Shwartz. 2012. Online Learning and Online Convex Optimization. <em>      <em>Found. Trends Mach. Learn.</em>     </em>4, 2 (Feb. 2012), 107&#x2013;194.</li>     <li id="BibPLXBIB0027" label="[27]">Shai Shalev-Shwartz <em>et al.</em> 2012. Online learning and online convex optimization. <em>      <em>Foundations and Trends&#x00AE; in Machine Learning</em>     </em>4, 2(2012), 107&#x2013;194.</li>     <li id="BibPLXBIB0028" label="[28]">Anh-Phuong Ta. 2015. Factorization machines with follow-the-regularized-leader for CTR prediction in display advertising. In <em>      <em>Big Data (Big Data), 2015 IEEE International Conference on</em>     </em>. IEEE, 2889&#x2013;2891.</li>     <li id="BibPLXBIB0029" label="[29]">Jialei Wang, Steven&#x00A0;C.H. Hoi, Peilin Zhao, and Zhi-Yong Liu. 2013. Online Multi-task Collaborative Filtering for On-the-fly Recommender Systems. In <em>      <em>Proceedings of the 7th ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;13). ACM, New York, NY, USA, 237&#x2013;244. <a class="link-inline force-break" href="https://doi.org/10.1145/2507157.2507176"      target="_blank">https://doi.org/10.1145/2507157.2507176</a></li>     <li id="BibPLXBIB0030" label="[30]">Jialei Wang, Peilin Zhao, and Steven C.&#x00A0;H. Hoi. 2016. Soft Confidence-Weighted Learning. <em>      <em>ACM Trans. Intell. Syst. Technol.</em>     </em>8, 1, Article 15 (Sept. 2016), 32&#x00A0;pages.</li>     <li id="BibPLXBIB0031" label="[31]">Jianpeng Xu, Kaixiang Lin, Pang&#x00A0;Ning Tan, and Jiayu Zhou. 2016. Synergies that Matter: Efficient Interaction Selection via Sparse Factorization Machine. In <em>      <em>Siam International Conference on Data Mining</em>     </em>. 108&#x2013;116.</li>     <li id="BibPLXBIB0032" label="[32]">Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Kishan Wimalawarne, Suleiman&#x00A0;A Khan, Samuel Kaski, Hiroshi Mamitsuka, and Yi Chang. 2015. Convex Factorization Machine for Regression. <em>      <em>arXiv preprint arXiv:1507.01073</em>     </em>(2015).</li>     <li id="BibPLXBIB0033" label="[33]">Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Kishan Wimalawarne, Suleiman&#x00A0;A. Khan, Samuel Kaski, Hiroshi Mamitsuka, and Yi Chang. 2017. Convex Factorization Machine for Toxicogenomics Prediction. In <em>      <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>(KDD &#x2019;17). ACM, New York, NY, USA, 1215&#x2013;1224.</li>     <li id="BibPLXBIB0034" label="[34]">Fajie Yuan, Guibing Guo, Joemon&#x00A0;M. Jose, Long Chen, Haitao Yu, and Weinan Zhang. 2017. BoostFM: Boosted Factorization Machines for Top-N Feature-based Recommendation. In <em>      <em>Proceedings of the 22Nd International Conference on Intelligent User Interfaces</em>     </em>(IUI &#x2019;17). ACM, New York, NY, USA, 45&#x2013;54.</li>     <li id="BibPLXBIB0035" label="[35]">Wenpeng Zhang, Peilin Zhao, Wenwu Zhu, Steven C.&#x00A0;H. Hoi, and Tong Zhang. 2017. Projection-free Distributed Online Learning in Networks. In <em>      <em>Proceedings of the 34th International Conference on Machine Learning</em>     </em>(Proceedings of Machine Learning Research), Doina Precup and Yee&#x00A0;Whye Teh (Eds.). Vol.&#x00A0;70. PMLR, International Convention Centre, Sydney, Australia, 4054&#x2013;4062.</li>     <li id="BibPLXBIB0036" label="[36]">Peilin Zhao and Steven&#x00A0;C.H. Hoi. 2013. Cost-sensitive Online Active Learning with Application to Malicious URL Detection. In <em>      <em>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>(KDD &#x2019;13). ACM, New York, NY, USA, 919&#x2013;927.</li>     <li id="BibPLXBIB0037" label="[37]">Peilin Zhao, Steven C.&#x00A0;H. Hoi, Rong Jin, and Tianbao Yang. 2011. Online AUC Maximization. In <em>      <em>Proceedings of the 28th International Conference on International Conference on Machine Learning</em>     </em>(ICML&#x2019;11). Omnipress, USA, 233&#x2013;240.</li>     <li id="BibPLXBIB0038" label="[38]">Erheng Zhong, Yue Shi, Nathan Liu, and Suju Rajan. 2016. Scaling Factorization Machines with Parameter Server. In <em>      <em>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</em>     </em>. ACM, 1583&#x2013;1592.</li>     <li id="BibPLXBIB0039" label="[39]">Martin Zinkevich. 2003. Online Convex Programming and Generalized Infinitesimal Gradient Ascent. In <em>      <em>Proceedings of the Twentieth International Conference on International Conference on Machine Learning</em>     </em>(ICML&#x2019;03). AAAI Press, 928&#x2013;935.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x002A;</sup></a>Equal Contribution, <a href="mailto:jackielinxiao@gmail.com">jackielinxiao@gmail.com</a></p>   <p id="fn2"><a href="#foot-fn2"><sup>&#x2020;</sup></a>Equal Contribution, Corresponding Author, <a href="mailto:zhangwenpeng0@gmail.com">zhangwenpeng0@gmail.com</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>&#x2021;</sup></a>Corresponding Author, <a href="mailto:z-m@tsinghua.edu.cn">z-m@tsinghua.edu.cn</a></p>   <p id="fn4"><a href="#foot-fn4"><sup>&#x00A7;</sup></a>Corresponding Author, <a href="mailto:wwzhu@tsinghua.edu.cn">wwzhu@tsinghua.edu.cn</a></p>   <p id="fn5">    <sup>1</sup>Usually <em>&#x03B3;<sub>t</sub>    </em>is set to <span class="inline-equation"><span class="tex">$\frac{1}{t^{1/2}}$</span>    </span>   </p>   <p id="fn6">    <sup>3</sup>Similarly, <em>&#x03B3;<sub>t</sub>    </em>is set to <span class="inline-equation"><span class="tex">$\frac{1}{t^{1/2}}$</span>    </span>   </p>   <p id="fn7">    <sup>4</sup>We have reivised the minor mistakes made in the original proof and thus give a slightly different bound here.</p>   <p id="fn8">    <sup>5</sup> <a class="link-inline force-break" href="https://grouplens.org/datasets/movielens/">https://grouplens.org/datasets/movielens/</a></p>   <p id="fn9">    <sup>6</sup> <a class="link-inline force-break" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/> ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186075">https://doi.org/10.1145/3178876.3186075</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
