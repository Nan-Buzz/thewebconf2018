<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>TEM: Tree-enhanced Embedding Model for Explainable
  Recommendation</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186066'>https://doi.org/10.1145/3178876.3186066</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186066'>https://w3id.org/oa/10.1145/3178876.3186066</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">TEM: Tree-enhanced Embedding
          Model for Explainable Recommendation</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Xiang</span> <span class=
          "surName">Wang</span>, National University of Singapore,
          <a href=
          "mailto:xiangwang@u.nus.edu">xiangwang@u.nus.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Xiangnan</span> <span class=
          "surName">He</span><a class="fn" href="#fn1" id=
          "foot-fn1"><sup>⁎</sup></a>, National University of
          Singapore, <a href=
          "mailto:xiangnanhe@gmail.com">xiangnanhe@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Fuli</span> <span class=
          "surName">Feng</span>, National University of Singapore,
          <a href=
          "mailto:fulifeng93@gmail.com">fulifeng93@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Liqiang</span> <span class=
          "surName">Nie</span>, ShanDong University, <a href=
          "mailto:nieliqiang@gmail.com">nieliqiang@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Tat-Seng</span> <span class=
          "surName">Chua</span>, National University of Singapore,
          <a href="mailto:dcscts@nus.edu.sg">dcscts@nus.edu.sg</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186066"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186066</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>While collaborative filtering is the dominant
        technique in personalized recommendation, it models
        user-item interactions only and cannot provide concrete
        reasons for a recommendation. Meanwhile, the rich side
        information affiliated with user-item interactions
        (<em>e.g.,</em> user demographics and item attributes),
        which provide valuable evidence that why a recommendation
        is suitable for a user, has not been fully explored in
        providing explanations.</small></p>
        <p><small>On the technical side, embedding-based methods,
        such as Wide&amp;Deep and neural factorization machines,
        provide state-of-the-art recommendation performance.
        However, they work like a black-box, for which the reasons
        underlying a prediction cannot be explicitly presented. On
        the other hand, tree-based methods like decision trees
        predict by inferring decision rules from data. While being
        explainable, they cannot generalize to unseen feature
        interactions thus fail in collaborative filtering
        applications.</small></p>
        <p><small>In this work, we propose a novel solution named
        <em>Tree-enhanced Embedding Method</em> that combines the
        strengths of embedding-based and tree-based models. We
        first employ a tree-based model to learn explicit decision
        rules (<em>aka.</em> cross features) from the rich side
        information. We next design an embedding model that can
        incorporate explicit cross features and generalize to
        unseen cross features on user ID and item ID. At the core
        of our embedding method is an easy-to-interpret attention
        network, making the recommendation process fully
        transparent and explainable. We conduct experiments on two
        datasets of tourist attraction and restaurant
        recommendation, demonstrating the superior performance and
        explainability of our solution.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Recommender systems;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Explainable
          Recommendation</small>,</span> <span class=
          "keyword"><small>Tree-based Model</small>,</span>
          <span class="keyword"><small>Embedding-based
          Model</small>,</span> <span class="keyword"><small>Neural
          Attention Network</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie, and
          Tat-Seng Chua. 2018. TEM: Tree-enhanced Embedding Model
          for Explainable Recommendation. In <em>WWW 2018: The 2018
          Web Conference,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 11 Pages.
          <a href="https://doi.org/10.1145/3178876.3186066" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186066</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Personalized recommendation is at the core of many online
      customer-oriented services, such as E-commerce, social media,
      and content-sharing websites. Technically speaking, the
      recommendation problem is usually tackled as a matching
      problem, which aims to estimate the relevance score between a
      user and an item based on their available profiles.
      Regardless of the application domain, a user's profile
      usually consists of an ID (to identify which specific user)
      and some side information like age, gender, and income level.
      Similarly, an item's profile typically contains an ID and
      some attributes like category, tags, and price.</p>
      <p>Collaborative filtering (CF) is the most prevalent
      technique for building a personalized recommendation
      system&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0021">21</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>]. It
      leverages users’ interaction histories on items to select the
      relevant items for a user. From the matching view, CF uses
      the ID information only as the profile for a user and an
      item, and forgoes other side information. As such, CF can
      serve as a generic solution for recommendation without
      requiring any domain knowledge. However, the downside is that
      it lacks necessary reasoning or explanations for a
      recommendation. Specially, the explanation mechanisms are
      either <em>because your friend also likes it</em>
      (<em>i.e.,</em> user-based CF&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>]) or <em>because the item
      is similar to what you liked before</em> (<em>i.e.,</em>
      item-based CF&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>]), which are too coarse-grained and
      may be insufficient to convince users on a
      recommendation&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0045">45</a>].</p>
      <p>To persuade users to perform actions on a recommendation,
      we believe it is crucial to provide more concrete reasons in
      addition to similar users or items. For example, we recommend
      <em>iPhone 7 Rose Gold</em> to user <em>Emine</em>, because
      we find females aged 20-25 with a monthly income over
      <font style="normal">$</font>10,000 (which are
      <em>Emine</em>’ demographics) generally prefer Apple products
      of pink color. To supercharge a recommender system with such
      informative reasons, the underlying recommender shall be able
      to (i) <strong>explicitly</strong> discover effective cross
      features from the rich side information of users and items,
      and (ii) estimate user-item matching score in an
      <strong>explainable</strong> way. In addition, we expect the
      use of side information will help in improving the
      performance of recommendation.</p>
      <p>Nevertheless, none of existing recommendation methods can
      satisfy the above two conditions together. In the literature,
      embedding-based methods such as matrix
      factorization&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>] is the most popular CF approach,
      owing to the strong power of embeddings in generalizing from
      sparse user-item relations. Many variants have been proposed
      to incorporate side information, such as factorization
      machine (FM)&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>], Neural FM&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>],
      Wide&amp;Deep&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>], and Deep Crossing&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0036">36</a>]. While
      these methods can learn feature interactions from raw data,
      we argue that the cross feature effects are only captured in
      a rather implicit way during the learning process; and most
      importantly, the cross features cannot be explicitly
      presented&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>]. Moreover, existing works on using
      side information have mainly focused on the cold-start
      issue&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>],
      leaving the explanation of recommendation relatively less
      touched.</p>
      <p>In this work, we aim to fill the research gap by
      developing a recommendation solution that is both accurate
      and explainable. By <strong>accurate</strong>, we expect our
      method to achieve the same level of performance as existing
      embedding-based approaches&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>]. By <strong>explainable</strong>, we
      would like our method to be transparent in generating a
      recommendation and is capable of identifying the key cross
      features for a prediction. Towards this end, we propose a
      novel solution named <em>Tree-enhanced Embedding Method</em>
      (TEM), which combines embedding-based methods with decision
      tree-based approaches. First, we build a <em>gradient
      boosting decision trees</em> (GBDT) on the side information
      of users and items to derive effective cross features. We
      then feed the cross features into an embedding-based model,
      which is a carefully designed neural attention network that
      reweights the cross features according to the current
      prediction. Owing to the explicit cross features extracted by
      GBDTs and the easy-to-interpret attention network, the
      overall prediction process is fully transparent and
      self-explainable. Particularly, to generate reasons for a
      recommendation, we just need to select the most predictive
      cross features based on their attention scores.</p>
      <p>As a main technical contribution, this work presents a new
      scheme that unifies the strengths of embedding-based and
      tree-based methods for recommendation. Embedding-based
      methods are known to have strong generalization
      ability&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>], especially in predicting the unseen
      crosses on user ID and item ID (<em>i.e.,</em> capturing the
      CF effect). However, when operating on the rich side
      information, embedding-based methods lose the important
      property of explainability — the cross features that
      contribute most to the prediction cannot be revealed. On the
      other hand, tree-based methods predict by generating explicit
      decision rules, making the resultant cross features directly
      interpretable. While such a way is highly suitable for
      learning from side information, it fails to predict unseen
      cross features, thus being unsuitable for incorporating user
      ID and item ID. To build an explainable recommendation
      solution, we combine the strengths of embedding-based and
      tree-based methods in a natural and effective manner, which
      to our knowledge has never been studied before.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span>
          Preliminary</h2>
        </div>
      </header>
      <p>We first review the embedding-based model, discussing its
      difficulty in supporting explainable recommendation. We then
      introduce the tree-based model and emphasize its explanation
      mechanism.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Embedding-based Model</h3>
          </div>
        </header>
        <p>Embedding-based model is a typical example of
        representation learning&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>], which aims to learn features from
        raw data for prediction. Matrix Factorization
        (MF)&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>] is a simple yet effective
        embedding-based model for collaborative filtering, for
        which the predictive model can be formulated as:</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{gather} \hat{y}_{MF}(u,
            i)=b_0 + b_u + b_i + {\mathbf {p}}^{\top }_{u}\mathbf
            {q}_{i}, \end{gather}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>where <em>b</em> <sub>0</sub>, <em>b<sub>u</sub></em>
        , <em>b<sub>i</sub></em> are bias terms, <span class=
        "inline-equation"><span class="tex">$\mathbf {p}_{u}\in
        \mathbb {R}^{k}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathbf {q}_{i}\in
        \mathbb {R}^{k}$</span></span> are the embedding vector for
        user <em>u</em> and item <em>i</em>, respectively, and
        <em>k</em> denotes the embedding size.
        <p></p>
        <p>In addition to IDs, users (items) are always associated
        with abundant side information, which may contain relevance
        signal of user preferences on items. Since most of these
        information are categorical variables, they are usually
        converted to real-valued feature vector via one-hot
        encoding&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0032">32</a>]. Let <strong>x</strong>
        <sub><em>u</em></sub> and <strong>x</strong>
        <sub><em>i</em></sub> denote the feature vector for user
        <em>u</em> and item <em>i</em>, respectively. To predict
        <em>y<sub>ui</sub></em> , a typical solution is to
        concatenate <strong>x</strong> <sub><em>u</em></sub> and
        <strong>x</strong> <sub><em>i</em></sub> , <em>i.e.,</em>
        <span class="inline-equation"><span class="tex">${\bf
        x}=[{\bf x}_u, {\bf x}_i] \in \mathbb
        {R}^{n}$</span></span> , which is then fed into a
        predictive model. FM&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0032">32</a>] is a representative of such
        predictive models, which is formulated as:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{gather}
            \hat{y}_{FM}({\bf x})=w_{0}+\sum
            _{t=1}^{n}w_{t}x_{t}+\sum _{t=1}^{n}\sum
            _{j=t+1}^{n}{\mathbf {v}}^{\top }_{t}\mathbf
            {v}_{j}\cdot x_{t}x_{j}, \end{gather}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where <em>w</em> <sub>0</sub> and
        <em>w<sub>t</sub></em> are bias terms, <span class=
        "inline-equation"><span class="tex">${\bf v}_t\in \mathbb
        {R}^{k}$</span></span> and <span class=
        "inline-equation"><span class="tex">${\bf v}_j\in \mathbb
        {R}^{k}$</span></span> denote the embedding for feature
        <em>t</em> and <em>j</em>, respectively. We can see that FM
        associates each feature with an embedding, modeling the
        interaction of every two (nonzero) features via the inner
        product of their embeddings. If only user ID and item ID
        are used as the features of <strong>x</strong>, FM can
        exactly recover the MF model; by feeding IDs and side
        features together into <strong>x</strong>, FM models all
        pairwise (<em>i.e.,</em> second-order) interactions among
        IDs and side features.
        <p></p>
        <p>With the recent advances of deep learning, neural
        network methods have also been employed to build
        embedding-based models&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>]. Specially,
        Wide&amp;Deep&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>] and Deep Crossing&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0036">36</a>] learn
        feature interactions by placing a multi-layer perceptron
        (MLP) above the concatenation of the embeddings of nonzero
        features; the MLP is claimed to be capable of learning
        any-order cross features. Neural FM (NFM)&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0020">20</a>] first
        applies a bilinear interaction pooling on feature
        embeddings (<em>i.e.,</em> <span class=
        "inline-equation"><span class="tex">$\sum _{t=1}^{n}\sum
        _{j=t+1}^{n}x_{t}\mathbf {v}_{t}\odot x_{j}\mathbf
        {v}_{j}$</span></span> ) to learn second-order feature
        interactions, followed by a MLP to learn high-order
        features interactions.</p>
        <p>Despite the strong representation ability of existing
        embedding-based methods in modeling side information, we
        argue that they are not suitable for providing
        explanations. FM models second-order feature interactions
        only and cannot capture high-order cross feature effects;
        moreover, it uniformly considers all second-order
        interactions and cannot distinguish which interactions are
        more important for a prediction&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0046">46</a>]. While neural embedding
        models are able to capture high-order cross features, they
        are usually achieved by a nonlinear neural network above
        feature embeddings. The neural network stacks multiple
        nonlinear layers and is theoretically guaranteed to fit any
        continuous function&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>], however, the fitting process is
        opaque and cannot be explained. To the best of our
        knowledge, there is no way to extract explicit cross
        features from the neural network and evaluate their
        contributions to a prediction.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Tree-based
            Model</h3>
          </div>
        </header>
        <p>In contrast to representation learning methods,
        tree-based models do not learn features for prediction.
        Instead, they perform prediction by learning decision rules
        from data. We represent the structure of a tree model as
        <span class="inline-equation"><span class="tex">$Q=\lbrace
        \mathcal {V},\mathcal {E}\rbrace$</span></span> , where
        <span class="inline-equation"><span class="tex">$\mathcal
        {V}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathcal
        {E}$</span></span> denote the nodes and edges,
        respectively. The nodes in <span class=
        "inline-equation"><span class="tex">$\mathcal
        {V}$</span></span> have three types: the root node
        <em>v</em> <sub>0</sub>, the internal (<em>aka.</em>
        decision) nodes <span class="inline-equation"><span class=
        "tex">$\mathcal {V}_{T}$</span></span> , and the leaf nodes
        <span class="inline-equation"><span class="tex">$\mathcal
        {V}_{L}$</span></span> . Figure <a class="fig" href=
        "#fig1">1</a> illustrates an example of a decision tree
        model. Each decision node <em>v<sub>t</sub></em> splits a
        feature <em>x<sub>t</sub></em> with two decision edges: for
        numerical feature (<em>e.g.,</em> time), it chooses a
        threshold <em>a<sub>j</sub></em> and splits the feature
        into [<em>x<sub>t</sub></em> &lt; <em>a<sub>j</sub></em> ]
        and [<em>x<sub>t</sub></em> ≥ <em>a<sub>j</sub></em> ]; for
        binary feature (<em>e.g.,</em> features after one-hot
        encoding on a categorical variable), it determines whether
        the feature equals to a value or not, <em>i.e.,</em> the
        decision edges are like [<em>x<sub>t</sub></em> =
        <em>a<sub>j</sub></em> ] and [<em>x<sub>t</sub></em> ≠
        <em>a<sub>j</sub></em> ].</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186066/images/www2018-75-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">An example of a GBDT model
            with two subtrees.</span>
          </div>
        </figure>
        <p></p>
        <p>A path from the root node to a leaf node forms a
        <em>decision rule</em>, which can also be seen as a
        <em>cross feature</em>, such as in Figure&nbsp;<a class=
        "fig" href="#fig1">1</a> the leaf node <span class=
        "inline-equation"><span class=
        "tex">$v_{L_{2}}$</span></span> represents <span class=
        "inline-equation"><span class=
        "tex">$[x_{0}{\lt}a_{0}]\&amp;[x_{3}\ge
        a_{3}]\&amp;[x_{2}\ne a_{2}]$</span></span> . Each leaf
        node <span class="inline-equation"><span class=
        "tex">$v_{L_i}$</span></span> has a value
        <em>w<sub>i</sub></em> , denoting the prediction value of
        the corresponding decision rule. Given a feature vector
        <strong>x</strong>, the tree model first determines which
        leaf node <strong>x</strong> falls on, and then takes the
        value of the leaf node as the prediction: <span class=
        "inline-equation"><span class="tex">$\hat{y}_{DT}({\bf
        x})=w_{Q(\mathbf {x})}$</span></span> , where <em>Q</em>
        maps the feature vector to the leaf node based on the tree
        structure. We can see that under such a prediction
        mechanism, the leaf node can be regarded as the most
        prominent cross feature for the prediction. As such, the
        tree-based model is self-interpretable by nature.</p>
        <p>As one single tree may not be expressive enough to
        capture complex patterns in data, a more widely used
        solution is to build a forest, such as gradient boosting
        decision trees (GBDT) which boosts the prediction by
        leveraging multiple additive trees:</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \hat{y}_{GBDT}({\bf x})=\sum _{s=1}^{S} \hat{y}_{DT_s}
            (\mathbf {x}), \end{equation}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>where <em>S</em> denote the number of additive trees,
        and <span class="inline-equation"><span class=
        "tex">$\hat{y}_{DT_s}$</span></span> denotes the predictive
        model for the <em>s</em>-th tree. We can see that GBDT
        extracts <em>S</em> rules to predict the target value of a
        given feature vector, whereas a single tree model predicts
        based on one rule. As such, GBDT usually leads to better
        accuracy than a single tree model&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0007">7</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0018">18</a>].
        <p></p>
        <p>While tree-based models are effective in generating
        interpretable predictions from rich side features, they
        suffer from generalizing to unseen feature interactions. As
        such, tree-based models cannot be used for collaborative
        filtering which needs to model the sparse ID features of
        users and items.</p>
        <p>We can see that the pros and cons of embedding-based and
        tree-based models complement each other, in terms of
        generalization ability and interpretability. Hence, to
        build an effective and explainable recommender systems, a
        natural solution is to combine the two types of models.</p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Tree-enhanced
          Embedding Method</h2>
        </div>
      </header>
      <p>We first present our tree-enhanced embedding method (TEM)
      that unifies the strengths of MF for sparse data modeling and
      GBDTs for cross feature learning. We then discuss the
      explainability and scrutability and analyze the time
      complexity of TEM.</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Predictive
            Model</h3>
          </div>
        </header>
        <p>Given a user <em>u</em>, an item <em>i</em>, and their
        feature vectors <span class="inline-equation"><span class=
        "tex">$[\mathbf {x}_{u},\mathbf {x}_{i}]=\mathbf {x}\in
        \mathbb {R}^{n}$</span></span> as the input, TEM predicts
        the user-item preference as,</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{gather}
            \hat{y}_{TEM}(u,i,\mathbf {x})=b_{0}+\sum
            _{t=1}^{n}b_{t}x_{t}+f_{\Theta }(u,i,\mathbf {x}),
            \end{gather}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where the first two terms model the feature biases
        similar to that of FM, and <em>f<sub>Θ</sub></em>
        (<em>u</em>, <em>i</em>, <strong>x</strong>) is the core
        component of TEM with parameters <em>Θ</em> to model the
        cross feature effect, which is shown in
        Figure&nbsp;<a class="fig" href="#fig2">2</a>. In what
        follows, we elaborate the design of <em>f<sub>Θ</sub></em>
        step by step.
        <p></p>
        <section id="sec-12">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.1.1</span>
              <strong>Constructing Cross Features</strong>.</h4>
            </div>
          </header>
          <p>Instead of embedding-based methods that capture the
          cross feature effect opaquely during the learning
          process, our primary consideration is to make the cross
          features explicit and explainable. A widely used solution
          in industry is to manually craft cross features, and then
          feed them into an interpretable method that can learn the
          importance of each cross feature, such as logistic
          regression. For example, we can cross all values of
          feature variables <em>age</em> and <em>traveler
          style</em> to obtain the second-order cross features like
          <em>[age ≥ 18] &amp; [traveler style=friends]</em>.
          However, the difficulty of such method is that it is not
          scalable. For modeling higher-order feature interactions,
          one has to cross multiple feature variables together,
          resulting in exponential increase in complexity. With a
          large space of billions of features, even performing
          feature selection&nbsp;[<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0043">43</a>] is highly challenging, not to
          mention learning from them. Although through careful
          feature engineering such as crossing important variables
          or values only&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0012">12</a>], one can control the complexity
          to a certain extent, it requires extensive domain
          knowledge to develop an effective solution and is not
          easily domain-adaptable.</p>
          <p>To avoid such labor-intensive feature engineering, we
          leverage the GBDT (briefed in Section&nbsp;<a class="sec"
          href="#sec-9">2.2</a>), to automatically identify useful
          cross features. While GBDT is not specially designed for
          extracting cross features, considering that a leaf node
          represents a cross feature and the trees are constructed
          by optimizing predictions on historical interactions, it
          is reasonable to think that the leaf nodes are useful
          cross features for prediction.</p>
          <p>Formally, we denote a GBDT as a set of decision trees,
          <span class="inline-equation"><span class="tex">$\mathcal
          {Q}=\lbrace Q_{1},\cdots ,Q_{S}\rbrace$</span></span> ,
          where each tree maps a feature vector <strong>x</strong>
          to a leaf node (with a weight); we use
          <em>L<sub>s</sub></em> to denote the number of leaf nodes
          in the <em>s</em>-th tree. Distinct from the original
          GBDT that sums over the weights of activated leaf nodes
          as the prediction, we keep the activated leaf nodes as
          cross features, feeding them into a neural attention
          model for more effective learning. We represent the cross
          features as a multi-hot vector <strong>q</strong>, which
          is a concatenation of multiple one-hot vectors (where a
          one-hot vector encodes the activated leaf node of a
          tree):</p>
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{gather} \mathbf
              {q}=GBDT(\mathbf {x}|\mathcal {Q})=[Q_{1}(\mathbf
              {x}),\cdots ,Q_{S}(\mathbf {x})].
              \end{gather}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>Here <strong>q</strong> is a sparse vector, where
          an element of value 1 indicates an activated leaf node
          and the number of nonzero elements in <strong>q</strong>
          is <em>S</em>. Let the size of <strong>q</strong> be
          <em>L</em> = ∑ <sub><em>s</em></sub>
          <em>L<sub>s</sub></em> . For example, in
          Figure&nbsp;<a class="fig" href="#fig1">1</a>, there are
          two subtrees <em>Q</em> <sub>1</sub> and <em>Q</em>
          <sub>2</sub> with 5 and 3 leaf nodes, respectively. If
          <strong>x</strong> ends up with the second and third leaf
          node of <em>Q</em> <sub>1</sub> and <em>Q</em>
          <sub>2</sub>, respectively, the resultant multi-hot
          vector <strong>q</strong> should be [0, 1, 0, 0, 0, 0, 0,
          1]. Let the semantics of feature variables (<em>x</em>
          <sub>0</sub> to <em>x</em> <sub>5</sub>) and values
          (<em>a</em> <sub>0</sub> to <em>a</em> <sub>5</sub>) of
          Figure&nbsp;<a class="fig" href="#fig1">1</a> be listed
          in Table&nbsp;<a class="tbl" href="#tab1">1</a>, then
          <strong>q</strong> implies the two cross features
          extracted from <strong>x</strong>:
          <p></p>
          <ol class="list-no-style">
            <li id="list1" label="(1)"><span class=
            "inline-equation"><span class=
            "tex">$v_{L_{1}}$</span></span> : <em>[Age &lt; 18]
            &amp; [Country ≠ France] &amp; [Restaurant Tag=
            French]</em>.<br /></li>
            <li id="list2" label="(2)"><span class=
            "inline-equation"><span class=
            "tex">$v_{L_{7}}$</span></span> : <em>[Expert Level ≥
            4] &amp; [Traveler Style ≠ Luxury
            Traveler]</em>.<br /></li>
          </ol>
        </section>
        <section id="sec-13">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.1.2</span>
              <strong>Prediction with Cross Features</strong>.</h4>
            </div>
          </header>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186066/images/www2018-75-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">Illustrative architecture
              of our TEM framework.</span>
            </div>
          </figure>
          <p>With the explicit cross features, we can employ sparse
          linear methods to learn the importance of each cross
          feature, and select the top cross features as the
          explanation for a prediction. The prior work by Facebook
          &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0022">22</a>] has demonstrated the
          effectiveness of such a solution, which feeds the leaf
          nodes of a GBDT into a logistic regression (LR) model. We
          term this solution as <em>GBDT+LR</em>. Although GBDT+LR
          is capable of learning the importance of cross features,
          it assigns a cross feature the same weight for
          predictions of all user-item pairs, which limits the
          modeling fidelity. In real applications, it is common
          that users with similar demographics may choose similar
          items, but they are driven by different intents or
          reasons.</p>
          <p>As an example, let (<em>u</em>, <em>i</em>,
          <strong>x</strong>) and (<em>u</em>′, <em>i</em>′,
          <strong>x</strong>′) be two positive instances. Assuming
          <strong>x</strong> equals to <strong>x</strong>′, then
          the two instances will have the same cross features from
          GBDT. Since each cross feature has a global weight
          independent of the training instance in LR, the
          predictions of (<em>u</em>, <em>i</em>) and (<em>u</em>′,
          <em>i</em>′) will be interpreted as the same top cross
          features, regardless of the possibility that the actual
          reasons behind <em>u</em> chose <em>i</em> and
          <em>u</em>′ chose <em>i</em>′ are different. To ensure
          the expressiveness, we believe it is important to score
          the cross features differently for different user-item
          pairs, <em>i.e.,</em> personalizing the weights on cross
          features rather than using a global weighting
          mechanism.</p>
          <p>Recent advances on neural recommender models such as
          Wide&amp;Deep&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0012">12</a>] and NFM&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0020">20</a>] can
          allow personalized importance on cross features. This is
          achieved by embedding user ID, item ID, and cross
          features together into a shared embedding space, and then
          performing nonlinear transformations (<em>e.g.,</em> by
          fully connected layers) on the embedding vectors. The
          strong representation power of nonlinear hidden layers
          enables complicated interactions among user ID, item ID,
          and cross features to be captured. As such, a cross
          feature can impact differently when predicting with
          different user-item pairs. However, such methods cannot
          interpret the personalized weights of cross features, due
          to the hardly explainable nonlinear hidden layers. As
          such, for explainability purpose we have to discard the
          use of fully connected hidden layers, although they are
          helpful to a model's performance in existing methods.</p>
          <p>To develop a method that is both effective and
          explainable, we introduce two essential ingredients of
          our TEM — embedding and attention. Specifically, we first
          associate each cross feature with an embedding vector,
          allowing the correlations among cross features to be
          captured. We then devise an attention mechanism to
          explicitly model the personalized weights on cross
          features. Lastly, the embeddings of user ID, item ID, and
          cross features are integrated together for the final
          prediction. The use of embedding and attention endows TEM
          strong representation ability and guarantees the
          effectiveness, even though it is a shallow model without
          any fully connected hidden layer. In what follows, we
          elaborate the two key ingredients of TEM.</p>
          <div class="table-responsive" id="tab1">
            <div class="table-caption">
              <span class="table-number">Table 1:</span>
              <span class="table-title">The semantics of feature
              variables and values of the GBDT model in
              Figure&nbsp;<a class="fig" href="#fig1">1</a>.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;"><em>x</em>
                  <sub>0</sub> ← <em>Age</em></td>
                  <td style="text-align:left;"><em>x</em>
                  <sub>1</sub> ← <em>Expert Level</em></td>
                  <td><em>x</em> <sub>2</sub> ← <em>Restaurant
                  Tag</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><em>a</em>
                  <sub>0</sub> ← 18</td>
                  <td style="text-align:left;"><em>a</em>
                  <sub>1</sub> ← 4</td>
                  <td><em>a</em> <sub>2</sub> ←
                  <em>French</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><em>x</em>
                  <sub>3</sub> ← <em>Country</em></td>
                  <td style="text-align:left;"><em>x</em>
                  <sub>4</sub> ← <em>Traveler Style</em></td>
                  <td><em>x</em> <sub>5</sub> ← <em>Price</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><em>a</em>
                  <sub>3</sub> ← <em>France</em></td>
                  <td style="text-align:left;"><em>a</em>
                  <sub>4</sub> ← <em>Luxury Traveler</em></td>
                  <td><em>a</em> <sub>5</sub> ← <em><font style=
                  "normal">$</font><font style=
                  "normal">$</font><font style=
                  "normal">$</font><font style=
                  "normal">$</font></em></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p><strong>Embedding</strong>. Given the cross feature
          vector <strong>q</strong> generated by GBDT, we project
          each cross feature <em>j</em> into an embedding vector
          <span class="inline-equation"><span class="tex">${\bf
          v}_j\in \mathbb {R}^{k}$</span></span> , where <em>k</em>
          is the embedding size. After the operation, we obtain a
          set of embedding vectors <span class=
          "inline-equation"><span class="tex">$\mathcal {V}=\lbrace
          q_1\mathbf {v}_{1},\cdots ,q_L\mathbf
          {v}_{L}\rbrace$</span></span> . Since <strong>q</strong>
          is a sparse vector with only a few nonzero elements, we
          only need to include the embeddings of nonzero features
          for a prediction, <em>i.e.,</em> <span class=
          "inline-equation"><span class="tex">$\mathcal {V}=\lbrace
          \mathbf {v}_{l}\rbrace$</span></span> where
          <em>q<sub>l</sub></em> ≠ 0. We use <strong>p</strong>
          <sub><em>u</em></sub> and <strong>q</strong>
          <sub><em>i</em></sub> to denote the user embedding and
          item embedding, respectively.</p>
          <p>There are two advantages of embedding the cross
          features into a vector space, compared to LR that uses a
          scalar to weight a feature. First, learning with
          embeddings can capture the correlations among features,
          <em>e.g.,</em> frequently co-occurred features may yield
          similar embeddings, which can alleviate the data sparsity
          issue. Second, it provides a means to seamlessly
          integrate the output of GBDT with the embedding-based
          collaborative filtering, being more flexible than a late
          fusion on the model predictions&nbsp;(<em>e.g.,</em>
          boosting GBDT with FM as used in [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0049">49</a>]).
          <strong>Attention</strong>. Inspired by the previous
          work&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0046">46</a>], we explicitly capture the
          varying importance of cross features on prediction by
          assigning an attentive weight for the embedding of each
          cross feature. Here we consider two ways to aggregate the
          embeddings of cross features, average pooling and max
          pooling, to obtain a unified representation <span class=
          "inline-equation"><span class="tex">$\mathbf
          {e}(u,i,\mathcal {V})$</span></span> for cross
          features:</p>
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{gather} \left\lbrace
              \begin{array}{@{}l@{\quad }l@{}}\mathbf
              {e}_{avg}(u,i,\mathcal {V})= \frac{1}{|\mathcal
              {V}|}\sum _{\mathbf {v}_{l}\in \mathcal {V}}
              w_{uil}\mathbf {v}_{l},\\\mathbf
              {e}_{max}(u,i,\mathcal {V})= max\_pool_{\mathbf
              {v}_{l}\in \mathcal {V}} (w_{uil}{\bf v}_l)
              ,\\\end{array}\right. \end{gather}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>where <em>w<sub>uil</sub></em> is a trainable
          parameter denoting the attentive weight of the
          <em>l</em>-th cross feature in constituting the unified
          representation, and importantly, it is personalized to be
          dependent with (<em>u</em>, <em>i</em>).
          <p></p>
          <p>While the above solution seems to be sound and
          explainable, the problem is that for (<em>u</em>,
          <em>i</em>) pairs that have never co-occurred before, the
          attentive weight <em>w<sub>uil</sub></em> cannot be
          estimated. In addition, the parameter space of <em>w</em>
          is too large — there are <em>UIL</em> weights in total
          (where <em>U</em>, <em>I</em>, and <em>L</em> denote the
          number of users, items, and the size of
          <strong>q</strong>, respectively), which is impractical
          to materialize for real-world applications. To address
          the generalization and scalability issues, we consider
          modeling <em>w<sub>uil</sub></em> as a function dependent
          on the embeddings of <em>u</em>, <em>i</em>, and
          <em>l</em>, rather than learning <em>w<sub>uil</sub></em>
          freely from data. Inspired by the recent
          success&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0046">46</a>] that uses multi-layer
          perceptrons (MLPs) to learn the attentive weights, we
          similarly use a MLP to parameterize
          <em>w<sub>uil</sub></em> . We call the MLP as the
          <em>attention network</em>, which is defined as:</p>
          <div class="table-responsive" id="eq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{gather} \left\lbrace
              \begin{array}{@{}l@{\quad }l@{}}w^{\prime
              }_{uil}&amp;={\mathbf {h}}^{\top }ReLU(\mathbf
              {W}\left(\left[\mathbf {p}_{u}\odot \mathbf
              {q}_{i},\mathbf {v}_{l} \right]\right)+\mathbf
              {b})\\w_{uil}&amp;=\frac{exp(w^{\prime }_{uil})}{\sum
              _{(u,i,\mathbf {x})\in \mathcal {O}}exp(w^{\prime
              }_{uil})} \end{array}\right.,
              \end{gather}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>where <span class="inline-equation"><span class=
          "tex">$\mathbf {W}\in \mathbb {R}^{a\times
          2k}$</span></span> and <span class=
          "inline-equation"><span class="tex">$\mathbf {b}\in
          \mathbb {R}^{a}$</span></span> denote the weight matrix
          and bias vector of the hidden layer, respectively, and
          <em>a</em> controls the size of the hidden layer. The
          vector <span class="inline-equation"><span class=
          "tex">$\mathbf {h}\in \mathbb {R}^{a}$</span></span>
          projects the hidden layer into the attentive weight for
          output. We used the rectifier as the activation function
          and normalized the attentive weights using softmax.
          Figure <a class="fig" href="#fig3">3</a> illustrates the
          architecture of our attention network, and we term
          <em>a</em> as the <em>attention size</em>.
          <p></p>
          <p><strong>Final Prediction</strong>. Having established
          the attentive embeddings, we obtain a unified embedding
          vector <span class="inline-equation"><span class=
          "tex">$\mathbf {e}(u,i,\mathcal {V})$</span></span> for
          cross features. To incorporate the CF modeling, we
          concatenate <span class="inline-equation"><span class=
          "tex">$\mathbf {e}(u,i,\mathcal {V})$</span></span> with
          <strong>p</strong> <sub><em>u</em></sub>
          ⊙<strong>q</strong> <sub><em>i</em></sub> , which
          reassembles MF to model the interaction between user ID
          and item ID. We then apply a linear regression to project
          the concatenated vector to the final prediction. This
          leads to the predictive model of our TEM as:</p>
          <div class="table-responsive" id="eq8">
            <div class="display-equation">
              <span class="tex mytex">\begin{gather}
              \hat{y}_{TEM}(u,i,\mathbf {x})=b_{0}+\sum
              _{t=1}^{m}b_{t}x_{t}+{\mathbf {r}}^{\top
              }_{1}(\mathbf {p}_{u}\odot \mathbf {q}_{i})+{\mathbf
              {r}}^{\top }_{2}\mathbf {e}(u,i,\mathcal {V}),
              \end{gather}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>where <span class="inline-equation"><span class=
          "tex">$\mathbf {r}_{1}\in \mathbb {R}^k$</span></span>
          and <span class="inline-equation"><span class=
          "tex">$\mathbf {r}_{2}\in \mathbb {R}^k$</span></span>
          are the weights of the final linear regression layer. As
          can be seen, our TEM is a shallow and additive model. To
          interpret a prediction, we can easily evaluate the
          contribution of each component. We use
          <strong>TEM-avg</strong> and <strong>TEM-max</strong> to
          denote the TEM that uses <strong>e</strong>
          <sub><em>avg</em></sub> (·) and <strong>e</strong>
          <sub><em>max</em></sub> (·), respectively, and discuss
          their explanation schemes in Section&nbsp;<a class="sec"
          href="#sec-16">3.3.1</a>.
          <figure id="fig3">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186066/images/www2018-75-fig3.jpg"
            class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span>
              <span class="figure-title">Illustration of the
              attention network in TEM.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span>
            Learning</h3>
          </div>
        </header>
        <p>Similar to the recent work on neural collaborative
        filtering&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>], we solve the item recommendation
        task as a binary classification problem. Specifically, an
        observed user-item interaction is assigned to a target
        value 1, otherwise 0. We optimize the pointwise log loss,
        which forces the prediction score <span class=
        "inline-equation"><span class=
        "tex">$\hat{y}_{ui}$</span></span> to be close to the
        target <em>y<sub>ui</sub></em> :</p>
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{gather} \mathcal
            {L}=\sum _{(u,i,\mathbf {x})\in \mathcal
            {O}}-y_{ui}\log {\sigma (\hat{y}_{ui})}-(1-y_{ui})\log
            {(1-\sigma (\hat{y}_{ui}))}, \end{gather}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>where <em>σ</em> is the activation function to
        restrict the prediction to be in (0, 1), set as sigmoid
        <em>σ</em>(<em>x</em>) = 1/(1 + <em>e</em> <sup>−
        <em>x</em></sup> ) in this work. The regularization terms
        are omitted here for clarity (we tuned the <em>L</em>
        <sub>2</sub> regularization in experiments when overfitting
        was observed). Note that optimizing other objective
        functions are also technically viable, such as the
        pointwise regression loss&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0041">41</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0042">42</a>] and ranking
        loss&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0009">9</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0033">33</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0044">44</a>]. In this
        work, we use the log loss as a demonstration of our TEM.
        <p></p>
        <p>Since TEM consists of two cascaded models, both them are
        trained to optimize the same log loss. We first train the
        GBDT, which greedily fits additive trees on the whole
        training data&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0010">10</a>]. After obtaining the cross
        features from GBDT, we optimize the embedding-based
        prediction model using the mini-batch
        Adagrad&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>]. Each mini-batch contains
        stochastic positive instances and randomly paired negative
        instances. Same as the optimal setting of [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0021">21</a>], we pair one positive
        instance with four negative instances, which empirically
        shows good performance.</p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Discussion</h3>
          </div>
        </header>
        <section id="sec-16">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.3.1</span>
              <strong>Explainability &amp;
              Scrutability</strong></h4>
            </div>
          </header>
          <p>The two pooling methods as defined in
          Equation&nbsp;(<a class="eqn" href="#eq6">6</a>)
          aggregate the embeddings of cross features differently,
          resulting in different explanation mechanisms for TEM-avg
          and TEM-max. Specifically, the average pooling linearly
          combines all embeddings, with each embedding a weight to
          denote its importance. As such, the
          <em>w<sub>uil</sub></em> of <span class=
          "inline-equation"><span class="tex">$\mathbf
          {e}_{avg}(u,i,\mathcal {V})$</span></span> can be
          directly used to select top cross features
          (<em>i.e.,</em> decision rules) as the explanation of a
          prediction&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0046">46</a>]. In contrast, the max pooling is
          a nonlinear operator, where the <em>d</em>-th dimension
          of <span class="inline-equation"><span class=
          "tex">$\mathbf {e}_{max}(u,i,\mathcal {V})$</span></span>
          is set to be that of the <em>l</em>-th cross feature
          embedding with the maximum
          <em>w<sub>uil</sub>v<sub>ld</sub></em> . As such, at most
          <em>k</em> cross feature embeddings will contribute to
          the unified representation<a class="fn" href="#fn2" id=
          "foot-fn2"><sup>1</sup></a>, and we can treat the max
          pooling as performing feature selection on cross features
          in the embedding space. To select top cross features for
          explanation, we need to track the embeddings of which
          cross features contribute most during the max pooling,
          rather than simply relying on <em>w<sub>uil</sub></em> .
          We conduct a case study on explainability of TEM in
          Section&nbsp;<a class="sec" href="#sec-28">4.4.1</a>.</p>
          <p>Empowered by the transparency in generating a
          recommendation, TEM allows the recommender to be
          scrutable&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0039">39</a>]. If a user is unsatisfied with a
          recommendation due to improper reasons, TEM allows a user
          to correct the reasoning process to obtain refreshed
          recommendations. As Equation&nbsp;(<a class="eqn" href=
          "#eq8">8</a>) shows, we can easily obtain the
          contribution of each cross feature on the final
          prediction, <em>e.g.,</em> <span class=
          "inline-equation"><span class=
          "tex">$y_{uil}=w_{uil}{\mathbf {r}}^{\top }_{2}\mathbf
          {v}_{l}$</span></span> for TEM-avg. When getting feedback
          from a user (<em>i.e.,</em> the signals indicating what
          she likes or not), we can localize the cross features
          that contain the signals, and then modify the
          corresponding attentive weights. As such, we can refresh
          the predictions and re-rank the recommendation list
          without re-training the whole model. We use a case study
          to demonstrate the scrutability of TEM in
          Section&nbsp;<a class="sec" href="#sec-29">4.4.2</a>.</p>
        </section>
        <section id="sec-17">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.3.2</span>
              <strong>Time Complexity Analysis</strong></h4>
            </div>
          </header>
          <p>As we separate the learning procedure into two phases,
          we can calculate the computational costs step by step.
          Generally, the time complexity of building a GBDT model
          is
          <em>O</em>(<em>SD</em>‖<strong>x</strong>‖<sub>0</sub>log <em>n</em>),
          where <em>S</em> is the number of trees, <em>D</em> is
          the maximum depth of trees, <em>n</em> is the number of
          training instances, and ‖<strong>x</strong>‖<sub>0</sub>
          denotes the average number of non-zero entries in the
          training instances. Moreover, we can speed up the greedy
          algorithm in GBDT by using the block structure like
          XGBoost&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0010">10</a>].</p>
          <p>For the embedding component, calculating the attention
          score for each (<em>u</em>, <em>i</em>, <em>l</em>) costs
          time <em>O</em>(2<em>ak</em>), where <em>a</em> and
          <em>k</em> are the attention and embedding size,
          respectively. Accordingly, adopting the pooling operation
          for each (<em>u</em>, <em>i</em>) costs
          <em>O</em>(2<em>akS</em>). As such, to train the
          embedding model of TEM over <em>n</em> training
          instances, the complexity is <em>O</em>(2<em>akSn</em>).
          Therefore, the overall time complexity for training TEM
          from scratch is
          <em>O</em>(<em>SD</em>‖<strong>x</strong>‖<sub>0</sub>log <em>n</em>
          + 2<em>akSn</em>).</p>
        </section>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span>
          Experiments</h2>
        </div>
      </header>
      <p>As the key contribution of the work is to generate
      accurate and explainable recommendations, we conduct
      experiments to answer the following questions:</p>
      <ol class="list-no-style">
        <li id="list3" label="(1)">
        <strong>RQ1</strong>:&nbsp;Compared with the
        state-of-the-art recommendation methods, can our TEM
        achieve comparable accuracy?<br /></li>
        <li id="list4" label="(2)"><strong>RQ2</strong>:&nbsp;Can
        TEM make the recommendation results easy-to-interpret by
        using cross features and the attention network?<br /></li>
        <li id="list5" label="(3)"><strong>RQ3</strong>:&nbsp;How
        do different hyper-parameter settings (<em>e.g.,</em> the
        number of trees and embedding size) affect TEM?<br /></li>
      </ol>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Data
            Description</h3>
          </div>
        </header>
        <p>We collect data from two populous cities in
        TripAdvisor<a class="fn" href="#fn3" id=
        "foot-fn3"><sup>2</sup></a>, London (LON) and New York City
        (NYC), and separately perform experiments of tourist
        attraction and restaurant recommendation. We term the two
        datasets as LON-A and NYC-R respectively. In particular, we
        crawl 1,001 tourist attractions (<em>e.g., British
        Museum</em>) from LON with the corresponding ratings
        written by 17,238 users from August 2014 to August 2017;
        similarly, 8,791 restaurants (<em>e.g., The River
        Cafe</em>) and 16,015 users are obtained from NYC. The
        ratings are transformed into binary implicit feedback as
        ground truth, indicating whether the user has interacted
        with the specific item. To ensure the quality of the data,
        we retain users/items with at least five ratings only. The
        statistics of two datasets are summarized in
        Table&nbsp;<a class="tbl" href="#tab2">2</a>. Moreover, we
        have collected the natural or system-generated labels that
        are affiliated with users and items as their side
        information (<em>aka.</em> profile). Particularly, the
        profile of each user includes gender (<em>e.g.,
        Female</em>), age (<em>e.g.,</em> 25-34), and traveler
        styles (<em>e.g., Foodie</em> and <em>Beach Goer</em>);
        meanwhile, the side information of an item consists of
        attributes (<em>e.g., Art Museum</em> and <em>French</em>),
        tags (<em>e.g., Rosetta Stone</em> and
        <em>Madelenies</em>), and price (<em>e.g., <font style=
        "normal">$</font><font style="normal">$</font><font style=
        "normal">$</font></em>). We have summarized all types of
        user/item side information in Table&nbsp;<a class="tbl"
        href="#tab3">3</a>.</p>
        <p>For each dataset, we holdout the latest 20% interaction
        history of each user to construct the test set, and
        randomly split the remaining data into training (70%) and
        validation (10%) sets. The validation set is used to tune
        hyper-parameters and the final performance comparison is
        conducted on the test set.</p>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Experimental Settings</h3>
          </div>
        </header>
        <section id="sec-21">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.1</span>
              <strong>Evaluation Protocols</strong></h4>
            </div>
          </header>
          <p>Given one positive user-item interaction in the
          testing set, we pair it with 50 negative instances that
          the user did not consume before. Then each method outputs
          prediction scores for these 51 instances. To evaluate the
          prediction scores, we adopt two metrics: the error-based
          log loss and the ranking-aware ndcg@<em>K</em>.</p>
          <ul class="list-no-style">
            <li id="list6" label="•">
              <strong>logloss</strong>: logarithmic
              loss&nbsp;[<a class="bib" data-trigger="hover"
              data-toggle="popover" data-placement="top" href=
              "#BibPLXBIB0036">36</a>] measures the probability
              that one predicted user-item interaction diverges
              from the ground truth. A lower logloss indicates a
              better performance.<br />
            </li>
            <li id="list7" label="•">
              <strong>ndcg@<em>K</em></strong> :
              ndcg@<em>K</em>&nbsp;[<a class="bib" data-trigger=
              "hover" data-toggle="popover" data-placement="top"
              href="#BibPLXBIB0017">17</a>, <a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0019">19</a>,
              <a class="bib" data-trigger="hover" data-toggle=
              "popover" data-placement="top" href=
              "#BibPLXBIB0021">21</a>, <a class="bib" data-trigger=
              "hover" data-toggle="popover" data-placement="top"
              href="#BibPLXBIB0029">29</a>, <a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0030">30</a>]
              assigns the higher importance to the items within the
              top <em>K</em> positions of the ranking list. A
              higher ndcg@<em>K</em> reflects a better accuracy of
              getting top ranks correct.<br />
            </li>
          </ul>
          <p>We report the average scores for all testing
          instances, where logloss indicates the generalization
          ability of each model, and ndcg reflects the performance
          for top-<em>K</em> recommendation. The same settings
          apply for the hyper-parameter tuning on the validation
          set.</p>
        </section>
        <section id="sec-22">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.2</span>
              <strong>Baselines</strong></h4>
            </div>
          </header>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span>
              <span class="table-title">Statistics of the
              datasets.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Dataset</th>
                  <th style="text-align:center;">User#</th>
                  <th style="text-align:center;">User Feature#</th>
                  <th style="text-align:center;">Item#</th>
                  <th style="text-align:center;">Item Feature#</th>
                  <th>Interaction#</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">LON-A</td>
                  <td style="text-align:center;">16,315</td>
                  <td style="text-align:center;">3,230</td>
                  <td style="text-align:center;">953</td>
                  <td style="text-align:center;">4,731</td>
                  <td>136,978</td>
                </tr>
                <tr>
                  <td style="text-align:center;">NYC-R</td>
                  <td style="text-align:center;">15,232</td>
                  <td style="text-align:center;">3,230</td>
                  <td style="text-align:center;">6,258</td>
                  <td style="text-align:center;">10,411</td>
                  <td>129,964</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span>
              <span class="table-title">Statistics of the side
              information, where the dimension of each feature is
              shown in parentheses.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Side
                  Information</th>
                  <th>Features (Category#)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">LON-A/NYC-R User
                  Feature</td>
                  <td>Age (6), Gender (2), Expert Level (6),</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td>Traveler Styles (18), Country (126),</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td>City (3,072)</td>
                </tr>
                <tr>
                  <td style="text-align:left;">LON-A Attraction
                  Feature</td>
                  <td>Attributes (89), Tags (4,635), Rating
                  (7)</td>
                </tr>
                <tr>
                  <td style="text-align:left;">NYC-R Restaurant
                  Feature</td>
                  <td>Attributes (100), Tags (10,301), Price
                  (3),</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td>Rating (7)</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>We compare our TEM with the following methods to
          justify the rationality of our proposal:</p>
          <ul class="list-no-style">
            <li id="list8" label="•">
              <strong>XGBoost</strong>&nbsp;[<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href=
              "#BibPLXBIB0010">10</a>]:&nbsp;This is the
              state-of-the-art tree-based method that captures
              complex feature dependencies (<em>aka.</em> cross
              features).<br />
            </li>
            <li id="list9" label="•">
              <strong>GBDT+LR</strong>&nbsp;[<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href=
              "#BibPLXBIB0022">22</a>]:&nbsp;This method feeds the
              cross features extracted from GBDT into the logistic
              regression, aiming to refine the weights for each
              cross feature.<br />
            </li>
            <li id="list10" label="•">
              <strong>GB-CENT</strong>&nbsp;[<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href=
              "#BibPLXBIB0049">49</a>]:&nbsp;Such state-of-the-art
              boosting method combines the prediction results from
              MF and GBDT. To adjust GB-CENT to perform our tasks,
              we input the ID features and side information to MF
              and GBDT, respectively.<br />
            </li>
            <li id="list11" label="•">
              <strong>FM</strong>&nbsp;[<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href=
              "#BibPLXBIB0032">32</a>]:&nbsp;This is a generic
              embedding-based model that encodes side information
              and IDs with embedding vectors. It implicitly models
              all the second-order cross features via the inner
              product of any two feature embeddings.<br />
            </li>
            <li id="list12" label="•">
              <strong>NFM</strong>&nbsp;[<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href=
              "#BibPLXBIB0020">20</a>]:&nbsp;Neural FM is the
              state-of-the-art factorization model under the neural
              network framework. It stacks multiple fully connected
              layers above the inner products of feature embeddings
              to capture higher-order and nonlinear cross features.
              Specially, we employed one hidden layers for NFM as
              suggested in&nbsp;[<a class="bib" data-trigger=
              "hover" data-toggle="popover" data-placement="top"
              href="#BibPLXBIB0020">20</a>].<br />
            </li>
          </ul>
        </section>
        <section id="sec-23">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.3</span>
              <strong>Parameter Settings</strong></h4>
            </div>
          </header>
          <p>For a fair comparison, we optimize all the methods
          with the same objective function of
          Equation&nbsp;(<a class="eqn" href="#eq9">9</a>). We
          implement our proposed TEM<a class="fn" href="#fn4" id=
          "foot-fn4"><sup>3</sup></a> using Tensorflow<a class="fn"
          href="#fn5" id="foot-fn5"><sup>4</sup></a>. We use
          XGBoost<a class="fn" href="#fn6" id=
          "foot-fn6"><sup>5</sup></a> to implement the tree-based
          components of all methods, where the number of trees and
          the maximum depth of trees is searched in {100, 200, 300,
          400, 500} and {3, 4, 5, 6}, respectively. For all
          embedding-based components, we test the embedding size of
          {5, 10, 20, 40}, and empirically set the attention size
          same as the embedding size. All embedding-based methods
          are optimized using the mini-batch Adagrad for a fair
          comparison, where the learning rate is searched in
          {0.005, 0.01, 0.05, 0.1, 0.5}. Moreover, the early
          stopping strategy is performed, where we stopped training
          if the logloss on the validation set increased for four
          successive epoches. Without special mention, we show the
          results of tree number 500, maximum depth 6, and
          embedding size 20, and more results of the key parameters
          are shown in Section&nbsp;<a class="sec" href=
          "#sec-30">4.5</a>.</p>
        </section>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Performance
            Comparison (RQ1)</h3>
          </div>
        </header>
        <p>We start by comparing the performance of all the
        methods. We then explore how the cross features affect the
        recommendation results.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186066/images/www2018-75-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Performance comparison of
            logloss <em>w.r.t.</em> the cross features on LON-A and
            NYC-R datasets.</span>
          </div>
        </figure>
        <p></p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Performance comparison between all the
            methods, where the significance test is based on
            logloss of TEM-max.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th colspan="3" style="text-align:center;">
                  LON-A
                  <hr />
                </th>
                <th colspan="3" style="text-align:center;">
                  NYC-R
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">Method</th>
                <th style="text-align:center;">logloss</th>
                <th style="text-align:center;">ndcg@5</th>
                <th style="text-align:center;">
                <em>p</em>-value</th>
                <th style="text-align:center;">logloss</th>
                <th>ndcg@5</th>
                <th><em>p</em>-value</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">XGBoost</td>
                <td style="text-align:center;">0.1251</td>
                <td style="text-align:center;">0.6785</td>
                <td style="text-align:center;">8e − 5</td>
                <td style="text-align:center;">0.1916</td>
                <td>0.3943</td>
                <td>4e − 5</td>
              </tr>
              <tr>
                <td style="text-align:center;">GBDT+LR</td>
                <td style="text-align:center;">0.1139</td>
                <td style="text-align:center;">0.6790</td>
                <td style="text-align:center;">2e − 4</td>
                <td style="text-align:center;">0.1914</td>
                <td>0.3997</td>
                <td>4e − 4</td>
              </tr>
              <tr>
                <td style="text-align:center;">GB-CENT</td>
                <td style="text-align:center;">0.1246</td>
                <td style="text-align:center;">0.6784</td>
                <td style="text-align:center;">6e − 5</td>
                <td style="text-align:center;">0.1918</td>
                <td>0.3995</td>
                <td>4e − 5</td>
              </tr>
              <tr>
                <td style="text-align:center;">FM</td>
                <td style="text-align:center;">0.0939</td>
                <td style="text-align:center;">0.6809</td>
                <td style="text-align:center;">1e − 2</td>
                <td style="text-align:center;">0.1517</td>
                <td>0.4018</td>
                <td>5e − 5</td>
              </tr>
              <tr>
                <td style="text-align:center;">NFM</td>
                <td style="text-align:center;">0.0892</td>
                <td style="text-align:center;">0.6812</td>
                <td style="text-align:center;">2e − 2</td>
                <td style="text-align:center;">0.1471</td>
                <td>0.4020</td>
                <td>8e − 4</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>TEM-avg</strong></td>
                <td style="text-align:center;">0.0818</td>
                <td style="text-align:center;">0.6821</td>
                <td style="text-align:center;">−</td>
                <td style="text-align:center;">0.1235</td>
                <td>0.4019</td>
                <td>−</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>TEM-max</strong></td>
                <td style="text-align:center;">
                <strong>0.0791</strong></td>
                <td style="text-align:center;">
                <strong>0.6828</strong></td>
                <td style="text-align:center;">−</td>
                <td style="text-align:center;">
                <strong>0.1192</strong></td>
                <td><strong>0.4038</strong></td>
                <td>−</td>
              </tr>
            </tbody>
          </table>
        </div>
        <section id="sec-25">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.3.1</span>
              <strong>Overall Comparison</strong></h4>
            </div>
          </header>
          <p>Table&nbsp;<a class="tbl" href="#tab4">4</a> displays
          the performance comparison <em>w.r.t.</em> logloss and
          ndcg@5 on LON-A and NYC-R datasets. We have the following
          observations:</p>
          <ul class="list-no-style">
            <li id="list13" label="•">XGBoost achieves poor
            performance since it treats sparse IDs as ordinary
            features and hardly derives useful cross features based
            on the sparse data. It hence fails to capture the
            collaborative filtering effect. Moreover, it cannot
            generalize to unseen feature dependencies. GBDT+LR
            slightly outperforms XGBoost, verifying the feasibility
            of treating cross features as the input of one
            classifier and revising the weight of each cross
            feature.<br /></li>
            <li id="list14" label="•">The performance of GB-CENT
            indicates that such boosting may be insufficient to
            fully facilitate information propagation between two
            models. Note that to reduce the computational
            complexity, the modified GB-CENT only conducts GBDT
            over all the instances, rather than performing GBDT
            over the supporting instances of each categorical
            feature as suggested in&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0049">49</a>].
            Such modification may contribute to the unsatisfactory
            performance.<br />
            </li>
            <li id="list15" label="•">When performing our
            recommendation tasks, FM and NFM, outperform XGBoost,
            GBDT+LR, and GB-CENT. It is reasonable since they are
            good at modeling the sparse interactions and the
            underlying second-order cross features. NFM benefits
            from the higher-order and nonlinear feature
            correlations by leveraging neural networks, thus leads
            to better performance than FM.<br /></li>
            <li id="list16" label="•">TEM achieves the best
            performance, substantially outperforming NFM
            <em>w.r.t.</em> logloss and obtaining a comparable
            ndcg@5. By integrating the embeddings of cross
            features, TEM can achieve the comparable expressiveness
            to NFM. While NFM treats all feature interactions
            equally, TEM can employ the attention networks on
            identifying the personalized attention of each cross
            feature. We further conduct one-sample t-tests to
            verify that all improvements are statistically
            significant with <em>p</em>-value &lt; 0.05.<br /></li>
          </ul>
        </section>
        <section id="sec-26">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.3.2</span>
              <strong>Effect of Cross Features</strong></h4>
            </div>
          </header>
          <p>To analyze the effect of cross features, we consider
          the variants that remove cross feature modeling, termed
          as FM-c, NFM-c, TEM-avg-c, and TEM-max-c. For FM and NFM,
          one user-item interaction is represented only by the sum
          of the user and item ID embeddings and their attribute
          embeddings, without any interactions among features. For
          TEM, we skip the cross feature extraction and direct feed
          into the raw features. As shown in Figure&nbsp;<a class=
          "fig" href="#fig4">4</a>, we have the following
          findings:</p>
          <ul class="list-no-style">
            <li id="list17" label="•">For all methods, removing
            cross feature modeling hurts the expressiveness
            adversely and degrades the recommendation performance.
            FM-c and NFM-c assume one user/item and her/its
            attributes are linearly independent, which fail to
            encode any interactions between them in the embedding
            space. Taking advantages of the attention network,
            TEM-avg-c and TEM-max-c still model the interactions
            between IDs and attributes, and achieve better
            representation ability than FM-c and NFM-c.<br /></li>
            <li id="list18" label="•">As
            Figures&nbsp;fig:logss-cross-ld
            and&nbsp;fig:logss-cross-nyc demonstrate, TEM
            significantly outperforms FM and NFM by a large margin
            <em>w.r.t.</em> logloss, verifying the substantial
            influence of explicit cross feature modeling. While FM
            and NFM consider all the underlying feature
            correlations, neither of them explicitly presents the
            cross features or identifies the importance of each
            cross feature. This makes them work as a black-box and
            hurts their explainability. Therefore, the improvement
            achieved by TEM again verifies the effectiveness of the
            explicit cross features refined from the tree-based
            component.<br /></li>
            <li id="list19" label="•">Lastly, while exhibiting the
            lowest logloss, TEM achieves only comparable
            performance <em>w.r.t.</em> ndcg@5 to that of NFM, as
            shown in Figures&nbsp;fig:ndcg-cross-ld
            and&nbsp;fig:ndcg-cross-nyc . It indicates the
            unsatisfied generalization ability of TEM, since the
            cross features extracted from GBDT only reflect the
            feature dependencies observed in the dataset and
            consequently TEM cannot generalize to the unseen rules.
            We leave the further exploration of the generalization
            ability of our TEM as the future work.<br /></li>
          </ul>
        </section>
      </section>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Case
            Studies (RQ2)</h3>
          </div>
        </header>
        <p>Apart from being comparable at predictive accuracy, the
        key advantage of TEM over other methods is that its
        learning process is transparent and easily explainable.
        Towards this end, we show examples drawn from TEM-avg on
        LON-A to demonstrate its explainability and
        scrutability.</p>
        <section id="sec-28">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.4.1</span>
              <strong>Explainability</strong></h4>
            </div>
          </header>
          <p>To demonstrate the explainability of TEM, we focus on
          a sampled user, whose profile is {age: <em>35-49</em>,
          gender: <em>female</em>, country: <em>the United
          Kingdom</em>, city: <em>London</em>, expert level:
          <em>4</em>, traveler styles: <em>Art and Architecture
          Lover, Peace and Quite Seeker, Family Vacationer, Urban
          Explorer</em>}; meanwhile, we randomly select five
          attractions, {<em>i</em> <sub>31</sub>:&nbsp;<em>National
          Theatre</em>, <em>i</em> <sub>45</sub>:&nbsp;<em>The View
          form the Shard</em>, <em>i</em>
          <sub>49</sub>:&nbsp;<em>The London Eye</em>, <em>i</em>
          <sub>93</sub>:&nbsp;<em>Camden Street Art Tours</em>,
          <em>i</em> <sub>100</sub>:&nbsp;<em>Royal opera
          House</em>}, from the user's holdout testing set.
          Figure&nbsp;<a class="fig" href="#fig5">5</a> visualizes
          the learning results, where a row represents an
          attraction, and a column represents a cross feature (we
          sample five cross features which are listed in
          Table&nbsp;<a class="fig" href="#fig5">5</a>). The left
          heat map presents her attention scores over the five
          sampled cross features and the right displays the
          contributions of these cross features for the final
          prediction.</p>
          <figure id="fig5">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186066/images/www2018-75-fig5.jpg"
            class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span>
              <span class="figure-title">Visualization of cross
              feature attentions produced by TEM-avg on LON-A. An
              entry of the left and right heat map visualizes the
              attention value <em>w<sub>uil</sub></em> and its
              contribution to the final prediction, <em>i.e.,</em>
              <span class="inline-equation"><span class=
              "tex">$w_{uil}{\mathbf {r}}^{\top }_{2}\mathbf
              {v}_{l}$</span></span> , respectively.</span>
            </div>
          </figure>
          <p></p>
          <div class="table-responsive" id="tab5">
            <div class="table-caption">
              <span class="table-number">Table 5:</span>
              <span class="table-title">Descriptions of the cross
              features in Figure&nbsp;<a class="fig" href=
              "#fig5">5</a>.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">ID</th>
                  <th style="text-align:left;">
                    Description of Cross Features shown in
                    Figure&nbsp;<a class="fig" href="#fig5">5</a>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;"><em>v</em>
                  <sub>1</sub></td>
                  <td>[User Country=UK] &amp; [User Style=Art and
                  Architecture Lover]</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td>&nbsp;&nbsp;&nbsp;⇒ [Item Attribute=Concerts
                  and Shows] &amp; [Item Tag=Imelda Staunton]</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>v<sub>22</sub></strong></td>
                  <td>[User Age=35-49] &amp; [User Country=UK]</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td>&nbsp;&nbsp;&nbsp;⇒ [Item Tag=<strong>Camden
                  Town</strong>] &amp; [Item Rating=4.0]</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>v<sub>130</sub></strong></td>
                  <td>[User Age ≠ 25-34] &amp; [User Gender=Female]
                  &amp; [User Style=Peace and Quiet Seeker]</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td>&nbsp;&nbsp;&nbsp;⇒ [Item Attribute=Sights
                  &amp; Landmarks] &amp; [Item Tag=<strong>Walk
                  Around</strong>]</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>v<sub>148</sub></strong></td>
                  <td>[User Age ≠ 50-64] &amp; [User Country ≠
                  USA]</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td>&nbsp;&nbsp;&nbsp;⇒ [Item Tag=<strong>Top
                  Deck &amp; Canary Wharf</strong>]</td>
                </tr>
                <tr>
                  <td style="text-align:center;"><em>v</em>
                  <sub>336</sub></td>
                  <td>[User Age=35-49] &amp; [User Country=UK]
                  &amp; [User Style=Art and Architecture
                  Lover]</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td>&nbsp;&nbsp;&nbsp;⇒ [Item Tag=Royal Opera
                  House] &amp; [Item Tag=Interval Drinks]</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>We first focus on the left heat map of attention
          scores. Examining the attention scores of a row, we can
          explain the recommendation for the corresponding
          attraction using the top cross features. For example, we
          recommend <em>The View from the Shard</em>
          (<em>i.e.,</em> the second row <em>i</em> <sub>45</sub>)
          for the user mainly because of the dominant cross feature
          <em>v</em> <sub>130</sub>, evidenced by the highest
          attention score of 1 (<em>cf.</em> the entry at the
          second row and the third column). Based on the attention
          scores, we can attribute her preferences on <em>The View
          from the Shard</em> to her special interests in the item
          aspects of <em>Walk Around</em> (from <em>v</em>
          <sub>130</sub>), <em>Top Deck &amp; Canary Wharf</em>
          (from <em>v</em> <sub>22</sub>), and <em>Camden Town</em>
          (from <em>v</em> <sub>148</sub>). To justify the
          rationality of the reasoning, we further check the user's
          visiting history, finding that the three item aspects
          have frequently occurred in her historical items.</p>
          <p>In right heat map of Figure&nbsp;<a class="fig" href=
          "#fig5">5</a>, an entry denotes the contribution of the
          corresponding cross feature (<em>i.e.,</em> <span class=
          "inline-equation"><span class="tex">$y^{\prime
          }_{uil}=w_{uil}{\mathbf {r}}^{\top }_{2}\mathbf
          {v}_{l}$</span></span> ) to the final prediction Jointly
          analyzing the left and right heat maps, we find that the
          attention score <em>w<sub>uil</sub></em> is generally
          consistent with <em>y<sub>uil</sub></em> , which contains
          useful cues about the user's preference. Based on such
          outcome, we can utilize the attention scores of cross
          features to explain a recommendation (<em>e.g.,</em> the
          user prefers <em>i</em> <sub>45</sub> owing to the top
          rules of <em>v</em> <sub>130</sub> and <em>v</em>
          <sub>148</sub> weighted with personalized attention
          scores of 1 and 0.33). This case demonstrates TEM's
          capability of providing more informative explanations
          according to a user's preferred cross features, which we
          believe are better than mere labels or similar user/item
          list.</p>
        </section>
        <section id="sec-29">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.4.2</span>
              <strong>Scrutability</strong></h4>
            </div>
          </header>
          <div class="table-responsive" id="tab6">
            <div class="table-caption">
              <span class="table-number">Table 6:</span>
              <span class="table-title">Scrutable recommendation
              for a sampled user on LON-A, where the first row and
              second row list the original and adjusted recommended
              attractions, respectively.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td colspan="2" style="text-align:center;">
                    Top Ranked Recommendation List on LON-A
                    <hr />
                  </td>
                </tr>
                <tr>
                  <td style="text-align:left;">1. Original</td>
                  <td>1. London Fields Park, 2. Old Compton
                  Street,</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td>3. The Mall, 4. West End, 5. Millennium
                  Bridge</td>
                </tr>
                <tr>
                  <td style="text-align:left;">2. Adjusted</td>
                  <td>1. London Fields Park, 2. Greenwich Foot
                  Tunnel,</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td>3. Covent Garden, 4. Kensington Gardens, 5.
                  West End</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Apart from making the recommendation process
          transparent, our TEM can further allow a user to correct
          the process, so as to refresh the recommendation as she
          desires. This property of adjusting recommendation is
          known as the scrutability&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0019">19</a>,
          <a class="bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0039">39</a>]. As
          for TEM, the attention scores of cross features serve as
          a gateway to exert control on the recommendation process.
          We illustrate it using another sampled user in
          Table&nbsp;<a class="tbl" href="#tab6">6</a>.</p>
          <p>The profile of this user indicates that she enjoys the
          traveler style of <em>Urban Explorer</em> most; moreover,
          most attractions in the historical interactions of her
          are tagged with <em>Sights &amp; Landmarks</em>,
          <em>Points of Interest</em> and <em>Neighborhoods</em>.
          Hence, TEM detects such frequent co-occurred cross
          features and accordingly recommends some attractions like
          <em>Old Compton Street</em> and <em>The Mall</em> to her.
          Assuming that the user attempts to scrutinize TEM and
          would like to visit some attractions tagged with
          <em>Garden</em> that are suitable for the <em>Nature
          Lover</em>. Towards this end, we assign the cross
          features containing <em>[User Style=Nature Lover] &amp;
          [Item Attribute=Garden]</em> with a higher attentive
          weight, and then get the predictions of TEM to refresh
          the recommendations. In the adjusted recommendation list,
          the <em>Greenwich Foot Tunnel</em>, <em>Covent
          Garden</em>, and <em>Kensington Gardens</em> are ranked
          at the top positions. Therefore, based on the
          transparency and simulated scrutability, we believe that
          our TEM is easy-to-interpret, explainable and
          scrutable.</p>
        </section>
      </section>
      <section id="sec-30">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span>
            Hyper-parameter Studies (RQ3)</h3>
          </div>
        </header>
        <p>We empirically study the influences of several factors,
        such as the number of trees and the embedding size, on our
        TEM method.</p>
        <section id="sec-31">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.5.1</span>
              <strong>Impact of Tree Number.</strong></h4>
            </div>
          </header>
          <p>The number of trees in TEM indicates the coverage of
          cross features, reflecting how much useful information is
          derived from the datasets.
          Figure&nbsp;fig:tree-num-logloss presents the performance
          <em>w.r.t.</em> logloss by varying the tree number
          <em>S</em>. We can see the logloss of TEM gradually
          decreases with more trees, whereas the performance is
          generally improved. Using a tree number of 400 and 500
          leads to the best performance on NYC-R and LON-A,
          respectively. When the tree number exceeds the optimal
          settings (<em>e.g., S</em> equals to 500 on NYC-R), the
          logloss increases, which may suffer from overfitting.
          This emphasizes the significance of the tree settings,
          which is consistent with&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0022">22</a>,
          <a class="bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0049">49</a>]</p>
        </section>
        <section id="sec-32">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.5.2</span>
              <strong>Impact of Embedding Size.</strong></h4>
            </div>
          </header>
          <p>The empirical results displayed in
          Figure&nbsp;fig:embedding-logloss indicates the
          substantial influence of embedding size upon TEM.
          Enlarging the embedding size, TEM benefits from more
          powerful representations of the user-item pairs.
          Moreover, TEM-max shows consistent improvement over
          TEM-avg in most cases. We attributed such improvement to
          the nonlinearity achieved by the max pooling operation,
          which can select most informative cross features out, as
          discussed in Section&nbsp;<a class="sec" href=
          "#sec-13">3.1.2</a>. However, the oversized embedding may
          cause overfitting and degrade the performance, which is
          consistent with&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0044">44</a>]</p>
          <figure id="fig6">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186066/images/www2018-75-fig6.jpg"
            class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span>
              <span class="figure-title">Performance comparison of
              logloss <em>w.r.t.</em> the tree number <em>S</em>
              and the embedding size <em>k</em>.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-33">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Related
          Work</h2>
        </div>
      </header>
      <p>We can roughly divide explanation styles into
      similarity-based and content-based categories. The
      similarity-based methods&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>] present explanations as a list of
      most similar users or items. For example, Behnoush <em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>] used Restricted Boltzmann Machines to
      compute the explainability scores of the items in the
      top-<em>K</em> recommendation list. While the
      similarity-based explanation can serve as a generic solution
      for explaining a CF recommender, the drawback is that it
      lacks concrete reasoning.</p>
      <p>Content-based works have considered various side
      information, ranging from item tags&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0038">38</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0040">40</a>], social
      relationships&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0037">37</a>], contextual reviews written by
      users&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0013">13</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0015">15</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0048">48</a>] to
      knowledge graphs&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0047">47</a>].</p>
      <p><strong>Item Tags.</strong>&nbsp;To explain a
      recommendation, the work [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0040">40</a>] considered the matching between the
      relevant tags of an item and the preferred tags of the
      user.</p>
      <p><strong>Social Relations.</strong>&nbsp;Considering the
      user friendships in social networks,&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>] proposed a generative
      model to investigate the effects of social explanations on
      user preferences.</p>
      <p><strong>Contextual Reviews.</strong>&nbsp;Zhang <em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0048">48</a>] developed an explicit factor model,
      which incorporated user sentiments <em>w.r.t.</em> item
      aspects as well as the user-item ratings, to facilitate
      generating aspect-based explanations. Similarly, He <em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] extracted item aspects from user
      reviews and modeled the user-item-aspect relations in a
      hybrid collaborative filtering model. More recently, Ren
      <em>et al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>] involved the viewpoints, a tuple of
      user sentiment and item aspect, and trusted social relations
      in a latent factor model to boost recommendation performance
      and present personalized viewpoints as explanations.</p>
      <p><strong>Knowledge Graphs.</strong>&nbsp;Knowledge graphs
      show great potential on explainable recommendation. Yu <em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0047">47</a>] introduced a meta-path-based factor
      model that paths learned from an information graph can
      enhance the user-item relations and further provide
      explainable reasoning. Recently, Alashkar <em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>] integrated domain knowledge
      represented as logic-rules with the neural recommendation
      method.</p>
      <p>Despite the promising attempts achieved, most previous
      works treat the extracted feature (<em>e.g.,</em> item
      aspect, user sentiment, or relationship) as an individual
      factor in factor models, same as the IDs. As such, little
      attention has been paid to discover the effects of cross
      features (or feature combinations) explicitly.</p>
      <p>In terms of techniques, existing works have also
      considered combining tree-based and embedding-based models,
      among which the most popular method is
      boosting&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0049">49</a>]. These solutions typically perform a
      late fusion on the prediction of two kinds of models. GB-CENT
      proposed in&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0049">49</a>] composes of embedding and tree
      components to achieve the merits of both models.
      Particularly, GB-CENT achieves CF effect by conducting MF
      over categorical features; meanwhile, it employs GBDT on the
      supporting instances of numerical features to capture the
      nonlinear feature interactions. Ling&nbsp;<em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>] shows that boosting neural networks
      with GBDT achieves the best performance in the CTR
      prediction. However, these boosting methods only fuse the
      outputs of different models and may be insufficient to fully
      propagate information between tree-based and embedding-based
      models. Distinct from the previous works, our TEM treats the
      cross features extracted from GBDT as the input of
      embedding-based model, facilitating the information
      propagation between two models. More importantly, the main
      focus of TEM is to provide explanations for a recommendation,
      rather than only for improving the performance.</p>
    </section>
    <section id="sec-34">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion</h2>
        </div>
      </header>
      <p>In this work, we proposed a tree-enhanced embedding method
      (TEM), which seamlessly combines the generalization ability
      of embedding-based models with the explainability of
      tree-based models. Owing to the explicit cross features
      extracted from tree-based part and the easy-to-interpret
      attention network, the whole prediction process of our
      solution is fully transparent and self-explainable.
      Meanwhile, TEM can achieve comparable performance as the
      state-of-the-art recommendation methods.</p>
      <p>In future, we will extend our TEM in three directions.
      First, we attempt to jointly learn the tree-based and
      embedding-based models, rather than separately modelling two
      components. This can facilitate the information propagation
      between two components. Second, we consider other context
      information, such as time, location, and user sentiments, to
      further enrich our explainability. Third, we will explore the
      effectiveness of involving knowledge graphs and logic rules
      into our TEM.</p>
      <p><strong>Acknowledgement</strong> This research is part of
      NExT++ project, supported by the National Research
      Foundation, Prime Minister's Office, Singapore under its
      IRC@Singapore Funding Initiative.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Behnoush Abdollahi and
        Olfa Nasraoui. 2016. Explainable Restricted Boltzmann
        Machines for Collaborative Filtering. (2016).</li>
        <li id="BibPLXBIB0002" label="[2]">Behnoush Abdollahi and
        Olfa Nasraoui. 2017. Using Explainability for Constrained
        Matrix Factorization. In <em><em>RecSys</em></em> .
        79–83.</li>
        <li id="BibPLXBIB0003" label="[3]">Taleb Alashkar, Songyao
        Jiang, Shuyang Wang, and Yun Fu. 2017. Examples-Rules
        Guided Deep Neural Network for Makeup Recommendation. In
        <em><em>AAAI.</em></em> 941–947.</li>
        <li id="BibPLXBIB0004" label="[4]">Dzmitry Bahdanau,
        Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine
        translation by jointly learning to align and translate. In
        <em><em>ICLR.</em></em></li>
        <li id="BibPLXBIB0005" label="[5]">Immanuel Bayer, Xiangnan
        He, Bhargav Kanagal, and Steffen Rendle. 2017. A Generic
        Coordinate Descent Framework for Learning from Implicit
        Feedback. In <em><em>WWW.</em></em> 1341–1350.</li>
        <li id="BibPLXBIB0006" label="[6]">Y. Bengio, A. Courville,
        and P. Vincent. 2013. Representation Learning: A Review and
        New Perspectives. <em><em>IEEE Transactions on Pattern
        Analysis and Machine Intelligence</em></em> 35, 8(2013),
        1798–1828.</li>
        <li id="BibPLXBIB0007" label="[7]">Leo Breiman. 2001.
        Random Forests. <em><em>Machine Learning</em></em> 45, 1
        (2001), 5–32.</li>
        <li id="BibPLXBIB0008" label="[8]">Rose Catherine and
        William&nbsp;W. Cohen. 2016. Personalized Recommendations
        using Knowledge Graphs: A Probabilistic Logic Programming
        Approach. In <em><em>RecSys.</em></em> 325–332.</li>
        <li id="BibPLXBIB0009" label="[9]">Jingyuan Chen, Hanwang
        Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-Seng
        Chua. 2017. Attentive Collaborative Filtering: Multimedia
        Recommendation with Item- and Component-Level Attention. In
        <em><em>SIGIR.</em></em> 335–344.</li>
        <li id="BibPLXBIB0010" label="[10]">Tianqi Chen and Carlos
        Guestrin. 2016. XGBoost: A Scalable Tree Boosting System.
        In <em><em>SIGKDD.</em></em> 785–794.</li>
        <li id="BibPLXBIB0011" label="[11]">Tianqi Chen, Linpeng
        Tang, Qin Liu, Diyi Yang, Saining Xie, Xuezhi Cao, Chunyang
        Wu, Enpeng Yao, Zhengyang Liu, Zhansheng Jiang, <em>et
        al.</em> 2012. Combining factorization model and additive
        forest for collaborative followee recommendation.
        <em><em>KDD CUP</em></em> (2012).</li>
        <li id="BibPLXBIB0012" label="[12]">Heng-Tze Cheng, Levent
        Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi
        Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa
        Ispir, <em>et al.</em> 2016. Wide &amp; deep learning for
        recommender systems. In <em><em>DLRS.</em></em> 7–10.</li>
        <li id="BibPLXBIB0013" label="[13]">Zhiyong Cheng, Ying
        Ding, Lei Zhu, and Mohan Kankanhalli. 2018. Aspect-Aware
        Latent Factor Model: Rating Prediction with Ratings and
        Reviews. In <em><em>WWW.</em></em></li>
        <li id="BibPLXBIB0014" label="[14]">Zhiyong Cheng and
        Jialie Shen. 2016. On Effective Location-Aware Music
        Recommendation. <em><em>TOIS</em></em> 34, 2 (2016),
        13:1–13:32.</li>
        <li id="BibPLXBIB0015" label="[15]">Qiming Diao, Minghui
        Qiu, Chao-Yuan Wu, Alexander&nbsp;J. Smola, Jing Jiang, and
        Chong Wang. 2014. Jointly modeling aspects, ratings and
        sentiments for movie recommendation (JMARS). In
        <em><em>KDD.</em></em> 193–202.</li>
        <li id="BibPLXBIB0016" label="[16]">John&nbsp;C. Duchi,
        Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient
        Methods for Online Learning and Stochastic Optimization.
        <em><em>JMLR</em></em> 12(2011), 2121–2159.</li>
        <li id="BibPLXBIB0017" label="[17]">Fuli Feng, Xiangnan He,
        Yiqun Liu, Liqiang Nie, and Tat-Seng Chua. 2018. Learning
        on Partial-Order Hypergraphs. In
        <em><em>WWW.</em></em></li>
        <li id="BibPLXBIB0018" label="[18]">Jerome&nbsp;H Friedman.
        2001. Greedy function approximation: a gradient boosting
        machine. <em><em>Annals of statistics</em></em> (2001),
        1189–1232.</li>
        <li id="BibPLXBIB0019" label="[19]">Xiangnan He, Tao Chen,
        Min-Yen Kan, and Xiao Chen. 2015. TriRank: Review-aware
        Explainable Recommendation by Modeling Aspects. In
        <em><em>CIKM.</em></em> 1661–1670.</li>
        <li id="BibPLXBIB0020" label="[20]">Xiangnan He and
        Tat-Seng Chua. 2017. Neural Factorization Machines for
        Sparse Predictive Analytics. In <em><em>SIGIR.</em></em>
        355–364.</li>
        <li id="BibPLXBIB0021" label="[21]">Xiangnan He, Lizi Liao,
        Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.
        2017. Neural Collaborative Filtering. In
        <em><em>WWW.</em></em> 173–182.</li>
        <li id="BibPLXBIB0022" label="[22]">Xinran He, Junfeng Pan,
        Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine
        Atallah, Ralf Herbrich, Stuart Bowers, and
        Joaquin&nbsp;Quiñonero Candela. 2014. Practical Lessons
        from Predicting Clicks on Ads at Facebook. In
        <em><em>ADKDD.</em></em> 5:1–5:9.</li>
        <li id="BibPLXBIB0023" label="[23]">Xiangnan He, Hanwang
        Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast Matrix
        Factorization for Online Recommendation with Implicit
        Feedback. In <em><em>SIGIR.</em></em> 549–558.</li>
        <li id="BibPLXBIB0024" label="[24]">Jonathan&nbsp;L
        Herlocker, Joseph&nbsp;A Konstan, and John Riedl. 2000.
        Explaining collaborative filtering recommendations. In
        <em><em>CSCW.</em></em> 241–250.</li>
        <li id="BibPLXBIB0025" label="[25]">K. Hornik, M.
        Stinchcombe, and H. White. 1989. Multilayer Feedforward
        Networks Are Universal Approximators. <em><em>Neural
        Networks</em></em> 2, 5 (1989), 359–366.</li>
        <li id="BibPLXBIB0026" label="[26]">Yehuda Koren and Robert
        Bell. 2015. Advances in collaborative filtering. In
        <em><em>Recommender systems handbook.</em></em>
        77–118.</li>
        <li id="BibPLXBIB0027" label="[27]">Xiaoliang Ling, Weiwei
        Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. 2017.
        Model Ensemble for Click Prediction in Bing Search Ads. In
        <em><em>WWW.</em></em> 689–698.</li>
        <li id="BibPLXBIB0028" label="[28]">Julian&nbsp;J. McAuley
        and Jure Leskovec. 2013. Hidden factors and hidden topics:
        understanding rating dimensions with review text. In
        <em><em>RecSys.</em></em> 165–172.</li>
        <li id="BibPLXBIB0029" label="[29]">Liqiang Nie, Meng Wang,
        Zheng-Jun Zha, and Tat-Seng Chua. 2012. Oracle in Image
        Search: A Content-Based Approach to Performance Prediction.
        <em><em>TOIS</em></em> 30, 2 (2012), 13:1–13:23.</li>
        <li id="BibPLXBIB0030" label="[30]">Liqiang Nie, Shuicheng
        Yan, Meng Wang, Richang Hong, and Tat-Seng Chua. 2012.
        Harvesting visual concepts for image search with complex
        queries. In <em><em>MM.</em></em> 59–68.</li>
        <li id="BibPLXBIB0031" label="[31]">Zhaochun Ren, Shangsong
        Liang, Piji Li, Shuaiqiang Wang, and Maarten de Rijke.
        2017. Social Collaborative Viewpoint Regression with
        Explainable Recommendations. In <em><em>WSDM.</em></em>
        485–494.</li>
        <li id="BibPLXBIB0032" label="[32]">Steffen Rendle. 2010.
        Factorization machines. In <em><em>ICDM.</em></em>
        995–1000.</li>
        <li id="BibPLXBIB0033" label="[33]">Steffen Rendle,
        Christoph Freudenthaler, Zeno Gantner, and Lars
        Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking
        from Implicit Feedback. In <em><em>UAI.</em></em>
        452–461.</li>
        <li id="BibPLXBIB0034" label="[34]">Steffen Rendle,
        Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010.
        Factorizing Personalized Markov Chains for Next-basket
        Recommendation. In <em><em>WWW.</em></em> 811–820.</li>
        <li id="BibPLXBIB0035" label="[35]">Badrul Sarwar, George
        Karypis, Joseph Konstan, and John Riedl. 2001. Item-based
        collaborative filtering recommendation algorithms. In
        <em><em>WWW.</em></em> 285–295.</li>
        <li id="BibPLXBIB0036" label="[36]">Ying Shan, T.&nbsp;Ryan
        Hoens, Jian Jiao, Haijing Wang, Dong Yu, and JC Mao. 2016.
        Deep Crossing: Web-Scale Modeling Without Manually Crafted
        Combinatorial Features. In <em><em>KDD.</em></em>
        255–262.</li>
        <li id="BibPLXBIB0037" label="[37]">Amit Sharma and Dan
        Cosley. 2013. Do social explanations work?: studying and
        modeling the effects of social explanations in recommender
        systems. In <em><em>WWW.</em></em> 1133–1144.</li>
        <li id="BibPLXBIB0038" label="[38]">Nava Tintarev. 2007.
        Explanations of recommendations. In
        <em><em>RecSys.</em></em> 203–206.</li>
        <li id="BibPLXBIB0039" label="[39]">Nava Tintarev and
        Judith Masthoff. 2011. Designing and evaluating
        explanations for recommender systems. <em><em>Recommender
        Systems Handbook</em></em> (2011), 479–510.</li>
        <li id="BibPLXBIB0040" label="[40]">Jesse Vig, Shilad Sen,
        and John Riedl. 2009. Tagsplanations: explaining
        recommendations using tags. In <em><em>IUI.</em></em>
        47–56.</li>
        <li id="BibPLXBIB0041" label="[41]">Meng Wang, Weijie Fu,
        Shijie Hao, Hengchang Liu, and Xindong Wu. 2017. Learning
        on Big Graph: Label Inference and Regularization with
        Anchor Hierarchy. <em><em>TKDE</em></em> 29, 5 (2017),
        1101–1114.</li>
        <li id="BibPLXBIB0042" label="[42]">Meng Wang, Weijie Fu,
        Shijie Hao, Dacheng Tao, and Xindong Wu. 2016. Scalable
        Semi-Supervised Learning by Efficient Anchor Graph
        Regularization. <em><em>TKDE</em></em> 28, 7 (2016),
        1864–1877.</li>
        <li id="BibPLXBIB0043" label="[43]">Suhang Wang, Charu
        Aggarwal, and Huan Liu. 2017. Randomized Feature
        Engineering As a Fast and Accurate Alternative to Kernel
        Methods. In <em><em>SIGKDD.</em></em> 485–494.</li>
        <li id="BibPLXBIB0044" label="[44]">Xiang Wang, Xiangnan
        He, Liqiang Nie, and Tat-Seng Chua. 2017. Item Silk Road:
        Recommending Items from Information Domains to Social
        Users. In <em><em>SIGIR.</em></em> 185–194.</li>
        <li id="BibPLXBIB0045" label="[45]">Xiang Wang, Liqiang
        Nie, Xuemeng Song, Dongxiang Zhang, and Tat-Seng Chua.
        2017. Unifying Virtual and Physical Worlds: Learning Toward
        Local and Global Consistency. <em><em>TOIS.</em></em> 36, 1
        (2017), 4:1–4:26.</li>
        <li id="BibPLXBIB0046" label="[46]">Jun Xiao, Hao Ye,
        Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
        2017. Attentional Factorization Machines: Learning the
        Weight of Feature Interactions via Attention Networks. In
        <em><em>IJCAI.</em></em> 3119–3125.</li>
        <li id="BibPLXBIB0047" label="[47]">Xiao Yu, Xiang Ren,
        Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal,
        Brandon Norick, and Jiawei Han. 2014. Personalized entity
        recommendation: a heterogeneous information network
        approach. In <em><em>WSDM.</em></em> 283–292.</li>
        <li id="BibPLXBIB0048" label="[48]">Yongfeng Zhang, Guokun
        Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma. 2014.
        Explicit factor models for explainable recommendation based
        on phrase-level sentiment analysis. In
        <em><em>SIGIR.</em></em> 83–92.</li>
        <li id="BibPLXBIB0049" label="[49]">Qian Zhao, Yue Shi, and
        Liangjie Hong. 2017. GB-CENT: Gradient Boosted Categorical
        Embedding and Numerical Trees. In <em><em>WWW.</em></em>
        1311–1319.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Xiangnan He is
    the corresponding author.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>Typically, the
    embedding size <em>k</em> is smaller than the number of trees
    <em>S</em> in GBDT.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://www.tripadvisor.com">https://www.tripadvisor.com</a>.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/xiangwang1223/TEM">https://github.com/xiangwang1223/TEM</a>.</p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class=
    "link-inline force-break" href=
    "https://www.tensorflow.org">https://www.tensorflow.org</a>.</p>
    <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class=
    "link-inline force-break" href=
    "https://xgboost.readthedocs.io">https://xgboost.readthedocs.io</a>.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186066">https://doi.org/10.1145/3178876.3186066</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
