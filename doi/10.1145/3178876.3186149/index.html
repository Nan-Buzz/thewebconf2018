<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Bayesian Models for Product Size Recommendations</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Bayesian Models for Product Size Recommendations</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Vivek</span>      <span class="surName">Sembium</span>,     Amazon, India, <a href="mailto:viveksem@amazon.com">viveksem@amazon.com</a>     </div>     <div class="author">     <span class="givenName">Rajeev</span>      <span class="surName">Rastogi</span>,     Amazon, India, <a href="mailto:rastogi@amazon.com">rastogi@amazon.com</a>     </div>     <div class="author">     <span class="givenName">Lavanya</span>      <span class="surName">Tekumalla</span>,     Amazon, India, <a href="mailto:lavanya@amazon.com">lavanya@amazon.com</a>     </div>     <div class="author">     <span class="givenName">Atul</span>      <span class="surName">Saroop</span>,     Amazon, India, <a href="mailto:asaroop@amazon.com">asaroop@amazon.com</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186149" target="_blank">https://doi.org/10.1145/3178876.3186149</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Lack of calibrated product sizing in popular categories such as apparel and shoes leads to customers purchasing incorrect sizes, which in turn results in high return rates due to fit issues. We address the problem of product size recommendations based on customer purchase and return data. We propose a novel approach based on Bayesian logit and probit regression models with ordinal categories {<tt>Small, Fit, Large</tt>} to model size fits as a function of the difference between latent sizes of customers and products. We propose posterior computation based on mean-field variational inference, leveraging the Polya-Gamma augmentation for the logit prior, that results in simple updates, enabling our technique to efficiently handle large datasets. Our Bayesian approach effectively deals with issues arising from noise and sparsity in the data providing robust recommendations. Offline experiments with real-life shoe datasets show that our model outperforms the state-of-the-art in 5 of 6 datasets. and leads to an improvement of 17-26% in AUC over baselines when predicting size fit outcomes.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Collaborative filtering;</strong> <strong>Recommender systems;</strong> &#x2022;<strong> Mathematics of computing </strong>&#x2192; <strong>Bayesian computation;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Bayesian</small>, </span>     <span class="keyword">      <small> Personalization</small>, </span>     <span class="keyword">      <small> Recommendation</small>, </span>     <span class="keyword">      <small> Polya-Gamma</small>, </span>     <span class="keyword">      <small> Probit</small>, </span>     <span class="keyword">      <small> Variational Inference</small>, </span>     <span class="keyword">      <small> Gibbs Sampling</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Vivek Sembium, Rajeev Rastogi, Lavanya Tekumalla, and Atul Saroop. 2018. Bayesian Models for Product Size Recommendations. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186149" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186149</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Recommending product sizes to customers is an important problem in the e-commerce domain. Though e-commerce is becoming increasingly popular, products such as apparel and shoes remain challenging to buy online and record high return rates. A key customer pain point that leads to excessive product returns is the <em>size fit problem</em>. Choosing the right size online is hard due to multiple reasons. Apart from the lack of touch and feel experience, variability in product sizing across brands and product types is a major challenge when shopping online. Essentially, brands and product types follow different sizing conventions, leading to different mappings from catalog sizes to physical sizes. For instance, the catalog size to physical size mappings for Reebok are: 6 = 15cm, 7 = 17cm, 8 = 21cm, while for Nike they are: 6 = 16cm, 7 = 18cm, 8 = 22cm. Further, there is variability in sizing even within the same brand. For instance, the running shoes of Nike have a different sizing scheme compared to walking shoes. Similarly, general running shoes have different sizing standards compared to those targeted towards marathon runners. Finally, incomplete and incorrect size charts provided by several sellers exacerbate this problem, making it difficult for customers to pick the correct size. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186149/images/www2018-158-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Customer selecting a size variant on a shoe web page.</span>     </div>     </figure>    </p>    <p>In the context of recommending product sizes, a customer&#x0027;s interaction with the e-commerce website takes place in the following manner: The customer visits the web page of a product and is provided with the option of choosing a particular size variant from several different sizes as shown in Figure <a class="fig" href="#fig1">1</a>. The <em>size recommendation problem</em> involves recommending the product size that would best fit the customer, thus improving customer experience. While there are several techniques [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] that address the general product recommendation problem, there is very little prior work related to generating size recommendations.</p>    <p>There are two inhibiting factors in generating accurate size recommendations. One, we do not have knowledge of the <em>customer&#x0027;s true size</em>. Two, accurate measurements of <em>product true sizes</em> are not available for all size variants. Nevertheless, valuable information about the true size of a customer lies in the <em>purchase history</em> of all the products previously purchased by the customer. Similarly, information about the true size of a product is available in the set of customers who have purchased it in the past. Another key piece of valuable information is the <em>return history</em>, which indicates if a product in the purchase history fit the customer or was returned due to incorrect choice of size.</p>    <p>However, recommending product sizes from purchase and return history is hard due to multiple reasons. Most e-commerce customers are occasional purchasers. Even the more frequent purchasers are unlikely to buy a large number of items from a specific category such as shoes. Hence, a majority of the customers have very few associated transactions. Similarly popular products tend to record huge sales, while a majority of products have very few or no associated transactions. As a consequence, customer transaction data is <em>highly sparse</em>. Further, return data is <em>highly noisy</em> as customers often do not indicate the right fitment information when returning an item. In fact, we have found a significant disagreement between the customer&#x0027;s input on size fit while returning an item and the customer&#x0027;s response to an independent fit related survey. Data sparsity and noise constitute a major impediment to making accurate size recommendations to the customer.</p>    <p>The inherent data sparsity necessitates a technique that enables learning the customer and product true size in a limited data setting, while the uncertainty arising from noisy purchase and return data necessitates computation of robust confidence estimates over the learned customer and product sizes to make confident recommendations. Motivated by this, we pursue a Bayesian treatment of the problem.</p>    <p>We propose a latent factor model that leverages a wide range of signals such as historical product purchases, returns by customers and product sizing information in the catalog to infer true (latent) sizes for customers and products. We propose Bayesian logit and probit regression models with ordinal categories to model the size fit data, which allows us to capture different <em>fitSuitabilityCodes</em> such as <tt>Small, Fit</tt> or <tt>Large</tt> provided by the customer. We propose efficient mean-field variational inference methods that leverage auxiliary variable techniques such as the Polya-Gamma augmentation to approximate the posterior distribution over customer and product true sizes, and use the posterior distribution to estimate the probability that a product with a particular size variant will fit the customer. Such a Bayesian treatment enables us to handle data sparsity issues by placing priors on customer and product true sizes based on catalog data, at the same time enabling computation of confidence intervals over the posterior distribution to propose confident recommendations despite noisy training data. In experiments with real-life shoe datasets, we show that our Bayesian models leveraging return information provided by customers have 17-26% higher AUC compared to baselines when predicting <em>fitSuitabilityCode</em> outcomes of purchase transactions.</p>    <p>The rest of the paper is structured as follows. In Section <a class="sec" href="#sec-6">2</a>, we review related work, while in Section <a class="sec" href="#sec-7">3</a>, we formally define the size recommendation problem. In Section <a class="sec" href="#sec-8">4</a>, we propose our Bayesian model for size recommendations that learns the latent size for customers and products, with efficient posterior computation based on mean-field variational inference. In Section <a class="sec" href="#sec-15">5</a>, we evaluate our models on real-world Amazon shoe datasets and synthetically generated data, and show a significant improvement over baselines. In Section <a class="sec" href="#sec-19">6</a>, we discuss various extensions to our models and provide concluding remarks in Section <a class="sec" href="#sec-23">7</a>.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>While several techniques have been proposed to solve the general recommendation problem&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>], there is very little work on size recommendation. Predominant amongst the techniques to solve the general recommendation problem, and most related to our work, are latent factor models based on Matrix Factorization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. There have been extensions that model general non-Gaussian distributions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], capture varying user preferences&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>], are based on tensor factorization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] and bilinear models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. In &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>], the authors take Bayesian approach to solve the recommendations problem and &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] leverage user reviews, product metadata and other information. There are also approaches that leverage other contextual information, including physical context such as age, weather and device information&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>]. However, the above techniques for the general recommendation problem are not suitable for size recommendations, as they do not leverage additional ordinal information such as (1) catalog size of products and (2) customer provided <em>fitSuitability</em> outcome for the purchased product.</p>    <p>In this paper, we build Bayesian models that exploit size semantics to predict the ordinal outcome (<tt>Small</tt>, <tt>Fit</tt>, <tt>Large</tt>) based on the difference between latent sizes of the customer and the product involved in a transaction. We note that there has been prior work that explores a Bayesian treatment of the ordinal regression problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. Our model differs from the usual setting of ordinal regression, since the predictor variable (the size difference between the customer and product) is itself latent. Our proposed inference strategies leverage state-of-the-art auxiliary variable techniques such as the Polya-Gamma augmentation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] in a variational inference setting leading to a simple and efficient algorithm for computing the posterior.</p>    <p>The only prior work to the best of our knowledge in the academic literature that also addresses the specific problem of size recommendation is [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>]. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], the authors propose a non-Bayesian approach that formulates the learning problem for customer and product true sizes as one of minimizing a carefully constructed loss function. The loss function minimization is carried out by an iterative algorithm that propagates updates between customer and product sizes through the purchase graph based on customer return instances. However the deterministic approach of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], learns point estimates of product and customer true sizes that may have high errors due to the noisy return codes supplied by customers, exacerbated by extreme data sparsity issues. This in turn can lead to incorrect recommendations. Our Bayesian treatment of this problem enables us to place priors on customer and product sizes and get a posterior on these latent sizes over which we average to obtain more robust fit probability estimates (along with confidence intervals) to make accurate recommendations. We experimentally compare our technique with that of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] in section <a class="sec" href="#sec-15">5</a> and outperform their technique in 5 of 6 datasets.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Size Recommendation Problem</h2>     </div>    </header>    <p>We now formally describe the size recommendation problem. A product is offered in multiple sizes, with the original product referred to as the <em>parent product</em>, while its various size variants are referred as <em>child products</em>. Let <em>C</em> denote the set of all customers and <em>P</em> the set of all child products in the catalog. The data comprises past customer transactions in the form of triplets (customer, child product, <em>fitSuitabilityCode</em>). Formally, let <em>D</em> = (<em>i</em>, <em>j</em>, <em>y<sub>ij</sub>     </em>) denote the transaction triplet data, where child product <em>j</em> was purchased by customer <em>i</em> and <em>y<sub>ij</sub>     </em> is <tt>Fit</tt> if the child product was not returned by the customer, or the <em>fitSuitabilityCode</em>     <tt>Small</tt> or <tt>Large</tt> provided by the customer if it was returned by the customer.</p>    <p>Our objective is to recommend the child product with the best size fit for a specific (customer, parent product) pair. We do this by inferring posterior distribution over the true (latent) sizes of customers and child products using past transactions in <em>D</em>. Let <em>s<sub>i</sub>     </em> be the latent true size of a customer <em>i</em> and <em>t<sub>j</sub>     </em> be the latent true size of child product <em>j</em>. Intuitively, (1) if <span class="inline-equation"><span class="tex">$(i, j, {\tt Fit}) \in D$</span>     </span>, then <em>s<sub>i</sub>     </em> should be as close to <em>t<sub>j</sub>     </em> as possible or |<em>s<sub>i</sub>     </em> &#x2212; <em>t<sub>j</sub>     </em>| &#x2192; 0; (2) if <span class="inline-equation"><span class="tex">$(i, j, {\tt Small}) \in D$</span>     </span>, then (<em>s<sub>i</sub>     </em> &#x2212; <em>t<sub>j</sub>     </em>) should be greater than a threshold <em>b</em>     <sub>1</sub> > 0; and (3) if <span class="inline-equation"><span class="tex">$(i, j, {\tt Large}) \in D$</span>     </span>, then (<em>s<sub>i</sub>     </em> &#x2212; <em>t<sub>j</sub>     </em>) should less than a threshold <em>b</em>     <sub>2</sub> < 0. Once we have the true size estimates, we recommend the child product <em>j</em> with the highest size fit probability to customer <em>i</em>.</p>    <p>We adopt a Bayesian approach that leads to the following formulation for our size recommendation problem:</p>    <p>     <strong>Problem Statement</strong>: Given past customer transactions <em>D</em>, catalog sizes {<em>c<sub>j</sub>     </em>} for child products, and priors on true sizes, compute the posterior distribution on true sizes {<em>s<sub>i</sub>     </em>} for customers and {<em>t<sub>j</sub>     </em>} for child products. For a customer <em>i</em> and child product <em>j</em>, use the computed true size posteriors to estimate the size fit probability <span class="inline-equation"><span class="tex">$P (y_{ij} = {\tt Fit}|D)$</span>     </span>. <span class="inline-equation"><span class="tex">$\Box$</span>     </span>    </p>    <p>To simplify the presentation in the remainder of the paper, we will refer to child products as simply products.</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Bayesian Size Recommendation Model</h2>     </div>    </header>    <p>In this section, we present our probabilistic model, and schemes for learning the posterior distributions of customer true sizes {<em>s<sub>i</sub>     </em>} and product true sizes {<em>t<sub>j</sub>     </em>} and leveraging them to estimate the fit size probability <span class="inline-equation"><span class="tex">$P (y_{ij} = {\tt Fit}|D)$</span>     </span>. We consider customer and product true sizes to be single dimensional and each account to belong to a single person. Generalizations of our techniques to handle multi-dimensional size vectors and multiple personas per customer account can be found in Section&#x00A0;<a class="sec" href="#sec-19">6</a>.</p>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Data Likelihood</h3>     </div>     </header>     <p>Let <em>f<sub>&#x03B1;</sub>     </em>(<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) = <em>&#x03B1;</em> &#x00B7; (<em>s<sub>i</sub>     </em> &#x2212; <em>t<sub>j</sub>     </em>) be a parametric function equal to the difference in true sizes between customer (<em>s<sub>i</sub>     </em>) and product (<em>t<sub>j</sub>     </em>) with a scale parameter <em>&#x03B1;</em>. Based on the earlier discussion in Section <a class="sec" href="#sec-7">3</a>, we want the data likelihood <em>P</em>(<em>y<sub>ij</sub>     </em>|<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) for <em>fitSuitabilityCode y<sub>ij</sub>     </em> to satisfy: (1) if <em>f<sub>&#x03B1;</sub>     </em>(<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) &#x2265; <em>b</em>     <sub>1</sub> then <span class="inline-equation"><span class="tex">$P(y_{ij}={\tt Small}| s_i, t_j)$</span>     </span> is large; (2) if <em>f<sub>&#x03B1;</sub>     </em>(<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) &#x2264; <em>b</em>     <sub>2</sub> then <span class="inline-equation"><span class="tex">$P(y_{ij}={\tt Large}| s_i, t_j)$</span>     </span> is large; and (3) if <em>b</em>     <sub>2</sub> < <em>f<sub>&#x03B1;</sub>     </em>(<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) < <em>b</em>     <sub>1</sub> then <span class="inline-equation"><span class="tex">$P(y_{ij}={\tt Fit}| s_i, t_j)$</span>     </span> is large. Below, we define a data likelihood function with the desired properties.</p>     <p>Let latent size vector <em>&#x03B2;</em> = (<em>s</em>     <sub>1</sub>, ..., <em>s<sub>c</sub>     </em>, <em>t</em>     <sub>1</sub>, .., <em>t<sub>p</sub>     </em>, <em>b</em>     <sub>1</sub>, <em>b</em>     <sub>2</sub>)<sup>      <em>T</em>     </sup>, and for a transaction (<em>i</em>, <em>j</em>, .) &#x2208; <em>D</em>, define data vectors <em>x<sub>ijs</sub>     </em> = (0, .., 0, <em>&#x03B1;</em>, 0, .., 0, &#x2212;<em>&#x03B1;</em>, 0, .., 0, &#x2212;1, 0)<sup>      <em>T</em>     </sup> and <em>x<sub>ijf</sub>     </em> = (0, .., 0, <em>&#x03B1;</em>, 0, .., 0, &#x2212;<em>&#x03B1;</em>, 0, .., 0, 0, &#x2212;1)<sup>      <em>T</em>     </sup>, where <em>&#x03B1;</em> and &#x2212; <em>&#x03B1;</em> are present in the positions of <em>s<sub>i</sub>     </em> and <em>t<sub>j</sub>     </em>, respectively. Notice that learning the posterior of {<em>s<sub>i</sub>     </em>} and {<em>t<sub>j</sub>     </em>} is equivalent to learning the posterior of <em>&#x03B2;</em>. Also, <em>&#x03B2;<sup>T</sup>     </em> &#x00B7; <em>x<sub>ijs</sub>     </em> = <em>f<sub>&#x03B1;</sub>     </em>(<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) &#x2212; <em>b</em>     <sub>1</sub> and <em>&#x03B2;<sup>T</sup>     </em> &#x00B7; <em>x<sub>ijf</sub>     </em> = <em>f<sub>&#x03B1;</sub>     </em>(<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>) &#x2212; <em>b</em>     <sub>2</sub>.</p>     <p>To define the likelihood function for the three classes corresponding to the <em>fitSuitabilityCodes</em>, we introduce two binary random variables <em>y<sub>ijs</sub>     </em> &#x2208; {0, 1} and <em>y<sub>ijf</sub>     </em> &#x2208; {0, 1} with the following probability distributions: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{gather} P(y_{ijs}|s_{i}, t_{j}, \alpha , b_1) = \sigma (y_{ijs},\beta ^T\cdot x_{ijs})\\ \nonumber P(y_{ijf}|s_{i}, t_{j}, \alpha , b_2) = \sigma (y_{ijf},\beta ^T\cdot x_{ijf})\end{gather} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div>     <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \text{where, ~~~} \sigma (y, \theta) = \frac{e^{y\theta }}{1+e^{\theta }} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>     </p>     <p>We note that an alternate choice for the link function <em>&#x03C3;</em> is the Probit, where <em>P</em>(<em>y<sub>ijs</sub>     </em> = 1|<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>, <em>&#x03B1;</em>, <em>b</em>     <sub>1</sub>) = <em>&#x03A6;</em>(<em>&#x03B2;<sup>T</sup>     </em> &#x00B7; <em>x<sub>ijs</sub>     </em>) and <em>P</em>(<em>y<sub>ijf</sub>     </em> = 1|<em>s<sub>i</sub>     </em>, <em>t<sub>j</sub>     </em>, <em>&#x03B1;</em>, <em>b</em>     <sub>1</sub>) = <em>&#x03A6;</em>(<em>&#x03B2;<sup>T</sup>     </em> &#x00B7; <em>x<sub>ijf</sub>     </em>), where <em>&#x03A6;</em> is the CDF of the normal distribution. The probit approach is explored in detail in the Appendix.</p>     <p>We now define the likelihoods of <tt>Small</tt>, <tt>Fit</tt> and <tt>Large</tt> using these random variables as follows: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P(y_{ij} = {\tt Small}|s_{i}, t_{j}, \alpha , b_1) = P(y_{ijs}=1|\beta ^T\cdot x_{ijs}) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>     <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P(y_{ij} = {\tt Fit} &#x0026; |s_{i}, t_{j}, \alpha , b_1, b_2) \\ &#x0026; = P(y_{ijs}=0|\beta ^T\cdot x_{ijs})\cdot P(y_{ijf}=1|\beta ^T\cdot x_{ijf}) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>     <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P(y_{ij} = {\tt Large} &#x0026; |s_{i}, t_{j}, \alpha , b_1, b_2) \\ &#x0026; = P(y_{ijs}=0|\beta ^T\cdot x_{ijs})\cdot P(y_{ijf}=0|\beta ^T\cdot x_{ijf}) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>     </p>     <p>Notice that,</p>     <ol class="list-no-style">     <li id="list1" label="(1)">If <em>s<sub>i</sub>      </em> > > <em>t<sub>j</sub>      </em>, then <em>f<sub>&#x03B1;</sub>      </em>(<em>s<sub>i</sub>      </em>, <em>t<sub>j</sub>      </em>) &#x2212; <em>b</em>      <sub>1</sub> > > 0. Then, <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Small}|.) \rightarrow 1$</span>      </span>.<br/></li>     <li id="list2" label="(2)">If <em>s<sub>i</sub>      </em> < < <em>t<sub>j</sub>      </em>, then <em>f<sub>&#x03B1;</sub>      </em>(<em>s<sub>i</sub>      </em>, <em>t<sub>j</sub>      </em>) &#x2212; <em>b</em>      <sub>1</sub> < < 0 and <em>f<sub>&#x03B1;</sub>      </em>(<em>s<sub>i</sub>      </em>, <em>t<sub>j</sub>      </em>) &#x2212; <em>b</em>      <sub>2</sub> < < 0. Then, <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Large}|.) \rightarrow 1$</span>      </span>.<br/></li>     <li id="list3" label="(3)">If <em>b</em>      <sub>2</sub> < <em>f<sub>&#x03B1;</sub>      </em>(<em>s<sub>i</sub>      </em>, <em>t<sub>j</sub>      </em>) < <em>b</em>      <sub>1</sub>. Then, <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Fit}|.) \rightarrow 1$</span>      </span>.<br/></li>     <li id="list4" label="(4)"><span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Small}|.) + P(y_{ij} = {\tt Fit}|.) + P(y_{ij} = {\tt Large}|.) = 1$</span>      </span>.<br/></li>     </ol>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Generative Model</h3>     </div>     </header>     <p>We define our generative model as follows:</p>     <ol class="list-no-style">     <li id="list5" label="(1)">For each customer <em>i</em>, draw true size <span class="inline-equation"><span class="tex">$s_i \sim \mathcal {N}(\mu _i, \sigma _s^{2})$</span>      </span>.<br/></li>     <li id="list6" label="(2)">For each product <em>j</em>, draw true size <span class="inline-equation"><span class="tex">$t_j \sim \mathcal {N}(c_j, \sigma _t^{2})$</span>      </span>.<br/></li>     <li id="list7" label="(3)">Draw model parameter <span class="inline-equation"><span class="tex">$b_1 \sim \mathcal {N}(\mu _{b_1}, \sigma _{b_1}^2)$</span>      </span> and model parameter <span class="inline-equation"><span class="tex">$b_2 \sim \mathcal {N}(\mu _{b_2}, \sigma _{b_2}^2)$</span>      </span>.<br/></li>     <li id="list8" label="(4)">For each transaction (<em>i</em>, <em>j</em>, <em>y<sub>ij</sub>      </em>) &#x2208; <em>D</em>:<br/>      <ul class="list-no-style">       <li id="list9" label="&#x2022;">Select <span class="inline-equation"><span class="tex">$y_{ij}={\tt Small}$</span>        </span> with probability <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Small}|s_{i}, t_{j}, \alpha , b_1)$</span>        </span> defined in Equation (<a class="eqn" href="#eq3">3</a>).<br/></li>       <li id="list10" label="&#x2022;">Select <span class="inline-equation"><span class="tex">$y_{ij}={\tt Fit}$</span>        </span> with probability <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Fit}|s_{i}, t_{j}, \alpha , b_1, b_2)$</span>        </span> defined in Equation (<a class="eqn" href="#eq4">4</a>).<br/></li>       <li id="list11" label="&#x2022;">Select <span class="inline-equation"><span class="tex">$y_{ij}={\tt Large}$</span>        </span> with probability <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Large}|s_{i}, t_{j}, \alpha , b_1, b_2)$</span>        </span> defined in Equation (<a class="eqn" href="#eq5">5</a>).<br/></li>      </ul></li>     </ol>     <p>We place Gaussian priors on customer and product true sizes. The product mean size is initialized with the catalog size <em>c<sub>j</sub>     </em> and the customer mean size <em>&#x03BC;<sub>i</sub>     </em> is initialized with the mean of all purchased product sizes that fit the customer <em>i</em>. We introduce Gaussian priors for <span class="inline-equation"><span class="tex">$b_1 \sim \mathcal {N}(\mu _{b_1}, \sigma _{b_1}^2)$</span>     </span> and <span class="inline-equation"><span class="tex">$b_2 \sim \mathcal {N}(\mu _{b_2}, \sigma _{b_2}^2)$</span>     </span> to control the movement of the model parameters.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Posterior Inference for Model Training</h3>     </div>     </header>     <p>The goal of model training is to learn the posterior distribution of latent sizes <em>P</em>(<em>&#x03B2;</em>|<em>D</em>). In this section, we restrict our discussion to inference with the Logit link function defined in Equation (<a class="eqn" href="#eq2">2</a>), while we explore variational inference with Probit in the Appendix. To simplify notation, we represent the collective data and labels of all the logistic functions in the likelihood terms in Equations (<a class="eqn" href="#eq3">3</a>), (<a class="eqn" href="#eq4">4</a>) and (<a class="eqn" href="#eq5">5</a>) by <strong>X</strong> and <strong>Y</strong>, respectively. We denote by <strong>D</strong> the set of all examples (<em>x<sub>i</sub>     </em>, <em>y<sub>i</sub>     </em>) corresponding to transactions in <em>D</em>. Thus, <strong>D</strong> contains the example (<em>x<sub>ijs</sub>     </em>, 1) for the transaction <span class="inline-equation"><span class="tex">$(i, j, {\tt Small})\in D$</span>     </span>, the examples (<em>x<sub>ijs</sub>     </em>, 0) and (<em>x<sub>ijf</sub>     </em>, 1) for the transaction <span class="inline-equation"><span class="tex">$(i, j, {\tt Fit})\in D$</span>     </span>, and the examples (<em>x<sub>ijs</sub>     </em>, 0) and (<em>x<sub>ijf</sub>     </em>, 0) for the transaction <span class="inline-equation"><span class="tex">$(i, j, {\tt Large})\in D$</span>     </span>. The joint data distribution can be expressed as: <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P(\beta ,{\bf D}|\Sigma)= &#x0026; \Bigg (\prod _{(x,y) \in {\bf D}}\sigma (y, \beta ^{T}\cdot x)\cdot \prod _{i}\mathcal {N}(s_i|\mu _i, \sigma _s^2)\cdot \\ &#x0026; \hspace{20.0pt}\prod _{j}\mathcal {N}(t_j|c_j,\sigma _t^2) \cdot \prod _{b \in \lbrace b_1,b_2\rbrace }\mathcal {N}(b|\mu _b,\sigma _b^2) \Bigg) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where, <span class="inline-equation"><span class="tex">$\Sigma =\lbrace \lbrace \mu _{i}\rbrace ,\lbrace c_j\rbrace ,\sigma _s,\sigma _t, \mu _{b_1}, \mu _{b_2}, \sigma _{b_1}, \sigma _{b_2}\rbrace$</span>     </span> and <em>&#x03C3;</em>(<em>y</em>, <em>&#x03B8;</em>) is the sigmoid function defined in Equation (<a class="eqn" href="#eq2">2</a>). The posterior distribution of <em>&#x03B2;</em> is given by: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(\beta |{\bf D},\Sigma) = \frac{P(\beta ,{\bf D}|\Sigma)}{\int _{\beta }P(\beta ,{\bf D}|\Sigma)d\beta }\propto P(\beta ,{\bf D}|\Sigma)\end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>     </p>     <p>Note that the posterior distribution in Equation&#x00A0;(<a class="eqn" href="#eq7">7</a>) above cannot be computed in closed form due to the presence of logistic likelihood terms and normal priors. In order to make the posterior tractable, we introduce a Polya-Gamma latent variable <em>w</em> for every (<em>x</em>, <em>y</em>) &#x2208; <strong>D</strong> that generates a logistic term in the likelihood as suggested in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>].</p>     <p>We first define the Polya-Gamma random variable: <em>X</em> &#x223C; <em>PG</em>(<em>b</em>, <em>a</em>) with <em>b</em> > 0 and <em>a</em> &#x2208; <em>R</em>, if <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} X\overset{D}{=}\frac{1}{2\pi ^{2}}\sum _{k=1}^{\infty }\frac{g_{k}}{(k-1/2)^2+a^2/(4\pi ^{2})}\end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\overset{D}{=}$</span>     </span> indicates equality in distribution and <em>g<sub>k</sub>     </em> &#x223C; <em>Ga</em>(<em>b</em>, 1) are independent gamma random variables.</p>     <p>Let <em>w</em> &#x223C; <em>PG</em>(1, 0). We define the joint likelihood distribution <span class="inline-equation"><span class="tex">$P(w, y|x, \beta) = \frac{1}{2}e^{((y-1/2)\cdot (\beta ^{T}\cdot x)-w\cdot (\beta ^{T}\cdot x)^{2}/2)}P(w)$</span>     </span>. It can be shown (see Theorem 1 in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]) that the expectation of <em>w</em> on the joint likelihood distribution is logistic, that is, <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} P(y|\beta , X) &#x0026; = \frac{e^{y\beta ^T\cdot x}}{1+e^{\beta ^{T}\cdot x}} \\ &#x0026; = \int _{0}^{\infty } \frac{1}{2}e^{((y-1/2)\cdot (\beta ^{T}\cdot x)-w(\beta ^{T}\cdot x)^{2}/2)} P(w)dw \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>     </p>     <p>Let <strong>W</strong> be the set of Polya-Gamma variables <em>w</em> for (<em>x</em>, <em>y</em>) &#x2208; <strong>D</strong>. Then, we have that &#x222B;<em>P</em>(<strong>W</strong>, <em>&#x03B2;</em>|<strong>D</strong>, <em>&#x03A3;</em>)<em>d</em>     <strong>W</strong>&#x221D;&#x222B;<em>P</em>(<strong>W</strong>, <em>&#x03B2;</em>, <strong>D</strong>|<em>&#x03A3;</em>)<em>d</em>     <strong>W</strong>&#x221D;<em>P</em>(<em>&#x03B2;</em>, <strong>D</strong>|<em>&#x03A3;</em>)&#x221D;<em>P</em>(<em>&#x03B2;</em>|<strong>D</strong>, <em>&#x03A3;</em>). In the following subsections, we present Gibbs sampling and variational inference procedures to approximate the augmented posterior <em>P</em>(<strong>W</strong>, <em>&#x03B2;</em>|<strong>D</strong>, <em>&#x03A3;</em>) &#x2013; these give us approximations of <em>&#x03B2;</em>&#x2019;s posterior.</p>     <section id="sec-12">     <p><em>4.3.1 Gibbs Sampling-Based Scheme.</em> Our Gibbs sampling procedure cycles through the <em>w<sub>i</sub>      </em> and <em>&#x03B2;<sub>j</sub>      </em> variables, drawing samples for each one from its distribution conditioned on values of the remaining variables. The conditional distribution of <em>w<sub>i</sub>      </em> is given by (see [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0025">25</a>] for details): <div class="table-responsive" id="eq10">       <div class="display-equation">        <span class="tex mytex">\begin{align} P(w_i|\beta , x_i) &#x0026; \propto e^{-w_i(\beta ^T\cdot x_i)^2/2}P(w_i)\nonumber \\ &#x0026; = PG(w_i|1, \beta ^T\cdot x_i) \end{align} </span>        <br/>        <span class="equation-number">(10)</span>       </div>      </div>     </p>     <p>The conditional distribution of <em>&#x03B2;<sub>j</sub>      </em> is given by <div class="table-responsive" id="eq11">       <div class="display-equation">        <span class="tex mytex">\begin{align} P(\beta _j &#x0026; |\beta _{-j}, {\bf W}, {\bf D}) \nonumber \\ &#x0026;= \frac{P({\bf W}, {\bf D}|\beta) P(\beta)}{\int _{\beta _j} P({\bf W}, {\bf D}|\beta) P(\beta)} d\beta _j \nonumber \\ &#x0026; \propto \prod _{(x_i, y_i)\in {\bf D}}\prod _{x_{ij}\ne 0}\Big (e^{((y_i-1/2)(\beta ^{T}\cdot x_i)-w_i\cdot (\beta ^{T}\cdot x_i)^{2}/2)}\cdot \nonumber \\ &#x0026; \hspace{80.0pt}\mathcal {N}(\beta _j|\mu _{\beta _j}, \sigma _{\beta _j}^2) \Big) \nonumber \\ &#x0026; = \mathcal {N}(\beta _j|m_j, V_j) \end{align} </span>        <br/>        <span class="equation-number">(11)</span>       </div>      </div>     </p>     <p>where, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} \frac{1}{V_j} = \frac{1}{\sigma _{\beta _j}^2} + \sum _{(x_i, y_i)\in {\bf D}} \sum _{x_{ij}\ne 0}w_i\cdot x_{ij}^2\nonumber\end{equation*} </span>        <br/>       </div>      </div> and <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} m_j = V_j\Bigg (\frac{\mu _{\beta _j}}{\sigma _{\beta _j}^2} + \sum _{(x_i, y_i)\in {\bf D}} \sum _{x_{ij}\ne 0} \Big ((y_i-1/2)\cdot x_{ij} - w_i\cdot x_{ij} \cdot \sum _{l\ne j} \beta _l\cdot x_{il}\Big)\Bigg)\nonumber\end{equation*} </span>        <br/>       </div>      </div>     </p>     </section>     <section id="sec-13">     <p><em>4.3.2 Variational Inference-Based Scheme.</em> We apply mean-field variational inference to make the training process efficient. We approximate the posterior distribution using a proposal distribution <em>q</em>(<strong>W</strong>, <em>&#x03B2;</em>) that factorizes as follows: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} q({\bf W},\beta) = \prod _{i}q(w_i).\prod _{j}q(\beta _j)\end{equation*} </span>        <br/>       </div>      </div> The best <em>q</em>      <sup>*</sup> distribution is one that minimizes the Kullback-Leibler divergence <em>KL</em>(<em>q</em>(<strong>W</strong>, <em>&#x03B2;</em>)||<em>P</em>(<strong>W</strong>, <em>&#x03B2;</em>|<strong>D</strong>)). For <em>q</em>(<em>w<sub>i</sub>      </em>), the KL divergence is minimum when <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \log q^{* }(w_i)=E_{-w_i}[\log P(w_i | \beta , x_i)] + const \] </span>        <br/>       </div>      </div>     </p>     <p>Substituting <em>P</em>(<em>w<sub>i</sub>      </em>|<em>&#x03B2;</em>, <em>x<sub>i</sub>      </em>) = <em>PG</em>(<em>w<sub>i</sub>      </em>|1, <em>&#x03B2;<sup>T</sup>x<sub>i</sub>      </em>) from Equation (<a class="eqn" href="#eq10">10</a>), in the above equation, we get: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray*} \begin{split} \log {q^{* }(w_i)} &#x0026; =E_{\beta }[\log P(w_i|\beta , x_i)] \\ &#x0026; = E_{\beta }[-w_i(\beta ^T \cdot x_{i})^{2}/2 + \log {P(w_i)}] + const \nonumber \\ &#x0026; = -w_i\cdot \Big [\Big (\sum _{j}E[\beta _j] \cdot x_{ij}\Big)^2 + \sum _{j}Var[\beta _j] \cdot x_{ij}^2\Big ]/2 \\ &#x0026; \hspace{30.0pt} + \log {P(w_i)} + const \nonumber \end{split}\end{eqnarray*} </span>        <br/>       </div>      </div>     </p>     <p>From the above expression, it can be seen that <em>q</em>      <sup>*</sup>(<em>w<sub>i</sub>      </em>) is <em>PG</em>(1, <em>a<sub>i</sub>      </em>), where <div class="table-responsive" id="eq12">       <div class="display-equation">        <span class="tex mytex">\begin{equation} a_i=\sqrt {\Big (\sum _{j}E[\beta _j] \cdot x_{ij}\Big)^2 + \sum _{j}Var[\beta _j] \cdot x_{ij}^2} \end{equation} </span>        <br/>        <span class="equation-number">(12)</span>       </div>      </div>     </p>     <p>Similarly, for <em>&#x03B2;<sub>j</sub>      </em>, the KL divergence is minimum when <div class="table-responsive" id="eq13">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \log q^{* }(\beta _{j}) \propto E_{-\beta _{j}}[\log {P(\beta _j|\beta _{-j}, {\bf W}, {\bf D})}] + const \end{equation} </span>        <br/>        <span class="equation-number">(13)</span>       </div>      </div>     </p>     <p>Substituting for <em>P</em>(<em>&#x03B2;<sub>j</sub>      </em>|<em>&#x03B2;</em>      <sub>&#x2212; <em>j</em>      </sub>, <strong>W</strong>, <strong>D</strong>) from Equation&#x00A0;(<a class="eqn" href="#eq11">11</a>), Equation&#x00A0;(<a class="eqn" href="#eq13">13</a>) simplifies to the following: <div class="table-responsive" id="eq14">       <div class="display-equation">        <span class="tex mytex">\begin{equation*} q^{* }(\beta _j) = \mathcal {N}(\hat{m_{j}}, \hat{V_{j}})\end{equation*} </span>        <br/>        <span class="equation-number">(4.3.2)</span>       </div>      </div>     </p>     <p>where, <div class="table-responsive" id="eq15">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \frac{1}{\hat{V_{j}}} = \Bigg {(}\frac{1}{\sigma _{\beta _{j}}^2}+\sum _{(x_i, y_i) \in {\bf D} \wedge x_{ij} \ne 0}E[w_i] \cdot x_{ij}^{2}\Bigg {)} \end{equation} </span>        <br/>        <span class="equation-number">(14)</span>       </div>      </div>      <div class="table-responsive" id="eq16">       <div class="display-equation">        <span class="tex mytex">\begin{eqnarray} \begin{split} \hat{m_{j}} = \hat{V_{j}}\Bigg {(}\frac{\mu _{\beta _{j}}}{\sigma _{\beta _{j}}^2} + &#x0026; \sum _{(x_i, y_i) \in {\bf D} \wedge x_{ij} \ne 0} \Big ((y_i-1/2)\cdot x_{ij} \\ &#x0026; \hspace{40.0pt}-E[w_{i}]\cdot x_{ij}\cdot \Big {(}\sum _{l \ne j}E[\beta _l]\cdot x_{il}\Big {)}\Big)\Bigg {)} \end{split} \end{eqnarray} </span>        <br/>        <span class="equation-number">(15)</span>       </div>      </div> For <em>q</em>      <sup>*</sup>(<em>w<sub>i</sub>      </em>) defined in Equation&#x00A0;(<a class="eqn" href="#eq12">12</a>), <span class="inline-equation"><span class="tex">$E[w_i] = \frac{1}{2a_i}\tanh (a_i/2)$</span>      </span>, and for <em>q</em>      <sup>*</sup>(<em>&#x03B2;<sub>j</sub>      </em>) defined in Equation&#x00A0;(<a class="eqn" href="#eq14">4.3.2</a>), <span class="inline-equation"><span class="tex">$E[\beta _j] = \hat{m_j}$</span>      </span> and <span class="inline-equation"><span class="tex">$Var[\beta _j] = \hat{V_j}$</span>      </span>. Starting with random initial values, we iteratively update <em>E</em>[<em>w<sub>i</sub>      </em>], <em>E</em>[<em>&#x03B2;<sub>j</sub>      </em>] and <em>Var</em>[<em>&#x03B2;<sub>j</sub>      </em>] until convergence. These are used to compute parameter values <em>a<sub>i</sub>      </em>, <span class="inline-equation"><span class="tex">$\hat{V_j}$</span>      </span> and <span class="inline-equation"><span class="tex">$\hat{m_j}$</span>      </span> as shown in Equations&#x00A0;(<a class="eqn" href="#eq12">12</a>), (<a class="eqn" href="#eq15">14</a>) and (<a class="eqn" href="#eq16">15</a>).</p>     <p>      <em>A Note on the computational complexity</em>: Equation (<a class="eqn" href="#eq12">12</a>) involves a constant time operation per transaction, owing to the sparse nature of data vectors <em>x<sub>ij</sub>      </em> leading to <em>O</em>(|<strong>D</strong>|) complexity for computing the updates for <em>w</em> variables. Computing the updates for all <em>&#x03B2;<sub>j</sub>      </em> variables leads to <em>O</em>(|<strong>D</strong>|) complexity since Equation (<a class="eqn" href="#eq15">14</a>) and (<a class="eqn" href="#eq16">15</a>) involve each transaction contributing twice while updating <em>&#x03B2;<sub>j</sub>      </em> variables, once for the corresponding customer and once for the product. Hence, our inference technique is efficient and scales linearly <em>O</em>(|<strong>D</strong>|) with the data.</p>     </section>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Recommending Product Sizes</h3>     </div>     </header>     <p>Given a customer <em>i</em>, we recommend the product <em>j</em> whose predictive distribution for size fit probability is maximum. Let <span class="inline-equation"><span class="tex">$\beta ^1=(s_1^1,\ldots , s_c^1, t_1^1,\ldots , t_p^1, b_1^1, b_2^1), \ldots \beta ^r =(s_1^r,\ldots , s_c^r, t_1^r,\ldots , t_p^r, b_1^r, b_2^r)$</span>     </span> be the <em>r</em> samples drawn from the posterior of <em>&#x03B2;</em> using the Gibbs sampling or variational inference procedures described in Sections&#x00A0;<a class="sec" href="#sec-12">4.3.1</a> and <a class="sec" href="#sec-13">4.3.2</a>, respectively. The predictive distribution for fit size probability is obtained by marginalizing with respect to the posterior distribution of <em>&#x03B2;</em>. <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray*} \begin{split} P(y_{ij} = {\tt Fit}|{\bf D}, \Sigma) &#x0026; = \int _{\beta } P(y_{ij} = {\tt Fit}|\beta , \Sigma)\cdot P(\beta |{\bf D}, \Sigma) d\beta \\ &#x0026; = \int _{\beta } \Big (P(y_{ijs} = 0| \beta ^T\cdot x_{ijs})\cdot P(y_{ijf} = 1| \beta ^T\cdot x_{ijf})\cdot \\ &#x0026;\hspace{30.0pt} P(\beta |{\bf D}, \Sigma) \Big) d\beta \nonumber \end{split}\end{eqnarray*} </span>       <br/>      </div>     </div>     <div class="table-responsive" id="eq17">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \hspace{20.0pt} \approx \frac{1}{r} \sum _{l=1}^r \frac{1}{1+e^{{\beta ^l}^T\cdot x_{ijs}}}\cdot \frac{e^{{\beta ^l}^T\cdot x_{ijf}}}{1+e^{{\beta ^l}^T\cdot x_{ijf}}} \end{equation} </span>       <br/>       <span class="equation-number">(16)</span>      </div>     </div>     </p>     <p>Thus, the fit probability score <span class="inline-equation"><span class="tex">$P(y_{ij} = {\tt Fit}|{\bf D}, \Sigma)$</span>     </span> is obtained by simply averaging the fit probability values for the <em>r</em> samples. We can also use the samples <em>&#x03B2;</em>     <sup>1</sup>, &#x2026;<em>&#x03B2;<sup>r</sup>     </em> to compute confidence intervals for the fit probability score. Let <em>p<sub>l</sub>     </em> = <em>&#x03C3;</em>(0, <em>&#x03B2;<sup>l</sup>     </em>     <sup>      <em>T</em>     </sup> &#x00B7; <em>x<sub>ijs</sub>     </em>) &#x00B7; <em>&#x03C3;</em>(1, <em>&#x03B2;<sup>l</sup>     </em>     <sup>      <em>T</em>     </sup> &#x00B7; <em>x<sub>ijf</sub>     </em>) be the fit probability score obtained from the <em>l<sup>th</sup>     </em> sample. Then, we can take different percentile values of the set of probability values <em>p</em>     <sub>1</sub>, &#x2026;, <em>p<sub>r</sub>     </em> to obtain the lower and upper boundaries of the confidence interval for the fit probability score. For instance, the 5<sup>      <em>th</em>     </sup> and 95<sup>      <em>th</em>     </sup> percentile values will give us the 90% confidence interval for the fit probability score. Note that for customers and products in a small number of transactions, the size posterior distributions will have high variance. Thus, the confidence intervals for such customers and products will be large. In contrast, customers and products involved in many transactions will have size posterior distributions with low variance and narrow confidence intervals.</p>     <p>Once we have the fit probability score and the confidence interval for the probability score for each product (child of parent product), one option is to recommend the product with the maximum fit probability score. Another alternative is to factor in the uncertainty in fit probability score estimates, and recommend the product with the highest lower bound of fit probability score confidence intervals.</p>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>     </div>    </header>    <p>In this section, we evaluate our Bayesian models for size recommendation on real-world Amazon shoes data and synthetically generated datasets. We first describe our experimental setup detailing our evaluation metrics and baselines in Section <a class="sec" href="#sec-16">5.1</a>, followed by experiments with Amazon datasets in Section <a class="sec" href="#sec-17">5.2</a> and synthetic datasets in Section <a class="sec" href="#sec-18">5.3</a>.</p>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Experimental Setup</h3>     </div>     </header>     <p>We use offline shoes datasets containing customer transactions for evaluating model performance. Such an offline setup is restrictive, since we do not observe (1) customer&#x0027;s response when our recommendation is shown (2) the true latent size of the customer. The only available data in an offline setting is the transaction carried out by the customer and the resulting <em>fitSuitabilityCode</em> of the transaction. Hence, we evaluate the performance of different size recommendation schemes by the ability of the classifier, constructed based on the learnt latent customer and product sizes from our model, to predict the <em>fitSuitabilityCode</em> outcome on unseen transactions. Note that a similar setup is also used in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] for carrying out model performance benchmarking.</p>     <p>We carry out a time-based split of available customer transactions into training and test set. We compare the results of our Bayesian models on held out test transactions against a baseline model with (1) catalog size as a latent size of products and (2) customer&#x0027;s latent size is the mean of all catalog sizes purchased by the customer with a <em>fitSuitabilityCode</em> of <tt>Fit</tt>. The baseline algorithm also learns customer and product true sizes on the training set. The learnt customer and product latent sizes are used to train a Logistic Regression (Linear) and Random Forest (RF) classifiers to produce <em>fitSuitabilityCode</em> as output. Our Bayesian models are compared against the performance metric of the baseline Linear and RF classifiers on the test set. For the Bayesian logit (&#x2018;Logit&#x2019;) and probit (&#x2018;Probit&#x2019;) schemes, we compute the probability scores on the test set using Equation (<a class="eqn" href="#eq17">16</a>), and then predict the <em>fitSuitabilityCode</em> outcome for a transaction as the one with the highest probability. We also compare our technique against the state-of-the-art technique of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] that learns customer and product sizes through coordinate descent on a loss function constructed from customer purchase and return history.</p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Experiments with Shoes Data</h3>     </div>     </header>     <p>     <strong>Data Analysis</strong>      <em>:</em>     </p>     <p>Our dataset comprises customer transactions: <em>customerId</em>, <em>productId</em> and <em>fitSuitabilityCode</em>      <span class="inline-equation"><span class="tex">$\in \lbrace {\tt Small}, {\tt Fit}, {\tt Large}\rbrace$</span>     </span>. Each transaction is further categorized based on product category and assigned a label A-F due to proprietary reasons. Table <a class="tbl" href="#tab1">1</a> represents the transaction counts for all product categories. For each product category, we partition the transactions in temporal order with first 80% in the training set and the remaining 20% in the test set.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Summary of transaction count for all datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>A</strong>        </th>        <th style="text-align:center;">        <strong>B</strong>        </th>        <th style="text-align:center;">        <strong>C</strong>        </th>        <th style="text-align:center;">        <strong>D</strong>        </th>        <th style="text-align:center;">        <strong>E</strong>        </th>        <th style="text-align:center;">        <strong>F</strong>        </th>       </tr> </thead> <tbody>       <tr>        <td style="text-align:center;">10.4M</td>        <td style="text-align:center;">17M</td>        <td style="text-align:center;">33.2M</td>        <td style="text-align:center;">25M</td>        <td style="text-align:center;">12.9M</td>        <td style="text-align:center;">27M</td>       </tr>      </tbody>     </table>     </div>     <p>The catalog size of products in our datasets represents the US sizing scheme with sizes varying between 4 and 20 at 0.5 intervals. The histogram of products and transactions for different catalog sizes is represented in Figures <a class="fig" href="#fig2">2</a> and <a class="fig" href="#fig3">3</a> respectively. The actual count of products and transactions are not reported to safeguard proprietary information. Figure <a class="fig" href="#fig2">2</a> and <a class="fig" href="#fig3">3</a> indicates that customers prefer buying full catalog sizes over half catalog sizes, perhaps due to lack of availability of half-sizes in the catalog for many products. Also, note that except for category &#x201C;C&#x201D;, majority of purchased products have catalog sizes between 7 and 10, while for category &#x201C;C&#x201D; majority of purchased products have catalog sizes between 8 and 12. Note that there are very few purchases for products with catalog sizes greater than 14, with almost negligible count of transactions for catalog sizes more than 17.</p>     <p>Figures <a class="fig" href="#fig4">4</a> and <a class="fig" href="#fig5">5</a> indicates the histogram of products and customers, respectively with different transactions counts. Once again, the actual numbers in the figures are not reported to safeguard propriety information. Note that very few products and customers have more than 3 transactions in the training data, indicating a sparse purchase graph. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186149/images/www2018-158-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Product count across catalog size.</span>      </div>     </figure>     <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186149/images/www2018-158-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Transaction count across catalog size.</span>      </div>     </figure>     <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186149/images/www2018-158-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Histogram of products with transaction count.</span>      </div>     </figure>     <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186149/images/www2018-158-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Histogram of customers with transaction count.</span>      </div>     </figure>     </p>     <p>     <strong>Experimental Results</strong>     <em>:</em> The results of our offline experiments with various datasets are shown in Table <a class="tbl" href="#tab2">2</a> (&#x2018;Logit&#x2019; refers to the Variational Bayesian model with logit link function and &#x2018;Probit&#x2019; refers to the Variational Bayesian model with probit link function). For proprietary reasons, we do not report the absolute weighted AUC numbers for different models, but rather report the percentage improvements in weighted AUC scores over and above the Baseline Linear model. We also compare our results with those obtained using work carried out by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. Observe that our Bayesian models show a relative improvement of 17-26% in weighted AUC over baseline model. Our models also outperform models built by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] on 5 out of the 6 datasets we experimented with, as reported in Table <a class="tbl" href="#tab2">2</a>. This can be attributed to our Bayesian models following a principled probabilistic approach that allows them to (1) overcome data sparsity by placing priors on latent sizes, (2) capture uncertainty due to noise in the data in the inferred latent size posteriors, and (3) propagate the uncertainty into the final fit probability scores by averaging over the posterior. Note that the logit and probit models perform similarly (except for &#x201C;B&#x201D; category), because the logit and probit link functions are very similar.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Summary of offline experimental results. We report percentage improvements in weighted AUC over and above the Baseline Linear model for the Logit, Probit, the model built by [30] and a baseline random forest model. (Best results shown first.)</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>Dataset</strong>        </th>        <th style="text-align:center;">        <strong>RF</strong>        </th>        <th style="text-align:center;">[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0030">30</a>]</th>        <th style="text-align:center;">        <strong>Logit</strong>        </th>        <th style="text-align:center;">        <strong>Probit</strong>        </th>       </tr> </thead> <tbody>       <tr>        <td style="text-align:center;">D</td>        <td style="text-align:center;">2.71%</td>        <td style="text-align:center;">20.16 %</td>        <td style="text-align:center;">25.78%</td>        <td style="text-align:center;">        <strong>26.16</strong>%</td>       </tr>       <tr>        <td style="text-align:center;">E</td>        <td style="text-align:center;">1.48%</td>        <td style="text-align:center;">15.58 %</td>        <td style="text-align:center;">        <strong>20.22</strong>%</td>        <td style="text-align:center;">20.04%</td>       </tr>       <tr>        <td style="text-align:center;">F</td>        <td style="text-align:center;">2.50%</td>        <td style="text-align:center;">17.31 %</td>        <td style="text-align:center;">19.42%</td>        <td style="text-align:center;">        <strong>20.00</strong>%</td>       </tr>       <tr>        <td style="text-align:center;">C</td>        <td style="text-align:center;">3.79%</td>        <td style="text-align:center;">16.10 %</td>        <td style="text-align:center;">        <strong>19.70</strong>%</td>        <td style="text-align:center;">19.32%</td>       </tr>       <tr>        <td style="text-align:center;">A</td>        <td style="text-align:center;">0.55%</td>        <td style="text-align:center;">17.16%</td>        <td style="text-align:center;">17.71%</td>        <td style="text-align:center;">        <strong>17.90</strong>%</td>       </tr>       <tr>        <td style="text-align:center;">B</td>        <td style="text-align:center;">2.05%</td>        <td style="text-align:center;">        <strong>21.27</strong> %</td>        <td style="text-align:center;">18.28%</td>        <td style="text-align:center;">20.52%</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Experiments with Synthetic Shoes Data</h3>     </div>     </header>     <p>In the real-world setup described in Section&#x00A0;<a class="sec" href="#sec-17">5.2</a>, due to problems associated with observability of latent variables like true sizes and variability in data, we are only able to make indirect inferences on the power of our algorithms in estimating the true size distribution. Hence, in this section, we carry out experiments to measure the ability of our inference procedures to recover true latent sizes for customers and products on simulated datasets.</p>     <p>     <strong>Data Generation Process</strong>     <em>:</em> We consider catalog sizes from 4 to 20 in increments of 0.5. We follow the generative process described in Section <a class="sec" href="#sec-10">4.2</a> to generate the true size of product <em>j</em> as <span class="inline-equation"><span class="tex">$t_j \sim \mathcal {N}(c_j, \hat{\sigma }_t^2)$</span>     </span> (where <em>c<sub>j</sub>     </em> is a randomly chosen catalog size) and the true size <span class="inline-equation"><span class="tex">$s_i \sim \mathcal {N}(10, 3)$</span>     </span> of customer <em>i</em> is drawn from a normal truncated between 4 and 20. We generate sizes for 10,000 child products and 10,000 customers. To generate a transaction, we pick a customer at random and randomly pick any child product having the catalog size closest to the customer true size. We generate thresholds <em>b</em>     <sub>1</sub> and <em>b</em>     <sub>2</sub> from a normal prior and categorize a transaction as {<tt>Fit, Small, Large</tt>} based on the generative process described in Section <a class="sec" href="#sec-10">4.2</a>.</p>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Summary of RMSE on synthetic data.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>Product Variance</strong>        </th>        <th style="text-align:center;">        <strong>Fit %</strong>        </th>        <th style="text-align:center;">        <strong>Cust RMSE</strong>        </th>        <th style="text-align:center;">        <strong>Cust RMSE</strong>        </th>        <th style="text-align:center;">        <strong>Product RMSE</strong>        </th>        <th style="text-align:center;">        <strong>Product RMSE</strong>        </th>        <th style="text-align:center;">        <strong>Weighted AUC</strong>        </th>        <th style="text-align:center;">        <strong>Weighted AUC</strong>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">Logit</th>        <th style="text-align:center;">Probit</th>        <th style="text-align:center;">Logit</th>        <th style="text-align:center;">Probit</th>        <th style="text-align:center;">Logit</th>        <th style="text-align:center;">Probit</th>       </tr> </thead> <tbody>       <tr>        <td style="text-align:center;">0.5</td>        <td style="text-align:center;">94.27</td>        <td style="text-align:center;">0.275</td>        <td style="text-align:center;">        <strong>0.227</strong>        </td>        <td style="text-align:center;">        <strong>0.412</strong>        </td>        <td style="text-align:center;">0.413</td>        <td style="text-align:center;">        <strong>0.901</strong>        </td>        <td style="text-align:center;">0.886</td>       </tr>       <tr>        <td style="text-align:center;">0.7</td>        <td style="text-align:center;">83.61</td>        <td style="text-align:center;">0.324</td>        <td style="text-align:center;">        <strong>0.305</strong>        </td>        <td style="text-align:center;">        <strong>0.454</strong>        </td>        <td style="text-align:center;">0.471</td>        <td style="text-align:center;">        <strong>0.932</strong>        </td>        <td style="text-align:center;">0.928</td>       </tr>       <tr>        <td style="text-align:center;">0.9</td>        <td style="text-align:center;">72.59</td>        <td style="text-align:center;">        <strong>0.318</strong>        </td>        <td style="text-align:center;">0.330</td>        <td style="text-align:center;">        <strong>0.473</strong>        </td>        <td style="text-align:center;">0.502</td>        <td style="text-align:center;">        <strong>0.956</strong>        </td>        <td style="text-align:center;">0.951</td>       </tr>       <tr>        <td style="text-align:center;">1.1</td>        <td style="text-align:center;">63.83</td>        <td style="text-align:center;">0.305</td>        <td style="text-align:center;">        <strong>0.266</strong>        </td>        <td style="text-align:center;">        <strong>0.507</strong>        </td>        <td style="text-align:center;">0.564</td>        <td style="text-align:center;">        <strong>0.959</strong>        </td>        <td style="text-align:center;">0.951</td>       </tr>       <tr>        <td style="text-align:center;">1.3</td>        <td style="text-align:center;">55.90</td>        <td style="text-align:center;">0.294</td>        <td style="text-align:center;">        <strong>0.269</strong>        </td>        <td style="text-align:center;">        <strong>0.582</strong>        </td>        <td style="text-align:center;">0.684</td>        <td style="text-align:center;">        <strong>0.968</strong>        </td>        <td style="text-align:center;">0.961</td>       </tr>       <tr>        <td style="text-align:center;">1.5</td>        <td style="text-align:center;">49.29</td>        <td style="text-align:center;">0.286</td>        <td style="text-align:center;">        <strong>0.269</strong>        </td>        <td style="text-align:center;">        <strong>0.708</strong>        </td>        <td style="text-align:center;">0.841</td>        <td style="text-align:center;">        <strong>0.972</strong>        </td>        <td style="text-align:center;">0.965</td>       </tr>      </tbody>     </table>     </div>     <p>     <strong>Experimental Results</strong>     <em>:</em> The RMSE between estimated and simulated latent true sizes for customers and products are shown in Table <a class="tbl" href="#tab3">3</a> along with the AUC on size fit prediction for different values of <span class="inline-equation"><span class="tex">$\sigma _t^2$</span>     </span> introduced when simulating product true sizes. We observe that (1) Both the &#x2018;Logit&#x2019; and &#x2018;Probit&#x2019; models perform very well on the size fit prediction with AUC, between 0.886 and 0.972 (2) As can be expected, our methods are able to infer the latent product and customer true sizes more accurately for smaller magnitudes of variance <span class="inline-equation"><span class="tex">$\sigma _t^2$</span>     </span> introduced when simulating product true sizes. We also note that for higher values of variance <span class="inline-equation"><span class="tex">$\sigma _t^2$</span>     </span>, the RMSE of product sizes to true sizes is much lower than the introduced variance in generating product sizes. Such a behavior can be attributed to a larger percentage of <tt>small</tt> and <tt>large</tt> transactions being available in the simulated data, leading to reduced class imbalance. A similar trend is also observed in terms of the weighted-AUC metric. (3) Like the results obtained on real-world datasets of Section&#x00A0;<a class="sec" href="#sec-17">5.2</a>, the &#x2018;Logit&#x2019; and &#x2018;Probit&#x2019; models perform similarly to each other, with differences being evident only in the third decimal place for the weighted-AUC metric.</p>    </section>   </section>   <section id="sec-19">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Extensions</h2>     </div>    </header>    <p>We describe extensions to our models to (1) address cold start scenarios using customer and product features in Section <a class="sec" href="#sec-20">6.1</a>, (2) handling purchases for multiple personas in an account in Section <a class="sec" href="#sec-21">6.2</a>, and (3) leverage multi-dimensional sizes like length and width in Section <a class="sec" href="#sec-22">6.3</a>.</p>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Leveraging Customer and Product Features</h3>     </div>     </header>     <p>We can leverage additional customer information such as gender, age and subscription to loyalty programs and product information such as category, brand and product type in addition to catalog size. Leveraging such features can handle cold start scenarios and combat data sparsity thus improving accuracy of our model.</p>     <p>Let the feature vectors of customer <em>i</em> and product <em>j</em> be <em>f<sub>i</sub>     </em> and <em>g<sub>j</sub>     </em>, respectively. We can incorporate the features in our model as follows:</p>     <ol class="list-no-style">     <li id="list12" label="(1)">Include the feature vectors <em>f<sub>i</sub>      </em> and <em>g<sub>j</sub>      </em> in the data vectors <em>x<sub>ijs</sub>      </em> and <em>x<sub>ijf</sub>      </em>, and the corresponding weight vectors <em>w<sub>f</sub>      </em> and <em>w<sub>g</sub>      </em> in the latent size vector <em>&#x03B2;</em>.<br/></li>     <li id="list13" label="(2)">For each customer <em>i</em>, draw true size <span class="inline-equation"><span class="tex">$s_i\sim \mathcal {N}(w_{f}^{\prime }\cdot f_i, \sigma _s^2)$</span>      </span>. For each customer <em>j</em>, draw true size <span class="inline-equation"><span class="tex">$t_j\sim \mathcal {N}(w_g^{\prime }\cdot g_j, \sigma _t^2)$</span>      </span>. Thus, the means of customer and product true sizes are obtained by performing regression over their features.<br/></li>     </ol>     <p>The weight values for <span class="inline-equation"><span class="tex">$w_f, w_g, w_f^{\prime }, w_g^{\prime }$</span>     </span> can be computed using a Monte-Carlo EM algorithm as described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>].</p>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Handling Personas</h3>     </div>     </header>     <p>In practice, a customer account may be shared by multiple individuals with different sizes, for example, different family members including children and adults. Thus, learning a single true size per customer account may lead to inaccurate true size estimates. We can model multiple personas per customer account by introducing new latent variables <em>z<sub>ij</sub>     </em> for each transaction (<em>i</em>, <em>j</em>, <em>y<sub>ij</sub>     </em>) &#x2208; <em>D</em>. The latent variables <em>z<sub>ij</sub>     </em> capture the persona of customer <em>i</em> that purchases product <em>j</em>. We associate a separate latent size <em>s<sub>ik</sub>     </em> with the <em>k</em>     <sup>th</sup> persona of customer <em>i</em>, and the sizes for all customer personas are included in the latent size vector <em>&#x03B2;</em>. Furthermore, the data vectors <em>x<sub>ijs</sub>     </em> and <em>x<sub>ijf</sub>     </em> have values <em>&#x03B1;</em> and &#x2212; <em>&#x03B1;</em> present in the positions of <span class="inline-equation"><span class="tex">$s_{iz_{ij}}$</span>     </span> and <em>t<sub>j</sub>     </em>, respectively. Our generative model (described in Section 4.2) is extended as follows. First, we draw a persona distribution <em>&#x03B8;<sub>i</sub>     </em> for each customer <em>i</em> from a Dirichlet distribution. For each transaction (<em>i</em>, <em>j</em>, <em>y<sub>ij</sub>     </em>), we first draw the persona <em>z<sub>ij</sub>     </em> from multinomial (<em>Mult</em>(<em>&#x03B8;<sub>i</sub>     </em>)) and then draw the transaction outcome with probabilities given by the likelihood functions defined in Equations (3)-(5). Our Gibbs sampling and variational inference procedures can be easily extended to approximate the posterior distributions of latent variables <em>&#x03B2;</em> and {<em>z<sub>ij</sub>     </em>}.</p>    </section>    <section id="sec-22">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Multi-Dimensional Sizes</h3>     </div>     </header>     <p>We can extend our techniques to infer multi-dimensional true size vectors for customers and products. The various dimensions capture different aspects related to size such as length and width for shoes, or waist and length for jeans. Let <em>d</em> be the number of size dimensions. Then the customer true sizes <em>s<sub>i</sub>     </em> and product true sizes <em>t<sub>j</sub>     </em> (in latent size vector <em>&#x03B2;</em>) as well as the scale parameter <em>&#x03B1;</em> (in data vectors <em>x<sub>ijs</sub>     </em> and <em>x<sub>ijf</sub>     </em>) are <em>d</em>-dimensional vectors.</p>    </section>   </section>   <section id="sec-23">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusions</h2>     </div>    </header>    <p>In this paper, we proposed novel Bayesian logit and probit regression models with ordinal categories to model size fits, and efficient algorithms for posterior inference based on mean-field variational inference and Polya-Gamma augmentation. In experiments with real-life shoe datasets, our Bayesian models delivered 17-26% higher AUCs compared to baselines when predicting size fit outcomes of customer purchase transactions. Our models also outperform state-of-the-art baselines in 5 of 6 real-world datasets. On simulated datasets, we show that our models are able to reduce significant proportion of introduced size variances. In terms of a choice between using either a logit or a probit link function for our Bayesion model, we find that the two models perform very similarly to each other on both real-world and simulated datasets. They also have similar inference times during the model training process (&#x223C; 1 minute per iteration), thereby not leaving us with an explicit winner between the two.</p>   </section>   <section id="sec-24">    <header>     <div class="title-info">     <h2>Acknowledgement</h2>     </div>    </header>    <p>We would like to thank Dr. Matthias Seeger for his valuable comments on an earlier version of the manuscript.</p>   </section>  </section>  <section class="back-matter">   <Appendix>    <section id="sec-25">     <header>     <div class="title-info">      <h2>       <span class="section-number">A</span> Appendix: Modeling with Probit</h2>     </div>     </header>     <p>In this section, we discuss inference for our model with the Probit link function. Consider data {<em>x<sub>i</sub>     </em>, <em>y<sub>i</sub>     </em>}, where {<em>y<sub>i</sub>     </em> &#x2208; {0, 1}} are the observed category labels and {<em>x<sub>i</sub>     </em> &#x2208; <em>R<sup>M</sup>     </em>} the features. Let <em>&#x03B2;</em> &#x2208; <em>R<sup>M</sup>     </em>. The binary probit is defined as follows where <em>&#x03A6;</em> is the C<strong>D</strong>F of the normal distribution with mean 0 and variance <span class="inline-equation"><span class="tex">$\sigma _p^2$</span>     </span>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ p(y_i=1 | x_i) = \Phi (x_i^T \beta ; \sigma _p^2) \] </span>       <br/>      </div>     </div>     </p>     <p>Since the CDF of the normal is not a tractable function[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>], an augmentation strategy was introduced by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] for bayesian inference where the probit is defined in terms of the following linear model in terms of an auxilliary random variable <em>z<sub>i</sub>     </em>: <div class="table-responsive" id="eq18">      <div class="display-equation">       <span class="tex mytex">\begin{align} z_i &#x0026; = \beta ^t x_i + \epsilon , ~~ \epsilon \in {\mathcal {N}}(0,\sigma _p^2) \\ \nonumber y_i &#x0026;= 1 ~~~\text{if}~~ z_i {\gt} 0 \\\nonumber &#x0026;= 0 ~~~\text{otherwise}\end{align} </span>       <br/>       <span class="equation-number">(17)</span>      </div>     </div>     </p>     <section id="sec-26">     <header>      <div class="title-info">       <h3>        <span class="section-number">A.1</span> Variational Inference with Probit for Size Recommendation</h3>      </div>     </header>     <p>We perform approximate inference using variational inference[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>] with the mean field approximation. Let <em>&#x03B2;</em> = {{<em>s<sub>i</sub>      </em>}, {<em>t<sub>j</sub>      </em>}, <em>b</em>      <sub>1</sub>, <em>b</em>      <sub>2</sub>}, the set of all latent variables in the model. Consider a set <em>Z</em> containing the auxiliary variables for all the probits defined in <strong>D</strong>, that is defined in Section <a class="sec" href="#sec-11">4.3</a>. The joint log likelihood <em>p</em>(<em>&#x03B2;</em>, <em>Z</em>, <strong>D</strong>) can be expanded as follows: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} \log ~ p(\beta , Z ,{\bf D}) =&#x0026; \sum _{\beta _j} \log ~ p(\beta _j) \nonumber \\ +&#x0026; \underset{\lbrace x_i,y_i\rbrace \in {\bf D}, y_i=0}{\sum } \log ~ ({\mathcal {N}}(z_i,\beta ^T x_i,\sigma _p^2) \cdot \delta (z_i{\gt}0)) \nonumber \\ +&#x0026;\underset{\lbrace x_i,y_i \rbrace \in {\bf D}, y_i=1}{\sum } \log ~ ({\mathcal {N}}(z_i,\beta ^T x_i,\sigma _p^2) \cdot \delta (z_i{\lt}0)) \nonumber\end{align*} </span>        <br/>       </div>      </div>     </p>     <p>We note that since we are working with the joint likelihood <em>p</em>(<em>&#x03B2;</em>, <em>Y</em>), we deal with terms containing <em>p</em>(<em>z<sub>i</sub>      </em>, <em>y<sub>i</sub>      </em>|<em>&#x03B2;</em>) that has a simpler form than the truncated Gaussian <em>p</em>(<em>z<sub>i</sub>      </em>|<em>y<sub>i</sub>      </em>, <em>&#x03B2;</em>) which has a not so tractable normalization constant. With this, we now derive the variational updates for the latent variables in our model. &#x00A0; <span style="text-decoration: underline;">Update for {<em>z<sub>i</sub>       </em>}</span>: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} \log ~ q^*(z_i) &#x0026; = E_{-z^i} [ \log ~ p(\beta , Z, {\bf D}) ] + const \nonumber \\ &#x0026; = E_{- z_i} [ \log ~ p(z_i, y_i | \beta) ]+ const \nonumber \\ &#x0026; = E_{- z_i} [ \log ~ (\delta (y_i, z_i{\gt}0) ~ \cdot {\mathcal {N}}(z_i; \beta ^T x_i, \sigma _p^2)) ] + const \nonumber \\ &#x0026;= E_{- z_i} [ \log ~ (\delta (y_i,1) \cdot {\mathcal {N}}^{+}(z_i; \beta ^T x_i, \sigma _p^2) \nonumber \\ &#x0026; \hspace{40.0pt}+ \delta (y_i,0) \cdot {\mathcal {N}}^-(z_i; \beta ^T x_i, \sigma _p^2)) ] + const \nonumber\end{align*} </span>        <br/>       </div>      </div>     </p>     <p>This takes the form of a truncated Gaussian. Hence, we obtain <em>q</em>      <sup>*</sup>(<em>z<sub>i</sub>      </em>) as a truncated Gaussian and <em>E</em>      <sup>*</sup>[<em>z<sub>i</sub>      </em>] the expectation of a truncated Gaussian[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>] : <div class="table-responsive" id="eq19">       <div class="display-equation">        <span class="tex mytex">\begin{align} q^*(z_i) &#x0026;= {\mathcal {N}}^-(z_i, E^*[\beta ]^T x_i, \sigma _p^2)^{ \delta (y_i, 0)} \cdot {\mathcal {N}}^+(z_i, E^*[\beta ]^T x_i, \sigma _p^2)^{\delta (y_i,1)} \nonumber \\E^*[z_i] &#x0026;= \delta (y_i,0) \cdot \Big (E^*[\beta ]^T x_i -\sigma _p \frac{\phi _i}{\Phi _i}\Big) \nonumber \\ &#x0026;\hspace{20.0pt}+\delta (y_i,1) \cdot \Big (E^*[\beta ]^T x_i + \sigma _p \frac{\phi _i}{1-\Phi _i}\Big) \end{align} </span>        <br/>        <span class="equation-number">(18)</span>       </div>      </div>     </p>     <p>Where <em>E</em>      <sup>*</sup>[<em>&#x03B2;</em>] is computed in later steps from <em>q</em>      <sup>*</sup>(<em>&#x03B2;</em>). Note that <em>&#x03D5;<sub>i</sub>      </em> and <em>&#x03A6;<sub>i</sub>      </em> are the pdf and the CDF of the standard normal evaluated at <span class="inline-equation"><span class="tex">$\frac{E^*[\beta ]^T x_i}{\sigma _p}$</span>      </span>.</p>     <p>      <span style="text-decoration: underline;">Update for {<em>&#x03B2;<sub>j</sub>       </em>}</span>: <div class="table-responsive" id="eq20">       <div class="display-equation">        <span class="tex mytex">\begin{align} \log ~ q^*(\beta _j) &#x0026; = E_{- \beta _j} [ \log ~ p(\beta , Z, {\bf D}) ] \\ \nonumber &#x0026;\end{align} </span>        <br/>        <span class="equation-number">(19)</span>       </div>      </div> Consolidating all the terms with <em>&#x03B2;<sub>j</sub>      </em> from Equation 18: <div class="table-responsive" id="eq21">       <div class="display-equation">        <span class="tex mytex">\begin{align} \log ~ q^*(\beta _j) &#x0026;= E_{-\beta _j} [ \log ~ p(\lbrace \beta _j\rbrace)+\log ~ p(\lbrace Z,{\bf D} | \beta) ] + const \nonumber \\ &#x0026;= E_{-\beta _j} [ \log ~ p(\beta _j; \mu _{\beta _j}, \sigma _{\beta _j}^2) \prod _{\lbrace x_i,y_i\rbrace \in {\bf D}} p(y_i) p(z_i | \beta , y_i) ] \nonumber \\ &#x0026;= - \frac{(\beta _j - \mu _{\beta _j})^2}{2 \sigma ^2_{\beta _j}} - \frac{1}{2} \underset{\lbrace x_i,y_i\rbrace \in {\bf D}}{\sum } E_{-\beta _j} [(z_i-\sum _{k \ne j} \beta _k^T x_{ik})^2 ] \nonumber \\ &#x0026;= {\mathcal {N}}(\frac{b_{\beta _j}}{a_{\beta _j}}, \frac{1}{a_{\beta _j}}) \end{align} </span>        <br/>        <span class="equation-number">(20)</span>       </div>      </div>      <div class="table-responsive" id="eq22">       <div class="display-equation">        <span class="tex mytex">\begin{align} a_{\beta _j} = &#x0026; \frac{1}{\sigma _{\beta _j}^2}+|{\bf D}| \nonumber \\b_{\beta _j} = &#x0026; \frac{\mu _{\beta _j}}{\sigma _{\beta _j}^2} + \sum _{ \lbrace x_i,y_i\rbrace \in {\bf D}} \Big (E^*[z_i]-\sum _{k \ne j} E^*[\beta _k] ~ x_{ik}\Big) \end{align} </span>        <br/>        <span class="equation-number">(21)</span>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ E^*[\beta _j]=\frac{\frac{\mu _{\beta _j}}{\sigma _{\beta _j}^2} + \sum _{\lbrace x_i,y_i\rbrace \in {\bf D}} (E^*[z_i]-\sum _{k \ne j} E^*[\beta _k]~ x_{ik})}{\frac{1}{\sigma _{s_i}^2}+|{\bf D}|} \] </span>        <br/>       </div>      </div> where |<strong>D</strong>| is the cardinality of set <strong>D</strong>.</p>     </section>    </section>   </Appendix>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Gediminas Adomavicius and Alexander Tuzhilin. 2011. Context-aware recommender systems. In <em>      <em>Recommender systems handbook</em>     </em>. Springer, 217&#x2013;253.</li>     <li id="BibPLXBIB0002" label="[2]">Deepak Agarwal and Bee-Chung Chen. 2009. Regression-based Latent Factor Models. In <em>      <em>Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>(KDD &#x2019;09). ACM, New York, NY, USA, 19&#x2013;28. <a class="link-inline force-break" href="https://doi.org/10.1145/1557019.1557029"      target="_blank">https://doi.org/10.1145/1557019.1557029</a></li>     <li id="BibPLXBIB0003" label="[3]">Charu&#x00A0;C Aggarwal. 2016. <em>      <em>Recommender systems</em>     </em>. Springer.</li>     <li id="BibPLXBIB0004" label="[4]">James&#x00A0;H Albert and Siddhartha Chib. 1993. Bayesian analysis of binary and polychotomous response data. <em>      <em>Journal of the American statistical Association</em>     </em>88, 422(1993), 669&#x2013;679.</li>     <li id="BibPLXBIB0005" label="[5]">Josef Bauer and Alexandros Nanopoulos. 2014. A Framework for Matrix Factorization Based on General Distributions. In <em>      <em>Proceedings of the 8th ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;14). ACM, New York, NY, USA, 249&#x2013;256. <a class="link-inline force-break" href="https://doi.org/10.1145/2645710.2645735"      target="_blank">https://doi.org/10.1145/2645710.2645735</a></li>     <li id="BibPLXBIB0006" label="[6]">Christopher&#x00A0;M. Bishop. 2006. <em>      <em>Pattern Recognition and Machine Learning</em>     </em>. Springer. http://research.microsoft.com/en-us/um/people/cmbishop/prml/</li>     <li id="BibPLXBIB0007" label="[7]">Laurent Charlin, Rajesh Ranganath, James McInerney, and David&#x00A0;M. Blei. 2015. Dynamic Poisson Factorization. In <em>      <em>Proceedings of the 9th ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;15). ACM, New York, NY, USA, 155&#x2013;162. <a class="link-inline force-break" href="https://doi.org/10.1145/2792838.2800174"      target="_blank">https://doi.org/10.1145/2792838.2800174</a></li>     <li id="BibPLXBIB0008" label="[8]">Ming-Hui Chen and Dipak&#x00A0;K Dey. 2000. Bayesian analysis for correlated ordinal data models. <em>      <em>BIOSTATISTICS-BASEL-</em>     </em>5(2000), 133&#x2013;158.</li>     <li id="BibPLXBIB0009" label="[9]">Wei Chu and Seung-Taek Park. 2009. Personalized Recommendation on Dynamic Content Using Predictive Bilinear Models. In <em>      <em>Proceedings of the 18th International Conference on World Wide Web</em>     </em>(WWW &#x2019;09). ACM, New York, NY, USA, 691&#x2013;700. <a class="link-inline force-break" href="https://doi.org/10.1145/1526709.1526802"      target="_blank">https://doi.org/10.1145/1526709.1526802</a></li>     <li id="BibPLXBIB0010" label="[10]">Karen Church, Barry Smyth, Paul Cotter, and Keith Bradley. 2007. Mobile Information Access: A Study of Emerging Search Behavior on the Mobile Internet. <em>      <em>ACM Trans. Web</em>     </em>1, 1, Article 4 (May 2007). <a class="link-inline force-break" href="https://doi.org/10.1145/1232722.1232726"      target="_blank">https://doi.org/10.1145/1232722.1232726</a></li>     <li id="BibPLXBIB0011" label="[11]">Prem&#x00A0;K Gopalan, Laurent Charlin, and David Blei. 2014. Content-based recommendations with poisson factorization. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 3176&#x2013;3184.</li>     <li id="BibPLXBIB0012" label="[12]">William&#x00A0;H Greene and David&#x00A0;A Hensher. 2010. <em>      <em>Modeling ordered choices: A primer</em>     </em>. Cambridge University Press.</li>     <li id="BibPLXBIB0013" label="[13]">Justin Grimmer. 2010. An introduction to Bayesian inference via variational approximations. <em>      <em>Political Analysis</em>     </em>19, 1 (2010), 32&#x2013;47.</li>     <li id="BibPLXBIB0014" label="[14]">Morgan Harvey, Mark&#x00A0;J. Carman, Ian Ruthven, and Fabio Crestani. 2011. Bayesian Latent Variable Models for Collaborative Item Rating Prediction. In <em>      <em>Proceedings of the 20th ACM International Conference on Information and Knowledge Management</em>     </em>(CIKM &#x2019;11). ACM, New York, NY, USA, 699&#x2013;708. <a class="link-inline force-break" href="https://doi.org/10.1145/2063576.2063680"      target="_blank">https://doi.org/10.1145/2063576.2063680</a></li>     <li id="BibPLXBIB0015" label="[15]">Bikash Joshi, Franck Iutzeler, and Massih-Reza Amini. 2016. Asynchronous Distributed Matrix Factorization with Similar User and Item Based Regularization. In <em>      <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;16). ACM, New York, NY, USA, 75&#x2013;78. <a class="link-inline force-break" href="https://doi.org/10.1145/2959100.2959161"      target="_blank">https://doi.org/10.1145/2959100.2959161</a></li>     <li id="BibPLXBIB0016" label="[16]">Alexandros Karatzoglou, Xavier Amatriain, Linas Baltrunas, and Nuria Oliver. 2010. Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering. In <em>      <em>Proceedings of the fourth ACM conference on Recommender systems</em>     </em>. ACM, 79&#x2013;86.</li>     <li id="BibPLXBIB0017" label="[17]">Hiroyuki Kawakatsu and Ann&#x00A0;G Largey. 2009. EM algorithms for ordered probit models with endogenous regressors. <em>      <em>The Econometrics Journal</em>     </em>12, 1 (2009), 164&#x2013;186.</li>     <li id="BibPLXBIB0018" label="[18]">Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. <em>      <em>Computer</em>     </em>42, 8 (2009).</li>     <li id="BibPLXBIB0019" label="[19]">Dawen Liang, Jaan Altosaar, Laurent Charlin, and David&#x00A0;M Blei. 2016. Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence. In <em>      <em>Proceedings of the 10th ACM conference on recommender systems</em>     </em>. ACM, 59&#x2013;66.</li>     <li id="BibPLXBIB0020" label="[20]">Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In <em>      <em>Proceedings of the 7th ACM conference on Recommender systems</em>     </em>. ACM, 165&#x2013;172.</li>     <li id="BibPLXBIB0021" label="[21]">Trevelyan&#x00A0;J McKinley, Michelle Morters, James&#x00A0;LN Wood, <em>et al.</em> 2015. Bayesian Model Choice in Cumulative Link Ordinal Regression Models. <em>      <em>Bayesian Analysis</em>     </em>10, 1 (2015), 1&#x2013;30.</li>     <li id="BibPLXBIB0022" label="[22]">Atsuhiro Narita, Kohei Hayashi, Ryota Tomioka, and Hisashi Kashima. 2012. Tensor factorization using auxiliary information. <em>      <em>Data Mining and Knowledge Discovery</em>     </em>25, 2 (2012), 298&#x2013;324.</li>     <li id="BibPLXBIB0023" label="[23]">Cosimo Palmisano, Alexander Tuzhilin, and Michele Gorgoglione. 2008. Using context to improve predictive modeling of customers in personalization applications. <em>      <em>IEEE transactions on knowledge and data engineering</em>     </em>20, 11(2008), 1535&#x2013;1549.</li>     <li id="BibPLXBIB0024" label="[24]">Umberto Panniello, Alexander Tuzhilin, Michele Gorgoglione, Cosimo Palmisano, and Anto Pedone. 2009. Experimental comparison of pre-vs. post-filtering approaches in context-aware recommender systems. In <em>      <em>Proceedings of the third ACM conference on Recommender systems</em>     </em>. ACM, 265&#x2013;268.</li>     <li id="BibPLXBIB0025" label="[25]">Nicholas&#x00A0;G Polson, James&#x00A0;G Scott, and Jesse Windle. 2013. Bayesian inference for logistic models using P&#x00F3;lya&#x2013;Gamma latent variables. <em>      <em>Journal of the American statistical Association</em>     </em>108, 504(2013), 1339&#x2013;1349.</li>     <li id="BibPLXBIB0026" label="[26]">Zhen Qin, Ish Rishabh, and John Carnahan. 2016. A Scalable Approach for Periodical Personalized Recommendations. In <em>      <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;16). ACM, New York, NY, USA, 23&#x2013;26. <a class="link-inline force-break" href="https://doi.org/10.1145/2959100.2959139"      target="_blank">https://doi.org/10.1145/2959100.2959139</a></li>     <li id="BibPLXBIB0027" label="[27]">Dimitrios Rafailidis and Alexandros Nanopoulos. 2014. Modeling the Dynamics of User Preferences in Coupled Tensor Factorization. In <em>      <em>Proceedings of the 8th ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;14). ACM, New York, NY, USA, 321&#x2013;324. <a class="link-inline force-break" href="https://doi.org/10.1145/2645710.2645758"      target="_blank">https://doi.org/10.1145/2645710.2645758</a></li>     <li id="BibPLXBIB0028" label="[28]">Ruslan Salakhutdinov and Andriy Mnih. 2008. Bayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo. In <em>      <em>Proceedings of the 25th International Conference on Machine Learning</em>     </em>(ICML &#x2019;08). ACM, New York, NY, USA, 880&#x2013;887. <a class="link-inline force-break" href="https://doi.org/10.1145/1390156.1390267"      target="_blank">https://doi.org/10.1145/1390156.1390267</a></li>     <li id="BibPLXBIB0029" label="[29]">Ruslan Salakhutdinov and Andriy Mnih. 2008. Probabilistic Matrix Factorization. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>, Vol.&#x00A0;20.</li>     <li id="BibPLXBIB0030" label="[30]">Vivek Sembium, Rajeev Rastogi, Atul Saroop, and Srujana Merugu. 2017. Recommending Product Sizes to Customers. In <em>      <em>Proceedings of the Eleventh ACM Conference on Recommender Systems</em>     </em>(RecSys &#x2019;17). ACM, New York, NY, USA, 243&#x2013;250. <a class="link-inline force-break" href="https://doi.org/10.1145/3109859.3109891"      target="_blank">https://doi.org/10.1145/3109859.3109891</a></li>     <li id="BibPLXBIB0031" label="[31]">Flavian Vasile, Elena Smirnova, and Alexis Conneau. 2016. Meta-Prod2Vec: Product Embeddings Using Side-Information for Recommendation. In <em>      <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>     </em>. ACM, 225&#x2013;232.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186149">https://doi.org/10.1145/3178876.3186149</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
