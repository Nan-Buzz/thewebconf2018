<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3188731'>https://doi.org/10.1145/3184558.3188731</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3188731'>https://w3id.org/oa/10.1145/3184558.3188731</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Amy X.</span> <span class="surName">Zhang</span> MIT CSAIL, Cambridge, MA, USA, <a href="mailto:axz@mit.edu">axz@mit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Aditya</span> <span class="surName">Ranganathan</span> Berkeley Institute for Data Science, Berkeley, CA, USA, <a href="mailto:adityarn@berkeley.edu">adityarn@berkeley.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Sarah Emlen</span> <span class="surName">Metz</span> Berkeley Institute for Data Science, Berkeley, CA, USA, <a href="mailto:emlen.metz@berkeley.edu">emlen.metz@berkeley.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Scott</span> <span class="surName">Appling</span> Georgia Institute of Technology, Atlanta, GA, USA, <a href="mailto:scott.appling@gtri.gatech.edu">scott.appling@gtri.gatech.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Connie Moon</span> <span class="surName">Sehat</span> Global Voices, London, UK, <a href="mailto:connie@globalvoices.org">connie@globalvoices.org</a>
        </div>
        <div class="author">
          <span class="givenName">Norman</span> <span class="surName">Gilmore</span> Berkeley Institute for Data Science, Berkeley, CA, USA, <a href="mailto:norman@virtualnorman.com">norman@virtualnorman.com</a>
        </div>
        <div class="author">
          <span class="givenName">Nick B.</span> <span class="surName">Adams</span> Berkeley Institute for Data Science, Berkeley, CA, USA, <a href="mailto:nickbadams@berkeley.edu">nickbadams@berkeley.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Emmanuel</span> <span class="surName">Vincent</span> Climate Feedback, University of California, Merced, Merced, CA, USA, <a href="mailto:emvincent@climatefeedback.org">emvincent@climatefeedback.org</a>
        </div>
        <div class="author">
          <span class="givenName">Jennifer 8.</span> <span class="surName">Lee</span> Hacks/Hackers, San Francisco, CA, USA, <a href="mailto:jenny@hackshackers.com">jenny@hackshackers.com</a>
        </div>
        <div class="author">
          <span class="givenName">Martin</span> <span class="surName">Robbins</span> Factmata, London, UK, <a href="mailto:martin.robbins@factmata.com">martin.robbins@factmata.com</a>
        </div>
        <div class="author">
          <span class="givenName">Ed</span> <span class="surName">Bice</span> Meedan, San Francisco, CA, USA, <a href="mailto:ed@meedan.com">ed@meedan.com</a>
        </div>
        <div class="author">
          <span class="givenName">Sandro</span> <span class="surName">Hawke</span> W3C, Cambridge, MA, USA, <a href="mailto:sandro@w3.org">sandro@w3.org</a>
        </div>
        <div class="author">
          <span class="givenName">David</span> <span class="surName">Karger</span> MIT CSAIL, Cambridge, MA, USA, <a href="mailto:karger@mit.edu">karger@mit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">An Xiao</span> <span class="surName">Mina</span> Meedan, San Francisco, CA, USA, <a href="mailto:an@meedan.com">an@meedan.com</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3188731" target="_blank">https://doi.org/10.1145/3184558.3188731</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The proliferation of misinformation in online news and its amplification by platforms are a growing concern, leading to numerous efforts to improve the detection of and response to misinformation. Given the variety of approaches, collective agreement on the indicators that signify credible content could allow for greater collaboration and data-sharing across initiatives. In this paper, we present an initial set of indicators for article credibility defined by a diverse coalition of experts. These indicators originate from both within an article's text as well as from external sources or article metadata. As a proof-of-concept, we present a dataset of 40 articles of varying credibility annotated with our indicators by 6 trained annotators using specialized platforms. We discuss future steps including expanding annotation, broadening the set of indicators, and considering their use by platforms and the public, towards the development of interoperable standards for content credibility.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>misinformation; disinformation; information disorder; credibility; news; journalism; media literacy; web standards</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Amy X. Zhang, Aditya Ranganathan, Sarah Emlen Metz, Scott Appling, Connie Moon Sehat, Norman Gilmore, Nick B. Adams, Emmanuel Vincent, Jennifer 8. Lee, Martin Robbins, Ed Bice, Sandro Hawke, David Karger, and An Xiao Mina. 2018. A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3184558.3188731" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3188731</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>While the propagation of false information existed well before the internet&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>], recent changes to our information ecosystem&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>] have created new challenges for distinguishing misinformation from credible content. Misinformation, or information that is false or misleading, can quickly reach thousands or millions of readers, helped by inattentive or malicious sharers and algorithms optimized for engagement. Many solutions to remedy the propagation of misinformation have been proposed—from initiatives for publishers to signal their credibility<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, to technologies for automatically labeling misinformation and scoring content credibility&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>], to the engagement of professional fact-checkers or experts&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], to campaigns to improve literacy&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] or crowdsource annotations&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>].</p>
      <p>While all these initiatives are valuable, the problem is so multifaceted that each provides only partial alleviation. Instead, a holistic approach, with reputation systems, fact-checking, media literacy campaigns, revenue models, and public feedback all contributing, could collectively work towards improving the health of the information ecosystem. To foster this cooperation, we propose a <em>shared vocabulary for representing credibility</em>. However, credibility is not a Boolean flag: there are many indicators, both human- and machine-generated, that can feed into an assessment of article credibility, and differing preferences for what indicators to emphasize or display. Instead of an opaque score or flag, a more transparent and customizable approach would be to allow publishers, platforms, and the public to both understand and communicate what aspects of an article contribute to its credibility and why.</p>
      <p>In this work, we describe a set of initial indicators for article credibility, grouped into <em>content</em> signals, that can be determined by only considering the text or content of an article, as well as <em>context</em> signals, that can be determined through consulting external sources or article metadata. These indicators were iteratively developed through consultations with journalists, researchers, platform representatives, and others during a series of conferences, workshops, and online working sessions. While there are many signals of credibility, we focus on article indicators that do not need a domain expert but require human judgment and training. This focus differentiates our work from efforts targeting purely computational or expert-driven indicators, towards broadening participation in credibility annotation and improving media literacy. To validate the indicators and examine how they get annotated, we gathered a dataset of 40 highly shared articles focused on two topics possessing a high degree of misinformation in popular media: public health&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>] and climate science&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>]. These articles were each annotated with credibility indicators by 6 annotators with training in journalism and logic and reasoning. Their rich annotations help us understand the consistency of the different indicators across annotators and how well they align with domain expert evaluations of credibility. We are releasing the data publicly<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, and will host an expanded dataset in service to the research community and public.</p>
      <p>The process outlined in this paper serves as a template for creating a standardized set of indicators for evaluating content credibility. With broad consensus, these indicators could then support an ecosystem of varied annotators and consumers of credibility data. By focusing on indicators, we leave open the question of who or what performs annotation. Indeed, the presence of a standard permits flexibility on the part of users and platforms to determine whose annotations to surface based on who they consider trustworthy. Our approach also leaves open the question of how annotations are generated, from the use of partial or full automation, to annotations by experts or publishers, to crowdsourced or friendsourced methods. However, results from our initial study suggest certain methods may be more or less fruitful for different indicators.</p>
      <p>Aligned with the goals of groups such as the W3C Credentials Community Group<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>, using our indicators, any interested party could contribute annotations using open standards developed during this work, while any system for displaying or sharing news could make their own decisions about how to aggregate, weight, filter and display credibility information. For instance, systems such as web browsers, search engines, or social platforms could surface information about a news article to benefit readers, much like how nutrition labels for food and browser security labels for webpages provide context in the moment. Readers could also verify an article by building on the annotations left by others or even interrogate the indicators that a particular publisher or other party provides. Finally, the data may be helpful to researchers and industry watchdogs seeking to monitor the ecosystem as a whole.</p>
    </section>
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>In recent years, researchers have sought to better define and characterize misinformation and its place in the larger information ecosystem. Some researchers have chosen to eschew the popularized term “fake news”, calling it overloaded&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>]. Instead, they have opted for terms such as “information pollution” and “information disorder”&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>], to focus not only on the authenticity of the content itself, but also the motivations and actions of creators, including disinformation agents&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0047">47</a>], readers, media companies&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>] and their advertising models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], platforms, and sharers. Accordingly, our approach covers a broad range of indicators developed by experts representing a range of disciplines and industries.</p>
      <p>An important aspect of characterizing misinformation is understanding how people perceive the credibility of information. Reviews of the credibility research literature&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>] describe various aspects of credibility attribution, including judgments about the credibility of a particular source or a broader platform (e.g., a blog versus social media)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0043">43</a>], as well as message characteristics that impact perceptions of credibility of the message or source&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]. Studies have pointed out the differences in perceived credibility that can occur based on differences in personal relevance&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0053">53</a>], individual online usage&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>], and the co-orientation of reader and writer views &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>], among others. This prior work suggests that a one-size-fits-all approach or an approach that provides an opaque “credibility score” will not be able to adapt to individual needs.</p>
      <p>However, research has also found that readers can be swayed by superficial qualities that may be manipulated, such as a user's avatar on social media&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] or number of sources quoted in an article&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0048">48</a>], demonstrating the need for greater media literacy. Another study found that fact-checkers correctly determine credibility using lateral searching, while non-experts fall victim to convincing logos&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0051">51</a>]. In response, researchers have considered how interfaces could provide better context for gauging information quality, in areas such as Wikipedia&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>], related articles in social media&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>], and flags on disputed content&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>]. By surfacing more nuanced signals about the credibility of an article, we hope to provide greater context to readers and platforms to make informed judgments.</p>
      <p>There exists a significant amount of related work on computational models related to information credibility&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>]. Many models focus on aspects of language that can be a signal of low credibility&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>], such as hedging&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>] or biased language&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>]. Researchers have also studied the linguistic characteristics of deceptively written content&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0054">54</a>] and their relation to credibility&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>], as well as misleading headlines&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. As social media is increasingly a space where misinformation is propagated, researchers have studied how rumors&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] as well as corrections&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] spread on social media. Building on this work, researchers have built models to predict credibility of social media posts&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>], as well as tools for investigating rumors or claims&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>]. In addition, researchers have focused on the credibility of individual claims or assertions within text&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]. In the area of computational fact-checking, researchers evaluate the truthfulness of claims by comparing concepts within knowledge graphs&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0052">52</a>]. Though the focus of this study is article indicators, all of these signals contribute to assessments of information credibility more broadly, and this prior work suggests some credibility indicators that might be automatable in the future.</p>
      <p>Finally, our research is related to prior work on human annotation of credibility, such as annotations of social media&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] or of television and newspaper content&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]. In contrast to this earlier work, we chose to use trained annotators as opposed to a random sampled population or Amazon Mechanical Turk workers, and we collected annotations about specific indicators instead of just overall credibility. These decisions allowed us to capture a richer and more informed characterization of credibility.</p>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Towards Structured Credibility Indicators for Online Journalism</h2>
        </div>
      </header>
      <p>The need for a common vocabulary around credibility became apparent at the first MisinfoCon<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>, a conference dedicated to misinformation that saw many projects to define and classify misinformation and credibility but no easy way to communicate findings and data across projects. From an initial meeting at the conference, workshops in San Francisco and New York were convened, with over 40 representatives from journalism and fact-checking groups, research labs, social and annotation platforms, web standards, and more. A broader alliance called the <em>Credibility Coalition</em> emerged among these participants, with weekly remote working sessions. From these sessions, participants drafted over 100 indicator suggestions, taking example from existing credibility initiatives, such as Climate Feedback<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a> and the Trust Project. As outside input is crucial for the success of this project, representatives presented to communities such as the Mozilla Festival<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> and the International Press Telecommunications Council Meeting<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>, to publicize the work and host workshops for gathering feedback.</p>
      <p>Over time, the indicators coalesced into 12 major categories, including reader behavior, revenue models, publication metadata, and inbound and outbound references. From this collection, 16 indicators were chosen for annotation. We chose article-level indicators that require human annotation from trained annotators but no domain expertise. Thus, we did not consider automated indicators for this study or ones that require significant expertise, such as domain knowledge of the subject matter, offline investigation, or data gathering requiring technical knowledge or access to proprietary data. As our current focus is articles, we chose to ignore indicators related to publishers, authors, or any multimedia content.</p>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Content Indicators</h3>
          </div>
        </header>
        <p>Content indicators are those that can be determined by analyzing the title and text of the article without consulting outside sources or metadata. We present the following 8 content indicators, their definitions, and what we asked of annotators.</p>
        <p><strong>Title Representativeness</strong>: Article titles can be misleading or opaque about the topic, claims, or conclusions of the content. Annotators were asked to rate the representativeness of the article title. If it was found unrepresentative, they were asked to clarify how the title was unrepresentative; for instance, by being off-topic, carrying little information, or overstating or understating claims.</p>
        <p><strong>“Clickbait” Title</strong>: “Clickbait” is defined as “a certain kind of web content...that is designed to entice its readers into clicking an accompanying link”&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]. Annotators were asked to rate the degree to which a headline was clickbait. If annotators rated a title as clickbait, they were asked to clarify the form of clickbait in a follow-up question, such as a “listicle” or a cliffhanger.</p>
        <p><strong>Quotes from Outside Experts</strong>: Articles often seek outside feedback from independent experts in the field. This additional validation provides support for the conclusions drawn and reveals a level of journalistic rigor&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0048">48</a>]. For this indicator, we asked annotators to highlight where experts were quoted in the article.</p>
        <p><strong>Citation of Organizations and Studies</strong>: Journalists can also cite or quote from a range of organizations or scientific studies to add context or support to the article and enhance its credibility. We asked annotators to highlight where any scientific studies or any organizations were cited, as well as indicate whether the article was primarily about a single study.</p>
        <p><strong>Calibration of Confidence</strong>: The use of tentative propositions in writing, often quantified, allows readers to assess claims with appropriate confidence. We asked annotators to mark whether authors used appropriate language to show confidence in their claims, and to highlight sections of an article where authors acknowledge their level of uncertainty (e.g. hedging, tentative, assertive language).</p>
        <p><strong>Logical Fallacies</strong>: Logical fallacies often mislead readers, as both writer and reader fall prey to poor but tempting arguments. Indeed, studies have shown that people find them more convincing than is rational&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. We asked our annotators to look for the <em>straw man fallacy</em> (presenting a counterargument as a more obviously wrong version of existing counterarguments), <em>false dilemma fallacy</em> (treating an issue as binary when it is not), <em>slippery slope fallacy</em> (assuming one small change will lead to a major change), <em>appeal to fear fallacy</em> (exaggerating the dangers of a situation), and the <em>naturalistic fallacy</em> (assuming that what is natural must be good).</p>
        <p><strong>Tone</strong>: Readers can be misled by the emotional tone of articles. Such language is common in opinion pieces, which readers may parse as straight news. We asked our annotators to look for exaggerated claims or emotionally charged sections, especially for expressions of contempt, outrage, spite, or disgust.</p>
        <p><strong>Inference</strong>: <em>Correlation</em> and <em>causation</em> are often conflated, and the implications can be dramatic, for example in medical trials. There is also the more subtle conflation between singular causation (“the drunk driver caused <em>that</em> accident”) and general causation (“drinking and driving causes accidents”). For this indicator, we asked annotators to determine what type of causality—correlation, singular causation, or general causation—was at play, and whether there is convincing evidence for the claims expressed.</p>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Context Indicators</h3>
          </div>
        </header>
        <p>In total, there were 8 context indicators collected by annotators. Context indicators require annotators to look outside of the article text and research external sources or examine the metadata surrounding the article text, such as advertising and layout.</p>
        <p><strong>Originality</strong>: Republishing text is a common practice in online news. Reasons include licensing agreements from a wire service such as Reuters, or the article can simply be stolen or reworded without attribution. We asked annotators to find whether the article was an original piece of writing or duplicated elsewhere, and if so, to check whether attribution was given.</p>
        <p><strong>Fact-checked</strong>: We asked annotators to determine whether the central claim of the article, if one exists, was fact-checked by an approved organization, as well as the outcome of the check. While many organizations conduct fact-checking&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>], we limited our consideration to organizations vetted by a verified signatory of Poynter's International Fact-Checking Network (IFCN)<a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a>. Because many IFCN members utilize schema.org's ClaimReview schema, there is potential to automate this process in the future&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>].</p>
        <p><strong>Representative Citations</strong>: Journalists are expected to accurately represent any sources that they cite or quote, such as articles, interviews, or other external materials. As an article may have many sources, we asked annotators to check the representation of only the first three sources mentioned in the article. Annotators were asked to find the original content and rate how accurately the description in the article represented the original content.</p>
        <p><strong>Reputation of Citations</strong>: Without domain experts, it is difficult to systematically evaluate the validity or credibility of a cited source. However, for scientific studies, there are at least some existing public measures such as impact factor, despite their documented issues&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>]. Thus, we asked annotators to find the impact factor of the publication of any scientific study cited.</p>
        <p><strong>Number of Ads</strong>: Most publications depend on ad content and recommendation engines as a core part of their business model. Per a recent Facebook strategy, a very high number of ads relative to content may be an indicator of a financially-motivated misinformation site&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>]. We asked annotators to count the number of display ads, content recommendation engines, such as Taboola or Outbrain, as well as recommended sponsored content.</p>
        <p><strong>Number of Social Calls</strong>: Most publications depend on social networks and viral content to drive traffic to their site. That said, a high number of exhortations to share content on social media, email the article, or join a mailing list can be an indicator of financially-motivated misinformation. We ask annotators to count the number of calls to share on social media, email, or join a mailing list.</p>
        <p><strong>“Spammy” Ads</strong>: As well as quantity, the ads on the page may be of a “spammy” nature, such as containing disturbing or titillating imagery, or celebrities, or clickbait titles. Thus, we asked annotators to rate the “spammyness” of the ads.</p>
        <p><strong>Placement of Ads and Social Calls</strong>: Finally, the placement of ads and social calls may be an indicator, for instance by appearing in pop-up windows, covering up article content, or distracting through additional animation and audio. We ask annotators to rate the aggressiveness of the placement of ads and social calls.</p>
      </section>
    </section>
    <section id="sec-20">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Data Collection</h2>
        </div>
      </header>
      <p>This section describes our process for gathering articles, finding annotators, and selecting platforms for credibility annotation.</p>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Articles</h3>
          </div>
        </header>
        <p>We focused on the topics of climate science and public health, where misinformation is prevalent despite a high degree of stable knowledge and expert consensus. Articles were selected using BuzzSumo<a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a>, a service that surfaces the most shared articles on social media for any search term. Terms we searched included “climate change” and “global warming” for climate science, and “health”, “vaccines”, and “disease” for public health. Articles returned from the year 2017 were collected into one list and sorted by most overall shares, so as to prioritize high impact articles with broad appeal. We removed 2 articles that were too long for annotation (3,500+ words), 2 that were primarily images, and one suspended article. Finally, the 40 most shared articles from the list were selected. In total, there were 22 articles related to public health, 10 related to climate science, 7 related to diseases, and 4 related to vaccines. The most shared article was about vaccines by a publisher called “Earth. We Are One” and shared 1.9 million times in 2017, according to BuzzSumo. To ensure that article content would not change or disappear during the study, they were archived using Archive.is<a class="fn" href="#fn10" id="foot-fn10"><sup>10</sup></a>.</p>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Annotators</h3>
          </div>
        </header>
        <p>Six annotators were recruited for this task, with 3 focused on content indicators and 3 marking context indicators, as content annotation requires different prior knowledge and training than context. The 3 content annotators were recruited from the teaching staff of a UC Berkeley course on scientific-style critical thinking called Sense and Sensibility and Science (SSS)<a class="fn" href="#fn11" id="foot-fn11"><sup>11</sup></a> The content annotators were selected because of their exemplary performance in the course and their skills in scientific critical thinking. The 3 context annotators were recruited by Meedan<a class="fn" href="#fn12" id="foot-fn12"><sup>12</sup></a> from a number of journalism schools. Annotators were either journalism students or recent graduates. Annotators were paid <font style="normal">$</font>150 for the entire task, or <font style="normal">$</font>3.75 per article. For a <font style="normal">$</font>15 wage per hour, this amounts to around 15 minutes spent per article, which we sought to target when devising annotations.</p>
        <p>The average age of the annotators was 22.1, and 5 annotators were female, while 1 was male. We sought to diversify our population in terms of political orientation to mitigate issues with bias. Asked about their political affiliation, 3 stated Democrat, 1 Republican, 1 Independent, and one stated none. On economic issues, 2 named themselves as very liberal, 1 moderately liberal, 1 moderate, 1 moderately conservative, and 1 as very conservative. On social issues, 4 considered themselves very liberal while 2 considered themselves moderate. When asked what publications they read regularly, 4 annotators mentioned The New York Times, while 2 mentioned CNN. The remaining 22 publications were mentioned only once. Our future work will aim towards greater diversity among annotators as we grow our pool.</p>
        <p>Finally, we collected a credibility score for each article from domain experts to serve as “gold standard” credibility scores. We determined the general topic of each article, such as climate science, psychology, or public health. Then we reached out on different platforms to find domain experts in those areas, such as scientists or industry practitioners, to score the article on a 5 point scale and leave notes. For articles dedicated to breaking news events, we had a journalist score them for credibility. Overall there were 5 domain expert annotators who volunteered their time.</p>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Annotation Platforms</h3>
          </div>
        </header>
        <figure id="fig1">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3188731/images/www18companion-239-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span> <span class="figure-title">Screenshot of TextThresher platform used for content indicator annotation.</span>
          </div>
        </figure>
        <p>The three annotators tasked with content indicators used a collaborative annotation software called TextThresher<a class="fn" href="#fn13" id="foot-fn13"><sup>13</sup></a>, which is also in use by the citizen-science misinformation and media literacy platform PublicEditor designed at the UC Berkeley Institute for Data Science&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>]. Figure&nbsp;<a class="fig" href="#fig1">1</a> shows the tool guiding contributors to answer a series of questions about article text, highlighting the portions of text that justify their answers.</p>
        <p>Because TextThresher supports the annotation of plain text files, the tool was useful for our approach to content indicator evaluation, which seeks to reduce annotator bias by removing text from its original context. Previous workshops we conducted revealed that participants’ assessments were strongly influenced by the name of the publication and its layout. With TextThresher, we only show annotators the article's title, any image captions, and the main text of each article. TextThresher also guides users to label specific words and phrases within articles, which enables the precise identification of specific phenomena. Users’ labels can then be displayed to news readers to improve their media literacy, and in high-traction supervised machine learning scenarios.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3188731/images/www18companion-239-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Screenshot of Check platform used for context indicator annotation.</span>
          </div>
        </figure>
        <p></p>
        <p>The three annotators tasked with context indicators used a tool called Check<a class="fn" href="#fn14" id="foot-fn14"><sup>14</sup></a> built by Meedan, a nonprofit software company that builds digital tools for journalists. Using Check, we showed a preview of the article with a link to see the article in context. Because these indicators involved looking at the information around the article as well as conducting research on external information, it was no longer possible to obfuscate the publisher or other information. Each annotator could see all their own annotation tasks related to an article on the page and mark each as complete when done. They could also keep track of their progress and go back to articles to edit or resume annotation.</p>
      </section>
    </section>
    <section id="sec-24">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Dataset Analysis</h2>
        </div>
      </header>
      <p>We next turn to analysis of the annotation data. Here, we focus on two measures: (1) how much annotators agreed with one another when identifying indicators, and (2) how much the annotators’ assessments of overall article credibility agreed with domain experts’ assessments. We calculate inter-rater reliability (IRR) using Krippendorff's alpha, as it can be used for more than 2 scorers and can be adapted to many different data types, including nominal, ordinal, and interval scores, all of which are present in the data we collected.</p>
      <p>When we aggregate annotations to then correlate with domain expert scores, in the case of ordinal and interval data, we compute an average across annotators, while in the case of nominal data, we use the category most chosen, if it exists. To determine correlation, in the case of ordinal and interval indicator data, we use the Spearman rank correlation as it allows for ordinal data, and relationships need not be linear. For nominal input data, as there is no concept of correlation, we convert the categories into binary variables and perform a multiple linear regression. We report the coefficient of determination (R<sup>2</sup>), which reports what percentage of the variance in domain expert scores is explained by the model.</p>
      <section id="sec-25">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Content Indicators</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Inter-rater reliability for content indicators and their relationship to expert scores of credibility. (*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001)</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>Content Indicator</strong></td>
                <td style="text-align:left;"><strong>Data Type</strong></td>
                <td style="text-align:left;"><strong>IRR</strong></td>
                <td style="text-align:left;"><strong>Relation to Experts</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">Title Representativeness</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.367</td>
                <td style="text-align:left;"><em>ρ</em>=0.234</td>
              </tr>
              <tr>
                <td style="text-align:left;">“Clickbait” Title</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.581</td>
                <td style="text-align:left;"><em>ρ</em>=-0.709***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Quotes from Outside Experts</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.673</td>
                <td style="text-align:left;"><em>ρ</em>=0.327</td>
              </tr>
              <tr>
                <td style="text-align:left;">Citation of Organizations</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.283</td>
                <td style="text-align:left;"><em>ρ</em>=0.145</td>
              </tr>
              <tr>
                <td style="text-align:left;">Citation of Studies</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.763</td>
                <td style="text-align:left;"><em>ρ</em>=0.107</td>
              </tr>
              <tr>
                <td style="text-align:left;">Single Study Article</td>
                <td style="text-align:left;">nominal</td>
                <td style="text-align:left;">0.877</td>
                <td style="text-align:left;">R<sup>2</sup>=0.031</td>
              </tr>
              <tr>
                <td style="text-align:left;">Confidence - Extent Claims Justified</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">-0.093</td>
                <td style="text-align:left;"><em>ρ</em>=0.690***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Confidence - Acknowledge Uncertainty</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.534</td>
                <td style="text-align:left;"><em>ρ</em>=-0.247</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logical Fallacies - Straw Man</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">-0.096</td>
                <td style="text-align:left;"><em>ρ</em>=-0.402*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logical Fallacies - False Dilemma</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.102</td>
                <td style="text-align:left;"><em>ρ</em>=-0.303</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logical Fallacies - Slippery Slope</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.478</td>
                <td style="text-align:left;"><em>ρ</em>=0.374*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logical Fallacies - Appeal to Fear</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.314</td>
                <td style="text-align:left;"><em>ρ</em>=-0.424*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logical Fallacies - Naturalistic</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.377</td>
                <td style="text-align:left;"><em>ρ</em>=-0.533**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Tone - Emotionally Charged</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.098</td>
                <td style="text-align:left;"><em>ρ</em>=0.611***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Tone - Exaggerated Claims</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.235</td>
                <td style="text-align:left;"><em>ρ</em>=0.606***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Inference - Type of Claims</td>
                <td style="text-align:left;">nominal</td>
                <td style="text-align:left;">0.154</td>
                <td style="text-align:left;">R<sup>2</sup>=0.029</td>
              </tr>
              <tr>
                <td style="text-align:left;">Inference - Convincing Evidence</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.540</td>
                <td style="text-align:left;"><em>ρ</em>=0.764***</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In Table&nbsp;<a class="tbl" href="#tab1">1</a>, we show both the IRR between the three annotators as well as the relationship to domain expert scores for each of the questions for content indicators. Some indicators had moderate to strong IRR, such as the ones involving highlighting number of citations, quotes, or scientific studies. Other indicators with moderate reliability included clickbait, which also had high correlation to expert ratings. The correlation with expert ratings combined with high IRR suggests that these types of indicators may be a useful signal of credibility to target for further annotation.</p>
        <p>Annotators had weaker agreement on questions related to logical fallacies generally due to scarcity and inadequate training. While at first glance, “false dillemma” showed up in 16.1% of articles and “straw man” applied to 37.7%, further analysis reveals that all these annotations were due to a single annotator. As the annotators were not given explicit definitions of the fallacies, differences in interpretation could lead to low IRR. Future annotation of logical fallacy indicators could include more training and examples.</p>
        <p>Indicators referencing claims (Confidence–extent claims justified; Scientific Inference–types of claims) also had low IRR. Some prior evidence suggests this was expected: of the 200+ students taking the Sense &amp; Sensibility &amp; Science course at UC Berkeley, less than half answered questions about the type of scientific inference in a claim correctly. However, annotators for this study had high IRR and high correlation for follow up questions (Scientific Inference–convincing evidence, etc.) This suggests that non-expert annotators may find it difficult to classify claims, but once a claim is classified, annotators can be used for further evaluation.</p>
        <p>We notice some indicators that had a moderate to strong correlation with domain experts, such as the perception of convincing evidence, that would be expected. Other indicators had a more unexpected relationship to expert credibility. For instance, the presence of slippery slope logical fallacies actually had a weak positive correlation with expert perception of credibility. Interestingly, some of the indicators such as “Tone–emotionally charged” that had low agreement between annotators still had strong correlation with experts, demonstrating that individual scores may have been calibrated to different levels but still moved similarly. However, normalizing scores for each annotator did not significantly alter IRR. We also found that there were several content indicators that were auto-correlated. Some were highly correlated within an indicator, such as the two questions related to Tone (<em>ρ</em>=0.736, <em>p</em> &lt; 0.001), suggesting that the number of questions could be reduced or that annotators could have been biased in one direction across questions. Future analysis, perhaps comparing with a gold standard set of annotations, is necessary. From discussions with annotators, some annotators did in fact say that some questions felt redundant. Several annotators also remarked that as they annotated more and more articles, their initial read of the article (before answering any of the questions) was already punctuated with a mental checklist.</p>
      </section>
      <section id="sec-26">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Context Indicators</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Inter-rater reliability for context indicators and their relationship to expert scores of credibility. (*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001)</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>Context Indicator</strong></td>
                <td style="text-align:left;"><strong>Data Type</strong></td>
                <td style="text-align:left;"><strong>IRR</strong></td>
                <td style="text-align:left;"><strong>Relation to Experts</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">Originality</td>
                <td style="text-align:left;">nominal</td>
                <td style="text-align:left;">0.346</td>
                <td style="text-align:left;">R<sup>2</sup>=0.068</td>
              </tr>
              <tr>
                <td style="text-align:left;">Fact-checked</td>
                <td style="text-align:left;">nominal</td>
                <td style="text-align:left;">0.303</td>
                <td style="text-align:left;">R<sup>2</sup>=0.309*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Representative Citations</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.312</td>
                <td style="text-align:left;"><em>ρ</em>=0.612***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Reputation of Citations</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.852</td>
                <td style="text-align:left;"><em>ρ</em>=-0.026</td>
              </tr>
              <tr>
                <td style="text-align:left;">Number of Ads</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.535</td>
                <td style="text-align:left;"><em>ρ</em>=-0.135</td>
              </tr>
              <tr>
                <td style="text-align:left;">Number of Content Recommendation</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">-0.088</td>
                <td style="text-align:left;"><em>ρ</em>=0.144</td>
              </tr>
              <tr>
                <td style="text-align:left;">Number of Sponsored Content</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.422</td>
                <td style="text-align:left;"><em>ρ</em>=-0.196</td>
              </tr>
              <tr>
                <td style="text-align:left;">Number of Social Calls</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.564</td>
                <td style="text-align:left;"><em>ρ</em>=0.179</td>
              </tr>
              <tr>
                <td style="text-align:left;">Number of Mailing List or Email Calls</td>
                <td style="text-align:left;">interval</td>
                <td style="text-align:left;">0.375</td>
                <td style="text-align:left;"><em>ρ</em>=0.453**</td>
              </tr>
              <tr>
                <td style="text-align:left;">“Spammy” Ads</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.554</td>
                <td style="text-align:left;"><em>ρ</em>=-0.309</td>
              </tr>
              <tr>
                <td style="text-align:left;">Placement of Ads and Social Calls</td>
                <td style="text-align:left;">ordinal</td>
                <td style="text-align:left;">0.326</td>
                <td style="text-align:left;"><em>ρ</em>=-0.417*</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In Table&nbsp;<a class="tbl" href="#tab2">2</a>, we show the IRR and correlation with domain experts for context indicators. Most indicators showed moderate to strong agreement between annotators. One major exception is the indicator asking annotators to count content recommendation boxes had no agreement between annotators. We found this was because some annotators counted every content recommendation article shown, while others counted an entire box of articles as a single entity. While there was weak agreement between annotators on the question of proper characterization of sources, this may be lower partially because we noticed annotators did not always choose to annotate the same three sources due to disagreement on what constitutes a source. However, the question about source characterization had the strongest correlation with expert credibility.</p>
        <p>When examining the advertising and social sharing indicators, it was interesting to note that counting the quantity of the different types of supplementary content was not significant except for the case of mailing lists and email. This suggests that both low and high credibility publications may be using similar monetization techniques. Likewise, if they are using similar advertising networks, they may both be serving up similarly “spammy” ads, as echoed by advertising industry experts&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>]. One difference however is in the aggressiveness of ad placement. Future work could consider signals determined by standards set by the Coalition for Better Ads<a class="fn" href="#fn15" id="foot-fn15"><sup>15</sup></a>.</p>
        <p>Finally, we notice a lack of correlation between the impact factor of scientific citations and credibility. In total, only 25% of articles had any annotations of impact factor. As there is no single structured database for journal impact factors, annotators went through a manual process of searching. In the future, a structured database of impact factors and other publication quality signals, such as number of citations, could make machine assessments easier.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">Inter-rater reliability for assessments of credibility by annotators before doing annotation as well as after, as well as their relationship to expert scores of credibility. (*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001)</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>Credibility Rating</strong></td>
                <td style="text-align:left;"><strong>IRR</strong></td>
                <td style="text-align:left;"><strong>Avg (SD)</strong></td>
                <td style="text-align:left;"><strong>Relation to Experts</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">Content Pre-Annotation</td>
                <td style="text-align:left;">0.695</td>
                <td style="text-align:left;">2.61 (0.98)</td>
                <td style="text-align:left;"><em>ρ</em>=0.630***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Content Post-Annotation</td>
                <td style="text-align:left;">0.665</td>
                <td style="text-align:left;">2.60 (0.97)</td>
                <td style="text-align:left;"><em>ρ</em>=0.748***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Context Pre-Annotation</td>
                <td style="text-align:left;">0.715</td>
                <td style="text-align:left;">2.81 (1.18)</td>
                <td style="text-align:left;"><em>ρ</em>=0.783***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Context Post-Annotation</td>
                <td style="text-align:left;">0.616</td>
                <td style="text-align:left;">2.70 (1.16)</td>
                <td style="text-align:left;"><em>ρ</em>=0.793***</td>
              </tr>
              <tr>
                <td style="text-align:left;">Domain Experts</td>
                <td style="text-align:left;">-</td>
                <td style="text-align:left;">2.29 (1.38)</td>
                <td style="text-align:left;">-</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Comparing Credibility Scores</h3>
          </div>
        </header>
        <p>We asked annotators to mark their overall impression of credibility of the article on a 5-point scale both before and after each article annotation. As seen in Table&nbsp;<a class="tbl" href="#tab3">3</a>, for both sets of annotators, the IRR dropped from before conducting annotation to after. While there was also slight differences between before and after mean scores, these were not significant. We notice that the correlation to domain expert scores increases from before annotation to after. This suggests that annotators became more aligned with domain experts after investigating our indicators.</p>
        <p>Finally, we also notice differences between the different sets of annotators and the domain experts. Overall, context annotators had a stronger correlation to experts than content annotators. These differences may partially be due to the information to which the annotators had access. In the case of content, annotators did not have information about the source or the presentation of the article webpage, which can be strong indicators of credibility. As both context annotators and domain experts had access to this context, their scores may be more aligned. However, context annotators rated articles significantly higher than experts both before and after annotation (paired t-test, <em>p</em> &lt; 0.05), while this difference was not significant between content annotators and experts.</p>
      </section>
      <section id="sec-28">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.4</span> Predicting Credibility from Indicators</h3>
          </div>
        </header>
        <p>To understand the predictive value provided by both sets of indicators, two backward stepwise multiple regression models were run to regress domain expert scores onto each set of indicators. As some indicators had high auto-correlation, we removed variables within the same indicator that were highly correlated with each other, leaving a single variable to represent that indicator. For the content model, <em>R</em> <sup>2</sup> for the overall model was 0.482 and adjusted <em>R</em> <sup>2</sup> was 0.448. After model convergence, two variables remained: Clickbait Title and Logical Fallacies–slippery slope. This model was found to significantly predict credibility (<em>F</em> = 13.972, <em>p</em> &lt; 0.001). For the context model, <em>R</em> <sup>2</sup> for the overall model was 0.750 and adjusted <em>R</em> <sup>2</sup> was 0.692. After model convergence, 6 variables remained: Fact-checked–reported false, Fact-checked–reported mixed results, Number of Social Calls, Number of Mailing List Calls, and Placement of Ads and Social Calls. Together, they were also found to significantly predict credibility (<em>F</em> = 12.986, <em>p</em> &lt; 0.001). The context regression model overall had a better fit than the content model which, along with issues of collinearity among the content indicators, lead us to believe more work is needed to differentiate the phenomena within specific indicators in the content category.</p>
      </section>
    </section>
    <section id="sec-29">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Discussion and Future Work</h2>
        </div>
      </header>
      <p>In this work, we outline a process for defining indicators of credibility and validating them through the collection of annotation data. The ability to quickly test new indicator definitions for both reliable annotation and correlation with expert-defined credibility will be important as we continue to scale to more indicators, more articles and other content, and more diverse annotators.</p>
      <p>From going through this process with a focused set of indicators and articles, we found several indicators that show reliability and correlation with domain expert scores of credibility, such as the presence of a clickbait title or the accurate representation of sources cited in the article. We also obtained findings that suggest certain indicators are less useful, such as the presence or spammy nature of advertising and social calls. Finally, we received feedback on the importance of indicators towards modeling expert credibility, which helps determine indicators that may be redundant or more or less predictive. This initial foray additionally allowed us to examine the distinction between content versus context indicators and the training and annotation interfaces required for accurate assessment of each. We found areas that may be out of reach for non-expert annotators, such as inferring types of claims, or that require more training or technical tools for lateral searching, such as assessing the reputation of citations. Looking forward, one can imagine different annotation strategies for different indicators based on these findings, with some fully or partially automatically captured, some annotated by experts or publishers, and some surfaced by the crowd or one's immediate trust network.</p>
      <p>In terms of immediate future steps, we aim to scale up our work to 5,000 to 10,000 annotated articles across a range of topics, styles, and publications, and work with researchers and web platform representatives to put this data to use towards building models of credibility that are both interpretable and robust to manipulation. In order to ensure the sustainability and inclusivity of our work as we continue to define credibility standards, we have formed the W3C Credible Web Community Group<a class="fn" href="#fn16" id="foot-fn16"><sup>16</sup></a>, first introduced as a session at W3C's Technical Plenary/Advisory Committee meeting<a class="fn" href="#fn17" id="foot-fn17"><sup>17</sup></a> in 2017.</p>
      <p>Our efforts are also aimed at improving media literacy and shrinking gaps of understanding between domain expertise and public knowledge. Through our work and outreach, we aim to convey how credibility is a negotiation among communicants&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>], where publishers and authors seek to convey credibility while readers and platforms seek to ascertain it. Greater, richer communication, understanding of dependencies between communicants, and tools to improve the transfer of information are necessary towards reducing the spread of misinformation. Along these lines, our work raises more long-term research questions that we aim to explore.</p>
      <p><strong>Indicator Resilience.</strong> Analogous to anti-spam efforts, the usefulness of automated credibility assessments may vary dramatically depending on the motivation and resources of the misinformation propagators. A nation-state actor with a geopolitical strategy may be harder to dissuade than financially motivated “fake news” creators. On the other hand, some indicators, such as raw number of ads or fact-checks from IFCN-verified signatories, may be more difficult to manipulate. Ultimately, we believe that the ability to compare the resilience of indicators is important in the context of increasingly machine-driven information landscapes.</p>
      <p><strong>Journalistic Practice</strong>. Also key is that the indicators are not just a tool for detecting misinformation but also the quality of information itself. For instance, recent efforts by Facebook to limit content with a high degree of clickbait suggests simple ways to improve quality&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>]. Annotators looking at content indicators found logical fallacies and incorrect use of causal claims even among some highly reputable news sources. We believe there is potential for the indicators to help improve standards for mainstream journalism, whether through custom tools or as a training methodology.</p>
      <p><strong>Media Literacy</strong>. Recent work from the Pew Research Center shows that more than half of adults think that “training in the digital realm would help them when it comes to accessing information that can aid in making decisions”&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>]. In our study, we found that annotators changed how they approached new articles as the process went on, and we also saw changes in their credibility scores after annotation that aligned better with experts. Indeed, many of our context indicators are designed to map to existing processes for fact checking, such as reading laterally&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0051">51</a>] and going “upstream” to find the source of a claim&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>]. Likewise, the ability to employ critical thinking or pick up on misleading language allows readers to reject misinformation that takes advantage of psychological biases. Going forward, we aim to develop a set of training materials so that anyone can get involved in annotation. We also will display all collected annotations on our website using Hypothes.is<a class="fn" href="#fn18" id="foot-fn18"><sup>18</sup></a>, a web annotation platform, for the public to be able to inspect the annotations in our dataset in context of the articles.</p>
      <p><strong>Freedom of Expression</strong>. How can attempts to detect and curb misinformation online meaningfully differ from efforts to censor the internet? The weaponization of “fake news” by autocratic countries already demonstrates the risks here: in a context where political leaders aim to centralize their control over the truth, determining blanket falsehood becomes a strategy of state control. In this regard, transparency presents a double-edged sword. As described earlier, it creates incentives for innovations in manipulation by agents of disinformation. At the same time, transparency helps reveal how annotators arrived at their conclusions about these indicators. We seek to study how greater transparency in indicators, by enabling the ability to share clear processes and findings, can help strike a balance between improving the health of our information ecosystem while preserving basic principles of free speech.</p>
    </section>
    <section id="sec-30">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Limitations</h2>
        </div>
      </header>
      <p>There are limits to the potential effects of these indicators, and understanding their applicability is important. Even if we are successful in curbing some of the psychological foundations of misinformation, such as frequency of exposure, more work is needed to fully address the many social and identity-related motivations for believing misinformation. These indicators were developed in the US and UK contexts and may not be applicable to other languages and parts of the world. As mentioned earlier, we focus on articles for this work but aim to expand to images, video, and other digital multimedia in the future. Additionally, digital initiatives will need to also consider the wider information ecosystem that includes television and talk radio.</p>
    </section>
    <section id="sec-31">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span> Conclusion</h2>
        </div>
      </header>
      <p>In this work, we presented a set of 16 indicators of article credibility, focused on article content as well as external sources and article metadata, refined over several months by a diverse coalition of media experts. We also presented a process for gathering annotations of these credibility indicators, including platform design and annotator recruitment, as well as an initial dataset of 40 articles annotated by 6 trained annotators and scored by domain experts. From analyzing our data, we isolated indicators that are reliably annotated across articles and that correlate with domain experts. Finally, we described the broader initiative of creating a set of standards around content credibility, of which this project is a part, as well as future directions for research.</p>
    </section>
    <section id="sec-32">
      <header>
        <div class="title-info">
          <h2><span class="section-number">9</span> Acknowledgements</h2>
        </div>
      </header>
      <p>This paper would not be possible without the valuable support and feedback of members of the Credibility Coalition, who have joined in-person meetings, weekly calls, and daily Slack chats to generously contribute their time, effort, and thinking to this project. There are too many to thank in the space we have, and we have included acknowledgments at <a class="link-inline force-break" href="http://www.credibilitycoalition.org">www.credibilitycoalition.org</a>.</p>
      <p>=0mu plus 1mu</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Jason Abbruzzese. 2017. Facebook is going to do something about those terrible ads on your website. (May 2017). <a class="link-inline force-break" href="http://mashable.com/2017/05/10/facebook-crackdown-bad-ads-news-feed/" target="_blank">http://mashable.com/2017/05/10/facebook-crackdown-bad-ads-news-feed/</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">Ahmer Arif, John&nbsp;J Robinson, Stephanie&nbsp;A Stanek, Elodie&nbsp;S Fichet, Paul Townsend, Zena Worku, and Kate Starbird. 2017. A Closer Look at the Self-Correcting Crowd: Examining Corrections in Online Rumors. <em>In <em>Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 155–168.</li>
        <li id="BibPLXBIB0003" label="[3]">Ahmer Arif, Kelley Shanahan, Fang-Ju Chou, Yoanna Dosouto, Kate Starbird, and Emma&nbsp;S Spiro. 2016. How information snowballs: Exploring the role of exposure in online rumor propagation. <em>In <em>Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 466–477.</li>
        <li id="BibPLXBIB0004" label="[4]">Leticia Bode and Emily&nbsp;K Vraga. 2015. In related news, that was wrong: The correction of misinformation through related stories functionality in social media. <em><em>Journal of Communication</em></em> 65, 4 (2015), 619–638.</li>
        <li id="BibPLXBIB0005" label="[5]">David&nbsp;B. Buller and Judee&nbsp;K. Burgoon. 1996. Interpersonal Deception Theory. <em><em>Communication Theory</em></em> 6, 3 (1996), 203–242. <a class="link-inline force-break" href="https://doi.org/10.1111/j.1468-2885.1996.tb00127.x" target="_blank">https://doi.org/10.1111/j.1468-2885.1996.tb00127.x</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on twitter. <em>In <em>Proceedings of the 20th international conference on World wide web</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 675–684.</li>
        <li id="BibPLXBIB0007" label="[7]">Michael&nbsp;A. Caulfield. 2017. Go Upstream to the Find the Source. (Jan 2017). <a class="link-inline force-break" href="https://webliteracy.pressbooks.com/chapter/go-upstream-to-find-the-source/" target="_blank">https://webliteracy.pressbooks.com/chapter/go-upstream-to-find-the-source/</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Yimin Chen, Niall&nbsp;J Conroy, and Victoria&nbsp;L Rubin. 2015. Misleading online content: Recognizing clickbait as false news. <em>In <em>Proceedings of the 2015 ACM on Workshop on Multimodal Deception Detection</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 15–19.</li>
        <li id="BibPLXBIB0009" label="[9]">Giovanni&nbsp;Luca Ciampaglia, Prashant Shiralkar, Luis&nbsp;M. Rocha, Johan Bollen, Filippo Menczer, and Alessandro Flammini. 2015. Computational Fact Checking from Knowledge Networks. <em><em>PLOS ONE</em></em> 10, 6 (06 2015), 1–13. <a class="link-inline force-break" href="https://doi.org/10.1371/journal.pone.0128193" target="_blank">https://doi.org/10.1371/journal.pone.0128193</a>
        </li>
        <li id="BibPLXBIB0010" label="[10]">Andrew&nbsp;J Flanagin and Miriam&nbsp;J Metzger. 2000. Perceptions of Internet information credibility. <em><em>Journalism &amp; Mass Communication Quarterly</em></em> 77, 3 (2000), 515–540.</li>
        <li id="BibPLXBIB0011" label="[11]">William&nbsp;B. Frakes. 1986. Information and misinformation: An investigation of the notions of information, misinformation, informing, and misinforming. <em><em>Journal of the American Society for Information Science</em></em> 37, 1(1986), 48–49. <a class="link-inline force-break" href="https://doi.org/10.1002/(SICI)1097-4571(198601)37:1%3C48::AID-ASI10%3E3.0.CO;2-3" target="_blank">https://doi.org/10.1002/(SICI)1097-4571(198601)37:1&lt;48::AID-ASI10&gt;3.0.CO;2-3</a>
        </li>
        <li id="BibPLXBIB0012" label="[12]">William&nbsp;K. Frankena. 1939. The naturalistic fallacy. <em><em>Mind</em></em> 48, 192, Article 4 (1939), 13&nbsp;pages. <a class="link-inline force-break" href="https://doi.org/10.1093/mind/XLVIII.192.464" target="_blank">https://doi.org/10.1093/mind/XLVIII.192.464</a>
        </li>
        <li id="BibPLXBIB0013" label="[13]">Daniel Funke. 2017. It's been a year since Facebook partnered with fact-checkers. How's it going? (Dec. 2017). <a class="link-inline force-break" href="http://delivery.acm.org/10.1145/3190000/3188731/Retrieved%20January%205,%202018%20from%20https://www.poynter.org/news/its-been-year-facebook-partnered-fact-checkers-hows-it-going" target="_blank">Retrieved January 5, 2018 from https://www.poynter.org/news/its-been-year-facebook-partnered-fact-checkers-hows-it-going</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Cecilie Gaziano and Kristin McGrath. 1986. Measuring the concept of credibility. <em><em>Journalism quarterly</em></em> 63, 3 (1986), 451–462.</li>
        <li id="BibPLXBIB0015" label="[15]">lucas graves and Tom glaisyer. 2012. <em><em>The Fact-Checking Universe in Spring 2012: An Overview</em></em> . The New America Foundation, Washington, DC, USA. <a class="link-inline force-break" href="https://www.issuelab.org/resource/the-fact-checking-universe-in-spring-2012-an-overview.html" target="_blank">https://www.issuelab.org/resource/the-fact-checking-universe-in-spring-2012-an-overview.html</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Jennifer&nbsp;D. Greer. 2003. Evaluating the Credibility of Online Information: A Test of Source and Advertising Influence. <em><em>Mass Communication and Society</em></em> 6, 1 (2003), 11–28. <a class="link-inline force-break" href="https://doi.org/10.1207/S15327825MCS0601_3" target="_blank">https://doi.org/10.1207/S15327825MCS0601_3</a>
        </li>
        <li id="BibPLXBIB0017" label="[17]">Manish Gupta, Peixiang Zhao, and Jiawei Han. 2012. Evaluating event credibility on twitter. <em>In <em>Proceedings of the 2012 SIAM International Conference on Data Mining</em></em> . SIAM, SIAM, 3600 Market Street, 6th Floor | Philadelphia, PA 19104-2688 USA, 153–164.</li>
        <li id="BibPLXBIB0018" label="[18]">John&nbsp;B. Horrigan and John Gramlich. 2017. Many Americans, especially blacks and Hispanics, are hungry for help as they sort through information. (Nov 2017). <a class="link-inline force-break" href="http://www.pewresearch.org/fact-tank/2017/11/29/many-americans-especially-blacks-and-hispanics-are-hungry-for-help-as-they-sort-through-information/" target="_blank">http://www.pewresearch.org/fact-tank/2017/11/29/many-americans-especially-blacks-and-hispanics-are-hungry-for-help-as-they-sort-through-information/</a>
        </li>
        <li id="BibPLXBIB0019" label="[19]">IREX.org. 2017. Ukrainians’ self-defense against disinformation: What we learned from Learn to Discern. (June 2017). <a class="link-inline force-break" href="http://delivery.acm.org/10.1145/3190000/3188731/Retrieved%20January%205,%202018%20from%20https://www.irex.org/insight/ukrainians-self-defense-against-disinformation-what-we-learned-learn-discern" target="_blank">Retrieved January 5, 2018 from https://www.irex.org/insight/ukrainians-self-defense-against-disinformation-what-we-learned-learn-discern</a>
        </li>
        <li id="BibPLXBIB0020" label="[20]">Alice Marwick and Rebecca Lewis. 2017. <em><em>Media Manipulation and Disinformation Online</em></em> . Report. Data &amp; Society Research Institute. <a class="link-inline force-break" href="https://edoc.coe.int/en/media-freedom/7495-information-disorder-%20toward-an-interdisciplinary-framework-for-research-and-policy-making.html" target="_blank">https://edoc.coe.int/en/media-freedom/7495-information-disorder- toward-an-interdisciplinary-framework-for-research-and-policy-making.html</a>
        </li>
        <li id="BibPLXBIB0021" label="[21]">Aaron&nbsp;M. McCright and Riley&nbsp;E. Dunlap. 2011. The Politicization of Climate Change and Polarization in the American Public's Views of Global Warming, 2001–2010. <em><em>Sociological Quarterly</em></em> 52, 2 (2011), 155–194. <a class="link-inline force-break" href="https://doi.org/10.1111/j.1533-8525.2011.01198.x" target="_blank">https://doi.org/10.1111/j.1533-8525.2011.01198.x</a>
        </li>
        <li id="BibPLXBIB0022" label="[22]">Marcelo Mendoza, Barbara Poblete, and Carlos Castillo. 2010. Twitter Under Crisis: Can we trust what we RT?. <em>In <em>Proceedings of the first workshop on social media analytics</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 71–79.</li>
        <li id="BibPLXBIB0023" label="[23]">Panagiotis&nbsp;Takas Metaxas, Samantha Finn, and Eni Mustafaraj. 2015. Using twittertrails.com to investigate rumor propagation. <em>In <em>Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work &amp; Social Computing</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 69–72.</li>
        <li id="BibPLXBIB0024" label="[24]">Miriam&nbsp;J. Metzger, Andrew&nbsp;J. Flanagin, Keren Eyal, Daisy&nbsp;R. Lemus, and Robert&nbsp;M. Mccann. 2003. Credibility for the 21st Century: Integrating Perspectives on Source, Message, and Media Credibility in the Contemporary Media Environment. <em><em>Annals of the International Communication Association</em></em> 27, 1(2003), 293–335. <a class="link-inline force-break" href="https://doi.org/10.1080/23808985.2003.11679029" target="_blank">https://doi.org/10.1080/23808985.2003.11679029</a>
        </li>
        <li id="BibPLXBIB0025" label="[25]">Hans&nbsp;K Meyer, Doreen Marchionni, and Esther Thorson. 2010. The journalist behind the news: credibility of straight, collaborative, opinionated, and blogged “news”. <em><em>American Behavioral Scientist</em></em> 54, 2 (2010), 100–119.</li>
        <li id="BibPLXBIB0026" label="[26]">Tanushree Mitra and Eric Gilbert. 2015. CREDBANK: A Large-Scale Social Media Corpus With Associated Credibility Annotations. <em>In <em>Ninth International AAAI Conference on Web and Social Media</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., Article 10582, 10&nbsp;pages.</li>
        <li id="BibPLXBIB0027" label="[27]">Meredith&nbsp;Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing?: understanding microblog credibility perceptions. <em>In <em>Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 441–450.</li>
        <li id="BibPLXBIB0028" label="[28]">Lucia Moses. 2016. ’The underbelly of the internet’: How content ad networks fund fake news. (Nov. 2016). <a class="link-inline force-break" href="http://delivery.acm.org/10.1145/3190000/3188731/Retrieved%20January%205,%202018%20from%20https://digiday.com/media/underbelly-internet-fake-news-gets-funded/" target="_blank">Retrieved January 5, 2018 from https://digiday.com/media/underbelly-internet-fake-news-gets-funded/</a>
        </li>
        <li id="BibPLXBIB0029" label="[29]">Ryosuke Nagura, Yohei Seki, Noriko Kando, and Masaki Aono. 2006. A Method of Rating the Credibility of News Documents on the Web. <em>In <em>Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> (<em>SIGIR ’06</em>). ACM, New York, NY, USA, Article 1148316, 2 pages. <a class="link-inline force-break" href="https://doi.org/10.1145/1148170.1148316" target="_blank">https://doi.org/10.1145/1148170.1148316</a>
        </li>
        <li id="BibPLXBIB0030" label="[30]">Daniel&nbsp;J O'keefe. 2002. <em><em>Persuasion: Theory and research</em></em> . Vol.&nbsp;2. Sage, Los Angeles, CA.</li>
        <li id="BibPLXBIB0031" label="[31]">Will Oremus. 2016. Only You Can Stop the Spread of Fake News. http://www.slate.com. (December 2016).</li>
        <li id="BibPLXBIB0032" label="[32]">Gordon Pennycook and David&nbsp;G Rand. 2017. <em><em>Assessing the effect of “disputed” warnings and source salience on perceptions of fake news accuracy</em></em> . Technical Report. SSRN. <a class="link-inline force-break" href="http://dx.doi.org/10.2139/ssrn.3035384" target="_blank">http://dx.doi.org/10.2139/ssrn.3035384</a>
        </li>
        <li id="BibPLXBIB0033" label="[33]">Peter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. <em>In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 1505–1508.</li>
        <li id="BibPLXBIB0034" label="[34]">Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum. 2016. Credibility assessment of textual claims on the web. <em>In <em>Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 2173–2178.</li>
        <li id="BibPLXBIB0035" label="[35]">Martin Potthast, Sebastian Köpsel, Benno Stein, and Matthias Hagen. 2016. Clickbait detection. <em>In <em>European Conference on Information Retrieval</em></em> . Springer, Springer International Publishing, Gewerbestrasse 11, 6330 Cham, Switzerland, 810–817.</li>
        <li id="BibPLXBIB0036" label="[36]">Aditya Ranganathan, Daniel Kim, Nick Adams, and Saul&nbsp;Perlmutter et al. 2017. <em><em>Crowdsourcing Credibility: A Citizen-Science Approach to NewsLiteracy via Public Editor</em></em> . Technical Report. University of Berkeley. <a class="link-inline force-break" href="https://northwestern.app.box.com/s/77ekftnfp0w8ixxkivkgodqubwhaumyv" target="_blank">https://northwestern.app.box.com/s/77ekftnfp0w8ixxkivkgodqubwhaumyv</a>
        </li>
        <li id="BibPLXBIB0037" label="[37]">Hannah Rashkin, Eunsol Choi, Jin&nbsp;Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. <em>In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em></em> . ACL, 209 N. Eighth Street, Stroudsburg PA 18360, USA, 2921–2927.</li>
        <li id="BibPLXBIB0038" label="[38]">Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno Gonçalves, Snehal Patil, Alessandro Flammini, and Filippo Menczer. 2011. Truthy: mapping the spread of astroturf in microblog streams. <em>In <em>Proceedings of the 20th international conference companion on World wide web</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 249–252.</li>
        <li id="BibPLXBIB0039" label="[39]">Marta Reacasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic Models for Analyzing and Detecting Biased Language. <em>In <em>51st Annual Meeting of the Association for Computational Linguistics</em></em> . ACL, ACL, 209 N. Eighth Street, Stroudsburg PA 18360, USA, 1650–1659.</li>
        <li id="BibPLXBIB0040" label="[40]">Paul Resnick, Samuel Carton, Souneil Park, Yuncheng Shen, and Nicole Zeffer. 2014. Rumorlens: A system for analyzing the impact of rumors and corrections in social media. <em>In <em>Proc. Computational Journalism Conference</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 5.</li>
        <li id="BibPLXBIB0041" label="[41]">Soo&nbsp;Young Rieh and David&nbsp;R. Danielson. 2007. Credibility: A multidisciplinary framework. <em><em>Annual Review of Information Science and Technology</em></em> 41, 1(2007), 307–364. <a class="link-inline force-break" href="https://doi.org/10.1002/aris.2007.1440410114" target="_blank">https://doi.org/10.1002/aris.2007.1440410114</a>
        </li>
        <li id="BibPLXBIB0042" label="[42]">Christine Schmidt. 2017. This project aims to “de-flatten” digital publishing by matching the best content with premium ads. (Nov 2017). <a class="link-inline force-break" href="http://www.niemanlab.org/2017/11/this-project-aims-to-de-flatten-digital-publishing-by-matching-the-best-content-with-premium-ads/" target="_blank">http://www.niemanlab.org/2017/11/this-project-aims-to-de-flatten-digital-publishing-by-matching-the-best-content-with-premium-ads/</a>
        </li>
        <li id="BibPLXBIB0043" label="[43]">Mike Schmierbach and Anne Oeldorf-Hirsch. 2012. A little bird told me, so I didn't believe it: Twitter, credibility, and issue perceptions. <em><em>Communication Quarterly</em></em> 60, 3 (2012), 317–337.</li>
        <li id="BibPLXBIB0044" label="[44]">Per&nbsp;O Seglen. 1997. Why the impact factor of journals should not be used for evaluating research. <em><em>BMJ: British Medical Journal</em></em> 314, 7079 (1997), 498.</li>
        <li id="BibPLXBIB0045" label="[45]">Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. Fake News Detection on Social Media: A Data Mining Perspective. <em><em>SIGKDD Explor. Newsl.</em></em> 19, 1, Article 3137600 (Sept. 2017), 15&nbsp;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/3137597.3137600" target="_blank">https://doi.org/10.1145/3137597.3137600</a>
        </li>
        <li id="BibPLXBIB0046" label="[46]">Henry Silverman and Lin Huang. 2017. News Feed FYI: Fighting Engagement Bait on Facebook. (Dec 2017). <a class="link-inline force-break" href="https://newsroom.fb.com/news/2017/12/news-feed-fyi-fighting-engagement-bait-on-facebook/" target="_blank">https://newsroom.fb.com/news/2017/12/news-feed-fyi-fighting-engagement-bait-on-facebook/</a>
        </li>
        <li id="BibPLXBIB0047" label="[47]">Kate Starbird. 2017. Examining the Alternative Media Ecosystem Through the Production of Alternative Narratives of Mass Shooting Events on Twitter.. <em>In <em>ICWSM</em></em> . ACM, ACM, 2 Penn Plaza, Suite 701, New York, NY 10121-0701., 230–239.</li>
        <li id="BibPLXBIB0048" label="[48]">S&nbsp;Shyam Sundar. 1998. Effect of source attribution on perception of online news stories. <em><em>Journalism &amp; Mass Communication Quarterly</em></em> 75, 1 (1998), 55–68.</li>
        <li id="BibPLXBIB0049" label="[49]">Lauren Vogel. 2017. Viral misinformation threatens public health. <em><em>Canadian Medical Association Journal</em></em> 189, 50, Article E1567 (Dec 2017), 1&nbsp;pages. <a class="link-inline force-break" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5738254/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5738254/</a>
        </li>
        <li id="BibPLXBIB0050" label="[50]">Claire Wardle and Hossein Derakhshan. 2017. <em><em>Information disorder: Toward an interdisciplinary framework for research and policy making</em></em> . Coucil of Europe Report DGI(2017)09. Council of Europe. <a class="link-inline force-break" href="https://edoc.coe.int/en/media-freedom/7495-information-disorder-%20toward-an-interdisciplinary-framework-for-research-and-policy-making.html" target="_blank">https://edoc.coe.int/en/media-freedom/7495-information-disorder- toward-an-interdisciplinary-framework-for-research-and-policy-making.html</a>
        </li>
        <li id="BibPLXBIB0051" label="[51]">Sam Wineburg and Sarah McGrew. 2017. <em><em>Lateral Reading: Reading Less and Learning More When Evaluating Digital Information</em></em> . Technical Report Working Paper No. 2017-A1. Stanford History Education Group. <a class="link-inline force-break" href="http://dx.doi.org/10.2139/ssrn.3048994" target="_blank">http://dx.doi.org/10.2139/ssrn.3048994</a>
        </li>
        <li id="BibPLXBIB0052" label="[52]">You Wu, Pankaj&nbsp;K Agarwal, Chengkai Li, Jun Yang, and Cong Yu. 2017. Computational Fact Checking through Query Perturbations. <em><em>ACM Transactions on Database Systems (TODS)</em></em> 42, 1 (2017), 4.</li>
        <li id="BibPLXBIB0053" label="[53]">Kenneth&nbsp;C.C. Yang. 2007. Factors influencing Internet users’ perceived credibility of news-related blogs in Taiwan. <em><em>Telematics and Informatics</em></em> 24, 2 (2007), 69 – 85. <a class="link-inline force-break" href="https://doi.org/10.1016/j.tele.2006.04.001" target="_blank">https://doi.org/10.1016/j.tele.2006.04.001</a>
        </li>
        <li id="BibPLXBIB0054" label="[54]">Wenlin Yao, Zeyu Dai, Ruihong Huang, and James Caverlee. 2017. Online Deception Detection Refueled by Real World Data Collection. <em>In <em>Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017</em></em> . INCOMA Ltd., Varna, Bulgaria, 793–802. <a class="link-inline force-break" href="https://doi.org/10.26615/978-954-452-049-6_102" target="_blank">https://doi.org/10.26615/978-954-452-049-6_102</a>
        </li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>The Trust Project: <a class="link-inline force-break" href="https://thetrustproject.org">https://thetrustproject.org</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Credibility Coalition: <a class="link-inline force-break" href="http://credibilitycoalition.org">http://credibilitycoalition.org</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>W3C Credentials Community Group: <a class="link-inline force-break" href="https://www.w3.org/community/credentials/">https://www.w3.org/community/credentials/</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>MisinfoCon: <a class="link-inline force-break" href="https://misinfocon.com">https://misinfocon.com</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>Climate Feedback process: <a class="link-inline force-break" href="https://climatefeedback.org/process/">https://climatefeedback.org/process/</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>Mozilla Festival (MozFest), London, Oct 2017: <a class="link-inline force-break" href="https://mozillafestival.org">https://mozillafestival.org</a></p>
    <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a>IPTC, Barcelona, Nov 2017: <a class="link-inline force-break" href="https://iptc.org">https://iptc.org</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>International Fact-Checking Network (IFCN): <a class="link-inline force-break" href="https://www.poynter.org/international-fact-checking-network-fact-checkers-code-principles">https://www.poynter.org/international-fact-checking-network-fact-checkers-code-principles</a></p>
    <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a>BuzzSumo: <a class="link-inline force-break" href="http://buzzsumo.com">http://buzzsumo.com</a></p>
    <p id="fn10"><a href="#foot-fn10"><sup>10</sup></a>Webpage archiving tool: <a class="link-inline force-break" href="http://archive.is">http://archive.is</a></p>
    <p id="fn11"><a href="#foot-fn11"><sup>11</sup></a>SSS course at Berkeley: <a class="link-inline force-break" href="http://sensesensibilityscience.com">http://sensesensibilityscience.com</a></p>
    <p id="fn12"><a href="#foot-fn12"><sup>12</sup></a>Meedan: <a class="link-inline force-break" href="https://meedan.com">https://meedan.com</a></p>
    <p id="fn13"><a href="#foot-fn13"><sup>13</sup></a>For details on the TextThresher software: <a class="link-inline force-break" href="http://www.goodlylabs.org/research/">http://www.goodlylabs.org/research/</a></p>
    <p id="fn14"><a href="#foot-fn14"><sup>14</sup></a>Check: <a class="link-inline force-break" href="https://meedan.com/en/check/">https://meedan.com/en/check/</a></p>
    <p id="fn15"><a href="#foot-fn15"><sup>15</sup></a>Coalition for Better Ads: <a class="link-inline force-break" href="https://www.betterads.org/standards/">https://www.betterads.org/standards/</a></p>
    <p id="fn16"><a href="#foot-fn16"><sup>16</sup></a>W3C Credible Web Community Group: <a class="link-inline force-break" href="https://www.w3.org/community/credibility/">https://www.w3.org/community/credibility/</a></p>
    <p id="fn17"><a href="#foot-fn17"><sup>17</sup></a>W3C TPAC: <a class="link-inline force-break" href="https://www.w3.org/2017/11/TPAC/">https://www.w3.org/2017/11/TPAC/</a></p>
    <p id="fn18"><a href="#foot-fn18"><sup>18</sup></a>Hypothes.is: <a class="link-inline force-break" href="https://web.hypothes.is/">https://web.hypothes.is/</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3188731">https://doi.org/10.1145/3184558.3188731</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
