<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Automatic Hierarchical Table of Contents Generation for
  Educational Videos</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186336'>https://doi.org/10.1145/3184558.3186336</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186336'>https://w3id.org/oa/10.1145/3184558.3186336</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Automatic Hierarchical Table of
          Contents Generation for Educational Videos</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Debabrata</span> <span class=
          "surName">Mahapatra</span> Videoken, Bangalore, India,
          <a href=
          "mailto:debabrata.mahapatra@videoken.com">debabrata.mahapatra@videoken.com</a>
        </div>
        <div class="author">
          <span class="givenName">Ragunathan</span> <span class=
          "surName">Mariappan</span> School of Computing National
          University of Singapore, <a href=
          "mailto:mragunathan@nus.edu.sg">mragunathan@nus.edu.sg</a>
        </div>
        <div class="author">
          <span class="givenName">Vaibhav</span> <span class=
          "surName">Rajan</span> School of Computing National
          University of Singapore, <a href=
          "mailto:vaibhav.rajan@nus.edu.sg">vaibhav.rajan@nus.edu.sg</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186336"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186336</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The number of freely available online educational
        videos from universities and other organizations is growing
        rapidly. Accurate indexing and summarization are essential
        for efficient search, recommendation and effective
        consumption of videos. In this paper, we describe a new
        method of automatically creating a hierarchical table of
        contents for a video. It provides a summary of the video
        content along with a textbook–like facility for nonlinear
        navigation and search through the video. Our multimodal
        approach combines new methods for shot level video
        segmentation and for hierarchical summarization. Empirical
        results demonstrate the efficacy of our approach on many
        educational videos.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Computing methodologies</strong>
        → <strong>Information extraction;</strong> <strong>Video
        summarization;</strong> <strong>Video
        segmentation;</strong> • <strong>Applied computing</strong>
        → <em>Education;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Table of Contents; Shot
          segmentation; Text Summarization; Tree
          knapsack</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Debabrata Mahapatra, Ragunathan Mariappan, and Vaibhav
          Rajan. 2018. Automatic Hierarchical Table of Contents
          Generation for Educational Videos. In <em>WWW '18
          Companion: The 2018 Web Conference Companion,</em>
          <em>April 23–27, 2018 (WWW ’18 Companion),</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 8 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186336" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186336</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Massive Open Online Courses (MOOCs) and other online
      learning resources have added many good quality educational
      videos on the Internet. The number, already in tens of
      thousands, is increasing by the day. Tools for search and
      recommendation are therefore indispensable for finding
      relevant content online. Search tools, in turn, rely on
      effective indexing and summarization based on metadata that
      describe the video content. Such metadata is usually manually
      created, for example through tags and titles of videos, which
      cannot scale and is often inaccurate.</p>
      <p>Lecture recordings are often long videos with duration of
      upto several hours. Learners may be interested in only
      certain topics in the video or may need to review specific
      sections within the video. Both these tasks can be
      facilitated through a <strong>Table of Contents
      (ToC)</strong> that can lead the learner directly to the
      relevant section in the video. A well constructed ToC can
      itself provide both a summary as well as metadata for
      effective search in video databases. Despite the trend
      towards short videos in MOOCs, there still exists a large
      number of previously recorded long lectures. Moreover some
      presentations (e.g. in classrooms) or certain topics (e.g.
      long proofs) may continue to remain long videos for which a
      ToC would be a useful summarization and indexing tool.</p>
      <p>Robust methods for Optical Character Recognition (OCR) are
      available [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0032">32</a>]
      to extract visual words from frames in a video. Tools for
      Automatic Speech Recognition (ASR) are also rapidly improving
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>]. The
      recognition accuracy of both these systems depends on video
      characteristics like image resolution, spoken language and
      accent used etc. but even with reasonably good outputs, there
      remains the problem of organizing the extracted information
      to form a coherent summary of the video. In this paper we
      develop a method of automatically creating a hierarchical
      table of contents for an educational video, focusing on two
      key aspects: video segmentation and hierarchical ToC
      creation.</p>
      <p>Video segmentation is relatively easier when shot
      boundaries have abrupt transitions. The problem is harder for
      educational videos that typically have gradual transitions
      between shots. We design a segmentation method that uses a
      new content representation to capture visual shape
      information and a new signal construction that can detect
      both gradual and abrupt transitions. Moreover, in contrast to
      previous methods that use various similarity metrics between
      frames and determine the shot boundaries when the similarity
      is below a fixed threshold, our method does not use a fixed
      threshold: it adaptively determines a threshold based on
      signal characteristics of the input video.</p>
      <p>Using the identified shots as the basic topical units, we
      develop a method that summarizes each shot and aggregates the
      summaries in a hierarchical manner to create a final
      hierarchical ToC for the video. Unlike previous extractive
      summarization methods that use text sources as inputs, our
      method infers dependency relationships from multi-modal,
      temporally dependent textual information extracted from
      videos. We develop a Tree Knapsack problem based formulation
      for generating the final ToC.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">First row shows color image frames (size
          240 × 352) extracted from the video UGS01.mpg obtained
          from TRECVID 2002 dataset [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0021">21</a>]; Second row images are
          corresponding Canny edge map <em>E<sub>c</sub></em> ;
          Third row shows the patch-wise entropy <em>S</em> (size
          29 × 43); Note that the noise in edge map from
          binarization and subtle changes in the shape of
          consecutive frames are overlooked in corresponding
          patch-wise entropy, while showing a significant change in
          the representation at the point of shot
          transition.</span>
        </div>
      </figure>
      <p></p>
      <p>To summarize our contributions in this paper are:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We develop the first method,
        called HMMToC, to automatically create a multi-modal
        hierarchical Table of Contents (ToC) for educational
        videos. The ToC provides a summary of the video and a
        textbook–like facility for nonlinear navigation and search
        through the video.<br /></li>
        <li id="list2" label="•">We develop a new method for
        shot–level segmentation of videos, that is used for ToC
        generation. Our threshold–free method has a new content
        representation and signal construction that can detect both
        gradual and abrupt shot transitions.<br /></li>
        <li id="list3" label="•">We empirically demonstrate that
        HMMToC is more accurate than previous (non-hierarchical)
        methods in terms of both obtaining the section title and
        the timing of the title in the generated Table of
        Contents.<br /></li>
      </ul>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Overall
          Design</h2>
        </div>
      </header>
      <p>A video is a multidimensional time-series signal
      <span class="inline-equation"><span class="tex">$V: \mathbb
      {T}\rightarrow \mathbb {N}^{h\times w \times
      c}$</span></span> , where <span class=
      "inline-equation"><span class="tex">$\mathbb {T}=\lbrace
      1\cdots T\rbrace$</span></span> is the set of discrete time
      points, with <em>h</em>, <em>w</em> and <em>c</em> being the
      height, width and number of color channels of each frame. In
      general, a video is comprised of many shots. A
      <strong>shot</strong> is a group of temporally adjacent
      frames, which are generated from an uninterrupted camera
      capture. A <strong>shot boundary</strong> is a break in the
      continuum of the video signal.</p>
      <p>Our ToC generation algorithm consists of three key
      steps.</p>
      <ol class="list-no-style">
        <li id="list4" label="(1)"><em>Segmentation and Key-frame
        Extraction.</em>The first step is the identification of the
        shot boundaries and segmentation of the input video into
        multiple shots. From multiple frames within each shot, we
        select representative
        <strong>key–frames</strong>.<br /></li>
        <li id="list5" label="(2)"><em>Information
        Extraction.</em>Second, we obtain textual information and
        associated metadata from the selected key–frames as well as
        from the audio transcript of the video.<br /></li>
        <li id="list6" label="(3)"><em>Table of Contents
        Generation.</em>In the final step, a summary for each shot
        is constructed using previously extracted information and
        all the summaries are aggregated to create a hierarchical
        ToC.<br /></li>
      </ol>
      <p>In the following sections, these three steps are described
      in detail.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Segmentation
          and Key-frame Extraction</h2>
        </div>
      </header>
      <p>In this section we describe the first key step of our ToC
      generation method: identification of shot boundaries and
      selection of representative key–frames.</p>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Shot
            Boundary Detection (SBD)</h3>
          </div>
        </header>
        <p>Based on the type of transition between two adjacent
        shots, shot boundaries (SB) can broadly be classified into
        two categories: (1) <em>Abrupt Transition</em>, where the
        change is sudden, and (2) <em>Gradual Transition</em>,
        where the change occurs smoothly. The latter has
        subcategories, such as <em>desolve, fade in&nbsp;out,
        wipe</em> etc. [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>].</p>
        <p>We describe our method using the framework of Cotsaces
        <em>et al.</em> for shot boundary detection [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0007">7</a>], comprising of three
        steps:</p>
        <ol class="list-no-style">
          <li id="list7" label="(1)">Visual Content
          Representation<br /></li>
          <li id="list8" label="(2)">Continuous Signal
          Construction, and<br /></li>
          <li id="list9" label="(3)">Classification of Shot
          Boundaries.<br /></li>
        </ol>
        <p>We design a novel content representation that captures
        information about shape of the objects in an image. We
        then, on a video level apply a novel method to construct
        multiple continuous signals at several time-resolutions to
        detect both types of shot transitions. Finally, for
        classification, where the SBs are chosen from the initial
        <em>T</em> − 1 frame boundaries, we design an adaptive
        method for finding a global threshold.</p>
        <section id="sec-8">
          <p><em>3.1.1 Visual Content Representation:.</em> Major
          variations in the video content are due to color and
          shape changes of objects that could be either in the
          foreground or background. Our novelty lies in a shape
          representation that is sensitive to major structural
          changes among neighboring frames, while maintaining
          invariability to subtle variations occurring by factors
          like motion, deformations, lighting conditions etc.</p>
          <p>Shape information can be captured by the edge image of
          the frame. In general, for faster computation, Canny edge
          detection algorithm is used to obtain the binary edge map
          <em>E<sub>c</sub></em> . However, <em>E<sub>c</sub></em>
          is sensitive to small variations in the object. To gain
          invariance, we compute the entropy of
          <em>E<sub>c</sub></em> in a patch-wise manner. Denoting
          <span class="inline-equation"><span class="tex">$\mathcal
          {P}$</span></span> as the set of patches extracted from
          <em>E<sub>c</sub></em> ,</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} S(i) = -
              \left(p_0^i \log (p_0^i) + p_1^i \log (p_1^i)\right)
              \quad 1\le i\le |\mathcal {P}|
              \end{align}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>is the entropy of <em>i</em> <sup>th</sup> patch in
          <span class="inline-equation"><span class="tex">$\mathcal
          {P}$</span></span> , with <span class=
          "inline-equation"><span class="tex">$p_0^i$</span></span>
          and <span class="inline-equation"><span class=
          "tex">$p_1^i$</span></span> being the proportions of 0s
          and 1s in that patch. To further make the patch-wise
          entropy feature <em>S</em> more robust, we use
          overlapping patches. Figure <a class="fig" href=
          "#fig1">1</a> illustrates the invariance of <em>S</em> to
          subtle variations in <em>E<sub>c</sub></em> , while being
          sensitive to significant changes in the shape. This
          process can be interpreted as a pooling operation
          performed on the edge map. Edge based approaches have
          been used previously in the literature [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0015">15</a>,
          <a class="bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0029">29</a>], but
          not in the context of shape representation.
          <p></p>
          <p>We capture variations in color by representing it with
          the 2–dimensional (2d) histogram <em>H</em> of Hue and
          Saturation values, as used in [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0003">3</a>].</p>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">A portion of continuous
              signals for the video UGS01.mpg; ground truth labels
              are obtained from TRECVID dataset. The difference
              between <strong>s</strong> <sup>0</sup>(<em>t</em>)
              and <em>s<sub>m</sub></em> (<em>t</em>) is a clear
              indicator of when, and for how long, the gradual
              transition occurs. This is validated by juxtaposing
              the ground truth smooth boundaries (green) with the
              predicted ones (blue).</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-9">
          <p><em>3.1.2 Continuous Signal Construction:.</em> We
          denote the boundary between a pair of frame numbers
          (<em>l</em>, <em>r</em>) as <em>b</em>. To start with,
          there are <em>T</em> − 1 boundaries. Two adjacent
          boundaries <em>b</em> <sub>1</sub> = (<em>l</em>
          <sub>1</sub>, <em>r</em> <sub>1</sub>) and <em>b</em>
          <sub>2</sub> = (<em>l</em> <sub>2</sub>, <em>r</em>
          <sub>2</sub>), such that <em>l</em> <sub>2</sub> =
          <em>r</em> <sub>1</sub>, can be merged to form another
          boundary <strong>b</strong> = <em>b</em>
          <sub>1</sub>∪<em>b</em> <sub>2</sub> = (<em>l</em>
          <sub>1</sub>, <em>r</em> <sub>2</sub>). To detect whether
          a particular <em>b</em> represents a shot transition, we
          compute a measure that is designed to capture the
          resemblance between its <em>l</em> and <em>r</em> frames
          as</p>
          <div class="table-responsive" id="eq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mu (b) =
              \sigma _c(H_l, H_r) \times \sigma _s(S_l, S_r),
              \end{align}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>where <em>σ<sub>c</sub></em> and
          <em>σ<sub>s</sub></em> are similarity metrics for color
          and shape features respectively. We use cosine similarity
          for <em>σ<sub>s</sub></em> . For <em>σ<sub>c</sub></em> ,
          we use the metric suggested in [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0003">3</a>], i.e.
          cross correlation
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{align*} \sigma _c(H_l,
              H_r)\!=\!\frac{\sum _{i=1}^{N} \left(H_l(i) -
              \bar{H}_l \right) \left(H_r(i) - \bar{H}_r \right)
              }{\sqrt {\sum _{i=1}^{N} \left(H_l(i)\!-\!\bar{H}_l
              \right)^2 \sum _{i=1}^{N}
              \left(H_r(i)\!-\!\bar{H}_r\right)^2}},\end{align*}</span><br />
            </div>
          </div>where <em>N</em> is the total number of bins in the
          2d histogram and <span class=
          "inline-equation"><span class=
          "tex">$\bar{H}$</span></span> is the mean.
          <p></p>
          <p>We introduce a multi-resolution approach to construct
          signals for a video, which will be useful for detecting
          both sudden and gradual transitions. At every boundary
          point <em>b<sub>t</sub></em> , 1 ≤ <em>t</em> ≤
          <em>T</em> − 1, we compute the measure for several merged
          boundaries <span class="inline-equation"><span class=
          "tex">$\mathbf {b}_t^d, 0\le d\le w$</span></span> , such
          that</p>
          <div class="table-responsive" id="eq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathbf {b}_t^d
              = \bigcup _{i=-d}^d b_{t+i} = (l_{t-d}, r_{t+d}),
              \end{align}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>where <em>w</em> is the window size. Consolidating
          all the <span class="inline-equation"><span class=
          "tex">$\mu (\mathbf {b}_t^d)$</span></span> , we denote
          the <em>w</em> + 1 dimensional signal as
          <strong>s</strong>(<em>t</em>). It has been empirically
          shown in the literature, for example [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0013">13</a>], that
          by appropriately thresholding <strong>s</strong>
          <sup>0</sup>(<em>t</em>) =
          <em>μ</em>(<em>b<sub>t</sub></em> ), which is an
          uni-resolution approach, one can detect <em>cuts</em>,
          the sudden transitions, with high precision and recall.
          We leverage the extra information available in
          <strong>s</strong>(<em>t</em>) and construct another
          signal
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} s_m(t) = \min
              _{0 \le d \le w} \mathbf {s}^d(t)
              \end{align}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>that is indicative of the gradual transitions in a
          similar way as <strong>s</strong>
          <sup>0</sup>(<em>t</em>) is for <em>cuts</em>. This is
          illustrated in Figure <a class="fig" href="#fig2">2</a>.
          <p></p>
        </section>
        <section id="sec-10">
          <p><em>3.1.3 Classification of Shot Boundaries:.</em> The
          range of values that <strong>s</strong>(<em>t</em>) can
          take differs for different videos. So, the threshold for
          classifying whether a boundary is SB or not has to be
          adaptively decided. In our method, we use a clustering
          based technique to choose the threshold.</p>
          <p>For detecting the <em>cuts</em>, first we quantize the
          values of <strong>s</strong> <sup>0</sup>(<em>t</em>)
          into <em>K</em> clusters, <em>c</em> <sub>1</sub> &lt;
          <em>c</em> <sub>2</sub>⋅⋅⋅ &lt; <em>c<sub>K</sub></em> ,
          by using <em>K Nearest Neighbor</em> algorithm. Then we
          pick one particular <em>c<sub>k</sub></em> as threshold,
          such that the membership count from
          <em>c<sub>k</sub></em> to <em>c</em> <sub><em>k</em> +
          1</sub> increases significantly. After thresholding
          <strong>s</strong> <sup>0</sup>(<em>t</em>) with
          <em>c<sub>k</sub></em> and obtaining a binary signal, to
          precisely locate the <em>cut</em> shot boundaries, we
          perform <em>run length encoding</em> on it. Start and end
          points of the runs of 1s are classified as the
          corresponding <em>l</em>’s and <em>r</em>’s of the shot
          boundaries.</p>
          <p>To detect gradual transitions, we perform the same
          steps on</p>
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} d(t) = 1 -
              (\mathbf {s}^0(t)-s_m(t)) \end{align}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>as was done with <strong>s</strong>
          <sup>0</sup>(<em>t</em>). After consolidating the
          boundaries obtained from <strong>s</strong>
          <sup>0</sup>(<em>t</em>) and <em>d</em>(<em>t</em>), we
          denote the set of shot boundaries as <span class=
          "inline-equation"><span class="tex">$\mathcal {B}=\lbrace
          sb_1,\cdots , sb_{n_s} \rbrace$</span></span> , where
          <em>n<sub>s</sub></em> is the total number of shot
          boundaries detected, and <em>sb</em> = (<em>l</em>,
          <em>r</em>) with <em>l</em> and <em>r</em> being the
          start and end of the corresponding runs of 1s.
          <p></p>
          <p>A particular example of detection based on this method
          (<em>w</em> = fps = 30 and <em>K</em> = 5) is shown in
          Figure <a class="fig" href="#fig2">2</a>, which clearly
          illustrates that the large value of <strong>s</strong>
          <sup>0</sup>(<em>t</em>) − <em>s<sub>m</sub></em>
          (<em>t</em>) is indeed informative for detecting
          <em>smooth</em> transitions.</p>
        </section>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Key-frame
            Extraction in a Shot</h3>
          </div>
        </header>
        <p>To avoid processing all the frames in a shot, which can
        be time consuming and unnecessary, we identify
        representative <em>key-frames</em> within each identified
        shot. While removing redundant information, the selection
        must not remove useful information. In the context of
        educational videos, where an instructor intends to convey
        information visually, the portion in a shot that involves
        least amount of <em>distraction</em> can be assumed to
        contain our key-frame. In general, these distractions arise
        from the motion of either objects in the scene or camera.
        We design a novel method to find the stationary portions in
        a shot that contain least distractions.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Signals for the entire span
            of a shot that is extracted from an educational video.
            Non-smoothness of <em>i</em>(<em>t</em>) indicates
            motion within the shot. The zoomed portion shows jitter
            removal from <span class="inline-equation"><span class=
            "tex">$\tilde{i}(t)$</span></span> .</span>
          </div>
        </figure>
        <p></p>
        <p>We first construct a time-series signal
        <em>i</em>(<em>t</em>) that is the entropy of edge map
        <em>E<sub>c</sub></em> for frame <em>V</em>(<em>t</em>).
        This is like equation <a class="eqn" href="#eq1">1</a>,
        where the whole <em>E<sub>c</sub></em> is considered as one
        patch. Construction of <em>i</em>(<em>t</em>) is inspired
        from [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>]. For demarcation of the stationary
        regions, we apply two levels of approximations that
        converts <em>i</em>(<em>t</em>) into a piece-wise constant
        signal.</p>
        <p><em>Mean Shift clustering</em> algorithm [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0006">6</a>] is used on the values of
        <em>i</em>(<em>t</em>) to get the cluster centers
        <span class="inline-equation"><span class="tex">$\mathcal
        {C} = \lbrace c_1, \cdots , c_k\rbrace$</span></span> . We
        then approximate <em>i</em>(<em>t</em>) to</p>
        <div class="table-responsive" id="Xeq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \tilde{i}(t) =
            \arg \,\min _{c_i \in \mathcal {C}} |c_i - i(t)|.
            \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>Although <span class="inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> is a piece-wise constant
        signal, due to the dynamism involved, even within a shot,
        it turns out to be highly jittery, as can be seen in figure
        <a class="fig" href="#fig3">3</a>. These variations are
        primarily due to the lack of monotonicity in
        <em>i</em>(<em>t</em>).
        <p></p>
        <p>We develop a signal smoothing operation, and apply it on
        <span class="inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> for the second level of
        approximation, to get rid of the false jitters. The noisy
        piece-wise constant signal <span class=
        "inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> can be expressed as</p>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \tilde{i}(t)
            &amp;= \sum _{i=1}^{n_p} \alpha _i \mathbb {I}_i(t)
            \end{align}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \text{where, \ }
            \mathbb {I}_i(t) &amp;= {\left\lbrace
            \begin{array}{@{}l@{\quad }l@{}}1, \quad t \in \mathcal
            {I}_i, \\ 0, \quad \text{otherwise},
            \end{array}\right.} \end{align}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>where, <span class="inline-equation"><span class=
        "tex">$\mathcal {I}_i$</span></span> is the interval in
        <span class="inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> with value
        <em>α<sub>i</sub></em> , <span class=
        "inline-equation"><span class="tex">$\mathbb
        {I}_i$</span></span> is the indicator for that interval,
        and <em>n<sub>p</sub></em> is the total number of
        piece-wise constant intervals in <span class=
        "inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> . In particular,
        <em>α<sub>i</sub></em> is one of the cluster centers, such
        that, <em>i</em>(<em>t</em>) is closest to it as compared
        to other cluster centers. Jitters in <span class=
        "inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> can be assumed as a
        group of temporally adjacent intervals having smaller
        lengths as compared to the stable intervals. We denote the
        desired noise-free piece-wise constant signal as
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \bar{i}(t) = \sum
            _{i=1}^{n_p} \beta _i \mathbb {I}_i(t),
            \end{align}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>where, <em>β<sub>i</sub></em> s are the parameters to
        be found by solving the following optimization problem:
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \beta _1^* \cdots
            \beta _{n_p}^* = \arg \!\!\!\min _{\beta _1 \cdots
            \beta _{n_p}} &amp;\sum _{i=1}^{n_p} \frac{1}{2}
            \left(\alpha _i - \beta _i\right)^2 |\mathcal {I}_i|
            \nonumber \\ + &amp; \lambda \sum _{i=1}^{n_p-1}
            \frac{\left(\beta _{i+1} - \beta _i\right)^2}{\min
            \left(|\mathcal {I}_i|, |\mathcal {I}_{i+1}|\right)},
            \end{align}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$|\mathcal {I}_i|$</span></span> is the length of
        interval <span class="inline-equation"><span class=
        "tex">$\mathcal {I}_i$</span></span> . The second term in
        equation <a class="eqn" href="#eq9">10</a> penalizes
        jitters in <span class="inline-equation"><span class=
        "tex">$\bar{i}(t)$</span></span> with appropriate
        smoothness factor <em>λ</em>, and the first term restricts
        <em>β<sub>i</sub></em> s to original values for stable
        intervals. The solution for equation <a class="eqn" href=
        "#eq9">10</a> can be found in closed form. Finally, by
        changing the <span class="inline-equation"><span class=
        "tex">$\beta _i^*$</span></span> s to the closest value
        among {<em>α</em> <sub><em>i</em> − 1</sub>,
        <em>α<sub>i</sub></em> , <em>α</em> <sub><em>i</em> +
        1</sub>} the jittery intervals get merged with the stable
        ones. As a result we obtain the desired <span class=
        "inline-equation"><span class=
        "tex">$\bar{i}(t)$</span></span> . This is illustrated in
        figure <a class="fig" href="#fig3">3</a>.
        <p></p>
        <p>The final signal <span class=
        "inline-equation"><span class=
        "tex">$\bar{i}(t)$</span></span> will have significantly
        less number of intervals than <span class=
        "inline-equation"><span class=
        "tex">$\tilde{i}(t)$</span></span> , and from each of these
        intervals one frame can be treated as a key–frame . The
        duration of a key–frame is same as that of the interval it
        is picked from.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">a) Output of OCR (green
            boxes) displayed on the input image; b) Output of FCN
            on input image as heat map; c) Mask from thresholding
            overlaid with OCR output; d) Filtered OCR
            output.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Information
          Extraction</h2>
        </div>
      </header>
      <p>In the second step of our ToC generation method, we
      extract textual information from all the selected key–frames
      and from the speech signal of the video. We use existing
      tools for Optical Character Recognition (OCR) to extract
      visual texts but apply a post–processing step to improve the
      precision. We assume that the audio transcript is available
      or can be obtained from an Automatic Speech Recognition (ASR)
      system. Saliency information for both visual and audio text
      are obtained.</p>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Visual-Text
            Extraction</h3>
          </div>
        </header>
        <p>We refer to a visual text entity as <em>vText</em> that
        comprises of the text string (in an image frame) along with
        its visual saliency.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Left–to–Right (a) Example
            key-frames from educational videos, with a title text
            and body texts providing information about the title in
            a hierarchical fashion. (b) Textual units in key-frames
            (<em>vText</em>s) represented as a template with
            Heading, Sub-headings and Points. (c) <em>vText</em>s
            have an inherent hierarchical/dependency relationship
            represented by a Tree as shown and is represented by a
            directed Tree with <em>vText</em>s as nodes and
            directed edges indicating the dependency relationship.
            We construct such <em>shot–level Trees
            T<sub>i</sub></em> for each shot in the video.</span>
          </div>
        </figure>
        <p></p>
        <section id="sec-14">
          <p><em>4.1.1 Text Detection and Recognition:.</em> In
          this work, we use a commercially available OCR engine,
          offered by the <em>Cognitive Services</em> of Microsoft
          Azure cloud computing. Given an image, this service
          returns a set of texts and there corresponding bounding
          boxes. As in the case of any automatic OCR, for
          low-resolution or content-rich images, it misreads some
          of the non-text objects as text. So, to further improve
          its precision we have used another filter on the OCR
          output. This is achieved by training a <em>Fully
          Convolutional Network</em> (FCN) [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0017">17</a>] to
          generate a heat map (HM) of the text regions in an input
          image, such that if a pixel in the HM is positive then it
          belongs to the text region. The HM is then binarized by
          thresholding with 0 in order to produce the mask of the
          text regions, see figure <a class="fig" href=
          "#fig4">4</a>. The idea of obtaining a text map for an
          image is introduced in [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0032">32</a>]. This network is trained with
          <em>COCO-Text</em> dataset [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0031">31</a>]. If a bounding box obtained from
          the OCR engine has significant overlap (85%) with
          positive regions in the mask, then the corresponding text
          is used for further processing; this is also illustrated
          in figure <a class="fig" href="#fig4">4</a>.</p>
        </section>
        <section id="sec-15">
          <p><em>4.1.2 Metadata and Salient Features:.</em> The
          timing and duration of a <em>vText</em> is inherited from
          the key–frame from which it is extracted. The salient
          features of a <em>vText</em> are <em>font size</em>
          (height of the bounding box), <em>boldness</em>, and
          <em>vertical location</em>. <em>Boldness</em> value is
          obtained by computing the mean stroke width of each
          character in the <em>vText</em> by using <em>Stroke Width
          Transform</em> (SWT) [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0010">10</a>].</p>
          <p>A key frame <em>kf</em> can be represented as a set of
          <em>vText</em>s. Within a shot, it may so happen that the
          <em>kf<sub>i</sub></em> ∩<em>kf</em> <sub><em>i</em> +
          1</sub> ≠ <em>ϕ</em>, i.e. some text may reappear in the
          next key-frame. For example, in a typical slide show,
          bulleted points come one after another. To avoid
          processing repeated texts in the pipeline, we agglomerate
          all the <em>kf</em>s in a shot as</p>
          <div class="table-responsive" id="eq10">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathcal {S} =
              \bigcup _{i=1}^{n_k} k\!f_i, \end{align}</span><br />
              <span class="equation-number">(11)</span>
            </div>
          </div>where <em>n<sub>k</sub></em> is the number of KFs
          in that shot, such that each <em>vText</em> in the final
          set <span class="inline-equation"><span class=
          "tex">$\mathcal {S}$</span></span> uniquely represents a
          visual text. While agglomerating, two <em>vText</em>s are
          deemed the same if their salient features are similar and
          the texts match. If two <em>vText</em>s are found to be
          same then one of them is picked and the metadata is
          modified to account for both. Finally, in terms of
          textual information, a video can be represented as
          <div class="table-responsive" id="eq11">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} \mathcal {V} =
              \lbrace \mathcal {S}_i; 1 \le i \le n_s \rbrace
              \end{align}</span><br />
              <span class="equation-number">(12)</span>
            </div>
          </div>
          <p></p>
        </section>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Audio Text
            Extraction</h3>
          </div>
        </header>
        <p>The spoken text in audio is augmented with prosodic
        feature and metadata, together denoted by <em>aText</em>.
        For a spoken word, <em>pitch</em> of the speaker is used as
        the prosodic feature, which captures the intonations and
        stresses in speech [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>]. We use an openly available tool
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0012">12</a>] for
        pitch extraction.</p>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Table of
          Contents Generation</h2>
        </div>
      </header>
      <p>In this section we describe the final step of how the text
      and associated metadata extracted from the selected
      key–frames as well as the identified shots are together used
      to create a Table of Contents (ToC) for a video.</p>
      <p>ToC generation can be viewed as an extractive
      summarization problem. Extractive summarization problems have
      been formulated as combinatorial optimization problems like
      Maximum Marginal Relevance [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>], Knapsack problem or Maximum Coverage
      problem [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0030">30</a>].
      Such formulations result in summaries that lack logical
      coherence because dependency relationships between the
      textual units (words or phrases) being summarized are
      ignored.</p>
      <p>To generate coherent summaries [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0016">16</a>] proposed a method for
      single document summarization that considers the discourse
      relationships i.e. logical connections between the textual
      units in the document. They constructed a Rhetorical
      Structure theory based discourse tree, inferred dependency
      relationships and took a tree trimming approach to
      summarization by formulating it as a Tree Knapsack problem
      (TKP). Our input text is extracted from video instead of text
      sources, and our method, inspired by their method, is novel
      in the techniques of inferring dependency relationships from
      multi-modal, temporally dependent information and in the
      choice of an appropriate cost function formulation for TKP.
      Our method comprises of three steps:</p>
      <ol class="list-no-style">
        <li id="list10" label="(1)"><strong>Shot level Tree
        construction.</strong> Creation of a hierarchical
        representation of information in a shot.<br /></li>
        <li id="list11" label="(2)"><strong>Agglomeration of
        Shots.</strong> Similar shots are agglomerated to create a
        hierarchical representation of the entire video as a single
        Tree.<br /></li>
        <li id="list12" label="(3)"><strong>Summary/ToC
        generation.</strong> Selection of a best subtree
        representing the summary of this video formulated as a Tree
        Knapsack problem.<br /></li>
      </ol>
      <p>In the following, we describe each of the three steps.</p>
      <figure id="fig6">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig6.jpg"
        class="img-responsive" alt="Figure 6" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 6:</span> <span class=
          "figure-title">A single Tree <em>T<sub>a</sub></em>
          representing the whole video is generated by bottom up
          agglomeration of the shot level Trees
          <em>T<sub>i</sub></em> . This illustration contains 4
          shots in total (shot 4 has two key–frames, others have 1
          each). <em>T</em> <sub>1</sub> − <em>T</em> <sub>4</sub>
          are the shot level Trees. First <em>T</em> <sub>2</sub>
          and <em>T</em> <sub>3</sub> are agglomerated to
          <em>T</em> <sub>2.3</sub> as shots 3 and 4 are about the
          same topic. Among <em>T</em> <sub>1</sub>, <em>T</em>
          <sub>2.3</sub>, <em>T</em> <sub>4</sub> the Trees
          <em>T</em> <sub>1</sub> and <em>T</em> <sub>2.3</sub> are
          agglomerated to <em>T</em> <sub>1.23</sub> since
          <em>T</em> <sub>2.3</sub> is an elaboration of a topic in
          <em>T</em> <sub>1</sub>. Finally <em>T</em>
          <sub>1.23</sub> and <em>T</em> <sub>4</sub> are
          agglomerated to a single Tree <em>T</em>
          <sub>1.23.4</sub> representing all the hierarchical
          relationships between <em>vText</em>s in the
          video.</span>
        </div>
      </figure>
      <p></p>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Shot level
            Tree construction</h3>
          </div>
        </header>
        <p>The shots of a typical slide based educational video
        contains key-frames whose <em>vText</em>s inherently share
        a dependency relationship. For instance, in Figure
        <a class="fig" href="#fig5">5</a> the <em>vText</em>s 2,3,4
        can be considered to be dependent on <em>vText</em> 1 i.e.
        <em>vText</em> 1 is a parent to <em>vText</em>s 2,3,4.
        Similarly <em>vText</em>s 2,3,4 are parents to
        <em>vText</em>s 5,6,7 respectively. Thus a shot can be
        represented as a Tree which captures these dependency
        relationships derived from the meta-data of
        <em>vText</em>s. Figure <a class="fig" href="#fig5">5</a>
        illustrates an example shot and the corresponding Tree
        representation. Following are the steps involved in
        construction of a shot level tree representation:</p>
        <ol class="list-no-style">
          <li id="list13" label="(1)">A Graph <em>G</em> is built
          with <em>vTexts</em> as vertices and the edges
          representing the strength of the dependency relationship
          between <em>vTexts</em>. The strength of dependency
          relationship <em>d</em> is computed using the
          corresponding saliency scores <em>vertical location</em>
          (<em>vl</em>) and <em>boldness</em> (<em>b</em>) which
          serve as a proxy to the dependency relationships between
          the <em>vTexts</em> in a key-frame (equation <a class=
          "eqn" href="#eq12">13</a>). The difference in the
          <em>vl</em> and <em>b</em> of the <em>vTexts</em> within
          a key-frame signifies the distance between them in the
          hierarchy as illustrated in figure <a class="fig" href=
          "#fig5">5</a> for typical slide based educational videos.
          The <em>λ</em> in equation <a class="eqn" href=
          "#eq12">13</a> is a fraction that governs the importance
          (weight) to be given to the differences in <em>vl</em>
          and <em>b</em>.
            <div class="table-responsive" id="eq12">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation}
                \begin{split} d(vText_i,vText_j) = \lambda *[
                vText_i^{vl} - vText_j^{vl}] + \\ (1-\lambda) * [
                vText_i^{b} - vText_j^{b}] \end{split}
                \end{equation}</span><br />
                <span class="equation-number">(13)</span>
              </div>
            </div><br />
          </li>
          <li id="list14" label="(2)">A Minimum Spanning Tree
          <em>T</em>′ is constructed from the Graph <em>G</em>
          after negating the edge weights. The resultant
          <em>T</em>′ will be a Tree with maximum weights or in
          other words with maximum strength of dependency
          relationships.<br /></li>
          <li id="list15" label="(3)">The rooted shot level
          directed dependency Tree <em>T</em> is constructed by
          Breadth First Traversal (BFT) of <em>T</em>′ with the
          <em>vText</em> that has highest <em>utility</em> as the
          root. The <strong>utility</strong> <em>u</em> of a
          <em>vText</em> in the hierarchy is computed using the
          corresponding saliency scores as in equation <a class=
          "eqn" href="#eq13">14</a>. Following similar intuition as
          that of equation <a class="eqn" href="#eq12">13</a>, we
          consider a <em>vText</em> with higher
          <em>vertical-location</em> (<em>vl</em>) and
          <em>boldness</em> (<em>b</em>) to be more useful in
          summarizing the content of the key-frame. Additionally we
          consider a <em>vText</em> to be more useful when it gets
          repeated in key-frames of a shot, captured by the
          <em>frequency</em> (<em>f</em>).
            <div class="table-responsive" id="eq13">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation}
                \begin{split} u(vText) = [\lambda * vText^{b} +
                (1-\lambda) * vText^{vl}] \\ * (1+log(vText^{f}))
                \end{split} \end{equation}</span><br />
                <span class="equation-number">(14)</span>
              </div>
            </div><br />
          </li>
        </ol>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span>
            Agglomeration of Shots</h3>
          </div>
        </header>
        <p>The shots of a typical education video unfold
        temporally. The content of a new shot detected may be
        closely related to the previous shot (e.g. elaboration of
        the previous topic) or may not be (e.g. beginning of a new
        topic). Thus a group of adjacent shots could be
        agglomerated to a single hierarchical representation i.e. a
        Tree by merging the constituent shot level trees in a
        manner that preserves the dependency relationship between
        the nodes/<em>vText</em>s. These agglomerated shot level
        trees could be further agglomerated in a bottom up fashion
        to generate a single Tree representing the whole video.
        Figure <a class="fig" href="#fig6">6</a> illustrates the
        process.</p>
        <p>The method is essentially hierarchical clustering with
        an additional constraint that only temporally adjacent
        shots can be merged and additional logic for merging the
        shot level trees. We use the Earth Mover's Distance
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0027">27</a>] between
        the corresponding visual information <span class=
        "inline-equation"><span class="tex">$V(\mathcal
        {S}_i)$</span></span> and <span class=
        "inline-equation"><span class="tex">$V(\mathcal
        {S}_j)$</span></span> as the distance metric (<em>ds</em>)
        between shots. Thus the following two steps are repeated
        until a single agglomerated Tree <em>T<sub>a</sub></em> is
        obtained:</p>
        <ol class="list-no-style">
          <li id="list16" label="(1)">Construct a min-heap of
          distances <span class="inline-equation"><span class=
          "tex">$ds(\mathcal {S}_i,\mathcal {S}_j)$</span></span>
          between every two <em>adjacent</em> shots <span class=
          "inline-equation"><span class="tex">$\mathcal
          {S}_i,\mathcal {S}_j$</span></span> .<br /></li>
          <li id="list17" label="(2)">Pop the adjacent shots with
          minimum <span class="inline-equation"><span class=
          "tex">$ds(\mathcal {S}_i,\mathcal {S}_j)$</span></span>
          and merge the corresponding shot level Trees
          <em>T<sub>i</sub></em> and <em>T<sub>j</sub></em> to
          obtain a merged shot <span class=
          "inline-equation"><span class="tex">$\mathcal
          {S}_{ij}$</span></span> and the associated merged shot
          level Tree <em>T<sub>ij</sub></em> . The shot level trees
          <em>T<sub>i</sub></em> and <em>T<sub>j</sub></em> such
          that <em>i</em> &gt; <em>j</em>, are merged as follows:
          If nodes at level 1 of <em>T<sub>i</sub></em> and
          <em>T<sub>j</sub></em> overlap then merge them by adding
          <em>subtrees</em>(<em>T<sub>j</sub></em> ) with minimum
          <em>ds</em>(<em>T<sub>i</sub></em> ,
          <em>subtrees</em>(<em>T<sub>j</sub></em> )) as a child of
          the overlapping node of <em>T<sub>i</sub></em> .
          Otherwise merge them by making <em>T<sub>i</sub></em> ,
          <em>T<sub>j</sub></em> siblings and children of a dummy
          root node <em>r</em>.<br /></li>
        </ol>
        <figure id="fig7">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig7.jpg"
          class="img-responsive" alt="Figure 7" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 7:</span>
            <span class="figure-title">The step of selecting the
            best subtree <em>T</em> <sup>*</sup> from
            <em>T<sub>a</sub></em> representing the summary of the
            video is formulated as a Tree Knapsack problem where an
            optimal subtree <em>T</em> <sup>*</sup> is selected to
            maximize the utility and constrained to the desired
            maximum length of the ToC <em>L</em>. The table
            illustrates how a ToC with optimum hierarchy
            information is generated with different values of
            <em>L</em>.</span>
          </div>
        </figure>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Summary/ToC
            generation</h3>
          </div>
        </header>
        <p>The problem of generating summary of desired length
        <em>L</em> (i.e. number of <em>vText</em>s) can be
        formulated as a Tree Knapsack combinatorial optimization
        problem [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>] - where an optimal rooted sub-tree
        <em>T</em> <sup>*</sup> is selected from
        <em>T<sub>a</sub></em> to maximize a summary utility
        function <em>F</em>(<em>T</em>). The summary utility for a
        given sub-tree is a function of various aspects of the
        <em>vText</em>s captured by the corresponding saliency
        scores. Further the utility of a <em>vText</em> is
        re-weighted based on its <em>pitch</em> from the audio
        metadata and the depth of the corresponding node in the
        hierarchy. As shown in equation <a class="eqn" href=
        "#eq14">15</a> a <em>vText</em> with higher <em>pitch</em>
        (<em>p</em>) and at a lesser depth has a higher utility
        value.</p>
        <div class="table-responsive" id="eq14">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \begin{split}
            F(T) = \sum _{i = 1}^{N} \frac{u(vText_i) *
            p(vText_i)}{depth(vText_i)} x_i\,\,, \end{split}
            \end{equation}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation*} \begin{split}
            x_i = \left\lbrace {\begin{array}{*10c}1 \qquad \textrm
            {if} \ vText_i \in T \\ 0 \qquad \textrm {otherwise}
            \end{array}}\right.
            \end{split}\end{equation*}</span><br />
          </div>
        </div>and <em>N</em> is the number of <em>vText</em>s or
        nodes in the Tree <em>T</em>. The <em>vText</em>s
        corresponding to the nodes of the optimal sub-tree
        <em>T</em> <sup>*</sup> selected forms the summary. The
        selection can be performed by solving the ILP problem
        <a class="eqn" href="#eq15">16</a>. Given the summary
        budget <em>L</em> the ILP problem solution, that can be
        found in polynomial-time [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>], will be an optimum set of
        <em>vTexts</em> forming a rooted subtree resulting in an
        hierarchical ToC. Figure <a class="fig" href="#fig7">7</a>
        illustrates how hierarchical ToCs can be generated by the
        ILP (equation <a class="eqn" href="#eq15">16</a>) from the
        agglomerated Tree <em>T</em> <sub>1.23.4</sub> shown in
        Figure <a class="fig" href="#fig6">6</a> for varying
        summary lengths <em>L</em>.
        <div class="table-responsive" id="eq15">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \begin{split} T^*
            &amp;= \max _T {F(T)} \\s.t. &amp;\quad \sum _{i=1}^{N}
            x_i {\lt} L \quad \textrm {and} \quad \forall i
            :x_{parent(i)} \ge x_i. \end{split}
            \end{align}</span><br />
            <span class="equation-number">(16)</span>
          </div>
        </div>
        <p></p>
        <figure id="fig8">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186336/images/www18companion-98-fig8.jpg"
          class="img-responsive" alt="Figure 8" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 8:</span>
            <span class="figure-title">The Hierarchical Table of
            Contents can be presented to the user facilitating
            non–linear navigation through the video. The user can
            click on a ToC entry to reach the corresponding section
            in the video.</span>
          </div>
        </figure>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Related
          Work</h2>
        </div>
      </header>
      <p>Segmentation of visual, speech and text data into coherent
      topics have been studied extensively. For example, TextTiling
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0014">14</a>] segments
      text documents by detecting topic changes through vocabulary
      comparison. TextTiling has been extended to TopicTiling with
      the use of topic models [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>]. A hierarchical Bayesian topic
      segmentation model for text documents is proposed in
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>]. Topic
      segmentation models to structure lecture resources into
      cohesive segments, making them suitable for MOOCs content
      browsing is studied in [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>].</p>
      <p>Spoken lecture segmentation is studied in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>] where the problem is
      modeled through a normalized cut formulation. A Bayesian
      approach to unsupervised topic segmentation for both text and
      speech is studied in [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>]. Discriminative topic segmentation of
      combined speech and text have also been investigated
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>].</p>
      <p>Detailed reviews on shot level segmentation of videos can
      be found in [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0007">7</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0033">33</a>]. Phung
      <em>et al.</em> develop a method of topic segmentation in
      instructional videos using variation in content density
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>] that is
      further extended with the features derived from the speech
      signal [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0024">24</a>].
      Phung <em>et al.</em> also develop topic transition detection
      methods in videos using Markov and Semi-Markov models
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>]. These
      methods are not specifically designed for slide–based
      educational videos but for instructional films and are based
      on cinematic expressive functions.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Experimental results on titles of ToC: Mean
          ( <sub><em>m</em></sub> ) and standard deviation (
          <sub><em>s</em></sub> ) of Precision (P), Recall (R) and
          F-score (F) over all videos in the dataset.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;">Method</td>
              <td colspan="2" style="text-align:center;">
                Precision
                <hr />
              </td>
              <td colspan="2" style="text-align:center;">
                Recall
                <hr />
              </td>
              <td colspan="2" style="text-align:center;">
                F-score
                <hr />
              </td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">
              <em>P<sub>m</sub></em></td>
              <td style="text-align:center;">
              <em>P<sub>s</sub></em></td>
              <td style="text-align:center;">
              <em>R<sub>m</sub></em></td>
              <td style="text-align:center;">
              <em>R<sub>s</sub></em></td>
              <td style="text-align:center;">
              <em>F<sub>m</sub></em></td>
              <td style="text-align:center;">
              <em>F<sub>s</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;">MMToC</td>
              <td style="text-align:center;">0.56</td>
              <td style="text-align:center;">0.3</td>
              <td style="text-align:center;">0.13</td>
              <td style="text-align:center;">0.08</td>
              <td style="text-align:center;">0.21</td>
              <td style="text-align:center;">0.12</td>
            </tr>
            <tr>
              <td style="text-align:center;">HMMToC</td>
              <td style="text-align:center;">
              <strong>0.74</strong></td>
              <td style="text-align:center;">0.23</td>
              <td style="text-align:center;">
              <strong>0.83</strong></td>
              <td style="text-align:center;">0.17</td>
              <td style="text-align:center;">
              <strong>0.76</strong></td>
              <td style="text-align:center;">0.20</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>A related system, Talkminer, is a lecture webcase search
      engine that creates a searchable text index for lecture
      videos but does not segment the video or create a ToC
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>]. Generation
      of ToC for educational videos was recently studied in
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>] where their
      algorithm, MMToC, that uses features from all three – text,
      speech and visual – modalities was shown to outperform
      previous topic model–based methods.</p>
    </section>
    <section id="sec-22">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Experiments</h2>
        </div>
      </header>
      <p>We test the performance of our ToC generation method
      (HMMToC) with respect to two aspects of ToC, viz. the timing
      and title, comparing with the previous best method of ToC
      generation, MMToC.</p>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.1</span> Data and
            Evaluation</h3>
          </div>
        </header>
        <p>For evaluating the quality of ToC entries, we create a
        labeled dataset using 46 freely available educational
        videos from Youtube. Each video is either a part of an
        online course or a webinar related to computer science.
        Five human users, who were familiar with the video content,
        were asked to summarize the videos in the form of a ToC.
        Based on this labeled dataset, ToCs generated from
        competing algorithms are compared on the following two
        aspects.</p>
        <ol class="list-no-style">
          <li id="list18" label="(1)">
            <strong>Timing:</strong> The chosen videos are of
            various lengths (5 to 35 minutes) with varying number
            of labeled ToC entries. Following the evaluation scheme
            of [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0004">4</a>] if the ToC entry output by an
            algorithm falls within an interval of 10 seconds of the
            labeled entry, it is considered as a hit; otherwise a
            mis–hit.<br />
          </li>
          <li id="list19" label="(2)"><strong>Title:</strong> A ToC
          entry obtained from an algorithm is considered a hit, if
          at least one of its words matches the ground truth ToC
          and if there is a hit with respect to timing as
          well.<br /></li>
        </ol>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.2</span>
            Results</h3>
          </div>
        </header>
        <p>Tables <a class="tbl" href="#tab2">2</a> and <a class=
        "tbl" href="#tab1">1</a> show the performance results of
        HMMToC and MMToC on our dataset. All three metrics,
        Precision, Recall and F-Score, for both the criteria,
        timing and title, are found to be better in HMMToC than in
        MMToC.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Experimental results on timings of ToC:
            Mean ( <sub><em>m</em></sub> ) and standard deviation (
            <sub><em>s</em></sub> ) of Precision (P), Recall (R)
            and F-score (F) over all videos in the dataset.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">Method</td>
                <td colspan="2" style="text-align:center;">
                  Precision
                  <hr />
                </td>
                <td colspan="2" style="text-align:center;">
                  Recall
                  <hr />
                </td>
                <td colspan="2" style="text-align:center;">
                  F-score
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">
                <em>P<sub>m</sub></em></td>
                <td style="text-align:center;">
                <em>P<sub>s</sub></em></td>
                <td style="text-align:center;">
                <em>R<sub>m</sub></em></td>
                <td style="text-align:center;">
                <em>R<sub>s</sub></em></td>
                <td style="text-align:center;">
                <em>F<sub>m</sub></em></td>
                <td style="text-align:center;">
                <em>F<sub>s</sub></em></td>
              </tr>
              <tr>
                <td style="text-align:center;">MMToC</td>
                <td style="text-align:center;">0.83</td>
                <td style="text-align:center;">0.22</td>
                <td style="text-align:center;">0.21</td>
                <td style="text-align:center;">0.14</td>
                <td style="text-align:center;">0.31</td>
                <td style="text-align:center;">0.14</td>
              </tr>
              <tr>
                <td style="text-align:center;">HMMToC</td>
                <td style="text-align:center;">
                <strong>0.86</strong></td>
                <td style="text-align:center;">0.23</td>
                <td style="text-align:center;">
                <strong>0.92</strong></td>
                <td style="text-align:center;">0.181</td>
                <td style="text-align:center;">
                <strong>0.85</strong></td>
                <td style="text-align:center;">0.21</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>A qualitative comparison also clearly shows the
        superiority of HMMToC over MMToC. JSON files containing the
        ToC generated by both HMMToC and MMToC for all 46 videos
        can be downloaded for viewing<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a>.</p>
      </section>
      <section id="sec-25">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.3</span>
            Illustration</h3>
          </div>
        </header>
        <p>Figure <a class="fig" href="#fig8">8</a> illustrates how
        the generated ToC can be presented to the user beside the
        video with hyperlinks to time points in the video thereby
        facilitating non–linear navigation through the video. The
        figure also shows how the hierarchy within the ToC is
        generated through the headings and sub–headings in slides
        present in the video.</p>
      </section>
    </section>
    <section id="sec-26">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span> Conclusion</h2>
        </div>
      </header>
      <p>We design HMMToC, the first method to automatically create
      a hierarchical multi-modal Table of Contents (ToC) for
      educational videos. Our method consists of three key steps.
      The first step is the identification of shot boundaries by
      segmentation of the input video, for which we design a new
      threshold–free method. The second step is the extraction of
      textual information and associated metadata from the selected
      key–frames in shots as well as from the audio transcript of
      the video. In the final step, a hierarchical ToC is generated
      using multimodal information from each shot that is
      formulated as a Tree Knapsack problem.</p>
      <p>A hierarchical ToC provides a valuable summary of the
      video and a textbook–like facility for nonlinear navigation
      and search through the video. We empirically demonstrate that
      HMMToC is more accurate than previous (non-hierarchical)
      methods in terms of both obtaining the section title and the
      timing of the title. Our method relies on frames containing
      presentation slides within the video to generate the ToC.
      Many, but not all, educational videos usually contain such
      frames. Generation of ToC for videos that do not contain any
      slides remains a challenging problem that is not yet solved
      well.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">John Adcock, Matthew
        Cooper, Laurent Denoue, Hamed Pirsiavash, and
        Lawrence&nbsp;A Rowe. 2010. Talkminer: a lecture webcast
        search engine. In <em><em>Proceedings of the 18th ACM
        international conference on Multimedia</em></em> . ACM,
        241–250.</li>
        <li id="BibPLXBIB0002" label="[2]">Ghada Alharbi and Thomas
        Hain. 2015. Using Topic Segmentation Models for the
        Automatic Organisation of MOOCs resources.. In
        <em><em>EDM</em></em> . 524–527.</li>
        <li id="BibPLXBIB0003" label="[3]">Evlampios Apostolidis
        and Vasileios Mezaris. 2014. Fast shot segmentation
        combining global and local visual descriptors. In
        <em><em>Acoustics, Speech and Signal Processing (ICASSP),
        2014 IEEE International Conference on</em></em> . IEEE,
        6583–6587.</li>
        <li id="BibPLXBIB0004" label="[4]">Arijit Biswas, Ankit
        Gandhi, and Om Deshmukh. 2015. Mmtoc: A multimodal method
        for table of content creation in educational videos. In
        <em><em>Proceedings of the 23rd ACM international
        conference on Multimedia</em></em> . ACM, 621–630.</li>
        <li id="BibPLXBIB0005" label="[5]">Jaime Carbonell and Jade
        Goldstein. 1998. The use of MMR, diversity-based reranking
        for reordering documents and producing summaries. In
        <em><em>Proceedings of the 21st annual international ACM
        SIGIR conference on Research and development in information
        retrieval</em></em> . ACM, 335–336.</li>
        <li id="BibPLXBIB0006" label="[6]">Dorin Comaniciu and
        Peter Meer. 2002. Mean shift: A robust approach toward
        feature space analysis. <em><em>IEEE Transactions on
        pattern analysis and machine intelligence</em></em> 24,
        5(2002), 603–619.</li>
        <li id="BibPLXBIB0007" label="[7]">Costas Cotsaces, Nikos
        Nikolaidis, and Ioannis Pitas. 2006. Video shot detection
        and condensed representation. a review. <em><em>IEEE signal
        processing magazine</em></em> 23, 2 (2006), 28–37.</li>
        <li id="BibPLXBIB0008" label="[8]">L. Du, W.&nbsp;L.
        Buntine, and M. Johnson. 2013. Topic segmentation with a
        structured topic model. In <em><em>HLT-NAACL</em></em> .
        190–200.</li>
        <li id="BibPLXBIB0009" label="[9]">Jacob Eisenstein and
        Regina Barzilay. 2008. Bayesian unsupervised topic
        segmentation. In <em><em>Proceedings of the Conference on
        Empirical Methods in Natural Language Processing</em></em>
        . Association for Computational Linguistics, 334–343.</li>
        <li id="BibPLXBIB0010" label="[10]">Boris Epshtein, Eyal
        Ofek, and Yonatan Wexler. 2010. Detecting text in natural
        scenes with stroke width transform. In <em><em>Computer
        Vision and Pattern Recognition (CVPR), 2010 IEEE Conference
        on</em></em> . IEEE, 2963–2970.</li>
        <li id="BibPLXBIB0011" label="[11]">Hiroya Fujisaki and
        Keikichi Hirose. 1984. Analysis of voice fundamental
        frequency contours for declarative sentences of Japanese.
        <em><em>Journal of the Acoustical Society of Japan
        (E)</em></em> 5, 4 (1984), 233–242.</li>
        <li id="BibPLXBIB0012" label="[12]">Theodoros
        Giannakopoulos. 2015. pyAudioAnalysis: An Open-Source
        Python Library for Audio Signal Analysis. <em><em>PloS
        one</em></em> 10, 12 (2015).</li>
        <li id="BibPLXBIB0013" label="[13]">Alan Hanjalic. 2002.
        Shot-boundary detection: Unraveled and resolved?
        <em><em>IEEE transactions on circuits and systems for video
        technology</em></em> 12, 2(2002), 90–105.</li>
        <li id="BibPLXBIB0014" label="[14]">Marti&nbsp;A Hearst.
        1997. TextTiling: Segmenting text into multi-paragraph
        subtopic passages. <em><em>Computational
        linguistics</em></em> 23, 1 (1997), 33–64.</li>
        <li id="BibPLXBIB0015" label="[15]">Wei&nbsp;Jyh Heng and
        King&nbsp;N Ngan. 2001. An object-based shot boundary
        detection using edge tracing and tracking. <em><em>Journal
        of Visual Communication and Image Representation</em></em>
        12, 3(2001), 217–239.</li>
        <li id="BibPLXBIB0016" label="[16]">Tsutomu Hirao, Yasuhisa
        Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki
        Nagata. 2013. Single-Document Summarization as a Tree
        Knapsack Problem.. In <em><em>EMNLP</em></em> ,
        Vol.&nbsp;13. 1515–1520.</li>
        <li id="BibPLXBIB0017" label="[17]">Jonathan Long, Evan
        Shelhamer, and Trevor Darrell. 2015. Fully Convolutional
        Networks for Semantic Segmentation. In <em><em>The IEEE
        Conference on Computer Vision and Pattern Recognition
        (CVPR)</em></em> .</li>
        <li id="BibPLXBIB0018" label="[18]">Igor Malioutov and
        Regina Barzilay. 2006. Minimum cut model for spoken lecture
        segmentation. In <em><em>Proceedings of the 21st
        International Conference on Computational Linguistics and
        the 44th annual meeting of the Association for
        Computational Linguistics</em></em> . Association for
        Computational Linguistics, 25–32.</li>
        <li id="BibPLXBIB0019" label="[19]">Mehryar Mohri, Pedro
        Moreno, and Eugene Weinstein. 2010. Discriminative topic
        segmentation of text and speech. In <em><em>Proceedings of
        the Thirteenth International Conference on Artificial
        Intelligence and Statistics</em></em> . 533–540.</li>
        <li id="BibPLXBIB0020" label="[20]">Masaaki Nishino,
        Norihito Yasuda, Tsutomu Hirao, Shin-ichi Minato, and
        Masaaki Nagata. 2015. A Dynamic Programming Algorithm for
        Tree Trimming-based Text Summarization.. In
        <em><em>HLT-NAACL</em></em> . 462–471.</li>
        <li id="BibPLXBIB0021" label="[21]">NIST. Online. Homepage
        of TRECVID Evaluation. (Online). <a class=
        "link-inline force-break" href=
        "http://www-nlpir.nist.gov/projects/trecvid/" target=
        "_blank">http://www-nlpir.nist.gov/projects/trecvid/</a>
        </li>
        <li id="BibPLXBIB0022" label="[22]">Dinh&nbsp;Q Phung,
        Thi&nbsp;V Duong, Svetha Venkatesh, and Hung&nbsp;H Bui.
        2005. Topic transition detection using hierarchical hidden
        Markov and semi-Markov models. In <em><em>Proceedings of
        the 13th annual ACM international conference on
        Multimedia</em></em> . ACM, 11–20.</li>
        <li id="BibPLXBIB0023" label="[23]">Dinh&nbsp;Q Phung,
        Svetha Venkatesh, and Chitra Dorai. 2002. High level
        segmentation of instructional videos based on content
        density. In <em><em>Proceedings of the tenth ACM
        international conference on Multimedia</em></em> . ACM,
        295–298.</li>
        <li id="BibPLXBIB0024" label="[24]">Dinh&nbsp;Q Phung,
        Svetha Venkatesh, and Chitra Dorai. 2003. Hierarchical
        topical segmentation in instructional films based on
        cinematic expressive functions. In <em><em>Proceedings of
        the eleventh ACM international conference on
        Multimedia</em></em> . ACM, 287–290.</li>
        <li id="BibPLXBIB0025" label="[25]">Liping Ren, Zhiyi Qu,
        Weiqin Niu, Chaoxin Niu, and Yanqiu Cao. 2010. Key frame
        extraction based on information entropy and edge matching
        rate. In <em><em>Future Computer and Communication (ICFCC),
        2010 2nd International Conference on</em></em> ,
        Vol.&nbsp;3. IEEE, V3–91.</li>
        <li id="BibPLXBIB0026" label="[26]">Martin Riedl and Chris
        Biemann. 2012. TopicTiling: a text segmentation algorithm
        based on LDA. In <em><em>Proceedings of ACL 2012 Student
        Research Workshop</em></em> . Association for Computational
        Linguistics, 37–42.</li>
        <li id="BibPLXBIB0027" label="[27]">Yossi Rubner, Carlo
        Tomasi, and Leonidas&nbsp;J Guibas. 2000. The earth mover's
        distance as a metric for image retrieval.
        <em><em>International journal of computer vision</em></em>
        40, 2 (2000), 99–121.</li>
        <li id="BibPLXBIB0028" label="[28]">Frank Seide and Amit
        Agarwal. 2016. CNTK: Microsoft's Open-Source Deep-Learning
        Toolkit. In <em><em>Proceedings of the 22nd ACM SIGKDD
        International Conference on Knowledge Discovery and Data
        Mining</em></em> . ACM, 2135–2135.</li>
        <li id="BibPLXBIB0029" label="[29]">Ananya SenGupta,
        Dalton&nbsp;Meitei Thounaojam, Kh&nbsp;Manglem Singh, and
        Sudipta Roy. 2015. Video shot boundary detection: A review.
        In <em><em>Electrical, Computer and Communication
        Technologies (ICECCT), 2015 IEEE International Conference
        on</em></em> . IEEE, 1–6.</li>
        <li id="BibPLXBIB0030" label="[30]">Hiroya Takamura and
        Manabu Okumura. 2009. Text summarization model based on
        maximum coverage problem and its variant. In
        <em><em>Proceedings of the 12th Conference of the European
        Chapter of the Association for Computational
        Linguistics</em></em> . Association for Computational
        Linguistics, 781–789.</li>
        <li id="BibPLXBIB0031" label="[31]">Andreas Veit, Tomas
        Matera, Lukas Neumann, Jiri Matas, and Serge&nbsp;J.
        Belongie. 2016. COCO-Text: Dataset and Benchmark for Text
        Detection and Recognition in Natural Images.
        <em><em>CoRR</em></em> abs/1601.07140(2016).</li>
        <li id="BibPLXBIB0032" label="[32]">Qixiang Ye and David
        Doermann. 2015. Text detection and recognition in imagery:
        A survey. <em><em>IEEE transactions on pattern analysis and
        machine intelligence</em></em> 37, 7(2015), 1480–1500.</li>
        <li id="BibPLXBIB0033" label="[33]">Jinhui Yuan, Huiyi
        Wang, Lan Xiao, Wujie Zheng, Jianmin Li, Fuzong Lin, and Bo
        Zhang. 2007. A formal study of shot boundary detection.
        <em><em>IEEE transactions on circuits and systems for video
        technology</em></em> 17, 2(2007), 168–186.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.dropbox.com/sh/k1mksog9yncuu2f/AABliPpsjMuYLI9Ezh7OgwWfa?dl=0">https://www.dropbox.com/sh/k1mksog9yncuu2f/AABliPpsjMuYLI9Ezh7OgwWfa?dl=0</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186336">https://doi.org/10.1145/3184558.3186336</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
