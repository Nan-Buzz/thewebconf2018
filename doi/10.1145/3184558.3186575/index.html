<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Automatic Translation of Competency Questions into
  SPARQL-OWL Queries</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186575'>https://doi.org/10.1145/3184558.3186575</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186575'>https://w3id.org/oa/10.1145/3184558.3186575</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Automatic Translation of
          Competency Questions into SPARQL-OWL Queries</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Dawid Wiśniewski Supervised by
          Agnieszka</span> <span class="surName">Ławrynowicz</span>
          Faculty of Computing, Poznan University of Technology,
          ul. Piotrowo 3Poznan 60-965Poland, <a href=
          "mailto:dwisniewski@cs.put.poznan.pl">dwisniewski@cs.put.poznan.pl</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186575"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186575</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The process of ontology authoring is inseparably
        connected with the quality assurance phase. One can verify
        the maturity and correctness of a given ontology by
        evaluating how many competency questions give correct
        answers. Competency questions are defined as a set of
        questions expressed in natural language that the finished
        ontology should be able to answer to correctly. Although
        this method can easily indicate what is the development
        status of an ontology, one has to translate competency
        questions from natural language into an ontology query
        language. This task is very hard and time consuming. To
        overcome this problem, my PhD thesis focuses on methods for
        automatically checking answerability of competency
        questions for a given ontology and proposing SPARQL-OWL
        query (OWL-aware SPARQL query) for each question where it
        is possible to create the query. Because the task of
        automatic translation from competency questions to
        SPARQL-OWL queries is a novel one, besides a method, we
        have proposed a new benchmark to evaluate such
        translation.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Web Ontology Language (OWL);</strong> •
        <strong>Computing methodologies</strong> → <strong>Natural
        language processing;</strong> <strong>Ontology
        engineering;</strong> <em>Neural networks;</em></small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Dawid Wiśniewski Supervised by Agnieszka Ławrynowicz.
          2018. Automatic Translation of Competency Questions into
          SPARQL-OWL Queries. In <em>WWW '18 Companion: The 2018
          Web Conference Companion,</em> <em>April 23th-27th,
          2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em>
          5 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186575" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186575</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <p>ontology, competency question, SPARQL-OWL, word
    embedding</p>
    <section id="sec-2">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Problem</h2>
        </div>
      </header>
      <section id="sec-3">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.1</span>
            Context</h3>
          </div>
        </header>
        <p>Ontologies are widely used to describe knowledge. In the
        Web context, they provide schemas for knowledge graphs.
        Because of their expressivity, conciseness of their
        representation and ability to entail new facts – they are
        used in various tasks like: question answering, knowledge
        extraction or data integration.</p>
        <p>Unfortunately, ontology authoring is still a complex
        task. Since ontologies are often expressed using
        description logics many engineers struggle to understand it
        and add new knowledge. The reason for that is the fact that
        ontologies are often developed by domain experts rather
        than logicians, so logical implications of used formalisms
        are hard to resolve for ontology authors.</p>
        <p>To help engineers develop ontologies, multiple methods
        and tools were proposed. Among them the Competency
        Questions-driven Ontology Authoring (CQOA)&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0014">14</a>] is one
        of the most interesting. With that approach the ontology
        engineer defines a set of competency questions (CQs)
        represented as natural language questions.</p>
        <p>CQs are defined as a set of questions that evaluate the
        quality of a given ontology. They represent functional
        requirements that a complete ontology should be able to
        answer to. They allow to asses both completeness and
        correctness of the ontology and one can assume that the
        ontology development is finished, when, for a given
        ontology, all CQs are answered correctly.</p>
        <p>Although stating functional requirements at the
        beginning of ontology creation process is a very
        interesting idea, the evaluation is a time consuming task.
        The reason for that is the fact that in order to verify
        whether an ontology can answer given competency questions –
        manual translation from natural language to an ontology
        query language such as SPARQL or SPARQL-OWL&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0008">8</a>] (an
        extension of SPARQL with Web Ontology Language entailment
        regimes) has to be performed. The translation phase is a
        challenge, because it requires wide knowledge about
        vocabulary stored in the ontology, and the formalism of the
        query language.</p>
      </section>
      <section id="sec-4">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.2</span> Problem
            definition</h3>
          </div>
        </header>
        <p>In my PhD thesis, I would like to propose a set of
        methods that will be able to automatically translate
        competency questions into SPARQL-OWL queries, so engineers
        will be able to automatically evaluate the correctness and
        completeness of a given ontology. Because there is no
        existing method for that task to compare to, a major part
        of my work is to create a set of benchmarks consisting of
        ontologies with competency questions defined for them as
        well as expected SPARQL-OWL translations.</p>
      </section>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.3</span> SPARQL-OWL
            usage: motivating example</h3>
          </div>
        </header>
        <p>The need of choosing SPARQL-OWL as the query language
        can be presented using an example of knowledge base from
        Table <a class="tbl" href="#tab1">1</a> (an excerpt from
        DBpedia&nbsp;v2016-10&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0010">10</a>]). The table represents both
        assertional and terminological part of an ontology defining
        concepts from the domain of software. Assume that the
        engineer states the competency question: ”Does every
        software has a licence?” to be answered using knowledge
        from Table&nbsp;<a class="tbl" href="#tab1">1</a>. The
        commonly used approach is to create a SPARQL query
        like:</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186575/images/www18companion-129-img1.svg"
        class="img-responsive" alt="" longdesc="" /> The NOT EXISTS
        semantics utilizes Closed World Assumption<a class="fn"
        href="#fn1" id="foot-fn1"><sup>1</sup></a>. The query will
        verify if there exists some software without defined
        licence – if so, not every software has a licence.</p>
        <p>The information in ontology from Table <a class="tbl"
        href="#tab1">1</a> is incomplete (of course, in the real
        world there is a licence defined for ”the Witcher”). The
        similar case occurs in the DBpedia in which, only about
        8500 from almost 33 000 pieces of software have declared
        licence. The above situation would lead the reasoner to
        create the answer <strong>true</strong> for that query
        (since there is no licence for the Witcher defined),
        leading to a conclusion that not every software has a
        licence.</p>
        <p>It is very easy to produce incomplete factual knowledge
        when authoring an ontology. To overcome the problem, the
        query should verify the terminological part of an ontology,
        because it can contain axioms with generic knowledge. For
        instance, in ontology from Table <a class="tbl" href=
        "#tab1">1</a> the axiom: SubClassOf(dbo:Software,
        ObjectSomeValuesFrom(dbo:licence, :Licence)) states that
        every software, even not defined completely in the ontology
        has some licence.</p>
        <p>To use such general terminological knowledge, one can
        use SPARQL-OWL (being able to utilize Open World
        Assumption) rather than simple SPARQL:</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186575/images/www18companion-129-img2.svg"
        class="img-responsive" alt="" longdesc="" /> The query
        checks whether every software has a licence, even if it is
        not explicitly asserted in an ontology for a particular
        software. Thus it generates the expected answer: ”Yes,
        every software has a licence defined”.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">A sample KB.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">(a): an assertional
                part, excerpt from DBpedia.</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                dbo:Software(dbr:Weka_(machine_learning)</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                dbo:licence(dbr:Weka_(machine_learning)</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                dbr:GNU_General_Public_licence)</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                dbo:Software(dbr:The_Witcher_(video_game))</td>
              </tr>
              <tr>
                <td style="text-align:left;">(b): a terminological
                part.</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                SubClassOf(dbo:Software,
                ObjectSomeValuesFrom(dbo:licence, :Licence))</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.4</span> Related
            problems</h3>
          </div>
        </header>
        <p>Although one can think of the stated problem as a
        question answering task, there is a major difference
        between them. Question Answering is focused on obtaining
        correct answers, while the task of my PhD thesis is to
        propose a correct SPARQL-OWL queries (without judging if
        the knowledge in an ontology is correct or not, the query
        should be as precise as possible for the current state of
        the ontology). My PhD thesis focuses on knowledge
        engineering, particularly regarding test-driven approach
        for ontology authoring&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>]. Therefore, instead of obtaining a
        correct answer, the goal is to test the ontology for the
        knowledge it contains, which also involves interpreting the
        results of queries which are a component of ontology
        testing algorithms&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>]. Otherwise, it would be hard to
        separate the errors caused by wrong query generation from
        errors caused by wrong knowledge modelled in an
        ontology.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.5</span> Research
            questions</h3>
          </div>
        </header>
        <p>In my PhD thesis, I would like to address the following
        research questions:</p>
        <ol class="list-no-style">
          <li id="list1" label="(1)">Can we detect when it is
          possible to create SPARQL-OWL query from a competency
          question? If it is impossible can we detect
          why?<br /></li>
          <li id="list2" label="(2)">Is it possible to create
          correct SPARQL-OWL queries out of competency
          questions?<br /></li>
          <li id="list3" label="(3)">Are OWL constructs like
          intersectionOf, unionOf, ...always useful for creating
          SPARQL-OWL queries? If and how to distinguish situations
          when they are really necessary from those which can be
          tackled by different modeling styles of SPARQL-OWL
          queries (being semantically equivalent with regard to an
          ontology)?<br /></li>
          <li id="list4" label="(4)">What are the difficulties and
          peculiarites of generating SPARQL-OWL queries out of
          CQs?<br /></li>
        </ol>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> State of the
          art</h2>
        </div>
      </header>
      <p>CQs have been successfully employed in many ontology
      authoring&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] methods and ontology engineering
      methodologies&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>].</p>
      <p>For instance, the On-To-Knowledge methodology [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>] includes
      the preparation of an ontology requirements specification
      document, where the requirements specification based on CQs
      serve to prepare a draft ontology version called “baseline
      ontology”, where the most important concepts and relations
      are identified informally, which is later refined in order to
      produce a mature “target ontology”. The NeOn methodology
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>] for
      building ontology networks also includes an ontology
      specification step during which an ontology requirements
      specification document is being prepared,</p>
      <p>A key aspect regarding the use of competency questions for
      ontology authoring is testing whether a CQ can be properly
      answered. Early works on ontology testing dealt with
      formalisations of CQs for unit testing&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>], and testing instances
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0009">9</a>]. Unit
      testing for subsumption test have also been included in the
      Tawny-Owl tool&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>] Other tools which deal with ontology
      authoring tests include the eXtreme Design with Content
      Ontology Design Patterns plugin to NeON toolkit and XD
      Tools&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0013">13</a>], and
      RapidOWL&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>].</p>
      <p>Ren et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>] analysed CQs and their patterns to
      derive CQ archetypes and mappings from each to a set of the
      ontology authoring tests. Testing requires formalization of
      CQs such as into description logic queries&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>] or (most
      often) into SPARQL queries&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>]. However, the transformation from a
      natural language question to a formal SPARQL query is often
      done manually. Ideally, authoring tests should be executed
      automatically &nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>].</p>
      <p>However, approaches like Test-Driven Development of
      ontologies&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>], whose aim is to test upfront to any
      axiom authoring whether a given ontology contains requested
      knowledge or not, assume to have a set of CQs already
      formalized, e.g. in SPARQL-OWL, and to the best of our
      knowledge, <em>there is no method for automatically
      translating CQs to SPARQL-OWL queries</em>.</p>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Proposed
          approach</h2>
        </div>
      </header>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Handcrafted
            data-tuned pipeline</h3>
          </div>
        </header>
        <p>Dealing with limited number of CQs with expected
        SPARQL-OWL translations defined, we prepared a pipeline of
        data-tuned modules that are able to propose translations
        for questions in an automatic way. The most important
        novelties of this approach are as follows: (i) It solves
        the new problem of CQ to SPARQL-OWL translation. (ii) It
        uses novel method to address the lexical gap (see:
        Sect.&nbsp;<a class="sec" href="#sec-13">3.1.3</a>).</p>
        <section id="sec-11">
          <p><em>3.1.1 Entity and relation extraction.</em> In each
          question, there are important fragments of texts, that
          have to be identified in order to find the answer to the
          question. First, the question is processed, so it
          produces a sequence of tokens with part-of-speech (POS)
          tags provided for each token. Moreover the dependency
          parse tree is generated for the question. Second, given
          above information, we use a set of handcrafted POS-tag
          based rules, that can extract both entities and
          relations. The entities are defined over noun phrases
          (the rules join multiple consecutive words into a phrase
          representing the full entity). Similarly, relations are
          built over verb phrases. Sometimes, relations are
          discontinuous phrases in which the auxilary part is
          separated from the main sequence of tokens. In those
          cases, information from dependency parse tree helps to
          attach the auxilary words to the relation name. The
          example of a processed question with discontinuous
          relation tagged can be found in Fig.&nbsp;<a class="fig"
          href="#fig1">1</a>.</p>
          <figure id="fig1">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186575/images/www18companion-129-fig1.jpg"
            class="img-responsive" alt="Figure 1" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 1:</span>
              <span class="figure-title">A question tagged with POS
              tags and dependency edges. Gray boxes represent a
              discontinuous relation.</span>
            </div>
          </figure>
          <p></p>
          <p>Having extracted both entities and relations, we check
          which entities are arguments to which relation. Such
          mapping is done using dependency tree analysis: if there
          is a single edge between any token from given entity and
          any token from given relation - the entity is an argument
          for that relation.</p>
          <p>Considering the example situation presented in Figure
          <a class="fig" href="#fig1">1</a>, the relation ”is
          available as” is joined with entities: ”Weka” and ”a web
          service” since there is a nsubj connection from ”is” word
          to ”Weka” and pobj connection from ”as” to ”service”. The
          relations with their arguments are an input for the next
          step.</p>
        </section>
        <section id="sec-12">
          <p><em>3.1.2 Answer type extraction.</em> Some questions
          specify the object type that the correct answer should
          have. For example in question ”Which algorithm has its
          implementation in Java standard library?” we want the
          answers to be of the type of ”algorithm”. Our experiments
          showed, that a good heuristics for extracting the
          question target is to take the first found entity as the
          target of given question.</p>
        </section>
        <section id="sec-13">
          <p><em>3.1.3 Vocabulary matching.</em> To verify whether
          it is possible to create a query and choose its correct
          form we have to identify vocabulary in an ontology that
          matches extracted entities and relations from the
          question. The matching part can be interpreted as an
          authoring test proposed in &nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0014">14</a>]
          stating the requirements that if satisfied, the ontology
          is able to produce an answer for a CQ.</p>
          <p>We focused on a method, that will be able to bridge
          the lexical gap between vocabulary found in extracted
          parts of the questions and vocabulary from ontology. The
          module aggregates information from three sources into one
          numeric value (described as Equation <a class="eqn" href=
          "#eq1">1</a>) that allows to rank all possible vocabulary
          mappings and choose the best one. For each relation we
          check each property label from an ontology and for each
          entity we check all class labels and individual labels as
          possible translations. Function <a class="eqn" href=
          "#eq1">1</a> creates a ranking, where top ranked
          proposition is used for further processing.</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \begin{split} \text{score} = \text{cos} -
              0.75(\text{support penalty} + \text{restriction
              penalty}) \\ \end{split} \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>The function takes as an input relation extracted
          from text as well as its arguments and currently analyzed
          translation propositions for the relation and arguments.
          It generates a value from range from -0.5 to 1.0. The
          elements of the equation <a class="eqn" href="#eq1">1</a>
          are defined as follows:
          <p></p>
          <p><strong>cos</strong> – We have trained a word
          embeddings model using a large domain-specific corpora
          and Fasttext as a training method, thus we obtained a
          mapping of words into their semantic space, where
          elements close in meaning are located close to each
          other. Having candidate translations for a relation and
          its arguments, we calculate the similarity of the
          relation and the given property label. Separately, we
          calculate similarity of each entity with a candidate
          class label or an individual label. The calculation uses
          cosine similarity in the embedding space. To calculate
          similarity between entities consisting of multiple words,
          we represent the whole entity as an average value of its
          word embedding vectors. If similarity score is lower than
          a threshold <em>θ</em> whose value is chosen with
          grid-search on a given ontology – the translation is not
          interpreted as a valid translation. This limitation
          reduces the complexity of the algorithm rejecting
          unlikely translations. Our experiments showed that for
          entities, the threshold value should be quite big (≈
          0.8), and for relations as small as possible (0.0). The
          relation names in an ontology are often general so
          sources other than semantic similarity of phrases should
          be used to choose the best property as the translation.
          Similarities of the relation and its arguments are
          aggregated into one number with their average value.</p>
          <p><strong>restriction penalty</strong> – is a value
          between 0.0 and 1.0 that is calculated after analysis of
          the domain and range of a given property. If property can
          co-occur with entities of given types - the penalty score
          is minimal, if it cannot occur with any of the proposed
          types - the penalty is maximal. For example, if a
          property candidate is: ”has defined licence” that expects
          ”Software” in its domain and ”licence” in range - if
          arguments translations are of different types, the
          penalty should be high, because they are not supposed as
          a good choice for that property. In that case different
          property or different argument translations should be
          used.</p>
          <p><strong>support penalty</strong> – axioms defined in
          an ontology are a good source of information. If the
          proposed property co-occurs with proposed entity
          translations - the penalty score is minimal (0.0), if
          there is no support for that proposition (no axiom uses
          the proposed translations together), the penalty is
          maximal (1.0).</p>
        </section>
        <section id="sec-14">
          <p><em>3.1.4 SPARQL-OWL query construction.</em> In order
          to construct SPARQL-OWL query we only consider the top
          ranked tuple (with the highest score value). If the top
          ranked value is scored below zero, the system returns
          information that it is not confident enough to generate
          the query, thus, probably there is no possible
          translation for that case. Else, the query is constructed
          in a rule-based way.</p>
        </section>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> End-to-end
            deep learning approach</h3>
          </div>
        </header>
        <p>The last years proved that deep neural networks are able
        to solve many tasks in which humans were superior to
        computers, including text processing. Some successful
        applications of deep learning from a related area of
        question answering (e.g., [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>]) motivate research direction
        aiming at an end-to-end learning approach for translating
        natural language to SPARQL-OWL. However, research on using
        deep learning for transforming language to complex, logical
        representations (like those grounded in description logics)
        have been so far scarce&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>]. Moreover, the existing approaches
        are not targeted to test-driven ontology authoring, and
        thus do not tackle issues of generating logical
        representations taking this task into account, nor they are
        targeted to interpreting the results of produced
        translations as components of authoring tests.</p>
        <p>The memory networks with inference components combined
        with a long-term memory component [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0022">22</a>] might be an interesting
        model choice since it has been used in a related task of
        question answering with success. Unfortunately, to create
        such a method and validate it properly we have to collect
        huge amount of CQs with SPARQL-OWL translations, so that
        the method will be able to capture patterns and translation
        rules. The idea of generating huge amount of CQs for a
        given ontology is the following process:</p>
        <p>(i) find an ontology with an existing set of competency
        questions,</p>
        <p>(ii) create a SPARQL-OWL query for each competency
        question (where possible),</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186575/images/www18companion-129-img3.svg"
        class="img-responsive" alt="" longdesc="" /> Where by CE1,
        CE2 are denoted class expressions and by PE1 a property
        expression and then generate all possible CE1, CE2, PE1
        fillings that are allowed by the ontology. If an ontology
        has many classes and properties - the generation step
        should output a big number of potential competency
        questions for a given pattern set. With that dataset we
        would like to conduct experiments using aforementioned deep
        learning approach.</p>
        <p>The work of Ren at al.&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0014">14</a>] shows that a huge
        fraction of most common patterns should occur in different
        ontologies as well.</p>
        <p>We would like to experiment with that procedure in the
        nearest future. The novelty of such a method will be in
        using a memory network for translation from a CQ to
        SPARQL-OWL.</p>
      </section>
    </section>
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span>
          Methodology</h2>
        </div>
      </header>
      <p>The problem is a novel one, so there is no existing
      benchmark available. In that case, our group collected real
      world competency questions created for publicly available
      ontologies that were then translated into SPARQL-OWL queries.
      Researchers from the European Informatics Institute and the
      University of Manchester created the Software Ontology,
      available at webpage<a class="fn" href="#fn2" id=
      "foot-fn2"><sup>2</sup></a>, being an artifact describing
      software. For that ontology, authors collected CQs that are
      available at webpage<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>3</sup></a>. In total, there are 91 (90
      without duplicates) questions formulated in a general way,
      for example: What are the alternatives to this software?,
      What other alternatives are there?.</p>
      <p>For every question, in order to propose SPARQL-OWL
      translation, 3 steps were performed:</p>
      <p>(i) Fill the questions with actual entities from the
      ontology – after this step, general questions like: ”What are
      the alternatives to this software?” changed to specific ones:
      ”What are the alternatives to Protégé?”</p>
      <p>(ii) Rephrase ungrammatical questions, and fill with
      additional important data – after this step questions like:
      ”What other alternatives are there” are changed to ”What
      other alternatives to Weka software are there?”</p>
      <p>(iii) Translate questions from natural language to
      SPARQL-OWL by hand, searching for best-fitting concepts from
      ontology. The translations were validated by another
      SPARQL-OWL specialist.</p>
      <p>The translation phase divided 90 questions into two
      separate groups, depending on the ability of experts to
      translate questions from our golden-standard into SPARQL-OWL
      queries:</p>
      <p>(i) Translatable group – Questions with translation
      defined: <strong>42</strong>.</p>
      <p>(ii) Untranslatable group – no translation defined (at the
      current ontology maturity it is impossible to create a
      correct query): <strong>48</strong>,</p>
      <p>The number of translatable questions is quite small, so at
      the moment we cannot use high-end methods like deep learning
      without creating examples artificially. Moreover, with such a
      small set, any machine learning method will be unable to
      learn the patterns. The experiments utilizing machine
      learning methods will be conducted when large enough dataset
      will be collected. Thus, we decided to create a general
      purpose method from manually created pipeline of tools and
      use the collected set to tune the parameters and measure
      evaluation metrics.</p>
      <p>For most questions from the translatable group, the
      ontology is unable to provide an answer although the proposed
      query is correct. The reason for that is the fact that the
      ontology has the needed vocabulary defined, but this
      vocabulary does not coexist within individual axioms.
      However, since we aim at testing ontology for the contained
      knowledge, we evaluate our translation method by calculating
      the percentage of the cases which are correctly translated,
      i.e., in which our golden-standard in SPARQL-OWL is identical
      to the generated one, currently by checking query string
      representation equivalence. In the future we would like to
      use more advanced method, like checking query containment
      with respect to an ontology. We divided the evaluation
      process into two phases:</p>
      <p>(i) Is the ontology able to propose a query? - the system
      should check whether there is a vocabulary needed for
      translation and if selected classes and properties can
      co-occur together in one query. If there is no sufficient
      vocabulary, the only result for that case should be the error
      information.</p>
      <p>(ii) Does the ontology provide correct SPARQL-OWL query? -
      when the system decides, that there is sufficient vocabulary
      and is certain that it is possible to construct a query, the
      system checks if the generated query is correct or not by
      verification if generated and expected queries are
      identical.</p>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Research
            questions - experiments</h3>
          </div>
        </header>
        <p>To address the research questions, we propose the
        following list of experiments:</p>
        <p><strong>Research Question 1</strong>: Both methods
        (handcrafted pipeline and neural network) will output the
        confidence score. After setting the best acceptance
        threshold value, computed using grid-search, we would
        interpret it as a confidence level above which we can state
        that it is possible to create the query.</p>
        <p><strong>Research Question 2</strong>: Because the golden
        standard is defined, we would like to measure how many
        generated queries are identical to expected ones. Our goal
        is to maximize the fraction of identical translation to all
        generated translations.</p>
        <p><strong>Research Question 3</strong>: Having defined
        golden standard, we would like to measure how many expected
        queries using those constructs are generated automatically
        in a correct way.</p>
        <p><strong>Research Question 4</strong>: Dividing the
        questions set into subcategories, where different
        subcategories contain different OWL constructs and
        evaluating them separately will show us what aspects of OWL
        language are hard to being modelled using our method.</p>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Results</h2>
        </div>
      </header>
      <p>Because the task is hard, we have split the translatable
      group into two categories of different difficulty:</p>
      <p>(i) Simple SPARQL-OWL - queries containing one property
      and built with the following constructs: rdfs:subClassOf,
      owl:someValuesFrom, owl:allValuesFrom. 14 cases were
      classified as simple ones.</p>
      <p>(ii) Complex SPARQL-OWL - queries with multiple properties
      or OWL constructs: owl:intersectionOf, owl:unionOf. 29 cases
      were classified as complex.</p>
      <p>Evaluation on Simple SPARQL-OWL proved that it is possible
      to generate correct SPARQL-OWL queries at least for that
      group. From 14 simple SPARQL-OWL questions, 8 were correctly
      translated by our algorithm. Thus, we obtain 57,14% of
      accuracy.</p>
      <p>The experiment showed that the method is able to generate
      SPARQL-OWL queries even when the expected translation is not
      obvious due to the lexical gap. For the CQ: Is Weka available
      as a web service? it generated a query that asks if Weka is
      connected with a web service by ’has interface’ property
      which captured the sense of the question even with very
      different property name. Handling complex questions is a
      currently researched part of the thesis.</p>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusions and
          future work</h2>
        </div>
      </header>
      <p>The first experiments showed that translation from
      competency questions into SPARQL-OWL queries is a challenge.
      Multiplicity of forms in which ontologies can be created,
      lack of vocabulary in an ontology and limited number of
      competency questions available make the task hard to solve.
      The experiments we made so far prove that it is possible to
      create translations at least in the case of simple
      SPARQL-OWL. With domain-trained word-embeddings, axioms from
      ontology and property restrictions defined, we can often
      choose the correct form of the query.</p>
      <p>The most interesting part of the work will be defining how
      to make use of more complex OWL classes like those involving
      intersection of classes, union of classes etc. These are
      challenging, because they allow to construct new class out of
      existing ones and map that complex construct to a part of
      text (for instance, intersection of classes is a new unnamed
      class with properties that are shared between intersected
      classes).</p>
      <p>In order to address the problem correctly we have to
      collect much more data. Since for most ontologies only very
      limited list of competency questions is provided, the
      interesting idea is to create a synthetically prepared set of
      CQ on which machine learning algorithms can be trained. For a
      big enough dataset, we plan to construct memory networks,
      that will try to solve the task in an end-to-end manner. The
      choice of memory networks is due to their successful
      application in logic-related tasks.</p>
      <p><strong>Acknowledgements.</strong> This work was supported
      by the National Science Center (Grant No 2014/13/D/ST6/02076)
      and from grant 09/91/DSPB/0627.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">S. Auer. 2006. The
        RapidOWL Methodology–Towards Agile Knowledge Engineering.
        In <em><em>Enabling Technologies: Infrastructure for
        Collaborative Enterprises, 2006. WETICE ’06. 15th IEEE
        International Workshops on</em></em> . 352–357. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1109/WETICE.2006.67" target=
        "_blank">https://doi.org/10.1109/WETICE.2006.67</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">C. Bezerra, F. Freitas,
        and F. Santana. 2013. Evaluating Ontologies with Competency
        Questions. In <em><em>2013 IEEE/WIC/ACM International
        Conferences on Web Intelligence and Intelligent Agent
        Technology</em></em> (<em>WI-IAT ’13</em>). 284–285.</li>
        <li id="BibPLXBIB0003" label="[3]">E Blomqvist, A.S.
        Sepour, and V. Presutti. 2012. Ontology testing –
        methodology and tool. In <em><em>18th International
        Conference on Knowledge Engineering and Knowledge
        Management (EKAW’12)</em></em> (<em>LNAI</em>),
        Vol.&nbsp;7603. Springer, 216–226.</li>
        <li id="BibPLXBIB0004" label="[4]">P.&nbsp;C&nbsp;Barbosa
        Fernandes, R.&nbsp;SS Guizzardi, and G. Guizzardi. 2011.
        Using goal modeling to capture competency questions in
        ontology-based systems. <em><em>Journal of Information and
        Data Management</em></em> 2, 3 (2011), 527.</li>
        <li id="BibPLXBIB0005" label="[5]">S. Garca-Ramos, A.
        Otero, and M Fernández-López. 2009. OntologyTest: A tool to
        evaluate ontologies through tests defined by the user. In
        <em><em>10th International Work-Conference on Artificial
        Neural Networks, IWANN 2009 Workshops, Proceedings, Part
        II</em></em> (<em>LNCS</em>), S.&nbsp;Omatuet&nbsp;al.
        (Eds.), Vol.&nbsp;5518. Springer, 91–98. Salamanca, Spain,
        June 10-12, 2009.</li>
        <li id="BibPLXBIB0006" label="[6]">Bikash Gyawali,
        Anastasia Shimorina, Claire Gardent, Samuel Cruz-Lara, and
        Mariem Mahfoudh. 2017. <em><em>Mapping Natural Language to
        Description Logic</em></em> . Springer International
        Publishing, Cham, 273–288. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1007/978-3-319-58068-5_17" target=
        "_blank">https://doi.org/10.1007/978-3-319-58068-5_17</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">C.&nbsp;M. Keet and A.
        Ławrynowicz. 2016. Test-Driven Development of Ontologies.
        In <em><em>The Semantic Web. Latest Advances and New
        Domains - 13th International Conference, ESWC
        2016</em></em> (<em>LNCS</em>), Harald Sacket&nbsp;al.
        (Eds.), Vol.&nbsp;9678. 642–657.</li>
        <li id="BibPLXBIB0008" label="[8]">I. Kollia, B. Glimm, and
        I. Horrocks. 2011. SPARQL Query Answering over OWL
        Ontologies. In <em><em>The Semantic Web: Research and
        Applications ESWC 2011</em></em> (<em>LNCS</em>), Grigoris
        A.et&nbsp;al. (Eds.), Vol.&nbsp;6643. Springer,
        382–396.</li>
        <li id="BibPLXBIB0009" label="[9]">D. Kontokostas, P.
        Westphal, Sören Auer, Sebastian Hellmann, Jens Lehmann,
        Roland Cornelissen, and Amrapali Zaveri. 2014. Test-driven
        Evaluation of Linked Data Quality. In <em><em>Proc. of
        WWW’14</em></em> . ACM proceedings, 747–758.</li>
        <li id="BibPLXBIB0010" label="[10]">J. Lehmann <em>et
        al.</em> 2014. DBpedia - A Large-scale, Multilingual
        Knowledge Base Extracted from Wikipedia. <em><em>Semantic
        Web Journal</em></em> (2014).</li>
        <li id="BibPLXBIB0011" label="[11]">Y. Malheiros and F.
        Freitas. 2013. A Method to Develop Description Logic
        Ontologies Iteratively Based on Competency Questions: an
        Implementation. In <em><em>Proc. of the 6th Seminar on
        Ontology Research in Brazil</em></em> . 142–153.</li>
        <li id="BibPLXBIB0012" label="[12]">Giulio Petrucci, Chiara
        Ghidini, and Marco Rospocher. 2016. <em><em>Ontology
        Learning in the Deep</em></em> . Springer International
        Publishing, Cham, 480–495. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1007/978-3-319-49004-5_31" target=
        "_blank">https://doi.org/10.1007/978-3-319-49004-5_31</a>
        </li>
        <li id="BibPLXBIB0013" label="[13]">V. Presutti, E Daga,
        <em>et al.</em> 2009. eXtreme design with content ontology
        design patterns. In <em><em>Proc. of WS on OP’09</em></em>
        (<em>CEUR-WS</em>), Vol.&nbsp;516. 83–97.</li>
        <li id="BibPLXBIB0014" label="[14]">Y. Ren, A. Parvizi,
        <em>et al.</em> 2014. Towards Competency Question-Driven
        Ontology Authoring. In <em><em>The Semantic Web: Trends and
        Challenges ESWC 2014</em></em> (<em>LNCS</em>), Valentina
        Presuttiet&nbsp;al. (Eds.), Vol.&nbsp;8465. Springer,
        752–767.</li>
        <li id="BibPLXBIB0015" label="[15]">Daniil Sorokin and
        Iryna Gurevych. 2017. <em><em>End-to-End Representation
        Learning for Question Answering with Weak
        Supervision</em></em> . Springer International Publishing,
        Cham, 70–83. <a class="link-inline force-break" href=
        "https://doi.org/10.1007/978-3-319-69146-6_7" target=
        "_blank">https://doi.org/10.1007/978-3-319-69146-6_7</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Steffen Staab, Rudi
        Studer, Hans-Peter Schnurr, and York Sure. 2001. Knowledge
        Processes and Ontologies. <em><em>IEEE Intelligent
        Systems</em></em> 16, 1 (2001), 26–34. <a class=
        "link-inline force-break" href=
        "http://dblp.uni-trier.de/db/journals/expert/expert16.html#StaabSSS01"
          target=
          "_blank">http://dblp.uni-trier.de/db/journals/expert/expert16.html#StaabSSS01</a>
        </li>
        <li id="BibPLXBIB0017" label="[17]">M.&nbsp;C.
        Suárez-Figueroa, A. Gómez-Pérez, and M. Fernández-López.
        2015. The NeOn Methodology framework: A scenario-based
        methodology for ontology development. <em><em>Applied
        Ontology</em></em> 10, 2 (2015), 107–145.</li>
        <li id="BibPLXBIB0018" label="[18]">Mari-Carmen
        Suarez-Figueroa, Asuncion Gomez-Perez, Enrico Motta, and
        Aldo Gangemi. 2012. <em><em>Ontology Engineering in a
        Networked World</em></em> . Springer, Berlin.</li>
        <li id="BibPLXBIB0019" label="[19]">M. Uschold and M.
        Gruninger. 1996. Ontologies: principles, methods and
        applications. <em><em>Knowledge Eng. Review</em></em> 11, 2
        (1996), 93–136.</li>
        <li id="BibPLXBIB0020" label="[20]">Danny Vrandečić and
        Aldo Gangemi. 2006. Unit tests for ontologies. In
        <em><em>OTM workshops 2006</em></em> (<em>LNCS</em>),
        Vol.&nbsp;4278. Springer, 1012–1020.</li>
        <li id="BibPLXBIB0021" label="[21]">J.&nbsp;D. Warrender
        and P. Lord. 2015. <em><em>How, What and Why to test an
        ontology</em></em> . Technical Report 1505.04112. Newcastle
        University. http://arxiv.org/abs/1505.04112.</li>
        <li id="BibPLXBIB0022" label="[22]">J. Weston, S. Chopra,
        and A. Bordes. 2014. Memory Networks. <em><em>ArXiv
        e-prints</em></em> (Oct. 2014). arxiv:cs.AI/1410.3916</li>
        <li id="BibPLXBIB0023" label="[23]">L. Zemmouchi-Ghomari
        and A.&nbsp;R. Ghomari. 2013. Translating natural language
        competency questions into SPARQL Queries: a case study. In
        <em><em>The First International Conference on Building and
        Exploring Web Based Environments</em></em> . 81–86.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.w3.org/TR/sparql11-query/#func-filter-exists">https://www.w3.org/TR/sparql11-query/#func-filter-exists</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "http://theswo.sourceforge.net">http://theswo.sourceforge.net</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://softwareontology.wordpress.com/2011/04/01/user-sourced-competency-questions-for-software/">https://softwareontology.wordpress.com/2011/04/01/user-sourced-competency-questions-for-software/</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186575">https://doi.org/10.1145/3184558.3186575</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
