<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Attention Network for Information Diffusion Prediction</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3184558.3186931"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186931'>https://doi.org/10.1145/3184558.3186931</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186931'>https://w3id.org/oa/10.1145/3184558.3186931</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Attention Network for Information Diffusion Prediction</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Zhitao</span> <span class="surName">Wang</span> Department of Computing, The Hong Kong Polytechnic University, Hong Kong
        </div>
        <div class="author">
          <span class="givenName">Chengyao</span> <span class="surName">Chen</span> Department of Computing, The Hong Kong Polytechnic University, Hong Kong
        </div>
        <div class="author">
          <span class="givenName">Wenjie</span> <span class="surName">Li</span> Department of Computing, The Hong Kong Polytechnic University, Hong Kong, <a href="mailto:csztwang,%20cscchen,%20cswjli@comp.polyu.edu.hk">csztwang, cscchen, cswjli@comp.polyu.edu.hk</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186931" target="_blank">https://doi.org/10.1145/3184558.3186931</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>In this paper, we propose an attention network for diffusion prediction problem. The developed diffusion attention module can effectively explore the implicit user-to-user diffusion dependency among information cascade users. Besides, the user-to-cascade importance and the time-decay effect are captured and utilized by the model. The superiority of the proposed model over state-of-the-art methods is demonstrated by experiments on real diffusion data.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Data mining;</strong> • <strong>Computing methodologies</strong> → <strong>Neural networks;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Information Diffusion</small>,</span> <span class="keyword"><small>Attention Network</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Zhitao Wang, Chengyao Chen, and Wenjie Li. 2018. Attention Network for Information Diffusion Prediction. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 3 Pages. <a href="https://doi.org/10.1145/3184558.3186931" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186931</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-2">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Information diffusion has been studied with a long history. Early researches focus on the expansion of the fundamental theoretical models, such as independent cascade (IC) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]. Recently, with the development of online media, information cascades of diffusion are massively traced. This provides great opportunities for applied studies, e.g., predicting real diffusion process. Different from theoretical assumption, real cascades are often recorded as sequences, where the underlying graph of user relationship is not available. This drives researchers to solve the problem from a data-driven view. They formulated diffusion prediction as the sequence prediction problem, which is to predict the future affected user given the previously affected users. A family of sequential models were proposed and their effectivenesses were demonstrated on the real diffusion data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>].</p>
      <p>However, due to the constraints of the underlying graph, the information cascades in real data do not strictly follow the assumptions of sequential models. For example, given a cascade {(<em>A</em>, <em>t<sub>A</sub></em> ), (<em>B</em>, <em>t<sub>B</sub></em> ), (<em>C</em>, <em>t<sub>C</sub></em> ), (<em>D</em>, <em>t<sub>D</sub></em> )} and a underlying graph with edges <em>A</em> → <em>B</em>, <em>A</em> → <em>C</em> and <em>B</em> → <em>D</em>, the sequential models assume that the infections of C and D are related to the sequential state at <em>t<sub>B</sub></em> and <em>t<sub>C</sub></em> , but in fact, C and D are directly dependent on A and B respectively. Though the gating mechanisms in existing sequential models can selectively drop the information of B when generating C at <em>t<sub>C</sub></em> , it also leads to the loss of dependency from D to B at next time step. The compressed state is not expressive enough for such non-sequential diffusion dependency, thus the prediction ability is limited.</p>
      <p>We propose an attention network to explore the diffusion dependency among cascade users. Specifically, we develop a diffusion attention module to capture user dependency and extract dependency-aware information in embedding space. A fusion gate is designed to integrate user self embedding and its dependency-aware information. Given the fused user embedding, the cascade embedding for prediction is generated through user information composition, where both the user-to-cascade importance and the time-decay effect are considered. We evaluate the proposed model on the real diffusion data. The better performance than popular graph-based models and state-of-the-art sequential models indicates that the proposed model effectively capture the underlying diffusion dependency with attention mechanisms.</p>
    </section>
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Method</h2>
        </div>
      </header>
      <p>A cascade can be represented as <em>c</em> = {(<em>u</em> <sub>1</sub>, <em>t</em> <sub>1</sub>), (<em>u</em> <sub>2</sub>, <em>t</em> <sub>2</sub>), ..., (<em>u<sub>c</sub></em> , <em>t<sub>c</sub></em> )}, where element denotes user <em>u<sub>i</sub></em> is infected at time <em>t<sub>i</sub></em> . The diffusion prediction problem is to predict next user <em>u</em> <sub><em>i</em> + 1</sub> with given the previously infected users {(<em>u</em> <sub>1</sub>, <em>t</em> <sub>1</sub>), ..., (<em>u<sub>i</sub></em> , <em>t<sub>i</sub></em> )}.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186931/images/www18companion-171-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title"><strong>Model Overview</strong></span>
        </div>
      </figure>
      <p></p>
      <p>The framework of the proposed <strong>D</strong>iffusion <strong>A</strong>ttention <strong>N</strong>etwork (<strong>DAN</strong>) is illustrated in Fig. <a class="fig" href="#fig1">1</a>. Each user <em>u</em> in cascade is initially embedded into a low-dimensional vector <span class="inline-equation"><span class="tex">$\mathbf {x}\in \mathbb {R}^{d_x}$</span></span> as the input of model. The model then transforms the input as hidden user embeddings with a feed-forward layer: <strong>h</strong> = ELU(<strong>W</strong> <sub><em>x</em></sub> <strong>x</strong> + <strong>b</strong> <sub><em>x</em></sub> ), where <span class="inline-equation"><span class="tex">$\mathbf {W}_x\in \mathbb {R}^{d_x\times d_h}$</span></span> , <span class="inline-equation"><span class="tex">$\mathbf {h}, \mathbf {b}_x\in \mathbb {R}^{d_h}$</span></span> and ELU represents the Exponential Linear Unit non-linear activation function. Given the user embeddings <strong>h</strong>, a dependency-aware information vector is constructed for each affected user through the Diffusion Attention Module. The module employs user-to-user attention mechanism to extract diffusion dependency between each user and its previously affected users. For cascade user <em>u<sub>j</sub></em> ∈ {<em>u</em> <sub>1</sub>, ..., <em>u<sub>i</sub></em> }, it defines the attention of <em>u<sub>j</sub></em> to its previous user <em>u<sub>k</sub></em> ∈ {<em>u</em> <sub>1</sub>, ..., <em>u</em> <sub><em>j</em> − 1</sub>} as follow:</p>
      <div class="table-responsive" id="Xeq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \alpha _{jk} = \frac{\exp (\langle \mathbf {W}_h^1\mathbf {h}_j,\mathbf {W}_h^2\mathbf {h}_k \rangle)}{\sum _{l=1}^{j-1} \exp (\langle \mathbf {W}_h^1\mathbf {h}_j,\mathbf {W}_h^2\mathbf {h}_l \rangle)} \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>where <span class="inline-equation"><span class="tex">$\mathbf {W}_h^1, \mathbf {W}_h^2\in \mathbb {R}^{d_h\times d_h}$</span></span> and ⟨, ⟩ represents inner product. Based on above attention, the dependency-aware information vector of <em>u<sub>j</sub></em> is computed as: <span class="inline-equation"><span class="tex">$\mathbf {d}_{j} = \sum _{k=1}^{j-1}\alpha _{jk}\mathbf {h}_k$</span></span> .
      <p></p>
      <p>To integrate user representation <strong>h</strong> <sub><em>j</em></sub> and its dependency-aware information <strong>d</strong> <sub><em>j</em></sub> , a fusion gating mechanism is designed as follow:</p>
      <div class="table-responsive" id="Xeq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mathbf {g}_j = \mbox{sigmoid}(\mathbf {W}_g^1\mathbf {h}_j + \mathbf {W}_g^2\mathbf {d}_j + \mathbf {b}_g) \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>
      <div class="table-responsive" id="Xeq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mathbf {u}_j = \mathbf {g}_j\odot \mathbf {h}_j + (1-\mathbf {g}_j)\odot \mathbf {d}_j \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>where <span class="inline-equation"><span class="tex">$\mathbf {W}_g^1, \mathbf {W}_g^2\in \mathbb {R}^{d_h\times d_h}$</span></span> and <span class="inline-equation"><span class="tex">$\mathbf {g}, \mathbf {b}_g\in \mathbb {R}^{d_h}$</span></span> .
      <p></p>
      <p>Because all previously infected users may trigger the next infection at time <em>t</em> <sub><em>i</em> + 1</sub>, we compose each extracted user information <strong>u</strong> in cascade at <em>t<sub>i</sub></em> through User Composition Module. It considers both the user-to-cascade importance and the time-decay effect. The importance of <em>u<sub>j</sub></em> to cascade is calculated by an attention mechanism as follow:</p>
      <div class="table-responsive" id="Xeq4">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \beta _{j} = \frac{\exp (\mathbf {w}^T \mbox{ELU}(\mathbf {W}_u\mathbf {u}_j + \mathbf {b}_u))}{\sum _{k=1}^{i} \exp (\mathbf {w}^T \mbox{ELU}(\mathbf {W}_u\mathbf {u}_k + \mathbf {b}_u))} \end{equation}</span><br />
          <span class="equation-number">(4)</span>
        </div>
      </div>where <span class="inline-equation"><span class="tex">$\mathbf {W}_u\in \mathbb {R}^{d_h\times d_h}$</span></span> , <span class="inline-equation"><span class="tex">$\mathbf {w}, \mathbf {b}_u\in \mathbb {R}^{d_h}$</span></span> . As for capturing time-decay effect on <em>u<sub>j</sub></em> at time <em>t<sub>i</sub></em> , we transform the time interval <em>t<sub>i</sub></em> − <em>t<sub>j</sub></em> to a one hot vector <span class="inline-equation"><span class="tex">$\mathbf {t}^j\in \mathbb {R}^{L}$</span></span> , where <span class="inline-equation"><span class="tex">$\mathbf {t}^j_n = 1$</span></span> if <em>t</em> <sub><em>n</em> − 1</sub> &lt; <em>t<sub>i</sub></em> − <em>t<sub>j</sub></em> &lt; <em>t<sub>n</sub></em> . The critical time points <em>t</em> <sub><em>n</em> − 1</sub>, <em>t<sub>n</sub></em> are defined by splitting the time range (0, <em>T</em>] into <em>L</em> disjoint intervals {(0, <em>t</em> <sub>1</sub>], ...(<em>t</em> <sub><em>n</em> − 1</sub>, <em>t<sub>n</sub></em> ], ..., (<em>t</em> <sub><em>L</em> − 1</sub>, <em>T</em>], where <em>T</em> is max observation time in dataset. In the experiments, we set <em>T</em> as 120 hours and the number of intervals <em>L</em> as 40. Given the <strong>t</strong> <sup><em>j</em></sup> , the model derives the time-decay weight as: <em>λ<sub>j</sub></em> = λ · <strong>t</strong> <sup><em>j</em></sup> , where <span class="inline-equation"><span class="tex">$ {\lambda }\in \mathbb {R}^{L}$</span></span> is the parameter vector to be learned. Based on the extracted user-to-cascade importance and time-decay weight, the module finally generates the embedding of cascade at time <em>t<sub>i</sub></em> as: <span class="inline-equation"><span class="tex">$\mathbf {c}_i = \sum _{j=1}^{i}\beta _{j}\lambda _{j}\mathbf {u}_j$</span></span> .
      <p></p>
      <p>Given the above embedding <strong>c</strong> <sub><em>i</em></sub> , the model can output the probability distribution of next infected user as follow:</p>
      <div class="table-responsive" id="Xeq5">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \hat{P}(u_{i+1}|\mathbf {c}_i) = \mbox{softmax}(\mathbf {W}_c\mathbf {c}_i + \mathbf {b}_c) \end{equation}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>where <span class="inline-equation"><span class="tex">$\mathbf {W}_c\in \mathbb {R}^{d_h\times d_h}$</span></span> and <span class="inline-equation"><span class="tex">$\mathbf {b}_c\in \mathbb {R}^{d_h}$</span></span> . The model is trained by minimizing the cross-entropy loss between the predicted <span class="inline-equation"><span class="tex">$\hat{P}$</span></span> and true probability of <em>u</em> <sub><em>i</em> + 1</sub>. The backpropagation algorithm is exploited and parameters are updated by Adadelta optimizer with mini-batch.
      <p></p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experiments</h2>
        </div>
      </header>
      <p>The representative real diffusion data, i.e., MemeTracker [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>], is used to evaluate the performance of the proposed model. The final size of MemeTracker data for experiment is: 1109 nodes, 33992 training cascades, 4250 validation cascades and 4250 testing cascades.</p>
      <p>We compare the proposed model with the following state-of-the-art baselines: Continuous Time Independent Cascade (CTIC) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] is a representative theoretical model; Embedded Independent Cascade (EIC) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] is a state-of-the-art representation learning model for diffusion prediction; RNN is the basic recurrent neural network sequential model; LSTM is a stronger RNN-based model which employs an effective gating mechanism; CYAN-RNN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] is the latest sequence-to-sequence method for cascade prediction. The input embedding and hidden embedding sizes are respectively set as 32 and 64 for all neural models. Other parameters of baselines follow the recommended settings in original papers. For the proposed DAN, the learning rate of Adadelta optimizer is set as 0.5.</p>
      <p>The performance is evaluated by predicting next infected user. Given previously infected users, models can estimate the next infection probability of each user. Due to the large number of potential targets, this prediction task is often regarded as a ranking problem [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]. We employ two widely used ranking metrics for evaluation: Mean Reciprocal Rank (<em>MRR</em>) and Accuracy on top k (<em>A@k</em>).</p>
      <p>The experimental results are shown in Table 1. The representative RNN-based sequential models achieve relatively better performance among baselines. In spite of employing sequential techniques, CYANRNN achieves relatively worse results, possibly because the sequence-to-sequence architecture in CYANRNN may be not well adaptive to the single-chain structure of cascades. The proposed DAN outperforms all baselines in terms of all metrics. It gains a significant improvement over the best baseline, i.e., LSTM. This demonstrates that, compared with the state-of-the-art sequence gating method, the proposed attention architecture is more aware of the non-sequential diffusion dependency among cascade users thus it has better prediction ability for diffusion cascades.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class="table-title">Diffusion Prediction Performance(%)</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;"></td>
              <td colspan="3" style="text-align:center;">
                <strong>Validation</strong>
                <hr />
              </td>
              <td colspan="3" style="text-align:center;">
                <strong>Testing</strong>
                <hr />
              </td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;"><em>MRR</em></td>
              <td style="text-align:center;"><em>A@5</em></td>
              <td style="text-align:center;"><em>A@10</em></td>
              <td style="text-align:center;"><em>MRR</em></td>
              <td style="text-align:center;"><em>A@5</em></td>
              <td style="text-align:center;"><em>A@10</em></td>
            </tr>
            <tr>
              <td style="text-align:center;">CTIC</td>
              <td style="text-align:center;">8.27</td>
              <td style="text-align:center;">10.35</td>
              <td style="text-align:center;">13.18</td>
              <td style="text-align:center;">7.83</td>
              <td style="text-align:center;">9.96</td>
              <td style="text-align:center;">12.54</td>
            </tr>
            <tr>
              <td style="text-align:center;">EIC</td>
              <td style="text-align:center;">6.36</td>
              <td style="text-align:center;">9.07</td>
              <td style="text-align:center;">10.29</td>
              <td style="text-align:center;">5.60</td>
              <td style="text-align:center;">8.23</td>
              <td style="text-align:center;">10.77</td>
            </tr>
            <tr>
              <td style="text-align:center;">RNN</td>
              <td style="text-align:center;">22.87</td>
              <td style="text-align:center;">31.08</td>
              <td style="text-align:center;">39.87</td>
              <td style="text-align:center;">23.26</td>
              <td style="text-align:center;">31.17</td>
              <td style="text-align:center;">40.23</td>
            </tr>
            <tr>
              <td style="text-align:center;">LSTM</td>
              <td style="text-align:center;">24.15</td>
              <td style="text-align:center;">32.96</td>
              <td style="text-align:center;">41.44</td>
              <td style="text-align:center;">24.53</td>
              <td style="text-align:center;">33.01</td>
              <td style="text-align:center;">41.87</td>
            </tr>
            <tr>
              <td style="text-align:center;">CYANRNN</td>
              <td style="text-align:center;">11.20</td>
              <td style="text-align:center;">16.78</td>
              <td style="text-align:center;">22.36</td>
              <td style="text-align:center;">10.63</td>
              <td style="text-align:center;">15.24</td>
              <td style="text-align:center;">21.97</td>
            </tr>
            <tr>
              <td style="text-align:center;">DAN</td>
              <td style="text-align:center;"><strong>26.20</strong></td>
              <td style="text-align:center;"><strong>36.01</strong></td>
              <td style="text-align:center;"><strong>45.82</strong></td>
              <td style="text-align:center;"><strong>26.17</strong></td>
              <td style="text-align:center;"><strong>35.53</strong></td>
              <td style="text-align:center;"><strong>45.79</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>The work in this paper was supported by Research Grants Council of Hong Kong (PolyU 152094/14E), National Natural Science Foundation of China (61272291) and The Hong Kong Polytechnic University (G-YBJP, 4-BCB5).</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Simon Bourigault, Sylvain Lamprier, and Patrick Gallinari. 2016. Representation learning for information diffusion through social networks: an embedded cascade model. In <em><em>Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</em></em> (<em>WSDM ’16</em>). ACM, 573–582.</li>
        <li id="BibPLXBIB0002" label="[2]">Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In <em><em>Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> (<em>KDD ’10</em>). ACM, 497–506.</li>
        <li id="BibPLXBIB0003" label="[3]">Manuel&nbsp;Gomez Rodriguez, David Balduzzi, and Bernhard Schölkopf. 2011. Uncovering the Temporal Dynamics of Diffusion Networks. In <em><em>Proceedings of the 28th International Conference on Machine Learning</em></em> (<em>ICML ’11</em>). ACM, 561–568.</li>
        <li id="BibPLXBIB0004" label="[4]">Kazumi Saito, Masahiro Kimura, Kouzou Ohara, and Hiroshi Motoda. 2009. Learning continuous-time information diffusion model for social behavioral data analysis. In <em><em>Asian Conference on Machine Learning</em></em> . Springer, 322–337.</li>
        <li id="BibPLXBIB0005" label="[5]">Shenghua Liu Jinhua Gao Xueqi&nbsp;Cheng Yongqing&nbsp;Wang, Huawei&nbsp;Shen. 2017. Cascade Dynamics Modeling with Attention-based Recurrent Neural Network. In <em><em>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</em></em> (<em>IJCAI ’17</em>). 2985–2991.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186931">https://doi.org/10.1145/3184558.3186931</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
