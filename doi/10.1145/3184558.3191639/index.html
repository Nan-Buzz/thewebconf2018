<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Deriving Validity Time in Knowledge Graph</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Deriving Validity Time in Knowledge Graph</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author"><a href="https://orcid.org/0000-0002-8662-0053" ref="author"><span class="givenName">Julien</span>      <span class="surName">Leblay</span></a>     Artificial Intelligence Research Center AIST Tokyo Waterfront, Tokyo, Japan, <a href="mailto:firstname.lastname@aist.go.jp">firstname.lastname@aist.go.jp</a><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>,</div>     <div class="author">     <span class="givenName">Melisachew Wudage</span>      <span class="surName">Chekol</span>     Data and Web Science Group University of Mannheim, Mannheim, Germany, <a href="mailto:mel@informatik.uni-mannheim.de">mel@informatik.uni-mannheim.de</a>     </div>            </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191639" target="_blank">https://doi.org/10.1145/3184558.3191639</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Knowledge Graphs (KGs) are a popular means to represent knowledge on the Web, typically in the form of node/edge labelled directed graphs. We consider temporal KGs, in which edges are further annotated with time intervals, reflecting when the relationship between entities held in time. In this paper, we focus on the task of predicting time validity for unannotated edges. We introduce the problem as a variation of relational embedding. We adapt existing approaches, and explore the importance example selection and the incorporation of side information in the learning process. We present our experimental evaluation in details.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <em>Temporal reasoning;</em> <em>Supervised learning;</em></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Temporal Knowledge Graph</small>, </span>     <span class="keyword">      <small> Factorization Machines</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Julien Leblay and Melisachew Wudage Chekol. 2018. Deriving Validity Time in Knowledge Graph. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 6 Pages. <a href="https://doi.org/10.1145/3184558.3191639" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191639</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Knowledge Graphs (KGs) encompass a class knowledge representation models, in which nodes correspond to entities, and directed labelled edges the relationships between them. Some well-known examples of KGs include Google&#x0027;s Knowledge Vault&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], NELL&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], YAGO&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>], and DBpedia&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. Whether the data is generated and maintained by users or computer programs, mistakes and omissions can easily proliferate, and the data can quickly become outdated. To make matters worse, some of the most popular formats used for data publishing, including RDF, JSON or CSV, do not provide built-in mechanisms to easily capture and retain information as the data changes over time. As an example, consider the following facts extracted from the DBpedia (<a class="link-inline force-break" href="http://dbpedia.org/page/Grover_Cleveland">http://dbpedia.org/page/Grover_Cleveland</a>) dataset about Grover Cleveland, the 22<sup>     <em>th</em>     </sup> and 24<sup>     <em>th</em>     </sup> president of the USA.</p>    <div class="table-responsive" id="inltbl1">     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <tt>(GCleveland, birthPlace, Caldwell)</tt>,</td>      </tr>      <tr>       <td style="text-align:left;">        <tt>(GCleveland, office, POTUS)</tt>,</td>      </tr>      <tr>       <td style="text-align:left;">        <tt>(GCleveland, office, NewYork_Governor)</tt>       </td>      </tr>     </tbody>     </table>    </div>    <p>The lack of temporal information is problematic in this example for several reasons. None of these facts is independently false, yet Grover Cleveland could not have been president and governor at the same time. Moreover, this information is missing since Grover Cleveland has been president twice, during two non consecutive periods. So, clearly temporal metadata would lift some ambiguity, yet not all facts typically need such metadata. For instance, his birth place is not expected to change over time.</p>    <p>Many KGs do not contain the validity period of facts, i.e.,&#x00A0;the period during which the fact is considered to hold. Notable exceptions include Wikidata&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] and YAGO, in which some facts that are endowed with temporal information. Our goal is to learn temporal meta-data on a knowledge graph where such information is incomplete. For the above example, we want to derive annotations of the following form:</p>    <div class="table-responsive" id="inltbl2">     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <tt>(GCleveland, office, POTUS):[1885-1889;1893-1897]</tt>       </td>      </tr>      <tr>       <td style="text-align:left;">        <tt>(GCleveland, office, NewYork_Governor):[1883-1885]</tt>       </td>      </tr>     </tbody>     </table>    </div>    <p>Note that Grover Cleveland was president during two distinct, non consecutive terms.</p>    <p>In the following section, we provide some formal background and review the related work. In Section&#x00A0;<a class="sec" href="#sec-11">3</a>, we first attempt to carry over techniques from relational embedding models, and study the limitations of these approaches. Then, we proceed to show that factorization machines are particularly well-suited for our temporal scope prediction task, allowing to take valuable side-information into account. In Section&#x00A0;<a class="sec" href="#sec-16">4</a>, we report early experimental results.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>     </div>    </header>    <p>In the following, we introduce temporal knowledge graphs formally, as well as the problem addressed in this paper. We present possible extensions of relational embedding approaches and factorization machines.</p>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Temporal Knowledge Graphs</h3>     </div>     </header>     <p>We are considering KGs of the form <span class="inline-equation"><span class="tex">$G=(\mathcal {E}, \mathcal {R})$</span>     </span>, where <span class="inline-equation"><span class="tex">$\mathcal {E}$</span>     </span> is a set of labeled nodes known as entities, and <em>R</em> is a set of labeled edges known as relations. Alternatively, we can refer to <em>G</em>, as a set of triples of the form (<em>subject</em>, ~<em>predicate</em>, ~<em>object</em>), where <em>subject</em> and <em>object</em> are node-labels, and <em>predicate</em> is an edge label. Labels act as unique identifiers for subjects and predicates, and either as identifier or literal value for objects. Hence, the presence of an edge <em>p</em> between two nodes <em>s</em> and <em>o</em> indicates that the fact (<em>s</em>, <em>p</em>, <em>o</em>) holds. In practice, knowledge is not static in time, thus we would like to capture when a given fact held over time. Thus, we assume a set of discrete time points <em>T</em>, and an additional labeling scheme on edges, which takes a set of time intervals over <em>T</em>, denoting the periods within which a fact was considered true. This yields a <em>temporal KG</em>.</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Problem statement</h3>     </div>     </header>     <p>Our goal is to learn associations between facts of a KG and one or more time points in <em>T</em>. This gives us the ability to tackle the following tasks:</p>     <ul class="list-no-style">     <li id="uid4" label="Time prediction:">given a query of the form (<em>s</em>, <em>p</em>, <em>o</em>, ?), predict the time point(s) for which the fact is consider valid/true.<br/></li>     <li id="uid5" label="Time-dependent query answering:">given a point in time and a fact with missing subject, predicate or object, predict the most likely label.<br/></li>     </ul>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Related Work</h3>     </div>     </header>     <p>We present the related work from three different angles: (i) temporal scoping of knowledge graph facts, (ii) relational embedding for link prediction, and (iii) factorization machines for triple classification.</p>     <section id="sec-8">     <p><em>2.3.1 Temporal scoping of KG facts.</em> The study of deriving the temporal scopes of KG facts has recently gained momentum. The most recent of which is Know-Evolve&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0019">19</a>]. A temporal KG in Know-Evolve is a set of facts where each fact has a timestamped relation. For embedding entities and timestamped relations, they use a bilinear model (RESCAL) and employ a deep recurrent neural network in order to learn non-linearly evolving entities. The learning phase espouses a point-process, by which the estimation of whether a fact hold at time <em>t</em> is based on the state at time <em>t</em> &#x2212; 1. That said, they do not exploit side information as we do in this work. Another closely related work is the time-aware KG embedding model of Jiang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0007">7</a>]. They focus on the prediction of an entity or relation given a time point in which the fact is supposed to be valid. Both Know-Evolve and time-aware KG completion methods use relational embedding models which are discussed below. Furthermore, in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0018">18</a>], the authors use tensor decomposition to assign validity scopes for KG facts. However, as reported in the paper, their models do not perform sufficiently well. Nonetheless, this can be improved by including side information as we did here.</p>     <p>In contrast, Rula et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>] extract time information contained in Web pages using syntactic rules. This process has three phases whereby candidate intervals for facts are matched, selected and then merged according to temporal consistency rules. YAGO&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>] is another earlier example, in which both time and space scopes were extracted using linguistic extraction rules, followed but conflict resolving post-processing.</p>     <p>In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0021">21</a>], the authors formulate the temporal scoping problem as a state change detection problem. In doing so, they enrich temporal profiles of entities with relevant contextual information (these are unigrams and bigrams surrounding mentions of an entity, for instance, for the entity Barack Obama relevant unigrams include &#x2018;elect&#x2019;, &#x2018;senator&#x2019; and so on). From there, they learn vectors that reflect change patterns in the contexts. For example, after &#x2018;becoming president&#x2019;, US presidents often see a drop in mentions of their previous job title state such as &#x2018;senator&#x2019; or &#x2018;governor&#x2019; in favor of &#x2018;president&#x2019;.</p>     <p>Another temporal scoping system developed by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>] relies on a language model consisting of patterns automatically derived from Wikipedia sentences that contain the main entity of a page and temporal slot-fillers extracted from the corresponding infoboxes.</p>     <p>Talukdar et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>] use frequency counts of fact mentions to define temporal profiles (basically a time-series of the occurrences of facts over time in a corpus of historical documents) of facts and analyze how the mentions of those facts rise and fall over time. They identify temporal scope over input facts, using a 3-phase procedure.</p>     <p>Yet, the approach is rather brittle in that it does not automatically adapt to new relations, and requires human experts at several steps in the process.</p>     <p>Bader&#x00A0;et&#x00A0;al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0002">2</a>] used matrix decomposition on the Enron email dataset, to estimate relationship among the scandal&#x0027;s stakeholders over time. Unlike in our settings, the relationships were not labeled.</p>     </section>     <section id="sec-9">     <p><em>2.3.2 Relational Embedding approaches.</em> Our problem is more generally related to relational embedding models, a paradigm of relational learning in low dimensional vector space, which has been widely used for tasks such as link prediction and fact classification. Such embeddings can be viewed as a special case of graph embedding, a very active research topic, which we omitted here for conciseness. We can broadly divide the models into three categories based on: (i) translational distance, (ii) tensor factorization (bilinear models), and more recently, (iii) neural networks.</p>     <p>Vectors are used to learn entity and relation embeddings in translational models, whereas additional matrices are used in the case of bilinear models and neural networks. While the translational models use a distance metric to measure the plausability of facts, bilinear models rely on the dot product of entity and relational embeddings. One of the most well known translational models is TransE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0003">3</a>]. Its simplicity allows for straightforward extensions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>]. The translation embedding of a triple (<em>s</em>, <em>p</em>, <em>o</em>) corresponds to <strong>s</strong> + <strong>p</strong> &#x2248; <strong>o</strong>. A scoring function <em>score</em>(<strong>s</strong>, <strong>p</strong>, <strong>o</strong>), either the &#x2113;<sub>1</sub> or &#x2113;<sub>2</sub> norm, is used to measure the distance (i.e., similarity) as: <div class="table-responsive" id="Xeq1">       <div class="display-equation">        <span class="tex mytex">\begin{equation} score(\mathbf {s}, \mathbf {p}, \mathbf {o}) = -||\mathbf {s}+\mathbf {p} - \mathbf {o}||_{\ell _{1/2}} \end{equation} </span>        <br/>        <span class="equation-number">(1)</span>       </div>      </div> The training set contains positive examples (<em>G</em>), and negative examples (<em>G</em>&#x2032;) generated as follows: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} G^{\prime }_{(s,p,o) \in G} =&#x0026;~ \lbrace (s^{\prime },p,o) \mid s^{\prime } \in \mathcal {E}, (s^{\prime }, p, o) \not\in G\rbrace \ \cup \\ &#x0026;~~\lbrace (s,p,o^{\prime }) \mid o^{\prime }\in \mathcal {E}, (s,p,o^{\prime }) \not\in G\rbrace .\end{align*} </span>        <br/>       </div>      </div> Hence, <em>G</em>&#x2032; contains triples with either <em>s</em> or <em>o</em> replaced by a random entity from the set <span class="inline-equation"><span class="tex">$\mathcal {E}$</span>      </span>.</p>     <p>RESCAL&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0011">11</a>], also referred to as <em>bilinear model</em>, uses a tensor factorization model by representing triples in a tensor. That is, for each triple <em>x<sub>ijk</sub>      </em> = (<em>s<sub>i</sub>      </em>, <em>p<sub>k</sub>      </em>, <em>o<sub>j</sub>      </em>), <em>y<sub>ijk</sub>      </em> = {0, 1} denotes its existence or nonexistence in a tensor <span class="inline-equation"><span class="tex">$\mathbf {Y} \in \lbrace 0,1\rbrace ^{|\mathcal {E}| \times |\mathcal {E}| \times |\mathcal {R}|}$</span>      </span>. RESCAL learns vector embeddings of entities and a matrix <span class="inline-equation"><span class="tex">$\mathbf {W}_{p} \in \mathbb {R}^{d \times d}$</span>      </span> for each relation <span class="inline-equation"><span class="tex">$r \in \mathcal {R}$</span>      </span> where each slice <strong>Y</strong> is factorized as: <strong>Y</strong> &#x2248; <strong>s</strong>      <sup>&#x22A4;</sup>      <strong>W</strong>      <sub>       <em>p</em>      </sub>      <strong>o</strong>. Hence, the scoring function for the bilinear model is: <div class="table-responsive" id="Xeq2">       <div class="display-equation">        <span class="tex mytex">\begin{equation} score(s,p,o) = \mathbf {s}^\top \mathbf {W}_p\mathbf {o}. \end{equation} </span>        <br/>        <span class="equation-number">(2)</span>       </div>      </div>     </p>     <p>Other notable relational embedding models are HolE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0010">10</a>] and Neural Tensor Networks (NTN)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>]. HolE improves the efficiency of RESCAL by using a circular correlation operation (it compresses the interaction between two entities) for scoring triples.</p>     <p>Almost all relational embedding approaches minimize a margin-based ranking loss function <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>      </span> over some training dataset. <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>      </span> is given by the following equation: <div class="table-responsive" id="Xeq3">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathcal {L} = \sum _{(s,p,o) \in G} \sum _{(s, p, o)^{\prime } \in G^{\prime }_{(s,p,o)}} [\gamma + score((\mathbf {s}, \mathbf {p}, \mathbf {o})) - score((\mathbf {s}, \mathbf {p}, \mathbf {o})^{\prime })]_+, \end{equation} </span>        <br/>        <span class="equation-number">(3)</span>       </div>      </div> where [<em>x</em>]<sub>+</sub> denotes the positive part of <em>x</em>, <em>&#x03B3;</em> > 0 is a margin hyperparameter. Different optimization functions such as stochastic gradient descent are used to minimize <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>      </span>.</p>     </section>     <section id="sec-10">     <p><em>2.3.3 Factorization Machines.</em> Unlike vector space embedding models, Factorization Machines (FMs) allow us to incorporate contextual information which improves prediction performance. Rendle&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>] introduced FMs to model the interaction between features using factorized parameters. One big advantage of FMs is that they allow to estimate all interactions between features even with very sparse data. In addition, FMs can mimic many different matrix factorization models such as biased matrix factorization, Singular Value Decomposition (SVD++)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0008">8</a>], and Pairwise Interaction Tensor Factorization (PITF)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>]. FMs provide flexibility in feature engineering as well as high prediction accuracy. Moreover, FMs can be applied to the following tasks: regression, binary classification, and ranking. The model of a factorization machine is given by the following equation: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} &#x0026;~ score(\mathbf {x}) := w_0 + \sum _{i=1}^n w_ix_i + \sum _{i=1}^n\sum _{j=i+1}^n\langle \mathbf {v}_i, \mathbf {v}_j\rangle x_i x_j, \\ &#x0026;~\langle \mathbf {v}_i, \mathbf {v}_j \rangle := \sum _{f=1}^k v_{i,f}\cdot v_{j,f},\end{align*} </span>        <br/>       </div>      </div> where <span class="inline-equation"><span class="tex">$score:\mathbb {R}^n \rightarrow T$</span>      </span> is a prediction function from a real valued feature vector <span class="inline-equation"><span class="tex">$\mathbf {x} \in \mathbb {R}^n$</span>      </span> to a target domain, <span class="inline-equation"><span class="tex">$T=\mathbb {R}$</span>      </span> for regression, <em>T</em> = { +, &#x2212;} for classification and so on. The model parameters: <em>w</em>      <sub>0</sub> denotes the global bias; <em>w<sub>i</sub>      </em> within <span class="inline-equation"><span class="tex">$\mathbf {w} \in \mathbb {R}^n$</span>      </span> indicates the strength of the i-th variable with <em>n</em> being the size of the feature vector; &#x27E8;<strong>v</strong>      <sub>       <em>i</em>      </sub>, <strong>v</strong>      <sub>       <em>j</em>      </sub>&#x27E9; models the interaction between the i-th and j-th variables. &#x27E8;., .&#x27E9; is the dot product of two vectors of size <em>k</em>.</p>     <p>Furthermore, the model parameter <strong>v</strong>      <sub>       <em>i</em>      </sub> in <span class="inline-equation"><span class="tex">$V \in \mathbb {R}^{n \times k}$</span>      </span> describes the i-th variable with <em>k</em> factors. <em>k</em> is a hyperparameter that defines the dimension of the factorization.</p>     <p>In this work, since we need to predict the validity of facts of (possible many) time points, we use factorization machine for classification rather than regression or ranking.</p>     </section>    </section>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Temporal Scope Prediction</h2>     </div>    </header>    <p>In the following we consider relational embedding models and factorization machines for temporal scope prediction.</p>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Relational Embedding Models for Temporal KGs</h3>     </div>     </header>     <p>We propose various approaches for representing temporal knowledge graphs in vector space. In particular, we investigate several extensions of existing relational embedding approaches.</p>     <section id="sec-13">     <p><em>3.1.1 TTransE.</em> Short for Temporal TransE, this is an extension of the well known embedding model TransE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0003">3</a>], by substituting its scoring function.</p>     <ol class="list-no-style">      <li id="list1" label="(1)">Naive-TTransE: time is encoded by way of synthetic relations. For each relation <em>r</em> in the vocabulary and each time point <em>t</em> &#x2208; <em>T</em>, we assume a synthetic relation <em>r</em>: <em>t</em>. For instance, the temporal fact <tt>(GCleveland, office, POTUS):1888</tt>, is encoding as <tt>(GCleveland, office:1888, POTUS)</tt>. The scoring function is unchanged (as in equation (1)): <div class="table-responsive" id="eq1">        <div class="display-equation">        <span class="tex mytex">\begin{equation} score(\mathbf {s}, \mathbf {p:t}, \mathbf {o}) = -||\mathbf {s}+\mathbf {p:t} - \mathbf {o}||_{\ell 1/2} \end{equation} </span>        <br/>        <span class="equation-number">(4)</span>        </div>       </div> While this model is simple, it is not scalable. Besides the link prediction does not distinguish between two consecutive timepoints, for instance, for the task (GCleveland, ?, POTUS), office:1988 and office:1989 are equally possible links.<br/></li>      <li id="list2" label="(2)">Vector-based TTransE: in this approach, time is represented in the same vector space as entities and relations. The scoring function becomes: <div class="table-responsive" id="eq2">        <div class="display-equation">        <span class="tex mytex">\begin{equation} score(\mathbf {s}, \mathbf {p}, \mathbf {o}, \mathbf {t}) = -|| \mathbf {s} + \mathbf {p} + \mathbf {t} - \mathbf {o} ||_{\ell 1/2} \end{equation} </span>        <br/>        <span class="equation-number">(5)</span>        </div>       </div> In this approach, time points have embedding representations, just like entities and relations. The rationale behind this scoring function is to drive a (subject, predicate)-pair close to the correct object, relative to any valid point in time.<br/></li>      <li id="list3" label="(3)">Coefficient-based TTransE: time points (or rather a normalization thereof) are used as a coefficient affecting the subject and relation embeddings of a triple. <div class="table-responsive" id="eq3">        <div class="display-equation">        <span class="tex mytex">\begin{equation} score(s, p, o, t) = -|| t * (\mathbf {s} + \mathbf {p}) - \mathbf {o} ||_{\ell 1/2} \end{equation} </span>        <br/>        <span class="equation-number">(6)</span>        </div>       </div> As a variant of this, only the relation is affected by time: <div class="table-responsive" id="eq4">        <div class="display-equation">        <span class="tex mytex">\begin{equation} score(s, p, o, t) = -|| \mathbf {s} + t * \mathbf {p} - \mathbf {o} ||_{\ell 1/2} \end{equation} </span>        <br/>        <span class="equation-number">(7)</span>        </div>       </div> Unlike Vector-based TTransE, time points are represented as real values in (0, 1], and thus are not directly affected by the optimization.<br/></li>     </ol>     </section>     <section id="sec-14">     <p><em>3.1.2 TRESCAL.</em> TRESCAL is a temporal extension of RESCAL. We extend its bilinear temporal scoring function as follows. As in Naive-TTransE, time is encoded by means of synthetic relations just like Naive-TTransE. <div class="table-responsive" id="eq5">       <div class="display-equation">        <span class="tex mytex">\begin{equation} score(s, p, o, t) = \mathbf {s}^\top \mathbf {W}_\mathbf {{p:t}}\mathbf {o} \end{equation} </span>        <br/>        <span class="equation-number">(8)</span>       </div>      </div> This model is straight forward extension of the bilinear model. Despite its simplicity, it does not scale well, besides, the prediction results are quite poor.</p>     <div class="table-responsive" id="tab1">      <div class="table-caption">       <span class="table-number">Table 1:</span>       <span class="table-title">Mean Rank (MR), Hits@{1,10} and cost reduction for our temporal embeddings methods on the FreeBase dataset.</span>      </div>      <table class="table">       <tbody>        <tr>        <td style="text-align:left;">         <strong>Approach</strong>        </td>        <td style="text-align:center;">         <strong>LR</strong>        </td>        <td style="text-align:center;">         <strong>M</strong>        </td>        <td style="text-align:center;">         <strong>D</strong>        </td>        <td style="text-align:center;">         <strong>E</strong>        </td>        <td style="text-align:center;">         <strong>MR (p)</strong>        </td>        <td style="text-align:center;">         <strong>Hits@1 (p)</strong>        </td>        <td style="text-align:center;">         <strong>MR (o)</strong>        </td>        <td style="text-align:center;">         <strong>Hits@10 (o)</strong>        </td>        <td style="text-align:center;">         <strong>MR (t)</strong>        </td>        <td style="text-align:center;">         <strong>Hits@10 (t)</strong>        </td>        <td style="text-align:center;">         <strong>Cost Red.</strong>        </td>        </tr>        <tr>        <td style="text-align:left;">Eq.&#x00A0;<a class="eqn" href="#eq1">4</a>        </td>        <td style="text-align:center;">0.1</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">537.51</td>        <td style="text-align:center;">0.6</td>        <td style="text-align:center;">2578.4</td>        <td style="text-align:center;">11.0</td>        <td style="text-align:center;">59.2</td>        <td style="text-align:center;">10.3</td>        <td style="text-align:center;">99.75%</td>        </tr>        <tr>        <td style="text-align:left;">Eq.&#x00A0;<a class="eqn" href="#eq2">5</a>        </td>        <td style="text-align:center;">0.01</td>        <td style="text-align:center;">1</td>        <td style="text-align:center;">200</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">141.67</td>        <td style="text-align:center;">22.69</td>        <td style="text-align:center;">1295.54</td>        <td style="text-align:center;">13.59</td>        <td style="text-align:center;">58.44</td>        <td style="text-align:center;">7.76</td>        <td style="text-align:center;">45.32%</td>        </tr>        <tr>        <td style="text-align:left;">Eq.&#x00A0;<a class="eqn" href="#eq3">6</a>        </td>        <td style="text-align:center;">0.1</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">835.22</td>        <td style="text-align:center;">0.55</td>        <td style="text-align:center;">9884.69</td>        <td style="text-align:center;">0.91</td>        <td style="text-align:center;">58.50</td>        <td style="text-align:center;">8.62</td>        <td style="text-align:center;">0.13%</td>        </tr>        <tr>        <td style="text-align:left;">Eq.&#x00A0;<a class="eqn" href="#eq4">7</a>        </td>        <td style="text-align:center;">0.01</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">50</td>        <td style="text-align:center;">500</td>        <td style="text-align:center;">796.65</td>        <td style="text-align:center;">0.18</td>        <td style="text-align:center;">9374.92</td>        <td style="text-align:center;">0.19</td>        <td style="text-align:center;">58.50</td>        <td style="text-align:center;">8.62</td>        <td style="text-align:center;">0.45%</td>        </tr>        <tr>        <td style="text-align:left;">Eq.&#x00A0;<a class="eqn" href="#eq5">8</a>        </td>        <td style="text-align:center;">0.01</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">100</td>        <td style="text-align:center;">1000</td>        <td style="text-align:center;">483.32</td>        <td style="text-align:center;">3.1</td>        <td style="text-align:center;">6588.6</td>        <td style="text-align:center;">1.9</td>        <td style="text-align:center;">58.5</td>        <td style="text-align:center;">12.1</td>        <td style="text-align:center;">99.99%</td>        </tr>       </tbody>      </table>     </div>     </section>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Factorization Machines for Temporal KGs</h3>     </div>     </header>     <p>Among the approaches described so far, the naive ones do not scale well with time domains of increasing size or resolution. Although the vector-based TTransE approach performs overall better than the other techniques, it did not show good enough performance to solve our problem in practice. In the following, we show how we used factorization machines to solve both or scability and performance issues.</p>     <p>     <em>Data/Feature Representation</em>. We consider a knowledge graph <em>G</em> = <em>G<sub>t</sub>     </em>&#x222A;<em>G<sub>c</sub>     </em> where <em>G<sub>t</sub>     </em> is a set of quadruples or timestamped triples, and <em>G<sub>c</sub>     </em> is a set of atemporal triples that we refer to as a <em>context graph</em>. For instance, the following is a temporal graph <em>G<sub>t</sub>     </em>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \begin{array}{l}({\texttt {GCleveland, office, POTUS):1888}}, \\ ({\texttt {GCleveland, office, POTUS):1895}}, \end{array} \] </span>       <br/>      </div>     </div> and its context graph <em>G<sub>c</sub>     </em> is given below: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \begin{array}{l}({\texttt {GCleveland, birthPlace, Caldwell}}). \end{array} \] </span>       <br/>      </div>     </div> An input to an FM is a feature vector representation of the pair (<em>G<sub>t</sub>     </em>, <em>G<sub>c</sub>     </em>). The feature vector encoding can be constructed in several ways such as one-hot encoding, bag-of-words (representing KG entities and relations in a bag or multiset) and so on&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. The features associated with a fact of the form (<em>s</em>, <em>p</em>, <em>o</em>) are {<em>bow</em>(<em>s</em>), <em>p</em>, <em>bow</em>(<em>o</em>)}, where <em>bow</em>(<em>x</em>) returns the bag of words of all the literals in relations with subject <em>x</em>.</p>     <p>     <em>Example Generation</em>. To generate positive examples, we used temporal sampling, guiding by input parameter TS, which consists in sampling uniformly <em>s<sub>t</sub>     </em> time points within the fact&#x0027;s validity intervals. A second parameter, NS, guides negative sampling, producing <em>s<sub>n</sub>     </em> for each positive time-point-based fact/example, using the same random corruption techniques as in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>].</p>    </section>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>We implemented our approach based on the scikit-kge library of RESCAL and TransE<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>, and libFM/pywFM<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>.</p>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>     </header>     <p>We originally experimented with theFreebase database often used in the related work (our first set of experiments). However, the facts on those dataset not having temporal information, we randomly generate such metadata for a subset of them, by picking two random years and using them as start and end validity dates. For this reason, it is hard to compare our results with corresponding work in the non-temporal relational embedding scenarios. Freebase has approximately 14K entities, et 1000 relations, with 60k examples. We later decided to switch to Wikidata, a knowledge base with reasonably high quality time information. More over Wikidata is much larger and up-to-date. We only briefly present the result obtained in the former dataset, and results were largely negative. Besides, using the Freebase and WordNet data set with the factorization machines approach was not possible because of the lack of side-information to exploit; the data sets contain very little plain text.</p>     <p>Our process in preparing the Wikidata data set was the following. We extracted triples from a recent dump, and partitioned them into two sets: (i) temporal facts: facts having some temporal annotations, such as point-in-time, start time, end time or any sub-property thereof, (ii) atemporal facts: atemporal facts, having no such annotations.</p>     <p>Temporal properties annotating temporal facts include &#x201D;start time&#x201D;, &#x201D;inception&#x201D;, &#x201D;demolition time&#x201D;, etc. In this work, we only consider years, and thus normalize all years to the Gregorian calendar and discard information of finer granularity. Facts annotated with a single point-in-time are associated with that time-point as start and end time.</p>     <p>During the learning phase, temporal facts are used to generate positive and negative examples and atemporal facts are used to collect side information. The complete data has 4.2M temporal facts. Out of approximately 3600 distinct properties, 2770 are strictly atemporal, i.e.,&#x00A0;none of their corresponding triples are temporal annotation. Out of the remaining properties, 17 are strictly temporal, i.e.,&#x00A0;all their corresponding triples have temporal annotations, while for the remaining 813 properties, only some triples are annotated. We partition the triples into two sets, respectively with and without temporal annotations, the former being our original example set. From this example set (temporal facts), we exclude the strictly temporal ones (since they are not candidate for prediction), the fact featuring the most frequent single frequent property &#x2014; covering nearly 1.2M examples &#x2014;, and those with properties covering less than 10 examples (approximately 397 properties). Ultimately, our example set contains 2.5M examples, much more than most datasets used in related approaches (see for example&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>]). We also report our results on a reduced version of this data set, containing 180K temporally annotated facts (i.e.,&#x00A0;approx. 5% of the overall data). Our dataset can be found online for reproducibility<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>.</p>     <p>The second set of triples (atemporal facts) is used for generating features. We also remove the set of triples with low semantic content such as those mapping a Wikidata entity ID to that of other datasets.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Temporal relational embeddings</h3>     </div>     </header>     <p>For this experiment, we use the modified Freebase dataset, and evaluated the approaches with a slightly modified version of that from the related work, which evaluate using query triples, i.e.,&#x00A0;facts in which one item is omitted and need to be predicted by the models. For query answering, <em>s</em> or <em>o</em> is omitted, while <em>p</em> is omitted in link prediction. The evaluation metrics are the Mean Rank of the correct answers among all answers order by their predicating probability. The lower, the better. Metrics also include the &#x201C;Hits@K&#x201D;, i.e.,&#x00A0;the percentage of case in which the correct answer is in the top K results. Hits@10 is a popular metric, yet for small domain (such as in link prediction), Hits@1 is usually preferred. In our setting, we deal with quadruple, therefore we extend the process to time prediction in which, time is omitted, and will evaluate how often a predicted validity time point is <em>within</em> the actual validity interval of the fact.</p>     <p>In Table&#x00A0;<a class="tbl" href="#tab1">1</a>, we only report the best results obtained with each approaches. We ran the approaches with learning rates (LR) among {.01, .1}, margins (M) among {2, 10}, dimensionalities of the vector space (D) among {20, 50, 100, 200}, and learning over 500 or 1000 epoqs (E). It is clear from the table that the performs are not satisfying. However, we can distinguish two general problems. For the naive methods (Eq.&#x00A0;<a class="eqn" href="#eq1">4</a>-<a class="eqn" href="#eq5">8</a>), the space explodes from the multiplication of &#x201C;virtual relations&#x201D; entailed by the methods. This is why performance are poor despite significant cost reductions achieved through the learning process. The other methods however do not achieve much cost reduction all together. Our best explanation for this is that learning time validity simple from the <em>structure</em> of the graph (i.e.,&#x00A0;using no other external information) is simply to hard. This conclusion led us to turn to the Factorization Machine approach, more akin to the incorporation of side information.</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Classification task on FM</h3>     </div>     </header>     <p>For the classification task, the learning is done on quadruples of the form (<em>s</em>, <em>p</em>, <em>o</em>, <em>t</em>) = &#x00B1;1, modeling whether the triple (<em>s</em>, <em>p</em>, <em>o</em>) held at time <em>t</em> or not. After the sampling, the effective number of examples increase. For instance with TS = 3, <tt>(GCleveland, office, POTUS):[1885, 1889]</tt>, will generate positive examples for the time points 1885, 1887, 1889. The evaluation, in turn, is performed on <em>time points</em> rather than time intervals. We use the standard definition of prediction, recall, F-measure and accuracy. The definitions of these measures are given below: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} precision =&#x0026;~ \frac{\#true~positives}{\# positive~predictions} \\recall =&#x0026;~ \frac{\#true~positives}{\# ground~truth~positives} \\F{-}measure =&#x0026;~ 2\times \frac{precision\times recall}{precision + recall} \\accuracy =&#x0026;~ \frac{\# correct~predictions}{\# all~predictions}\end{align*} </span>       <br/>      </div>     </div>     </p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Precision, recall, F1-measure and accuracy on the WD_180K dataset with varying temporal sampling at 100 iterations (OM: Optimization Method, TS: temporal sample size).</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;"/>        <td style="text-align:right;">OM</td>        <td style="text-align:right;">TS</td>        <td style="text-align:center;">        <strong>Precision</strong>        </td>        <td style="text-align:center;">        <strong>Recall</strong>        </td>        <td style="text-align:center;">        <strong>F1</strong>        </td>        <td style="text-align:center;">        <strong>Accuracy</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_180K</strong>        </td>        <td style="text-align:right;">ALS</td>        <td style="text-align:right;">1</td>        <td style="text-align:center;">58.44%</td>        <td style="text-align:center;">71.27%</td>        <td style="text-align:center;">64.22%</td>        <td style="text-align:center;">60.23%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_180K</strong>        </td>        <td style="text-align:right;">ALS</td>        <td style="text-align:right;">10</td>        <td style="text-align:center;">67.94%</td>        <td style="text-align:center;">88.95%</td>        <td style="text-align:center;">77.04%</td>        <td style="text-align:center;">73.48%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_180K</strong>        </td>        <td style="text-align:right;">ALS</td>        <td style="text-align:right;">100</td>        <td style="text-align:center;">74.56%</td>        <td style="text-align:center;">92.47%</td>        <td style="text-align:center;">82.56%</td>        <td style="text-align:center;">80.45%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_2.5M</strong>        </td>        <td style="text-align:right;">ALS</td>        <td style="text-align:right;">10</td>        <td style="text-align:center;">78.15%</td>        <td style="text-align:center;">97.64%</td>        <td style="text-align:center;">86.81%</td>        <td style="text-align:center;">85.16%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_180K</strong>        </td>        <td style="text-align:right;">MCMC</td>        <td style="text-align:right;">1</td>        <td style="text-align:center;">64.98%</td>        <td style="text-align:center;">81.07%</td>        <td style="text-align:center;">72.14%</td>        <td style="text-align:center;">68.64%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_180K</strong>        </td>        <td style="text-align:right;">MCMC</td>        <td style="text-align:right;">10</td>        <td style="text-align:center;">69.55%</td>        <td style="text-align:center;">89.69%</td>        <td style="text-align:center;">78.35%</td>        <td style="text-align:center;">75.21%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_180K</strong>        </td>        <td style="text-align:right;">MCMC</td>        <td style="text-align:right;">100</td>        <td style="text-align:center;">79.28%</td>        <td style="text-align:center;">92.28%</td>        <td style="text-align:center;">85.28%</td>        <td style="text-align:center;">84.07%</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>WD_2.5M</strong>        </td>        <td style="text-align:right;">MCMC</td>        <td style="text-align:right;">10</td>        <td style="text-align:center;">85.41%</td>        <td style="text-align:center;">97.64%</td>        <td style="text-align:center;">91.12%</td>        <td style="text-align:center;">90.48%</td>       </tr>      </tbody>      <tfoot>       <tr>        <td>max width=</td>        <td/>        <td/>        <td/>        <td/>        <td/>        <td/>       </tr>      </tfoot>     </table>     </div>     <p>We used the optimization functions Alternating Least Square (ALS), and Markov Chain Monte Carlo (MCMC). We report the precision, recall, F-measure and accuracy in Table&#x00A0;<a class="tbl" href="#tab2">2</a>, which shows the results for experiments run on a Wikidata data set of 180K and 2.5M examples, using bag-of-words as side information, with increasing temporal sampling size. The results for high NS are omitted since the greater number of negative examples tends to biases the model towards negative predictions, resulting in high accuracy, despite poor precision. With a balanced set of positive and negative examples, precision is positively correlated with TS. Using a temporal sampling of 100, with our smaller dataset, precision and recall peak at 74.5% and 92% respectively after 100 iterations, with an F1-measure and accuracy around 82%. Using a temporal sampling size of 10, with our bigger dataset, the F1-measure and accuracy reach 90%. Increasing the sample size, also improves performance, yet producing positive examples for all time points within a time interval degrades the performance, probably due to over-fitting. Our result also shows that a precision of around 70% can be achieved with only 10 iterations.</p>     <p>Our most demanding experiment took sightly over 6 hours to complete on a regular laptop, with 16GB of RAM, and a 2.8 GHz Intel Core i5 processor.</p>     <p>We have excluded experimental results for TTransE and TRESCAL as our result showed the methods were not competitive.</p>    </section>   </section>   <section id="sec-20">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusion</h2>     </div>    </header>    <p>In this work, we studied the problem of temporal scope prediction. We adapted several existing relational embedding approaches in which our experimental results have shown that they suffer from either scalability or accuracy. Factorization machines overcome these shortcomings as they provide a way to incorporate side information which improves prediction performance. We designed a new dataset by carefully analyzing Wikidata and carried out several experiments. We believed our experimental results are quite promising. Next, we plan to turn our attention to neural network-based approaches, extend our current framework to support time-aware link prediction and query answering, and applies our finding to other types are context prediction, such as space or provenance. We all plan to apply the approach in an open information extraction setting.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">S&#x00F6;ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In <em>      <em>The semantic web</em>     </em>. Springer, 722&#x2013;735.</li>     <li id="BibPLXBIB0002" label="[2]">Brett&#x00A0;W Bader, Richard&#x00A0;A Harshman, and Tamara&#x00A0;G Kolda. 2007. Temporal analysis of semantic graphs using ASALSAN. In <em>      <em>Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on</em>     </em>. IEEE, 33&#x2013;42.</li>     <li id="BibPLXBIB0003" label="[3]">Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In <em>      <em>Advances in neural information processing systems</em>     </em>. 2787&#x2013;2795.</li>     <li id="BibPLXBIB0004" label="[4]">Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam&#x00A0;R Hruschka&#x00A0;Jr, and Tom&#x00A0;M Mitchell. 2010. Toward an Architecture for Never-Ending Language Learning.. In <em>      <em>AAAI</em>     </em>, Vol.&#x00A0;5. 3.</li>     <li id="BibPLXBIB0005" label="[5]">Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion. In <em>      <em>SIGKDD</em>     </em>. 601&#x2013;610.</li>     <li id="BibPLXBIB0006" label="[6]">Johannes Hoffart, Fabian&#x00A0;M Suchanek, Klaus Berberich, Edwin Lewis-Kelham, Gerard De&#x00A0;Melo, and Gerhard Weikum. 2011. YAGO2: exploring and querying world knowledge in time, space, context, and many languages. In <em>      <em>Proceedings of the 20th international conference companion on World wide web</em>     </em>. ACM, 229&#x2013;232.</li>     <li id="BibPLXBIB0007" label="[7]">Tingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Baobao Chang, Sujian Li, and Zhifang Sui. 2016. Towards Time-Aware Knowledge Graph Completion.. In <em>      <em>COLING</em>     </em>. 1715&#x2013;1724.</li>     <li id="BibPLXBIB0008" label="[8]">Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In <em>      <em>Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</em>     </em>. ACM, 426&#x2013;434.</li>     <li id="BibPLXBIB0009" label="[9]">Dat&#x00A0;Quoc Nguyen. 2017. An overview of embedding models of entities and relationships for knowledge base completion. <em>      <em>arXiv preprint arXiv:1703.08098</em>     </em>(2017).</li>     <li id="BibPLXBIB0010" label="[10]">Maximilian Nickel, Lorenzo Rosasco, Tomaso&#x00A0;A Poggio, and others. 2016. Holographic Embeddings of Knowledge Graphs.. In <em>      <em>AAAI</em>     </em>. 1955&#x2013;1961.</li>     <li id="BibPLXBIB0011" label="[11]">Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In <em>      <em>Proceedings of the 28th international conference on machine learning (ICML-11)</em>     </em>. 809&#x2013;816.</li>     <li id="BibPLXBIB0012" label="[12]">Steffen Rendle. 2012. Factorization machines with libfm. <em>      <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em>     </em>3, 3(2012), 57.</li>     <li id="BibPLXBIB0013" label="[13]">Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In <em>      <em>Proceedings of the third ACM international conference on Web search and data mining</em>     </em>. ACM, 81&#x2013;90.</li>     <li id="BibPLXBIB0014" label="[14]">Anisa Rula, Matteo Palmonari, Axel-Cyrille&#x00A0;Ngonga Ngomo, Daniel Gerber, Jens Lehmann, and Lorenz B&#x00FC;hmann. 2014. Hybrid acquisition of temporal scopes for rdf data. In <em>      <em>European Semantic Web Conference</em>     </em>. Springer, 488&#x2013;503.</li>     <li id="BibPLXBIB0015" label="[15]">Avirup Sil and Silviu Cucerzan. 2014. Temporal scoping of relational facts based on Wikipedia data. <em>      <em>CoNLL-2014</em>     </em> (2014), 109.</li>     <li id="BibPLXBIB0016" label="[16]">Richard Socher, Danqi Chen, Christopher&#x00A0;D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In <em>      <em>Advances in neural information processing systems</em>     </em>. 926&#x2013;934.</li>     <li id="BibPLXBIB0017" label="[17]">Partha&#x00A0;Pratim Talukdar, Derry Wijaya, and Tom Mitchell. 2012. Coupled temporal scoping of relational facts. In <em>      <em>Proceedings of the fifth ACM international conference on Web search and data mining</em>     </em>. ACM, 73&#x2013;82.</li>     <li id="BibPLXBIB0018" label="[18]">Volker Tresp, Yunpu Ma, Stephan Baier, and Yinchong Yang. 2017. <em>      <em>Embedding Learning for Declarative Memories</em>     </em>. Springer International Publishing, Cham, 202&#x2013;216. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-58068-5_13"      target="_blank">https://doi.org/10.1007/978-3-319-58068-5_13</a></li>     <li id="BibPLXBIB0019" label="[19]">Rakshit Trivedi, Mehrdad Farajtabar, Yichen Wang, Hanjun Dai, Hongyuan Zha, and Le Song. 2017. Know-Evolve: Deep Reasoning in Temporal Knowledge Graphs. <em>      <em>arXiv preprint arXiv:1705.05742</em>     </em>(2017).</li>     <li id="BibPLXBIB0020" label="[20]">Denny Vrande&#x010D;i&#x0107; and Markus Kr&#x00F6;tzsch. 2014. Wikidata: a free collaborative knowledgebase. <em>      <em>Commun. ACM</em>     </em>57, 10 (2014), 78&#x2013;85.</li>     <li id="BibPLXBIB0021" label="[21]">Derry&#x00A0;Tanti Wijaya, Ndapandula Nakashole, and Tom&#x00A0;M Mitchell. 2014. CTPs: Contextual Temporal Profiles for Time Scoping Facts using State Change Detection.. In <em>      <em>EMNLP</em>     </em>. 1930&#x2013;1936.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>Dr. Leblay&#x0027;s work is supported by the KAKENHI grant number 17K12786.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://github.com/mnick/scikit-kge">https://github.com/mnick/scikit-kge</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://github.com/srendle/libfm">https://github.com/srendle/libfm</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a>http://staff.aist.go.jp/julien.leblay/datasets/</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191639">https://doi.org/10.1145/3184558.3191639</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
