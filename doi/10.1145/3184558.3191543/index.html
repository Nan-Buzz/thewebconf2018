<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>CrowdED: Guideline for Optimal Crowdsourcing Experimental
  Design</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">CrowdED: Guideline for Optimal
          Crowdsourcing Experimental Design</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <a href="https://orcid.org/0000-0003-3239-4588" ref=
          "author"><span class="givenName">Amrapali</span>
          <span class="surName">Zaveri</span></a> Institute of Data
          Science, Maastricht University, Universiteitsingel
          60Maastricht, Limburg 6229ERThe Netherlands, <a href=
          "mailto:amrapali.zaveri@maastrichtuniversity.nl">amrapali.zaveri@maastrichtuniversity.nl</a>
        </div>
        <div class="author">
          <span class="givenName">Pedro Hernandez</span>
          <span class="surName">Serrano</span> Institute of Data
          Science, Maastricht University, Universiteitsingel
          60Maastricht, LimburgThe Netherlands, <a href=
          "mailto:p.hernandezserrano@maastrichtuniversity.nl">p.hernandezserrano@maastrichtuniversity.nl</a>
        </div>
        <div class="author">
          <span class="givenName">Manisha</span> <span class=
          "surName">Desai</span> Stanford University, Stanford,
          USA, <a href=
          "mailto:manishad@stanford.edu">manishad@stanford.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Michel</span> <span class=
          "surName">Dumontier</span> Institute of Data Science,
          Maastricht University, Universiteitsingel 60Maastricht,
          LimburgThe Netherlands, <a href=
          "mailto:michel.dumontier@maastrichtuniversity.nl">michel.dumontier@maastrichtuniversity.nl</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191543"
        target=
        "_blank">https://doi.org/10.1145/3184558.3191543</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Crowdsourcing involves the creating of HITs
        (Human Intelligent Tasks), submitting them to a
        crowdsourcing platform and providing a monetary reward for
        each HIT. One of the advantages of using crowdsourcing is
        that the tasks can be highly parallelized, that is, the
        work is performed by a high number of workers in a
        decentralized setting. The design also offers a means to
        cross-check the accuracy of the answers by assigning each
        task to more than one person and thus relying on majority
        consensus as well as reward the workers according to their
        performance and productivity. Since each worker is paid per
        task, the costs can significantly increase, irrespective of
        the overall accuracy of the results. Thus, one important
        question when designing such crowdsourcing tasks that arise
        is how many workers to employ and how many tasks to assign
        to each worker when dealing with large amounts of tasks.
        That is, the main research questions we aim to answer is:
        ‘Can we a-priori estimate optimal workers and tasks’
        assignment to obtain maximum accuracy on all tasks?’. Thus,
        we introduce a two-staged statistical guideline, CrowdED,
        for optimal crowdsourcing experimental design in order to
        a-priori estimate optimal workers and tasks’ assignment to
        obtain maximum accuracy on all tasks. We describe the
        algorithm and present preliminary results and discussions.
        We implement the algorithm in Python and make it openly
        available on Github, provide a Jupyter Notebook and a R
        Shiny app for users to re-use, interact and apply in their
        own crowdsourcing experiments.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Crowdsourcing;</strong> • <strong>Human-centered
        computing</strong> → <strong>HCI design and evaluation
        methods;</strong> <strong>User models;</strong> <strong>HCI
        theory, concepts and models;</strong> <em>User
        studies;</em> Human computer interaction (HCI); •
        <strong>Applied computing</strong> → <em>Life and medical
        sciences;</em></small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Amrapali Zaveri, Pedro Hernandez Serrano, Manisha Desai,
          and Michel Dumontier. 2018. CrowdED: Guideline for
          Optimal Crowdsourcing Experimental Design. In <em>WWW '18
          Companion: The 2018 Web Conference Companion,</em>
          <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New
          York, NY, USA</em> 8 Pages. <a href=
          "https://doi.org/10.1145/3184558.3191543" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3191543</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <p>crowdsourcing, biomedical, metadata, data quality, FAIR,
    reproducibility</p>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Crowdsourcing
          as a means of Quality Assessment</h2>
        </div>
      </header>
      <p>Enormous amounts of (biomedical) data have been and are
      being produced at an unprecedented rate by researchers all
      over the world. However, in order to enable this reuse, there
      is an urgent need to understand the structure of the
      experimental data, the conditions under which they were
      produced and the relevant information that other
      investigators may need to make sense of the
      data&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>].
      That is, there is a need for good quality i.e. structured,
      accurate and complete description of the data – defined as
      <em>metadata</em>. Good quality metadata is essential in
      finding, interpreting, and reusing existing data beyond what
      the original investigators envisioned. This, in turn, can
      facilitate a data-driven approach by combining and analyzing
      similar data to uncover novel insights or even more subtle
      trends in the data. These insights can then be formed into
      hypothesis that can be tested in the
      laboratory&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>].</p>
      <p>One of the means to assess the quality of this biomedical
      metadata that we propose is by the use of microtask
      crowdsourcing i.e. non-expert workers in order to reduce the
      cost and time involved for performing the same assessment by
      means of domain experts&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>]. Crowdsourcing involves the creating
      of HITs (Human Intelligent Tasks), submitting them to a
      crowdsourcing platform (e.g. Amazon Mechanical Turk
      (MTurk)<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a>) and providing a monetary reward
      for each HIT&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>]. The tasks primarily rely on basic
      human abilities and natural language understanding but less
      on acquired skills such as domain knowledge. A great share of
      the tasks addressed via microtask platforms like MTurk could
      be referred to as ‘routine tasks’ - recognizing objects in
      images, transcribing audio and video material and text
      editing.</p>
      <p>One of the advantages of using crowdsourcing is that the
      tasks can be highly parallelized, that is, the work is
      performed by a high number of workers in a decentralized
      setting. The design also offers a means to cross-check the
      accuracy of the answers by assigning each task to more than
      one person and thus relying on majority consensus as well as
      reward the workers according to their performance and
      productivity. Since each worker is paid per task, the costs
      can significantly increase, irrespective of the overall
      accuracy of the results. Thus, one important question when
      designing such crowdsourcing tasks that arise is how many
      workers to employ and how many tasks to assign to each worker
      when dealing with large amounts of tasks. That is, how do we
      optimally design the task such that the right combination of
      workers and tasks can produce the maximum accuracy and can we
      determine this number a-priori.</p>
      <p>In order to determine the number of workers as well as
      number of tasks that would be ‘ideal’ in order to solve the
      problem, we propose <em>CrowdED</em>, a two-staged
      <em>Crowd</em>sourcing <em>E</em>xperimental <em>D</em>esign.
      CrowdED provides a guideline for designing optimal
      crowdsourcing experiments. The main research questions we aim
      to answer is:</p>
      <p><em>Can we a-priori estimate optimal workers and tasks’
      assignment to obtain maximum accuracy on all tasks?</em></p>
      <p>We describe the use case in&nbsp;sec:dataset. We describe
      our two-staged statistical guideline, CrowdED
      in&nbsp;sec:crowded. Preliminary results are reported
      in&nbsp;sec:results. Related work is discussed
      in&nbsp;sec:related. Finally, we conclude with an outlook on
      future work in&nbsp;sec:conclusion.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Use Case: GEO
          Metadata</h2>
        </div>
      </header>
      <p>Amongst the several biomedical databases available on the
      Web, the Gene Expression Omnibus (GEO) is one of the largest,
      best-known biomedical databases&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>]. GEO is an international
      public repository for high-throughput microarray and
      next-generation sequence functional genomic data submitted by
      the research community. The GEO database hosts &gt;32,000
      public series (study records) submitted directly by 3,000
      laboratories, comprising 800,000 samples derived from
      &gt;1600 organisms (as of 2012). In GEO, a Sample Record
      describes the specific conditions under which an individual
      sample was handled, the manipulations it underwent, and the
      abundance measurement of each element derived from it.</p>
      <p>In a sample, from the different metadata elements, we
      specifically chose the semi-structured ‘characteristics’
      field, which contains information about, for example, the
      disease, strain, cell line etc. used in the study. This
      information is captured in a <tt>key:&nbsp;value</tt> pair
      format. Currently users can submit data to GEO via three
      ways: (i) spreadsheets, (ii) SOFT format (plain text) or
      (iii) MINiML format (XML). When users submit data to GEO via
      a spreadsheet (namely <em>GEOarchive spreadsheet</em>), it
      requires them to fill out a metadata template that follows
      the guidelines set out by the Minimum Information About a
      Microarray Experiment (MIAME) guidelines&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>]. The metadata template
      includes fields for title, overall design, summary, the
      protocols (e.g. treatment, extraction, labeling,
      hybridization and data processing) as well as sample
      characteristics (e.g. organism, cell type, tissue). After
      submission, a curator checks the content and validity of the
      information provided&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>]. This process is not only error-prone
      but also time consuming considering the amount of manual
      labor that is involved. Moreover, without a standardized set
      of terms with which to fill out the template fields, there
      are different versions of the same entity without any
      (semantic) links between them, thus leading to several
      quality issues.</p>
      <p>Quality issues such as inaccuracy, inconsistency and
      incompleteness hamper the uptake of the datasets and also the
      reliability of the resultant applications making use of this
      data. All the 44,000,000+ <tt>key:&nbsp;value</tt> pairs in
      GEO suffer from these quality problems, which raises the
      scalability issue of performing large-scale curation.
      Additionally, with a scarcity of domain experts to curate the
      large amount of data in GEO, there is a need for more
      efficient methods for curating the metadata. Thus, we propose
      the use of crowdsourcing to perform metadata quality
      assessment.</p>
      <p>We design a microtask as a classification task of
      identifying the correct category for a given key. These key
      categories belong to the top most frequently occurring keys
      in GEO. Additionally, five top most frequently occurring
      values are also provided to the worker. For example, the key
      e.g. ‘disease specific survival years’ along with five key
      categories, namely, ‘cell line’, ‘disease’, ‘gender’,
      ‘strain’ and ‘time’ is provided to the worker along with the
      values 8.22, 17.66, 4.51, 0.89 and 12.19. The worker's task
      is to choose ‘one’ of the categories that the given key
      (best) belongs to. In this example, the worker should choose
      ‘time’ as the correct answer since the values are numerical
      indicating a time period. However, with 44,000,000+
      <tt>key:&nbsp;value</tt> pairs in GEO, we are faced the
      question of how many workers to employ in order to perform
      this large-scale curation and how many tasks to assign per
      worker so as to achieve maximum accuracy of consensus. This
      led to the design of CrowdED as described in the following
      section.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> CrowdED</h2>
        </div>
      </header>
      <p>In this section, we describe the details of the two-staged
      design for which we provide guidance on choosing the optimal
      number of workers to obtain maximum accuracy for their
      experiment. Figure&nbsp;<a class="fig" href="#fig1">1</a>
      provides an overview of the CrowdED guideline, which is
      divided into two stages. We assume a scenario where the
      worker's task (as described in&nbsp;sec:dataset) is to choose
      <em>one</em> correct answer among <em>five</em> given key
      categories. There are two stages, where the first stage
      gathers information on tasks difficulty and worker
      competency. The second stage is then designed based on what
      is learned from the first stage.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3200000/3191543/images/www18companion-282-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Overview of the CrowdED algorithm showing
          the steps involved in the two stages.</span>
        </div>
      </figure>
      <p></p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Stage
            1</h3>
          </div>
        </header>
        <p>In the first stage, the user (the requester) has the
        option to configure the following variables that represent
        the user's a priori assumptions. If unspecified, default
        (example) values are assumed, as specified in
        parentheses.</p>
        <ul class="list-no-style">
          <li id="list1" label="•">No. of tasks (100)<br /></li>
          <li id="list2" label="•">No. of workers (40)<br /></li>
          <li id="list3" label="•">No. of tasks assigned to each
          worker (7)<br /></li>
          <li id="list4" label="•">Proportion of easy (or hard)
          tasks (hard tasks - 0.2)<br /></li>
          <li id="list5" label="•">Proportion of competent or
          so-called good (or less competent or poor) workers
          (competent workers - 0.8)<br /></li>
          <li id="list6" label="•">Proportion of training tasks
          (0.4)<br /></li>
        </ul>
        <p>The number of workers per task is chosen such that it is
        an odd number, greater than the number of possible answers.
        This parameter is chosen in order to deal with cases with
        no consensus (e.g. if there are 5 possible answers and each
        of the 5 workers chooses a different answer). In our use
        case, the number of answers is 5, thus the number of
        workers per task is set to 7. However, each worker can have
        more than 7 tasks as we want to ensure that each of the
        (40% of the) training tasks have been evaluated by some of
        the workers. After setting the initial parameters, the
        algorithm randomly assigns (without replacement<a class=
        "fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>) which
        tasks and workers are easy/hard and good/poor.
        Additionally, the true answer for each task is generated by
        randomly selecting from the set of answers such that they
        are evenly distributed across all the tasks. Next, the
        exact probabilities of each worker and each task of getting
        the answer right is calculated under the following
        assumptions.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} p_w=
            {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}p_w \ge
            3/4,&amp; \text{if competent worker}\\ 1/2 {\lt} p_w
            {\lt} 3/4,&amp; \text{if less competent worker} \\ 0,
            &amp; \text{otherwise} \end{array}\right.} \\p_t=
            {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}p_t \ge
            3/4,&amp; \text{if easy task}\\ 1/2 {\lt} p_t {\lt}
            3/4,&amp; \text{if hard task} \\ 0, &amp;
            \text{otherwise}
            \end{array}\right.}\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>These values are chosen to represent variation in
        performance across workers. Based on the value specified
        for the proportion of tasks to train (40 in our example)
        the number of workers per task (7 in our example) are
        assigned to each task and the worker answer is
        generated.</p>
        <p>In practice, true answers will not be known, and thus we
        rely on an agreement statistic to gauge the performance of
        the worker using the following metric: the average
        proportion of times a worker is in agreement with other
        workers for a given tasks over all tasks considered by the
        worker. The range of the performance value spans from 0 to
        1. The values close to 1 indicate that the the worker had
        large consensus with other workers. Values close to 0
        indicate that there was no consensus for that worker among
        other workers. Then, cut off values, above the median, of
        the performance of the worker and also the probability of
        getting the answer right is set to choose which workers get
        carried forward to Stage 2. The probability and the
        performance of the worker is combined since it is not
        always the case that the workers who had a high probability
        of getting the answer right in the beginning necessarily
        performed well in the actual tasks. Thus, this combination
        ensures that the <em>best</em> workers with high
        probabilities for both measures are identified.
        Additionally, for each task we determine whether it is an
        easy or hard task based on the workers’ answers. That is,
        for all pairwise comparisons between the workers’ answer
        the truth, we match how many pairs of workers arrived at
        the same answer for each task.</p>
        <p>At the end of Stage 1, we get:</p>
        <ul class="list-no-style">
          <li id="list7" label="•">Poor workers, those that did not
          achieve high consensus amongst other workers performing
          the same task.<br />
            <ul class="list-no-style">
              <li id="list8" label="•">These workers are flagged
              and not chosen for Stage 2.<br /></li>
            </ul>
          </li>
          <li id="list9" label="•">Best workers, those with a good
          performance value and assigned the good worker status at
          the beginning.<br />
            <ul class="list-no-style">
              <li id="list10" label="•">These workers are chosen
              for Stage 2.<br /></li>
            </ul>
          </li>
          <li id="list11" label="•">Easy tasks, those that have the
          predicted performance to be 3/4 to 1.<br />
            <ul class="list-no-style">
              <li id="list12" label="•">These tasks are considered
              to have achieved majority consensus and are not
              carried forward to Stage 2 for
              re-assessment.<br /></li>
            </ul>
          </li>
          <li id="list13" label="•">Hard tasks, those that have the
          predicted performance to be below 3/4. That is, those
          that did not achieve majority consensus in Stage 1.<br />

            <ul class="list-no-style">
              <li id="list14" label="•">These tasks are then
              carried forward to Stage 2 to be
              re-assessed.<br /></li>
              <li id="list15" label="•">Unassigned tasks<br />
                <ul class="list-no-style">
                  <li id="list16" label="•">The total number of
                  tasks (100) minus the proportion of tasks to
                  train (40) = 60.<br /></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Stage
            2</h3>
          </div>
        </header>
        <p>In this stage, the algorithm assigns the hard and
        unassigned tasks to the best workers, generates the
        workers’ answers and calculates the overall accuracy of all
        the tasks. Stage 2 begins with:</p>
        <ul class="list-no-style">
          <li id="list17" label="•">Best workers<br /></li>
          <li id="list18" label="•">Hard tasks<br /></li>
          <li id="list19" label="•">Unassigned tasks<br /></li>
        </ul>
        <p>Before the best workers are assigned to the remaining of
        the tasks, it should be ensured that the workers do not
        perform the same tasks that they were assigned in Stage 1.
        To ensure this, the following pseudo-code is used:</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3200000/3191543/images/www18companion-282-img1.svg"
        class="img-responsive" alt="" longdesc="" /> Then, the
        worker answers are generated, as described in Stage 1.
        Next, the performance of each of the workers is calculated.
        Finally, data from all the tasks from Stage 1 and Stage 2
        are merged to get the final dataset of all tasks and all
        workers. After merging the datasets, a final answer is
        assigned to each task based on the majority consensus of
        the workers’ answers<a class="fn" href="#fn3" id=
        "foot-fn3"><sup>3</sup></a>. Additionally, the proportion
        of tasks for which the workers got the right answer is also
        calculated.</p>
        <p>Finally, the accuracy of all the tasks as well as the
        workers is calculated using the formula described
        below.</p>
        <p>Let <em>T</em> = total number of tasks in the
        experiment</p>
        <p>Let <span class="inline-equation"><span class=
        "tex">$\widehat{t_i} =$</span></span> (number of workers
        correctly answered task <em>t<sub>i</sub></em> ) / (number
        of workers doing task <em>t<sub>i</sub></em> ) considering
        <span class="inline-equation"><span class=
        "tex">$\widehat{t_i} \in [0,1]$</span></span></p>
        <p>Let C = subset of tasks which achieved consensus is
        greater than <span class="inline-equation"><span class=
        "tex">$\frac{1}{2}$</span></span> , more than a half means
        a majority.</p>
        <p>Let <em>n<sub>C</sub></em> = number of elements in the
        subset<em>C</em>.</p>
        <p>Then the subset <em>C</em> is defined as follows:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} C = \lbrace
            \forall \hspace{5.69046pt} \widehat{t_i}
            \hspace{5.69046pt} \vert \hspace{5.69046pt}
            \hspace{5.69046pt} \widehat{t_i} {\gt} \frac{1}{2}
            \rbrace\end{align*}</span><br />
          </div>
        </div>Finally, the accuracy of consensus is a combination
        of the following two statistics of the subset <em>C</em>.
        <p></p>
        <p>Mean of consensus:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \widehat{a}=\frac{1}{n_C}
            \sum _{t_i \in C} \widehat{t_i} \]</span><br />
          </div>
        </div>Proportion of consensus:
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \widehat{p}=\frac{n_C}{T}
            \]</span><br />
          </div>
        </div>The proportion of consensus can be seen as the
        percentage of tasks under consensus and the mean of
        consensus is how accurate the consensus is. These consensus
        values help determine the accuracy and thus the optimal
        number of workers one needs for the total number of tasks.
        <p></p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Implementation</h3>
          </div>
        </header>
        <p>The algorithm is written in Python and openly available
        for reuse at <a class="link-inline force-break" href=
        "https://github.com/pedrohserrano/crowdED">https://github.com/pedrohserrano/crowdED</a>.
        A Python package is available at <a class=
        "link-inline force-break" href=
        "https://pypi.python.org/pypi/crowdED">https://pypi.python.org/pypi/crowdED</a>
        (requires Python 3 or later versions) where one can use
        CrowdED to test with one's own values. A Jupyter Notebook
        version is available<a class="fn" href="#fn4" id=
        "foot-fn4"><sup>4</sup></a> where one can see the exact
        steps of CrowdED. Additionally, we provide a user interface
        at <a class="link-inline force-break" href=
        "https://pedrohserrano.shinyapps.io/crowdapp/">https://pedrohserrano.shinyapps.io/crowdapp/</a>,
        built using the R Shiny apps<a class="fn" href="#fn5" id=
        "foot-fn5"><sup>5</sup></a> (depicted in
        Figure&nbsp;<a class="fig" href="#fig5">5</a>) to visualize
        the interaction of the variables and their effects on the
        overall accuracy. Even though at this stage, the app does
        not allow direct user input (which is part of the future
        work), in the ‘Analysis’ tab, one can vary the number of
        simulations to see the effect on accuracy in the form of
        graphs.</p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Preliminary
          Results</h2>
        </div>
      </header>
      <p>We tested our algorithm by generating random value
      distributions for the variables as:</p>
      <ul class="list-no-style">
        <li id="list20" label="•">tasks = [60, 80, 100, 120, 140,
        160, 180]<br /></li>
        <li id="list21" label="•">workers = [20, 30, 40]<br /></li>
        <li id="list22" label="•">answers key = [”liver”, ”blood”,
        ”lung”, ”brain”, ”heart”]<br /></li>
        <li id="list23" label="•">good workers = [0.1, 0.3, 0.5,
        0.7, 0.9]<br /></li>
        <li id="list24" label="•">hard tasks = [0.1, 0.3, 0.5, 0.7,
        0.9]<br /></li>
        <li id="list25" label="•">proportion of training tasks =
        [0.2, 0.3, 0.4, 0.5, 0.6]<br /></li>
        <li id="list26" label="•">workers per task = [3, 5, 7, 9,
        11]<br /></li>
      </ul>
      <p>In total, there were 13,125 of combinations that were
      tested for each of the variables, and every combination were
      simulated one thousand times, preliminary results of which
      are described below.</p>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3200000/3191543/images/www18companion-282-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">(a) Matrix showing accuracy values for
          different number of tasks, number of workers and varying
          proportions of the good workers. The darker green cells
          show higher accuracy while the blue ones show lower
          accuracy. Results suggest that starting out with good
          workers does not always lead to high accuracy. The
          performance of the workers in combination with whether
          they were a good worker ensures that they are the best
          workers. This is why we need a two-staged crowdsourcing
          design. (b) Matrix showing accuracy values for different
          number of tasks, number of workers and varying
          proportions of the hard tasks. The darker green cells
          show higher accuracy while the blue ones show lower
          accuracy. Results support our intuition that the lesser
          the hard tasks (10%), the higher the accuracy.</span>
        </div>
      </figure>
      <p></p>
      <p><em>Proportion of good and poor workers</em>. In most
      crowdsourcing platforms, one has the option to choose ‘good’
      workers before launching the tasks. For example, in Mturk the
      so-called ‘Master workers’ can be chosen by specifying their
      HIT (Human Intelligence Task) acceptance rate. These workers
      are assigned this status depending on their performance over
      all the tasks they have attempted and their acceptance rate
      for these. However, based on our results, we observe that
      starting out with good workers does not always lead to high
      accuracy. Figure&nbsp;<a class="fig" href="#fig2">2</a>(a)
      shows a matrix with the accuracy values for different number
      of tasks, number of workers and varying proportions of the
      good workers. The darker green cells show higher accuracy
      while the blue ones show lower accuracy. With 90% of good
      workers at the start, the accuracy ranges from 0.82 to 0.86
      whereas starting with 10% of the tasks, the accuracy ranges
      from 0.84 to 0.88. Thus, it is inconclusive of the proportion
      of good workers to start with. However, adopting the
      two-staged algorithm ensures that only the best workers are
      chosen to perform all the tasks. Therefore, calculating the
      performance of the workers in combination with whether she
      was a good worker (from the beginning) ensures that she is
      the best worker. This is why we need a two-staged
      crowdsourcing design in order to test the workers performance
      and choosing only the best workers to perform the total set
      of tasks in order to achieve high accuracy.</p>
      <p><em>Proportion of easy and hard tasks</em>. We determined
      the effect on accuracy depending on the proportion of hard
      and easy tasks. Figure&nbsp;<a class="fig" href=
      "#fig2">2</a>(b) shows a matrix of the accuracy values for
      different number of tasks, number of workers and varying
      proportions of the hard tasks. The darker green cells show
      higher accuracy while the blue ones show lower accuracy. With
      10% of hard tasks, the accuracy ranges from 0.88 to 0.9
      whereas with 90% of hard tasks, the accuracy ranges from 0.78
      to 0.8. Results support the intuition that reduced difficulty
      (10%) in tasks result in higher accuracy.</p>
      <p><em>Proportion of training tasks</em>. We analyzed the
      results for the ideal proportion of total tasks that should
      be trained in Stage 1. Figure&nbsp;<a class="fig" href=
      "#fig3">3</a> shows a heat map with the accuracy values for
      different workers per task and the percentage of tasks
      trained in Stage 1. The darker green bubbles show higher
      accuracy while the blue ones show lower accuracy. The values
      inside the bubble are ‘a’ is mean and ‘p’ is proportion of
      consensus (as described in Section&nbsp;<a class="sec" href=
      "#sec-7">3</a>). With 20%, 30%, 40% of training tasks and 3,
      5, 7 and 9 workers per task, the accuracy ‘a’ is lower as
      compared to 40%, 50%, 60% of training tasks with 3, 5 and 7
      workers per task. Results suggest that ideal is to use 3, 5
      or 7 workers per task and train 40% to 60% of the task in
      Stage 1 to achieve high accuracy.</p>
      <figure id="fig3">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3200000/3191543/images/www18companion-282-fig3.jpg"
        class="img-responsive" alt="Figure 3" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 3:</span> <span class=
          "figure-title">Heat map showing the accuracy for
          different workers per task vs. the percentage of tasks
          trained in Stage 1. Green bubbles show higher accuracy
          while the blue bubbles indicate lower accuracy. The
          values inside the bubble are ‘a’ is accuracy of
          consensuses and ‘p’ is proportion of consensus. Results
          suggest that ideal is to use 3, 5 or 7 workers per task
          and train 40% to 60% of the task in Stage 1 to achieve
          high accuracy.</span>
        </div>
      </figure>
      <p></p>
      <p><em>Number of workers per task</em>. We examined how the
      number of workers per task affects the accuracy and the
      proportion of consensus. Figure&nbsp;<a class="fig" href=
      "#fig4">4</a> shows how the ratio of all workers over all
      tasks (X-axis) compares to proportion of accuracy and
      proportion of consensus (Y-axis) when the number of workers
      is different per task (3, 5, 7 and 9). With 3, 5 and 7
      workers per task, the accuracy of consensus remains stable at
      a range of 0.8 to 0.9. However, the accuracy declines
      significantly with 9 workers per task. Additionally, the
      proportion of consensus increases uniformly along with
      significant p-values for each variation with 3, 5 and 7
      workers per task. However, with 9 workers per task, the
      proportion for accuracy also decreases along with a
      non-significant p-value. Results suggest that after 9 workers
      per task the accuracy and proportion of consensus
      decreases.</p>
      <figure id="fig4">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3200000/3191543/images/www18companion-282-fig4.jpg"
        class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class=
          "figure-title">Plots of the ratio of all workers over all
          tasks (X-axis) with respect to the proportion of accuracy
          (above) and proportion of consensus (below) on the Y-axis
          for varying number of workers per task (3, 5, 7 and
          9).</span>
        </div>
      </figure>
      <p></p>
      <p><em>Overall results</em>. The preliminary results of these
      simulations suggest that in order to achieve high
      accuracy:</p>
      <ul class="list-no-style">
        <li id="list27" label="•">the number of workers should be
        40% to 60% of the total number of tasks<br /></li>
        <li id="list28" label="•">to train workers on 40% to 60% of
        the tasks in Stage 1<br /></li>
        <li id="list29" label="•">to set the number of workers per
        task to be either 3, 5 or 7 (or fewer than 9)<br /></li>
        <li id="list30" label="•">to reduce the number of hard
        tasks<br /></li>
        <li id="list31" label="•">to adopt the two-staged algorithm
        to identify the best workers<br /></li>
      </ul>
      <figure id="fig5">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3200000/3191543/images/www18companion-282-fig5.jpg"
        class="img-responsive" alt="Figure 5" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 5:</span> <span class=
          "figure-title">Screenshot of the CrowdED R Shiny app
          showing a beta version of the interface available at
          <a class="link-inline force-break" href=
          "https://pedrohserrano.shinyapps.io/crowdapp/">https://pedrohserrano.shinyapps.io/crowdapp/</a>.
          The ‘Analysis’ tab is pre-configured to the default
          values and provides a slider for 10 simulations. The
          accuracy calculated for each simulation is depicted as
          graphs.</span>
        </div>
      </figure>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Related
          Work</h2>
        </div>
      </header>
      <p>There have been empirical studies to determine the
      ‘optimal’ number of workers per task. However, these studies
      only focus on their domain or task at hand. For example,
      there is an adaptive model&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>] which studied different scenarios of
      increasing complexity of tasks wrt. the worker quality. This
      strategy was applied particularly to labeling tasks. However,
      they assume that all workers are of the same quality Another
      strategy employs active learning algorithms (changing the
      assignments per tasks in real-time) to minimize the number of
      questions asked to the crowd to maximize the number of
      tasks&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>].
      However, reportedly this model is extremely expensive to
      adapt in a real-world experiment. Another study assigns tasks
      based on the quality of workers and suggest that, for
      example, between three and eight workers is
      ideal&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>].
      In&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0013">13</a>],
      test questions created from a generalized knowledge base are
      used to estimate the reliability of the new workers. Their
      result suggest that this approach performs better than using
      gold-standard tasks automated selection of knowledge base
      questions for quality control [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>] used a hybrid approach of
      self-rating and gold-standard task for estimating the
      expertise of workers, however the self-assessment does not
      ensure high accuracy on the actual tasks.</p>
      <p>Two models&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>] and&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0006">6</a>] provided approaches for
      cost-quality and cost-time optimization respectively.
      However, the former model is focused on each task and
      requires that the pay be set based on progress of the total
      number of tasks. The latter model assumes a fixed number of
      workers per task and and does not optimize quality by taking
      a variable number of workers based on each task difficulty. A
      recent study&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>] introduced an AI agent, OCTOPUS, to
      jointly balance the quality of work, total cost incurred and
      time to completion and significantly outperformed existing
      state-of-the-art approaches. However, OCTOPUS only tested for
      tasks that contained a binary choice for the answer. CrowdED
      is distinct from all these studies as it offers a two-staged
      statistical model that can <em>a-priori</em> estimate the
      number of workers to assign per task in order to gain maximum
      accuracy whereas OCTOPUS optimizes on-the-fly.</p>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusions and
          Future Work</h2>
        </div>
      </header>
      <p>In this paper, we describe a two-staged statistical
      guideline, CrowdED, for designing optimal crowdsourcing tasks
      in order to a-priori estimate optimal workers and tasks’
      assignment to obtain maximum accuracy on all tasks. We
      implemented the algorithm in Python and made it openly
      available on Github, a Python package, provided a Jupyter
      Notebook and a R Shiny app for users to re-use, interact and
      apply in their own crowdsourcing experiments. Our preliminary
      results suggest the ‘optimal’ values for each of the
      variables in order to achieve maximum accuracy for the tasks.
      This is a first step towards answering our research question
      of estimate a-priori the optimal task-worker assignment
      towards high accuracy.</p>
      <p>At this stage, CrowdED only simulates multiple-choice
      questions type of crowdsourcing experiments and not free text
      answers. In future work, we will explore the feasibility of
      Natural Language Processing (NLP) approaches to evaluate the
      accuracy of free-text answers. Also as part of future work,
      we will assess the operating characteristics of this design,
      and perform testing of the algorithm on our use case as well
      as other real-world input data. Additionally, we will compare
      the results of these approaches to the baseline approaches
      that are standard crowdsourcing platforms (e.g.
      CrowdFlower<a class="fn" href="#fn6" id=
      "foot-fn6"><sup>6</sup></a>, MTurk). Moreover, we will
      account for the budgetary constraints in the optimization
      algorithm. Also, we will extend the interface such that a
      user can vary parameters and assumptions to see how sensitive
      the design is to various assumptions.</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Acknowledgements</h2>
        </div>
      </header>
      <p>The authors would like to acknowledge that this project
      was funded by NCATS (National Center for Advancing
      Translational Science <a class="link-inline force-break"
      href="https://ncats.nih.gov/">https://ncats.nih.gov/</a>)
      Deeplink grant (no. 35700002N).</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Brazma A, Hingamp P,
        Quackenbush J, Sherlock G, Spellman P, Stoeckert C, Aach J,
        Ansorge W, Ball CA, Causton HC, Gaasterland T, Glenisson P,
        Holstege FC, Kim IF, Markowitz V, Matese JC, Parkinson H,
        Robinson A, Sarkans U, Schulze-Kremer S, Stewart J, Taylor
        R, Vilo J, and Vingron M.2011. Minimum information about a
        microarray experiment (MIAME)-toward standards for
        microarray data. <em><em>Nature Genetics</em></em>
        29(2011), 365–371. Issue 4.</li>
        <li id="BibPLXBIB0002" label="[2]">Tanya Barrett, Dennis B.
        Troup, Stephen E. Wilhite, Pierre Ledoux, Carlos
        Evangelista, Irene F. Kim, Maxim Tomashevsky, Kimberly A.
        Marshall, Katherine H. Phillippy, Patti M. Sherman, Rolf N.
        Muertter, Michelle Holko, Oluwabukunmi Ayanbule, Andrey
        Yefanov, and Alexandra Soboleva. 2011. NCBI GEO: archive
        for functional genomics data sets — 10 years on.
        <em><em>Nucleic Acids Research</em></em> 39 (2011), 991 –
        995.</li>
        <li id="BibPLXBIB0003" label="[3]">Good BM, Nanis M, Wu C,
        and Su AI. 2015. Microtask crowdsourcing for disease
        mention annotation in PubMed abstracts. <em><em>Pac Symp
        Biocomput.</em></em> (2015), 282–293.</li>
        <li id="BibPLXBIB0004" label="[4]">C. L. Borgman. 2012. The
        conundrum of sharing research data. <em><em>Journal of the
        American Society for Information Science and
        Technology</em></em> 63(2012), 1059 – 1078. Issue 6.</li>
        <li id="BibPLXBIB0005" label="[5]">Peng Dai, Christopher H.
        Lin, Mausam, and Daniel S. Weld. 2013. POMDP-based Control
        of Workflows for Crowdsourcing. <em><em>Artif.
        Intell.</em></em> 202, 1 (Sept. 2013), 52–85. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1016/j.artint.2013.06.002" target=
        "_blank">https://doi.org/10.1016/j.artint.2013.06.002</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">Yihan Gao and Aditya
        Parameswaran. 2014. Finish Them!: Pricing Algorithms for
        Human Computation. <em><em>Proc. VLDB Endow.</em></em> 7,
        14 (Oct. 2014), 1965–1976. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.14778/2733085.2733101" target="_blank">
          https://doi.org/10.14778/2733085.2733101</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Karan Goel, Shreya
        Rajpal, and Mausam. 2017. Octopus: A Framework for
        Cost-Quality-Time Optimization in Crowdsourcing.
        <em><em>CoRR</em></em> abs/1702.03488(2017).
        arxiv:1702.03488<a class="link-inline force-break" href=
        "http://arxiv.org/abs/1702.03488" target=
        "_blank">http://arxiv.org/abs/1702.03488</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Jeff Howe. 2006. The
        Rise of Crowdsourcing. <em><em>Wired Magazine</em></em> 14,
        6 (06 2006). <a class="link-inline force-break" href=
        "http://www.wired.com/wired/archive/14.06/crowds.html"
        target=
        "_blank">http://www.wired.com/wired/archive/14.06/crowds.html</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">Barzan Mozafari, Purna
        Sarkar, Michael Franklin, Michael Jordan, and Samuel
        Madden. 2014. Scaling up crowd-sourcing to very large
        datasets: a case for active learning. <em><em>Proceedings
        of the VLDB Endowment</em></em> 8 (2014), 125–136. Issue
        2.</li>
        <li id="BibPLXBIB0010" label="[10]">Victor S. Sheng, Foster
        Provost, and Panagiotis G. Ipeirotis. 2008. Get another
        label? improving data quality and data mining using
        multiple, noisy labelers. In <em><em>Proceedings of the
        14th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em> . 614 – 622.</li>
        <li id="BibPLXBIB0011" label="[11]">Barrett T, Wilhite SE,
        Ledoux P, Evangelista C, Kim IF, Tomashevsky M, Marshall
        KA, Phillippy KH, Sherman PM, Holko M, Yefanov A, Lee H,
        Zhang N, Robertson CL, Serova N, Davis S, and Soboleva
        A.2013. NCBI GEO: archive for functional genomics data
        sets–update. <em><em>Nucleic Acids Research</em></em> 41
        (2013), 991 – 995.</li>
        <li id="BibPLXBIB0012" label="[12]">U. Ul Hassan, S.
        O‘Riain, and E. Curry. 2013. Effects of expertise
        assessment on the quality of task routing in human
        computation. <em><em>Proceedings of the 2nd International
        Workshop on Social Media for Crowdsourcing and Human
        Computation</em></em> (2013).</li>
        <li id="BibPLXBIB0013" label="[13]">Umair ul Hassan,
        Amrapali Zaveri, Edgard Marx, Edward Curry, and Jens
        Lehmann. 2016. ACRyLIQ: Leveraging DBpedia for Adaptive
        Crowdsourcing in Linked Data Quality Assessment. In
        <em><em>Knowledge Engineering and Knowledge
        Management</em></em> , Eva Blomqvist, Paolo Ciancarini,
        Francesco Poggi, and Fabio Vitali (Eds.). Springer
        International Publishing, Cham, 681–696.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "http://mturk.com">http://mturk.com</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://www.ma.utexas.edu/users/parker/sampling/repl.htm">https://www.ma.utexas.edu/users/parker/sampling/repl.htm</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>This is done
    because there may be cases where the workers converge on an
    answer different than the truth value, which need not be
    necessarily incorrect.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/pedrohserrano/crowdED/blob/master/notebooks/Crowdsourcing.ipynb">https://github.com/pedrohserrano/crowdED/blob/master/notebooks/Crowdsourcing.ipynb</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class=
    "link-inline force-break" href=
    "https://shiny.rstudio.com/">https://shiny.rstudio.com/</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class=
    "link-inline force-break" href=
    "http://crowdflower.com">crowdflower.com</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3191543">https://doi.org/10.1145/3184558.3191543</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
