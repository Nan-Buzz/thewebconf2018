<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Multi-task Learning for Author Profiling with Hierarchical Features</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3184558.3186926"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186926'>https://doi.org/10.1145/3184558.3186926</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186926'>https://w3id.org/oa/10.1145/3184558.3186926</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Multi-task Learning for Author Profiling with Hierarchical Features</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Zhile</span> <span class="surName">Jiang</span> <sup>1</sup>College of Computer Science, Sichuan University
        </div>
        <div class="author">
          <span class="givenName">Shuai</span> <span class="surName">Yu</span> <sup>2</sup> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
        </div>
        <div class="author">
          <span class="givenName">Qiang</span> <span class="surName">Qu</span> <sup>2</sup> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
        </div>
        <div class="author">
          <span class="givenName">Min</span> <span class="surName">Yang</span> <sup>2</sup> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
        </div>
        <div class="author">
          <span class="givenName">Junyu</span> <span class="surName">Luo</span> <sup>1</sup>College of Computer Science, Sichuan University
        </div>
        <div class="author">
          <span class="givenName">Juncheng</span> <span class="surName">Liu</span> <sup>1</sup>College of Computer Science, Sichuan University<a class="fn" href="#fn1" id="foot-fn1"><sup>⁎</sup></a>,
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186926" target="_blank">https://doi.org/10.1145/3184558.3186926</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Author profiling is an important but challenging task. In this paper, we propose a novel Multi-Task learning framework for Author Profiling (<em>MTAP</em>), in which a document modeling module is shared across three different author profiling tasks (i.e., age, gender and job classification tasks). To further boost author profiling, we integrate hierarchical features learned by different models. Concretely, we employ CNN, LSTM and topic model to learn the character-level, word-level and topic-level features, respectively. <em>MTAP</em> thus leverages the benefits of supervised deep neural neural networks as well as an unsupervised probabilistic generative model to enhance the document representation learning. Experimental results on a real-life blog dataset show that <em>MTAP</em> has robust superiority over competitors and sets state-of-the-art for all the three author profiling tasks<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Author profiling</small>,</span> <span class="keyword"><small>Multi-task learning</small>,</span> <span class="keyword"><small>Hierarchical features</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Zhile Jiang, Shuai Yu, Qiang Qu, Min Yang, Junyu Luo, and Juncheng Liu. 2018. Multi-task Learning for Author Profiling with Hierarchical Features. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 2 Pages. <a href="https://doi.org/10.1145/3184558.3186926" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186926</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Author Profiling is to ascertain various author characteristics like age, gender, native country, job by analyzing their written documents. It is of growing importance in the fields of forensics, security, and marketing. For example, from a forensic linguistics perspective one would like to know the linguistic profile of the author of a harassing text message on the web to narrow down the investigation. Recently, numerous author profiling methods have been developed [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>].</p>
      <p>Despite the remarkable progress of previous methods, we argue that the author profiling of informal texts still remains challenging in real-world. (i) Most previous approaches consider the profiling tasks independent from each other and train the tasks separately. However, the profiling tasks are often mutually dependent; (ii) The principle behind author profiling is that persons with different characteristics write in different ways, thus everyone's writing styles are different. The information about word morphology and shape is crucial for capturing authors’ characteristics. (iii) The prior studies are often based on the bag-of-word assumption. They neglect the semantics of words though these factors are important in identifying author's characteristics, thus inevitably losing the distinguishable information; (iv) The authors with different characteristics have their own special interests, as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], the interest topics of authors should be taken into account to further improve the performance of author profiling.</p>
      <p>To alleviate the aforementioned limitations, <em>MTAP</em> simultaneously optimizes three coupled profiling tasks: age classification, gender classification and job classification. In addition, <em>MTAP</em> integrates hierarchical features learned by different models to further boost the performance of author profiling. First, long short-term memory (LSTM) is used to encode the character-level features. Second, a convolutional neural network (CNN) model with 1-D convolutional filters is employed to focus on learning useful word-level n-gram features. Meanwhile, we employ Latent Dirichlet Allocation (LDA) to obtain topic-level features. Some word distributions learned by LDA correspond to authorship style as reflected by the vocabulary and interests of authors.</p>
      <p>The main contribution of our approach is threefold: (1) To the best of our knowledge, we are the first to use the multi-task learning to jointly train three typical author profiling tasks. (2) <em>MTAP</em> integrates hierarchical features learned by different models, it can be regarded as an ensemble classifier that improves the robustness and generalization performance of a set of classifiers; (3) The experimental results show that <em>MTAP</em> consistently outperforms competitive methods on different profiling tasks.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Our Method</h2>
        </div>
      </header>
      <p>We use <em>x</em> = {<em>w</em> <sub>1</sub>, <em>w</em> <sub>2</sub>, …, <em>w<sub>N</sub></em> } to denote the sequence of input document <em>x</em> , where <em>N</em> is the length of the sequence. To prevent conceptual confusion, we use superscripts ”<em>chr</em>”, ”<em>word</em>”, ”<em>topic</em>” to indicate the variables that are related to character-level, word-level and topic-level features, respectively.</p>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Character-level feature representation</h3>
          </div>
        </header>
        <p>We compute the character-level feature representation of each word with LSTM model. Given a word <em>w</em> composed of <em>M</em> characters <em>c</em> <sub>1</sub>, …, <em>c<sub>M</sub></em> , we first transform each characters <em>c<sub>m</sub></em> into a character embedding <span class="inline-equation"><span class="tex">$r_{c_{m}}^{chr}$</span></span> that is encoded by column vectors in the character embedding matrix <span class="inline-equation"><span class="tex">$W^{chr}\in \mathbf {R}^{d^{chr}\times |V^{chr}|}$</span></span> : <span class="inline-equation"><span class="tex">$r_{c_{m}}^{chr}=W^{chr}v_{c_{m}}^{chr}$</span></span> (<span class="inline-equation"><span class="tex">$v_{c_m}^{chr}$</span></span> : the one-hot representation of character <em>c<sub>m</sub></em> ; <em>d<sup>chr</sup></em> : the size of the character embedding; |<em>V<sup>chr</sup></em> |: the size of character vocabulary). Then, we use a LSTM layer to convert the sequence of character embeddings <span class="inline-equation"><span class="tex">$\lbrace r_{c_1}^{chr}, \dots , r_{c_M}^{chr}\rbrace$</span></span> into to a sequence of hidden states <span class="inline-equation"><span class="tex">$\lbrace h_{c_1}^{chr}, \dots , h_{c_M}^{chr}\rbrace$</span></span> . In general, we represent each word <em>w</em> with the last hidden state, that is <span class="inline-equation"><span class="tex">$h_{w}^{chr}=h_{c_M}^{chr}$</span></span> . The sequence of word representations are fed into another LSTM and the last hidden state of the LSTM is the input of a non-linear layer, resulting a <em>T</em>-dimensional document representation <span class="inline-equation"><span class="tex">$emb_{x}^{chr}$</span></span> that models the character-level features of document <em>x</em>.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Word-level feature representation</h3>
          </div>
        </header>
        <p>For each document <em>x</em>, we encode the the sequence of word representations with a CNN layer that can learn the abstract representations of n-grams effectively and tackle the sentences with variable lengths naturally. We use multiple filters with varying window sizes to obtain multiple features and then apply a max-over-time pooling operation over the features. These pooled features are passed to a fully connected non-linear layer whose output is <em>T</em>-dimensional word-level document representation <span class="inline-equation"><span class="tex">$emb_{x}^{word}$</span></span> .</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Topic-level feature representation</h3>
          </div>
        </header>
        <p>We apply LDA to model the corpus. Under the assumptions of LDA, each document <em>x</em> with <em>N</em> tokens is generated by firstly choosing a document topic distribution <em>θ<sup>topic</sup></em> . Then, each token <em>w</em> in the document is generated by choosing a topic <em>z</em> from the document topic distribution <em>θ<sup>topic</sup></em> and choosing a token from the topic word distribution <span class="inline-equation"><span class="tex">$\phi _z^{topic}$</span></span> . The model can be inferred from the corpus using Gibbs sampling, as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. In this paper, we aim to obtain the document topic distributions <em>θ<sup>topic</sup></em> , which is fed into a non-linear layer to produce a <em>T</em>-dimensional representation <span class="inline-equation"><span class="tex">$emb_{x}^{topic}$</span></span> , representing topic-level features.</p>
        <p>Finally, for document <em>x</em> we combine the character-level, word-level and topic-level features with simple Hadamard product, obtaining the final document representation <span class="inline-equation"><span class="tex">$emb_{x} = emb_{x}^{chr} \circ emb_{x}^{word} \circ emb_{x}^{topic}$</span></span> . The final document embedding <em>emb<sub>x</sub></em> is referred as the ensemble embedding of all hierarchical features we extracted, and is then fed to a <em>softmax</em> classifier to predict the author profile of the given document.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">The accuracies of author profiling tasks on Blog dataset</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">Method</td>
                <td style="text-align:center;">Gender</td>
                <td style="text-align:center;">Age</td>
                <td>Job</td>
              </tr>
              <tr>
                <td style="text-align:left;">SVM</td>
                <td style="text-align:center;">66.7</td>
                <td style="text-align:center;">62.3</td>
                <td>28.5</td>
              </tr>
              <tr>
                <td style="text-align:left;">LDAH</td>
                <td style="text-align:center;">63.2</td>
                <td style="text-align:center;">61.7</td>
                <td>20.3</td>
              </tr>
              <tr>
                <td style="text-align:left;">BMR</td>
                <td style="text-align:center;">76.1</td>
                <td style="text-align:center;">77.7</td>
                <td>-</td>
              </tr>
              <tr>
                <td style="text-align:left;">LSTM</td>
                <td style="text-align:center;">70.3</td>
                <td style="text-align:center;">65.9</td>
                <td>29.7</td>
              </tr>
              <tr>
                <td style="text-align:left;">CNN</td>
                <td style="text-align:center;">71.1</td>
                <td style="text-align:center;">66.0</td>
                <td>29.2</td>
              </tr>
              <tr>
                <td style="text-align:left;">MTAP</td>
                <td style="text-align:center;"><strong>79.2</strong></td>
                <td style="text-align:center;"><strong>79.6</strong></td>
                <td><strong>37.6</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;">w/o multitask</td>
                <td style="text-align:center;">77.4</td>
                <td style="text-align:center;">77.2</td>
                <td>36.4</td>
              </tr>
              <tr>
                <td style="text-align:left;">w/o chr-level features</td>
                <td style="text-align:center;">79.1</td>
                <td style="text-align:center;">79.4</td>
                <td>37.0</td>
              </tr>
              <tr>
                <td style="text-align:left;">w/o word-level features</td>
                <td style="text-align:center;">75.3</td>
                <td style="text-align:center;">74.8</td>
                <td>35.3</td>
              </tr>
              <tr>
                <td style="text-align:left;">w/o topic-level features</td>
                <td style="text-align:center;">74.2</td>
                <td style="text-align:center;">72.1</td>
                <td>34.9</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.4</span> Multi-task learning</h3>
          </div>
        </header>
        <p><em>MTAP</em> consists of three subtasks, each has its own training objective. We use two task-specific fully connect layers (i.e., <em>softmax</em> layer) to predict the gender, age, or job of the author of a set of documents. Each classifier is trained by minimizing the cross-entropy between the predicted distribution and the ground truth distribution. We denote the objective functions of gender, age, and job as <em>L<sup>gender</sup></em> , <em>L<sup>age</sup></em> and <em>L<sup>job</sup></em> , respectively. For the purpose of improving the document learning, we train these three related tasks simultaneously. The joint multi-task objective function is minimized by: <em>L</em> = <em>L<sup>gender</sup></em> + <em>L<sup>age</sup></em> + <em>L<sup>job</sup></em> . We use a minibatch stochastic gradient descent (SGD) algorithm to update the parameters of the model.</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experiments</h2>
        </div>
      </header>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Datasets</h3>
          </div>
        </header>
        <p>We conduct experiments on <em>Blog</em> dataset, which consists of 678,161 blog posts by 19,320 bloggers. Each blog contains the self-reported gender, age, industry and astrological sign. The statistics of the dataset can be found in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]. For each task, we use 80% documents of each author as the training data, 10% documents of each author as the validation data, and the remaining are used for testing.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Experimental results</h3>
          </div>
        </header>
        <p>We compare our approach with several strong baseline methods including SVM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>], LDAH [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>], BRM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], LSTM, and CNN.</p>
        <p>In the experiments, the results are evaluated using classification accuracy. The experimental results are summarized in Table 1. We observe that our method substantially outperforms the competitive methods and gets state-of-the-art on all the three tasks. This verifies the effectiveness of the proposed model.</p>
        <p>We also report the ablation test of our model in terms of discarding multi-task learning, character-level features, word-level features and topic-level features, respectively (bottom four rows of Table 1). Generally, all four kinds of factors contribute, and topic-level features contribute most. This is within our expectation since the word distributions correspond to authorship style reflect authors’ vocabulary and interests.</p>
      </section>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Kholoud Alsmearat, Mahmoud Al-Ayyoub, and Riyad Al-Shalabi. 2014. An extensive study of the bag-of-words approach for gender identification of arabic articles. In <em><em>AICCSA</em></em> . IEEE, 601–608.</li>
        <li id="BibPLXBIB0002" label="[2]">Emad AlSukhni and Qasem Alequr. 2016. Investigating the Use of Machine Learning Algorithms in Detecting Gender of the Arabic Tweet Author. <em><em>IJACSA</em></em> 7, 7 (2016), 319–328.</li>
        <li id="BibPLXBIB0003" label="[3]">Shlomo Argamon, Moshe Koppel, James&nbsp;W Pennebaker, and Jonathan Schler. 2009. Automatically profiling the author of an anonymous text. <em><em>Commun. ACM</em></em> 52, 2 (2009), 119–123.</li>
        <li id="BibPLXBIB0004" label="[4]">Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In <em><em>ACL</em></em> . 79–86.</li>
        <li id="BibPLXBIB0005" label="[5]">J Schler, M Koppel, S Argamon, and J Pennebaker. 2006. Effects of Age and Gender on Blogging. In <em><em>AAAI</em></em> .</li>
        <li id="BibPLXBIB0006" label="[6]">Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert. 2011. Authorship Attribution with Latent Dirichlet Allocation. In <em><em>CoNLL</em></em> .</li>
        <li id="BibPLXBIB0007" label="[7]">Min Yang, Xiaojun Chen, Wenting Tu, Ziyu Lu, Jia Zhu, and Qiang Qu. 2018. A Topic Drift Model for authorship attribution. <em><em>Neurocomputing</em></em> 273(2018), 133–140.</li>
        <li id="BibPLXBIB0008" label="[8]">Min Yang, Jincheng Mei, Fei Xu, Wenting Tu, and Ziyu Lu. 2016. Discovering author interest evolution in topic modeling. In <em><em>SIGIR</em></em> . ACM, 801–804.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Min Yang is the corresponding author (min.yang@siat.ac.cn). The work was partially supported by the CAS Pioneer Hundred Talents Program.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Data and codes are available at:<a class="link-inline force-break" href="https://goo.gl/vsSxGt">https://goo.gl/vsSxGt</a></p>
    <p id="fn3"><label>3</label>http://u.cs.biu.ac.il/&nbsp;koppel/BlogCorpus.htm</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186926">https://doi.org/10.1145/3184558.3186926</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
