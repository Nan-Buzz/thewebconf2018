<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Emmanouil</span>      <span class="surName">Krasanakis</span>,     CERTH-ITI, Thessaloniki, Greece, <a href="mailto:maniospas@iti.gr">maniospas@iti.gr</a>     </div>     <div class="author">     <span class="givenName">Eleftherios</span>      <span class="surName">Spyromitros-Xioufis</span>,     CERTH-ITI, Thessaloniki, Greece, <a href="mailto:espyromi@iti.gr">espyromi@iti.gr</a>     </div>     <div class="author">     <span class="givenName">Symeon</span>      <span class="surName">Papadopoulos</span>,     CERTH-ITI, Thessaloniki, Greece, <a href="mailto:papadop@iti.gr">papadop@iti.gr</a>     </div>     <div class="author">     <span class="givenName">Yiannis</span>      <span class="surName">Kompatsiaris</span>,     CERTH-ITI, Thessaloniki, Greece, <a href="mailto:ikom@iti.gr">ikom@iti.gr</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186133" target="_blank">https://doi.org/10.1145/3178876.3186133</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Machine learning bias and fairness have recently emerged as key issues due to the pervasive deployment of data-driven decision making in a variety of sectors and services. It has often been argued that unfair classifications can be attributed to bias in training data, but previous attempts to &#x201C;repair&#x201D; training data have led to limited success. To circumvent shortcomings prevalent in data repairing approaches, such as those that weight training samples of the sensitive group (e.g. gender, race, financial status) based on their misclassification error, we present a process that iteratively adapts training sample weights with a theoretically grounded model. This model addresses different kinds of bias to better achieve fairness objectives, such as trade-offs between accuracy and disparate impact elimination or disparate mistreatment elimination. We show that, compared to previous fairness-aware approaches, our methodology achieves better or similar trades-offs between accuracy and unfairness mitigation on real-world and synthetic datasets.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Cost-sensitive learning;</strong> &#x2022;<strong> Theory of computation </strong>&#x2192; <em>Boosting;</em> &#x2022;<strong> Applied computing </strong>&#x2192; Law;</small> </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Emmanouil Krasanakis, Eleftherios Spyromitros-Xioufis, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186133" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186133</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>As machine learning systems are currently deployed in an ever-growing number of services that affect people&#x0027;s lives, fairness concerns have become increasingly important. Such concerns are well justified, since automated decision-making systems can be biased against sensitive groups, if not properly constrained. For example, training a logistic regression classifier on the ProPublica COMPAS dataset of crime recidivism [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] yields differences between black and white defendants that amount to 17% for false positive and 25% for false negative rates. Hence, it is understandable why legal measures are in place to explicitly protect the right of minorities to not be subjected to different policies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>].</p>    <p>Fairness concern formulations compare aspects of a classifier between sensitive and non-sensitive groups (see Subsection&#x00A0;<a class="sec" href="#sec-7">2.1</a>). The measure of choice often depends on the respective legal setting, as well as on whether the ground truth is biased. For example, if the ground truth is historically unbiased, it is preferable to mitigate misclassification differences between sensitive and non-sensitive groups [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>].</p>    <p>Researchers have previously recognized that classification bias is often caused by data rather than classifiers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. For example, Kamishima et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] categorize sources of unfair labeling as prejudice stemming from correlations between features and a sensitive attribute, underestimation due to inadequate convergence of the training algorithm and negative legacy of (historical) human biases in labeling training data. Hence, it has often been argued that we should look for ways to remove bias from training data (instead of constraining the training process), either through massaging the training labels [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] or reweighting training samples according to an estimated probability that they belong to a sensitive group [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>].</p>    <p>Methods based on removing bias from training data usually fail to perform on par with the state-of-the-art (e.g. covariance-based models by Zafar et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>]). However, we argue that this happens not due to an inherent inability to treat datasets, but rather due to methodological deficiencies in previous approaches. In Subsection&#x00A0;<a class="sec" href="#sec-10">2.4</a> we discuss some of the most common deficiencies, such as preprocessing limitations, heuristic statistical models and inability to justify all types of fairness-aware edits. If those deficiencies are appropriately handled, we expect dataset editing methods to perform on the same level or even better than state-of-the-art, since they directly work on the source of bias instead of its outcome.</p>    <p>In this paper we propose an adaptive sensitive reweighting mechanism and a weight estimation model that do not suffer from these shortcomings. Our approach assumes that there exists an (unobservable) underlying set of class labels corresponding to training samples that, if predicted, would yield unbiased classification with respect to a fairness objective. It then searches for sample weights that make weighted training on the original dataset also train towards those labels, without explicitly observing them. To obtain those weights our approach employs a non-linear probability inference model, which we call <em>CULEP</em>, standing for Convex Underlying Label Error Perturbation. This model can be trained to convert classification error to a probability that the estimated labels approach the desired underlying labels. We then use it to infer training weights based on classifier outputs and iteratively retrain the classifier on these new weights.</p>    <p>This bias mitigation mechanism encapsulates both fairness- and classifier-related information and thus allows a more precise stochastic analysis. Furthermore, it avoids concerns that arise from label editing approaches, since training is still conducted on the original labels. Finally, in Subsection&#x00A0;<a class="sec" href="#sec-19">4.4</a> we explain that different CULEP parameters can help achieve different fairness goals, such as obtaining designated trade-offs between accuracy and various fairness metrics, such as those outlined in Subsection&#x00A0;<a class="sec" href="#sec-8">2.2</a>.</p>    <p>The novelty of our approach lies in the ability of CULEP to help an iterative reweighting process recognize sources of bias and diminish their impact without affecting features or labels. This way, the classification model is trained on <em>original</em> (possibly biased) dataset labels while still achieving designated fairness goals.<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> The CULEP model improves previous reweighting mechanisms with regards to estimating compliance between estimated and underlying labels and can be used to mitigate various types of unfairness.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Background</h2>     </div>    </header>    <p>Throughout this work, we consider binary classifiers that produce label estimations <span class="inline-equation"><span class="tex">$\hat{y_i}\in \lbrace 0,1\rbrace$</span>     </span> for samples <em>i</em> of features <em>x<sub>i</sub>     </em> and labels <em>y<sub>i</sub>     </em> &#x2208; {0, 1}. A certain group of samples <span class="inline-equation"><span class="tex">$\mathcal {S}$</span>     </span> is recognized as <em>sensitive</em> compared to its non-sensitive complement <span class="inline-equation"><span class="tex">$\mathcal {S}^{\prime }$</span>     </span> based on a sensitive real-world attribute, such as gender, race or financial status. Bias arises when a statistical property for the distribution of <span class="inline-equation"><span class="tex">$\lbrace \hat{y_i},i\in \mathcal {S}\rbrace$</span>     </span> is different for the distribution of <span class="inline-equation"><span class="tex">$\lbrace \hat{y_i},i\in \mathcal {S}^{\prime }\rbrace$</span>     </span>. Fairness-aware classification methods attempt to mitigate such differences.</p>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Types of Unfairness</h3>     </div>     </header>     <p>As outlined by Zafar et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>], classification unfairness is often expressed through the notions of disparate treatment, disparate impact and disparate mistreatment. Fairness objectives aim to eliminate these types of unfairness.</p>     <p>     <strong>Disparate treatment elimination</strong> reflects the ability of a trained classifier to yield the same outputs <span class="inline-equation"><span class="tex">$\hat{y_i}$</span>     </span> for features <em>x<sub>i</sub>     </em> regardless of whether the sample belongs to the sensitive group <span class="inline-equation"><span class="tex">$\mathcal {S}$</span>     </span> or not: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(\hat{y_i}|x_i,i\in \mathcal {S})=P(\hat{y_i}|x_i) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> Effectively, this fairness objective requires samples with similar features to be similarly classified. For example, if gender is a sensitive attribute for a classifier, males and females with otherwise similar features should be assigned to the same class under the principle of disparate treatment elimination.</p>     <p>A simple way to avoid disparate treatment is refraining from using information about the sensitive group for classification. This avoids discrimination or reverse discrimination [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>], but the accuracy cost can sometimes be too high [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>].</p>     <p>     <strong>Disparate impact elimination</strong> reflects the ability of a classifier to achieve statistical parity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>], i.e. assign the same portion of users to a class for sensitive and non-sensitive groups: <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(\hat{y_i}=1|i\in \mathcal {S})=P(\hat{y_i}=1|i\not\in \mathcal {S}) \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> For example, if financial status is a sensitive attribute for term deposit predictions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], disparate impact elimination would ensure that the portion of positive predictions is the same between low-income and high-income clients.</p>     <p>     <strong>Disparate mistreatment elimination</strong> reflects the ability of a classifier to achieve equal misclassification rates across <em>sound ground truth labels</em> (i.e. not suffering from dataset construction problems, such as historical biases) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>]. For example if race is a sensitive attribute for prediction of criminal behavior [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], disparate mistreatment elimination would ensure the same error rate between white and non-white defendants.</p>     <p>Recent works [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>] have shown that it is impossible to simultaneously satisfy all notions of disparate mistreatment elimination, unless the classifier is 100% accurate. More commonly adopted are the disparate mistreatment elimination constraints of equal false positive rates (FPR) and equal false negative rates (FNR): <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{align} P(\hat{y_i}\ne y_i|y_i=1,i\in \mathcal {S})&#x0026;=P(\hat{y_i}\ne y_i|y_i=1,i\not\in \mathcal {S})\end{align} </span>       <br/>       <span class="equation-number">(3a)</span>      </div>     </div>     <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{align} P(\hat{y_i}\ne y_i|y_i=0,i\in \mathcal {S})&#x0026;=P(\hat{y_i}\ne y_i|y_i=0,i\not\in \mathcal {S}) \end{align} </span>       <br/>       <span class="equation-number">(3b)</span>      </div>     </div>     </p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Metrics</h3>     </div>     </header>     <p>Following earlier fairness-aware approaches, in this work we measure classifier performance using accuracy, i.e. the proportion of correctly classified samples, disparate impact using the <span class="inline-equation"><span class="tex">$p\%$</span>     </span> rule and disparate mistreatment using the difference between sensitive and non-sensitive FPR and FNR.</p>     <p>The <span class="inline-equation"><span class="tex">$p\%$</span>     </span> rule [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] is an empirical rule which does not allow sensitive group identification to be lower than a set percentage of non-sensitive group identification: <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} pRule=\min \bigg \lbrace \frac{P\big (\hat{y_i}=1|i\in \mathcal {S}\big)}{P\big (\hat{y_i}=1|i\not\in \mathcal {S}\big)},\frac{P\big (\hat{y_i}=1|i\not\in \mathcal {S}\big)}{P\big (\hat{y_i}=1|i\in \mathcal {S}\big)}\bigg \rbrace \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> This metric is correlated to the Calders-Verwer measure [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] which calculates the disparity between those two percentages. Since these two measures share the same optimal point, we prefer reporting the <em>pRule</em>, for which there exists a set legal context. More specifically, the Uniform Guidelines on Employee Selection Procedures require at least 80% rule adherence [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>].</p>     <p>To measure disparate mistreatment, it is common to measure how deviation from set goals differs between the sensitive and non-sensitive group. In alignment with the common disparate mistreatment elimination conditions outlined in Eq.&#x00A0;3, we employ the following measures of disparate mistreatment: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{align} D_{FPR}&#x0026;=P(\hat{y_i}\ne y_i|y_i=1,i\in \mathcal {S})-P(\hat{y_i}\ne y_i|y_i=1,i\not\in \mathcal {S})\end{align} </span>       <br/>       <span class="equation-number">(5a)</span>      </div>     </div>     <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{align} D_{FNR}&#x0026;=P(\hat{y_i}\ne y_i|y_i=0,i\in \mathcal {S})-P(\hat{y_i}\ne y_i|y_i=0,i\not\in \mathcal {S}) \end{align} </span>       <br/>       <span class="equation-number">(5b)</span>      </div>     </div> To report the overall disparate mistreatment, we combine those two metrics into the quantity: <div class="table-responsive" id="Xeq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} |D_{FPR}|+|D_{FNR}|\end{equation} </span>       <br/>       <span class="equation-number">(5c)</span>      </div>     </div>     </p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Previous Work</h3>     </div>     </header>     <p>Works aiming to reduce classification unfairness can be categorized in the following approaches: a) preprocessing training data, b) training under fairness constraints, c) attempts to &#x2018;fix&#x2019; posteriors.</p>     <p>Approaches based on training data preprocessing aim to remove disparate impact from training data, under the assumption that the disparate impact of the trained classifier follows the disparate impact of training data. Such approaches include massaging the dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] by changing class labels that are identified as mislabeled due to bias and reweighting (usually heuristically) training samples so that more importance is placed on sensitive ones [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>]. Concerning massaging techniques, it must be noted that altering labels for training, even under bias concerns, can result in legal implications [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>].</p>     <p>Approaches training under fairness constraints select a disparate impact or mistreatment metric and attempt to properly adjust the training rules, either via editing the rules themselves [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>] (e.g. by inserting an appropriate regularization term towards fairness) or by introducing appropriate linear program constraints [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>] that reflect the desired optimization goals.</p>     <p>Finally, certain approaches attempt to edit posteriors in a way that satisfies fairness constraints [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>]. Such strategies are typically centered around some form of group-based thresholding. It must be noted that such systems require information about the sensitive group to make an appropriate decision. Although Hardt et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] argue that privacy concerns can be alleviated by remotely obtaining the different decision-making rules between sensitive and non-sensitive groups and locally applying the appropriate rule, such practices may still be inapplicable under certain legal settings, since they introduce disparate treatment.</p>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Discussion on Dataset Editing Deficiencies</h3>     </div>     </header>     <p>In this subsection we discuss three common shortcomings across previous dataset editing fairness-aware mechanisms.</p>     <p>     <strong>Limitations of preprocessing.</strong> Dataset editing approaches are commonly formulated by defining types of bias in the training data and then trying to statistically eliminate them. This process is indeed suitable for mitigating simple dataset-related biases, but fails to take into account more intricate sources of unfairness. For example, there may exist weaker feature correlations (e.g. through a chain of correlation of unobserved features, which may require external explanatory attributes to identify [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]) that cause bias against only a subset of the sensitive group. Furthermore, certain data could cause biases to only certain types of classifiers. For example, linear classifiers may have difficulty eliminating non-linear types of bias. Since biases emerge through systems of high complexity, which often tend to exhibit non-linear behavior, it is difficult to identify them through simple stochastic analysis and develop specific elimination strategies using only training data. Instead, it could be more informative to directly observe the effect of biases on the classifier and suitably perform adjustments while training. Following this line of thought, in this work, we propose an adaptive scheme, which iteratively adapts training data until stable behavior is achieved with respect to the classifier trained on this data.</p>     <p>     <strong>Heuristic statistical models.</strong> Another shortcoming of dataset editing approaches is the introduction of ad hoc assumptions on the nature of unfairness, the most prominent one being that classifier bias closely follows the bias of training data. Although there often exists a high degree of correlation between the two, other structural difficulties may cause inadequate bias elimination or introduction of inverse bias, even if statistically optimal methods are employed to remove dataset bias. As a result, statistical models often arrive at a minimum condition that guarantees correct but not necessarily full treatment of training bias. For example, it is common practice to assume that higher prediction errors of robust classifiers indicate mislabeled data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>], but we show (see Subsection&#x00A0;<a class="sec" href="#sec-17">4.2</a>) that this -otherwise intuitive- assumption discards cases where classification error relates differently to mislabeling for sensitive and non-sensitive groups. In Subsection&#x00A0;<a class="sec" href="#sec-18">4.3</a> we propose a statistical model that takes such differences into account and can be trained to satisfy various fairness objectives.</p>     <p>     <strong>Inability to justify disparate mistreatment elimination.</strong> Disparate mistreatment is an emerging fairness concern that is attributed to difficulties in reaching similar misclassification rates between groups rather than direct dataset biases. Since we cannot attribute such concerns to biased data, it is difficult to justify a disparate mistreatment elimination method which attempts to treat the dataset. Furthermore, since disparate mistreatment is not necessarily caused by disparate impact, constructing datasets unbiased with respect to disparate impact does not treat disparate mistreatment. In other words, the relation between dataset bias and disparate mistreatment remains unclear to date. This detriment is more apparent when trying to develop massaging approaches that edit dataset labels with a target other than disparate impact elimination; to the authors&#x2019; knowledge, there exists no clear (ethical or legal) justification to develop disparate mistreatment elimination procedures in a label editing process, as it cannot be attributed to any previously proposed source of dataset bias. For example, even in the well-formulated partial dataset repair mechanism of Feldman et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>], it becomes impossible to legally justify further label editing to remove disparate mistreatment, since dataset-related bias has already been treated. In this work, we try to bypass such limitations through a reweighting scheme, thus discovering sample weights that train towards desired objectives instead of editing training labels towards the same objectives.</p>    </section>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Adaptive Sensitive Reweighting</h2>     </div>    </header>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Training Objective</h3>     </div>     </header>     <p>Our analysis is conducted on a binary probabilistic classifier, which produces probability estimates <span class="inline-equation"><span class="tex">$\hat{P}(Y=y_i)=1-\hat{P}(Y\ne y_i)$</span>     </span> for samples <em>i</em> (with features <em>x<sub>i</sub>     </em>) and each class label <em>Y</em> &#x2208; {0, 1}. Such a classifier estimates class labels as: <div class="table-responsive" id="Xeq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \hat{y_i}=\mathop{argmax}_{Y\in \lbrace 0,1\rbrace }\hat{P}(Y= y_i)=\mathop{argmin}_{Y\in \lbrace 0,1\rbrace }\hat{P}(Y\ne y_i) \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> For ease of understanding, we prefer referring to the estimated label error <span class="inline-equation"><span class="tex">$\hat{P}(Y\ne y_i)$</span>     </span>, since for a well-calibrated classifier, <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne y_i)$</span>     </span> approaches the misclassification error <span class="inline-equation"><span class="tex">$P(\hat{y_i}\ne y_i)$</span>     </span>, which is usually the desired minimization target of the learning process. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186133/images/www2018-142-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Probabilistic classifier training.</span>      </div>     </figure>     </p>     <p>As previously described, this work assumes that unfairness is often caused by skewed group and label distributions in the dataset. However, the available ground truth may not always suffer from biases but yield disparate mistreatment due to other reasons, such as correlations between the sensitive group and certain attributes. To avoid confusion, we propose a common formulation for different fairness goals on a classifier. For training samples <em>i</em>, features <em>x<sub>i</sub>     </em> and class labels <em>y<sub>i</sub>     </em>, there exist underlying (i.e. unobservable) class labels <span class="inline-equation"><span class="tex">$\tilde{y_i}$</span>     </span> that yield estimated labels <span class="inline-equation"><span class="tex">$\hat{y_i}$</span>     </span> which conform to designated fairness and accuracy trade-offs.</p>     <p>In this setting, training goals are twofold: a) make the classifier yield accurate predictions, i.e. minimize <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne y_i)$</span>     </span> and b) make classifier predictions approach the underlying labels, i.e. minimize <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne \tilde{y_i})$</span>     </span>.<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> Obviously, there is difficulty in simultaneously training towards both of these objectives when data labels and underlying labels do not coincide. Training towards data labels could be achieved through the scheme demonstrated in Fig.&#x00A0;<a class="fig" href="#fig1">1</a> and training towards underlying labels could be achieved through the scheme demonstrated in Fig.&#x00A0;<a class="fig" href="#fig2">2</a>. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186133/images/www2018-142-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Directly training on observable desired labels. This can be ethically or legally questionable.</span>      </div>     </figure>     </p>     <p>Furthermore, estimating the underlying labels and directly using them for training can be argued to be an act of data falsification under certain legal settings. Therefore, not only should training be conducted on the original data labels, it is also desirable to fully abstain from any observation of underlying labels. In this respect, the scheme demonstrated in Fig.&#x00A0;<a class="fig" href="#fig2">2</a> is inadequate.</p>     <p>To solve these contradictions, we propose selecting weights <em>w<sub>i</sub>     </em> for training samples <em>i</em> that make weighted training on data labels equivalent to unweighted training on underlying labels. This way, we can focus on estimating weights <em>w<sub>i</sub>     </em> that help achieve designated fairness objectives without observing underlying labels. In other words, we try to minimize both weighted error on observed labels as well as the distance between weighted observed labels and unweighted underlying labels: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;\min \sum _i w_i \hat{P}(\hat{y_i}\ne y_i)\\ &#x0026;\min \sum _i \bigg (w_i \hat{P}(\hat{y_i}\ne y_i)-\hat{P}(\hat{y_i}\ne \tilde{y_i})\bigg)^2\end{align*} </span>       <br/>      </div>     </div>     </p>     <p>For simplification purposes, in this paper we set the second minimization goal to 0 and attempt to analytically derive the weights <em>w<sub>i</sub>     </em> rather than tuning towards them with a gradient-based method, as per Eq.&#x00A0;7. In future work, training towards minimizing differences between underlying and weighted estimation could be conducted in place of analytical calculation, so as to make convergence more robust against noise. <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026;\min \sum _i w_i \hat{P}(\hat{y_i}\ne y_i)\end{align} </span>       <br/>       <span class="equation-number">(7a)</span>      </div>     </div>     <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026;w_i \hat{P}(\hat{y_i}\ne y_i)=\hat{P}(\hat{y_i}\ne \tilde{y_i})\,\forall i \end{align} </span>       <br/>       <span class="equation-number">(7b)</span>      </div>     </div>     </p>     <p>Contrary to previous works on treating dataset bias, we assume that <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne \tilde{y_i})$</span>     </span> cannot be estimated through simple sample- or group-specific dependencies. Instead, in Section&#x00A0;<a class="sec" href="#sec-15">4</a> we propose a model which, in addition to knowledge of whether samples <em>i</em> belong to the sensitive group, employs conditional probabilities to make more informed estimations based on <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne y_i)$</span>     </span>. Such a model allows the classifier to satisfy all previously outlined goals, namely estimating the underlying labels while training on an appropriately weighted original training label objective. This is more clearly demonstrated in Fig.&#x00A0;<a class="fig" href="#fig3">3</a>. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186133/images/www2018-142-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Training on unobservable desired labels.</span>      </div>     </figure>     </p>     <p>Such a process shifts the focus of the training scheme to discovering a probability estimation model <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne \tilde{y_i})$</span>     </span> that can train towards the desired goals rather than searching for the underlying labels themselves. This is a substantial improvement compared to heuristically defining label editing procedures.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Why Unobserved Underlying Labels?</h3>     </div>     </header>     <p>Essentially, the training objectives in Eq.&#x00A0;7 are equivalent to training the classifier on underlying labels. However, if we adequately estimate <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne \tilde{y_i})$</span>     </span> using only <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne y_i)$</span>     </span>, we can train for those labels using <em>only</em> the original training labels <em>y<sub>i</sub>     </em>. Previous works (e.g. the original massaging approach of Calders et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]) try to infer and directly utilize underlying labels. However, we refrain from doing so, as not directly observing those labels yields three significant advantages.</p>     <p>Firstly, the classifier cannot be accused of being trained on falsified data. This practice can be ethically or legally questionable, however well-intended the &#x2018;falsification&#x2019; as a form of label editing is. Instead, the classifier is trained over the labels <em>y<sub>i</sub>     </em> for weights that help it achieve its target objective, which is a widely accepted process in machine learning.</p>     <p>Secondly, we can select models for the estimation <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne \tilde{y_i})$</span>     </span> that allow training towards objectives that could not be formulated as deficiencies in training data. In fact, training objectives include unbiased underlying label discovery instead of training data label discovery. Since discovered underlying labels are not directly utilized in training, we can then select probability estimation models that train towards objectives other than simple disparate impact, such as disparate mistreatment or fairness and accuracy trade-offs.</p>     <p>Thirdly, there is no need to introduce massaging-like heuristics about how relabeling should be distributed across classes and/or groups. This way, the underlying label <em>distribution</em> becomes more important than identifying which sample labels are biased. This property is important, as the relation between data and certain notions of unfairness is not yet clear, but there exist clear definitions on whether a label distribution adheres to a notion of fairness.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Training Algorithm</h3>     </div>     </header>     <p>To simultaneously adjust training weights alongside classifier training using Eq.&#x00A0;7, we adopt a classifier-agnostic iterative approach, in which we first fully train a classifier based on uniform weights and then appropriately readjust those weights, repeating these steps until convergence. This process is specified in Algorithm&#x00A0;1.</p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186133/images/www2018-142-img1.svg" class="img-responsive" alt="" longdesc=""/>     </p>     <p>As per our previous formulation, we directly set new weight estimations rather than partially editing existing ones. We do so under the assumption that the adaptation model fails to converge only if the underlying probability estimation model does not adequately model the desired underlying labels. Otherwise, as long as the estimator model is convex, locality is preserved and thus the updating process should eventually converge to points or tracks of optimal weights. We experimentally assert this behavior in Subsection&#x00A0;<a class="sec" href="#sec-27">6.1</a>.</p>     <p>Recent works [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>] have occasionally proposed similar iterative methods as baselines to compare themselves to. However, our work differs in that it employs an inferred rather than a heuristic model to produce bias-related probabilities (see Section&#x00A0;<a class="sec" href="#sec-15">4</a>).</p>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Underlying Label Error Estimation</h2>     </div>    </header>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Motivation</h3>     </div>     </header>     <p>In the previous section&#x0027;s methodology, it is important to accurately model the error <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne \tilde{y_i})$</span>     </span> of classified labels <span class="inline-equation"><span class="tex">$\hat{y_i}$</span>     </span> deviating from underlying labels <span class="inline-equation"><span class="tex">$\tilde{y_i}$</span>     </span> as a function of the classifier error <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne y_i)$</span>     </span> for original training labels <em>y<sub>i</sub>     </em>.</p>     <p>To do so, we propose a model that performs convex perturbations of classifier error to parameterize the deviation between original and underlying labels for sensitive and non-sensitive group samples. This model can then be used to estimate weights that help achieve various fairness objectives. In this section we explain why this process is superior to simpler error-based weighting (e.g. boosting) and why it can be fine-tuned towards the more common fairness objectives.</p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Weighting by Error is Inadequate</h3>     </div>     </header>     <p>Previous attempts on sensitive reweighting (e.g. baselines employed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>]) propose that weights in Eq.&#x00A0;<a class="eqn" href="#eq5">7a</a> should be proportional to classifier error. However, by solving Eq.&#x00A0;<a class="eqn" href="#eq6">7b</a> this leads to: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} w_i\approx \hat{P}(\hat{y_i}\ne y_i)\Leftrightarrow \hat{P}(\hat{y_i}\ne \tilde{y_i})\approx \hat{P}^2(\hat{y_i}\ne y_i)\end{align*} </span>       <br/>      </div>     </div>     </p>     <p>Furthermore, this assumption ignores differences in classifier error stemming from matching vs. non-matching dataset and underlying labels. The Bayes rule yields: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} &#x0026;\hat{P}(\hat{y_i}\ne \tilde{y_i}) \\ &#x0026;\quad =\hat{P}(\hat{y_i}\ne y_i|y_i=\tilde{y_i})\hat{P}(y_i=\tilde{y_i}) \\ &#x0026;\quad +\hat{P}(\hat{y_i}\ne y_i|y_i\ne \tilde{y_i})\hat{P}(y_i\ne \tilde{y_i}) \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> Therefore, since <span class="inline-equation"><span class="tex">$\hat{P}(y_i=\tilde{y_i})+\hat{P}(y_i\ne \tilde{y_i})=1$</span>     </span>, the proposed condition can always hold true only if: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \hat{P}(\hat{y_i}\ne y_i|y_i=\tilde{y_i})\approx \hat{P}(\hat{y_i}\ne y_i|y_i\ne \tilde{y_i})\approx \hat{P}(\hat{y_i}\ne y_i)\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>However, the above conditions are impossible to uphold for every dataset, since any sequence of datasets on which misclassification progressively becomes independent of desired underlying labels (e.g. progressively becomes unbiased) converges to the contradictory <span class="inline-equation"><span class="tex">$\hat{P}(\hat{y_i}\ne y_i|y_i=\tilde{y_i})\approx \hat{P}(\hat{y_i}\ne y_i|y_i\ne \tilde{y_i})\approx \hat{P}(\hat{y_i}\ne y_i)\ne \hat{P}^2(\hat{y_i}\ne y_i)$</span>     </span>. In other words, the previously proposed heuristic cannot always be met with success in removing bias.</p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Convex Underlying Label Error Perturbation (CULEP) Model</h3>     </div>     </header>     <p>In this work, we recognize that conditional classifier error can be different when underlying labels coincide with original labels compared to when they do not; if classifier error would be overestimated compared to its estimation under the condition that original and underlying labels coincide, then it would be underestimated under the condition that they do not coincide and conversely. In other words: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \big (\hat{P}(\hat{y_i}\ne y_i|y_i=\tilde{y_i})-\hat{P}(\hat{y_i}=y_i)\big)\big (\hat{P}(\hat{y_i}\ne y_i|y_i\ne \tilde{y_i})-\hat{P}(\hat{y_i}\ne y_i)\big){\lt}0\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>To satisfy this property, we propose estimating those conditional probabilities by perturbing classifier error of training samples <em>i</em>. To do so, we multiply it with values obtained through a non-decreasing convex function <span class="inline-equation"><span class="tex">$L_{\beta _i}\big (p_i)\ge 0,\,L_{\beta _i}\big (0)=1$</span>     </span> of perturbation parameters <em>p<sub>i</sub>     </em> &#x2208; [ &#x2212; 1, 1], whose Lipschitz constant is proportional to <em>&#x03B2;<sub>i</sub>     </em>.<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>     </p>     <p>Without loss of generality, we model perturbation parameters as <span class="inline-equation"><span class="tex">$|p_i|=\hat{P}(\hat{y_i}\ne y_i)$</span>     </span>, where their signs depend on whether conditional probabilities are overestimated or underestimated. Since probability spaces are continuous, whether to overestimate or underestimate original and underlying label coincidence during perturbations should be maintained throughout training samples. Adopting the &#x00B1;, &#x2213; notation<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> the above can be written as: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \hat{P}(\hat{y_i}\ne y_i|y_i=\tilde{y_i})&#x0026;=L_{\beta _i}\big (\pm \hat{P}(\hat{y_i}\ne y_i)\big)\hat{P}(\hat{y_i}\ne y_i)\\\hat{P}(\hat{y_i}\ne y_i|y_i\ne \tilde{y_i})&#x0026;=L_{\beta _i}\big (\mp \hat{P}(\hat{y_i}\ne y_i)\big)\hat{P}(\hat{y_i}\ne y_i)\end{align*} </span>       <br/>      </div>     </div>     </p>     <p>The sensitive group <span class="inline-equation"><span class="tex">$\mathcal {S}$</span>     </span> and the non-sensitive group <span class="inline-equation"><span class="tex">$\mathcal {S}^{\prime }$</span>     </span> can adhere to different misclassification biases, which skew error in different ways (e.g. the coefficient of variation for classifier error for females mislabeled as males may be different for males mislabeled as females). Hence, we select different Lipschitz constants between those groups to produce different perturbations: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \beta _i=\left\lbrace \begin{array}{lr}\beta _{\mathcal {S}} &#x0026; \text{if }i\in \mathcal {S}\\\beta _{\mathcal {S}^{\prime }} &#x0026; \text{if }i\not\in \mathcal {S}\end{array}\right\rbrace\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>Finally, conditional probability estimations require the probability of biased or inadequate labeling. These probabilities may differ between the sensitive and non-sensitive groups (e.g. dataset construction may have been impartial between males and only biased against females) and can be affected by many unknown social- and dataset-related parameters. However, as long as these parameters remain approximately constant during dataset creation (e.g. because all data were gathered from the same regions during the same time period), their cumulative effect also remains approximately constant. Hence, data mislabeling would occur with a fixed probability, depending on whether samples belong to the sensitive group and can be modeled as two Bernoulli processes, one for sensitive group samples with mean value <span class="inline-equation"><span class="tex">$q_\mathcal {S}$</span>     </span> and another for non-sensitive group samples with mean value <span class="inline-equation"><span class="tex">$q_{\mathcal {S}^{\prime }}$</span>     </span>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \hat{P}(y_i\ne \tilde{y_i})=q_i=\left\lbrace \begin{array}{lr}q_{\mathcal {S}} &#x0026; \text{if }i\in \mathcal {S}\\q_{\mathcal {S}^{\prime }} &#x0026; \text{if }i\not\in \mathcal {S}\end{array}\right\rbrace\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>Substituting the above in Eq.&#x00A0;<a class="eqn" href="#eq6">7b</a> we obtain: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026;w_i\hat{P}(\hat{y_i}\ne y_i)=\hat{P}(\hat{y_i}\ne \tilde{y_i}) \\ &#x0026;\quad \Leftrightarrow w_i\hat{P}(\hat{y_i}\ne y_i) \\ &#x0026;\quad \quad \quad \quad =L_{\beta _i}\big (\pm \hat{P}(\hat{y_i}\ne y_i)\big)\hat{P}(\hat{y_i}\ne y_i)q_i \\ &#x0026;\quad \quad \quad \quad +L_{\beta _i}\big (\mp \hat{P}(\hat{y_i}\ne y_i)\big)\hat{P}(\hat{y_i}\ne y_i)(1-q_i)\end{align*} </span>       <br/>      </div>     </div> This Convex Underlying Label Error Perturbation (CULEP) model obtained through the previous propositions can be rewritten as: <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \boxed{ \begin{aligned} &#x0026;w_i=\alpha _i L_{\beta _i}\big (\hat{P}(\hat{y_i}\ne y_i)\big)+(1-\alpha _i)L_{\beta _i}\big (-\hat{P}(\hat{y_i}\ne y_i)\big)\\ &#x0026;\beta _i=\left\lbrace \begin{array}{lr}\beta _{\mathcal {S}} &#x0026; \text{if }i\in \mathcal {S}\\\beta _{\mathcal {S}^{\prime }} &#x0026; \text{if }i\not\in \mathcal {S}\end{array}\right\rbrace \ge 0 \quad \alpha _i=\left\lbrace \begin{array}{lr}\alpha _{\mathcal {S}} &#x0026; \text{if }i\in \mathcal {S}\\\alpha _{\mathcal {S}^{\prime }} &#x0026; \text{if }i\not\in \mathcal {S}\end{array}\right\rbrace \in [0,1] \end{aligned} } \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> For each selection of (<em>q<sub>i</sub>     </em>, &#x00B1;), parameters <em>&#x03B1;<sub>i</sub>     </em> can be calculated as <em>&#x03B1;<sub>i</sub>     </em> = <em>q<sub>i</sub>     </em> or <em>&#x03B1;<sub>i</sub>     </em> = 1 &#x2212; <em>q<sub>i</sub>     </em> depending on the sign of &#x00B1; . Therefore, when tuning Eq.&#x00A0;<a class="eqn" href="#eq8">9</a>, it suffices to search only for values of <em>&#x03B1;<sub>i</sub>     </em> instead of both the values of <em>q<sub>i</sub>     </em> and the sign of &#x00B1; .</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Achieving Fairness with the CULEP Model</h3>     </div>     </header>     <p>In this subsection, we discuss how the CULEP model allows us to select parameters in Eq.&#x00A0;<a class="eqn" href="#eq8">9</a> such that we can train towards accuracy, disparate impact elimination and disparate mistreatment elimination objectives. As a result, it is possible to tune those parameters (see Subsection&#x00A0;<a class="sec" href="#sec-24">5.4</a>) to satisfy various such objectives or trade-offs between them.</p>     <p>     <strong>Accuracy objectives.</strong> Training towards maximal accuracy of the classification model is achieved when all training weights are equal, i.e. <span class="inline-equation"><span class="tex">$w_i=1\,\forall i\Leftrightarrow \beta _\mathcal {S}=\beta _{\mathcal {S}^{\prime }}=0$</span>     </span>.</p>     <p>     <strong>Disparate mistreatment objectives.</strong> As <em>&#x03B1;<sub>i</sub>     </em> &#x2192; 1 we obtain <span class="inline-equation"><span class="tex">$w_i\rightarrow L_{\beta _i}\big (\hat{P}(\hat{y_i}\ne y_i)\big)$</span>     </span> and hence place higher importance on misclassified samples. Whereas as <em>&#x03B1;<sub>i</sub>     </em> &#x2192; 0 we obtain <span class="inline-equation"><span class="tex">$w_i\rightarrow L_{\beta _i}\big (-\hat{P}(\hat{y_i}\ne y_i)\big)$</span>     </span> and hence place higher importance on correctly classified samples. Therefore, <em>&#x03B1;<sub>i</sub>     </em> &#x2208; [0, 1] interpolate between the importance of correct vs. incorrect classification for each sample. As <em>&#x03B2;<sub>i</sub>     </em> &#x2192; &#x221E;, these trade-offs dominate classifier training pertaining to respective samples.</p>     <p>Based on these observations, we recognize two cases of disparate mistreatment with respect to the signs of <em>D<sub>FPR</sub>     </em> and <em>D<sub>FNR</sub>     </em>:</p>     <p>     <strong>a)</strong>      <strong>      <em>D<sub>FPR</sub>D<sub>FNR</sub>      </em> > 0</strong>, i.e. false positives and false negatives are either both overestimated or both underestimated for the sensitive group. In this case, as <span class="inline-equation"><span class="tex">$(\alpha _\mathcal {S},\alpha _{\mathcal {S}^{\prime }})\rightarrow (0,1)$</span>     </span> more importance is placed on sensitive compared to non-sensitive group sample misclassification. The opposite happens as <span class="inline-equation"><span class="tex">$(\alpha _\mathcal {S},\alpha _{\mathcal {S}^{\prime }})\rightarrow (1,0)$</span>     </span>. This means that |<em>D<sub>FPR</sub>     </em>| and |<em>D<sub>FNR</sub>     </em>| are reduced as values of <em>&#x03B1;<sub>i</sub>     </em> move towards one of those two antipodal points. Large enough <span class="inline-equation"><span class="tex">$\beta _\mathcal {S}$</span>     </span> and/or <span class="inline-equation"><span class="tex">$\beta _{\mathcal {S}^{\prime }}$</span>     </span> can magnify this effect in a way that minimizes either of those metrics or trade-offs between them.</p>     <p>     <strong>b)</strong>      <strong>      <em>D<sub>FPR</sub>D<sub>FNR</sub>      </em> < 0</strong>, i.e. false positives and false negatives are not overestimated or underestimated simultaneously for the sensitive group. In this case, we obtain opposite increments to <em>D<sub>FPR</sub>     </em> and <em>D<sub>FNR</sub>     </em> as either <span class="inline-equation"><span class="tex">$(\alpha _\mathcal {S},\alpha _{\mathcal {S}^{\prime }})\rightarrow (0,0)$</span>     </span> or <span class="inline-equation"><span class="tex">$(\alpha _\mathcal {S},\alpha _{\mathcal {S}^{\prime }})\rightarrow (1,1)$</span>     </span>. Similarly to before, for large enough <span class="inline-equation"><span class="tex">$\beta _\mathcal {S}$</span>     </span> and/or <span class="inline-equation"><span class="tex">$\beta _{\mathcal {S}^{\prime }}$</span>     </span>, |<em>D<sub>FPR</sub>     </em>| and |<em>D<sub>FNR</sub>     </em>| or a trade-off between them can be minimized as <em>a<sub>i</sub>     </em> move towards one of those two antipodal points.</p>     <p>     <strong>Disparate impact objectives.</strong> Positive discovery is more sensitive either towards higher or lower misclassification weights for each group. Hence, there exist parameters <span class="inline-equation"><span class="tex">$\alpha _\mathcal {S},\alpha _{\mathcal {S}^{\prime }}$</span>     </span> that either increase or decrease positive discoveries. Therefore, there also exist large enough <span class="inline-equation"><span class="tex">$\beta _\mathcal {S},\beta _{\mathcal {S}^{\prime }}$</span>     </span> that maximize the <em>pRule</em>.</p>     <p>Summarizing the above, we can see that the CULEP model introduces four degrees of freedom (one for each of its parameters), with regards to positive or negative importance of misclassification rates and the degree of this importance for sensitive and non-sensitive groups. Therefore, those parameters are able to place different importances on accuracy and mitigation of sensitive and non-sensitive group differences on quantities correlated with misclassification (e.g. disparate mistreatment metrics) or discovery (e.g. disparate impact metrics).</p>    </section>   </section>   <section id="sec-20">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments Setup</h2>     </div>    </header>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Datasets</h3>     </div>     </header>     <p>To assert the validity of our approach, we experiment with two synthetic datasets suffering from disparate mistreatment previously proposed by Zafar et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>], as well as with three well-known real world datasets: the <em>Adult</em> income dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], the <em>Bank</em> marketing dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] from the UCI repository [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>] and the ProPublica <em>COMPAS</em> dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] of criminal recidivism.</p>     <p>The two synthetic datasets suffering from disparate mistreatment comprise 10,000 samples with 2 features, a binary sensitive label and a binary classification label. Their features are obtained through bivariate normal distributions, chosen so that their sensitive labels yield <em>D<sub>FPR</sub>D<sub>FNR</sub>     </em> < 0 and opposite-sign <em>D<sub>FPR</sub>D<sub>FNR</sub>     </em> > 0, respectively, for a logistic regression classifier. This way, we can explore the ability of our approach to handle the two different cases of disparate mistreatment recognized in Subsection&#x00A0;<a class="sec" href="#sec-19">4.4</a>. The synthetic dataset with opposite signs of disparate mistreatment between FPR and FNR, which we call <em>SynthOpp</em>, is constructed by sampling the following distributions 2,500 times each: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} x_{i,y_i=1,i\not\in \mathcal {S}}&#x0026;\sim N([2,0], [5,1;1,5])\\x_{i,y_i=1,i\in \mathcal {S}}&#x0026;\sim N([2,3], [5,1;1,5])\\x_{i,y_i=0,i\not\in \mathcal {S}}&#x0026;\sim N([-1,-3], [5,1;1,5])\\x_{i,y_i=0,i\in \mathcal {S}}&#x0026;\sim N([-1,0], [5,1;1,5])\end{align*} </span>       <br/>      </div>     </div> The synthetic dataset with same signs of disparate mistreatment between FPR and FNR, which we call <em>SynthSame</em>, is constructed by sampling the following distributions, 2,500 times each: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} x_{i,y_i=1,i\not\in \mathcal {S}}&#x0026;\sim N([1,2], [5,2;2,5])\\x_{i,y_i=1,i\in \mathcal {S}}&#x0026;\sim N([2,0], [10,1;1,4])\\x_{i,y_i=0,i\not\in \mathcal {S}}&#x0026;\sim N([0,-1], [7,1;1,7])\\x_{i,y_i=0,i\in \mathcal {S}}&#x0026;\sim N([-5,0], [5,1;1,5])\end{align*} </span>       <br/>      </div>     </div>     </p>     <p>The Adult dataset comprises 48,842 test samples with 14 features and a binary label indicating whether income is above 50K. For this dataset, we consider gender as the sensitive feature.</p>     <p>The Bank dataset comprises 41,188 samples with 20 features and a binary label, indicating whether a client has subscribed to a term deposit. For this dataset, ages less than 25 and more than 60 years are considered sensitive.</p>     <p>We select a subset of the COMPAS dataset previously used for fairness experiments [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>], which comprises 6,150 samples with five features (age category, gender, race, priors count and charge degree) and a binary label indicating whether the defendant reoffended within two years. The race is considered as the sensitive attribute and, to make it binary, we follow earlier approaches in selecting only Black and White individuals. It must be noted that the selected features aim to facilitate fairness experiments comparable to previous approaches rather than high predictive accuracy.</p>    </section>    <section id="sec-22">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Fairness Objectives</h3>     </div>     </header>     <p>Fairness-aware classifiers are usually able to train towards mitigating various fairness metrics. At the same time, they need to preserve the accuracy (<em>acc</em>) of the base classification model as much as possible. Otherwise, it could be possible for their outputs to be misleading.</p>     <p>When a classifier targets multiple objectives [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>], it can employ either linear scalarization, where a linear trade-off is set between the objectives, or &#x03F5;-constraints, which bound individual objectives. Since there usually exist legal bounds for disparate impact (e.g. the 80% rule) but not for mistreatment, it is easier to formulate disparate impact as an &#x03F5;-constraint and disparate mistreatment as linear scalarization. However, Miglierina et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>] theoretically show the duality between those two types of objectives. Furthermore, it is easier to tune the parameters of Eq.&#x00A0;<a class="eqn" href="#eq8">9</a> in a linear than in a constrained space. Therefore, we opt for relaxing training bounds by setting linear scalarization goals for all fairness objectives.</p>     <p>In particular, the Adult and Bank datasets are commonly considered to suffer from disparate impact and thus we train the CULEP model towards eliminating disparate treatment while preserving accuracy: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \max (acc+pRule)\end{equation*} </span>       <br/>      </div>     </div> On the other hand, the COMPAS and synthetic datasets are considered to contain sound ground truth and thus we place more emphasis on overall disparate mistreatment elimination, as previously discussed in Subsection&#x00A0;<a class="sec" href="#sec-8">2.2</a>. For our experiments, we consider accuracy to be equally important to each fairness constraint: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} \max (2acc-|D_{FPR}|-|D_{FNR}|)\end{equation*} </span>       <br/>      </div>     </div>     </p>    </section>    <section id="sec-23">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Validation</h3>     </div>     </header>     <p>For Adult and Bank dataset experiments we perform a 70: 30 random split and for the COMPAS and synthetic dataset experiments we perform a 50: 50 random data split to obtain training and test data. These splits are used by previous works exploring those datasets and thus allow our results to be comparable across approaches. In both cases, we use the training set to tune the CULEP model on Algorithm&#x00A0;1 and then train the base classifier on the training set. We use the evaluation set only to calculate the accuracy and disparate impact and mistreatment elimination of the resulting classifier. For robustness, we again follow the validation methodology of previous approaches, which repeat this process 5 times and report the average measures across experiments.</p>     <p>We employ <em>logistic regression</em> without regularization as our base classifier of choice. To speed up training time we normalize numeric attributes in real-world datasets by dividing with their mean value. We encode nominal attributes using a one-hot scheme to convert them to binary arrays. Finally, CULEP conditional probabilities are modeled as Gaussian processes, which is a popular generic model in stochastic analysis, as it often arises in all sorts of physical and theoretical systems: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} L_{\beta _i}(p)=\exp (\beta _i p)\end{equation*} </span>       <br/>      </div>     </div>     </p>    </section>    <section id="sec-24">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Training the CULEP Model</h3>     </div>     </header>     <p>The CULEP model outlined in Eq.&#x00A0;<a class="eqn" href="#eq8">9</a> is non-linear parametric model and thus tuning needs to be accurate. Furthermore, Algorithm&#x00A0;1, can exhibit non-smooth behavior, since a different number of adjustments can arise from different parameter selections. Hence, to optimize CULEP parameters, we employ the DIvided RECTangles (DIRECT) method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>], which is guaranteed to yield globally optimal solutions in Lipschitz-continuous objective spaces. Since <span class="inline-equation"><span class="tex">$a_\mathcal {S},a_{\mathcal {S}^{\prime }}$</span>     </span> lie in [0, 1] as either probabilities or complements of probabilities, <span class="inline-equation"><span class="tex">$b_\mathcal {S},b_{\mathcal {S}^{\prime }}$</span>     </span> are non-negative constants and exp&#x2009;(<em>&#x03B2;p</em>) quickly converges to higher variation coefficients for larger <em>&#x03B2;</em>, we search for optimal parameters in the space <span class="inline-equation"><span class="tex">$(a_\mathcal {S},a_{\mathcal {S}^{\prime }},b_\mathcal {S},b_{\mathcal {S}^{\prime }})\in [0,1]^2\times [0,3]^2$</span>     </span>. Each combination of parameters is evaluated with a full run of Algorithm&#x00A0;1 on the training set.</p>    </section>    <section id="sec-25">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.5</span> Compared Methods</h3>     </div>     </header>     <p>Zafar et al. previously tested various fairness-aware approaches for disparate impact elimination on the Adult dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>] and disparate mistreatment elimination on the COMPAS dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]. We compare our method with those they report to yield superior results to the rest. These &#x2018;best&#x2019; methods also happen to use logistic regression. The methods compared in our experiments are:</p>     <ul class="list-no-style">     <li id="list1" label="&#x2022;"><strong>ASR+CULEP</strong>. Adaptive Sensitive Reweighting using the CULEP model as described throughout this paper, which can be used to mitigate disparate impact and mistreatment. Our implementation is available online.<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>      <br/></li>     <li id="list2" label="&#x2022;"><strong>Covariance</strong>. Models proposed by Zafar et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0044">44</a>] employing covariance to approximate linear program constraints that mitigate disparate impact [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0041">41</a>] and mistreatment [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0044">44</a>].<br/></li>     <li id="list3" label="&#x2022;"><strong>Group Thresholding</strong>. Model proposed by Hardt et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>] for disparate mistreatment elimination.<br/></li>     <li id="list4" label="&#x2022;"><strong>Regularizer</strong>. Approach proposed by Kamishima et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0028">28</a>] to remove prejudice-related disparate impact. It suffers from disparate treatment, since it takes into account whether samples are sensitive during classification.<br/></li>     </ul>    </section>   </section>   <section id="sec-26">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Results</h2>     </div>    </header>    <section id="sec-27">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Exploring Convergence</h3>     </div>     </header>     <p>In this subsection, we explore the convergence of Algorithm&#x00A0;1 towards optimal weights and the impact on the objective function. This way, we can get a general idea about convergence speed, as well as the effect of multiple iterations in our scheme.</p>     <p>To explore convergence after selecting CULEP model parameters, we measure the objective functions formulated in Subsection&#x00A0;<a class="sec" href="#sec-22">5.2</a> for each dataset on training data. We also measure the root mean square weight edits on each iteration of Algorithm&#x00A0;1:</p>     <p>     <span class="inline-equation"><span class="tex">$\sqrt {\tfrac{1}{N}\sum _i (w_i-w_{i,prev})^2}$</span>     </span>     </p>     <p>where <em>N</em> is the number of training samples.</p>     <p>As root mean square edits approach zero, Algorithm&#x00A0;1 approaches (locally) optimal weights. On the other hand, if root mean square edits approach a fixed constant, the adaptive scheme alternates between similar weights in a locally unstable way which is close to a global optimum. Hence, we can consider that weights converge to a stable state as long as weight edits approach a fixed value.</p>     <p>In Fig.&#x00A0;<a class="fig" href="#fig4">4</a> we can see that weight edits converge in very few iterations for the studied datasets. However, they do not stabilize immediately but need a small number of repetitions to converge to a fixed value. Furthermore, we can see that, after weights converge, the objective functions yield substantial improvements compared to the first iteration. These findings align with our hypothesis that single-step methodologies are insufficient to fully discover appropriate weights and that more iterations should be performed. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186133/images/www2018-142-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Objective and weight editing across datasets for each iteration in Algorithm&#x00A0;1 using the trained CULEP model to re-estimate underlying label error (disparate treatment was avoided for the synthetic datasets).</span>      </div>     </figure>     </p>     <p>ASR retrains the base classifier only a few times (&#x223C; 5) before converging. Furthermore, DIRECT training achieves precise estimation up to the fifth decimal point (which empirically suffices) for the four CULEP parameters within at most &#x2308;log&#x2009;<sub>2</sub>(3/0.000005)&#x2309;2<sup>4</sup> = 320 evaluations of ASR. Therefore, ASR+CULEP trains the base classifier at most 320 &#x00B7; 5 = 1, 600 times. Although this computational cost could be prohibitive for more complex base classifiers, it scales linearly without further approximations and hence is suited to simpler classifiers, such as logistic regression.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Disparate mistreatment elimination for logistic regression on both |<em>D<sub>FPR</sub>       </em>| and |<em>D<sub>FNR</sub>       </em>| constraints for disparate treatment (i.e. the sensitive group is a feature) and avoiding disparate treatment.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;"/>        <th colspan="9" style="text-align:center;">Disparate Treatment<hr/>        </th>        <th colspan="6" style="text-align:center;">Avoiding Disparate Treatment<hr/>        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th colspan="3" style="text-align:center;">COMPAS<hr/>        </th>        <th colspan="3" style="text-align:center;">SynthOpp<hr/>        </th>        <th colspan="3" style="text-align:center;">SynthSame<hr/>        </th>        <th colspan="3" style="text-align:center;">SynthOpp<hr/>        </th>        <th colspan="3" style="text-align:center;">SynthSame<hr/>        </th>       </tr>       <tr>        <th style="text-align:left;">Fairness Approach</th>        <th style="text-align:right;">acc</th>        <th style="text-align:right;">        <em>D<sub>FPR</sub>        </em>        </th>        <th style="text-align:right;">        <em>D<sub>FNR</sub>        </em>        </th>        <th style="text-align:right;">acc</th>        <th>        <em>D<sub>FPR</sub>        </em>        </th>        <th>        <em>D<sub>FNR</sub>        </em>        </th>        <th>acc</th>        <th>        <em>D<sub>FPR</sub>        </em>        </th>        <th>        <em>D<sub>FNR</sub>        </em>        </th>        <th>acc</th>        <th>        <em>D<sub>FPR</sub>        </em>        </th>        <th>        <em>D<sub>FNR</sub>        </em>        </th>        <th>acc</th>        <th>        <em>D<sub>FPR</sub>        </em>        </th>        <th>        <em>D<sub>FNR</sub>        </em>        </th>       </tr>      </thead>      <tbody>       <tr style="border-top: solid 2px">        <td style="text-align:left;">None</td>        <td style="text-align:right;">66%</td>        <td style="text-align:right;">17%</td>        <td style="text-align:right;">-25%</td>        <td style="text-align:right;">78%</td>        <td>-16%</td>        <td>19%</td>        <td>80%</td>        <td>25%</td>        <td>14%</td>        <td>78%</td>        <td>-16%</td>        <td>19%</td>        <td>80%</td>        <td>25%</td>        <td>14%</td>       </tr>       <tr>        <td style="text-align:left;">ASR+CULEP 2<em>acc</em> &#x2212; |<em>D<sub>FPR</sub>        </em>| &#x2212; |<em>D<sub>FNR</sub>        </em>|</td>        <td style="text-align:right;">65%</td>        <td style="text-align:right;">-1%</td>        <td style="text-align:right;">-1%</td>        <td style="text-align:right;">81%</td>        <td>0%</td>        <td>0%</td>        <td>77%</td>        <td>0%</td>        <td>-16%</td>        <td>77%</td>        <td>0%</td>        <td>-1%</td>        <td>75%</td>        <td>0%</td>        <td>-13%</td>       </tr>       <tr>        <td style="text-align:left;">Covariance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0041">41</a>]</td>        <td style="text-align:right;">66%</td>        <td style="text-align:right;">3%</td>        <td style="text-align:right;">-11%</td>        <td style="text-align:right;">80%</td>        <td>1%</td>        <td>2%</td>        <td>77%</td>        <td>14%</td>        <td>6%</td>        <td>75%</td>        <td>-1%</td>        <td>1%</td>        <td>69%</td>        <td>-1%</td>        <td>6%</td>       </tr>       <tr>        <td style="text-align:left;">Group Thresholding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0020">20</a>]</td>        <td style="text-align:right;">65%</td>        <td style="text-align:right;">-1%</td>        <td style="text-align:right;">-1%</td>        <td style="text-align:right;">79%</td>        <td>0%</td>        <td>-1%</td>        <td>67%</td>        <td>2%</td>        <td>0%</td>        <td>-</td>        <td>-</td>        <td>-</td>        <td>-</td>        <td>-</td>        <td>-</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-28">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Results for Disparate Mistreatment</h3>     </div>     </header>     <p>Our experiments for disparate mistreatment attempt to explore mistreatment elimination both when disparate treatment is avoided and when it is not. In the first case, we do not include information about the sensitive group in the training and validation datasets, whereas in the second case we do. It must be noted that not all classifier and datasets can account for disparate treatment. For example, in the COMPAS dataset, removing the sensitive group feature, i.e. race, yields inadequate levels of prediction for the explored dataset, whereas group thresholding approaches inherently require information about the sensitive group. Although Hardt et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] argue that thresholding can be performed without information disclosure (e.g. locally), avoiding disparate treatment may still be important in certain legal settings. Our ASR+CULEP model is compared to previous ones based on its ability to eliminate overall disparate mistreatment (i.e. minimize both |<em>D<sub>FPR</sub>     </em>| and |<em>D<sub>FNR</sub>     </em>|).</p>     <p>In Table&#x00A0;<a class="tbl" href="#tab1">1</a> we can see that, when information about the sensitive group is available, ASR+CULEP outperforms covariance-based constraints in eliminating disparate mistreatment and yields equally favorable results to group thresholding. On the COMPAS dataset it reduces overall mistreatment by 12% more in exchange for 1% accuracy compared to covariance-based methods and yields identical results to group thresholding. It also performs slightly better in all respects on the SynthOpp dataset and manages to maintain high accuracy on the SynthSame dataset while reducing overall mistreatment 4% more compared to covariance-based constraints.</p>     <p>Table&#x00A0;<a class="tbl" href="#tab1">1</a> also shows that, when avoiding disparate treatment, ASR+CULEP produces better accuracy vs. overall mistreatment elimination trade-offs compared to covariance-based linear constraints. In particular, it yields slightly better (&#x223C; 1%) overall disparate mistreatment elimination while better preserving accuracy on the SynthOpp dataset and trades 6% overall disparate mistreatment to gain 6% accuracy on the SynthSame dataset.</p>     <p>Considering all comparisons, ASR+CULEP is only inferior to other methods in eliminating mistreatment on the SynthSame dataset. However, this can be attributed to the choice of optimization goals while training the CULEP model. Indeed, in every instance where residual mistreatment is worse compared to other methods, significantly higher accuracy is retained to compensate.</p>    </section>    <section id="sec-29">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Results for Disparate Impact</h3>     </div>     </header>     <p>Table&#x00A0;<a class="tbl" href="#tab2">2</a> demonstrates the ability of our methodology to eliminate disparate impact compared to state-of-the-art approaches. ASR+CULEP is able to achieve higher pRule for smaller accuracy trade-off than the best two previous approaches on the Adult dataset and yields better pRule than Covariance but worse than the Regularizer approach on the Bank dataset. Hence, it has merit, especially if disparate treatment is important (Regularizer employs disparate treatment to make results more fair). In this case, compared with Covariance, it attains 6% pRule gain for the same accuracy on the Adult dataset and 16% pRule gain for 2% accuracy loss on the Bank dataset.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Adult dataset disparate impact elimination for logistic regression. We compare our method to the highest pRule obtained by other methods.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;"/>        <th style="text-align:right;"/>        <th colspan="2" style="text-align:center;">Adult<hr/>        </th>        <th colspan="2" style="text-align:center;">Bank<hr/>        </th>       </tr>       <tr>        <th colspan="2" style="text-align:left;">Fairness Approach        </th>        <th style="text-align:right;">pRule</th>        <th style="text-align:right;">acc</th>        <th style="text-align:right;">pRule</th>        <th>acc</th>       </tr>      </thead>      <tbody>       <tr style="border-top: solid 2px">        <td style="text-align:left;">None</td>        <td style="text-align:right;"/>        <td style="text-align:right;">27%</td>        <td style="text-align:right;">85%</td>        <td style="text-align:right;">31%</td>        <td>91%</td>       </tr>       <tr>        <td colspan="2" style="text-align:left;">ASR+CULEP <em>acc</em> + <em>pRule</em>        </td>        <td style="text-align:right;">100%</td>        <td style="text-align:right;">82%</td>        <td style="text-align:right;">99%</td>        <td>89%</td>       </tr>       <tr>        <td style="text-align:left;">Covariance</td>        <td style="text-align:right;">[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0044">44</a>]</td>        <td style="text-align:right;">94%</td>        <td style="text-align:right;">82%</td>        <td style="text-align:right;">83%</td>        <td>91%</td>       </tr>       <tr>        <td style="text-align:left;">Regularizer</td>        <td style="text-align:right;">[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0028">28</a>]</td>        <td style="text-align:right;">85%</td>        <td style="text-align:right;">83%</td>        <td style="text-align:right;">100%</td>        <td>91%</td>       </tr>      </tbody>     </table>     </div>     <p>It must be noted that smaller ASR+CULEP accuracies result from the fairness objective <em>acc</em> + <em>pRule</em>, which incentivizes small accuracy losses in favor of significant fairness improvements. However, this method is superior in that it allows higher margins in optimizing towards pRule and yield better trade-offs for this objective.</p>    </section>   </section>   <section id="sec-30">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusions and Future Work</h2>     </div>    </header>    <p>In this work we presented an Adaptive Sensitive Reweighting (ASR) scheme that uses a convex model (CULEP) to estimate distributions of underlying labels with which to adapt weights. Our method can be applied on multiple types of fairness objectives and can also avoid disparate treatment. Experiments on logistic regression classifiers show that it performs similarly to covariance-based methods in trading-off accuracy and bias if disparate treatment is avoided. If we do not avoid disparate treatment and provide information about the sensitive group in evaluation data though, our approach performs better than these methods in trading-off disparate impact and mistreatment elimination for small accuracy losses and is comparable (even superior in some aspects) to methods specifically designed to take advantage of disparate treatment.</p>    <p>Our results indicate that there is merit in further developing non-heuristic dataset editing mechanisms as competent alternatives to existing fairness-aware approaches - i.e. such approaches are equally valid to existing ones.</p>    <p>For future work, we propose exploring ways to faster train or estimate the CULEP model parameters or adjust them during the training process, as well as developing methods with theoretically guaranteed convergence by training towards optimal sample weights rather than analytically deriving them. Furthermore, our methodology can be tested on more datasets, including multiclass and regression tasks. Finally, since recent works [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] argue that stochastic methods can be inefficient in removing differences pertaining to certain sub-groups, further work should be conducted to examine whether the proposed CULEP model also suffers from the same limitations and, if so, extend it to multimodal distribution formulations, which can adequately model subgroups.</p>   </section>   <section id="sec-31">    <header>     <div class="title-info">     <h2>Acknowledgments</h2>     </div>    </header>    <p>This work was supported by the InVID and hackAIR projects under contract nr. 687786 and 688363 respectively, funded by the European Commission. The authors thank Nikolaos Nikolaou from the University of Manchester for his valuable feedback on this paper.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Solon Barocas and Andrew&#x00A0;D Selbst. 2016. Big data&#x0027;s disparate impact. (2016).</li>     <li id="BibPLXBIB0002" label="[2]">Dan Biddle. 2006. <em>      <em>Adverse impact and test validation: A practitioner&#x0027;s guide to valid and defensible employment testing</em>     </em>. Gower Publishing, Ltd.</li>     <li id="BibPLXBIB0003" label="[3]">Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. 2009. Building classifiers with independency constraints. In <em>      <em>Data mining workshops, 2009. ICDMW&#x2019;09. IEEE international conference on</em>     </em>. IEEE, 13&#x2013;18.</li>     <li id="BibPLXBIB0004" label="[4]">Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang. 2013. Controlling attribute effect in linear regression. In <em>      <em>Data Mining (ICDM), 2013 IEEE 13th International Conference on</em>     </em>. IEEE, 71&#x2013;80.</li>     <li id="BibPLXBIB0005" label="[5]">Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for discrimination-free classification. <em>      <em>Data Mining and Knowledge Discovery</em>     </em>21, 2 (2010), 277&#x2013;292.</li>     <li id="BibPLXBIB0006" label="[6]">L&#x00A0;Elisa Celis, Damian Straszak, and Nisheeth&#x00A0;K Vishnoi. 2017. Ranking with Fairness Constraints. <em>      <em>arXiv preprint arXiv:1704.06840</em>     </em>(2017).</li>     <li id="BibPLXBIB0007" label="[7]">Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. <em>      <em>arXiv preprint arXiv:1703.00056</em>     </em>(2017).</li>     <li id="BibPLXBIB0008" label="[8]">Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and the cost of fairness. <em>      <em>arXiv preprint arXiv:1701.08230</em>     </em>(2017).</li>     <li id="BibPLXBIB0009" label="[9]">Georges Dionne and Casey Rothschild. 2014. Economic effects of risk classification bans. <em>      <em>The Geneva Risk and Insurance Review</em>     </em>39, 2 (2014), 184&#x2013;221.</li>     <li id="BibPLXBIB0010" label="[10]">Neil&#x00A0;A Doherty, Anastasia&#x00A0;V Kartasheva, and Richard&#x00A0;D Phillips. 2012. Information effect of entry into credit ratings market: The case of insurers&#x2019; ratings. <em>      <em>Journal of Financial Economics</em>     </em>106, 2 (2012), 308&#x2013;330.</li>     <li id="BibPLXBIB0011" label="[11]">Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In <em>      <em>Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>     </em>. ACM, 214&#x2013;226.</li>     <li id="BibPLXBIB0012" label="[12]">Michael Feldman. 2015. Computational Fairness: Preventing Machine-Learned Discrimination. (2015).</li>     <li id="BibPLXBIB0013" label="[13]">Michael Feldman, Sorelle&#x00A0;A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certifying and removing disparate impact. In <em>      <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 259&#x2013;268.</li>     <li id="BibPLXBIB0014" label="[14]">Daniel&#x00A0;E Finkel. 2003. DIRECT optimization algorithm user guide. <em>      <em>Center for Research in Scientific Computation, North Carolina State University</em>     </em>2(2003).</li>     <li id="BibPLXBIB0015" label="[15]">Daniel&#x00A0;E Finkel and CT Kelley. 2004. Convergence analysis of the DIRECT algorithm. <em>      <em>Optimization Online</em>     </em>14, 2 (2004), 1&#x2013;10.</li>     <li id="BibPLXBIB0016" label="[16]">Benjamin Fish, Jeremy Kun, and Ad&#x00E1;m&#x00A0;D Lelkes. 2015. Fair boosting: a case study. In <em>      <em>Workshop on Fairness, Accountability, and Transparency in Machine Learning</em>     </em>.</li>     <li id="BibPLXBIB0017" label="[17]">Benjamin Fish, Jeremy Kun, and &#x00C1;d&#x00E1;m&#x00A0;D Lelkes. 2016. A confidence-based approach for balancing fairness and accuracy. In <em>      <em>Proceedings of the 2016 SIAM International Conference on Data Mining</em>     </em>. SIAM, 144&#x2013;152.</li>     <li id="BibPLXBIB0018" label="[18]">Kazuto Fukuchi and Jun Sakuma. 2015. Fairness-Aware Learning with Restriction of Universal Dependency using f-Divergences. <em>      <em>arXiv preprint arXiv:1506.07721</em>     </em>(2015).</li>     <li id="BibPLXBIB0019" label="[19]">Gabriel Goh, Andrew Cotter, Maya Gupta, and Michael&#x00A0;P Friedlander. 2016. Satisfying Real-world Goals with Dataset Constraints. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 2415&#x2013;2423.</li>     <li id="BibPLXBIB0020" label="[20]">Moritz Hardt, Eric Price, Nati Srebro, and others. 2016. Equality of opportunity in supervised learning. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 3315&#x2013;3323.</li>     <li id="BibPLXBIB0021" label="[21]">Qinghua Hu, Pengfei Zhu, Yongbin Yang, and Daren Yu. 2011. Large-margin nearest neighbor classifiers via sample weight learning. <em>      <em>Neurocomputing</em>     </em>74, 4(2011), 656&#x2013;660.</li>     <li id="BibPLXBIB0022" label="[22]">Anatoli Iouditski and Yuri Nesterov. 2014. Primal-dual subgradient methods for minimizing uniformly convex functions. <em>      <em>arXiv preprint arXiv:1401.1792</em>     </em>(2014).</li>     <li id="BibPLXBIB0023" label="[23]">Donald&#x00A0;R Jones, Cary&#x00A0;D Perttunen, and Bruce&#x00A0;E Stuckman. 1993. Lipschitzian optimization without the Lipschitz constant. <em>      <em>Journal of Optimization Theory and Applications</em>     </em>79, 1 (1993), 157&#x2013;181.</li>     <li id="BibPLXBIB0024" label="[24]">Faisal Kamiran and Toon Calders. 2009. Classifying without discriminating. In <em>      <em>Computer, Control and Communication, 2009. IC4 2009. 2nd International Conference on</em>     </em>. IEEE, 1&#x2013;6.</li>     <li id="BibPLXBIB0025" label="[25]">Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for classification without discrimination. <em>      <em>Knowledge and Information Systems</em>     </em>33, 1 (2012), 1&#x2013;33.</li>     <li id="BibPLXBIB0026" label="[26]">Faisal Kamiran, Toon Calders, and others. 2011. Handling conditional discrimination. In <em>      <em>Proc. of the 11th IEEE Int&#x0027;l Conf. on Data Mining</em>     </em>.</li>     <li id="BibPLXBIB0027" label="[27]">Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. 2010. Discrimination aware decision tree learning. In <em>      <em>2010 IEEE International Conference on Data Mining</em>     </em>. IEEE, 869&#x2013;874.</li>     <li id="BibPLXBIB0028" label="[28]">Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware classifier with prejudice remover regularizer. In <em>      <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>     </em>. Springer, 35&#x2013;50.</li>     <li id="BibPLXBIB0029" label="[29]">Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. 2011. Fairness-aware learning through regularization approach. In <em>      <em>Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on</em>     </em>. IEEE, 643&#x2013;650.</li>     <li id="BibPLXBIB0030" label="[30]">Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei&#x00A0;Steven Wu. 2017. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness. <em>      <em>arXiv preprint arXiv:1711.05144</em>     </em>(2017).</li>     <li id="BibPLXBIB0031" label="[31]">Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent trade-offs in the fair determination of risk scores. <em>      <em>arXiv preprint arXiv:1609.05807</em>     </em>(2016).</li>     <li id="BibPLXBIB0032" label="[32]">Ron Kohavi. 1996. Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid.. In <em>      <em>KDD</em>     </em>, Vol.&#x00A0;96. Citeseer, 202&#x2013;207.</li>     <li id="BibPLXBIB0033" label="[33]">J. Larson, S. Mattu, L. Kirchner, and J. Angwin. 2017. COMPAS dataset. (2017). <a href="https://github.com/propublica/compas-analysis" target="_blank">https://github.com/propublica/compas-analysis</a></li>     <li id="BibPLXBIB0034" label="[34]">Yuan Li, Chang Huang, and Ram Nevatia. 2009. Learning to associate: Hybridboosted multi-target tracker for crowded scene. In <em>      <em>Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</em>     </em>. IEEE, 2953&#x2013;2960.</li>     <li id="BibPLXBIB0035" label="[35]">M. Lichman. 2013. UCI Machine Learning Repository. (2013). <a href="http://archive.ics.uci.edu/ml" target="_blank">http://archive.ics.uci.edu/ml</a></li>     <li id="BibPLXBIB0036" label="[36]">Enrico Miglierina and Elena Molho. 2002. Scalarization and stability in vector optimization. <em>      <em>Journal of Optimization Theory and Applications</em>     </em>114, 3 (2002), 657&#x2013;670.</li>     <li id="BibPLXBIB0037" label="[37]">Sergio Moro, Raul Laureano, and Paulo Cortez. 2011. Using data mining for bank direct marketing: An application of the crisp-dm methodology. In <em>      <em>Proceedings of European Simulation and Modelling Conference-ESM&#x2019;2011</em>     </em>. Eurosis, 117&#x2013;121.</li>     <li id="BibPLXBIB0038" label="[38]">Shelly&#x00A0;L Peffer. 2009. Title VII and disparate-treatment discrimination versus disparate-impact discrimination: The Supreme Court&#x0027;s decision in Ricci v. DeStefano. <em>      <em>Review of Public Personnel Administration</em>     </em>29, 4 (2009), 402&#x2013;410.</li>     <li id="BibPLXBIB0039" label="[39]">Andrea Romei and Salvatore Ruggieri. 2014. A multidisciplinary survey on discrimination analysis. <em>      <em>The Knowledge Engineering Review</em>     </em>29, 05 (2014), 582&#x2013;638.</li>     <li id="BibPLXBIB0040" label="[40]">Robert&#x00A0;E Schapire. 2003. The boosting approach to machine learning: An overview. In <em>      <em>Nonlinear estimation and classification</em>     </em>. Springer, 149&#x2013;171.</li>     <li id="BibPLXBIB0041" label="[41]">Muhammad&#x00A0;Bilal Zafar, Isabel Valera, Manuel Gomez&#x00A0;Rodriguez, and Krishna&#x00A0;P. Gummadi. 2017. Fairness Beyond Disparate Treatment &#x0026;#38; Disparate Impact: Learning Classification Without Disparate Mistreatment. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web</em>     </em>     <em>(WWW &#x2019;17)</em>. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 1171&#x2013;1180. <a class="link-inline force-break"      href="http://dx.doi.org/10.1145/3038912.3052660"      target="_blank">http://dx.doi.org/10.1145/3038912.3052660</a></li>     <li id="BibPLXBIB0042" label="[42]">Muhammad&#x00A0;Bilal Zafar, Isabel Valera, Manuel&#x00A0;Gomez Rodriguez, and Krishna&#x00A0;P Gummadi. 2015. Fairness constraints: A mechanism for fair classification. <em>      <em>stat</em>     </em>1050(2015), 19.</li>     <li id="BibPLXBIB0043" label="[43]">Muhammad&#x00A0;Bilal Zafar, Isabel Valera, Manuel&#x00A0;Gomez Rodriguez, and Krishna&#x00A0;P Gummadi. 2015. Learning Fair Classifiers. <em>      <em>stat</em>     </em>1050(2015), 29.</li>     <li id="BibPLXBIB0044" label="[44]">Muhammad&#x00A0;Bilal Zafar, Isabel Valera, Manuel&#x00A0;Gomez Rogriguez, and Krishna&#x00A0;P Gummadi. 2017. Fairness Constraints: Mechanisms for Fair Classification. In <em>      <em>Artificial Intelligence and Statistics</em>     </em>. 962&#x2013;970.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>Learning training weights is hardly a new concept in machine learning, but usually weights are learned to help boost weak learner accuracy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>] and only seldom to satisfy other training objectives [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>].</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Probability maximization is equivalent to loss minimization: for the loss <span class="inline-equation"><span class="tex">$\mathcal {L}_i$</span>    </span> calculated for sample <em>i</em> we can formulate <span class="inline-equation"><span class="tex">$\mathcal {L}_i\propto \hat{P}(\hat{y_i}\ne y_i)$</span>    </span>. We prefer a probabilistic formulation, since it allows us to infer conditional relations in a theoretically sound manner.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>If the derivative of a function exists, its Lipschitz constant coincides with the derivative&#x0027;s supremum. Convex functions, such as <em>exp</em>(<em>&#x03B2;<sub>i</sub>p</em>), are Lipschitz-continuous in bounded sets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>].</p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>&#x00B1; represents either the positive or the negative sign and &#x2213; its opposite sign.</p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break"     href="https://github.com/MKLab-ITI/adaptive-fairness">https://github.com/MKLab-ITI/adaptive-fairness</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186133">https://doi.org/10.1145/3178876.3186133</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
