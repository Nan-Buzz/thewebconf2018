<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>A User Centred Perspective on Structured Data Discovery</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3184558.3186574'>https://doi.org/10.1145/3184558.3186574</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186574'>https://w3id.org/oa/10.1145/3184558.3186574</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">A User Centred Perspective on Structured Data Discovery</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author"><a href="https://orcid.org/0000-0003-4110-1759" ref="author"><span class="givenName">Laura</span>      <span class="surName">Koesten</span></a>     University of Southampton; The Open Data InstituteSouthampton/London, UK 43017-6221<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>, <a href="mailto:laura.koesten@theodi.org">laura.koesten@theodi.org</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186574" target="_blank">https://doi.org/10.1145/3184558.3186574</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Structured data is becoming critical in every domain and its availability on the web is increasing rapidly. Despite its abundance and variety of applications, we know very little about how people find data, understand it, and put it to use. This work aims to inform the design of data discovery tools and technologies from a user centred perspective. We aim to better understand what type of information supports people in finding and selecting data relevant for their respective tasks. We conducted a mixed-methods study looking at the workflow of data practitioners when searching for data. From that we identified textual summaries as a key element that supports the decision making process in information seeking activities for data. Based on these results we performed a mixed-methods study to identify attributes people choose when summarising a dataset. We found text summaries are laid out according to common structures, contain four main information types, and cover a set of dataset features. We describe follow-up studies that are planned to validate these findings and to evaluate their applicability in a dataset search scenario.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Laura Koesten. 2018. A User Centred Perspective on Structured Data Discovery. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href="https://doi.org/10.1145/3184558.3186574" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186574</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <p>data search, human data interaction, data discovery, data portals</p>   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Problem</h2>     </div>    </header>    <p>As the use of data-driven technology and with it the economic value of services based on data is growing &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>], the availability of data sources that are published on the web is increasing. More than one million datasets were made available by governments worldwide by 2013 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. In Europe there were more than 750,000 datasets published by regional and national authorities, as indexed in September 2017 by the European Data Portal<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>. The site Data Planet<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a> lists no less than 4.9 billion statistical datasets, many of which are public. This work focuses on structured data: data that is organised explicitly - such as in spreadsheets, web tables, databases or maps - whatever its format.</p>    <p>The scenario we focus on is when a user tries to find and use data that matches an information need. This can be someone trying to find an answer to a question requiring data, or someone searching for a full dataset to analyse. This work aims to understand how the discovery of new data sources can be improved by supporting users in assessing their fitness for use when selecting data sources from a pool of search results.</p>    <p>Let us take as an example a data journalist writing an article about the runway expansion at London&#x0027;s main airports in the UK. Examples of such stories can be found, for instance, on the Data Blog of the Guardian newspaper<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>. The journalist will look for factual evidence to substantiate her story, in the form of reports, news on similar topics, as well as data about the economic, social, and environmental ramifications of the project, arguing for or against expansion plans. A large share of the relevant data is already available online, published by governmental agencies, researchers, and other journalists. However, finding it is not always straightforward. The journalist could use regular search engines, fact checking services<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a><a class="fn" href="#fn6" id="foot-fn6"><sup>5</sup></a>, and other channels in the same way she does when looking for less structured kinds of information. She might also know of the existence of a particular data catalogue, which offers access to collections of data resources released by one or several organisations. She might even query the API (Application Programming Interface) of a trusted data provider, or crawl the web looking for bespoke data snippets. Once she has identified several matches, her next step would be to explore the most promising among them and assemble a set of datasets most relevant to the narrative of her article. Depending on the tools used to discover the data, this step might involve downloading data files; working with different formats (e.g., CSV, XML, HTML, RDF, relational tables); choosing between several versions of a dataset; alongside sensemaking tasks such as establishing what exactly a dataset covers (its &#x2018;attributes&#x2019; or &#x2018;schema&#x2019;), and how accurate, complete, or up-to-date the data is.</p>    <p>This example illustrates the unique characteristics of the information seeking process for structured data opposed to for textual documents from a user perspective. Therefore the high level research question of this work is formulated as follows: <em>How can we help people select data that is relevant and useful for their task?</em>    </p>    <p>This paper present two studies. The first focuses on the information seeking process of data professionals searching for structured data on the web, to better understand search strategies, tasks and selection criteria in dataset search. The second explores the characteristics of text summaries for data and their usefulness in a dataset selection scenario.</p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> State of the art</h2>     </div>    </header>    <section id="sec-4">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Information seeking for structured data</h3>     </div>     </header>     <p>While literature describes user centric models for information seeking in web search, little research explicitly focuses on data as the information source. Information Retrieval, rooted in Computer Science, is concerned with the technologies that support finding and presenting of information [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] and was traditionally focused on whether a system can retrieve relevant documents. Information seeking, as a discipline rooted in Library Sciences, places the people and the finding or searching activity in the centre of attention [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. Interactive Information Retrieval (IIR) studies users interacting with systems and information, the focus here is whether people can use a system to retrieve information [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. This work is based in IIR and information seeking, aiming to understand the specific characteristics of how people retrieve structured data as the source of information opposed to other sources, such as textual documents or web pages.</p>     <p>Information seeking that is centred around (structured) data is not often discussed in literature [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. The user&#x0027;s goal and information need might differ, as the type of information available differs when being able to search at the data level [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. In a study with social scientists [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] found that users are willing to put higher effort in the searching and selection process of datasets than they do when searching for literature and that the quantity and quality of metadata are far more critical in dataset search than in literature search, where convenience is most important. As one of the few models explicitly focusing on structured data, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>] present the data science process, which is describing activities to collect relevant data, to explore data in order to make sense of it, and finally to build an analysis model to draw conclusions from it. Closer to this area is literature on interaction with databases [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>]. The focus there has been mostly on how users compose queries and the degree to which these queries can be translated into SQL or similar [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]. Our scenario is much more open in the range of data sources it targets.</p>    </section>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Data versus documents</h3>     </div>     </header>     <p>There are several aspects that add to the complexity of search tasks for data. Web data is heterogeneous and comes from different sources, it needs to be understood in its context - much more than traditional documents [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. This presents unique interaction challenges that require thinking about the user, as well as about the underlying system and the design of the interface. In contrast to document search, users need skills to access and download data; interpret different or limited formats the data might be available in; and understand connected licences and metadata. Furthermore, data requires context to create meaning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>], to make sense of it. Sensemaking is defined as the process of constructing meaning from information [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>]. Rieh et al. describe the sensemaking process as creating knowledge structures between information that has been acquired through an information seeking task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. While this applies to information seeking activities generally, we believe that this process has unique characteristics when the source of information is structured data. In their work on accessing statistical information Marchionini et al. emphasise that people need context as well as means to reveal the story behind numbers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>]. This varies with the level of expertise of the user in terms of technical skills, prior knowledge and data literacy. People looking to find answers with data will sometimes require not just to be pointed to the right database, spreadsheet, or list that might contain the information they need, but to the record that answers their question. This has implications for how data search is implemented, as algorithms that focus on metadata, which are the de facto standard in existing portals, do not have this capability. In addition to technological limitations, there is little research examining data search from a user perspective. This research, as much of the related work in data search and sensemaking with data, is based on the assumption that, in order to offer the best user experience, we cannot simply reuse or re-purpose principles, models, and tools that have been proposed for less structured sources of information.</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Summarisation of datasets</h3>     </div>     </header>     <p>In search scenarios, we are used to being presented with a snippet, the short summarising text component that is returned by search engines. This helps users to make a decision about the relevance of a search result when searching for textual documents [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. We are able to create snippets that adjust their content dependent on the query, which have proven to be more effective [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. However we are still far away from being able to provide the same user experience for data search. Johnson defines a summary as a brief statement that condenses information and reflects the central ideas or essence of the discourse [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. A good summary should be able to represent the core idea, and effectively convey the meaning of the source [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. Meaningful summaries of datasets can support the discovery process [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] by enabling users to understand the content of the dataset and skip irrelevant search results. We know that textual representations of data can be more effective, comprehensible and helpful in decision making than corresponding graphical representations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>], even when the data is uncertain [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>].</p>     <p>Textual summaries of datasets could generated automatically but in current practice they are created by people. Community guidelines for data sharing, such as the W3C&#x0027;s Data on the Web Best Practices<a class="fn" href="#fn7" id="foot-fn7"><sup>6</sup></a> or SharePSI<a class="fn" href="#fn8" id="foot-fn8"><sup>7</sup></a> have a technical focus on the machine readability of data. Dataset descriptions are included in their recommendations, though usually without much specification of what that should contain. This can be seen in metadata standards, such as DCAT, a vocabulary to describe datasets in catalogues<a class="fn" href="#fn9" id="foot-fn9"><sup>8</sup></a>, or schema.org<a class="fn" href="#fn10" id="foot-fn10"><sup>9</sup></a>, a set of schemas for structured data markup on web pages. We know that datasets become more useful when metadata descriptions are available and data becomes potentially more understandable [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. However, we know little about what meaningful summaries of data should look like. In their review of summary generation from text, Gambhir et al. point out the subjectivity of the task and the lack of criteria for what is important in a summary [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] which is reflected in the general lack of guidance in current metadata standards.</p>    </section>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Proposed approach</h2>     </div>    </header>    <p>We break down our high level research question into the following sub-questions for the initial study: (RQ1) How do people currently search for data? (RQ2) What are the characteristics of information seeking tasks for data? (RQ3) How do people evaluate data that they find? (RQ4) What types of tasks do people do with data? (RQ5) How do people explore data that they have found?</p>    <p>We believe that these aspects give us the necessary background knowledge to understand how we can support people in selecting and assessing data in a search scenario. User-system interactions are influenced by factors that are not easily observable or measurable [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. For this reason, and given the investigative nature of our research, we focused the study on its qualitative element and used in-depth interviews to get the rich data about interaction processes and workflows we were looking for and complemented the results with a search log analysis of a data portal.</p>    <p>Based on the data science process by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] we discuss the process of working with data from a user perspective in five pillars: <em>tasks, search, evaluate, explore and use.</em> This model was used as the basis for the initial study in this work [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>]. The assumption is that this process is not linear and involves multiple iterations and backwards movements between pillars for many data centric tasks. The rationale behind this rather general approach is the lack of relevant literature that focuses explicitly on peoples&#x2019; interaction with structured data on the web.</p>    <p>Based on the findings of this study we designed a second experiment to explore dataset summaries. From a user perspective, summaries of datasets are currently critical in the discovery process; they provide a basis for selecting a dataset from search results. Representing data as text is known to help people make sense of it [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>], but we know little about how a good summary should look in a data search context. Therefore this study explores the attributes that people choose to describe a dataset. We defined the following research questions for this study on textual summarisation of datasets:</p>    <p>(RQ1) What types of attributes of data do people choose when summarising a dataset? i) Do these attributes and attribute types vary from one summary to another for the same dataset? ii) Do these attributes and attribute types vary from one dataset to another? (RQ2) Do data practitioners and crowdworkers summarise datasets differently?</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Methodology</h2>     </div>    </header>    <p>This PhD project is at the beginning of its third year. After an extensive literature review we conducted two mixed-methods studies, informed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] for the purpose of this work. The initial study aimed to shed light on how data practitioners look for data online, with a focus on a qualitative component using in-depth interviews with twenty data professionals from various backgrounds. To supplement the in-depth interviews, we analysed a unique dataset of search logs of a large open government data portal. The sample consisted of a total of more than 100,000 queries, of which more than 50,000 were unique queries. This gave us a less obtrusive way to learn about the behaviour of data search users [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], of which our interviewees were a subset of (17 out of 20 participants mentioned they used this portal to search for datasets).</p>    <p>For the second study we conducted a task-based lab experiment in which 30 participants described and summarised datasets in a writing task. Subsequently, we conducted a crowdsourcing study in which we replicated the lab experiment with a larger variety of datasets; and asked crowdworkers to rate the dataset summaries according to perceived quality. This allowed us to get a better understanding of the influence of the underlying dataset and of differences in participants and settings on the resulting summary. We collected 150 long and 150 short summaries from the lab experiment and 250 crowdsourced summaries and analysed these qualitatively and quantitatively.</p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Results</h2>     </div>    </header>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Initial study on information seeking for structured data</h3>     </div>     </header>     <p>Key findings from the initial study showed that finding data is challenging, even for data professionals and that searching for data is more often than not exploratory and complex. It was evident that people across different skill sets and professional backgrounds follow common workflows when engaging with structured data. The majority of participants reported trying to obtain recommendations from people working in the respective field who are likely to know about a dataset. The majority of our participants reported often finding it difficult to locate the data they need. For instance, 80% of the participants described finding data online as a complex, iterative, process:</p>     <Quote>     <p>(By an experiment participant) <em>I would get some things that looked really promising but weren&#x0027;t and then finally, through some kind of mysterious combination of search terms, I suddenly came across the dataset I&#x0027;d been looking for the entire time</em>     </p>     </Quote>     <p>Selection criteria for datasets in a search scenario showed unique characteristics. We found that when selecting data on the web people generally do not think they have enough information about the content of a dataset to make an informed decision. We identified dataset relevance, usability and quality as the high level dimensions of selection criteria in dataset search, as listed in Table <a class="tbl" href="#tab1">1</a>. Key factors that help to select datasets from a pool of search results emerged to be information about provenance and about the methodology of data collection and analysis. The findings also showed that the concept of data quality is inherently task dependent, which is in line with literature (for instance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]).</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Information needs in dataset selection</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;">        <em>Assess</em>        </td>        <td style="text-align:center;">        <em>Information needed about</em>        </td>       </tr>       <tr>        <td style="text-align:center;">Relevance</td>        <td style="text-align:center;">context, coverage, original purpose, granularity, summary, time frame</td>       </tr>       <tr>        <td style="text-align:center;">Usability</td>        <td style="text-align:center;">labeling, documentation, license, access, machine readability, language used, format, schema, ability to share</td>       </tr>       <tr>        <td style="text-align:center;">Quality</td>        <td style="text-align:center;">collection methods, provenance, consistency of formatting / labeling, completeness, what has been excluded</td>       </tr>      </tbody>      <tfoot>       <tr>        <td>&#x00A0;</td>        <td/>       </tr>      </tfoot>     </table>     </div>     <p>We further found that the majority of textual summaries of data are perceived to be of low quality and limited usefulness.</p>     <p>The quote below illustrates a common response in this study which was part of the rationale behind the follow-up dataset summarisation study:</p>     <Quote>     <p>(By an experiment participant) <em>It&#x0027;s very difficult first when you download new data, to have a quick idea of what the data represents, a quick summary of the data.</em>     </p>     </Quote>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Dataset summarisation study</h3>     </div>     </header>     <p>As the results of this study are still unpublished we keep this section as an overview for the purpose of describing our approach and the type of results that can be expected. The findings of this study show that textual summaries were laid out according to common structures. We were able to isolate different components that the summaries were made of (four main information types), alongside common structures and detailed features. User-created summaries included judgements of which parts of the dataset are most important and aggregated elements of key importance into semantic groups. The results point to a number of features that could easily be extracted, but for which there is no standard form of reporting in general-purpose metadata schemata.</p>     <p>The study suggests a range of characteristics which people consider important when engaging with unfamiliar datasets. Some of them could be generated automatically, others would still require manual input, for example from the dataset creator or from other (potential) users. We saw for instance that all dataset summaries, as expected, refer explicitly to the content in the dataset. Extracting content features directly from the dataset, and representing them as text is still a subject of research, in particular in the context of extractive dataset summarisation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] or semantic labelling of numerical data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>].</p>     <p>Our findings can inform the design of these methods by suggesting parts of a dataset that matter for human data engagement. At the same time, our analysis shows that most summaries also cover information that goes beyond content-related aspects. While abstractive approaches to automatically generate summaries exist, we believe that the levels of abstraction and grouping needed for the creation of meaningful textual representations of data are not yet being realised. We believe that to be truly useful, a summary needs to be a combination of extractable features, combined with contextual information, human judgement, and creativity. This applies to selecting the right content to consider, as well as to representing this content in a meaningful way.</p>     <p>     <strong>Template for dataset summaries</strong> Based on our findings, we propose a template for text-centric data summaries, in the form of a set of questions that can be used as guidance in the summary writing process. Each question describes one of the attributes that were common in the participant generated summaries. Studies on text summarisation found that people create better summaries when they are given an outline or a narrative structure that serves as a template, as opposed to having to create text from scratch [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>].</p>    </section>   </section>   <section id="sec-12">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions and future work</h2>     </div>    </header>    <p>By conducting in-depth interviews with data practitioners, we were able to obtain a better understanding of data-centric tasks, as well as about search, evaluation, and exploration strategies and the data qualities that influence the outcomes of these activities. In contrast to searching for digital objects, such as e.g. physical artifacts in a digital library, datasets contain information which can be used to contextualise them and so support a search process. We currently rely on metadata, which varies in quality and availability. In this work we argue that utilising the original data to enrich metadata can support users in data discovery and, moreover, potentially provide relevant indexable content which could make data search more effective. Our second study presents to the best of our knowledge, a first in-depth characterisation of human generated dataset summaries. This enables us to better define evaluation criteria for textual summaries of datasets and gives insights into selection criteria in dataset search. These results not only support our understanding of how people interact with and communicate about data, but could further inform automatic summary generation and future metadata standards.</p>    <p>We have identified two potential directions of this work, a study on selection criteria in data search and an evaluation of the dataset summaries, described in Section 5.2, in a search scenario. Based on findings from our initial study we hypothesise that selection criteria are specific to dataset search. We believe that in order to make a stronger case for the type of information that should be presented to users in a search scenario, we need a more in-depth study to validate our findings focusing on user defined selection criteria. We are proposing to apply a qualitative approach, using a diary study. We will use thematic analysis to better understand motives and considerations during the selection process of datasets in the context of a particular task. The results of this study can be used to validate our prior findings. More importantly they will contribute to a more in-depth understanding of selection criteria in dataset search.</p>    <p>We further propose to evaluate the effectiveness and usefulness of summaries from our second study in a search scenario as described below. We aim to aggregate the findings of this work in an experiment evaluating five different modes (1-5) of representing the content of summaries in a selection scenario: (a) Textual summaries created with the template, (b) Non-textual representation of summaries (table), (c) Textual summaries created with the template + non-textual representation of the summaries (table), (d) Textual summaries created with the template + visualisation of temporal/geospatial aspects in the data, (e) Sample previews of the data without a summary or other representation. This experiment will focus on the selection of a dataset by a user out of a list of search results and aim to answer the following research questions:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">(RQ1) Are people faster, more confident and do they select more relevant and useful results when they get presented with a summary in a dataset selection scenario?<br/></li>     <li id="list2" label="&#x2022;">(RQ2) Are textual or non-textual summaries most effective and useful?<br/></li>     <li id="list3" label="&#x2022;">(RQ3) Are summaries created based on the template more effective and useful?<br/></li>    </ul>    <p>Based on the findings of the initial study we hypothesise that textual summaries of datasets will be more effective and perceived as more useful in the selection process of datasets in a search scenario. We further assume that summaries created based on the template will perform better in comparison to summaries created without them. We also assume that textual summaries will be more effective and perceived to be more useful than non-textual summaries. However, the design of this experiment is still in progress and leaves room for discussion of whether it is the right direction and how to select suitable natural tasks, the environment in which such an experiment is conducted, as well as the choice of evaluation metrics.</p>    <p>The two studies outlined in this section would enable us to validate findings of this work. This allows for instance to iterate over the framework for human interaction with structured data which resulted from the initial study. We plan to refine and validate the template for dataset summary creation that resulted from our second study by evaluating the usefulness of the template and it&#x0027;s individual attributes in a dataset selection scenario. The results of this study can inform the development and improvement of manual as well as automatic summary generation, with the ultimate goal of improving people&#x0027;s interaction with data by making the experience more accessible and understandable. Additional work could be carried out on refining a semi-automatic approach to generating summaries, using the template by prompting crowdworkers to extract these elements from datasets.</p>    <p>This research aims to present a novel perspective on the conceptualisation of data centric search tasks, an in-depth analysis of selection criteria in data search and potential solutions to support users in the selection process of a dataset out of a pool of search results by proposing a more standardised approach of creating meaningful textual summaries of datasets for human consumption.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Lorena&#x00A0;Leal Bando, Falk Scholer, and Andrew Turpin. 2010. Constructing Query-biased Summaries: A Comparison of Human and System Generated Snippets. In <em>      <em>Proceedings of the Third Symposium on Information Interaction in Context</em>     </em>(<em>IIiX &#x2019;10</em>). ACM, New York, NY, USA, 195&#x2013;204. <a class="link-inline force-break" href="https://doi.org/10.1145/1840784.1840813"      target="_blank">https://doi.org/10.1145/1840784.1840813</a></li>     <li id="BibPLXBIB0002" label="[2]">Bruce&#x00A0;E Bargmeyer and Daniel&#x00A0;W Gillman. 2000. Metadata standards and metadata registries: An overview. In <em>      <em>International Conference on Establishment Surveys II, Buffalo, New York</em>     </em>.</li>     <li id="BibPLXBIB0003" label="[3]">Ann Blandford and Simon Attfield. 2010. Interacting with information. <em>      <em>Synthesis Lectures on Human-Centered Informatics</em>     </em>3, 1(2010), 1&#x2013;99.</li>     <li id="BibPLXBIB0004" label="[4]">Ria&#x00A0;Mae Borromeo, Maha Alsaysneh, Sihem Amer-Yahia, and Vincent Leroy. 2017. Crowdsourcing Strategies for Text Creation Tasks. In <em>      <em>Proceedings of the 20th International Conference on Extending Database Technology, EDBT 2017, Venice, Italy, March 21-24, 2017.</em>     </em>450&#x2013;453. <a class="link-inline force-break" href="https://doi.org/10.5441/002/edbt.2017.42"      target="_blank">https://doi.org/10.5441/002/edbt.2017.42</a></li>     <li id="BibPLXBIB0005" label="[5]">Alan Bryman. 2006. Integrating quantitative and qualitative research: how is it done?<em>      <em>Qualitative research</em>     </em>6, 1 (2006), 97&#x2013;113.</li>     <li id="BibPLXBIB0006" label="[6]">Tiziana Catarci. 2000. What happened when database researchers met usability. <em>      <em>Information Systems</em>     </em>25, 3 (2000), 177&#x2013;212.</li>     <li id="BibPLXBIB0007" label="[7]">Gabriella Cattaneo, Mike Glennon, Rosanna Lifonti, Giorgio Micheletti, Alys Woodward, Marianne Kolding, Angela Vacca, Carla La&#x00A0;Croce, and David Osimo. 2015. IDC, European Data Market SMART 2013/0063, D6 - First Interim Report. (16 October 2015). <a class="link-inline force-break"      href="https://idc-emea.app.box.com/s/k7xv0u3gl6xfvq1rl667xqmw69pzk790">https://idc-emea.app.box.com/s/k7xv0u3gl6xfvq1rl667xqmw69pzk790</a>.</li>     <li id="BibPLXBIB0008" label="[8]">Brenda Dervin. 1997. Given a context by any other name: Methodological tools for taming the unruly beast. <em>      <em>Information seeking in context</em>     </em>13 (1997), 38.</li>     <li id="BibPLXBIB0009" label="[9]">Rafael Ferreira, Luciano de Souza&#x00A0;Cabral, Rafael&#x00A0;Dueire Lins, Gabriel&#x00A0;Pereira e Silva, Fred Freitas, George&#x00A0;D.C. Cavalcanti, Rinaldo Lima, Steven&#x00A0;J. Simske, and Luciano Favaro. 2013. Assessing sentence scoring techniques for extractive text summarization. <em>      <em>Expert Systems with Applications</em>     </em>40, 14 (2013), 5755 &#x2013; 5764. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.eswa.2013.04.023"      target="_blank">https://doi.org/10.1016/j.eswa.2013.04.023</a></li>     <li id="BibPLXBIB0010" label="[10]">Mahak Gambhir and Vishal Gupta. 2017. Recent automatic text summarization techniques: a survey. <em>      <em>Artif. Intell. Rev.</em>     </em>47, 1 (2017), 1&#x2013;66. <a class="link-inline force-break"      href="https://doi.org/10.1007/s10462-016-9475-9"      target="_blank">https://doi.org/10.1007/s10462-016-9475-9</a></li>     <li id="BibPLXBIB0011" label="[11]">Dimitra Gkatzia, Oliver Lemon, and Verena Rieser. 2016. Natural Language Generation enhances human decision-making with uncertain information. In <em>      <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers</em>     </em>. <a class="link-inline force-break"      href="http://aclweb.org/anthology/P/P16/P16-2043.pdf"      target="_blank">http://aclweb.org/anthology/P/P16/P16-2043.pdf</a></li>     <li id="BibPLXBIB0012" label="[12]">Suzanne Hidi and Valerie Anderson. 1986. Producing written summaries: Task demands, cognitive operations, and implications for instruction. <em>      <em>Review of educational research</em>     </em>56, 4 (1986), 473&#x2013;493. <a class="link-inline force-break"      href="https://doi.org/10.3102/00346543056004473"      target="_blank">https://doi.org/10.3102/00346543056004473</a></li>     <li id="BibPLXBIB0013" label="[13]">Jozef Hvoreck&#x1EF3;, Martin Drl&#x00ED;k, and Michal Munk. 2010. Enhancing database querying skills by choosing a more appropriate interface. In <em>      <em>IEEE EDUCON 2010 Conference</em>     </em>. IEEE, 1897&#x2013;1905.</li>     <li id="BibPLXBIB0014" label="[14]">Bernard&#x00A0;J Jansen. 2006. Search log analysis: What it is, what&#x0027;s been done, how to do it. <em>      <em>Library &#x0026; information science research</em>     </em>28, 3 (2006), 407&#x2013;432.</li>     <li id="BibPLXBIB0015" label="[15]">Diane Kelly. 2009. Methods for Evaluating Interactive Information Retrieval Systems with Users. <em>      <em>Foundations and Trends in Information Retrieval</em>     </em>3, 1-2(2009), 1&#x2013;224. <a class="link-inline force-break" href="https://doi.org/10.1561/1500000012"      target="_blank">https://doi.org/10.1561/1500000012</a></li>     <li id="BibPLXBIB0016" label="[16]">Dagmar Kern and Brigitte Mathiak. 2015. Are There Any Differences in Data Set Retrieval Compared to Well-Known Literature Retrieval?. In <em>      <em>Research and Advanced Technology for Digital Libraries - 19th International Conference on Theory and Practice of Digital Libraries, TPDL 2015, Pozna&#x0144;, Poland, September 14-18, 2015. Proceedings</em>     </em>. 197&#x2013;208. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-24592-8_15"      target="_blank">https://doi.org/10.1007/978-3-319-24592-8_15</a></li>     <li id="BibPLXBIB0017" label="[17]">Dagmar Kern and Brigitte Mathiak. 2015. Are There Any Differences in Data Set Retrieval Compared to Well-Known Literature Retrieval?. In <em>      <em>Research and Advanced Technology for Digital Libraries - 19th International Conference on Theory and Practice of Digital Libraries, TPDL 2015, Pozna&#x0144;, Poland, September 14-18, 2015. Proceedings</em>     </em>. 197&#x2013;208. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-24592-8_15"      target="_blank">https://doi.org/10.1007/978-3-319-24592-8_15</a></li>     <li id="BibPLXBIB0018" label="[18]">Joy&#x00A0;O. Kim and Andr&#x00E9;s Monroy-Hern&#x00E1;ndez. 2016. Storia: Summarizing Social Media Content based on Narrative Theory using Crowdsourcing. In <em>      <em>Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &#x0026; Social Computing, CSCW 2016, San Francisco, CA, USA, February 27 - March 2, 2016</em>     </em>. 1016&#x2013;1025. <a class="link-inline force-break" href="https://doi.org/10.1145/2818048.2820072"      target="_blank">https://doi.org/10.1145/2818048.2820072</a></li>     <li id="BibPLXBIB0019" label="[19]">Laura&#x00A0;M. Koesten, Emilia Kacprzak, Jenifer F.&#x00A0;A. Tennison, and Elena Simperl. 2017. The Trials and Tribulations of Working with Structured Data: -a Study on Information Seeking Behaviour. In <em>      <em>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em>     </em>(<em>CHI &#x2019;17</em>). ACM, New York, NY, USA, 1277&#x2013;1289. <a class="link-inline force-break" href="https://doi.org/10.1145/3025453.3025838"      target="_blank">https://doi.org/10.1145/3025453.3025838</a></li>     <li id="BibPLXBIB0020" label="[20]">Anna&#x00A0;S. Law, Yvonne Freer, Jim Hunter, Robert&#x00A0;H. Logie, Neil Mcintosh, and John Quinn. 2005. A Comparison of Graphical and Textual Presentations of Time Series Data to Support Medical Decision Making in the Neonatal Intensive Care Unit. <em>      <em>Journal of Clinical Monitoring and Computing</em>     </em>19, 3 (01 Jun 2005), 183&#x2013;194. <a class="link-inline force-break"      href="https://doi.org/10.1007/s10877-005-0879-3"      target="_blank">https://doi.org/10.1007/s10877-005-0879-3</a></li>     <li id="BibPLXBIB0021" label="[21]">Gary Marchionini, Stephanie&#x00A0;W. Haas, Junliang Zhang, and Jonathan&#x00A0;L. Elsas. 2005. Accessing Government Statistical Information. <em>      <em>IEEE Computer</em>     </em>38, 12 (2005), 52&#x2013;61. <a class="link-inline force-break" href="https://doi.org/10.1109/MC.2005.393"      target="_blank">https://doi.org/10.1109/MC.2005.393</a></li>     <li id="BibPLXBIB0022" label="[22]">Michael Chui Peter Groves Diana Farrell Steve Van Kuiken Elizabeth Almasi&#x00A0;Doshi McKinsey Global&#x00A0;Institute, James&#x00A0;Manyika. 2013. Open data: Unlocking innovation and performance with liquid information. (2013).</li>     <li id="BibPLXBIB0023" label="[23]">C Naumer, K Fisher, and Brenda Dervin. 2008. Sense-Making: a methodological perspective. In <em>      <em>Sensemaking Workshop, CHI&#x2019;08</em>     </em>.</li>     <li id="BibPLXBIB0024" label="[24]">T.&#x00A0;T. Nguyen, Q.&#x00A0;V.&#x00A0;H. Nguyen, M. Weidlich, and K. Aberer. 2015. Result selection and summarization for Web Table search. In <em>      <em>2015 IEEE 31st International Conference on Data Engineering</em>     </em>. 231&#x2013;242. <a class="link-inline force-break"      href="https://doi.org/10.1109/ICDE.2015.7113287"      target="_blank">https://doi.org/10.1109/ICDE.2015.7113287</a></li>     <li id="BibPLXBIB0025" label="[25]">Hanspeter Pfister and Joe Blitzstein. 2015. cs109/2015, Lectures 01-Introduction. https://github.com/cs109/2015/tree/master/Lectures. (2015).</li>     <li id="BibPLXBIB0026" label="[26]">Minh Pham, Suresh Alse, Craig&#x00A0;A. Knoblock, and Pedro Szekely. 2016. <em>      <em>Semantic Labeling: A Domain-Independent Approach</em>     </em>. Springer International Publishing, Cham, 446&#x2013;462. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-46523-4_27"      target="_blank">https://doi.org/10.1007/978-3-319-46523-4_27</a></li>     <li id="BibPLXBIB0027" label="[27]">Soo&#x00A0;Young Rieh, Kevyn Collins-Thompson, Preben Hansen, and Hye-Jung Lee. 2016. Towards searching as a learning process: A review of current perspectives and future directions. <em>      <em>J. Information Science</em>     </em>42, 1 (2016), 19&#x2013;34. <a class="link-inline force-break" href="https://doi.org/10.1177/0165551515615841"      target="_blank">https://doi.org/10.1177/0165551515615841</a></li>     <li id="BibPLXBIB0028" label="[28]">Stefano Spaccapietra and Ramesh Jain. 2013. <em>      <em>Visual Database Systems 3: Visual Information Management</em>     </em>. Springer.</li>     <li id="BibPLXBIB0029" label="[29]">M van&#x00A0;der Meulen, R&#x00A0;H. Logie, Y Freer, C Sykes, N McIntosh, and J Hunter. 2010. When a graph is poorer than 100 words: A comparison of computerised natural language generation, human generated descriptions and graphical displays in neonatal intensive care. <em>      <em>Applied Cognitive Psychology</em>     </em>24, 1 (2010), 77&#x2013;89. <a class="link-inline force-break" href="https://doi.org/10.1002/acp.1545"      target="_blank">https://doi.org/10.1002/acp.1545</a></li>     <li id="BibPLXBIB0030" label="[30]">Max&#x00A0;L. Wilson, Bill Kules, m.&#x00A0;c. schraefel, and Ben Shneiderman. 2010. From Keyword Search to Exploration: Designing Future Search Interfaces for the Web. <em>      <em>Foundations and Trends in Web Science</em>     </em>2, 1 (2010), 1&#x2013;97. <a class="link-inline force-break" href="https://doi.org/10.1561/1800000003"      target="_blank">https://doi.org/10.1561/1800000003</a></li>     <li id="BibPLXBIB0031" label="[31]">Amrapali Zaveri, Anisa Rula, Andrea Maurino, Ricardo Pietrobon, Jens Lehmann, and S&#x00F6;ren Auer. 2016. Quality assessment for Linked Data: A Survey. <em>      <em>Semantic Web</em>     </em>7, 1 (2016), 63&#x2013;93. <a class="link-inline force-break" href="https://doi.org/10.3233/SW-150175"      target="_blank">https://doi.org/10.3233/SW-150175</a></li>     <li id="BibPLXBIB0032" label="[32]">Hai Zhuge. 2015. Dimensionality on Summarization. <em>      <em>CoRR</em>     </em>abs/1507.00209(2015). <a class="link-inline force-break" href="http://arxiv.org/abs/1507.00209"      target="_blank">http://arxiv.org/abs/1507.00209</a></li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>Supervised by Elena Simperl and Jeni Tennison</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break"     href="https://www.europeandataportal.eu/data/en/dataset">https://www.europeandataportal.eu/data/en/dataset</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://www.data-planet.com/">https://www.data-planet.com/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="https://www.theguardian.com/data">https://www.theguardian.com/data</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a>http://factcheck.org/</p>   <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a>https://fullfact.org/</p>   <p id="fn7"><a href="#foot-fn7"><sup>6</sup></a><a class="link-inline force-break" href="https://www.w3.org/TR/dwbp/">https://www.w3.org/TR/dwbp/</a>   </p>   <p id="fn8"><a href="#foot-fn8"><sup>7</sup></a><a class="link-inline force-break" href="https://www.w3.org/2013/share-psi/bp/">https://www.w3.org/2013/share-psi/bp/</a>   </p>   <p id="fn9"><a href="#foot-fn9"><sup>8</sup></a><a class="link-inline force-break" href="https://www.w3.org/TR/vocab-dcat/">https://www.w3.org/TR/vocab-dcat/</a>   </p>   <p id="fn10"><a href="#foot-fn10"><sup>9</sup></a>http://schema.org/Dataset</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186574">https://doi.org/10.1145/3184558.3186574</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
