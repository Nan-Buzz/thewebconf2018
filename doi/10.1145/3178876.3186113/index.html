<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Co-Regularized Deep Multi-Network Embedding</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3186113'>https://doi.org/10.1145/3178876.3186113</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186113'>https://w3id.org/oa/10.1145/3178876.3186113</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Co-Regularized Deep Multi-Network Embedding</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Jingchao</span>     <span class="surName">Ni</span>,     College of Information Sciences and Technology, Pennsylvania State University, <a href="mailto:jzn47@ist.psu.edu">jzn47@ist.psu.edu</a>    </div>    <div class="author">     <span class="givenName">Shiyu</span>     <span class="surName">Chang</span>,     IBM T. J. Watson Research Center, <a href="mailto:shiyu.chang@ibm.com">shiyu.chang@ibm.com</a>    </div>    <div class="author">     <span class="givenName">Xiao</span>     <span class="surName">Liu</span>,     Department of Biomedical Engineering, Pennsylvania State University, <a href="mailto:xxl213@engr.psu.edu">xxl213@engr.psu.edu</a>    </div>    <div class="author">     <span class="givenName">Wei</span>     <span class="surName">Cheng</span>,     NEC Laboratories America, <a href="mailto:weicheng@nec-labs.com">weicheng@nec-labs.com</a>    </div>    <div class="author">     <span class="givenName">Haifeng</span>     <span class="surName">Chen</span>,     NEC Laboratories America, <a href="mailto:haifeng@nec-labs.com">haifeng@nec-labs.com</a>    </div>    <div class="author">     <span class="givenName">Dongkuan</span>     <span class="surName">Xu</span>,     College of Information Sciences and Technology, Pennsylvania State University, <a href="mailto:dux19@ist.psu.edu">dux19@ist.psu.edu</a>    </div>    <div class="author">     <span class="givenName">Xiang</span>     <span class="surName">Zhang</span>,     College of Information Sciences and Technology, Pennsylvania State University, <a href="mailto:xzhang@ist.psu.edu">xzhang@ist.psu.edu</a>    </div>                                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186113" target="_blank">https://doi.org/10.1145/3178876.3186113</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Network embedding aims to learn a low-dimensional vector representation for each node in the social and information networks, with the constraint to preserve network structures. Most existing methods focus on single network embedding, ignoring the relationship between multiple networks. In many real-world applications, however, multiple networks may contain complementary information, which can lead to further refined node embeddings. Thus, in this paper, we propose a novel multi-network embedding method, <SmallCap>DMNE</SmallCap>. <SmallCap>DMNE</SmallCap> is flexible. It allows different networks to have different sizes, to be (un)weighted and (un)directed. It leverages multiple networks via cross-network relationships between nodes in different networks, which may form many-to-many node mappings, and be associated with weights. To model the non-linearity of the network data, we develop <SmallCap>DMNE</SmallCap> to have a new deep learning architecture, which coordinates multiple neural networks (one for each input network data) with a co-regularized loss function. With multiple layers of non-linear mappings, <SmallCap>DMNE</SmallCap> progressively transforms each input network to a highly non-linear latent space, and in the meantime, adapts different spaces to each other through a co-regularized learning schema. Extensive experimental results on real-life datasets demonstrate the effectiveness of our method.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Multi-network; Network embedding; Representation learning</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Jingchao Ni, Shiyu Chang, Xiao Liu, Wei Cheng, Haifeng Chen, Dongkuan Xu, and Xiang Zhang. 2018. Co-Regularized Deep Multi-Network Embedding. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186113" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186113</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Networks (or graphs) are pervasive in real-life applications. The rapid growth of information has generated a large volume of network data, such as social networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>], document citation networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], and biological networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. Network data are characterized by the complex dependencies between nodes. To analyze network data, one fundamental problem is to resolve the dependencies and learn low-dimensional vector representation for each node, such that the network structure is preserved in the learned vector space [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. By doing so, network analysis such as node classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>], node clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and link prediction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] can then be readily performed in vector space by using the vast off-the-shelf machine learning algorithms. Usually, learning network representation is also known as network embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. The low-dimensional vectors to be learned are called node embeddings. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186113/images/www2018-122-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Examples of multi-network. In (b) and (c), the dotted lines represent cross-network relationships. The value on each dotted line indicates the weight of the relationship.</span>     </div>    </figure>    </p>    <p>To tackle this problem, many methods have been proposed recently. For example, Laplacian Eigenmaps [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] solves leading eigenvectors of the Laplacian matrix of a graph as node embeddings, which can preserve the direct relationship between nodes in the graph. DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] uses random walks to extract local communities of each node in a network, which are preserved via a word embedding technique called skip-gram [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] optimizes a KL-divergence function to learn embeddings, which can preserve both 1st- and 2nd-order proximities between nodes in a network. node2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] further extends DeepWalk by adopting a biased random walk, so as to preserve both breadth first search (BFS) and depth first search (DFS) based neighborhoods of each node.</p>    <p>Despite the encouraging progress, the focus of most existing methods is single network embedding. In many emerging applications, however, related multiple networks are common to observe. For example, nowadays users are often involved in more than one online social networks. Thus multiple social networks from Facebook, Twitter, LinedIn, etc. are related one another by those common users. Fig. 1(a) illustrates this example, where the three social networks may be collected from different social platforms. Some users in them are common (e.g., U1, U2, U3) while others may be unique (e.g., U7, U8). In this scenario, there is a one-to-one correspondence between users from different networks, which represents the identity of users across different social networks, e.g., U1 represents the same user who appears in all networks.</p>    <p>In some applications, nodes in different networks may represent different entities. Fig. 1(b) shows a different type of multi-network. In this case, multiple networks may contain nodes of different domains, such as text documents, users and color images. Here, text-text links are formed by the hyper-links between different Web documents. Users are involved in a social network. Images co-occurring within the same Web page provide explicit linkages between them. Moreover, users may interact with texts and images by responding to them or clicking on them, which forms the cross-network relationships that interconnect multiple networks, as represented by the dotted lines in Fig. 1(b) . In this scenario, the mappings between nodes in different networks may form a many-to-many correspondence, instead of one-to-one, e.g., one user may interact with multiple Web documents, and vice versa.</p>    <p>Similar examples can also be observed in the information networks of many other fields. In bioinformatics, one important problem is to classify genetic diseases in a disease similarity network [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. In this network, each node is a disease, and each edge depicts the phenotype similarity between two diseases [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>]. To reflect the molecular foundation, we may explore the disease similarity network together with its underlying protein-protein interaction (PPI) network, where disease nodes and protein nodes are related one another via the known disease-protein associations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. The disease-protein associations form a many-to-many mapping. This is because multiple proteins can function synergistically to cause a single disease and a single protein can participate in the formation of multiple diseases.</p>    <p>In practice, because of measurement errors and data access limitations, a single network may contain dummy nodes and false links (i.e., noise), and missing nodes and missing links (i.e., incompleteness). Such defects can largely reduce the learned embedding quality. Whereas, the false or missing information in one network may be corrected in other related networks. Therefore, a promising approach to overcome the limitation of single network embedding is to exploit the compatible and complementary information in multiple networks to refine the embedding quality. Based on this intuition and the prevalence of multi-network data on the web, in this paper, we propose to investigate network embedding in the context of multiple networks.</p>    <p>Fig. 1(c) shows a general example of multi-network that covers the instances in both Fig. 1(a) and 1(b) . There are several characteristics that should be noticed. First, different networks may be about either the same or different sets of nodes, thus may have different sizes. Second, a node in one network may be associated with multiple nodes in another, making the cross-network relationship a many-to-many mapping, which is a generalization of one-to-one mapping. Third, each cross-network relationship may be associated with a weight, which is a generalization of a binary relationship. Fourth, some nodes in one network may not have corresponding node in another, making the cross-network relationship an incomplete, partial mapping.</p>    <p>In the previous social and biological applications, all these characteristics can be observed. For instance, in Fig. 1(b), a user-image relationship may be weighted by the frequency of the interaction. In the biological example, domain experts may specify weights on the disease-protein relationships using their prior knowledge, so as to mark the correlation levels of some disease-protein pairs.</p>    <p>To be practically useful, all the characteristics in Fig. 1(c) should be properly handled, so that refined embeddings can be learned from both instances of multi-network in Fig. 1(a) and 1(b) . So far, few methods have been developed to multi-network embedding problem. Until very recently, there is one method proposed on embedding multi-view network [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>]. However, this method can only be applied to a special case of Fig. 1(c) when different networks are about the same set of nodes, with a strict one-to-one cross-network relationship. Hence, a more flexible method is in demand.</p>    <p>Another vital challenge in our problem is how to model the non-linearity of network data. As shown by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>], the underlying structures of many real-life information networks are highly non-linear, which cannot be fully captured by linear projection approaches such as SVD. To be effective, a preferable approach thus should offer the ability to catch the non-linearity of the data.</p>    <p>Motivated by the powerful representation learning ability of deep learning and its intrinsic non-linearity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>], we propose a novel algorithm, Deep Multi-Network Embedding (<SmallCap>DMNE</SmallCap>), based on a deep learning model. <SmallCap>DMNE</SmallCap> coordinates multiple neural networks (one for each input network data) with a co-regularized loss function to manipulate cross-network relationships, which can be many-to-many, weighted and incomplete. With multiple layers of non-linear functions, <SmallCap>DMNE</SmallCap> can progressively map each input network into a highly non-linear latent space. In the meantime, different latent spaces are adaptive to each other via a joint learning procedure. Our contributions are summarized as follows.</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">We propose to investigate the problem of multi-network embedding in a general context, where multiple networks can be (un)weighted and (un)directed, the cross-network relationships can be many-to-many, weighted and incomplete. This problem finds wide applications in real practice.<br/></li>    <li id="list2" label="&#x2022;">We propose the first deep learning based multi-network embedding algorithm <SmallCap>DMNE</SmallCap>, which not only allows the general multi-network in Fig. 1(c), but also can capture the non-linear structures in multi-network data.<br/></li>    <li id="list3" label="&#x2022;">We design an effective optimization algorithm, which has a solid theoretical guarantee on convergence, and can be easily parallelized to scale to large datasets.<br/></li>    <li id="list4" label="&#x2022;">We perform comprehensive experiments on real-life datasets including document networks, social networks, biological networks. The results demonstrate <SmallCap>DMNE</SmallCap> outperforms recent network embedding methods by a large margin.<br/></li>    </ul>    <p>The rest of the paper is organized as follows. Sec. <a class="sec" href="#sec-9">2</a> gives the problem definition. Sec. <a class="sec" href="#sec-11">3</a> introduces <SmallCap>DMNE</SmallCap> method. Sec. <a class="sec" href="#sec-17">4</a> introduces the optimization solution. Sec. <a class="sec" href="#sec-18">5</a> discusses the experimental results. Sec. <a class="sec" href="#sec-25">6</a> reviews the related work. Sec. <a class="sec" href="#sec-26">7</a> concludes the paper.</p>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Problem and Background</h2>    </div>    </header>    <p>Suppose we have <em>g</em> networks, each is represented by an adjacency matrix <span class="inline-equation"><span class="tex">${\bf G}^{(i)} \in \mathbb {R}_{+}^{n_{i} \times n_{i}}$</span>    </span> (1 &#x2264; <em>i</em> &#x2264; <em>g</em>), where <em>n<sub>i</sub>    </em> denotes the number of nodes in the <em>i</em>-th network. In this paper, our analysis applies to any (un)directed and (un)weighted network. Thus <strong>G</strong>    <sup>(<em>i</em>)</sup> can be either symmetric or asymmetric, and either continued or binary, with <span class="inline-equation"><span class="tex">${\bf G}_{xy}^{(i)}$</span>    </span> indicating the edge weight between nodes <em>x</em> and <em>y</em> in <strong>G</strong>    <sup>(<em>i</em>)</sup>. We denote the set of pairwise cross-network relationships by <span class="inline-equation"><span class="tex">$\mathcal {I} = \lbrace (i,j)\rbrace$</span>    </span>. For example, <span class="inline-equation"><span class="tex">$\mathcal {I} = \lbrace (1,2), (2,3)\rbrace$</span>    </span> contains two cross-network relationships: the relationships between networks <strong>G</strong>    <sup>(1)</sup> and <strong>G</strong>    <sup>(2)</sup>, and the relationships between <strong>G</strong>    <sup>(2)</sup> and <strong>G</strong>    <sup>(3)</sup>. Each pair (<em>i</em>, <em>j</em>) is coupled with a matrix <span class="inline-equation"><span class="tex">${\bf S}^{(ij)} \in \mathbb {R}_{+}^{n_{i} \times n_{j}}$</span>    </span>, with <span class="inline-equation"><span class="tex">${\bf S}_{xy}^{(ij)}$</span>    </span> measuring the weight between node <em>x</em> in <strong>G</strong>    <sup>(<em>i</em>)</sup> and node <em>y</em> in <strong>G</strong>    <sup>(<em>j</em>)</sup>. For clarity, important notations are summarized in Table <a class="tbl" href="#tab1">1</a>.</p>    <div class="table-responsive" id="tab1">    <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Summary of notation.</span>    </div>    <table class="table">     <thead>      <tr>       <th style="text-align:left;">       <strong>Symbol</strong>       </th>       <th style="text-align:left;">       <strong>Meaning</strong>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">       <em>g</em>       </td>       <td style="text-align:left;">The number of networks</td>      </tr>      <tr>       <td style="text-align:left;">       <em>n<sub>i</sub>       </em>       </td>       <td style="text-align:left;">The number of nodes in the <em>i</em>-th network</td>      </tr>      <tr>       <td style="text-align:left;">       <em>d<sub>i</sub>       </em>       </td>       <td style="text-align:left;">The dimensionality of the <em>i</em>-th embedding space</td>      </tr>      <tr>       <td style="text-align:left;">       <em>L<sub>i</sub>       </em>       </td>       <td style="text-align:left;">The number of neural net. layers for the <em>i</em>-th net. data</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>G</strong>       <sup>(<em>i</em>)</sup>       </td>       <td style="text-align:left;">The adjacency matrix of the <em>i</em>-th network</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>A</strong>       <sup>(<em>i</em>)</sup>       </td>       <td style="text-align:left;">The structural context matrix of the <em>i</em>-th network</td>      </tr>      <tr>       <td style="text-align:left;">       <strong>S</strong>       <sup>(<em>ij</em>)</sup>       </td>       <td style="text-align:left;">The relationship matrix between nodes in <strong>G</strong>       <sup>(<em>i</em>)</sup> and <strong>G</strong>       <sup>(<em>j</em>)</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">       <span class="inline-equation"><span class="tex">${\bf \tilde{S}}^{(ij)}$</span>       </span>       </td>       <td style="text-align:left;">The row-normalized version of <strong>S</strong>       <sup>(<em>ij</em>)</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">       <strong>U</strong>       <sup>(<em>i</em>)</sup>       </td>       <td style="text-align:left;">The embedding matrix of the <em>i</em>-th network</td>      </tr>      <tr>       <td style="text-align:left;">       <span class="inline-equation"><span class="tex">$\lbrace {\bf W}_{l}^{(i)}\rbrace _{l=1}^{L_{i}}$</span>       </span>       </td>       <td style="text-align:left;">The weight matrices for the <em>i</em>-th network data</td>      </tr>      <tr>       <td style="text-align:left;">       <span class="inline-equation"><span class="tex">$\lbrace {\bf b}_{l}^{(i)}\rbrace _{l=1}^{L_{i}}$</span>       </span>       </td>       <td style="text-align:left;">The bias vectors for the <em>i</em>-th network data</td>      </tr>      <tr>       <td style="text-align:left;">       <em>&#x03B8;</em>       <sup>(<em>i</em>)</sup>       </td>       <td style="text-align:left;">The model parameters <span class="inline-equation"><span class="tex">$\theta ^{(i)} = \lbrace {\bf W}_{l}^{(i)}, {\bf b}_{l}^{(i)}\rbrace _{l=1}^{L_{i}}$</span>       </span>       </td>      </tr>      <tr>       <td style="text-align:left;">       <span class="inline-equation"><span class="tex">$\mathcal {I}$</span>       </span>       </td>       <td style="text-align:left;">The set of cross-network relationships</td>      </tr>     </tbody>    </table>    </div>    <p>After embedding, each node <em>x</em> in network <strong>G</strong>    <sup>(<em>i</em>)</sup> (1 &#x2264; <em>i</em> &#x2264; <em>g</em>) will obtain a low dimensional vector, i.e., the embedding vector. We use <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)} \in \mathbb {R}^{1 \times d_{i}}$</span>    </span> to represent this vector, where <em>d<sub>i</sub>    </em> is the dimensionality of the embedding space of network <strong>G</strong>    <sup>(<em>i</em>)</sup>, which can be different for different <em>i</em>&#x2019;s. In this work, our goal is to learn embedding vectors of all nodes in all networks, based on the structures of <span class="inline-equation"><span class="tex">$\lbrace {\bf G}^{(i)}\rbrace _{i=1}^{g}$</span>    </span> and the regularizing constraints implicitly represented by the cross-network relationships in <span class="inline-equation"><span class="tex">$\mathcal {I}$</span>    </span>.</p>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Structural Context Extraction</h3>     </div>    </header>    <p>Real-life networks are often so sparse that only using the very limited observed links is insufficient to capture reasonable relationships between nodes. In addition to direct neighbors, nodes in a network also have dependencies with indirect neighbors. Therefore, existing embedding methods usually employ certain sampling strategies to extract local community information as the <em>structural context</em> of each node [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. Among different strategies, random walk is the most widely used because of its intrinsic effectiveness in local clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. In this paper, we follow existing approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] and use random walk with restart (RWR) to obtain structural context of each node.</p>    <p>Given a network <strong>G</strong> of <em>n</em> nodes, a starting node <em>x</em>, we introduce a <em>k</em>-step RWR vector <span class="inline-equation"><span class="tex">${\bf p}^{(k)} \in \mathbb {R}_{+}^{1 \times n}$</span>     </span>, with <span class="inline-equation"><span class="tex">${\bf p}_{y}^{(k)}$</span>     </span> indicates the probability of visiting node <em>y</em> after <em>k</em> step transitions from <em>x</em>. Let <strong>p</strong>     <sup>(0)</sup> be the initial vector with <span class="inline-equation"><span class="tex">${\bf p}_{x}^{(0)}=1$</span>     </span> and all other entries being 0, then a RWR process is defined as [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>] <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} {\bf p}^{(k)} = c{\bf p}^{(k-1)}[({\bf D})^{-1}{\bf G}] + (1-c){\bf p}^{(0)} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <strong>D</strong> is a diagonal matrix with <span class="inline-equation"><span class="tex">${\bf D}_{xx} = \sum _{y=1}^{n}{\bf G}_{xy}$</span>     </span>, and 1 &#x2212; <em>c</em> (0 < <em>c</em> < 1) represents the probability that the random walker will restart from node <em>x</em>.</p>    <p>To capture local information, we follow [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] and use several short-step RWR vectors to define the structural context vector <strong>a</strong> (<span class="inline-equation"><span class="tex">${\bf a} \in \mathbb {R}^{1 \times n}$</span>     </span>) for each node <em>x</em>. <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} {\bf a} = \sum _{k=1}^{K}{\bf p}^{(k)} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <em>K</em> is a small integer indicating the number of considered steps. In practice, <em>K</em> = 3 is usually sufficient. Thus the computation of this step is fast.</p>    <p>After obtaining vectors <strong>a</strong> for all nodes, a <em>structural context matrix</em>     <strong>A</strong> can be formed with each row as <strong>a</strong> for one node. An entry <strong>A</strong>     <sub>      <em>xy</em>     </sub> indicates to what degree a node <em>y</em> will appear in the local community of node <em>x</em>. Using the same approach, we can obtain structural context matrices {<strong>A</strong>     <sup>(1)</sup>, ..., <strong>A</strong>     <sup>(<em>g</em>)</sup>} for all networks {<strong>G</strong>     <sup>(1)</sup>, ..., <strong>G</strong>     <sup>(<em>g</em>)</sup>}. In the following, <span class="inline-equation"><span class="tex">$\lbrace {\bf A}^{(i)}\rbrace _{i=1}^{g}$</span>     </span> will be used as the input of our multi-network embedding method.</p>    </section>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Deep Multi-Network Embedding</h2>    </div>    </header>    <p>In this section, we introduce <SmallCap>DMNE</SmallCap>, a deep model that partially incorporates AutoEncoder [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]. Fig. <a class="fig" href="#fig2">2</a> illustrates the key architecture of <SmallCap>DMNE</SmallCap> for two networks as an example. Here, each input network (i.e., <strong>A</strong>    <sup>(<em>i</em>)</sup>) will be fed into a neural network to minimize the reconstruction error. Meanwhile, the bottleneck layer representations of all input networks, which are the desired node embeddings, are adapted to each other via a co-regularization function (i.e., ED loss or PD loss) that leverages the weighted cross-network relationships (i.e., <strong>S</strong>    <sup>(<em>ij</em>)</sup>). Next, to be self-contained, we first review the key idea of deep AutoEncoder. Then, we propose the two kinds of loss functions to leverage cross-network relationships. Finally, we discuss a speedup strategy and the joint optimization problem.</p>    <figure id="fig2">    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186113/images/www2018-122-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 2:</span>     <span class="figure-title">The architecture of <SmallCap>DMNE</SmallCap> for two networks as an example. <strong>A</strong>      <sup>(1)</sup> and <strong>A</strong>      <sup>(2)</sup> are structural contexts obtained from the two networks. <strong>S</strong>      <sup>(12)</sup> and <strong>S</strong>      <sup>(21)</sup> are weighted cross-network relationships. (<strong>H</strong>      <sup>(<em>i</em>)</sup>)<sub>       <em>l</em>      </sub> contains vectors that represent the latent representations of all nodes at the <em>l</em>-th layer of the <em>i</em>-th network. ED and PD are two kinds of loss functions for cross-network regularization. In <SmallCap>DMNE</SmallCap>, different input networks can have different numbers of neural network layers. Here the two input networks have 5 and 7 layers, respectively.</span>    </div>    </figure>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Single-Network Embedding</h3>     </div>    </header>    <p>For the <em>i</em>-th input network <strong>A</strong>     <sup>(<em>i</em>)</sup>, the neural network consists of <em>L<sub>i</sub>     </em> + 1 layers for performing <em>L<sub>i</sub>     </em> non-linear transformations. The first <em>L<sub>i</sub>     </em>/2 hidden layers are <em>encoders</em> to learn a set of compact representations (i.e., dimension reduction) and the last <em>L<sub>i</sub>     </em>/2 layers are <em>decoders</em> to progressively reconstruct the input. For ease of presentation, we first provide the following definitions. Let <span class="inline-equation"><span class="tex">$({\bf h}_{x}^{(i)})_{0} = {\bf A}_{x*}^{(i)} \in \mathbb {R}^{1 \times n_{i}}$</span>     </span> (i.e., the <em>x</em>-th row of <strong>A</strong>     <sup>(<em>i</em>)</sup>) be an input vector of node <em>x</em> to the first layer and <div class="table-responsive" id="Xeq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} ({\bf h}_{x}^{(i)})_{l} = \sigma (({\bf h}_{x}^{(i)})_{l-1}{\bf W}_{l}^{(i)} + {\bf b}_{l}^{(i)}) \in \mathbb {R}^{1 \times k_{l}} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> be the output of the <em>l</em>-th layer, where <em>l</em> = 1, ..., <em>L<sub>i</sub>     </em>, and <em>&#x03C3;</em>(&#x00B7;) is a non-linear activation function<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. <em>k<sub>l</sub>     </em> denotes the dimensionality of the output at the <em>l</em>-th layer, <span class="inline-equation"><span class="tex">${\bf W}_{l}^{(i)} \in \mathbb {R}^{k_{l-1} \times k_{l}}$</span>     </span> and <span class="inline-equation"><span class="tex">${\bf b}_{l} \in \mathbb {R}^{1 \times k_{l}}$</span>     </span> denote the weight and bias associated with the <em>l</em>-th layer, respectively. Thus, given <span class="inline-equation"><span class="tex">${\bf A}_{x*}^{(i)}$</span>     </span> as the input, <span class="inline-equation"><span class="tex">$({\bf h}_{x}^{(i)})_{L_{i}}$</span>     </span> (i.e., the last layer representation) is the reconstruction of <span class="inline-equation"><span class="tex">${\bf A}_{x*}^{(i)}$</span>     </span>, while <span class="inline-equation"><span class="tex">$({\bf h}_{x}^{(i)})_{L_{i}/2}$</span>     </span> is the desired embedding of node <em>x</em>. Let <span class="inline-equation"><span class="tex">${\bf \hat{A}}^{(i)}$</span>     </span> be an <em>n<sub>i</sub>     </em> by <em>n<sub>i</sub>     </em> matrix with <em>x</em>-th row as <span class="inline-equation"><span class="tex">$({\bf h}_{x}^{(i)})_{L_{i}}$</span>     </span>, the goal of AutoEncoder is to minimize the reconstruction error <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \min _{\theta ^{(i)}}~{\mathcal L}_{ae}^{(i)} = \Vert {\bf A}^{(i)} - {\bf \hat{A}}^{(i)}\Vert _{F}^{2} + \lambda \sum _{l=1}^{L_{i}}\Vert {\bf W}_{l}^{(i)}\Vert _{F}^{2} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\theta ^{(i)} = \lbrace {\bf W}_{l}^{(i)}, {\bf b}_{l}^{(i)}\rbrace _{l=1}^{L_{i}}$</span>     </span> are model parameters. The last &#x2113;<sub>2</sub> norm terms are used to prevent overfitting, and <em>&#x03BB;</em> is the regularization parameter.</p>    <p>Since each row of the input <strong>A</strong>     <sup>(<em>i</em>)</sup> in Eq.&#x00A0;(<a class="eqn" href="#eq2">4</a>) encodes the local community of a node, minimizing the reconstruction error will enforce the learned embeddings to preserve the local neighborhood of each node. This is desirable because neighboring nodes in the network should also be close to each other in the embedding space.</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Cross-Network Regularization</h3>     </div>    </header>    <p>To incorporate the cross-network relationship, the key idea is to add pairwise regularizers to the single-network embedding objective function. We develop two kinds of loss functions to regularize the cross-network embeddings. Both are designed to penalize the embedding inconsistency with the given cross-network relationships. The first loss function, ED loss, considers a simple case when the dimensionality of embeddings are the same in different networks. The second loss function, PD loss, is more flexible and has no such constraint.</p>    <section id="sec-14">     <p><em>3.2.1 Embedding Disagreement (ED) Loss Function.</em> We start with a simple case when the dimensionality of embeddings are the same for different networks, i.e., <em>d</em>      <sub>1</sub> = ... = <em>d<sub>g</sub>      </em> = <em>d</em>. For simplicity, in the following, we use <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}$</span>      </span> to represent the embedding of a node <em>x</em> in network <strong>G</strong>      <sup>(<em>i</em>)</sup>, i.e., <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)} = ({\bf h}_{x}^{(i)})_{L_{i}/2}$</span>      </span>. Intuitively, if a node <em>x</em> in network <strong>G</strong>      <sup>(<em>i</em>)</sup> is mapped to a node <em>y</em> in network <strong>G</strong>      <sup>(<em>j</em>)</sup>, then the embeddings <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}$</span>      </span> and <span class="inline-equation"><span class="tex">${\bf h}_{y}^{(j)}$</span>      </span> should be similar. Now we generalize the relationship to many-to-many. We use <span class="inline-equation"><span class="tex">$\mathcal {N}^{(i\rightarrow j)}(x)$</span>      </span> to denote the set of nodes in <strong>G</strong>      <sup>(<em>j</em>)</sup> that are mapped to <em>x</em> in <strong>G</strong>      <sup>(<em>i</em>)</sup> with positive weights. To penalize the inconsistency of cross-network embeddings, we propose the following loss function. <div class="table-responsive" id="eq3">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \mathcal{L}_{x} = \Vert {\bf h}_{x}^{(i)} - {\bf h}_{x}^{(i\rightarrow j)}\Vert _{F}^{2} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>       </div>      </div> where <div class="table-responsive" id="eq4">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} {\bf h}_{x}^{(i\rightarrow j)} = \frac{1}{\sum _{y \in \mathcal {N}^{(i\rightarrow j)}(x)}{\bf S}_{xy}^{(ij)}}\sum _{y \in \mathcal {N}^{(i\rightarrow j)}(x)}{\bf S}_{xy}^{(ij)}{\bf h}_{y}^{(j)} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>       </div>      </div> is the weighted mean of the embeddings of nodes in network <strong>G</strong>      <sup>(<em>j</em>)</sup> that are mapped to <em>x</em>. Recall that <span class="inline-equation"><span class="tex">${\bf S}_{xy}^{(ij)}$</span>      </span> is the weight on the cross-network relationship between node <em>x</em> in <strong>G</strong>      <sup>(<em>i</em>)</sup> and node <em>y</em> in <strong>G</strong>      <sup>(<em>j</em>)</sup>.</p>     <p>Let <span class="inline-equation"><span class="tex">${\bf \tilde{S}}^{(ij)}$</span>      </span> be row-normalized <strong>S</strong>      <sup>(<em>ij</em>)</sup>. That is <div class="table-responsive" id="Xeq3">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} {\bf \tilde{S}}_{xy}^{(ij)} = \frac{{\bf S}_{xy}^{(ij)}}{\sum _{z=1}^{n_{j}}{\bf S}_{xz}^{(ij)}} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>       </div>      </div> and let <span class="inline-equation"><span class="tex">${\bf H}^{(i)} = [({\bf h}_{1}^{(i)})^{T}, ..., ({\bf h}_{n_{i}}^{(i)})^{T}]^{T} \in \mathbb {R}^{n_{i} \times d_{i}}$</span>      </span>. Then, by summing up Eq.&#x00A0;(<a class="eqn" href="#eq3">5</a>) over all nodes in network <strong>G</strong>      <sup>(<em>i</em>)</sup>, we have the following embedding disagreement (ED) loss function. <div class="table-responsive" id="eq5">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \mathcal{L}_{ed}^{(ij)} = \Vert {\bf O}^{(ij)}{\bf H}^{(i)} - {\bf \tilde{S}}^{(ij)}{\bf H}^{(j)}\Vert _{F}^{2} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>       </div>      </div> where we introduce a diagonal indicator matrix <span class="inline-equation"><span class="tex">${\bf O}^{(ij)} \in \lbrace 0,1\rbrace ^{n_{i}\times n_{i}}$</span>      </span>, with <span class="inline-equation"><span class="tex">${\bf O}_{xx}^{(ij)}=0$</span>      </span> if the <em>x</em>-th row of <span class="inline-equation"><span class="tex">${\bf \tilde{S}}^{(ij)}$</span>      </span> is all-zero; and <span class="inline-equation"><span class="tex">${\bf O}_{xx}^{(ij)}=1$</span>      </span> otherwise.</p>    </section>    <section id="sec-15">     <p><em>3.2.2 Proximity Disagreement (PD) Loss Function.</em> Next, we develop a more flexible loss function. The intuition is based on the following shortcoming of ED loss. In Eq.&#x00A0;(<a class="eqn" href="#eq4">6</a>), we observe that <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i\rightarrow j)}$</span>      </span> is a weighted mean of the embeddings in <span class="inline-equation"><span class="tex">$\mathcal {N}^{(i\rightarrow j)}(x)$</span>      </span>. The ED loss compare <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}$</span>      </span> and <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i\rightarrow j)}$</span>      </span> directly to make them consistent. This is reasonable when the nodes in <span class="inline-equation"><span class="tex">$\mathcal {N}^{(i\rightarrow j)}(x)$</span>      </span> are close to each other within network <strong>G</strong>      <sup>(<em>j</em>)</sup>. When nodes in <span class="inline-equation"><span class="tex">$\mathcal {N}^{(i\rightarrow j)}(x)$</span>      </span> are far from each other (e.g., in different communities), their embeddings should be dissimilar to each other within <strong>G</strong>      <sup>(<em>j</em>)</sup>. However, directly making their mean consistent with <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}$</span>      </span> will enforce them to be similar to each other, which is counterintuitive.</p>     <p>To overcome this problem, the key is to avoid direct comparison between <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}$</span>      </span> and <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i\rightarrow j)}$</span>      </span>. Thus, for each pair of nodes <em>x</em> and <em>z</em> in network <strong>G</strong>      <sup>(<em>i</em>)</sup>, we first measure the proximity between <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}$</span>      </span> and <span class="inline-equation"><span class="tex">${\bf h}_{z}^{(i)}$</span>      </span>, and the proximity between <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i\rightarrow j)}$</span>      </span> and <span class="inline-equation"><span class="tex">${\bf h}_{z}^{(i\rightarrow j)}$</span>      </span>. Then, we measure the disagreement between these two proximity values. Taking Fig. 1(c) as an example. Note node <em>x</em> in network 1 is mapped to node <em>1</em> in network 2. Node <em>z</em> in network 1 is mapped to nodes {<em>2</em>, <em>3</em>} in network 2. Intuitively, if the proximity between the embedding of node <em>1</em> and the mean embedding of nodes {<em>2</em>, <em>3</em>} is small, the proximity between node <em>x</em> and node <em>z</em> should also be small. In this paper, we choose inner product to measure the proximity between two embeddings, e.g., <span class="inline-equation"><span class="tex">${\bf h}_{x}^{(i)}({\bf h}_{z}^{(i)})^{T}$</span>      </span>. Therefore, the cross-network proximity disagreement (PD) loss function is defined as <div class="table-responsive" id="eq6">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026;\mathcal{L}_{pd}^{(ij)} = \sum _{x=1}^{n_{i}}\sum _{z=1}^{n_{i}}[{\bf h}_{x}^{(i)}({\bf h}_{z}^{(i)})^{T} - {\bf h}_{x}^{(i\rightarrow j)}({\bf h}_{z}^{(i\rightarrow j)})^{T}]^{2}\\ &#x0026;= \Vert {\bf O}^{(ij)}{\bf H}^{(i)}({\bf O}^{(ij)}{\bf H}^{(i)})^{T} - {\bf \tilde{S}}^{(ij)}{\bf H}^{(j)}({\bf \tilde{S}}^{(ij)}{\bf H}^{(j)})^{T}\Vert _{F}^{2} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>       </div>      </div>     </p>     <p>It is worth to note that in Eq.&#x00A0;(<a class="eqn" href="#eq6">9</a>), without direct comparison of embeddings, PD loss allows embeddings in different networks <strong>G</strong>      <sup>(<em>i</em>)</sup> to have the different dimensionality <em>d<sub>i</sub>      </em>, which is more flexible than ED loss. Moreover, both ED loss in Eq.&#x00A0;(<a class="eqn" href="#eq5">8</a>) and PD loss in Eq.&#x00A0;(<a class="eqn" href="#eq6">9</a>) can handle many-to-many, weighted and incomplete cross-network relationships as encoded in <span class="inline-equation"><span class="tex">$\lbrace {\bf S}^{(ij)}\rbrace _{(i,j) \in \mathcal {I}}$</span>      </span>.</p>    </section>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Joint Optimization with Speedup Strategy</h3>     </div>    </header>    <p>Next, we further develop our model to allow efficient optimization via stochastic gradient descent (SGD). In Eq.&#x00A0;(<a class="eqn" href="#eq5">8</a>) and (<a class="eqn" href="#eq6">9</a>), all training samples, i.e., the rows of <strong>H</strong>     <sup>(<em>j</em>)</sup>, are coupled together through the multiplication <span class="inline-equation"><span class="tex">${\bf \tilde{S}}^{(ij)}{\bf H}^{(j)}$</span>     </span>, so SGD cannot be applied in sample-wise. As pointed out by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>], to speedup SGD via parallelization and to save memory, the model should allow samples to be divided by minibatch, which means an objective function should be decomposed into sums over training samples. Therefore, we relax Eq.&#x00A0;(<a class="eqn" href="#eq5">8</a>) and (<a class="eqn" href="#eq6">9</a>) by introducing new variables <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>     </span> to replace <span class="inline-equation"><span class="tex">$\lbrace {\bf H}^{(i)}\rbrace _{i=1}^{g}$</span>     </span> such that <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026;\mathcal{L}_{ed}^{(ij)} = \Vert {\bf O}^{(ij)}{\bf U}^{(i)} - {\bf \tilde{S}}^{(ij)}{\bf U}^{(j)}\Vert _{F}^{2}\\ &#x0026;\mathcal{L}_{pd}^{(ij)} = \Vert {\bf O}^{(ij)}{\bf U}^{(i)}({\bf O}^{(ij)}{\bf U}^{(i)})^{T} - {\bf \tilde{S}}^{(ij)}{\bf U}^{(j)}({\bf \tilde{S}}^{(ij)}{\bf U}^{(j)})^{T}\Vert _{F}^{2} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> and introduce a regularizer <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \mathcal{L}_{hu}^{(i)} = \Vert {\bf U}^{(i)} - {\bf H}^{(i)}\Vert _{F}^{2}\\\end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div> to require <strong>U</strong>     <sup>(<em>i</em>)</sup> to be similar to <strong>H</strong>     <sup>(<em>i</em>)</sup>.</p>    <p>Now, we can integrate individual network reconstruction in Eq.&#x00A0;(<a class="eqn" href="#eq2">4</a>), the loss function in Eq.&#x00A0;(<a class="eqn" href="#eq7">10</a>) and the regularizer Eq.&#x00A0;(<a class="eqn" href="#eq8">11</a>) into a unified objective function <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} \min _{\lbrace \theta ^{(i)}, {\bf U}^{(i)}\rbrace _{i=1}^{g}}\mathcal{L} = \sum _{i=1}^{g}\mathcal{L}_{ae}^{(i)} + \alpha \sum _{(i,j) \in \mathcal {I}}\mathcal{L}_{R}^{(ij)} + \beta \sum _{i=1}^{g}\mathcal{L}_{hu}^{(i)} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(12)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathcal{L}_{R}^{(ij)}$</span>     </span> can be either <span class="inline-equation"><span class="tex">$\mathcal{L}_{ed}^{(ij)}$</span>     </span> or <span class="inline-equation"><span class="tex">$\mathcal{L}_{pd}^{(ij)}$</span>     </span>. <span class="inline-equation"><span class="tex">$\lbrace \theta ^{(i)}\rbrace _{i=1}^{g}$</span>     </span> are weights and biases (see Eq.&#x00A0;(<a class="eqn" href="#eq2">4</a>)). <em>&#x03B1;</em>, <em>&#x03B2;</em> &#x2265; 0 are trade-off parameters.</p>    <p>Note in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>), there is no multiplication <span class="inline-equation"><span class="tex">${\bf \tilde{S}}^{(ij)}{\bf H}^{(j)}$</span>     </span>, thus Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>) can be decomposed into sums over rows of <strong>H</strong>     <sup>(<em>i</em>)</sup>, which means <span class="inline-equation"><span class="tex">$\lbrace \theta ^{(i)}\rbrace _{i=1}^{g}$</span>     </span> can be solved efficiently using SGD via parallelization. To solve <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>     </span>, we also develop efficient iterative algorithm, which will be detailed in next section.</p>    <p>Theoretically, Eq.&#x00A0;(<a class="eqn" href="#eq8">11</a>) is the negative log likelihood function of a sampling process <span class="inline-equation"><span class="tex">${\bf U}_{x*}^{(i)}\sim \mathcal {N}({\bf H}_{x*}^{(i)},{\bf I}_{d_{i}}/2)$</span>     </span> where <span class="inline-equation"><span class="tex">${\bf H}_{x*}^{(i)}$</span>     </span> represents the mean of the Gaussian distribution, <span class="inline-equation"><span class="tex">${\bf I}_{d_{i}}$</span>     </span> is a <em>d<sub>i</sub>     </em>-by-<em>d<sub>i</sub>     </em> identity matrix, and <span class="inline-equation"><span class="tex">${\bf U}_{x*}^{(i)}$</span>     </span> represents the sampled embedding of node <em>x</em> in network <strong>G</strong>     <sup>(<em>i</em>)</sup>. Therefore, in our method, we use <strong>U</strong>     <sup>(<em>i</em>)</sup>, instead of <strong>H</strong>     <sup>(<em>i</em>)</sup>, as the learned embeddings of nodes in <strong>G</strong>     <sup>(<em>i</em>)</sup>.</p>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Learning Algorithm</h2>    </div>    </header>    <p>In this section, we develop an alternating minimization algorithm to optimize <em>L</em> in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>). That is, the objective function is alternately minimized w.r.t. one variable while fixing others, until a stationary point is achieved. Next, we provide the solution to <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>    </span> and <span class="inline-equation"><span class="tex">$\lbrace \theta \rbrace _{i=1}^{g}$</span>    </span>, respectively.</p>    <p>    <strong>Learning </strong>    <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>    </span>    <strong>.</strong> Given current <span class="inline-equation"><span class="tex">$\lbrace \theta ^{(i)}\rbrace _{i=1}^{g}$</span>    </span>, we derive a multiplicative updating rule to solve <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>    </span>. The solutions are summarized in the following theorems, which are derived using the Auxiliary Function approach [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. The detailed proofs of the theorems are omitted for brevity, which can be found in an online Supplementary Material<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>.</p>    <p>    <div class="theorem" id="enc1">     <Label>Theorem 1.</Label>     <p> For ED loss, updating <strong>U</strong>      <sup>(<em>i</em>)</sup> by Eq.&#x00A0;(<a class="eqn" href="#eq10">13</a>) monotonically decreases the objective value in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>) until convergence. <div class="table-responsive" id="eq10">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} {\bf U}^{(i)} = {\bf U}^{(i)}\circ \bigg (\frac{\sum _{(i,j) \in \mathcal {I}}\Theta ^{(ij)} + \sum _{(j,i) \in \mathcal {I}}\Lambda ^{(ji)} + \beta {\bf H}^{(i)}}{\sum _{(i,j) \in \mathcal {I}}\Phi ^{(ij)} + \sum _{(j,i) \in \mathcal {I}}\Pi ^{(ji)} + \beta {\bf U}^{(i)}}\bigg)^{\frac{1}{2}} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(13)</span>       </div>      </div> where <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026;\Theta ^{(ij)} = \alpha ({\bf O}^{(ij)})^{T}{\bf \tilde{S}}^{(ij)}{\bf U}^{(j)},~\Phi ^{(ij)} = \alpha ({\bf O}^{(ij)})^{T}{\bf O}^{(ij)}{\bf U}^{(i)}\\ &#x0026;\Lambda ^{(ji)} = \alpha ({\bf \tilde{S}}^{(ji)})^{T}{\bf O}^{(ji)}{\bf U}^{(j)},~\Pi ^{(ji)} = \alpha ({\bf \tilde{S}}^{(ji)})^{T}{\bf \tilde{S}}^{(ji)}{\bf U}^{(i)} \end{aligned}\end{equation} </span>       <br/>       </div>      </div>     </p>    </div>    </p>    <p>    <div class="theorem" id="enc2">     <Label>Theorem 2.</Label>     <p> For PD loss, updating <strong>U</strong>      <sup>(<em>i</em>)</sup> by Eq.&#x00A0;(<a class="eqn" href="#eq11">14</a>) monotonically decreases the objective value in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>) until convergence. <div class="table-responsive" id="eq11">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} {\bf U}^{(i)} = {\bf U}^{(i)}\circ \bigg (\frac{\sum _{(i,j) \in \mathcal {I}}\hat{\Theta }^{(ij)} + \sum _{(j,i) \in \mathcal {I}}\hat{\Lambda }^{(ji)} + \beta {\bf H}^{(i)}}{\sum _{(i,j) \in \mathcal {I}}\hat{\Phi }^{(ij)} + \sum _{(j,i) \in \mathcal {I}}\hat{\Pi }^{(ji)} + \beta {\bf \tilde{U}}^{(i)}}\bigg)^{\frac{1}{4}} \end{aligned} \end{equation} </span>       <br/>       <span class="equation-number">(14)</span>       </div>      </div> where <div class="table-responsive">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026;\hat{\Theta }^{(ij)} = 2\alpha ({\bf O}^{(ij)})^{T}{\bf \tilde{S}}^{(ij)}{\bf U}^{(j)}({\bf \tilde{S}}^{(ij)}{\bf U}^{(j)})^{T}{\bf O}^{(ij)}{\bf \tilde{U}}^{(i)}\\ &#x0026;\hat{\Phi }^{(ij)} = 2\alpha ({\bf O}^{(ij)})^{T}{\bf O}^{(ij)}{\bf U}^{(i)}({\bf U}^{(i)})^{T}({\bf O}^{(ij)})^{T}{\bf O}^{(ij)}{\bf U}^{(i)}\\ &#x0026;\hat{\Lambda }^{(ji)} = 2\alpha ({\bf \tilde{S}}^{(ji)})^{T}{\bf O}^{(ji)}{\bf U}^{(j)}({\bf O}^{(ji)}{\bf U}^{(j)})^{T}{\bf \tilde{S}}^{(ji)}{\bf U}^{(i)}\\ &#x0026;\hat{\Pi }^{(ji)} = 2\alpha ({\bf \tilde{S}}^{(ji)})^{T}{\bf \tilde{S}}^{(ji)}{\bf U}^{(i)}({\bf U}^{(i)})^{T}({\bf \tilde{S}}^{(ji)})^{T}{\bf \tilde{S}}^{(ji)}{\bf U}^{(i)} \end{aligned}\end{equation} </span>       <br/>       </div>      </div>     </p>    </div>    </p>    <p>where &#x25CB;, <span class="inline-equation"><span class="tex">$\frac{[\cdot ]}{[\cdot ]}$</span>    </span>, <span class="inline-equation"><span class="tex">$(\cdot)^{\frac{1}{2}}$</span>    </span> and <span class="inline-equation"><span class="tex">$(\cdot)^{\frac{1}{4}}$</span>    </span> are entry-wise operators.</p>    <p>    <strong>Learning </strong>    <span class="inline-equation"><span class="tex">$\lbrace {\bf W}_{l}^{(i)},{\bf b}_{l}^{(i)}\rbrace _{i=1}^{g}$</span>    </span>    <strong>.</strong> Given current <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>    </span>, we can learn the weight and bias for each layer using the back-propagation (BP) algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]. Here, the key step is to calculate the gradients of <em>L</em> in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>) w.r.t. <span class="inline-equation"><span class="tex">${\bf W}_{l}^{(i)}$</span>    </span> and <span class="inline-equation"><span class="tex">${\bf b}_{l}^{(i)}$</span>    </span>, which are <div class="table-responsive" id="eq12">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} \nabla _{{\bf W}_{l}^{(i)}}\mathcal{L} &#x0026;= ({\bf H}_{l-1}^{(i)})^{T}(\delta _{l}^{(i)} + \beta \Delta _{l}^{(i)}) + \lambda {\bf W}_{l}^{(i)}\\\nabla _{{\bf b}_{l}^{(i)}}\mathcal{L} &#x0026;= ({\bf 1}^{(i)})^{T}(\delta _{l}^{(i)} + \beta \Delta _{l}^{(i)}) \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(15)</span>     </div>    </div> where <div class="table-responsive" id="Xeq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} \delta _{l}^{(i)} &#x0026;= {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}2({\bf \hat{A}}^{(i)}-{\bf A}^{(i)})\circ \sigma ^{\prime }({\bf Z}_{l}^{(i)}) &#x0026; l=L_{i}\\ {} [\delta _{l+1}^{(i)}({\bf W}_{l}^{(i)})^{T}]\circ \sigma ^{\prime }({\bf Z}_{l}^{(i)}) &#x0026; \text{otherwise} \end{array}\right.}\\\Delta _{l}^{(i)} &#x0026;= {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}2({\bf H}^{(i)}-{\bf U}^{(i)})\circ \sigma ^{\prime }({\bf Z}_{l}^{(i)}) &#x0026; l=L_{i}/2\\ {} [\Delta _{l+1}^{(i)}({\bf W}_{l}^{(i)})^{T}]\circ \sigma ^{\prime }({\bf Z}_{l}^{(i)}) &#x0026; l{\lt}L_{i}/2\\ {\bf 0} &#x0026; \text{otherwise} \end{array}\right.} \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(16)</span>     </div>    </div> and <strong>1</strong>    <sup>(<em>i</em>)</sup> is a length-<em>n<sub>i</sub>    </em> column vector with all entries as 1, <em>&#x03C3;</em>&#x2032;(&#x00B7;) is the derivative of <em>&#x03C3;</em>(&#x00B7;), and <span class="inline-equation"><span class="tex">${\bf Z}_{l}^{(i)} = {\bf H}_{l-1}^{(i)}{\bf W}_{l}^{(i)} + {\bf 1}^{(i)}{\bf b}_{l}^{(i)}$</span>    </span>.</p>    <p>    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186113/images/www2018-122-img1.jpg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>    <strong>Summary.</strong> Alg. 1 summarizes our algorithm, which alternates between the updating of <span class="inline-equation"><span class="tex">$\lbrace {\bf U}^{(i)}\rbrace _{i=1}^{g}$</span>    </span> and <span class="inline-equation"><span class="tex">$\lbrace \theta ^{(i)}\rbrace _{i=1}^{g}$</span>    </span>. According to Theorem <a class="enc" href="#enc1">1</a> and <a class="enc" href="#enc2">2</a>, updating <strong>U</strong>    <sup>(<em>i</em>)</sup> monotonically decreases the objective <em>L</em> in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>). Using SGD, updating <span class="inline-equation"><span class="tex">$\lbrace \theta ^{(i)}\rbrace _{i=1}^{g}$</span>    </span> also decreases the objective value with proper learning rate and momentum [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. Because Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>) is bounded below by 0, the alternating algorithm will eventually converge.</p>    <p>Since the key difference between standard BP and our algorithm is the updating of <strong>U</strong>    <sup>(<em>i</em>)</sup>, we analyze the time complexity for Eq.&#x00A0;(<a class="eqn" href="#eq10">13</a>) and (<a class="eqn" href="#eq11">14</a>). Let <em>M</em> be the maximal number of cross-network links in any <strong>S</strong>    <sup>(<em>ij</em>)</sup>, <em>N</em> be the maximal number of nodes in any network, <em>D</em> be the maximal dimensionality of embeddings <em>d<sub>i</sub>    </em>. We can verify that, using sparse matrix multiplication, the complexity of Eq.&#x00A0;(<a class="eqn" href="#eq10">13</a>) and (<a class="eqn" href="#eq11">14</a>) are <em>O</em>((<em>M</em> + <em>N</em>)<em>D</em>) and <em>O</em>(<em>MD</em> + <em>ND</em>    <sup>2</sup>), respectively. In practice, <em>D</em> is a small number, <em>M</em> is often linear in <em>N</em> for sparse networks, thus the actual time complexity can be considered as linear <em>O</em>(<em>N</em>).</p>   </section>   <section id="sec-18">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>    </div>    </header>    <p>In this section, we conduct extensive experiments to evaluate our method. Specifically, we focus on two widely considered applications: multi-label classification and data visualization.</p>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Datasets</h3>     </div>    </header>    <p>We use four publicly available social/information networks with class labels in our experiments, which are detailed in the following. The statistics of the datasets are summarized in Table <a class="tbl" href="#tab2">2</a>.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Statistics of datasets.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Dataset</strong>       </th>       <th style="text-align:left;">        <strong># Networks</strong>       </th>       <th style="text-align:left;">        <strong># Nodes</strong>       </th>       <th style="text-align:left;">        <strong># Links</strong>       </th>       <th style="text-align:left;">        <strong># CrossLinks</strong>       </th>       <th style="text-align:left;">        <strong>LabeledNet.</strong>       </th>       <th style="text-align:left;">        <strong># LabeledNodes</strong>       </th>       <th style="text-align:left;">        <strong># Classes</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">6-NG</td>       <td style="text-align:left;">5</td>       <td style="text-align:left;">4,500</td>       <td style="text-align:left;">16,447</td>       <td style="text-align:left;">66,756</td>       <td style="text-align:left;">All</td>       <td style="text-align:left;">4,500</td>       <td style="text-align:left;">6</td>       </tr>       <tr>       <td style="text-align:left;">9-NG</td>       <td style="text-align:left;">5</td>       <td style="text-align:left;">6,750</td>       <td style="text-align:left;">24,778</td>       <td style="text-align:left;">100,585</td>       <td style="text-align:left;">All</td>       <td style="text-align:left;">6,750</td>       <td style="text-align:left;">9</td>       </tr>       <tr>       <td style="text-align:left;">DP-NET</td>       <td style="text-align:left;">2</td>       <td style="text-align:left;">13,583</td>       <td style="text-align:left;">51,918</td>       <td style="text-align:left;">2,107</td>       <td style="text-align:left;">Disease</td>       <td style="text-align:left;">675</td>       <td style="text-align:left;">18</td>       </tr>       <tr>       <td style="text-align:left;">DBIS</td>       <td style="text-align:left;">2</td>       <td style="text-align:left;">24,535</td>       <td style="text-align:left;">85,184</td>       <td style="text-align:left;">38,035</td>       <td style="text-align:left;">Collaboration</td>       <td style="text-align:left;">2,890</td>       <td style="text-align:left;">4</td>       </tr>       <tr>       <td style="text-align:left;">CiteSeer-M10</td>       <td style="text-align:left;">3</td>       <td style="text-align:left;">15,533</td>       <td style="text-align:left;">56,548</td>       <td style="text-align:left;">11,828</td>       <td style="text-align:left;">Collaboration</td>       <td style="text-align:left;">3,284</td>       <td style="text-align:left;">10</td>       </tr>      </tbody>     </table>    </div>    <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186113/images/www2018-122-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Multi-label classification results of the compared methods on different datasets.</span>     </div>    </figure>     <p>     <strong>20-Newsgroup</strong> dataset<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> contains about 20,000 documents of 20 classes. Following [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], we constructed two kinds of networks: <strong>6-NG</strong> and <strong>9-NG</strong>, which are formed by documents of 6 and 9 different classes, respectively. For brevity, we omit the names of selected classes, which are listed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]. For 6-NG, we generate 5 networks of different sizes. The first network contains randomly sampled 600 documents (100 from each class). For the remaining four networks, the numbers of sampled documents of each class are {125, 150, 175, 200}, forming networks of sizes {750, 900, 1050, 1200}. Using the same approach, 9-NG contains 5 networks of sizes {900, 1125, 1350, 1575, 1800}. Here, link weight is the cosine similarity between the <em>tf-idf</em> vectors of two documents. To reduce noises, we further construct a <em>k</em>-nn graph for each network with <em>k</em> = 5. The cross-network relationship is calculated by cosine similarity between documents from each pair of networks.</p>    <p>     <strong>Disease-protein network (DP-NET)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] consists of a disease network and a protein-protein interaction (PPI) network. The disease network has 5,080 nodes (diseases) and 19,729 links. Each link is weighted by the phenotype similarity between two diseases. The PPI network has 8,503 nodes (proteins) and 32,189 links. Each link has a binary weight, where 1 indicates a functional interaction between two proteins. Moreover, the disease network and the PPI network are interconnected by 2,107 disease-protein association relationships. Here, only 675 diseases are labeled, each in one of 18 classes (disease categories).</p>    <p>     <strong>DBIS</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>] is a social collaboration dataset, which contains a collaboration network of 12,002 nodes (authors) and 37,587 links. Each link is weighted by the number of co-authored papers. It also has a paper-paper similarity network of 12,533 nodes (papers). Each paper is first represented by a <em>tf-idf</em> vector based on its title, then pairwise cosine similarities are calculated using these vectors. To reduce noises, a <em>k</em>-nn graph is constructed for the paper network with <em>k</em> = 5, resulting in 47,597 weighted links. Additionally, collaboration network and paper network are interconnected by 38,035 author-paper publication relationships. Here, 2,890 authors are labeled, each in one of 4 classes (research areas) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>].</p>    <p>     <strong>CiteSeer-M10</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] is a subset of CiteSeerX data. It has a collaboration network of 3,284 nodes (authors) and 13,781 weighted links, and two paper networks: a citation network and a similarity network. The citation network has 2,035 nodes (papers) and 3,356 binary weighted links. Each link indicate a citation relationship. The similarity network is constructed in the same way as DBIS dataset, with 10,214 nodes (papers) and 39,411 weighted links. The collaboration network and paper citation network are interconnected by 2,634 author-paper relationships, and the number is 7,173 between collaboration network and paper similarity network. The two paper networks are interconnected by 2,021 one-to-one correspondence between papers. Here, each author is labeled in one of 10 classes (research areas).</p>    <p>In the first three datasets, the cross-network relationships are many-to-many. CiteSeer-M10 contains two paper networks with different link types, thus has a mixture of one-to-one and many-to-many cross-network relationships. This represents a mixture of the cases in Fig. 1(a) and 1(b), which cannot be handled by existing embedding methods.</p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Comparing Methods</h3>     </div>    </header>    <p>Since no existing method can handle the co-regularized multi-network embedding problem, we compare the proposed <SmallCap>DMNE</SmallCap> with the following eight state-of-the-art network embedding methods:</p>    <p>(1) <strong>Laplacian Eigenmaps (LE)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]: it uses leading eigenvectors of the Laplacian matrix of a network as node embeddings.</p>    <p>(2) <strong>Spectral clustering (Spectral)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]: it also uses eigenvectors, but differs from LE by using a normalized Laplacian matrix.</p>    <p>(3) <strong>DeepWalk</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]: it uses truncated random walk and skip-gram to generate node embeddings.</p>    <p>(4) <strong>LINE</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>]: it minimizes a loss function to learn embeddings that preserve both 1st- and 2nd-order proximity between nodes.</p>    <p>(5) <strong>GraRep</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]: a SVD based embedding method that preserves high-order proximity between nodes.</p>    <p>(6) <strong>node2vec</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]: it extends DeepWalk by using a biased random walk to generate node embeddings.</p>    <p>(7) <strong>DNGR</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]: a stacked AutoEncoder based embedding method that uses the structural contexts in Eq.&#x00A0;(<a class="eqn" href="#eq1">2</a>) as input, and trains the neural network in layer-wise.</p>    <p>(8) <strong>AutoEncoder (AE)</strong> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>]: it also uses the structural contexts in Eq.&#x00A0;(<a class="eqn" href="#eq1">2</a>) as input but trains the neural network as a whole.</p>    <p>There is another AutoEncoder based method SDNE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] which only preserves 1st- and 2nd-order proximity between nodes. We found by using RWR based structural contexts in Eq.&#x00A0;(<a class="eqn" href="#eq1">2</a>), AE outperforms SDNE. Thus we omit SDNE for brevity. Note multi-view network embedding method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] cannot be applied on these datasets since the cross-network relationships are many-to-many.</p>    <p>The parameters of the compared methods are set as follows. LE and Spectral do not have model parameters. For DeepWalk, LINE, GraRep, and node2vec, we set their parameters the same as the optimal settings in their papers. Specifically, DeepWalk uses <em>window size</em> 10, <em>walk length</em> 40, <em>walks per vertex</em> 80; LINE uses <em>learning rate</em> 0.025, <em># negative samples</em> 5, <em># total samples</em> 10 billion. We use its advanced version with both 1st- and 2nd-order of node proximity; GraRep uses <em># transition step</em> 3; node2vec uses <em>window size</em> 10, <em>walk length</em> 80, <em>walks per vertex</em> 10, <em>return p</em> = 1, <em>In-out q</em> = 1. For DNGR, AE and <SmallCap>DMNE</SmallCap>, we follow [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] to set <em>c</em> = 0.98 and <em>K</em> = 3 in Eq.&#x00A0;(<a class="eqn" href="#eq1">2</a>), and set the dimensionality of each layer as below.</p>    <div class="table-responsive" id="tab3">     <table class="table">      <tbody>       <tr>       <td style="text-align:left;">6NG, 9NG</td>       <td style="text-align:left;">        <em>B</em>-200-100-200-<em>B</em>       </td>       </tr>       <tr>       <td style="text-align:left;">DP-NET</td>       <td style="text-align:left;">        <em>B</em>-500-100-500-<em>B</em>       </td>       </tr>       <tr>       <td style="text-align:left;">DBIS</td>       <td style="text-align:left;">        <em>B</em>-1000-500-100-500-1000-<em>B</em>       </td>       </tr>       <tr>       <td style="text-align:left;">CiteSeer-M10</td>       <td style="text-align:left;">author net., paper citation net.: <em>B</em>-500-100-500-<em>B</em>       </td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">paper similarity net.: <em>B</em>-1000-500-100-500-1000-<em>B</em>       </td>       </tr>      </tbody>     </table>    </div>    <p>For 6-NG, 9-NG, DP-NET, DBIS, the dimensionality are set the same for different networks, due to the relatively small change in the sizes of networks. Here, <em>B</em> represents the number of nodes in each network. For example, <em>B</em> = 5, 080 for the disease network in DP-NET, and <em>B</em> = 8, 503 for the PPI network. For all methods, the dimensionality of embeddings are 100. For <SmallCap>DMNE</SmallCap>, the penalty parameter <em>&#x03BB;</em> is set as 10<sup>&#x2212; 4</sup>. The model parameters <em>&#x03B1;</em> and <em>&#x03B2;</em> are set as 1. A study about these model parameters will be discussed later. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186113/images/www2018-122-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Visualization results of the compared methods.</span>      </div>     </figure>    </p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Multi-Label Classification</h3>     </div>    </header>    <p>First, we compare different methods through a classification task. On each dataset, the embeddings are learned from the full data. Then, the embeddings of labeled nodes are used as input to the SVM classifier in LIBLINEAR package [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. When training the classifier, we randomly sample a portion of the labeled nodes as training data and the rest as testing data. The ratio of training data is varied from 10% to 90% for all datasets. The classification accuracy is evaluated using the widely used Macro-F1 and Micro-F1 scores [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. For each method, the prediction experiment is repeated 100 times and the averaged performance is reported.</p>    <p>Fig. <a class="fig" href="#fig3">3</a> shows the results on all datasets. Considering the available labeled data, on 6-NG and 9-NG, the accuracy is averaged over all networks; on DP-NET, the disease network is evaluated; on DBIS and CiteSeer-M10, their collaboration networks are evaluated.</p>    <p>From the figure, we have the following observations. First, <SmallCap>DMNE</SmallCap> significantly outperforms all competitors in terms of both metrics. This is because <SmallCap>DMNE</SmallCap> leverages the complementary information in multiple networks to refine node embeddings, while the baseline methods are subject to the noises and incompleteness in individual networks. Especially, <SmallCap>DMNE</SmallCap> is much better than other methods when the training ratio is small, e.g., 10%, which means <SmallCap>DMNE</SmallCap> is more useful in real practice when the available labels are scarce. This advantage comes from the reinforcement learning of <SmallCap>DMNE</SmallCap> which better uses the available information in multiple networks. On DP-NET, the performance gain of <SmallCap>DMNE</SmallCap> is relatively small. This is because the number of available disease-protein relationships is small, limiting the cross-network reinforcement of embeddings. We also notice, for <SmallCap>DMNE</SmallCap>, PD loss is usually better than ED loss. This verifies our early discussions in Sec. <a class="sec" href="#sec-15">3.2.2</a> about the superiority of PD loss when handling cross-network relationships.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Visualization</h3>     </div>    </header>    <p>To better understand the difference between the compared methods, we visualize their embeddings using the visualization tool t-SNE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>], which projects the learned embeddings of each method to a 2D space. Fig. <a class="fig" href="#fig4">4</a> shows the results on the first network of 6-NG, which has 600 nodes. The colors (or shapes) represent 6 classes.</p>    <p>From the figure, we can observe the eigenvector based methods LE and Spectral cannot effectively identify different classes. Other baseline methods can detect the classes to varying extents. Both <SmallCap>DMNE</SmallCap> (ED) and <SmallCap>DMNE</SmallCap> (PD) perform best as they clearly separate red, cyan and purple classes from each other, with large boundaries. For the other three classes, although all methods have difficulty to separate them, <SmallCap>DMNE</SmallCap>, especially <SmallCap>DMNE</SmallCap> (PD), still detects the boundaries among them. These results further demonstrate the better quality of the embeddings learned by <SmallCap>DMNE</SmallCap>. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186113/images/www2018-122-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Performance evaluation of <SmallCap>DMNE</SmallCap>.</span>      </div>     </figure>    </p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.5</span> Insights of Effectiveness</h3>     </div>    </header>    <p>To get more insights about <SmallCap>DMNE</SmallCap>, we perform experiments to evaluate <SmallCap>DMNE</SmallCap> in detail using 6-NG and 9-NG datasets.</p>    <p>     <strong>Varying cross-network relationships.</strong> First, we divide the pairwise cross-network relationships in 6-NG into 5 equal parts. Each time, we add one part (i.e., 20% relationships) into the data and apply <SmallCap>DMNE</SmallCap>. Fixing the training data ratio at 10%, Fig. 5(a) shows the classification accuracy w.r.t. the ratio of available cross-network relationships. From the figure, both loss functions are effective to enhance the embedding quality as more cross-network relationships are added. Consistent with the results in Fig. <a class="fig" href="#fig3">3</a>, PD loss is superior than ED loss when handling cross-network relationships.</p>    <p>     <strong>Parameter study.</strong> In our model in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>), there are two major parameters <em>&#x03B1;</em> and <em>&#x03B2;</em>. Fig. 5(b) and 5(c) show the classification accuracy on 6-NG by changing one parameter while fixing another as 1, with training data ratio at 10%. As can be seen, both loss functions are stable w.r.t. these parameters, and PD loss is better. For both loss functions, <em>&#x03B1;</em> and <em>&#x03B2;</em> are almost best as 1. Thus it is reasonable to set them at 1 in our experiments. Moreover, the non-zero choices of <em>&#x03B1;</em> and <em>&#x03B2;</em> demonstrate the importance of the regularization terms in our model <em>L</em> in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>).</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Micro-F1 score results on number of layers.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:center;">Dataset</td>       <td colspan="3" style="text-align:center;">        <SmallCap>DMNE</SmallCap> (ED)<hr/>       </td>       <td colspan="3" style="text-align:center;">        <SmallCap>DMNE</SmallCap> (PD)<hr/>       </td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">3 layer</td>       <td style="text-align:center;">5 layer</td>       <td style="text-align:center;">7 layer</td>       <td style="text-align:center;">3 layer</td>       <td style="text-align:center;">5 layer</td>       <td style="text-align:center;">7 layer</td>       </tr>       <tr>       <td style="text-align:center;">6-NG</td>       <td style="text-align:center;">0.8091</td>       <td style="text-align:center;">0.8250</td>       <td style="text-align:center;">        <strong>0.8276</strong>       </td>       <td style="text-align:center;">0.8328</td>       <td style="text-align:center;">        <strong>0.8428</strong>       </td>       <td style="text-align:center;">0.8399</td>       </tr>       <tr>       <td style="text-align:center;">9-NG</td>       <td style="text-align:center;">0.7309</td>       <td style="text-align:center;">0.7850</td>       <td style="text-align:center;">        <strong>0.7939</strong>       </td>       <td style="text-align:center;">0.7948</td>       <td style="text-align:center;">        <strong>0.8224</strong>       </td>       <td style="text-align:center;">0.8188</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Shallow model vs deep model.</strong> To see the effectiveness of <SmallCap>DMNE</SmallCap> in capturing the non-linear structures of multi-network data, we vary the neural network structures by <em>B</em>-100-<em>B</em> (3 layer), <em>B</em>-200-100-200-<em>B</em> (5 layer) and <em>B</em>-200-200-100-200-200-<em>B</em> (7 layer). Table <a class="tbl" href="#tab4">3</a> shows the Micro-F1 scores when training data ratio is 10%. For both ED and PD loss functions, deep models (i.e., 5 or 7 layer) are better than shallow models (3 layer), indicating the importance of using deep structures. For PD loss, we also notice it starts to overfit when the model exceeds 5 layers.</p>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.6</span> Convergence Evaluation</h3>     </div>    </header>    <p>In this section, we study the performance of the proposed <SmallCap>DMNE</SmallCap> algorithm, in terms of the number of iterations before converging to a local optima. Fig. <a class="fig" href="#fig5">5(d)</a> shows the value of the objective function <em>L</em> in Eq.&#x00A0;(<a class="eqn" href="#eq9">12</a>) with respect to the number of iterations on different datasets. From the figure, we observe the objective function value decreases steadily with more iterations. Usually, less than 100 iterations are sufficient for convergence.</p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Related Work</h2>    </div>    </header>    <p>To our best knowledge, this is the first work to study deep multi-network embedding problem. Existing network embedding methods are mostly developed on a single network, such as eigenvector based methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], skip-gram based methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>], SVD based methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] and AutoEncoder based methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. As discussed before, these single network based methods are subject to noises and incompleteness in individual information network.</p>    <p>Recently, several methods have been proposed to embed an attributed network, in which each node is associated with an attribute vector [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>]. Their key idea is to integrate a dimension reduction component of attribute vectors into a network embedding framework to leverage the complementary information in node attributes and network structure. Despite their success in using node attributes to improve performance, these methods never consider multiple networks.</p>    <p>There are also some methods on embedding heterogeneous information network (HIN) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. In these methods, an HIN is a special case of our multi-network in Fig. 1(c) in two aspects. First, they ignore the scenario when any two networks are about the same nodes but have different topological structures, i.e., the case in Fig. 1(a) . Second, they neglect edge weights, either within-network or cross-network (as shown by their mathematical formations). More specifically, the method in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] was proposed on an HIN with node attributes. Its prerequisite is that each node must be associated an attribute vector, otherwise it cannot learn node embeddings from network structure only. However, in many applications, we only have link information, which necessitates methods on embedding network structures. The method in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>], on the other hand, requires users to specify meta-paths as its input, which is hard to choose in practice due to the absence of gold standard. This makes it hard to generalize to any applications. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], there is a text based HIN embedding method, which, however, is strictly designed for word-word, word-doc and word-label networks. It requires label information to be provided because it is a semi-supervised method. Then, how to generalize it to multiple networks of any shape for unsupervised learning is unknown. Clearly, all these methods have strong limitations, preventing them from being applied to our problem. More importantly, the goal of HIN based methods is to resolve the semantic meanings of different types of links, rather than using the complementary information in multiple networks to refine embedding quality. Therefore, there is a distinct difference between HIN based methods and multi-network based methods.</p>    <p>The multi-view network embedding method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] may be the most relevant work, but, as discussed before in Sec. <a class="sec" href="#sec-8">1</a>, it cannot be applied when the cross-network relationship is many-to-many, weighted and incomplete, as shown in Fig. 1(c) .</p>    <p>Our method is also inspired by traditional multi-view and multi-graph learning methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>], which aim to integrate multiple data sources in a certain task, such as instance clustering, to obtain performance gain. However, these methods are not designed for multi-network embedding, and none of them uses deep model to exploit the non-linear structures of the network data.</p>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion</h2>    </div>    </header>    <p>Integrating the rich social and information networks on the web is important to improve the robustness of representation learning methods. In this paper, we propose a flexible co-regularized deep multi-network embedding algorithm <SmallCap>DMNE</SmallCap>, which is developed on a very practical scenario of multi-network, thus is widely applicable. <SmallCap>DMNE</SmallCap> manipulates cross-network relationships to reinforce the learning of node embeddings in different networks. Its deep architecture also provides the ability to capture the highly non-linear structures of multiple networks. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-27">    <header>    <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>    </div>    </header>    <p>This work was partially supported by the National Science Foundation grants IIS-1664629 and CAREER.</p>   </section>   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Mikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps for dimensionality reduction and data representation. <em>      <em>Neural Comput.</em>     </em>15, 6 (2003), 1373&#x2013;1396.</li>    <li id="BibPLXBIB0002" label="[2]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In <em>      <em>CIKM</em></em>. ACM, 891&#x2013;900.</li>    <li id="BibPLXBIB0003" label="[3]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for learning graph representations. In <em>      <em>AAAI</em></em>. 1145&#x2013;1152.</li>    <li id="BibPLXBIB0004" label="[4]">Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu&#x00A0;C Aggarwal, and Thomas&#x00A0;S Huang. 2015. Heterogeneous network embedding via deep architectures. In <em>      <em>KDD</em></em>. ACM, 119&#x2013;128.</li>    <li id="BibPLXBIB0005" label="[5]">Wei Cheng, Xiang Zhang, Zhishan Guo, Yubao Wu, Patrick&#x00A0;F Sullivan, and Wei Wang. 2013. Flexible and robust co-regularized multi-domain graph clustering. In <em>      <em>KDD</em></em>. ACM, 320&#x2013;328.</li>    <li id="BibPLXBIB0006" label="[6]">Yuxiao Dong, Nitesh&#x00A0;V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable representation learning for heterogeneous networks. In <em>      <em>KDD</em></em>. ACM, 135&#x2013;144.</li>    <li id="BibPLXBIB0007" label="[7]">Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. <em>      <em>J. Mach. Learn. Res.</em>     </em>9, Aug (2008), 1871&#x2013;1874.</li>    <li id="BibPLXBIB0008" label="[8]">Kwang-Il Goh, Michael&#x00A0;E Cusick, David Valle, Barton Childs, Marc Vidal, and Albert-L&#x00E1;szl&#x00F3; Barab&#x00E1;si. 2007. The human disease network. <em>      <em>Proc. Natl. Acad. Sci. U.S.A.</em>     </em>104, 21 (2007), 8685&#x2013;8690.</li>    <li id="BibPLXBIB0009" label="[9]">Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In <em>      <em>KDD</em></em>. ACM, 855&#x2013;864.</li>    <li id="BibPLXBIB0010" label="[10]">Geoffrey&#x00A0;E Hinton and Ruslan&#x00A0;R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. <em>      <em>Science</em>     </em>313, 5786 (2006), 504&#x2013;507.</li>    <li id="BibPLXBIB0011" label="[11]">Xiao Huang, Jundong Li, and Xia Hu. 2017. Accelerated attributed network embedding. In <em>      <em>SDM</em></em>. SIAM, 633&#x2013;641.</li>    <li id="BibPLXBIB0012" label="[12]">Xiao Huang, Jundong Li, and Xia Hu. 2017. Label informed attributed network embedding. In <em>      <em>WSDM</em></em>. ACM, 731&#x2013;739.</li>    <li id="BibPLXBIB0013" label="[13]">TaeHyun Hwang, Gowtham Atluri, MaoQiang Xie, Sanjoy Dey, Changjin Hong, Vipin Kumar, and Rui Kuang. 2012. Co-clustering phenome&#x2013;genome for phenotype classification and disease gene discovery. <em>      <em>Nucleic Acids Res.</em>     </em>40, 19 (2012), e146&#x2013;e146.</li>    <li id="BibPLXBIB0014" label="[14]">Ming Ji, Jiawei Han, and Marina Danilevsky. 2011. Ranking-based classification of heterogeneous information networks. In <em>      <em>KDD</em></em>. ACM, 1298&#x2013;1306.</li>    <li id="BibPLXBIB0015" label="[15]">Alex Krizhevsky, Ilya Sutskever, and Geoffrey&#x00A0;E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In <em>      <em>NIPS</em></em>. 1097&#x2013;1105.</li>    <li id="BibPLXBIB0016" label="[16]">Abhishek Kumar, Piyush Rai, and Hal Daume. 2011. Co-regularized multi-view spectral clustering. In <em>      <em>NIPS</em></em>. 1413&#x2013;1421.</li>    <li id="BibPLXBIB0017" label="[17]">Daniel&#x00A0;D Lee and H&#x00A0;Sebastian Seung. 2001. Algorithms for non-negative matrix factorization. In <em>      <em>NIPS</em></em>. 556&#x2013;562.</li>    <li id="BibPLXBIB0018" label="[18]">Kar&#x00A0;Wai Lim and Wray Buntine. 2015. Bibliographic analysis with the citation network topic model. In <em>      <em>ACML</em></em>. 142&#x2013;158.</li>    <li id="BibPLXBIB0019" label="[19]">Kar&#x00A0;Wai Lim and Wray Buntine. 2016. Bibliographic analysis on research publications using authors, categorical labels and the citation network. <em>      <em>Machine Learning</em>     </em>103, 2 (2016), 185&#x2013;213.</li>    <li id="BibPLXBIB0020" label="[20]">Laurens van&#x00A0;der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. <em>      <em>J. Mach. Learn. Res.</em>     </em>9, Nov (2008), 2579&#x2013;2605.</li>    <li id="BibPLXBIB0021" label="[21]">Sofus&#x00A0;A Macskassy and Foster Provost. 2007. Classification in networked data: A toolkit and a univariate case study. <em>      <em>J. Mach. Learn. Res.</em>     </em>8, May (2007), 935&#x2013;983.</li>    <li id="BibPLXBIB0022" label="[22]">Julian McAuley and Jure Leskovec. 2012. Learning to discover social circles in ego networks. In <em>      <em>NIPS</em></em>. 539&#x2013;547.</li>    <li id="BibPLXBIB0023" label="[23]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&#x00A0;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em>      <em>NIPS</em></em>. 3111&#x2013;3119.</li>    <li id="BibPLXBIB0024" label="[24]">Jingchao Ni, Wei Cheng, Wei Fan, and Xiang Zhang. 2018. ComClus: A self-grouping framework for multi-network clustering. <em>      <em>IEEE Trans. Knowl. Data Eng.</em>     </em>30, 3 (2018), 435&#x2013;448.</li>    <li id="BibPLXBIB0025" label="[25]">Jingchao Ni, Hanghang Tong, Wei Fan, and Xiang Zhang. 2014. Inside the atoms: ranking on a network of networks. In <em>      <em>KDD</em></em>. ACM, 1356&#x2013;1365.</li>    <li id="BibPLXBIB0026" label="[26]">Jingchao Ni, Hanghang Tong, Wei Fan, and Xiang Zhang. 2015. Flexible and robust multi-network clustering. In <em>      <em>KDD</em></em>. ACM, 835&#x2013;844.</li>    <li id="BibPLXBIB0027" label="[27]">Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric transitivity preserving graph embedding. In <em>      <em>KDD</em></em>. ACM, 1105&#x2013;1114.</li>    <li id="BibPLXBIB0028" label="[28]">Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, and Yang Wang. 2016. Tri-party deep network representation. In <em>      <em>IJCAI</em></em>. 1895&#x2013;1901.</li>    <li id="BibPLXBIB0029" label="[29]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In <em>      <em>KDD</em></em>. ACM, 701&#x2013;710.</li>    <li id="BibPLXBIB0030" label="[30]">Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, and Jiawei Han. 2017. An Attention-based Collaboration Framework for Multi-View Network Representation Learning. In <em>      <em>CIKM</em></em>. ACM, 1767&#x2013;1776.</li>    <li id="BibPLXBIB0031" label="[31]">Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. <em>      <em>IEEE Trans. Pattern Anal. Mach. Intell.</em>     </em>22, 8 (2000), 888&#x2013;905.</li>    <li id="BibPLXBIB0032" label="[32]">Daniel&#x00A0;A Spielman and Shang-Hua Teng. 2004. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In <em>      <em>STOC</em></em>. ACM, 81&#x2013;90.</li>    <li id="BibPLXBIB0033" label="[33]">Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Predictive text embedding through large-scale heterogeneous text networks. In <em>      <em>KDD</em></em>. ACM, 1165&#x2013;1174.</li>    <li id="BibPLXBIB0034" label="[34]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In <em>      <em>WWW</em></em>. International World Wide Web Conferences Steering Committee, 1067&#x2013;1077.</li>    <li id="BibPLXBIB0035" label="[35]">Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In <em>      <em>KDD</em></em>. ACM, 990&#x2013;998.</li>    <li id="BibPLXBIB0036" label="[36]">Joshua&#x00A0;B Tenenbaum, Vin De&#x00A0;Silva, and John&#x00A0;C Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. <em>      <em>Science</em>     </em>290, 5500 (2000), 2319&#x2013;2323.</li>    <li id="BibPLXBIB0037" label="[37]">Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. 2014. Learning deep representations for graph clustering. In <em>      <em>AAAI</em></em>. 1293&#x2013;1299.</li>    <li id="BibPLXBIB0038" label="[38]">Hanghang Tong, Christos Faloutsos, and Jia-yu Pan. 2006. Fast random walk with restart and its applications. In <em>      <em>ICDM</em></em>. IEEE, 613&#x2013;622.</li>    <li id="BibPLXBIB0039" label="[39]">Marc&#x00A0;A Van&#x00A0;Driel, Jorn Bruggeman, Gert Vriend, Han&#x00A0;G Brunner, and Jack&#x00A0;AM Leunissen. 2006. A text-mining analysis of the human phenome. <em>      <em>Eur. J. Hum. Genet.</em>     </em>14, 5 (2006), 535.</li>    <li id="BibPLXBIB0040" label="[40]">Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In <em>      <em>KDD</em></em>. ACM, 1225&#x2013;1234.</li>    <li id="BibPLXBIB0041" label="[41]">Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. 2015. On deep multi-view representation learning. In <em>      <em>ICML</em></em>. 1083&#x2013;1092.</li>    <li id="BibPLXBIB0042" label="[42]">Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward&#x00A0;Y Chang. 2015. Network representation learning with rich text information. In <em>      <em>IJCAI</em></em>. 2111&#x2013;2117.</li>    <li id="BibPLXBIB0043" label="[43]">Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. 2016. Homophily, structure, and content augmented network representation learning. In <em>      <em>ICDM</em></em>. IEEE, 609&#x2013;618.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>In this work, we use sigmoid function <span class="inline-equation"><span class="tex">$\sigma (x)=\frac{1}{1+exp(-x)}$</span>    </span>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a href="https://nijingchao.github.io/dmnesup/dmnesup.pdf" target="_blank">https://nijingchao.github.io/dmnesup/dmnesup.pdf</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a href="http://qwone.com/%7Ejason/20Newsgroups/" target="_blank">http://qwone.com/%7Ejason/20Newsgroups/</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW' 18, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186113">https://doi.org/10.1145/3178876.3186113</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
