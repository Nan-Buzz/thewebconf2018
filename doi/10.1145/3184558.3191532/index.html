<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Construction and Applications of TeKnowbase – A Knowledge
  Base of Computer Science Concepts</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191532'>https://doi.org/10.1145/3184558.3191532</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191532'>https://w3id.org/oa/10.1145/3184558.3191532</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Construction and Applications of
          TeKnowbase – A Knowledge Base of Computer Science
          Concepts</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Prajna</span> <span class=
          "surName">Upadhyay</span>, Indian Institute of
          Technology, New Delhi, Delhi, <a href=
          "mailto:prajna.upadhyay@cse.iitd.ac.in">prajna.upadhyay@cse.iitd.ac.in</a>
        </div>
        <div class="author">
          <span class="givenName">Ashutosh</span> <span class=
          "surName">Bindal</span>, Indian Institute of Technology,
          New Delhi, Delhi, <a href=
          "mailto:ashutoshbindal@gmail.com">ashutoshbindal@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Manjeet</span> <span class=
          "surName">Kumar</span>, Indian Institute of Technology,
          New Delhi, Delhi, <a href=
          "mailto:manjeetk9497@gmail.com">manjeetk9497@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Maya</span> <span class=
          "surName">Ramanath</span>, Indian Institute of
          Technology, New Delhi, Delhi, <a href=
          "mailto:ramanath@cse.iitd.ac.in">ramanath@cse.iitd.ac.in</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191532"
        target=
        "_blank">https://doi.org/10.1145/3184558.3191532</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>In this paper, we make two main contributions.
        First, we describe the construction and evaluation of
        TeKnowbase, a knowledge-base of technical concepts in
        computer science. And second, we show how to use TeKnowbase
        in a variety of applications, including, generation of
        pre-requisite concepts for learning a new topic,
        classification of technical text and querying and ranking
        computer science articles.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <em>Ontologies;</em> <em>Information
        extraction;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>knowledge
          base</small>,</span> <span class=
          "keyword"><small>technical concept</small>,</span>
          <span class=
          "keyword"><small>pre-requisites</small>,</span>
          <span class="keyword"><small>ranking</small>,</span>
          <span class=
          "keyword"><small>classification</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Prajna Upadhyay, Ashutosh Bindal, Manjeet Kumar, and Maya
          Ramanath. 2018. Construction and Applications of
          TeKnowbase – A Knowledge Base of Computer Science
          Concepts. In <em>WWW '18 Companion: The 2018 Web
          Conference Companion,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 9 Pages.
          <a href="https://doi.org/10.1145/3184558.3191532" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3191532</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>With advances in information extraction research, and the
      availability of large amounts of structured and unstructured
      (textual) data, <em>automatic</em> construction of
      knowledge-bases are not only possible, but also desirable
      because of the coverage they can offer. There are already
      many such general-purpose knowledge-bases such as Yago
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>] and
      DBPedia [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0011">11</a>].
      Moreover, projects such as OpenIE [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>] and NELL [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0003">3</a>] aim to extract information
      from unstructured textual sources on a large scale. However,
      there is a paucity of high quality and <em>specialized</em>
      KBs for specific domains. For some domains, for example, for
      the bio-medical domain, there are well-curated ontologies
      which partially address this gap (see, for example, the Gene
      Ontology project [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>]). However, for domains such as
      Computer Science or IT in general, where such curation
      efforts are hard and the field itself is rapidly growing, it
      becomes critical to revisit the automatic construction
      processes that take advantage of domain-specific
      resources.</p>
      <p>In this paper, we describe the construction of a
      <em>technical</em> knowledge-base (TKB) of computer science
      concepts, called <em>TeKnowbase</em> from Wikipedia,
      technical websites and online textbooks. As with any
      general-purpose KBs, TeKnowbase can be used in applications
      such as classification, disambiguation, entity linking,
      semantic search, etc., of <em>computer science</em>
      resources. However, in addition to these “standard”
      applications, TeKnowbase can be used in the domain of
      education for problems which are specific to it. One such
      problem is the generation of pre-requisite concepts. Briefly,
      this problem arises when a student wants to learn about a new
      concept, but is unsure of what pre-requisites are required.
      The goal is to develop a system that automatically generates
      the required pre-requisites. As we illustrate in Section
      <a class="sec" href="#sec-22">3.1</a>, using TeKnowbase as a
      resource to generate these pre-requisites results in improved
      coverage and accuracy. In addition to pre-requisite
      generation, TeKnowbase can be used as a resource for
      automatic question generation (to generate, for example,
      multiple choice or fill-in-the-blanks questions) to test the
      student's understanding of a subject. The utility of
      TeKnowbase in these kinds of education-specific applications
      makes it a valuable resource.</p>
      <section id="sec-6">
        <section id="sec-7">
          <p><em>Construction methodology.</em> In order to
          construct TeKnowbase, we first acquired a dictionary of
          concepts and entities relevant to computer science
          (Section <a class="sec" href="#sec-11">2.1</a>). Using
          this dictionary, we extracted relationships among them
          (Section <a class="sec" href="#sec-12">2.2</a>). We make
          use of the semantic web standard, RDF, where information
          is represented as triples of the form
          ⟨subject⟩⟨predicate⟩⟨object⟩, to represent these
          relationships. The ⟨subject⟩ and the ⟨object⟩ are
          entities and the ⟨predicate⟩ represents the relation. In
          a nutshell, each triple makes a statement about the
          ⟨subject⟩. Table <a class="tbl" href="#tab1">1</a> shows
          examples of the kind of triples we extract and the number
          of such triples in our knowledge-base.</p>
          <div class="table-responsive" id="tab1">
            <div class="table-caption">
              <span class="table-number">Table 1:</span>
              <span class="table-title">Statistics for and examples
              of a subset of relationships extracted. The The first
              set of 5 relations were extracted from structured
              sources (see Section <a class="sec" href=
              "#sec-13">2.2.1</a>) and the second part with 3
              relations from unstructured textual sources (see
              Section <a class="sec" href=
              "#sec-16">2.2.2</a>).</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">Relation</td>
                  <td style="text-align:left;">Examples of
                  ⟨<tt>head_entity, tail_entity</tt>⟩</td>
                  <td style="text-align:left;"># Triples</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><tt>type</tt></td>
                  <td style="text-align:left;">
                  ⟨<tt>topological_sorting,
                  graph_algorithm</tt>⟩</td>
                  <td style="text-align:left;">27,078</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <tt>concept</tt></td>
                  <td style="text-align:left;">
                  ⟨<tt>nash_equilibrium, game_theory</tt>⟩</td>
                  <td style="text-align:left;">595</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <tt>subTopic</tt></td>
                  <td style="text-align:left;">⟨<tt>hamming_code,
                  algebraic_coding_theory</tt>⟩</td>
                  <td style="text-align:left;">2,026</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <tt>application</tt></td>
                  <td style="text-align:left;">⟨<tt>group_testing,
                  coding_theory</tt>⟩</td>
                  <td style="text-align:left;">324</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <tt>terminology</tt></td>
                  <td style="text-align:left;">⟨<tt>blob_detection,
                  image_Processing</tt>⟩</td>
                  <td style="text-align:left;">27,018</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <tt>is_a_high-speed_form_of</tt></td>
                  <td style="text-align:left;">
                  ⟨<tt>gigabig_ethernet, ethernet</tt>⟩</td>
                  <td style="text-align:left;">n/a</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <tt>is_an_adaptation_of</tt></td>
                  <td style="text-align:left;">⟨<tt>ironpython,
                  python</tt>⟩</td>
                  <td style="text-align:left;">n/a</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><tt>uses</tt></td>
                  <td style="text-align:left;">⟨<tt>utc,
                  gregorian_calendar</tt>⟩</td>
                  <td style="text-align:left;">n/a</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-8">
          <header>
            <div class="title-info">
              <h4>Availability of TeKnowbase.</h4>
            </div>
          </header>
          <p>TeKnowbase is made available under the Creative
          Commons Attribution Licence 3.0 and can be downloaded
          from <a class="link-inline force-break" href=
          "https://github.com/prajnaupadhyay/TeKnowbase">https://github.com/prajnaupadhyay/TeKnowbase</a>.
          Some statistics about TeKnowbase are listed in Table
          <a class="tbl" href="#tab2">2</a>. Line 1 in Table
          <a class="tbl" href="#tab2">2</a> reports the number of
          unique entities as well as the variations such as
          plurals. Entities that we have in common with DBPedia and
          Freebase are linked using the <tt>owl:sameAs</tt>
          relation.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span>
              <span class="table-title">Statistics from TeKnowbase.
              Apart from 70,285 unique entities, there are 32,458
              variations (disambiguated as well as stemmed
              versions) of them.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">No. of unique
                  entities</td>
                  <td style="text-align:right;">70,285</td>
                </tr>
                <tr>
                  <td style="text-align:left;">No. of unique
                  relations</td>
                  <td style="text-align:right;">2,574</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Taxonomic relations
                  (<tt>typeOf</tt>)</td>
                  <td style="text-align:right;">27,078</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Total no. of
                  triples</td>
                  <td style="text-align:right;">146,657</td>
                </tr>
                <tr>
                  <td style="text-align:left;">No. of overlapping
                  entities with DBPedia</td>
                  <td style="text-align:right;">17,987</td>
                </tr>
                <tr>
                  <td style="text-align:left;">No. of overlapping
                  entities with Freebase</td>
                  <td style="text-align:right;">34,785</td>
                </tr>
                <tr>
                  <td style="text-align:left;">No. of triples
                  extracted from Wikipedia</td>
                  <td style="text-align:right;">99,357</td>
                </tr>
                <tr>
                  <td style="text-align:left;">No. of triples
                  extracted from Unstructured sources</td>
                  <td style="text-align:right;">3,506</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Each entity in TeKnowbase is associated with a URI.
          The URI, consists of the URL from which the entity was
          extracted. The URL is typically a page dedicated to
          describing the entity. Further, since each triple in
          TeKnowbase is derived through a series of heuristics
          (described in Sections <a class="sec" href=
          "#sec-11">2.1</a> and <a class="sec" href=
          "#sec-12">2.2</a>), we maintain the entire provenance of
          the triple. This data is available on request.</p>
        </section>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">1.1</span>
            Contributions and Organization</h3>
          </div>
        </header>
        <p>Our contributions are as follows.</p>
        <ol class="list-no-style">
          <li id="list1" label="(1)">We describe the construction
          of TeKnowbase in Sections <a class="sec" href="#sec-11">
            2.1</a> and <a class="sec" href="#sec-12">2.2</a>. We
            conducted a thorough evaluation of the quality of
            TeKnowbase and we report our results in Section
            <a class="sec" href="#sec-18">2.3</a>. TeKnowbase is a
            freely available online resource.<br />
          </li>
          <li id="list2" label="(2)">We describe 3 applications of
          TeKnowbase: i) Generation of pre-requisite concepts, ii)
          classification of posts in Stack Overflow (a computer
          science forum), and, iii) ranking of computer science
          research articles for keyword search. We show through our
          evaluations that using TeKnowbase as a resource can
          improve the accuracy of all 3 applications.<br /></li>
        </ol>
        <p>Our paper is organized as follows. We describe the
        construction of TeKnowbase in Sections <a class="sec" href=
        "#sec-11">2.1</a> and <a class="sec" href=
        "#sec-12">2.2</a>. We present three applications of
        TeKnowbase in Section <a class="sec" href="#sec-21">3</a>.
        We present an evaluation of our knowledge-base in Section
        <a class="sec" href="#sec-18">2.3</a>. We briefly describe
        related work in Section <a class="sec" href="#sec-38">4</a>
        and conclude in Section <a class="sec" href=
        "#sec-39">5</a>.</p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Construction of
          TeKnowbase</h2>
        </div>
      </header>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Acquiring a
            list of entities</h3>
          </div>
        </header>
        <p>We extracted nearly 78,000 entities. We used Wikipedia's
        article titles as well as it's category system as a source
        of concepts. Our corpus of Wikipedia articles consists of
        all articles under the category <tt>Computing</tt>. In all,
        there were approximately 54,000 articles. The title of each
        article was considered an entity. Examples entities we
        found were <tt>Heap_Sort</tt>,
        <tt>Naive-Bayes_Classifier</tt>, etc. While Wikipedia has
        articles on a number of technical entities and concepts, it
        is not exhaustive. For example, the terms
        <tt>average_page_depth</tt> (related to Web Analytics) and
        <tt>fraction_ridge</tt> (related to biometrics) could not
        be found in Wikipedia, and therefore, we looked for several
        online resources to augment our entity list.</p>
        <p>Our second set of resources were two websites,
        Webopedia<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a> and TechTarget<a class="fn"
        href="#fn2" id="foot-fn2"><sup>2</sup></a>. Each website
        consists of a number of technical terms and their
        definitions in a specific format. By writing appropriate
        wrappers for both these websites, we extracted
        approximately 24,000 entities. Finally, we extracted 16,500
        entities from the <em>indexes</em> of online textbooks (8
        online textbooks were used). We used edit distance to
        resolve overlapping entities from these sources and ended
        up retaining over 70,000 entities.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Acquiring
            relationships between entities</h3>
          </div>
        </header>
        <p>We divided our relationship extraction task into two
        parts. First, extraction from structured sources, and
        second, extraction from unstructured, textual sources. Our
        goal was two-fold: to extract as many different kinds of
        relationships as possible as well as construct a taxonomy
        of entities/concepts.</p>
        <section id="sec-13">
          <p><em>2.2.1 Structured sources.</em> Since our aim was
          to construct a technical knowledge-base, we manually made
          a list of relations that our knowledge-base should
          contain – this is our list of known relations. The
          relations included the taxonomic relation <tt>typeOf</tt>
          (as in, ⟨<tt>jpeg typeOf file_format</tt>⟩) and other
          interesting relationships such as <tt>algorithmFor</tt>,
          <tt>subTopicOf</tt>, <tt>applicationOf</tt>,
          <tt>techniqueFor</tt>, etc. In all, we identified 18
          relationships that we felt were interesting and
          formulated techniques to extract them from Wikipedia.</p>
          <figure id="fig1">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3200000/3191532/images/www18companion-271-fig1.jpg"
            class="img-responsive" alt="Figure 1" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 1:</span>
              <span class="figure-title">Snippet of different
              structured sources.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-14">
          <p><em>Overview pages.</em> (500 pages): We made use of
          two kinds of structured pages – “List” pages and
          “Outline” pages (for example, the pages, <tt>List of
          machine learning concepts</tt>, <tt>Outline of
          Cryptography</tt>, etc.). These pages organize lists of
          entities with headings and sub-headings. Extracting this
          information gives us the relations <tt>typeOf</tt> and
          <tt>subTopicOf</tt> with good accuracy (see Section
          <a class="sec" href="#sec-18">2.3</a> for an evaluation
          of these relations). Figure <a class="fig" href=
          "#fig1">1</a> shows an example of a list page for data
          structures. We see a list of terms under a heading and
          can extract triples of the form ⟨<tt>doubly_linked_list
          typeof list</tt>⟩. Further we were able to extract
          <em>taxonomic hierarchies</em> of two levels by relating
          the headings to the article title. Continuing the
          previous example, ⟨<tt>list typeOf data_Structure</tt>⟩
          was extracted based on the article title.</p>
        </section>
        <section id="sec-15">
          <p><em>Articles on specific topics:.</em> These pages
          refer to discussion on specific topics such as, say,
          “Coding Theory”. These pages consist of many structured
          pieces of information as follows:</p>
          <p><strong>The table of contents (TOC)</strong> (1838
          TOCs): From our list of known relations, we searched for
          keywords within the TOC. If the keyword occurred in an
          item of the TOC, then the sub-items were likely to be
          related to it. For example, in Figure <a class="fig"
          href="#fig1">1</a>, the Coding Theory page consists of
          the following item in its TOC: “Other applications of
          coding theory” and this in turn consists of two sub-items
          “Group testing” and “Analog coding”. Since one of the
          keywords from our known relations is “application”, and
          the page under consideration is “Coding Theory”, we
          extract the triples ⟨<tt>group_testing applicationOf
          coding_theory</tt>⟩ and ⟨<tt>analog_coding applicationOf
          coding_theory</tt>⟩.</p>
          <p><strong>Section-List within articles</strong> (1909
          section lists): Next, there are several sub-headings in
          articles which consist of links to other topics. For
          example, the page on “Automated Theorem Proving” consists
          of a subheading “Popular Techniques” – this section
          simply consists of a list of techniques which are linked
          to their wikipedia page. Since “technique” is a keyword
          from our list of known relations, we identify this
          section-list pattern and acquire triples such as
          ⟨<tt>model_checking techniqueFor
          automated_theorem_proving</tt>⟩.</p>
          <p><strong>List hierarchies in articles</strong> (113
          list hierarchies): As in the case of “List” pages and
          “Outline” pages, we make use of list hierarchies in
          articles to extract the <tt>typeOf</tt>
          relationships.</p>
          <p><strong>Templates</strong> (1139 templates): Figure
          <a class="fig" href="#fig1">1</a> shows an example of a
          template from the “Database Management Systems” page. We
          extracted row headings as potential relations and added
          triples such as ⟨<tt>query_optimization functionOf
          database_management_systems</tt>⟩. However, based on our
          evaluation (see Section <a class="sec" href=
          "#sec-18">2.3</a>), we found that templates could have a
          variety of row headers and they were not always reliable.
          Therefore, we did not canonicalise these relations, but
          instead treated them as a generic “relatedTo” relation
          (therefore, instead of <tt>functionOf</tt>, we had
          <tt>relatedTo_(functionOf)</tt>).</p>
        </section>
        <section id="sec-16">
          <p><em>2.2.2 Unstructured sources.</em> Our unstructured
          sources include textual description of terms in both
          Webopedia and Techtarget as well as Wikipedia text of
          articles corresponding to entities. We limited the text
          in Wikipedia to the first paragraph. As mentioned
          previously, we acquired a number of <tt>synonymOf</tt>
          relations. We were successful in identifying the
          <tt>synonymOf</tt> relation by using the patterns “is
          abbreviation for”, “X (Y)” and “is short for”. We
          obtained over 1,000 such triples.</p>
          <p>Next, we ran the latest version of OpenIE [<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0014">14</a>] on
          each of these sources. While the number of extractions
          was quite large (approximately 400,000 triples), there
          were a lot of “junk” extractions such as
          ⟨<tt>as_a_specific example_for file_distribution</tt>⟩.
          In order to improve the accuracy, we applied the
          following filters to the extracted triples.</p>
          <p><strong>Filter 1:</strong> Triples which did not
          contain entities from our entity dictionary were
          discarded. This filter alone reduced the number of
          unuseable triples by nearly 300,000.</p>
          <p><strong>Filter 2</strong>: We found that entities in
          the triple could be too long. Therefore, our second
          filter retained only those triples which had an entity
          match of at least 50% – that is, 50% of the entity
          identified by OpenIE was a match for an entity in our
          dictionary.</p>
          <p><strong>Filter 3</strong>: Despite these two filters,
          we found cases where the subject or object referred to
          conceptual terms such as <tt>the_algorithm</tt> as part
          of the triple (the algorithm;generates;a path), where the
          entities are algorithm and path. Therefore, our third
          filter removed triples where the subject or object
          started with the word “the” and were less than 3 words in
          length.</p>
          <p>After applying all these filters, we retained 3506
          triples. We canonicalised a small percentage of the
          relations and we omit the details here for lack of space.
          In general, we were able to identify fine-grained
          relationships among entities. Table <a class="tbl" href=
          "#tab1">1</a> shows a few examples.</p>
        </section>
        <section id="sec-17">
          <p><em>2.2.3 TeKnowbase completion.</em> So far we have
          shown techniques to acquire entities and triples from
          different web-resources using heuristics. In this
          section, we focus on acquiring more triples between the
          entities based on the triples already present in
          TeKnowbase. This task is well-known as <em>inferencing in
          knowledge bases</em> or <em>knowledge base
          completion</em> [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0021">21</a>]. For example, if we have the
          triple ⟨<tt>knights_tour typeof state_space_search</tt>⟩
          and ⟨<tt>state_space_search typeof graph_algorithm</tt>⟩,
          it can be easily inferred that ⟨<tt>knights_tour typeof
          graph_algorithm</tt>⟩ using the transitive property, even
          if this triple is not present in the knowledge base. To
          extract such rules, [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0021">21</a>] have introduced a <em>neural
          tensor network</em> (NTN) that models each relation
          through its hyperparameters and generalizes several other
          neural network models. The bilinear tensor product is the
          primary operation used to relate entities to each other
          in a neural tensor network, unlike other neural networks.
          Each entity in the knowledge base is represented as a
          vector in higher dimension. This vector is obtained by
          taking a reduced mean of the vectors of the constituent
          words in the entity name. The vectors for each word is
          obtained using Word2Vec [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0015">15</a>] using the Skip-gram model.
          Moreover, they have also shown that the accuracy of the
          model improves when it is initialized with vectors
          trained on large unstructured corpora. We have used a
          similar approach and experimented with the following
          models:</p>
          <ul class="list-no-style">
            <li id="list3" label="•">Word2Vec (with the entities
            treated as one unit) on wikipedia corpus and textbooks:
            We trained phrase vectors for each entity using
            Word2Vec. The training set consisted of unstructured
            Wikipedia corpus as well as text from online textbooks.
            These vectors were then used to initialize the neural
            tensor network model.<br /></li>
            <li id="list4" label="•">Word2Vec (with the entities
            treated as one unit) trained only on wikipedia corpus:
            Same as above but the vectors were only trained on
            Wikipedia corpus. Information from textbooks was not
            included.<br /></li>
            <li id="list5" label="•">Word2Vec trained on wikipedia
            corpus and textbooks: We used Word2Vec to train vectors
            for each word and later obtained vector representation
            for each entity by the reduced mean of component word
            vectors. Similar to i), we used text from online
            textbooks (apart from unstructured text from Wikipedia)
            to train these vectors and used them to initialize the
            neural tensor network model.<br /></li>
            <li id="list6" label="•">Word2Vec trained only on
            wikipedia corpus: Same as above, except that we
            excluded textbook information to train vectors used to
            initialize the neural tensor network model.<br /></li>
          </ul>
          <p>We found that i) performed the best, i.e. Word2Vec
          with entities treated as one unit initialized with
          textbooks information. A total of 428 triples could be
          added to TeKnowbase using the inferencing model. Some of
          them are listed in Table <a class="tbl" href=
          "#tab3">3</a>. Evaluation of these triples is described
          in Section <a class="sec" href="#sec-18">2.3</a>.</p>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span>
              <span class="table-title">Some new triples that were
              added to TeKnowbase using the Neural Tensor Network
              inferencing model.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">1.</td>
                  <td style="text-align:left;">
                  ⟨<tt>xbasic,typeOf,programming_language</tt>⟩</td>
                </tr>
                <tr>
                  <td style="text-align:left;">2.</td>
                  <td style="text-align:left;">
                  ⟨<tt>binomial_heap,typeOf,tree</tt>⟩</td>
                </tr>
                <tr>
                  <td style="text-align:left;">3.</td>
                  <td style="text-align:left;">
                  ⟨<tt>palmdos,typeOf,operating_system</tt>⟩</td>
                </tr>
                <tr>
                  <td style="text-align:left;">4.</td>
                  <td style="text-align:left;">
                  ⟨<tt>delayed_column_generation,typeOf,convex_programming</tt>⟩</td>
                </tr>
                <tr>
                  <td style="text-align:left;">5.</td>
                  <td style="text-align:left;">
                  ⟨<tt>levenshtein_coding,typeOf,entropy_coding</tt>⟩</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Evaluation
            of TeKnowbase</h3>
          </div>
        </header>
        <section id="sec-19">
          <p><em>2.3.1 Setup.</em> We chose the top-5 frequent
          relations extracted for evaluation. These were:
          <tt>typeOf</tt>, <tt>terminology</tt>, <tt>synonymOf</tt>
          <tt>subTopicOf</tt> and <tt>conceptIn</tt>. Together,
          these five relations constitute about 63% of the triples
          in our KB. We used stratified sampling to sample from
          each type of relation. For each relation, we sampled 2%
          of the triples. Since not every triple extracted from the
          <em>unstructured sources</em> were canonicalised, we
          evaluated these triples separately by sampling about 2%
          of the triples. Each triple was evaluated by two
          evaluators and we marked a triple as correct only if both
          evaluators agreed. For inferencing, we experimented with
          4 different models (as described in Section <a class=
          "sec" href="#sec-17">2.2.3</a>). To evaluate these
          models, we created test set in the following way – we
          generated a list of <em>true</em> triples using the
          transitive property on the <tt>typeOf</tt> relations. For
          example, given triples ⟨<tt>palmdos,typeOf,dr_dos</tt>⟩
          and tripledr_dos,typeOf,operating_system in the training
          set, we returned a triple
          ⟨<tt>palmdos,typeOf,operating_system</tt>⟩ in the test
          set, ensuring that it does not already exist in the
          training set. To generate negative examples, we shuffled
          the head entities of the positive set of triples. We
          evaluated the accuracies of these models on this test
          set.</p>
          <div class="table-responsive" id="tab4">
            <div class="table-caption">
              <span class="table-number">Table 4:</span>
              <span class="table-title">Evaluation of a subset of
              triples in the TKB.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">#</td>
                  <td style="text-align:left;">
                  <strong>Relation</strong> (rows 1–5)</td>
                  <td style="text-align:center;"><strong>#
                  Evaluated triples</strong></td>
                  <td style="text-align:left;">
                  <strong>Accuracy</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;">1.</td>
                  <td style="text-align:left;"><tt>typeOf</tt></td>
                  <td style="text-align:center;">515</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$99.0\% \pm
                  0.8\%$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">2.</td>
                  <td style="text-align:left;">
                  <tt>terminologyOf</tt></td>
                  <td style="text-align:center;">676</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$98.9\% \pm
                  0.7\%$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">3.</td>
                  <td style="text-align:left;">
                  <tt>synonymOf</tt></td>
                  <td style="text-align:center;">70</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$100\% \pm
                  0.0 \%$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">4.</td>
                  <td style="text-align:left;">
                  <tt>subTopicOf</tt></td>
                  <td style="text-align:center;">42</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$91.3\% \pm
                  8.2\%$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">5.</td>
                  <td style="text-align:left;">
                  <tt>conceptIn</tt></td>
                  <td style="text-align:center;">334</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$95.4\% \pm
                  2.1\%$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">6.</td>
                  <td style="text-align:left;"><em>Unstructured
                  sources</em></td>
                  <td style="text-align:center;">435</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$63.2 \% \pm
                  3.7\%$</span></span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">7.</td>
                  <td style="text-align:left;"><em>Inferencing with
                  NTN</em></td>
                  <td style="text-align:center;">428</td>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$64.2 \% \pm
                  4.5\%$</span></span></td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-20">
          <p><em>2.3.2 Results and Analysis.</em> Table <a class=
          "tbl" href="#tab4">4</a> shows the accuracy of triples
          for each relation. We computed the Wilson interval at 95%
          confidence for each relation. On closer examination of
          these results, we found that we achieved the best results
          for the <tt>synonym</tt> relation consisting of
          expansions of abbreviations, such as <tt>ALU</tt> and
          <tt>Arithmetic Logic Unit</tt> as well as alternate
          terminology such as <tt>Photoshop</tt> and <tt>Adobe
          Photoshop</tt>.</p>
          <p>The best source of extractions are the Wikipedia list
          pages. In our list of top-5 relations, only 3 were
          extracted from Wikipedia list pages – <tt>typeOf</tt>,
          <tt>subtopicOf</tt> and <tt>synonymOf</tt> – and all of
          them were nearly 100% accurate.</p>
          <p>The major source of errors in many of these relations
          was due to extractions from TOC items (Section <a class=
          "sec" href="#sec-13">2.2.1</a>). This heuristic did not
          work well to identify the correct relation. For example,
          one of the errors was made when “Game types” was an item
          in the TOC of the page “Game Theory”. It listed
          “Symmetric/Asymmetric” as a type of game, but we
          extracted ⟨<tt>symmetric/asymmetric typeOf
          game_theory</tt>⟩ which is incorrect.</p>
          <p><em>Unstructured sources.</em> The overall accuracy of
          triples from unstructured text was found to be 63.2%.
          Recall that we ensure that the entities are always
          correct since we filter out triples which do not match an
          entity in our dictionary. Therefore, the main reason for
          low accuracy is that extracted relations were incorrect.
          Some incorrect extractions include: ⟨<tt>user requests
          mail</tt>⟩, ⟨<tt>packet switching protocol</tt>⟩. We are
          analysing these errors in more detail and improving the
          accuracy of these extractions in future work.</p>
          <p><em>Inferencing with Neural Tensor Network.</em> The
          neural tensor network model performed the best when
          training corpora from textbooks was added. It improved
          the accuracy of prediction for both Word2Vec models (with
          words treated separately and with entities treated as a
          single unit). Additionally, we also observed that
          Word2Vec with entities treated as one unit outperformed
          the model where separate vectors are learnt for each word
          in the technical domain. We were able to add 428 triples
          to TeKnowbase using this model with an accuracy of
          <span class="inline-equation"><span class="tex">$64.25
          \%$</span></span> .</p>
        </section>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Applications of
          TeKnowbase</h2>
        </div>
      </header>
      <p>In this section, we show that TeKnowbase can be useful in
      3 application settings – determining pre-requisites for
      technical concepts, classification of technical documents and
      improving ranking of academic search.</p>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Determining
            pre-requisites for technical concepts</h3>
          </div>
        </header>
        <p>Learning a new technical concept can be challenging
        because it involves identifying and studying its
        <em>pre-requisites</em>. A pre-requisite is any concept
        that has to be studied before another for better
        understanding. Although it is possible to acquire a number
        of relevant documents by searching for the concept in a
        search engine, there could be concepts mentioned in those
        documents that need to be searched again, eventually
        leading to cascade of searches and wasting a lot of user's
        time. In such a scenario, a retrieval system that
        identifies and returns the pre-requisites is extremely
        useful. For example, given the concept <tt>logitboost</tt>,
        we would like the system to return pre-requisites as shown
        in Figure <a class="fig" href="#fig2">2</a> (a). In this
        section, we show how TeKnowbase can be used to retrieve
        pre-requisites for technical concepts.</p>
        <section id="sec-23">
          <p><em>3.1.1 Problem.</em> Previous work on finding
          pre-requisites can be categorized into i) constructing
          pre-requisite functions to determine if a concept is a
          pre-requisite of the other [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0024">24</a>] ii) generating concept graphs
          given a set of concepts [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0025">25</a>]. All these techniques have
          typically relied on features from textual sources,
          structure of textbooks and learning from training
          examples to construct or learn pre-requisite
          relationships. To retrieve pre-requisites with better
          precision and recall, we need to consider relationships
          between the concepts. We wish to do the same using
          TeKnowbase. Two key problems that we address using a
          technical knowledge base are:</p>
          <p><em>Low Precision.</em> We have observed that all the
          pre-requisites are not equally “relevant”. For example,
          both <tt>binary_search_tree</tt> and <tt>file_system</tt>
          are returned as pre-requisites of <tt>b-tree</tt> by the
          pre-requisite function <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> proposed in
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0013">13</a>]. A person having no knowledge of
          <tt>b-tree</tt> would definitely gain more if
          <tt>binary_search_tree</tt> is prioritized over
          <tt>file_system</tt> because <tt>binary_search_tree</tt>
          helps understand what <tt>b-tree</tt> is better than
          <tt>file_system</tt>. <tt>file_system</tt> addresses
          another aspect towards understanding <tt>b-tree</tt> –
          stating that <tt>b-tree</tt> is <em>used</em> in the
          implementation of <tt>file_system</tt>. <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> models each
          concept as a “frame” of related concepts. The choice of
          frame is crucial towards determining relevant
          pre-requisites. <span class=
          "inline-equation"><span class="tex">$\operatorname{RefD}$</span></span>
          uses concepts that are Wikipedia neighbours to determine
          the frame. However, not all concepts that are Wikipedia
          neighbours should be included in the frame. As described
          in the example previously, <tt>file_system</tt> and
          <tt>b-tree</tt> are mentioned in the first paragraph of
          the Wikipedia page of <tt>b-tree</tt>. Due to this, it
          retrieves non-relevant pre-requisites like
          <tt>file_system</tt>. To discard such concepts from the
          frame, we model each concept as a vector in a higher
          dimensional space. These vector representations or
          <em>embeddings</em> are obtained by running
          <em>Node2Vec</em> [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0008">8</a>] on TeKnowbase with an emphasis on
          <tt>typeOf</tt> relation. As a result, the vector
          representations of concepts related via <tt>typeOf</tt>
          will be closer to each other than the rest. For example,
          the vector representations of <tt>b-tree</tt> and
          <tt>binary_search_tree</tt> will be closer to each other
          than <tt>file_system</tt>. We use this intuition to
          discard non-relevant concepts from the frame. One of the
          approaches to do so is to cluster the neighbours and
          retain the cluster that <tt>b-tree</tt> belongs to as the
          new frame.</p>
          <p><em>Low Recall.</em> Another limitation of previous
          work is that they focus on improving the precision of the
          pre-requisites and ignore the problem of low recall. We
          have observed that pre-requisites of similar concepts are
          the broadly the same. For example, Figure <a class="fig"
          href="#fig2">2</a> (a). and (b). shows the pre-requisites
          for <tt>logitboost</tt> and <tt>brownboost</tt>
          respectively. Both <tt>logitboost</tt> and
          <tt>brownboost</tt> are algorithms for <tt>boosting</tt>.
          It is clear that the pre-requisites of both the concepts
          are roughly the same and that adding the pre-requisites
          of <tt>brownboost</tt> (shown in the same figure)
          improves the recall of <tt>logitboost</tt>. We have
          exploited this simple observation to improve the recall
          of pre-requisites. To determine that <tt>brownboost</tt>
          is most similar to <tt>logitboost</tt>, we make use of
          entity embeddings trained on our technical knowledge
          base. Because embeddings have performed very well in
          determining “similar” words or entities, we have used
          them to determine the most similar concept to borrow
          pre-requisites from.</p>
          <p>Our technique is briefly described in Algorithm .
          Basically, TeKnowbase is used in 2 ways – i) to improve
          the precision by determining better frames and ii) to
          improve the recall by identifying the siblings from
          TeKnowbase taxonomy whose pre-requisites can be borrowed.
          Please note that although we have used <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0013">13</a>] for
          our experiments, our technique consisting of determining
          better frames and improving recall can be used with
          <em>any</em> pre-requisite function.</p>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3200000/3191532/images/www18companion-271-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">Pre-requisites for
              <tt>logitboost</tt> and <tt>brownboost</tt>.</span>
            </div>
          </figure>
          <figure id="fig3">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3200000/3191532/images/www18companion-271-fig3.jpg"
            class="img-responsive" alt="" longdesc="" />
          </figure>
          <p></p>
        </section>
        <section id="sec-24">
          <p><em>3.1.2 Setup.</em> We used the Wikipedia data dump
          of 2015 for our experiments. Our technique is referred to
          as <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> , which uses
          embeddings trained on TeKnowbase to determine the frame.
          For the pre-requisite function, we used <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> , proposed in
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0013">13</a>], which uses all the concepts
          mentioned in the Wikipedia page as the frame. Apart from
          <span class="inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> , we used 2
          other baselines, described as follows:</p>
          <p><span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> . This
          uses our technique, but the entity embeddings are
          generated by training on Wikipedia text. Phrase vectors
          are trained for each entity using Word2Vec algorithm by
          treating entities as a single unit.</p>
          <p><span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> . This
          also uses our technique, but the entity embeddings are
          generated by concatenating the embeddings used in
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> .</p>
        </section>
        <section id="sec-25">
          <p><em>3.1.3 Benchmarks.</em> We selected 26 queries
          spanning different areas of computer science and
          generated their pre-requisites using <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> , <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> ,
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> .</p>
        </section>
        <section id="sec-26">
          <p><em>3.1.4 Gold standard.</em> The gold standard list
          of pre-requisites for each query was generated by experts
          in that domain. This gold standard list is used to
          measure the precision and recall.</p>
        </section>
        <section id="sec-27">
          <p><em>3.1.5 Evaluation metrics and Results.</em> We
          evaluated the precision, recall and F1 scores for the
          pre-requisites. The results are shown in Table <a class=
          "tbl" href="#tab5">5</a>. Additionally, we also asked two
          evaluators to rate each edge (<em>a</em>, <em>b</em>) in
          the pre-requisite graph <em>P<sub>G</sub></em> as
          follows:</p>
          <p><strong>Option 1</strong>. When <em>b</em> is required
          and crucial for understanding <em>a</em></p>
          <p><strong>Option 2</strong>. When <em>b</em> improves
          understanding of <em>a</em></p>
          <p><strong>Option 3</strong>. When <em>b</em> is a field
          of study</p>
          <p><strong>Option 4</strong>. When <em>a</em> is a
          prerequisite of <em>b</em>, i.e. inversely related</p>
          <p><strong>Option 5</strong>. None of the above</p>
          <p>We computed the <em>accuracy</em> of edges as
          follows:</p>
          <div class="table-responsive" id="Xeq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \begin{split} Acc_1 = {}&amp; \frac{\sum _{e_{i} \in
              E_{1}} I(e_{i})}{|E_1|}, \text{where $I(e_{i}) = 1$,
              if the edge $e_{i}$} \\ &amp; \text{is either marked
              as Option 1 or Option 2, else $I(e_{i})=0$}
              \end{split} \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>where <em>E</em> <sub>1</sub> is the set of all
          edges in <em>P<sub>G</sub></em> . We also evaluated the
          accuracy for edge-sequences of length 2. The accuracy of
          edge-sequences of length 2 is computed as follows:
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \begin{split} Acc_2 = {}&amp; \frac{\sum _{e_{i} \in
              E_{2}} I(e_{i})}{|E_2|}, \text{where $I(e_{i}) = 1$,
              if both the edges in edge sequence $e_{i}$} \\ &amp;
              \text{are marked as Option 1 or Option 2, else
              $I(e_{i}) = 0$} \end{split}
              \end{equation}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>where <em>E</em> <sub>2</sub> is the set of all
          edge-sequences of length 2 in <em>P<sub>G</sub></em> .
          <p></p>
          <p>Table <a class="tbl" href="#tab5">5</a> compares the
          precision, recall and F1 scores of <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> , <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> ,
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> after
          improving the frames. <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> returns the
          best precision of 86% as compared to <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> , <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> which
          return 29%, 50% and 25% precision respectively. Clearly,
          this is due to using better frames generated by the
          technical knowledge base. The recall of <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> is
          <span class="inline-equation"><span class="tex">$34
          \%$</span></span> and lower than that of <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> but it still
          performs better in terms of F1-score while the recall
          obtained by <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> are
          even worse.</p>
          <p>To improve the scores further,
          <tt>improveRecall()</tt> function is used. It finds out
          the most similar sibling from the knowledge base taxonomy
          and adds its pre-requisites using different techniques –
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> ,
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> . Table
          <a class="tbl" href="#tab6">6</a> shows the precision and
          recall values after this function improves the recall for
          each of the techniques. The precision of <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> before
          adding was 86% but the recall was low – only 34%, even
          lesser than the recall of the baseline <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> . The recall
          improved to 60%, almost by a factor of 2 at the cost of
          bringing the precision down by only 14% and improving the
          F1-score from 0.49 to 0.64. Hence, it performs the best.
          However, the improvement is not very significant for
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> or
          <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> . The
          reason is poor quality of frames retrieved by them that
          returned non-relevant pre-requisites. The recall of
          <span class="inline-equation"><span class=
          "tex">$\operatorname{ OWN\_COMBINED}$</span></span>
          improved from 18% to <span class=
          "inline-equation"><span class="tex">$28\%$</span></span>
          at the cost of bringing precision down from 50% to 43%.
          The F1- score, however, increased slightly from 0.26 to
          0.29. Although its precision (43%) is higher than
          <span class="inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> (29%), it
          failed to beat <span class="inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> in recall or
          F1-score. <span class="inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> performs
          the worst. Its recall increased by a small margin – from
          8% to 12% at the cost of 4% reduction in precision – 25%
          to 21%. The F1-score also increased by a small range from
          0.12 to 0.16. The reason for it performing poorly is
          using concepts that occur near to each other in text in
          the frame. The poor performance of <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> led to the
          sub-par performance of <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> since
          the embeddings used in <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span> are
          constructed from <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span>
          embeddings. Overall, <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> performs
          better than each of the other techniques due to the
          better quality of frames.</p>
          <div class="table-responsive" id="tab5">
            <div class="table-caption">
              <span class="table-number">Table 5:</span>
              <span class="table-title">Precision, Recall and F1
              scores obtained after using different frames for
              <span class="inline-equation"><span class=
              "tex">$\operatorname{RefD}$</span></span> ,
              <span class="inline-equation"><span class=
              "tex">$\operatorname{OWN\_KB}$</span></span> ,
              <span class="inline-equation"><span class=
              "tex">$\operatorname{OWN\_TEXT}$</span></span> and
              <span class="inline-equation"><span class=
              "tex">$\operatorname{OWN\_COMBINED}$</span></span> .
              Pruning the frame is used to improve the precision of
              relevant pre-requisites.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">Representation
                  scheme</td>
                  <td style="text-align:right;">Precision</td>
                  <td style="text-align:right;">Recall</td>
                  <td style="text-align:right;">F1-score</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{RefD}$</span></span></td>
                  <td style="text-align:right;">0.29</td>
                  <td style="text-align:right;">
                  <strong>0.42</strong></td>
                  <td style="text-align:right;">0.34</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_KB}$</span></span></td>
                  <td style="text-align:right;">
                  <strong>0.86</strong></td>
                  <td style="text-align:right;">0.34</td>
                  <td style="text-align:right;">
                  <strong>0.49</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_TEXT}$</span></span></td>
                  <td style="text-align:right;">0.25</td>
                  <td style="text-align:right;">0.08</td>
                  <td style="text-align:right;">0.12</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_COMBINED}$</span></span></td>
                  <td style="text-align:right;">0.50</td>
                  <td style="text-align:right;">0.18</td>
                  <td style="text-align:right;">0.26</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab6">
            <div class="table-caption">
              <span class="table-number">Table 6:</span>
              <span class="table-title">Precision, Recall and F1
              scores obtained after identifying the most similar
              sibling using <span class=
              "inline-equation"><span class=
              "tex">$\operatorname{OWN\_KB}$</span></span> ,
              <span class="inline-equation"><span class=
              "tex">$\operatorname{OWN\_TEXT}$</span></span> and
              <span class="inline-equation"><span class=
              "tex">$\operatorname{OWN\_COMBINED}$</span></span>
              and adding its pre-requisites. The pre-requisites of
              siblings from knowledge graph taxonomy are roughly
              the same and improve recall by significant margin for
              <span class="inline-equation"><span class=
              "tex">$\operatorname{OWN\_KB}$</span></span> .</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">Representation
                  scheme</td>
                  <td style="text-align:right;">Precision</td>
                  <td style="text-align:right;">Recall</td>
                  <td style="text-align:right;">F1-score</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{RefD}$</span></span></td>
                  <td style="text-align:right;">0.29</td>
                  <td style="text-align:right;">0.42</td>
                  <td style="text-align:right;">0.34</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_KB}$</span></span></td>
                  <td style="text-align:right;">
                  <strong>0.72</strong></td>
                  <td style="text-align:right;">
                  <strong>0.60</strong></td>
                  <td style="text-align:right;">
                  <strong>0.64</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_TEXT}$</span></span></td>
                  <td style="text-align:right;">0.21</td>
                  <td style="text-align:right;">0.12</td>
                  <td style="text-align:right;">0.16</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_COMBINED}$</span></span></td>
                  <td style="text-align:right;">0.43</td>
                  <td style="text-align:right;">0.28</td>
                  <td style="text-align:right;">0.29</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Table <a class="tbl" href="#tab7">7</a> lists the
          accuracies – <em>Acc</em> <sub>1</sub> and <em>Acc</em>
          <sub>2</sub> obtained from user evaluation. The
          accuracies have been calculated as described in Section
          <a class="sec" href="#sec-27">3.1.5</a>. Again, the
          pre-requisite graph generated using <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> obtains the
          highest accuracy of 83% for edges and 60% for edge
          sequences of length 2. <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span>
          obtains an accuracy of 69% for the edges which is not bad
          but performs poorly for edge sequences of length 2 (40%).
          However, it performs better than <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> . This is
          evident because we earlier found that <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_COMBINED}$</span></span>
          obtained better precision than <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{RefD}$</span></span> . The
          pre-requisite graph generated using <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_TEXT}$</span></span> performs
          the worst with 48% and 12% accuracies for edges and edge
          sequences of length 2. Overall, <span class=
          "inline-equation"><span class=
          "tex">$\operatorname{OWN\_KB}$</span></span> is the
          winner.</p>
          <div class="table-responsive" id="tab7">
            <div class="table-caption">
              <span class="table-number">Table 7:</span>
              <span class="table-title">Results from user
              evaluation.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">Representation
                  scheme</td>
                  <td style="text-align:right;"><em>Acc</em>
                  <sub>1</sub></td>
                  <td style="text-align:right;"><em>Acc</em>
                  <sub>2</sub></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{RefD}$</span></span></td>
                  <td style="text-align:right;">0.42</td>
                  <td style="text-align:right;">0.21</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_KB}$</span></span></td>
                  <td style="text-align:right;">
                  <strong>0.83</strong></td>
                  <td style="text-align:right;">
                  <strong>0.60</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_TEXT}$</span></span></td>
                  <td style="text-align:right;">0.48</td>
                  <td style="text-align:right;">0.12</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><span class=
                  "inline-equation"><span class=
                  "tex">$\operatorname{OWN\_COMBINED}$</span></span></td>
                  <td style="text-align:right;">0.69</td>
                  <td style="text-align:right;">0.40</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
      <section id="sec-28">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span>
            Classification experiment</h3>
          </div>
        </header>
        <p>[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0006">6</a>]
        showed that the addition of features from domain specific
        ontological KBs can improve classification accuracy.
        Adapting this idea for our setting of a technical KB, a
        document belonging to the class “databases” may not
        actually contain the term “database”, but simply have terms
        related to databases. If this relationship is explicitly
        captured in TeKnowbase, then that is a useful feature to
        add. We adapted this idea for our setting of a technical KB
        and classified posts from StackOverflow<a class="fn" href=
        "#fn3" id="foot-fn3"><sup>3</sup></a>.</p>
        <section id="sec-29">
          <p><em>3.2.1 Setup.</em> StackOverflow is a forum for
          technical discussions. A page in the website consists of
          a question asked by a user followed by several answers to
          that question. The question itself may be tagged by the
          user with several hashtags. The administrators of the
          site classify the question into one of several technical
          categories. Our task is to classify a given question
          <em>automatically</em> into a specific technical
          category. We downloaded the StackOverflow data dump and
          chose questions from 3 different categories: “databases”,
          “networking”, and, “data-structures” We created a corpus
          of 1500 questions including the title (500 for each
          category). The category into which the questions were
          manually classified by the StackOverflow site were taken
          as the ground truth.</p>
        </section>
        <section id="sec-30">
          <p><em>3.2.2 Features generation.</em> We generated the
          following set of features for training.</p>
          <p><strong>BOW</strong>: Bag of words.</p>
          <p><strong>BOW+BOE</strong>: Bag of words and bag of
          entities. Entities are treated as a whole, rather than a
          bag of words. Note that these entities were identified
          using the entity list of TeKnowbase.</p>
          <p><strong>BOW+BOE+TKB</strong>: In addition to the words
          and entities above, for each <em>entity</em>, we added
          features from the 1-hop neighborhood of the entity. For
          example, if the entity <tt>run_length_encoding</tt>
          occurred in the post, then we added as a feature,
          <tt>data_compression</tt> since we have the triple
          ⟨<tt>run_length_encoding methodOf
          data_compression</tt>⟩.</p>
        </section>
        <section id="sec-31">
          <p><em>3.2.3 Classification algorithms.</em> We trained
          both a Naive-bayes classifier as well as SVM with each of
          the feature sets above.</p>
        </section>
        <section id="sec-32">
          <p><em>3.2.4 Results.</em> We performed 5-fold cross
          validation with each classifier and feature set and
          report the accuracies in Table <a class="tbl" href=
          "#tab8">8</a>. Clearly, simply adding new features from
          TeKnowbase helps in improving the accuracies of the
          classifiers. This result is encouraging and we expect
          that optimizing the addition of features (for example,
          coming up with heuristics to decide which relations to
          use) will result in further gains.</p>
          <div class="table-responsive" id="tab8">
            <div class="table-caption">
              <span class="table-number">Table 8:</span>
              <span class="table-title">Average classification
              accuracies.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:right;">
                  <strong>BOW</strong></td>
                  <td style="text-align:right;"><strong>BOW +
                  BOE</strong></td>
                  <td style="text-align:right;"><strong>BOW + BOE +
                  TKB</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong>SVM</strong></td>
                  <td style="text-align:right;">82.1%</td>
                  <td style="text-align:right;">87.1%</td>
                  <td style="text-align:right;">92.0%</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><strong>Naive
                  Bayes</strong></td>
                  <td style="text-align:right;">86.3%</td>
                  <td style="text-align:right;">88.4%</td>
                  <td style="text-align:right;">89.6%</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab9">
            <div class="table-caption">
              <span class="table-number">Table 9:</span>
              <span class="table-title">NDCG@20 values obtained
              with tf-idf and BM-25 ranking models. The numbers of
              queries where these models won (W), tied (T) or lost
              (L) to BOW is listed along with the NDCG
              scores.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:center;">
                  <strong>BOW</strong></td>
                  <td style="text-align:right;"><strong>BOW + BOE +
                  TKB, W/T/L</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong>tf-idf</strong></td>
                  <td style="text-align:center;">0.373</td>
                  <td style="text-align:right;">0.380, 32/9/40</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong>BM-25</strong></td>
                  <td style="text-align:center;">0.312</td>
                  <td style="text-align:right;">0.326, 41/9/31</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
      <section id="sec-33">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Ranking
            experiment</h3>
          </div>
        </header>
        <p>The use of a bag-of-entities (BOE) model to represent
        queries and documents for document retrieval is outlined in
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0026">26</a>]. We
        adapt this method to our setting to retrieve research
        articles in computer science. We note that the use of
        <em>knowledge-base embeddings</em> has been further
        explored in [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0027">27</a>] (though their knowledge-base is
        different in structure and content to ours). We made use of
        the data generously provided by [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0027">27</a>] to conduct our own
        ranking experiment with a BOE model.</p>
        <section id="sec-34">
          <p><em>3.3.1 Setup.</em> The data consists of 100
          technical queries (such as <tt>semantic web</tt>,
          <tt>natural language interface</tt>, etc.) derived from
          an analysis of the query log of Semantic Scholar<a class=
          "fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>, and a
          list of documents that are relevant to each query. We
          chose 81 of these 100 queries which contained entities
          from the knowledge bases we used for ranking and ran our
          experiments on those queries.</p>
        </section>
        <section id="sec-35">
          <p><em>3.3.2 Techniques.</em> We experimented with the
          following representations of both queries and
          documents.</p>
          <p><strong>BOW</strong>: Bag of words.</p>
          <p><strong>BOW+BOE+TKB</strong>: Bag of words and bag of
          entities. A pre-processing step identifies all entities
          occurring in the document and these entities are from the
          entity list of TeKnowbase. These entities are retained as
          a whole and not treated as a bag of words. Additionally,
          we expanded each entity tagged in the query/document by
          the entities occurring in 1-hop neighborhood in
          TeKnowbase.</p>
        </section>
        <section id="sec-36">
          <p><em>3.3.3 Ranking models.</em> We have used tf-idf
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0018">18</a>] and BM-25 [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0020">20</a>]
          ranking models to rank the candidate documents.</p>
        </section>
        <section id="sec-37">
          <p><em>3.3.4 Results.</em> We calculated NDCG@20
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0010">10</a>] and report the values in Table
          <a class="tbl" href="#tab9">9</a>. We also counted on how
          many queries the NDCG score won (T), tied (T) or lost (L)
          to the bag of words model using TeKnowbase. We conclude
          that identifying and using entities in the document and
          query representations using TeKnowbase improves the
          quality of results. We believe that more sophisticated
          ranking models that use <em>entity embeddings</em> such
          as in [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0027">27</a>] can further improve the quality
          of results, and this is a direction for future work. This
          establishes the usability of TeKnowbase in the ranking
          scenario.</p>
        </section>
      </section>
    </section>
    <section id="sec-38">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Related
          Work</h2>
        </div>
      </header>
      <p>Recently, systems which facilitate knowledge-base
      construction from heterogeneous sources have been proposed.
      For example, DeepDive [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>] aims to consume a large number of
      heterogeneous data sources for extraction and combines
      evidence from different sources to output a probabilistic KB.
      Similarly, Google's Knowledge Vault [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>] also aims to fuse data
      from multiple resources on the Web to construct a KB. Our
      effort is similar in that we make use of heterogeneous data
      sources and customise our extractions.</p>
      <p><strong>Entity extraction:</strong> One of the important
      aspects of building domain-specific knowledge-bases is that a
      dictionary of terms that are relevant to the domain should be
      acquired. It is possible that such dictionaries are already
      available (for example, lists of people), but for others, we
      need techniques to build this dictionary. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0019">19</a>] gives an overview of
      supervised and unsupervised methods to recognize entities
      from text. We follow a more straightforward approach – we
      specifically target technology websites and write wrappers to
      extract a list of entities related to computer science.</p>
      <p><strong>Information Extraction:</strong> Research in
      information extraction to build knowledge-bases make use of a
      variety of techniques (see [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>] for an overview). In general,
      information extraction can be done from mostly structured
      resources such as Wikipedia (see, for example, YAGO
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>]) or from
      unstructured sources (for example, OpenIE [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>]) where the relations are
      not known ahead of time. Moreover, there are rule-based
      systems such as SystemT [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>], using surface patterns and
      supervised techniques for known relations [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0009">9</a>], distant supervision
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>], etc. We
      use a mix of these approaches – we formulate different ways
      to exploit the structured information sources in Wikipedia,
      and use surface patterns and OpenIE to extract relationships
      from unstructured sources.</p>
    </section>
    <section id="sec-39">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span>
          Conclusions</h2>
        </div>
      </header>
      <p>In this paper, we described the construction of a
      knowledge-base of technical concepts related to computer
      science from both structured and unstructured sources. Our
      approach consisted of two steps – constructing a dictionary
      of terms related to computer science and to extract
      relationships among them. Our experiments showed that our
      triples are accurate. Apart from using heuristics to extract
      triples, we also experimented with inferencing models and
      showed that all models improve after adding information from
      online textbooks. We have also demonstrated the usability of
      TeKnowbase in 3 application settings. Other interesting
      applications that can benefit from TeKnowbase are question
      answer generation and summarization of technical concepts.
      Our future work consists of developing such applications.</p>
    </section>
    <section id="sec-40">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span>
          Acknowledgements</h2>
        </div>
      </header>
      <p>We want to thank Ashwini Purkar, Tanuma Patra and Akanksha
      Bansal for their contributions in building TeKnowbase.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Michael Ashburner <em>et
        al.</em> 2000. Gene Ontology: tool for the unification of
        biology. <em><em>Nature Genetics</em></em> 25, 1 (2000),
        25–29.</li>
        <li id="BibPLXBIB0002" label="[2]">Michele Banko <em>et
        al.</em> 2007. Open Information Extraction from the Web.
        <em><em>IJCAI</em></em> (2007), 2670–2676.</li>
        <li id="BibPLXBIB0003" label="[3]">Andrew Carlson <em>et
        al.</em> 2010. Toward an Architecture for Never-Ending
        Language Learning. <em><em>AAAI</em></em> (2010),
        1306–1313.</li>
        <li id="BibPLXBIB0004" label="[4]">Christopher De&nbsp;Sa
        <em>et al.</em> 2016. DeepDive - Declarative Knowledge Base
        Construction. <em><em>SIGMOD Record</em></em> 45, 1 (2016),
        60–67.</li>
        <li id="BibPLXBIB0005" label="[5]">Xin Dong <em>et al.</em>
        2014. Knowledge vault - a web-scale approach to
        probabilistic knowledge fusion. <em><em>KDD</em></em>
        (2014), 601–610.</li>
        <li id="BibPLXBIB0006" label="[6]">Evgeniy Gabrilovich and
        Shaul Markovitch. 2005. Feature Generation for Text
        Categorization Using World Knowledge.
        <em><em>IJCAI</em></em> (2005), 1048–1053.</li>
        <li id="BibPLXBIB0007" label="[7]">Jonathan Gordon <em>et
        al.</em> 2016. Modeling Concept Dependencies in a
        Scientific Corpus. <em><em>ACL</em></em> (2016),
        866–875.</li>
        <li id="BibPLXBIB0008" label="[8]">Aditya Grover and Jure
        Leskovec. 2016. Node2Vec: Scalable Feature Learning for
        Networks. <em><em>KDD</em></em> (2016), 855–864.</li>
        <li id="BibPLXBIB0009" label="[9]">Marti&nbsp;A Hearst.
        1992. Automatic Acquisition of Hyponyms from Large Text
        Corpora. <em><em>COLING</em></em> (1992), 539–545.</li>
        <li id="BibPLXBIB0010" label="[10]">Kalervo Järvelin and
        Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation of
        IR Techniques. <em><em>ACM Trans. Inf. Syst.</em></em> 20,
        4 (2002), 422–446.</li>
        <li id="BibPLXBIB0011" label="[11]">Jens Lehmann <em>et
        al.</em> 2015. DBpedia - A large-scale, multilingual
        knowledge base extracted from Wikipedia. <em><em>Semantic
        Web</em></em> 6, 2 (2015), 167–195.</li>
        <li id="BibPLXBIB0012" label="[12]">Yunyao Li, Frederick
        Reiss, and Laura Chiticariu. 2011. SystemT - A Declarative
        Information Extraction System. <em><em>ACL</em></em>
        (2011), 109–114.</li>
        <li id="BibPLXBIB0013" label="[13]">Chen Liang <em>et
        al.</em> 2015. Measuring prerequisite relations among
        concepts. <em><em>EMNLP</em></em> (2015), 1668–1674.</li>
        <li id="BibPLXBIB0014" label="[14]">Mausam <em>et al.</em>
        2012. Open Language Learning for Information Extraction.
        <em><em>EMNLP-CoNLL</em></em> (2012), 523–534.</li>
        <li id="BibPLXBIB0015" label="[15]">Tomas Mikolov <em>et
        al.</em> 2013. Efficient Estimation of Word Representations
        in Vector Space. arxiv:1301.3781. Technical report
        available from: <a class="link-inline force-break" href=
        "http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.
        (2013).
        </li>
        <li id="BibPLXBIB0016" label="[16]">Mike Mintz <em>et
        al.</em> 2009. Distant supervision for relation extraction
        without labeled data. <em><em>ACL/IJCNLP</em></em> (2009),
        1003–1011.</li>
        <li id="BibPLXBIB0017" label="[17]">Dat&nbsp;Quoc Nguyen.
        2017. An overview of embedding models of entities and
        relationships for knowledge base completion. Technical
        report available from: <a class="link-inline force-break"
        href=
        "http://arxiv.org/abs/1703.08098">http://arxiv.org/abs/1703.08098</a>.
        (2017).
        </li>
        <li id="BibPLXBIB0018" label="[18]">Juan Ramos. 1999. Using
        TF-IDF to Determine Word Relevance in Document Queries.
        Technical report available from: <a class=
        "link-inline force-break" href=
        "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424rep=rep1type=pdf">
          http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424rep=rep1type=pdf</a>.
          (1999).
        </li>
        <li id="BibPLXBIB0019" label="[19]">Xiang Ren <em>et
        al.</em> 2016. Automatic Entity Recognition and Typing in
        Massive Text Corpora. <em><em>WWW</em></em> (2016),
        1025–1028.</li>
        <li id="BibPLXBIB0020" label="[20]">Stephen Robertson and
        Hugo Zaragoza. 2009. The Probabilistic Relevance Framework:
        BM25 and Beyond. <em><em>Found. Trends Inf. Retr.</em></em>
        3, 4 (2009), 333–389.</li>
        <li id="BibPLXBIB0021" label="[21]">Richard Socher <em>et
        al.</em> 2013. Reasoning with Neural Tensor Networks for
        Knowledge Base Completion. <em><em>NIPS</em></em> (2013),
        926–934.</li>
        <li id="BibPLXBIB0022" label="[22]">Fabian&nbsp;M Suchanek,
        Gjergji Kasneci, and Gerhard Weikum. 2007. Yago - a core of
        semantic knowledge. <em><em>WWW</em></em> (2007),
        697–706.</li>
        <li id="BibPLXBIB0023" label="[23]">Fabian&nbsp;M Suchanek
        and Gerhard Weikum. 2014. Knowledge Bases in the Age of Big
        Data Analytics. <em><em>PVLDB</em></em> 7, 13 (2014),
        1713–1714.</li>
        <li id="BibPLXBIB0024" label="[24]">Partha&nbsp;Pratim
        Talukdar and William&nbsp;W. Cohen. 2012. Crowdsourced
        Comprehension: Predicting Prerequisite Structure in
        Wikipedia. <em><em>BEA@NAACL-HLT</em></em> (2012),
        307–315.</li>
        <li id="BibPLXBIB0025" label="[25]">Shuting Wang <em>et
        al.</em> 2015. Concept Hierarchy Extraction from Textbooks.
        <em><em>DocEng</em></em> (2015), 147–156.</li>
        <li id="BibPLXBIB0026" label="[26]">Chenyan Xiong, Jamie
        Callan, and Tie-Yan Liu. 2016. Bag-of-Entities
        Representation for Ranking. <em><em>ICTIR</em></em> (2016),
        181–184.</li>
        <li id="BibPLXBIB0027" label="[27]">Chenyan Xiong, Russell
        Power, and Jamie Callan. 2017. Explicit Semantic Ranking
        for Academic Search via Knowledge Graph Embedding.
        <em><em>WWW</em></em> (2017), 1271–1279.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "http://www.webopedia.com">http://www.webopedia.com</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "http://www.techtarget.com/">http://www.techtarget.com/</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "http://stackoverflow.com">stackoverflow.com</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class=
    "link-inline force-break" href=
    "https://www.semanticscholar.org">https://www.semanticscholar.org</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC BY 4.0) license. Authors
      reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18 Companion, April 23-27, 2018, Lyon,
      France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC BY 4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3191532">https://doi.org/10.1145/3184558.3191532</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
