<!DOCTYPE html> htmlhtmlhtmlhtmlhtmlhtml<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>VERSE: Versatile Graph Embeddings from Similarity Measures</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">VERSE: Versatile Graph Embeddings from Similarity Measures</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Anton</span>      <span class="surName">Tsitsulin</span>,     Hasso Plattner Institute, <a href="mailto:anton.tsitsulin@hpi.de">anton.tsitsulin@hpi.de</a>     </div>     <div class="author">     <span class="givenName">Davide</span>      <span class="surName">Mottin</span>,     Hasso Plattner Institute, <a href="mailto:davide.mottin@hpi.de">davide.mottin@hpi.de</a>     </div>     <div class="author">     <span class="givenName">Panagiotis</span>      <span class="surName">Karras</span>,     Aarhus University, <a href="mailto:panos@cs.au.dk">panos@cs.au.dk</a>     </div>     <div class="author">     <span class="givenName">Emmanuel</span>      <span class="surName">M&#x00FC;ller</span>,     Hasso Plattner Institute, <a href="mailto:emmanuel.mueller@hpi.de">emmanuel.mueller@hpi.de</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186120" target="_blank">https://doi.org/10.1145/3178876.3186120</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>      <em>Embedding</em> a web-scale information network into a low-dimensional vector space facilitates tasks such as link prediction, classification, and visualization. Past research has addressed the problem of extracting such embeddings by adopting methods from words to graphs, without defining a clearly comprehensible graph-related objective. Yet, as we show, the objectives used in past works implicitly utilize similarity measures among graph nodes.</small>     </p>     <p>     <small>In this paper, we carry the similarity orientation of previous works to its logical conclusion; we propose VERtex Similarity Embeddings (<SmallCap>VERSE</SmallCap>), a simple, versatile, and memory-efficient method that derives graph embeddings explicitly calibrated to preserve the distributions of a selected vertex-to-vertex similarity measure. <SmallCap>VERSE</SmallCap> learns such embeddings by training a singlelayer neural network. While its default, <em>scalable</em> version does so via sampling similarity information, we also develop a variant using the <em>full</em> information per vertex. Our experimental study on standard benchmarks and real-world datasets demonstrates that <SmallCap>VERSE</SmallCap> , instantiated with diverse similarity measures, outperforms state-of-the-art methods in terms of precision and recall in major data mining tasks and supersedes them in time and space efficiency, while the scalable sampling-based variant achieves equally good results as the non-scalable full variant.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel M&#x00FC;ller. 2018. VERSE: Versatile Graph Embeddings from Similarity Measures. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186120" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186120</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Graph data naturally arises in many domains, including social networks, protein networks, and the web. Over the past years, numerous graph mining techniques have been proposed to analyze and explore such real-world networks. Commonly, such techniques apply machine learning to address tasks such as node classification, link prediction, anomaly detection, and node clustering. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Three node properties are highlighted on the same graph. Can a single model capture these properties?</span>     </div>     </figure>    </p>    <p>Machine learning algorithms require a set of expressive discriminant features to characterize graph nodes and edges. To this end, one can use features representing <em>similarities</em> among nodes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. However, feature engineering is tedious work, and the results do not translate well across tasks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>].</p>    <p>An alternative to feature design is <em>to learn</em> feature vectors, or <em>embeddings</em> by solving an optimization problem in <em>unsupervised</em> fashion. Yet devising and solving a universal and tractable optimization problem for learning representations has withstood research efforts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. One line of research&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] applies classical dimensionality reduction methods, such as SVD, to similarity matrices over the graph; yet these methods are burdened with constructing the matrix. While a recent approach&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] overcomes this impediment, it results in poor quality in prediction tasks due to its <em>linear nature</em>.</p>    <p>Another line of research aims to generate features capturing neighborhood locality, usually through an objective that can be optimized by Stochastic Gradient Descent (SGD)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. Such methods rely on an implicit, albeit rigid, notion of node neighborhood; yet this one-size-fits-all approach cannot grapple with the diversity of real-world networks and applications. Grover&#x00A0;et&#x00A0;al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] discerned this inflexibility in the notion of the local neighborhood; to ameliorate it, they proposed <SmallCap>Node2vec</SmallCap>, which biases the exploration strategy of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] using two hyperparameters. Yet this <em>hyperparametertuned</em> approach raises a cubic worst-case space complexity and compels the user to traverse several feature sets and gauge the one that attains the best performance in the downstream task. Besides, a local neighborhood, even when found by hyperparameter tuning, still represents only one locality-based class of features; hence, <SmallCap>Node2vec</SmallCap> does not adequately escape the rigidity it tries to mend.</p>    <p>We argue that features extracted by a more <em>versatile</em> similarity notion than that of a local neighborhood would achieve the flexibility to solve diverse data mining tasks in a large variety of graphs. Figure&#x00A0;<a class="fig" href="#fig1">1</a> makes a case for such a versatile similarity notion by exposing three distinct kinds of similarity on a graph: <em>community structure</em> guides community detection tasks, <em>roles</em> are typically used in classification, while <em>structural equivalence</em> defines peer correspondences in knowledge graphs. As real-world tasks rely on a mix of such properties, a versatile feature learning algorithm should be capable of capturing all such similarities.</p>    <p>In this paper, we propose <SmallCap>VERSE</SmallCap>, the first, to our knowledge, versatile graph embedding method that explicitly learns <em>any</em> similarity measures among nodes. In its learning core, <SmallCap>VERSE</SmallCap> stands between deep learning approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>] on the one hand and the direct decomposition of the similarity matrix&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] on the other hand. Instead, <SmallCap>VERSE</SmallCap> trains a simple, yet expressive, single-layer neural network to reconstruct similarity distributions between nodes. Thereby, it outperforms previous methods in terms of both runtime and quality on a variety of large real networks and tasks.</p>    <p>Thanks to its ability to choose any appropriate similarity measure for the task at hand, <SmallCap>VERSE</SmallCap> adjusts to that task without needing to change its core. Thereby, it fully ameliorates the rigidity observed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>], and integrates representation learning with feature engineering: any similarity measure, including those developed in feature engineering, can be used as input to <SmallCap>VERSE</SmallCap>. For the sake of illustration, we instantiate our generic methodology using three popular similarity measures, namely Personalized PageRank (PPR)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>], SimRank&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>], and adjacency similarity. We also show that versatility <em>does not</em> imply a new burden to the user, merely substituting hyperparameter tuning with similarity measure tuning: using PPR as a <em>default</em> choice for the similarity measure leads to good performance in nearly all tasks and networks we examined.</p>    <p>We summarize our contributions as follows.</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">We propose a versatile framework for graph embeddings that explicitly learns the distribution of <strong>any</strong> vertex similarity measure for each graph vertex.<br/></li>     <li id="list2" label="&#x2022;">We interpret previous graph embeddings through the lens of our similarity framework, and instantiate <SmallCap>VERSE</SmallCap> with Personalized PageRank, SimRank, and Adjacency similarity.<br/></li>     <li id="list3" label="&#x2022;">We devise an efficient algorithm, linear in graph size, based on a single-layer neural network minimizing the divergence from real to reconstructed similarity distributions.<br/></li>     <li id="list4" label="&#x2022;">In a thorough experimental evaluation, we show that <SmallCap>VERSE</SmallCap> outperforms the state-of-the-art approaches in various graph mining tasks in quality while being even more efficient.<br/></li>    </ul>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>In the absence of a general-purpose representation for graphs, graph analysis tasks require domain experts to craft features&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] or to use specialized feature selection algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. Recently, specialized methods were introduced to learn representations of different graph parts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and graphs with annotations on nodes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0055">55</a>], or edges&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0049">49</a>]. We focus on learning representations of <em>nodes</em> in graphs without <em>any</em> prior or additional information other than graph structure.</p>    <p>     <strong>Traditional feature learning</strong> learns features by compressing representations such as the Laplacian or adjacency matrix to a low-dimensional space. Early works in this area include spectral techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] and nonlinear dimensionality reduction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>]. In another vein, Marginal Fisher Analysis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>] analyzes the dimensionality reduction of a point data set as the embedding of a graph capturing its statistic and geometric properties. Such methods cannot be applied to large graphs, as they operate on dense matrices.</p>    <p>Some efforts have been made to overcome this limitation using enhanced linear algebra tools. Ahmed&#x00A0;et&#x00A0;al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] adopt stochastic gradient optimization for fast adjacency matrix eigendecomposition; Ou&#x00A0;et&#x00A0;al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] utilize sparse generalized SVD to generate a graph embedding, <SmallCap>HOPE</SmallCap>, from a similarity matrix amenable to decomposition into two sparse proximity matrices. <SmallCap>HOPE</SmallCap> is the first to support diverse similarity measures; however, it still requires the entire graph matrix as input and views the problem as one of linear dimensionality reduction rather than as one of nonlinear learning. This way, it deviates not only from current research on graph embeddings but also from older works&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>].</p>    <p>     <strong>Neural methods for representation 		 learning.</strong> Advances in machine learning have led to the adoption of neural methods for learning representations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. Building on the success of deep learning in domains such as image processing&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] and Natural Language Processing (NLP)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], <SmallCap>word2vec</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] builds word embeddings by training a single-layer neural network to guess the contextual words of a given word in a text. Likewise, GloVe&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] learns a word space through a stochastic version of SVD in a transformed cooccurrence matrix. While such text-based methods inherently take neighbor relationships into account, they require conceptual adaptations to model graphs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>].</p>    <p>     <strong>Neural Graph Embeddings.</strong> The success of neural word embeddings inspired natural extensions towards learning graph representations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>]. <SmallCap>DeepWalk</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] first proposed to learn latent representations in a lowdimensional vector space exploiting <em>local</em> node neighborhoods. It runs a series of random walks of fixed length from each vertex and creates a matrix of <em>d</em>-dimensional vertex representations using the SkipGram algorithm of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. These representations maximize the posterior probability of observing a neighboring vertex in a random walk. <SmallCap>DeepWalk</SmallCap> embeddings can inform classification tasks using a simple linear classifier such as logistic regression.</p>    <p>     <SmallCap>GraRep</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] suggests using Singular Value Decomposition (SVD) on a log-transformed <SmallCap>DeepWalk</SmallCap> transition probability matrix of different orders, and then concatenate the resulting representations. <SmallCap>Struc2vec</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] rewires the graph to reflect isomorphism among nodes and capture structural similarities, and then derives an embedding relying on the <SmallCap>DeepWalk</SmallCap> core. Works such as [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>] investigate deep learning approaches for graph embeddings. Their results amount to complex models that require elaborate parameter tuning and computationally expensive optimization, leading to time and space complexities unsuitable for large graphs.</p>    <p>Nevertheless, all <SmallCap>DeepWalk</SmallCap>-based approaches use objective functions that are not tailored to graph structures. Some works&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] try to infuse graph-native principles into the learning process. <SmallCap>LINE</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] proposed graph embeddings that capture more elaborate proximity notions. However, even <SmallCap>LINE</SmallCap>&#x2019;s notion of proximity is restricted to the immediate neighborhoods of each node; that is insufficient to capture the complete palette of node relationships&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. Furthermore, <SmallCap>Node2vec</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] introduces two hyperparameters to regulate the generation of random walks and thereby tailor the learning process to the graph at hand in <em>semi-supervised</em> fashion. However, <SmallCap>Node2vec</SmallCap> remains attached to the goal of preserving local neighborhoods and requires laborious tuning for each dataset and each task.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Outline of related work 		  in terms of fulfilled 		  (<img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic3.jpg" 		  style="display: inline" class="img-responsive" 		  alt="" longdesc=""/>) and missing (<img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic4.jpg" style="display: inline" class="img-responsive" alt="" longdesc=""/>) properties of algorithm and similarity measure.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic2.jpg" class="img-responsive" alt="" longdesc=""/>       </td>      </tr>     </tbody>     </table>    </div>    <p>     <strong>Overview.</strong> Table&#x00A0;<a class="tbl" href="#tab1">1</a> outlines five desirable properties for a graph embedding, and the extent to which previous methods possess them. We distinguish between properties of <em>algorithms</em>, on the one hand, and those of any implicit or explicit <em>similarity measure</em> among nodes a method may express, on the other hand.</p>    <ul class="list-no-style">     <li id="list5" label="&#x2022;"><strong>local</strong>: not requiring the entire graph matrix as input; <SmallCap>GraRep</SmallCap> , <SmallCap>DNGR</SmallCap> , and <SmallCap>HOPE</SmallCap> fail in this respect.<br/></li>     <li id="list6" label="&#x2022;"><strong>scalable</strong>: capable to process graphs with more than 10<sup>6</sup> nodes in less than a day; some methods fail in this criterion due to the dense matrix (<SmallCap>GraRep</SmallCap>), deep learning computations (<SmallCap>SDNE</SmallCap>), or both (<SmallCap>DNGR</SmallCap>).<br/></li>     <li id="list7" label="&#x2022;"><strong>nonlinear</strong>: employing nonlinear transformations; <SmallCap>HOPE</SmallCap> relies on a linear dimensionality reduction method, SVD; that is detrimental to its performance on building graph representations, just like linear dimensionality reduction methods fail to confer the advantages of their nonlinear counterparts in general&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>].<br/></li>     <li id="list8" label="&#x2022;"><strong>global</strong>: capable to model relationships between <em>any</em> pair of nodes; <SmallCap>LINE</SmallCap> and <SmallCap>SDNE</SmallCap> do not share this property as they fail to look beyond a node&#x0027;s immediate neighborhood.<br/></li>     <li id="list9" label="&#x2022;"><strong>versatile</strong>: supporting diverse similarity functions; <SmallCap>HOPE</SmallCap> does so, yet is compromised by its <em>linear</em> character.<br/></li>    </ul>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Versatile Graph Embedding</h2>     </div>    </header>    <p>     <SmallCap>VERSE</SmallCap> possesses all properties mentioned in our taxonomy; it employs <em>nonlinear</em> transformation, desirable for dimensionality reduction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]; it is <em>local</em> in terms of the input it requires per node, but <em>global</em> in terms of the potential provenance of that input; it is <em>scalable</em> as it is based on sampling, and <em>versatile</em> by virtue of its generality.</p>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> VERSE Objective</h3>     </div>     </header>     <p>Given a graph = (<em>V</em>, <em>E</em>), where <em>V</em> = (<em>v</em>     <sub>1</sub>, &#x2026;, <em>v<sub>n</sub>     </em>), <em>n</em> = |<em>V</em>|, is the set of vertices and <em>E</em>&#x2286;(<em>V</em> &#x00D7; <em>V</em>) the set of edges, we aim to learn a <em>nonlinear representation</em> of vertices <em>v</em> &#x2208; <em>V</em> to <em>d</em>-dimensional embeddings, where <em>d</em> &#x226A; <em>n</em>. Such representation is encoded into a <em>n</em> &#x00D7; <em>d</em> matrix <em>W</em>; the embedding of a node <em>v</em> is the row <em>W</em>     <sub>      <em>v</em>, &#x00B7;</sub> in the matrix; we denote it as <em>W<sub>v</sub>     </em> for compactness.</p>     <p>Our embeddings reflect <em>distributions</em> of a given graph similarity <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}: V \times V \rightarrow \mathbb {R}$</span>     </span> for every node <em>v</em> &#x2208; <em>V</em>. As such, we require that the similarities from any vertex <em>v</em> to all other vertices <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}(v,\cdot)$</span>     </span> are amenable to be interpreted as a distribution with <span class="inline-equation"><span class="tex">$\sum _{u \in V}{{sim}_\mathrm{G}(v,u)} = 1$</span>     </span> for all <em>v</em> &#x2208; <em>V</em>. We aim to devise <em>W</em> by a scalable method that requires neither the <em>V</em> &#x00D7; <em>V</em> stochastic similarity matrix nor its explicit materialization.</p>     <p>The corresponding node-to-node similarity in the embedded space is <span class="inline-equation"><span class="tex">${sim}_\mathrm{E}: V \times V \rightarrow \mathbb {R}$</span>     </span>. As an optimization objective, we aim to minimize the Kullback-Leibler (KL) divergence from the given similarity distribution <em>sim</em> to that of <em>sim</em>     <sub>E</sub> in the embedded space:</p>     <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \sum _{v \in V} \mathrm{KL}\left({sim}_\mathrm{G}(v,\cdot) \;||\; {sim}_\mathrm{E}(v, \cdot)\right) \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div>     <p>We illustrate the usefulness of this objective using a small similarity matrix. Figure&#x00A0;<a class="fig" href="#fig2">2</a> shows (a) the Personalized PageRank matrix, (b) the reconstruction of the same matrix by <SmallCap>VERSE</SmallCap>, and (c) the reconstruction of the same matrix using SVD. It is visible that the nonlinear minimization of KL divergence between distributions preserves most of the information in the original matrix, while the linear SVD-based reconstruction fails to differentiate some nodes. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">An example similarity matrix and its reconstructions by <SmallCap>VERSE</SmallCap> and SVD. Karate club graph&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0053">53</a>], dimensionality <em>d</em> = 4 for both methods.</span>      </div>     </figure>     </p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> VERSE Embedding Model</h3>     </div>     </header>     <p>We define the unnormalized distance between two nodes <em>u</em>, <em>v</em> in the embedding space as the dot product of their embeddings <span class="inline-equation"><span class="tex">$W_u \cdot W_v^\top {}$</span>     </span>. The similarity distribution in the embedded space is then normalized with softmax: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {sim}_\mathrm{E}(v, \cdot) = \frac{\exp (W_v W^{\top })}{\sum _{i=1}^{n}{\exp \left(W_v \cdot W_i\right)} } \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>     </p>     <p>By Equation&#x00A0;<a class="eqn" href="#eq1">1</a>, we should minimize the KL-divergence from <em>sim</em> to <em>sim</em>     <sub>E</sub>; omitting parts dependent on <em>sim</em> only, this objective is equivalent to minimizing the cross-entropy loss function&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>]:</p>     <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \mathcal {L}= -\sum _{v \in V} {{sim}_\mathrm{G}(v,\cdot) \log \left({sim}_\mathrm{E}\left(v,\cdot \right)\right) } \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>     </div>     <p>We can accommodate this objective by stochastic 		 gradient descent, which allows updating the model 		 on each node singularly. However, a na&#x00EF;ve 		 version of gradient descent would require the full 		 materialization of <em>sim</em><sub>E</sub> and 		 <span class="inline-equation"><span class="tex">${sim}_{\rm 		  G}$</span>     </span>. Even in case <span class="inline-equation"><span class="tex">${sim}_{\rm 		  G}$</span>     </span> is easy to compute on the fly, such as the adjacency matrix, the softmax in Equation&#x00A0;<a class="eqn" href="#eq2">2</a> still has to be normalized over all nodes in the graph.</p>     <p>We use Noise Contrastive Estimation (NCE)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], which allows us to learn a model that provably converges to its objective (see&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>],&#x00A0;Theorem&#x00A0;2). NCE trains a binary classifier to distinguish between node samples coming from the empirical similarity distribution <em>sim</em> and those generated by a noise distribution <span class="inline-equation"><span class="tex">$\mathcal {Q}{}$</span>     </span> over the nodes. Consider an auxiliary random variable <em>D</em> for node classification, such that <em>D</em> = 1 for a node drawn from the empirical distribution and <em>D</em> = 0 for a sample drawn from the noise distribution. Given a node <em>u</em> drawn from some distribution <span class="inline-equation"><span class="tex">$\mathcal {P}{}$</span>     </span> and a node <em>v</em> drawn from the 		 distribution of 		 <span class="inline-equation"><span class="tex">${sim}_{\rm 		  G}\left(u,\cdot \right)$</span>     </span>, we draw <em>s</em> &#x226A; <em>n</em> nodes <span class="inline-equation"><span class="tex">$\widetilde{v}{}$</span>     </span> from <span class="inline-equation"><span class="tex">$\mathcal {Q}(u)$</span>     </span> and use logistic regression to minimize the negative log-likelihood:</p>     <div class="table-responsive" id="eq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} \mathcal {L}_{NCE} = \sum _{{{{\scriptstyle {\begin{array}{*10c} u\sim \mathcal {P}\\v \sim {sim}_\mathrm{G}(u,\cdot) \end{array}}}}}} \Big [ &#x0026; \log \textstyle \Pr _W(D=1|{sim}_\mathrm{E}(u, v))+ \\s \mathbb {E}_{\widetilde{v}\sim \mathcal {Q}(u)} &#x0026; \log \textstyle \Pr _{W}(D=0|{sim}_\mathrm{E}(u, \widetilde{v}))\Big ] \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>     </div>     <p>where <span class="inline-equation"><span class="tex">$\Pr _W$</span>     </span> is computed from <em>W</em> as a sigmoid <em>&#x03C3;</em>(<em>x</em>) = (1 + <em>e</em>     <sup>&#x2212; <em>x</em>     </sup>)<sup>&#x2212; 1</sup> of the dot product between vectors <em>W<sub>u</sub>     </em> and <em>W<sub>v</sub>     </em>, while we compute <em>sim</em>     <sub>E</sub>(<em>u</em>, &#x00B7;) <em>without</em> the normalization of Equation&#x00A0;<a class="eqn" href="#eq2">2</a>. As the number of noise samples <em>s</em> increases, the NCE derivative provably converges to the gradient of cross-entropy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]; thus, by virtue of NCE&#x0027;s asymptotic convergence guarantees, we are in effect minimizing the KL-divergence from <em>sim</em>. NCE&#x0027;s theoretical guarantees depend on <em>s</em>, yet small values work well in practice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. In our experiments, we use <em>s</em> = 3. These convergence guarantees of NCE are not affected by choice of distributions <span class="inline-equation"><span class="tex">$\mathcal {P}{}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {Q}{}$</span>     </span> (see&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>],&#x00A0;Corollary&#x00A0;5); however, its performance is empirically dependent on <span class="inline-equation"><span class="tex">$\mathcal {Q}{}$</span>     </span>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>].</p>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Instantiations of <SmallCap>VERSE</SmallCap>      </h3>     </div>     </header>     <p>While <SmallCap>VERSE</SmallCap> can be used with 		 any similarity function, we choose to instantiate our model to widely used similarities <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}$</span>     </span>, namely Personalized PageRank (PPR), Adjacency Similarity, and SimRank.</p>     <p>     <strong>Personalized PageRank.</strong> Personalized PageRank&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] is a common similarity measure among nodes, practically used for many graph mining tasks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>].</p>     <div class="definition" id="enc1">     <Label>Definition 3.1.</Label>     <p> Given a starting node distribution <em>s</em>, <em>damping factor &#x03B1;</em>, and the normalized adjacency matrix <em>A</em>, the <strong>Personalized PageRank</strong> vector <em>&#x03C0;<sub>s</sub>      </em> is defined by the recursive equation: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \pi _s = \alpha s +(1-\alpha)\pi _{s}A \] </span>        <br/>       </div>      </div>     </p>     </div>     <p>The stationary distribution of a random walk with restart with probability <em>&#x03B1;</em> converges to PPR&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>]. Thus, a sample from <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}(v, \cdot)$</span>     </span> is the last node in a single random walk from node <em>v</em>. The damping factor <em>&#x03B1;</em> controls the average size of the explored neighborhood. In Section&#x00A0;<a class="sec" href="#sec-13">3.6</a> we show that <em>&#x03B1;</em> is tightly coupled with the window size parameter <em>w</em> of <SmallCap>DeepWalk</SmallCap> and <SmallCap>Node2vec</SmallCap>.</p>     <p>     <strong>Adjacency similarity</strong>. A straightforward similarity measure is the normalized adjacency matrix; this similarity corresponds to the <SmallCap>LINE</SmallCap>-1 model and takes into account only the immediate neighbors of each node. More formally, given the out degree <em>Out</em>(<em>u</em>) of node <em>u</em>     </p>     <div class="table-responsive" id="eq5">     <div class="display-equation">      <span class="tex mytex">\begin{align} {sim}_\mathrm{G}^{\mathrm{ADJ}}(u,v) = {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}1/Out(u) &#x0026; \text{if } (u,v) \in E \\ 0 &#x0026; \text{otherwise} \end{array}\right.} \end{align} </span>      <br/>      <span class="equation-number">(5)</span>     </div>     </div>     <p>We experimentally demonstrate that <SmallCap>VERSE</SmallCap> model is effective even in preserving the adjacency matrix of the graph.</p>     <p>     <strong>SimRank</strong>. SimRank&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] is a measure of structural relatedness between two nodes, based on the assumption that two nodes are similar if they are connected to other similar nodes; SimRank is defined recursively as follows:</p>     <div class="table-responsive" id="eq6">     <div class="display-equation">      <span class="tex mytex">\begin{equation} {sim}_\mathrm{G}^{\mathrm{SR}}(u,v) = \frac{C}{\left|I(u)\right| \left|I(v)\right|} \sum _{i=1}^{\left|I(u)\right|}\sum _{j=1}^{\left|I(v)\right|} {sim}_\mathrm{G}^{\mathrm{SR}}(I_i(u), I_j(v)) \end{equation} </span>      <br/>      <span class="equation-number">(6)</span>     </div>     </div>     <p>where <em>I</em>(<em>v</em>) denotes the set of in-neighbors of node <em>v</em>, and <em>C</em> is a number between 0 and 1 that geometrically discounts the importance of farther nodes. SimRank is a recursive procedure that involves computationally expensive operations: the straightforward method has the complexity of <span class="inline-equation"><span class="tex">$\mathcal {O}(n^4)$</span>     </span>.</p>     <p>SimRank values can be approximated up to a multiplicative factor dependent on <em>C</em> through SimRank-Aware Random Walks (SARW)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. SARW computes a SimRank approximation through two reversed random walks with restart where the damping factor <em>&#x03B1;</em> is set to <span class="inline-equation"><span class="tex">$\alpha = \sqrt {C}$</span>     </span>. A reversed random walk traverses any edge (<em>u</em>, <em>v</em>) in the opposite direction (<em>v</em>, <em>u</em>). Since we are only interested in the distribution of each <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}^{\mathrm{SR}}(v, \cdot)$</span>     </span>, we can ignore the multiplicative factor in the approximation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>] that has little impact on our task. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-fig3.jpg" class="img-responsive" alt="" longdesc=""/>     </figure>     </p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> VERSE Algorithm</h3>     </div>     </header>     <p>Algorithm&#x00A0;1 presents the overall flow of <SmallCap>VERSE</SmallCap>. Given a graph, a similarity function <em>sim</em>, and the embedding space dimensionality <em>d</em>, we initialize the output embedding matrix <em>W</em> to <span class="inline-equation"><span class="tex">$\mathcal {N}(0, \frac{1}{d})$</span>     </span>. Then, we optimize our objective (Equation&#x00A0;<a class="eqn" href="#eq4">4</a>) by gradient descent using the NCE algorithm discussed in the previous section. To do so, we repeatedly sample a node from the positive distribution <span class="inline-equation"><span class="tex">$\mathcal {P}{}$</span>     </span>, sample the <em>sim</em> (e.g. pick a neighboring node), and draw <em>s</em> negative examples. The <em>&#x03C3;</em> in Line&#x00A0;13 represents the sigmoid function <em>&#x03C3;</em> = (1 + <em>e</em>     <sup>&#x2212; <em>x</em>     </sup>)<sup>&#x2212; 1</sup>, and <em>&#x03BB;</em> the learning rate. We choose <span class="inline-equation"><span class="tex">$\mathcal {P}{}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {Q}{}$</span>     </span> to be distributed uniformly by <span class="inline-equation"><span class="tex">$\mathcal {U}(1,n)$</span>     </span>.</p>     <p>As a strong baseline for applications handling smaller graphs, we also consider an elaborate, exhaustive variant of <SmallCap>VERSE</SmallCap>, which computes <em>full similarity distribution</em> vectors per node instead of performing NCE-based sampling. We name this variant <SmallCap>fVERSE</SmallCap> and include it in our experimental study.</p>     <p>Figure&#x00A0;<a class="fig" href="#fig4">3</a> presents our measures on the ability to reconstruct a similarity matrix for (i) <SmallCap>VERSE</SmallCap> using NCE; (ii) <SmallCap>VERSE</SmallCap> using Negative Sampling (NS) (also used in <SmallCap>Node2vec</SmallCap>); and (ii) the exhaustive <SmallCap>fVERSE</SmallCap> variant. We observe that, while NCE approaches the exhaustive method in terms of matching the ground truth top-100 most similar nodes, NS fails to deliver the same quality. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-fig4.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Ranking preformance in terms of NDCG for reconstructing PPR similarity, averaged across nodes in a graph.</span>      </div>     </figure>     </p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Complexity Comparison</h3>     </div>     </header>     <p>Table&#x00A0;<a class="tbl" href="#tab2">2</a> presents the average (<em>&#x0398;</em>) and worst-case (<span class="inline-equation"><span class="tex">$\mathcal {O}{}$</span>     </span>) time and space complexity of <SmallCap>VERSE</SmallCap>, along with those of methods in previous works; <em>d</em> is the embedding dimensionality, <em>n</em> the number of nodes, <em>m</em> the number of edges, and <em>s</em> the number of samples used, and <em>t</em> the number of iterations in <SmallCap>GraRep</SmallCap>. Methods that rely on fast sampling (<SmallCap>VERSE</SmallCap> and <SmallCap>LINE</SmallCap>) require time linear in <em>n</em> and space quadratic in <em>n</em> in the worst case. <SmallCap>DeepWalk</SmallCap> requires <span class="inline-equation"><span class="tex">$\mathcal {O}(n\log n)$</span>     </span> time due to its use of hierarchical softmax. <SmallCap>Node2vec</SmallCap> stores the neighborsofaneighbor, incurring a quadratic cost in sparse graphs, but cubic in dense graphs. Thus, <SmallCap>VERSE</SmallCap> comes at the low end of complexities compared to previous work on graph embeddings. Remarkably, even the computationally expensive <SmallCap>fVERSE</SmallCap> affords complexity comparable to some previous works.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Comparison of neural embedding methods in terms of average (<em>&#x0398;</em>) and worst-case (<span class="inline-equation"><span class="tex">$\mathcal {O}{}$</span>       </span>) time and space complexity.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic8.jpg" class="img-responsive" alt="" longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.6</span> Similarity Notions in Previous Approaches</h3>     </div>     </header>     <p>Here, we provide additional theoretical considerations of <SmallCap>VERSE</SmallCap> compared to <SmallCap>LINE</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>], <SmallCap>DeepWalk</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] and <SmallCap>Node2vec</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] and demonstrate how our general model subsumes and extends previous research in versatility and scalability.</p>     <p>     <strong>Comparison with</strong>      <SmallCap>      <strong>DeepWalk</strong>     </SmallCap>     <strong>and</strong>      <SmallCap>      <strong>Node2vec</strong>     </SmallCap>     <strong>.</strong>     <SmallCap>DeepWalk</SmallCap> and <SmallCap>Node2vec</SmallCap> generate samples from random walks of fixed window size <em>w</em> by the <SmallCap>word2vec</SmallCap> sampling strategy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. We derive a relationship between the window size <em>w</em> of that strategy and the damping factor <em>&#x03B1;</em> of Personalized PageRank.</p>     <div class="lemma" id="enc2">     <Label>Lemma 3.2.</Label>     <p> Let <em>X<sub>r</sub>      </em> be the random variable that represents the length of a random walk <em>r</em> sampled with parameter <em>w</em> by the <SmallCap>word2vec</SmallCap> sampling strategy. Then for any 0 < <em>j</em> &#x2264; <em>w</em>      <div class="table-responsive" id="eq7">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \Pr (X_r=j) = \frac{2}{w(w+1)}(w-j + 1) \end{equation} </span>        <br/>        <span class="equation-number">(7)</span>       </div>      </div>     </p>     </div>     <div class="proof" id="proof1">     <Label>Proof.</Label>     <p>For each node <em>v</em> &#x2208; <em>V</em>, <SmallCap>word2vec</SmallCap> strategy samples two random walks of length <em>w</em> starting from <em>v</em> &#x2208; <em>V</em>. These two random walks represents the <em>context</em> of <em>v</em>, where <em>v</em> is the central node of a walk of length 2<em>w</em> + 1. The model is then trained on increasing context size up to <em>w</em>. Therefore, the number of nodes sampled for each random walk amount to <span class="inline-equation"><span class="tex">$\sum _{i=1}^w i= \frac{w(w+1)}{2}$</span>      </span>. A node at distance 0 < <em>j</em> &#x2264; <em>w</em> is sampled (<em>w</em> &#x2212; <em>j</em> + 1) times; thus, the final probability is <span class="inline-equation"><span class="tex">$\frac{2}{w(w+1)}(w-j + 1)$</span>      </span>.</p>     </div>     <p>Personalized PageRank provides the maximum likelihood estimation for the distribution in Equation&#x00A0;<a class="eqn" href="#eq7">7</a> for <span class="inline-equation"><span class="tex">$\alpha = \frac{w-1}{w+1}$</span>     </span>. Then, <em>w</em> = 10 corresponds to <em>&#x03B1;</em> = 0.82, which is close to the standard <em>&#x03B1;</em> = 0.85, proved effective in practice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>]. On the other hand, <em>&#x03B1;</em> = 0.95, which, for example, achieves the best performance on a task in Section&#x00A0;<a class="sec" href="#sec-16">4.2</a>, corresponds to <em>w</em> = 39. Such large <em>w</em> prohibitively increases the computation time for <SmallCap>DeepWalk</SmallCap> and <SmallCap>Node2vec</SmallCap>.</p>     <p>     <strong>Comparison with</strong>      <SmallCap>      <strong>LINE</strong>     </SmallCap>     <strong>.</strong>     <SmallCap>LINE</SmallCap> introduces the concept of first- and second-order proximities to model complex node relationships. As we discussed, in <SmallCap>VERSE</SmallCap>, first-order proximity corresponds to the dot-product among the similarity vectors in the embedding space: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} {sim}_\mathrm{E}(u,v) = W_u \cdot W_v\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>On the other hand, second-order proximity corresponds to letting <SmallCap>VERSE</SmallCap> learn one more matrix <em>W</em>&#x2032;, so as to model <em>asymmetric</em> similarities of nodes in the embedding space. We do that by defining <em>sim</em>     <sub>E</sub> asymmetrically, using both <em>W</em> and <em>W</em>&#x2032;: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} {sim}_\mathrm{E}(u,v) = W_u \cdot W^{\prime }_v\end{equation*} </span>       <br/>      </div>     </div>     </p>     <p>The intuition behind second-order proximity is the same as that of SimRank: similar nodes have similar neighborhoods. Every previous method, except for <SmallCap>LINE</SmallCap>-1, used second-order proximities, due to the <SmallCap>word2vec</SmallCap> interpretation of embeddings borrowed by <SmallCap>DeepWalk</SmallCap> and <SmallCap>Node2vec</SmallCap>. In our model, second-order proximities can be encoded by adding an additional matrix; we empirically evaluate their effectiveness in Section&#x00A0;<a class="sec" href="#sec-14">4</a>.</p>    </section>   </section>   <section id="sec-14">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <p>We evaluate <SmallCap>VERSE</SmallCap> against several state-of-the-art graph embedding algorithms. For repeatability purposes, we provide all data sets and the C++ source code for <SmallCap>VERSE</SmallCap> <a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, <SmallCap>DeepWalk</SmallCap> <a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> and <SmallCap>Node2vec</SmallCap> <a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. We run the experiments on an Amazon AWS c4.8 instance with 60Gb RAM. Each method is assessed on the best possible parameters, with early termination of the computation in case no result is returned within one day. We provide the following state-of-the-art graph embedding methods for comparison:</p>    <ul class="list-no-style">     <li id="list10" label="&#x2022;"><SmallCap>DeepWalk</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]: This approach learns an embedding by sampling random walks from each node, applying <SmallCap>word2vec</SmallCap>-based learning on those walks. We use the default parameters described in the paper, i.e., walk length <em>t</em> = 80, number of walks per node <em>&#x03B3;</em> = 80, and window size <em>w</em> = 10.<br/></li>     <li id="list11" label="&#x2022;"><SmallCap>LINE</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]: This approach learns a <em>d</em>-dimensional embedding in two steps , both using adjacency similarity. First, it learns <em>d</em>/2 dimensions using first-order proximity; then, it learns another <em>d</em>/2 features using second-order proximity. Last, the two halves are normalized and concatenated. We obtained a copy of the code<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> and run experiments with total <em>T</em> = 10<sup>10</sup> samples and <em>s</em> = 5 negative samples, as described in the paper.<br/></li>     <li id="list12" label="&#x2022;"><SmallCap>GraRep</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>]: This method factorizes the full adjacency similarity matrix using SVD, multiplies the matrix by itself, and repeats the process <em>t</em> times. The final embedding is obtained by concatenating the factorized vectors. We use <em>t</em> = 4 and 32 dimensions for each SVD factorization; thus, the final embedding has <em>d</em> = 128.<br/></li>     <li id="list13" label="&#x2022;"><SmallCap>HOPE</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]: This method is a revised Singular Value Decomposition restricted to sparse similarity matrices. We report the results obtained running <SmallCap>HOPE</SmallCap> with the default parameters, i.e, Katz similarity (an extension of Katz centrality [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>]) as the similarity measure and <em>&#x03B2;</em> inversely proportional to the spectral radius. Since Katz similarity does not converge on directed graphs with sink nodes, we used Personalized PageRank with <em>&#x03B1;</em> = 0.85 for the CoCit dataset.<br/></li>     <li id="list14" label="&#x2022;"><SmallCap>Node2vec</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]: This is a <em>hyperparameter-supervised</em> approach that extends <SmallCap>DeepWalk</SmallCap> by adding two parameters, <em>p</em> and <em>q</em>, so as to control <SmallCap>DeepWalk</SmallCap>&#x2019;s random walk sampling. The special case with parameters <em>p</em> = 1, <em>q</em> = 1 corresponds to <SmallCap>DeepWalk</SmallCap> ; yet, sometimes <SmallCap>Node2vec</SmallCap> shows worse performance than <SmallCap>DeepWalk</SmallCap> in our evaluation, due to the fact it uses negative sampling, while <SmallCap>DeepWalk</SmallCap> uses hierarchical softmax. We fine-tuned the hyperparameters <em>p</em> and <em>q</em> on each dataset and task. Moreover, we used a large training data to fairly compare to <SmallCap>DeepWalk</SmallCap>, i.e., walk length <em>l</em> = 80, number of walks per node <em>r</em> = 80, and window size <em>w</em> = 10.<br/></li>    </ul>    <p>     <strong>Baselines.</strong> In addition to graph embeddings methods, we implemented the following baselines.</p>    <ul class="list-no-style">     <li id="list15" label="&#x2022;">Logistic regression: We use the well-known logistic regression method as a baseline for link prediction. We train the model on a set of common node-specific features, namely node degree, number of common neighbors, Adamic-Adar, Jaccard coefficient, preferential attachment, and resource allocation index&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>].<br/></li>     <li id="list16" label="&#x2022;">Louvain community detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]: We employ a standard partition method for community detection as a baseline for graph clustering, reporting the best partition in terms of modularity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>].<br/></li>    </ul>    <div class="table-responsive" id="tab3">     <div class="table-caption">     <span class="table-number">Table 3:</span>     <span class="table-title">Dataset characteristics: number of vertices |<em>V</em>|, number of edges |<em>E</em>|; number of node labels <span class="inline-equation"><span class="tex">$|\mathcal {L}|$</span>      </span>; average node degree; modularity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0032">32</a>]; density defined as <span class="inline-equation"><span class="tex">$|E|/\binom{|V|}{2}$</span>      </span>.</span>     </div>     <table class="table"> 		 <thead>      <tr>       <th style="text-align:center;"/>       <th colspan="3" style="text-align:center;">        <strong>Size</strong>        <hr/>       </th>       <th colspan="3" style="text-align:center;">        <strong>Statistics</strong>        <hr/>       </th>      </tr>      <tr>       <th style="text-align:left;">        <em>dataset</em>       </th>       <th style="text-align:center;">|<em>V</em>|</th>       <th style="text-align:center;">|<em>E</em>|</th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$|\mathcal {L}|$</span>        </span>       </th>       <th style="text-align:center;">Avg.&#x00A0;degree</th>       <th style="text-align:center;">Mod.</th>       <th style="text-align:right;">Density</th>      </tr> 		  </thead>     <tbody>      <tr>       <td style="text-align:left;">BlogCatalog</td>       <td style="text-align:center;">10k</td>       <td style="text-align:right;">334k</td>       <td style="text-align:center;">39</td>       <td style="text-align:center;">64.8</td>       <td style="text-align:center;">0.24</td>       <td style="text-align:right;">6.3 &#x00D7; 10<sup>&#x2212; 3</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">CoCit</td>       <td style="text-align:center;">44k</td>       <td style="text-align:right;">195k</td>       <td style="text-align:center;">15</td>       <td style="text-align:center;">8.86</td>       <td style="text-align:center;">0.72</td>       <td style="text-align:right;">2.0 &#x00D7; 10<sup>&#x2212; 4</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">CoAuthor</td>       <td style="text-align:center;">52k</td>       <td style="text-align:right;">178k</td>       <td style="text-align:center;">&#x2014;</td>       <td style="text-align:center;">6.94</td>       <td style="text-align:center;">0.84</td>       <td style="text-align:right;">1.3 &#x00D7; 10<sup>&#x2212; 4</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">VK</td>       <td style="text-align:center;">79k</td>       <td style="text-align:right;">2.7M</td>       <td style="text-align:center;">2</td>       <td style="text-align:center;">34.1</td>       <td style="text-align:center;">0.47</td>       <td style="text-align:right;">8.7 &#x00D7; 10<sup>&#x2212; 4</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">YouTube</td>       <td style="text-align:center;">1.1M</td>       <td style="text-align:right;">3M</td>       <td style="text-align:center;">47</td>       <td style="text-align:center;">5.25</td>       <td style="text-align:center;">0.71</td>       <td style="text-align:right;">9.2 &#x00D7; 10<sup>&#x2212; 6</sup>       </td>      </tr>      <tr>       <td style="text-align:left;">Orkut</td>       <td style="text-align:center;">3.1M</td>       <td style="text-align:right;">234M</td>       <td style="text-align:center;">50</td>       <td style="text-align:center;">70</td>       <td style="text-align:center;">0.68</td>       <td style="text-align:right;">2.4 &#x00D7; 10<sup>&#x2212; 5</sup>       </td>      </tr>     </tbody>     </table>    </div>    <p>     <strong>Parameter settings.</strong> In line with previous research&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] we set the embedding dimensionality <em>d</em> to 128. The learning procedure (Algorithm&#x00A0;1, Line&#x00A0;3) is run 10<sup>5</sup> times for <SmallCap>VERSE</SmallCap> and 250 times for <SmallCap>fVERSE</SmallCap> ; the difference in setting is motivated by the number of model updates which is <span class="inline-equation"><span class="tex">$\mathcal {O}(n)$</span>     </span> in <SmallCap>VERSE</SmallCap> and <span class="inline-equation"><span class="tex">$\mathcal {O}(n^2)$</span>     </span> in <SmallCap>fVERSE</SmallCap>.</p>    <p>We use LIBLINEAR&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] to perform logistic regression with default parameter settings. Unlike previous work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] we employ a stricter assumption for multi-label node classification: the number of correct classes is not known apriori, but found through the Label Powerset multi-label classification approach&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>].</p>    <p>For link prediction and multi-label classification, we evaluated each individual embedding 10 times in order to reduce the noise introduced by the classifier. Unless otherwise stated, we run each experiment 10 times, and report the average value among the runs. Throughout our experimental study, we use the above parameters as default, unless indicated otherwise.</p>    <p>     <strong>Datasets.</strong> We test our methods on six real datasets; we report the main data characteristics in Table&#x00A0;<a class="tbl" href="#tab3">3</a>.</p>    <ul class="list-no-style">     <li id="list17" label="&#x2022;">BlogCatalog &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0054">54</a>] is a network of social interactions among bloggers in the BlogCatalog website. Node-labels represent topic categories provided by authors.<br/></li>     <li id="list18" label="&#x2022;">Microsoft Academic Graph&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>] is a network of academic papers, citations, authors, and affiliations from Microsoft Academic website released for the KDD-2016 cup. It contains 150 million papers up to February 2016 spanning various disciplines from math to biology. We extracted two separate subgraphs from the original network, using 15 conferences in data mining, databases, and machine learning. The first, CoAuthor, is a co-authorship network among authors. The second, CoCit, is a network of papers citing other papers; labels represent conferences in which papers were published.<br/></li>     <li id="list19" label="&#x2022;">VK is a Russian all-encompassing social network. We extracted two snapshots of the network in November 2016 and May 2017 to obtain information about link appearance. We use the gender of the user for classification and country for clustering.<br/></li>     <li id="list20" label="&#x2022;">YouTube &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] is a network of social interactions among users of the YouTube video platform. The labels represent groups of viewers by video genres.<br/></li>     <li id="list21" label="&#x2022;">Orkut &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0052">52</a>] is a network of social interactions among users of the Orkut social network platform. The labels represent communities of users. We extracted the 50 biggest communities and use them as labels for classification.<br/></li>    </ul>    <p>     <strong>Evaluation methodology.</strong> The default form of <SmallCap>VERSE</SmallCap> runs Personalized PageRank with <em>&#x03B1;</em> = 0.85. For the sake of fairness, we design a <em>hyperparameter-supervised</em> variant of <SmallCap>VERSE</SmallCap>, by analogy to the hyperparameter-tuned variant of <SmallCap>DeepWalk</SmallCap> introduced by <SmallCap>Node2vec</SmallCap> &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. This variant, <SmallCap>hsVERSE</SmallCap>, selects the best similarity with cross-validation across two proximity orders (as discussed in Section&#x00A0;<a class="sec" href="#sec-13">3.6</a>) and three similarities (Section&#x00A0;<a class="sec" href="#sec-10">3.3</a>) with <em>&#x03B1;</em> &#x2208; {0.45, 0.55, 0.65, 0.75, 0.85, 0.95} for <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}^{\mathrm{PPR}}$</span>     </span> and <em>C</em> &#x2208; {0.15, 0.25, 0.35, 0.45, 0.55, 0.65} for <span class="inline-equation"><span class="tex">${sim}_\mathrm{G}^{\mathrm{SR}}$</span>     </span>.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">     <span class="table-number">Table 4:</span>     <span class="table-title">Vector operators used for link-prediction task for each <em>u</em>, <em>v</em> &#x2208; <em>V</em> and corresponding embeddings <span class="inline-equation"><span class="tex">$\mathbf {a}, \mathbf {b} \in \mathbb {R}^d$</span>      </span>.</span>     </div>     <table class="table"> 		 <thead>      <tr>       <th style="text-align:center;">Operator</th>       <th style="text-align:center;">Result</th>      </tr> 		  </thead>     <tbody>      <tr>       <td style="text-align:center;">Average</td>       <td style="text-align:center;">(<strong>a</strong> + <strong>b</strong>)/2</td>      </tr>      <tr>       <td style="text-align:center;">Concat</td>       <td style="text-align:center;">[<strong>a</strong>        <sub>1</sub>, &#x2026;, <strong>a</strong>        <sub>        <em>d</em>        </sub>, <strong>b</strong>        <sub>1</sub>, &#x2026;, <strong>b</strong>        <sub>        <em>d</em>        </sub>]</td>      </tr>      <tr>       <td style="text-align:center;">Hadamard</td>       <td style="text-align:center;">[<strong>a</strong>        <sub>1</sub>*<strong>b</strong>        <sub>1</sub>, &#x2026;, <strong>a</strong>        <sub>        <em>d</em>        </sub>*<strong>b</strong>        <sub>        <em>d</em>        </sub>]</td>      </tr>      <tr>       <td style="text-align:center;">Weighted L1</td>       <td style="text-align:center;">[|<strong>a</strong>        <sub>1</sub> &#x2212; <strong>b</strong>        <sub>1</sub>|, &#x2026;, |<strong>a</strong>        <sub>        <em>d</em>        </sub> &#x2212; <strong>b</strong>        <sub>        <em>d</em>        </sub>|]</td>      </tr>      <tr>       <td style="text-align:center;">Weighted L2</td>       <td style="text-align:center;">[(<strong>a</strong>        <sub>1</sub> &#x2212; <strong>b</strong>        <sub>1</sub>)<sup>2</sup>, &#x2026;, (<strong>a</strong>        <sub>        <em>d</em>        </sub> &#x2212; <strong>b</strong>        <sub>        <em>d</em>        </sub>)<sup>2</sup>]</td>      </tr>     </tbody>     </table>    </div>    <div class="table-responsive" id="tab5">     <div class="table-caption">     <span class="table-number">Table 5:</span>     <span class="table-title">Link prediction results on the CoAuthor coauthorship graph. Best results per method are underlined.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic9.jpg" class="img-responsive" alt="" longdesc=""/>       </td>      </tr>     </tbody>     </table>    </div>    <div class="table-responsive" id="tab6">     <div class="table-caption">     <span class="table-number">Table 6:</span>     <span class="table-title">Link prediction results on the VK social graph. Best results per method are underlined.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic10.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>      </tr>     </tbody>     </table>    </div>    <div class="table-responsive" id="tab7">     <div class="table-caption">     <span class="table-number">Table 7:</span>     <span class="table-title">Multi-class classification results in CoCit dataset.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic11.jpg" class="img-responsive" alt=""         longdesc=""/>       </td>      </tr>     </tbody>     </table>    </div>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Link Prediction</h3>     </div>     </header>     <p>Link prediction is the task of anticipating the appearance of a link between two nodes in a network. Conventional measures for link prediction include Adamic-Adar, Preferential attachment, Katz, and Jaccard coefficient. We train a Logistic regression classifier on edge-wise features obtained with the methods shown in Table&#x00A0;<a class="tbl" href="#tab4">4</a>. For instance, for a pair of nodes <em>u</em>, <em>v</em>, the Concat operator returns a vector as the sequential concatenation of the embeddings <em>f</em>(<em>u</em>) and <em>f</em>(<em>v</em>). On the CoAuthor data, we predict new links for 2015 and 2016 co-authorships, using the network until 2014 for training; on VK, we predict whether a new friendship link appears between November 2016 and May 2017, using 50% of the new links for training and 50% for testing. We train the binary classifier by sampling non-existing edges as negative examples. Tables&#x00A0;<a class="tbl" href="#tab5">5</a> and&#x00A0;<a class="tbl" href="#tab6">6</a> report the attained accuracy. As a baseline, we use a logistic regression classifier trained on the respective data sets&#x2019; features.</p>     <p>     <SmallCap>VERSE</SmallCap> with Hadamard product of vectors is consistently the best edge representation. We attribute this quality to the explicit reconstruction we achieve using noise contrastive estimation. <SmallCap>VERSE</SmallCap> consistently outperforms the baseline in the tested datasets. Besides, the hyperparameter-supervised <SmallCap>hsVERSE</SmallCap> variant outruns <SmallCap>Node2vec</SmallCap> on all datasets.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Node Classification</h3>     </div>     </header>     <p>We now conduct an extensive evaluation on classification and report results for all the methods, where possible, with the CoCit , VK , YouTube , and Orkut graphs. Node classification aims to predict of the correct node labels in a graph, as described previously in this section.</p>     <p>We evaluate accuracy by the Micro-F1 and Macro-F1 percentage measures. We report only Macro-F1, since we experience similar behaviors with Micro-F1. For each dataset we conduct multiple experiments, selecting a random sample of nodes for training and leaving the remaining nodes for testing. The results for four datasets, shown in Tables&#x00A0;<a class="tbl" href="#tab7">7</a>-<a class="tbl" href="#tab10">10</a>, exhibit similar trends: <SmallCap>VERSE</SmallCap> yields predictions comparable or superior to those of the other contestants, while it scales to large networks such as&#x00A0;Orkut. <SmallCap>LINE</SmallCap> outperforms <SmallCap>VERSE</SmallCap> only in VK, where the gender of users is better captured using the direct neighborhood. The hyperparameter-supervised variant, <SmallCap>hsVERSE</SmallCap>, is on a par with <SmallCap>Node2vec</SmallCap> in terms of quality on CoCit and VK; on the largest datasets YouTube and Orkut, <SmallCap>hsVERSE</SmallCap> keeps outperforming unsupervised alternatives, while <SmallCap>Node2vec</SmallCap> depletes the memory.</p>     <div class="table-responsive" id="tab8">     <div class="table-caption">      <span class="table-number">Table 8:</span>      <span class="table-title">Multi-class classification results in VK dataset.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic12.jpg" class="img-responsive" alt=""          longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab9">     <div class="table-caption">      <span class="table-number">Table 9:</span>      <span class="table-title">Multi-label classification results in YouTube dataset.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic13.jpg" class="img-responsive" alt=""          longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab10">     <div class="table-caption">      <span class="table-number">Table 10:</span>      <span class="table-title">Multi-class classification results in Orkut dataset.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic14.jpg" class="img-responsive" alt=""          longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Node Clustering</h3>     </div>     </header>     <p>Graph clustering detects groups of nodes with similar characteristics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. We assess the embedding methods, using the <em>k</em>-means algorithm with <em>k</em>-means<tt>++</tt> initialization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] to cluster the embedded points in a <em>d</em>-dimensional space. Table&#x00A0;<a class="tbl" href="#tab11">11</a> reports the Normalized Mutual Information (NMI) with respect to the original label distribution. On CoAuthor, <SmallCap>VERSE</SmallCap> has comparable performance with <SmallCap>DeepWalk</SmallCap>; yet on VK, <SmallCap>VERSE</SmallCap> outperforms all other methods.</p>     <p>We also assess graph embeddings on their ability to capture the graph community structure. We apply <em>k</em>-means with different <em>k</em> values between 2 and 50 and select the best modularity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>] score. Table&#x00A0;<a class="tbl" href="#tab12">12</a> presents our results, along with the modularity obtained by the Louvain method, the state-of-the-art modularity maximization algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]. <SmallCap>VERSE</SmallCap> variants produce result almost equal that those of Louvain, outperforming previous methods, while the three methods that could manage the Orkut data perform similarly.</p>     <div class="table-responsive" id="tab11">     <div class="table-caption">      <span class="table-number">Table 11:</span>      <span class="table-title">Node clustering results in terms of NMI.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic15.jpg" class="img-responsive" alt=""          longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab12">     <div class="table-caption">      <span class="table-number">Table 12:</span>      <span class="table-title">Node clustering results in terms of modularity.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic16.jpg" class="img-responsive" alt=""          longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Graph Reconstruction</h3>     </div>     </header>     <p>Good graph embeddings should preserve the graph structure in the embedding space. We evaluate the performance of our method on reconstructing the graph&#x0027;s adjacency matrix. Since each adjacent node should be close in the embedding space, we first sort any node other than the one considered by decreasing cosine distance among the vectors. Afterwards, we take a number of nodes equal to the actual degree of the node in the graph and connect to the considered node to create the graph structure.</p>     <p>Table&#x00A0;<a class="tbl" href="#tab13">13</a> reports the relative accuracy measured as the number of correct nodes in the neighborhood of a node in the embedding space. Again, <SmallCap>VERSE</SmallCap> performs comparably well; its exhaustive variant, <SmallCap>fVERSE</SmallCap>, which harnesses the full similarity does even better; however, the top performer is <SmallCap>hsVERSE</SmallCap> , which achieves the obtained result when instantiated to the Adjacency Similarity. This result is unsurprising, given that the adjacency similarity measure tailors <SmallCap>hsVERSE</SmallCap> for the task of graph reconstruction.</p>     <div class="table-responsive" id="tab13">     <div class="table-caption">      <span class="table-number">Table 13:</span>      <span class="table-title">Graph reconstruction % for all datasets.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:left;">        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic17.jpg" class="img-responsive" alt=""          longdesc=""/>        </td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Parameter Sensitivity</h3>     </div>     </header>     <p>We also evaluate the sensitivity of <SmallCap>VERSE</SmallCap> to parameter choice. Figures&#x00A0;, depict node classification performance in terms of Micro-F1 on the BlogCatalog dataset, with 10% of nodes labeled.</p>     <p>The dimensionality <em>d</em> determines the size of the embedding, and hence the possibility to compute more fine-grained representations. The performance grows linearly as the number of dimensions approaches 128, while with larger <em>d</em> we observe no further improvement. Sampled <SmallCap>VERSE</SmallCap> instead, performs comparably better than <SmallCap>fVERSE</SmallCap> in low dimensional spaces, but degrades as <em>d</em> becomes larger than 128; this behavior reflects a characteristic of node sampling that tends to preserve similarities of close neighborhoods in low-dimensional embeddings, while the <SmallCap>VERSE</SmallCap> leverages the entire graph structure for larger dimensionality</p>     <p>The last parameter we study is the damping factor <em>&#x03B1;</em> which amounts to the inverse of the probability of restarting random walks from the initial node. As shown in Figure&#x00A0;, the quality of classification accurary is quite robust with respect to <em>&#x03B1;</em> for both <SmallCap>VERSE</SmallCap> and <SmallCap>fVERSE</SmallCap>, only compromised by extreme values. An <em>&#x03B1;</em> value close to 0 reduces PPR to an exploration of the immediate neighborhood of the node. On the other hand, a value close to 1 amounts to regular PageRank, deeming all nodes as equally important. This result vindicates our work and distinguishes it from previous methods based on local neighborhood expansion.</p>    </section>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.6</span> Scalability</h3>     </div>     </header>     <p>We now present runtime results on synthetic graphs of growing size, generated by the Watts Strogatz model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0050">50</a>], setting <SmallCap>VERSE</SmallCap> against scalable methods with C<tt>++</tt> implementations, namely <SmallCap>DeepWalk</SmallCap> , <SmallCap>LINE</SmallCap>, and <SmallCap>Node2vec</SmallCap> . For each method, we report the total wall-clock time, with graph loading and necessary preprocessing steps included. We used <SmallCap>LINE</SmallCap> -2 time for fair comparison. As Figure&#x00A0;<a class="fig" href="#fig5">4</a> shows, <SmallCap>VERSE</SmallCap> is comfortably the most efficient and scalable method, processing 10<sup>6</sup> nodes in about 3 hours, while <SmallCap>DeepWalk</SmallCap> and <SmallCap>LINE</SmallCap> take from 6 to 15 hours. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-fig5.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Classification performance of various parameters in Fig.&#x00A0;,&#x00A0; and scalability of different methods in Fig.&#x00A0;.</span>      </div>     </figure>     </p>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.7</span> Visualization</h3>     </div>     </header>     <p>Last, we show how different embeddings are visualized on a plane. We apply t-SNE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>] with default 		 parameters to each embedding for a subset of 1500 		 nodes from the CoCit dataset, equally distributed 		 in 5 classes (i.e., conferences); we set the 		 density areas for each class by Kernel Density 		 Estimation. Figure&#x00A0;<a class="fig" 		 href="#fig6">5</a> depicts the result. VERSE 		 produces well separated clusters with low noise, 		 even finding distinctions among papers of the same 		 community, namely ICDE 		 (<img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic19.jpg" 		 class="img-responsive" style="display: inline" alt=""       longdesc=""/>) and VLDB 		 (<img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic20.jpg" style="display: inline" class="img-responsive" alt=""       longdesc=""/>). <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-fig6.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Visualizations of a subset of nodes from CoCit graph with selected conferences: <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic20.jpg" style="display: inline" class="img-responsive" alt=""       longdesc=""/> VLDB, <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic19.jpg" 		 class="img-responsive" style="display: inline" alt=""       longdesc=""/> ICDE, 			 <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic24.jpg" 		 class="img-responsive" style="display: inline" alt=""       longdesc=""/> KDD, 			 <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic25.jpg" 		 class="img-responsive" style="display: inline" alt=""       longdesc=""/> WWW, and 			 <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186120/images/www2018-129-graphic26.jpg" 		 class="img-responsive" style="display: inline" alt=""       longdesc=""/> NIPS. Note that the number of nodes per class is the same for all conferences.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-22">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusions</h2>     </div>    </header>    <p>We introduced a new perspective on graph embeddings: to be expressive, a graph embedding should capture <em>some</em> similarity measure among nodes. Armed with this perspective, we developed a scalable embedding algorithm, <SmallCap>VERSE</SmallCap> . In a departure from previous works in the area, <SmallCap>VERSE</SmallCap> aims to reconstruct the distribution of any chosen similarity measure for each graph node. Thereby, <SmallCap>VERSE</SmallCap> brings in its scope a global view of the graph, while substantially reducing the number of parameters required for training. <SmallCap>VERSE</SmallCap> attains linear time complexity, hence it scales to large real graphs, while it only requires space to store the graph. Besides, we have shed light on some previous works on graph embeddings, looking at them and interpreting them through the prism of vertex similarity.</p>    <p>Our thorough experimental study shows that, even instantiated with PPR as a <em>default</em> similarity notion, <SmallCap>VERSE</SmallCap> consistently outperforms stateoftheart approaches for graph embeddings in a plethora of graph tasks, while a hyperparametersupervised variant does even better. Thus, we have provided strong evidence that embeddings genuinely based on vertex similarity address graph mining challenges better than others.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">2016. Microsoft 		 Academic Graph - KDD cup 2016. 		 <a class="link-inline force-break" 		  href="https://kddcup2016.azurewebsites.net/Data">https://kddcup2016.azurewebsites.net/Data</a>. (2016). Accessed: 2016-04-30.</li>     <li id="BibPLXBIB0002" label="[2]">Sami Abu-El-Haija, Bryan Perozzi, and Rami Al-Rfou. 2017. Learning Edge Representations via Low-Rank Asymmetric Projections. <em>      <em>CIKM</em></em> (2017).</li>     <li id="BibPLXBIB0003" label="[3]">Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander&#x00A0;J Smola. 2013. Distributed large-scale natural graph factorization. In <em>      <em>WWW</em></em>. ACM, 37&#x2013;48.</li>     <li id="BibPLXBIB0004" label="[4]">Leman Akoglu, Mary McGlohon, and Christos Faloutsos. 2010. Oddball: Spotting anomalies in weighted graphs. In <em>      <em>PAKDD</em></em>. 410&#x2013;421.</li>     <li id="BibPLXBIB0005" label="[5]">David Arthur and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In <em>      <em>SIAM</em></em>. 1027&#x2013;1035.</li>     <li id="BibPLXBIB0006" label="[6]">Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In <em>      <em>NIPS</em></em>. 585&#x2013;591.</li>     <li id="BibPLXBIB0007" label="[7]">Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. <em>      <em>TPAMI</em></em> (2013), 1798&#x2013;1828.</li>     <li id="BibPLXBIB0008" label="[8]">Yoshua Bengio, R&#x00E9;jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. <em>      <em>JMLR</em></em> (2003), 1137&#x2013;1155.</li>     <li id="BibPLXBIB0009" label="[9]">Vincent&#x00A0;D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. <em>      <em>Journal of statistical mechanics: theory and experiment</em>     </em>10 (2008).</li>     <li id="BibPLXBIB0010" label="[10]">Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. <em>      <em>Computer Networks and ISDN Systems</em>     </em>(1998), 107 &#x2013; 117.</li>     <li id="BibPLXBIB0011" label="[11]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. In <em>      <em>CIKM</em></em>. 891&#x2013;900.</li>     <li id="BibPLXBIB0012" label="[12]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep Neural Networks for Learning Graph Representations. In <em>      <em>AAAI</em></em>. 1145&#x2013;1152.</li>     <li id="BibPLXBIB0013" label="[13]">Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. <em>      <em>JMLR</em>     </em>9, Aug (2008), 1871&#x2013;1874.</li>     <li id="BibPLXBIB0014" label="[14]">Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. <em>      <em>Deep learning</em></em>. MIT Press.</li>     <li id="BibPLXBIB0015" label="[15]">Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In <em>      <em>KDD</em></em>. 855&#x2013;864.</li>     <li id="BibPLXBIB0016" label="[16]">Michael Gutmann and Aapo Hyv&#x00E4;rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.. In <em>      <em>AISTATS</em></em>. 297&#x2013;304.</li>     <li id="BibPLXBIB0017" label="[17]">Michael&#x00A0;U Gutmann and Aapo Hyv&#x00E4;rinen. 2012. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. <em>      <em>JMLR</em>     </em>13(2012), 307&#x2013;361.</li>     <li id="BibPLXBIB0018" label="[18]">Keith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi-Rad, Hanghang Tong, and Christos Faloutsos. 2011. It&#x0027;s who you know: graph mining using recursive structural features. In <em>      <em>KDD</em></em>. 663&#x2013;671.</li>     <li id="BibPLXBIB0019" label="[19]">Jiafeng Hu, CK Cheng, Zhipeng Huang, Yixiang Fang, and Siqiang Luo. 2017. On Embedding Uncertain Graphs. In <em>      <em>CIKM</em></em>. ACM.</li>     <li id="BibPLXBIB0020" label="[20]">Xiao Huang, Jundong Li, and Xia Hu. 2017. Label informed attributed network embedding. In <em>      <em>WSDM</em></em>. ACM, 731&#x2013;739.</li>     <li id="BibPLXBIB0021" label="[21]">Glen Jeh and Jennifer Widom. 2002. SimRank: a measure of structural-context similarity. In <em>      <em>KDD</em></em>. 538&#x2013;543.</li>     <li id="BibPLXBIB0022" label="[22]">Minhao Jiang, Ada Wai-Chee Fu, and Raymond Chi-Wing Wong. 2017. READS: a random walk approach for efficient and accurate dynamic SimRank. <em>      <em>VLDB</em>     </em>10, 9 (2017), 937&#x2013;948.</li>     <li id="BibPLXBIB0023" label="[23]">Leo Katz. 1953. A new status index derived from sociometric analysis. <em>      <em>Psychometrika</em>     </em>18, 1 (1953), 39&#x2013;43.</li>     <li id="BibPLXBIB0024" label="[24]">Alex Krizhevsky, Ilya Sutskever, and Geoffrey&#x00A0;E Hinton. 2012. Imagenet classification with deep convolutional neural networks. 1097&#x2013;1105.</li>     <li id="BibPLXBIB0025" label="[25]">Matthieu Labeau and Alexandre Allauzen. 2017. An experimental analysis of Noise-Contrastive Estimation: the noise distribution matters. <em>      <em>EACL</em>     </em> (2017).</li>     <li id="BibPLXBIB0026" label="[26]">John&#x00A0;A. Lee and Michel Verleysen. 2007. <em>      <em>Nonlinear Dimensionality Reduction</em>(1st ed.)</em>. Springer Publishing Company, Incorporated.</li>     <li id="BibPLXBIB0027" label="[27]">Ryan&#x00A0;N Lichtenwalter, Jake&#x00A0;T Lussier, and Nitesh&#x00A0;V Chawla. 2010. New perspectives and methods in link prediction. In <em>      <em>KDD</em></em>. 243&#x2013;252.</li>     <li id="BibPLXBIB0028" label="[28]">Linyuan L&#x00FC; and Tao Zhou. 2011. Link prediction in complex networks: A survey. <em>      <em>Physica A: Statistical Mechanics and its Applications</em>     </em>390, 6(2011), 1150&#x2013;1170.</li>     <li id="BibPLXBIB0029" label="[29]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory&#x00A0;S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In <em>      <em>NIPS</em></em>. 3111&#x2013;3119.</li>     <li id="BibPLXBIB0030" label="[30]">Andriy Mnih and Yee&#x00A0;Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In <em>      <em>ICML</em></em>. 1751&#x2013;1758.</li>     <li id="BibPLXBIB0031" label="[31]">Annamalai Narayanan, Mahinthan Chandramohan, Lihui Chen, Yang Liu, and Santhoshkumar Saminathan. 2016. subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs. <em>      <em>arXiv preprint arXiv:1606.08928</em>     </em>(2016).</li>     <li id="BibPLXBIB0032" label="[32]">Mark&#x00A0;EJ Newman. 2006. Modularity and community structure in networks. <em>      <em>PNAS</em>     </em> (2006), 8577&#x2013;8582.</li>     <li id="BibPLXBIB0033" label="[33]">Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric transitivity preserving graph embedding. In <em>      <em>KDD</em></em>. 1105&#x2013;1114.</li>     <li id="BibPLXBIB0034" label="[34]">Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: bringing order to the web.(1999).</li>     <li id="BibPLXBIB0035" label="[35]">Jeffrey Pennington, Richard Socher, and Christopher&#x00A0;D Manning. 2014. Glove: Global Vectors for Word Representation.. In <em>      <em>EMNLP</em></em>. 1532&#x2013;1543.</li>     <li id="BibPLXBIB0036" label="[36]">Bryan Perozzi, Leman Akoglu, Patricia Iglesias&#x00A0;S&#x00E1;nchez, and Emmanuel M&#x00FC;ller. 2014. Focused clustering and outlier detection in large attributed graphs. In <em>      <em>KDD</em></em>. 1346&#x2013;1355.</li>     <li id="BibPLXBIB0037" label="[37]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: online learning of social representations. In <em>      <em>KDD</em></em>. 701&#x2013;710.</li>     <li id="BibPLXBIB0038" label="[38]">Leonardo&#x00A0;FR Ribeiro, Pedro&#x00A0;HP Saverese, and Daniel&#x00A0;R Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In <em>      <em>KDD</em></em>. ACM, 385&#x2013;394.</li>     <li id="BibPLXBIB0039" label="[39]">Sam&#x00A0;T Roweis and Lawrence&#x00A0;K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. <em>      <em>science</em>     </em>290, 5500 (2000), 2323&#x2013;2326.</li>     <li id="BibPLXBIB0040" label="[40]">Jiliang Tang and Huan Liu. 2012. Unsupervised Feature Selection for Linked Social Media Data. In <em>      <em>KDD</em></em>. 904&#x2013;912.</li>     <li id="BibPLXBIB0041" label="[41]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. In <em>      <em>WWW</em></em>. 1067&#x2013;1077.</li>     <li id="BibPLXBIB0042" label="[42]">Lei Tang and Huan Liu. 2009. Relational learning via latent social dimensions. In <em>      <em>KDD</em></em>. 817&#x2013;826.</li>     <li id="BibPLXBIB0043" label="[43]">Lei Tang and Huan Liu. 2009. Scalable learning of collective behavior based on sparse social dimensions. In <em>      <em>CIKM</em></em>. 1107&#x2013;1116.</li>     <li id="BibPLXBIB0044" label="[44]">Joshua&#x00A0;B Tenenbaum, Vin De&#x00A0;Silva, and John&#x00A0;C Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. <em>      <em>science</em>     </em>290, 5500 (2000), 2319&#x2013;2323.</li>     <li id="BibPLXBIB0045" label="[45]">Grigorios Tsoumakas and Ioannis Katakis. 2006. Multi-label classification: An overview. <em>      <em>IJDWM</em>     </em>3, 3 (2006).</li>     <li id="BibPLXBIB0046" label="[46]">Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. 2016. Max-Margin DeepWalk: Discriminative Learning of Network Representation. In <em>      <em>IJCAI</em></em>. 3889&#x2013;3895.</li>     <li id="BibPLXBIB0047" label="[47]">L.J.P. van der Maaten and G.E. Hinton. 2008. Visualizing High-Dimensional Data Using t-SNE. <em>      <em>Journal of Machine Learning Research</em>     </em>9 (2008), 2579&#x2013;2605.</li>     <li id="BibPLXBIB0048" label="[48]">Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural Deep Network Embedding. In <em>      <em>KDD</em></em>. 1225&#x2013;1234.</li>     <li id="BibPLXBIB0049" label="[49]">Suhang Wang, Charu Aggarwal, Jiliang Tang, and Huan Liu. 2017. Attributed Signed Network Embedding. <em>      <em>CIKM</em>     </em> (2017).</li>     <li id="BibPLXBIB0050" label="[50]">Duncan&#x00A0;J Watts and Steven&#x00A0;H Strogatz. 1998. Collective dynamics of &#x2018;small-world&#x2019; networks. <em>      <em>Nature</em>     </em>393(1998), 440&#x2013;442.</li>     <li id="BibPLXBIB0051" label="[51]">Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. 2007. Graph embedding and extensions: A general framework for dimensionality reduction. <em>      <em>TPAMI</em>     </em>29, 1 (2007).</li>     <li id="BibPLXBIB0052" label="[52]">Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network communities based on ground-truth. <em>      <em>Knowledge and Information Systems</em>     </em>42 (2015), 181&#x2013;213.</li>     <li id="BibPLXBIB0053" label="[53]">Wayne&#x00A0;W Zachary. 1977. An information flow model for conflict and fission in small groups. <em>      <em>Journal of anthropological research</em>     </em>33, 4 (1977), 452&#x2013;473.</li>     <li id="BibPLXBIB0054" label="[54]">R. Zafarani and H. Liu. 2009. Social Computing Data Repository at ASU. (2009). <a class="link-inline force-break" href="http://socialcomputing.asu.edu"      target="_blank">http://socialcomputing.asu.edu</a></li>     <li id="BibPLXBIB0055" label="[55]">Daokun Zhanga, Jie Yinb, Xingquan Zhuc, and Chengqi Zhanga. 2017. User profile preserving social network embedding. In <em>      <em>IJCAI</em></em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://github.com/xgfs/verse">https://github.com/xgfs/verse</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://github.com/xgfs/deepwalk-c">https://github.com/xgfs/deepwalk-c</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://github.com/xgfs/node2vec-c">https://github.com/xgfs/node2vec-c</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break" href="https://github.com/tangjianpku/LINE">https://github.com/tangjianpku/LINE</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186120">https://doi.org/10.1145/3178876.3186120</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
