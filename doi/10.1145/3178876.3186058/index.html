<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>DeepMove: Predicting Human Mobility with Attentional
  Recurrent Networks</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3186058'>https://doi.org/10.1145/3178876.3186058</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186058'>https://w3id.org/oa/10.1145/3178876.3186058</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">DeepMove: Predicting Human
          Mobility with Attentional Recurrent Networks</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Jie</span> <span class=
          "surName">Feng</span>, Department of Electronic
          Engineering, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Yong</span> <span class=
          "surName">Li</span>, Department of Electronic
          Engineering, Tsinghua University, Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Chao</span> <span class=
          "surName">Zhang</span>, Dept. of Computer Science,
          University of Illinois at Urbana-Champaign, Urbana, IL,
          USA
        </div>
        <div class="author">
          <span class="givenName">Funing</span> <span class=
          "surName">Sun</span>, Tencent Inc., Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Fanchao</span> <span class=
          "surName">Meng</span>, Tencent Inc., Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Ang</span> <span class=
          "surName">Guo</span>, Tencent Inc., Beijing, China
        </div>
        <div class="author">
          <span class="givenName">Depeng</span> <span class=
          "surName">Jin</span>, Department of Electronic
          Engineering, Tsinghua University, Beijing, China,
          <a href="mailto:liyong07@tsinghua.edu.cn">liyong07@tsinghua.edu.cn</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186058"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186058</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Human mobility prediction is of great importance
        for a wide spectrum of location-based applications.
        However, predicting mobility is not trivial because of
        three challenges: 1) the complex sequential transition
        regularities exhibited with time-dependent and high-order
        nature; 2) the multi-level periodicity of human mobility;
        and 3) the heterogeneity and sparsity of the collected
        trajectory data. In this paper, we propose DeepMove, an
        attentional recurrent network for mobility prediction from
        lengthy and sparse trajectories. In DeepMove, we first
        design a multi-modal embedding recurrent neural network to
        capture the complicated sequential transitions by jointly
        embedding the multiple factors that govern the human
        mobility. Then, we propose a historical attention model
        with two mechanisms to capture the multi-level periodicity
        in a principle way, which effectively utilizes the
        periodicity nature to augment the recurrent neural network
        for mobility prediction. We perform experiments on three
        representative real-life mobility datasets, and extensive
        evaluation results demonstrate that our model outperforms
        the state-of-the-art models by more than 10%. Moreover,
        compared with the state-of-the-art neural network models,
        DeepMove provides intuitive explanations into the
        prediction and sheds light on interpretable mobility
        prediction.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Location based services;</strong> <strong>Data
        mining;</strong> • <strong>Human-centered
        computing</strong> → <strong>Ubiquitous and mobile
        computing design and evaluation
        methods;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>recurrent neural network;
          attention; human mobility</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Jie Feng<sup>1</sup>, Yong Li<sup>1</sup>, Chao
          Zhang<sup>2</sup>, Funing Sun<sup>3</sup>, Fanchao
          Meng<sup>3</sup>, Ang Guo<sup>3</sup>, and Depeng
          Jin<sup>1</sup>. 2018. DeepMove: Predicting Human
          Mobility with Attentional Recurrent Networks. In <em>WWW
          2018: The 2018 Web Conference,</em> <em>April 23–27,
          2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em>
          10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186058" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186058</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Human mobility prediction is of great importance in a wide
      spectrum of applications, ranging from smart transportation
      and urban planning, to resource management in mobile
      communications, personalized recommender systems, and mobile
      healthcare services. By predicting the future locations
      people tend to visit, governments can design better
      transportation planning and scheduling strategies to
      alleviate traffic jams and handle crowd aggregations.
      Ride-sharing platforms like Uber and Didi also heavily rely
      on accurate mobility prediction techniques, for better
      estimating the travel demands of their customers and
      scheduling resources to meet such demands accordingly. With
      the proliferation of such mobile applications, it has become
      a pressing need to understand the mobility patterns of people
      from their historical traces and foresee their future
      whereabouts.</p>
      <p>By measuring the entropy of individual's trajectory, Song
      et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0028">28</a>]
      find remarkable stability in the predictability of human
      mobility — 93% human movements are predictable according to
      their study on a million-scale user base. So far, lots of
      research efforts&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0043">43</a>] have been taken to turn this
      identified predictability into actual mobility prediction
      models. Early methods for mobility prediction are mostly
      pattern-based&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0042">42</a>]. They first discover pre-defined
      mobility patterns (e.g., sequential patterns, periodic
      patterns) from the trajectory traces, and then predict future
      locations based on these extracted patterns. Such methods,
      however, not only suffer from the one-sided nature of the
      pre-defined patterns, but also ignore personal preferences
      that are critical for mobility prediction. More recent
      developments turn to model-based methods&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0016">16</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0022">22</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0043">43</a>] for mobility prediction.
      They leverage sequential statistical models (e.g., Markov
      chain or recurrent neural network) to capture the transition
      regularities of human movements and learn the model
      parameters from the given training corpus.</p>
      <p>Despite the inspiring results of model-based mobility
      prediction, there are several key challenges that remain to
      be solved to realize the high potential predictability of
      human movements: (1) First, human mobility exhibits
      <em>complex sequential transition regularities</em>. In
      practice, the transitions between two arbitrary locations can
      be time-dependent and high-order. For instance, the
      probability of moving from home to office for a commuter is
      higher in workday mornings but often low in weekend mornings.
      Meanwhile, the transition may not follow the simple and exact
      Markov chain assumption, as people can go to different places
      (e.g., breakfast places) in their commute routes, which lead
      to high-order and irregular transition patterns. (2) Second,
      there is often <em>multi-level periodicity</em> that governs
      human mobility. Periodicity has been demonstrated as an
      important factor that governs human movements&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0041">41</a>]. However,
      existing mobility prediction models are mostly sequential
      models that only capture the transitional regularities.
      Further, the mobility periodicity is often complex and
      multi-level, involving daily routines, weekend leisure,
      yearly festivals, and even other personal periodic
      activities. All these periodic activities interweave with
      each other in complex ways and are difficult to be captured.
      (3) The third challenge is <em>heterogeneity and
      sparsity</em> in the data recording human mobility. Unlike
      intentionally collected tracking data like taxi trajectories,
      most data recording human mobility is low-sampling in nature,
      and the location information is recorded only when the user
      accesses the location service. Such sparsity makes it
      difficult for training a mobility model for each individual.
      Aggregating the data of all users, on the other hand, may
      face the problem of mixing personalized mobility patterns
      together and suffer from low prediction accuracy.</p>
      <p>In this paper, we propose DeepMove, an attentional
      recurrent neural network model for predicting human mobility
      from lengthy and sparse trajectories. In DeepMove, we utilize
      a multi-modal recurrent neural network to capture the
      multiple factors that govern the transition regularities of
      human movements. Specifically, we design a multi-modal
      embedding module that converts sparse features (e.g., time of
      day, region, user) into dense representations, which are then
      fed into a recurrent neural network to model long-range and
      complex dependencies in a trajectory sequence. DeepMove is
      capable of discovering the transitional regularities that are
      shared by all the users, while flexibly leveraging user
      embeddings to capture personalized movement preferences.
      Another key component in DeepMove is a historical attention
      module, which captures the multi-level periodicity of human
      mobility in a principled way. The attention component is
      jointly trained to select historical records that are highly
      correlated with the current prediction timestamp. It thus
      flexibly utilizes periodic movement regularities to augment
      the recurrent neural network and improve prediction accuracy.
      Better still, the learned attention weights offer an
      easy-to-interpret way to understand which historical
      activities are emphasized in the prediction process.</p>
      <p>Our contributions can be summarized as follows:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We propose an attentional
        recurrent model, DeepMove, to predict human mobility from
        long-range and sparse trajectories. Our model combines two
        regularities in a principled way: heterogeneous transition
        regularity and multi-level periodicity. To the best of our
        knowledge, DeepMove is the first model that simultaneously
        combines these two important regularities for accurate
        mobility prediction.<br /></li>
        <li id="list2" label="•">We design two attention mechanisms
        that are tailored to cooperate with the recurrent module.
        The first is to directly embed historical record into
        independent latent vectors and use the current status to
        selectively focus on relevant historical steps; while the
        second preserves the sequential information among
        historical records. Both unveil the periodicity of human
        mobility by matching historical records with the current
        status, and rationalize the prediction making
        process.<br /></li>
        <li id="list3" label="•">We perform extensive experiments
        on three representative real-life mobility datasets. Our
        results demonstrate that DeepMove outperforms
        state-of-the-art mobility prediction models by more than
        10%. DeepMove shows outstanding generalization ability and
        is robust across trajectory datasets that have different
        natures. Furthermore, compared with existing RNN models,
        DeepMove provides intuitive explanations into the
        prediction and sheds light on interpretable mobility
        prediction.<br /></li>
      </ul>
      <p>The rest of this paper is organized as follows. We first
      formulate the problem and discuss the motivation of our work
      in Section 2. Following the motivation, we introduce details
      of the architecture of DeepMove in Section 3. After that, we
      apply our model on three real-world mobility datasets and
      conduct extensive analysis on the performance in Section 4.
      After systematically reviewing the related works in Section
      5, we finally conclude our paper in Section 6.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span>
          PRELIMINARIES</h2>
        </div>
      </header>
      <p>In this section, we first formally formulate the mobility
      prediction problem, and then briefly introduce the recurrent
      neural network. Finally, we discuss the motivation and
      overview our solution.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Problem
            Formulation</h3>
          </div>
        </header>
        <div class="definition" id="enc1">
          <label>Definition 1 (Trajectory Sequence).</label>
          <p>Spatiotemporal point <em>q</em> is a tuple of time
          stamp <em>t</em> and location identification <em>l</em>,
          i.e., <em>q</em> = (<em>t</em>, <em>l</em>). Given a user
          identification <em>u</em>, trajectory sequence <em>S</em>
          is a spatiotemporal point sequence, i.e.,
          <em>S<sup>u</sup></em> = <em>q</em> <sub>1</sub>
          <em>q</em> <sub>2</sub>...<em>q<sub>n</sub></em> .</p>
        </div>
        <div class="definition" id="enc2">
          <label>Definition 2 (Trajectory).</label>
          <p>Given a trajectory sequence <em>S<sup>u</sup></em> and
          a time window <em>t<sub>w</sub></em> , trajectory is a
          subsequence <span class="inline-equation"><span class=
          "tex">$S^{u}_{t_w}=q_iq_{i+1}..q_{i+k}$</span></span> of
          <em>S<sup>u</sup></em> in the time window
          <em>t<sub>w</sub></em> , if ∀ 1 &lt; j ≤ k, <span class=
          "inline-equation"><span class=
          "tex">$t_{q_j}$</span></span> belongs to
          <em>t<sub>w</sub></em> .</p>
        </div>
        <p>At the <em>m</em>-<em>th</em> time window <span class=
        "inline-equation"><span class="tex">$t_{w_m}$</span></span>
        , the current trajectory of user <em>u</em> can be defined
        as <span class="inline-equation"><span class=
        "tex">$S^{u}_{t_{w_m}}$</span></span> and the trajectory
        history can be denoted as <span class=
        "inline-equation"><span class=
        "tex">$S^{u}_{t_{w_1}}S^{u}_{t_{w_2}}...S^{u}_{t_{w_{m-1}}}$</span></span>
        , where <em>t<sub>w</sub></em> can be one specific day, one
        week or one month in the year.</p>
        <div class="problem" id="enc3">
          <label>Problem 1 (Mobility prediction).</label>
          <p>Given the current trajectory <span class=
          "inline-equation"><span class=
          "tex">$S^{u}_{t_{w_m}}=q_1q_2..q_n$</span></span> and the
          corresponding trajectory history <span class=
          "inline-equation"><span class=
          "tex">$S^{u}_{t_{w_1}}S^{u}_{t_{w_2}}...S^{u}_{t_{w_{m-1}}}$</span></span>
          , predict the next spatiotemporal point <em>q</em>
          <sub><em>n</em> + 1</sub> in the trajectory.</p>
        </div>
        <p>In this paper, we quantify the time interval of the
        spatiotemporal point into a fixed value, i.e., 30 minutes.
        Thus, the mobility prediction is simplified to predict the
        next location identification <em>l</em> in the next time
        interval.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Recurrent
            Neural Network</h3>
          </div>
        </header>
        <p>Recurrent Neural Network [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>] is a class of neural networks with
        cycle and internal memory units to capture sequential
        information. Long short-term memory (LSTM) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0014">14</a>] and gated recurrent
        unit (GRU) [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>] are widely used recurrent units.
        LSTM consists of one cell state and three controlled
        <em>gates</em> to keep and update the cell state. Based on
        the input and last cell state, LSTM first updates the cell
        state with parts to keep and parts to drop. Then, LSTM
        generates the output from the cell state with learnable
        weight. GRU is a popular variant of LSTM which replaces the
        forget gate and the input gate with only one update gate.
        The updating formulations of GRU are as follows,</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} &amp;f_t = \sigma
            (W_{fx}x_t + W_{fh}h_{t-1} +
            b_f),\end{align}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} &amp;r_t = \sigma
            (W_{rx}x_t + W_{rh}h_{t-1} +
            b_r),\end{align}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} &amp;c_t =
            tanh(W_{cx}x_t + r_t * (W_{ch}h_{t-1}) +
            b_c),\end{align}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} &amp;h_t =
            (1-f_t)*c_t + f_t*h_{t-1}, \end{align}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where <em>x<sub>t</sub></em> is the input in time
        <em>t</em>, <em>h</em> <sub><em>t</em> − 1</sub> is the
        last output of GRU unit, multiple matrix <em>W</em> are
        different gate parameters, multiple vectors <em>b</em> are
        the bias vectors for different part, * means element-wise
        multiplication, <em>f<sub>t</sub></em> is the update
        weight, <em>r<sub>t</sub></em> is the reset gates,
        <em>c<sub>t</sub></em> is the candidate and
        <em>h<sub>t</sub></em> is the output result. According to
        Chung et al. [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>], GRU achieves the similar
        performance in multiple tasks with less computation, which
        is used as the basic recurrent unit in our proposed model.
        <p></p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span>
            Overview</h3>
          </div>
        </header>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">Performance varies with
            trajectory length.</span>
          </div>
        </figure>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Motivation and Intuition of
            our solution.</span>
          </div>
        </figure>
        <p>As a powerful sequence modeling tool, the recurrent
        neural network can capture long-range dependencies of
        sequential information. However, when the sequence is too
        long, i.e., a long sentence with more than 20 words, its
        performance will degrade rapidly [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0001">1</a>]. According to the
        typical mobility datasets, the average length of one day's
        trajectory for mobile application data varies from 20 to
        100, which obviously exceeds the processing ability of
        recurrent neural network. Figure <a class="fig" href=
        "#fig1">1</a> plots the prediction accuracy obtained by a
        simple recurrent neural network. It shows that the
        prediction accuracy varies significantly with the testing
        trajectory. The longer the time extends, the worse
        performance the prediction achieves. Thus, with the
        recurrent neural network, we can only process limited
        length trajectory with a short duration of one day or even
        shorter.</p>
        <p>Directly applying the recurrent neural network to solve
        the mobility prediction problem is intuitive but
        inefficient. Except the long-term nature mentioned above,
        some other challenges also make it failed. The first is the
        multi-level periodicity of human mobility. Generally, there
        exists several periodicities in human activities: day,
        week, month and even other personal periodicities, and thus
        the multi-level periodicity becomes a universe pattern that
        governs the human mobility. However, the general recurrent
        neural network can do little to handle this because of the
        long-term effect and the complicated influencing factors
        from the trajectory. Besides, because users will not report
        their activities in every location (unlike uniform sampling
        traces of taxi&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>]), most collected mobility data is
        sparse and incomplete, which cannot record the periodical
        nature of human mobility. In general, the data quality
        problem degrades the performance in two ways. The first one
        is that the missing data will puzzle the recurrent neural
        network, and induce it to learn the wrong transition
        information. The other one is that the sparse data makes it
        difficult to train model for every individual. Even for
        capturing the transition relations, the recurrent neural
        network faces the problem of the time-dependent and
        high-order nature of human mobility. In conclusion, the
        recurrent neural network faces the problem of periodicity,
        data sparsity and complicated transitions, which prevent it
        to achieve high prediction accuracy for human mobility.</p>
        <p>Based on the above observations, we propose DeepMove, an
        attentional recurrent neural network for predicting human
        mobility from the lengthy, periodical and incomplete
        trajectories. Figure <a class="fig" href="#fig2">2</a>
        presents the intuition behind our solution: not only the
        sequential information from the current activities decides
        the next mobility status but also the periodical
        information from the trajectory history takes effects. In
        DeepMove, we first involve multi-modal recurrent neural
        network to capture the complicated transition relationship.
        In the multi-modal recurrent neural network, we design a
        multi-modal embedding module to convert the sparse features
        (e.g., user, location, time of day) into dense
        representations, which are more expressing and computable.
        Then, they are jointly fed into a recurrent neural network
        to capture the complicated transition relationship. With
        the help of user embedding, DeepMove can distinguish every
        user and learn the personal preference while training a
        single model for all users to learn and share similar
        mobility regularities. Besides, the time representation
        involved in the embedding gives recurrent neural network
        the ability to model the time-dependent nature.</p>
        <p>Another key component of DeepMove is the historical
        attention module, which is designed to capture the
        multi-level periodical nature of human mobility by jointly
        selecting the most related historical trajectories under
        the current mobility status. The historical attention
        module first extracts spatiotemporal features from the
        historical trajectories by an extractor. Then, these
        features are selected by the current mobility status based
        on the spatiotemporal relations to generate the most
        related context. By combining this context with the current
        mobility status, we could predict the mobility based on not
        only the sequential relation but also the multi-level
        periodical regularity.</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> The DeepMove
          Model</h2>
        </div>
      </header>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Multi-modal
            Recurrent Prediction Framework</h3>
          </div>
        </header>
        <p>Figure <a class="fig" href="#fig3">3</a> presents the
        architecture of Deepmove. It consists of three major
        components: 1) feature extracting and embedding; 2)
        recurrent module and historical attention; and 3)
        prediction.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Main architecture of
            DeepMove.</span>
          </div>
        </figure>
        <section id="sec-11">
          <p><em>3.1.1 Feature Extracting and Embedding.</em> The
          trajectories are partitioned into two parts: current
          trajectory and historical trajectory. The current
          trajectory is processed by the recurrent layer to model
          the complicated sequential information. The trajectory
          history is handled by the historical attention module to
          extract the regularity of mobility. Before that, all the
          trajectories are first embedded by the multi-modal
          embedding module. Simple model like Markov chains can
          only describe the transitions between independent states
          like locations. However, mobility transitions are
          governed by multiple factors like time of day and user
          preference. Thus, we design a multi-modal embedding
          module to jointly embed the spatiotemporal features and
          the personal features into dense representations to help
          model the complicated transitions. In practice, all the
          available features of one trajectory point including
          time, location, user ID can be numbered. Then, the
          numbered features are translated into one-hot vectors and
          inputted to the multi-modal embedding module. The ID
          number of any new user, which is not appeared in the
          training set, is fixed as 0. According to the word2vec
          project [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0024">24</a>], compared with the limited
          one-hot representation, the dense representation can
          better capture the precise semantic spatiotemporal
          relationship. Another advantage is that this dense
          representation is always lower dimension, which benefits
          the follow-up computation.</p>
        </section>
        <section id="sec-12">
          <p><em>3.1.2 Recurrent Module and Historical
          Attention.</em> The recurrent module aims to capture the
          complicated sequential information or long-range
          dependencies contained in the current trajectory. We
          select GRU as the basic recurrent unit because of its
          computation efficiency without performance decay. The
          recurrent lay takes the spatiotemporal vector sequence
          embedded by the multi-modal embedding layer as input and
          outputs the hidden state step by step. These hidden
          states are called as the current status of the mobility.
          In every step, the output hidden state flows to
          historical attention module and prediction module.
          Paralleled with the recurrent module is the historical
          attention module, which is designed to capture mobility
          regularity from the lengthy historical records. It takes
          the historical trajectory as the input and outputs the
          most related context vector when queried by a query
          vector from the recurrent module. More details about the
          historical attention module are discussed in the next
          section.</p>
        </section>
        <section id="sec-13">
          <p><em>3.1.3 Prediction.</em> The prediction module is
          the final component that combines the context from
          different modules to complete the prediction task. It
          consists of a concatenate layer, several fully connected
          layers and an output layer. The concatenate layer
          combines all the features from the historical attention
          module, recurrent module, and embedding module into a new
          vector. Following the concatenate layer, fully connected
          layers further process the feature vector into a smaller
          and more expressing vector. Finally, the output layer
          consists of a soft-max layer with negative sampling.
          Negative sampling [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0024">24</a>] can approximately maximize the
          log probability of the soft-max, which is widely used in
          natural language processing because of its large
          vocabulary. In our problem, the size of the location
          candidate set can also be up to ten thousand, which makes
          the location representation sparse and the system
          difficult to train. With the help of negative sampling,
          our model can converge rapidly. In the practice, the
          instances of negative sampling are generated by following
          the uniform distribution.</p>
        </section>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Historical
            Attention Module</h3>
          </div>
        </header>
        <p>To capture the multi-level periodicity of human
        mobility, we need an auto-selector to choose the most
        related historical records of current mobility status from
        the trajectory history as the periodicity representation.
        Inspired by the human visual attention nature and the
        attention mechanism widely used in natural language
        translation [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>], we design a historical attention
        module to implement the auto-selector. As Figure <a class=
        "fig" href="#fig3">3</a> presents, it is comprised of two
        components: 1) an attention candidate generator to generate
        the candidates, which are exactly the regularities of the
        mobility; 2) an attention selector to match the candidate
        vectors with the query vector, i.e., the current mobility
        status. We first introduce the basic formulation of
        attention module, and then discuss two specific candidate
        generation mechanisms.</p>
        <section id="sec-15">
          <p><em>3.2.1 Attention Selector.</em> The goal of
          attention module is to calculate the <em>similarity</em>
          between the query vector (i.e., the current mobility
          status) and the candidate vectors to generate the context
          vector. The attention module is parametrized as a
          feed-forward neural network that can be trained with the
          whole neural network. Figure <a class="fig" href=
          "#fig4">4</a>(a) presents the framework of this neural
          network. The attention computation formulations are as
          follows,</p>
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} &amp;c_t=\sum
              {{\alpha _i}s_i}, \end{align}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{align} &amp;{\alpha
              _i}=\sigma {(f(h_t,s_i))}, \end{align}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>
          <div class="table-responsive" id="eq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{align}
              &amp;f(h_t,s)=tanh(h_tWs), \end{align}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>where <em>s</em> represents the historical
          features, <em>W</em> is the learnable parameters,
          <em>h<sub>t</sub></em> is the query vector which denotes
          current mobility status from the recurrent layer,
          <em>f</em> represents the score function, <em>σ</em> is
          the soft-max function and <em>c<sub>t</sub></em> is the
          context output representing the periodicity related to
          the current mobility status. While there are many other
          variations of attention model [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0021">21</a>], we
          choose the original one for its simplicity and general
          expressions.
          <figure id="fig4">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig4.jpg"
            class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span>
              <span class="figure-title">Architecture of the
              historical attention module.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-16">
          <p><em>3.2.2 Attention Candidate Generator.</em> To
          provide the candidate vectors for the attention selector,
          we discuss two specific generation mechanisms.</p>
          <p><em>1) Embedding Encode Module.</em>. The first is the
          embedding encode mechanism, whose implementation
          structure is presented in Figure <a class="fig" href=
          "#fig4">4</a>(b). The embedding encode module directly
          embeds the historical records into independent latent
          vectors as the candidate vectors. It is composed of three
          components: 1) a shaping layer for disorganizing the
          ordered trajectory sequence into a history matrix with
          fixed-length temporal dimension and variable-length
          spatial dimension; 2) a sampling layer for the location
          sampling; 3) fully connected layers. The shaping layer is
          a fixed layer, whose structure and parameters are manual
          assigned. In this layer, we reorganize the trajectory
          vectors into a two dimension matrix (for the convenience
          of discussion, we omit the embedding dimension logically
          for the time being). In the temporal dimension, we align
          all the time of trajectory into one week or two days,
          which is designed to simulate the periodical nature of
          human mobility. In the spatial dimension, we collect all
          the locations appeared in the same time period to keep a
          visited location set for every time slot. Following the
          shaping layer is a sampling layer which is designed to
          sample location from the visited location set in every
          time slot. We design three kinds of sampling strategies:
          1) average sampling; 2) maximum sampling; 3) none
          sampling. Average sampling strategy adds up all the
          location embedding vectors in the set at every time slot
          and calculates the mean value as their representation. In
          this way, all the historical information can be reserved.
          The maximum sampling strategy is based on the periodical
          assumption of human mobility: the most frequently visited
          location means a lot to the user. It works by selecting
          the most frequent location embedding vector as the
          representation for every time slot. None sampling
          strategy is to reserve all the location and flatten them
          along the temporal dimension without any processing. In
          the last of the paper, the sampling layer with average
          sampling strategy is the default settings for the
          embedding encode mechanism. The final fully connected
          layers further process the historical spatiotemporal
          vectors into the appropriate shape.</p>
          <p><em>2) Sequential Encode Module.</em>. The second
          mechanism is the sequential encode mechanism, whose
          implementation structure is presented in Figure <a class=
          "fig" href="#fig4">4</a>(c). It consists of a recurrent
          neural network. The sequential encode module takes the
          historical records as input and keeps the intermediate
          outputs of every step as the candidate vectors. Different
          from the embedding encode module, it does not directly
          simulate the periodicity and reserves all the
          spatiotemporal information. Based on multi-modal
          embedding method mentioned above, the recurrent neural
          network can extract complex sequential information from
          the historical records. Compared with the embedding
          encode module, sequential encode module relies on the
          follow-up attention selector to capture the periodical
          information. Besides, the sequential encode module
          projects the historical records into a latent space which
          is similar to the current mobility status in. This
          similar projection result also benefits the follow-up
          attentional selection.</p>
        </section>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Training
            Algorithm</h3>
          </div>
        </header>
        <p>Algorithm 1 outlines the training process of DeepMove.
        DeepMove works in an end-to-end manner without requiring
        hand-crafting features. In general, next location
        prediction from the limited discrete location list can be
        regarded as a multi-classification problem, we choose the
        cross-entropy loss as our loss function. In practice, we
        use Backward Propagation Through Time (BPTT) and Adam
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0017">17</a>] to train
        it. The historical attention module is parameterized to a
        feed-forward neural network that can be jointly trained
        with the whole recurrent neural network.</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-img1.svg"
        class="img-responsive" alt="" longdesc="" /></p>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Performance
          Evaluation</h2>
        </div>
      </header>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Datasets</h3>
          </div>
        </header>
        <p>We collect three representative real-life mobility
        datasets to evaluate the performance of our proposed model.
        The first one is the public Foursquare check-in data, the
        second one is a mobile application location data from a
        popular social network vendor, and the last one is call
        detail records (CDR) data from a major cellular network
        operator. The generation mechanism of trajectory records of
        three data is totally different, which represent three main
        location generation mechanisms in the reality.</p>
        <ul class="list-no-style">
          <li id="list4" label="•">Call detail records data with
          location records generates in the base station of
          cellular network when users access it for communication
          and data accessing.<br /></li>
          <li id="list5" label="•">Mobile application data with
          location records generates in the application servers
          when users request location service in the application
          like search, check-in and so on.<br /></li>
          <li id="list6" label="•">In Foursquare, users always
          intentionally publish their location information to share
          with other friends and the public, which is the check-in
          location.<br /></li>
        </ul>
        <p>Besides, three datasets are collected among three
        different cities during different time period. All of these
        features ensure the representativeness of our data. The
        basic information of three mobility datasets is presented
        in Table <a class="tbl" href="#tab1">1</a>. Figure
        <a class="fig" href="#fig5">5</a> shows the spatiotemporal
        features of three mobility data. The details about the
        datasets and basic preprocessing procedure are discussed as
        follows.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Basic statistics of mobility
            datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">
                <strong>Dataset</strong></th>
                <th style="text-align:center;">
                <strong>Foursquare</strong></th>
                <th style="text-align:center;"><strong>Mobile
                Application</strong></th>
                <th><strong>Cellular Network</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">City</td>
                <td style="text-align:center;">New York</td>
                <td style="text-align:center;">Beijing</td>
                <td>Shanghai</td>
              </tr>
              <tr>
                <td style="text-align:center;">Duration</td>
                <td style="text-align:center;">1 year</td>
                <td style="text-align:center;">1 month</td>
                <td>1 month</td>
              </tr>
              <tr>
                <td style="text-align:center;">Users</td>
                <td style="text-align:center;">15639</td>
                <td style="text-align:center;">5000</td>
                <td>1075</td>
              </tr>
              <tr>
                <td style="text-align:center;">Records</td>
                <td style="text-align:center;">293559</td>
                <td style="text-align:center;">15007511</td>
                <td>491077</td>
              </tr>
              <tr>
                <td style="text-align:center;">Locaitons</td>
                <td style="text-align:center;">43379</td>
                <td style="text-align:center;">31522</td>
                <td>17785</td>
              </tr>
              <tr>
                <td style="text-align:center;">Loc./User</td>
                <td style="text-align:center;">40</td>
                <td style="text-align:center;">48</td>
                <td>40</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p><strong>Foursquare Check-in Data:</strong> This data is
        collected from Foursquare API from Feb. 2010 to Jan. 2011
        in New York. Every record in the data consists of user ID,
        timestamp, GPS location and POI ID. The data is sparse and
        the average length of records for one user is about 18
        during one year. Thus, we filter out the users with less
        than 10 records and then cut the left trajectories into
        several sessions based on the interval between two neighbor
        records. Further, we filter out the sessions with less than
        5 records and the users with less than 5 sessions. Here, we
        choose 72 hours as the default interval threshold based on
        the practice. Besides, we normalize the time stamp into one
        week with keeping the original order of the trajectory.</p>
        <p><strong>Mobile Application Data:</strong> This data is
        collected from the most popular social network vendor in
        China. It records the location of users whenever they
        request the localization service in the applications. The
        data is collected from 17 Nov. 2016 to 31 Oct. 2016. The
        localization of the records is mainly achieved by GPS
        modules on the mobile phone and enhanced by other possible
        sensors. For the convenience of representation and
        computation, the GPS location is projected into street
        block, which can be represented as a street block ID.</p>
        <p><strong>Call Detail Records Data:</strong> This data is
        collected from one major cellular network operator in
        China. It records the spatiotemporal information of users
        when they access the cellular networks (i.e., making phone
        calls, sending short messages, or consuming data plan). The
        data is collected from 1 Jan. 2016 to 31 Jan. 2016. The
        spatial granularity of it is the cellular base station,
        which is similar to street block.</p>
        <p>Different from the sparse Foursquare check-in data,
        mobile application data and call detail records data are
        both dense daily mobility data&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0034">34</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>]. In order to obtain
        meaningful trajectory from them, we first split the whole
        trajectory into different sessions by the date. Further, we
        split one day into 48 pieces and aggregate the records in
        the same time slot into one record. In practice, because of
        the duplication of the raw mobility data, we filter out
        these records during the same period of time.</p>
        <p>Other information about three mobility data can refer to
        Table <a class="tbl" href="#tab1">1</a> and Figure
        <a class="fig" href="#fig5">5</a>. To protect the privacy
        of the users, the base station ID, the street block ID and
        the user ID are all anonymous. Meanwhile, we want to point
        out that only the core researchers can access to the data
        with the strict non-disclosure agreements. Besides, all the
        data are stored in a secure local server. After processing
        data without leaking user privacy, we will open and publish
        these datasets and codes for the community.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Spatiotemporal features of
            three mobility data.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Experimental Setup</h3>
          </div>
        </header>
        <p>To evaluate the accuracy of our predictive model, we
        compared the proposed model with several most updated
        methods: (1) <strong>Markov model</strong> is widely used
        to predict human prediction&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0016">16</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0022">22</a>] for a long time. They
        regard all the visited locations as states and build a
        transition matrix to capture the first order transition
        probabilities between them. (2)
        <strong>PMM</strong>&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>], which is recently proposed,
        assumes that mobility location follows a spatiotemporal
        mixture model and predicts the next locations with
        periodicity into consideration. (3) <strong>RNN-based
        model</strong>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>] can be regarded as a
        simplification version of our model without historical
        attention module. We adapt the ST-RNN&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>] to our scene where only
        anonymous location ID is known, while the original version
        focuses on modeling the continuous spatiotemporal
        information. In the experiments, RNN-Short means that the
        trajectory is fed into the model day by day, while RNN-Long
        means that the whole trajectory lasting one month is
        directly fed into the model.The recurrent module of
        RNN-based model is GRU, which is the same with our
        model.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">The default parameter settings for
            DeepMove.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"><strong>Training
                Settings</strong></th>
                <th style="text-align:center;">
                <strong>Value</strong></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"><strong>Feature
                (Size)</strong></th>
                <th style="text-align:center;">
                <strong>Input</strong></th>
                <th><strong>Output</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">learning rate
                (<em>lr</em>)</td>
                <td style="text-align:center;">1e-3</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">location</td>
                <td style="text-align:center;">≈ 10000</td>
                <td>256</td>
              </tr>
              <tr>
                <td style="text-align:center;">the decay of
                <em>lr</em></td>
                <td style="text-align:center;">0.1</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">time</td>
                <td style="text-align:center;">{48,168}</td>
                <td>16</td>
              </tr>
              <tr>
                <td style="text-align:center;">L2 penalty</td>
                <td style="text-align:center;">1e-5</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">user ID</td>
                <td style="text-align:center;">≈ 1000</td>
                <td>16</td>
              </tr>
              <tr>
                <td style="text-align:center;">gradient clip</td>
                <td style="text-align:center;">1.0</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">hidden state</td>
                <td style="text-align:center;">300</td>
                <td>256</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The default training settings for our proposed model are
        presented in Table <a class="tbl" href="#tab2">2</a>, while
        the same parameters are used to train and evaluate the
        baseline RNN model. The experiments are conducted in terms
        of test-train mode, where the first 80% of each users’
        trajectory are selected as training data, the remaining 20%
        as testing data. In the following experiments, we utilize
        GRU as the default recurrent module of our model. In
        practice, the performance of GRU and LSTM are close, which
        are much better than the performance of vanilla
        RNN&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>]. We repeat our experiments at least
        3 times with different random seed.</p>
      </section>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Overall
            Performance</h3>
          </div>
        </header>
        <p>We evaluate our model with the baseline methods on three
        mobility datasets to present the performance of our model.
        We rank the candidate locations by the probabilities
        generated from the model, and check whether the
        ground-truth location <em>v</em> appears in the
        top-<em>k</em> candidate locations. The results of top-1
        and top-5 prediction accuracy are presented in Figure
        <a class="fig" href="#fig6">6</a>.</p>
        <figure id="fig6">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig6.jpg"
          class="img-responsive" alt="Figure 6" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 6:</span>
            <span class="figure-title">Performance comparison with
            baselines.</span>
          </div>
        </figure>
        <p></p>
        <p>We first analyze the result of Foursquare check-in data
        in Figure <a class="fig" href="#fig6">6</a>. In baseline
        methods, general RNN model works better than others
        significantly because of its powerful sequence modeling
        ability. Compared with the performance of the general RNN,
        we find that the prediction accuracy of our model is about
        31.63% better on average. This suggests that there indeed
        exist periodical regularity in the human mobility, which
        helps to improve the prediction accuracy. As the general
        RNN captures the complex sequential transition from the
        current trajectory, recurrent part of our model can also do
        like this. Nevertheless, our model utilizes the historical
        attention module to capture the periodical regularity from
        the lengthy trajectory history. Such attention mechanism on
        the trajectory helps our model understand the human
        mobility and achieve much better prediction accuracy.</p>
        <p>Evaluation results of the other two mobility datasets
        also demonstrate the superiority and generalization of our
        model. Compared with the Foursquare check-in data, cellular
        neural network data and mobile application data completely
        record human's daily life. As Figure <a class="fig" href=
        "#fig6">6</a> presents, the performance of our historical
        attention model outperforms the general RNN model over
        8.04% on average in mobile application data. The
        performance gain is 5.16% on average in cellular network
        data, which demonstrates the generalization of our model.
        Compared with the general RNN, the advantage of our model
        is that it can capture the periodical regularity of human
        mobility from trajectory history. In general, our model
        significantly outperforms all the baseline methods on three
        different mobility data in terms of prediction
        accuracy.</p>
        <p>Besides, we compare our models with the baseline methods
        on the raw schema, i.e., without deleting the duplication,
        of two daily mobility data. In Table <a class="tbl" href=
        "#tab3">3</a>, we can observe that our model outperforms
        the general RNN model by only about 1%, which achieves the
        prediction accuracy of 69.4%. Meanwhile, even the
        prediction accuracy of Markov model comes up to 50% in two
        mobility data. According to a further analysis of the data,
        we find that many users always stay in a location for
        several hours during the day. For this kind of trajectory,
        we can achieve pretty well results in location prediction
        only by simply copying the current input, where complex
        methods like attention mechanism takes minor effect because
        of the principal influence of the current input.
        Apparently, the performance gain of our DeepMove will be
        limited. However, it will work well on the mobility data
        where users move around different places.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Prediction performance on dense mobility
            data.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">cellular
                network</th>
                <th style="text-align:center;">mobile
                application</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Markov</td>
                <td style="text-align:center;">0.459</td>
                <td style="text-align:center;">0.595</td>
              </tr>
              <tr>
                <td style="text-align:center;">RNN</td>
                <td style="text-align:center;">0.595</td>
                <td style="text-align:center;">0.690</td>
              </tr>
              <tr>
                <td style="text-align:center;">Our model</td>
                <td style="text-align:center;">0.593</td>
                <td style="text-align:center;">0.694</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Reason
            Interpretation: Visualization of Historical Attention
            Weights</h3>
          </div>
        </header>
        <p>Because of the importance of periodicity of human
        mobility, our model, especially the historical attention
        module, is designed to capture the periodicity of human
        mobility. Thus, in this section, we discuss whether our
        periodicity assumption appears and whether our model really
        captures it.</p>
        <p>In Figure <a class="fig" href="#fig7">7</a>, we
        visualize the output of historical attention module to
        demonstrate this. To obtain the visualization, we first
        collect the normalized weight of historical attention
        module for a little seed users, and align them together on
        the time dimension. Then, we re-normalize the weights and
        draw them in Figure <a class="fig" href="#fig7">7</a> in
        terms of the heatmap. The horizontal axis and vertical axis
        of every square matrix in Figure <a class="fig" href=
        "#fig7">7</a> are both time period, the shade of the grids
        describe the weight, where the deeper green means the
        larger weight. For example, the top-left square matrix in
        Figure 7(a) shows us the distribution of the historical
        attention weight from 8.am. to 8. pm. during the weekday
        via the weekday's historical trajectories in mobile
        application data. The diagonal entries of it are remarkably
        larger than other entries, which shows the day-level
        regularity of human mobility in the different workday. The
        top-left square matrix in Figure 7(b) shows the similar
        result, while it is based on another cellular network data.
        The bottom-right square matrix in Figure 7(b) shows the
        attention distribution in the weekend in cellular network
        data, which also reveals a remarkably day-level regularity.
        In general, the results of Figure <a class="fig" href=
        "#fig7">7</a> show that our model indeed captures the
        regularity and periodicity from the historical trajectory.
        Meanwhile, our historical attention module not only
        improves the prediction accuracy but also offers an
        easy-to-interpret way to understand which historical
        activities are emphasized in the future mobility.</p>
        <figure id="fig7">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig7.jpg"
          class="img-responsive" alt="Figure 7" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 7:</span>
            <span class="figure-title">Visualization of the output
            of historical attention module. Every matrix presents
            the correlations between current trajectory and
            historical trajectory. The diagonal entries of matrix
            present the correlations of trajectories in the same
            time period of different days. The shade of the grids
            describe the weight where the deeper green means the
            larger weight. For example, the top-left matrix in (a)
            shows the correlations of the current trajectory and
            the historical trajectory on workdays on cellular
            network data. The highlight diagonal entries tell us
            that trajectories during the workday are periodical.
            The bottom-right matrix in (b) tells us that
            trajectories during the weekend are even more
            periodical than the workday.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Model
            Variations</h3>
          </div>
        </header>
        <p>In order to present the efficiency of historical
        attention module, we first compare two proposed historical
        attention modules in terms of prediction accuracy and
        computation efficiency, and then discuss how different
        sampling strategies in the embedding encode attention
        module can influence the final results. Finally, we discuss
        the effect of user embedding and present the effectiveness
        of our model in describing personal preference.</p>
        <p>We compare the performance and efficiency of our
        proposed two historical attention module on two datasets.
        The results are presented in Table <a class="tbl" href=
        "#tab4">4</a>. The sequential encode attention module works
        better than the embedding encode attention module in most
        of the time especially in mobile application data, while
        the latter one computes more efficiently. Two reasons may
        account for the better performance of the sequential encode
        attention module: 1) it captures sequential information
        along the lengthy trajectory to some extent, while the
        embedding encoder cannot; 2) the latent space of output of
        it is more similar to the current mobility status's because
        of the similar generation structure.</p>
        <p>Besides, we evaluate the system performance of different
        sampling strategies in the sampling layer of embedding
        encode attention module. As mentioned in the model section,
        we design three kinds of sampling strategies in the
        historical attention module: average sampling, maximum
        sampling, and none sampling. Figure 8(a) shows the
        evaluation results of three different samplings in two
        datasets in terms of top-1 prediction accuracy. In general,
        the average sampling strategy works better among three
        strategies, while the maximum sampling strategy performs a
        little worse, the result shows us that most people own
        regular mobility pattern and limited locations to visit.
        The performance gap of three strategies on cellular network
        data is larger than the mobile application data.</p>
        <p>Finally, we identify every single user with a user ID
        and add user ID embedding feature to the model to capture
        the personality. The results are presented in Figure
        <a class="fig" href="#fig8">8(b)</a>. Obvious performance
        gain can be observed in the general RNN model after adding
        the user ID embedding. However, the performance gain of our
        model can be omitted, which demonstrates that our proposed
        model not only capture deeper periodical pattern but also
        characterize personal regularities.</p>
        <figure id="fig8">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig8.jpg"
          class="img-responsive" alt="Figure 8" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 8:</span>
            <span class="figure-title">Performance variation with
            the model design.</span>
          </div>
        </figure>
        <p></p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Efficiency of two attention
            models.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">
                <strong>Dataset</strong></th>
                <th style="text-align:center;">
                <strong>Model</strong></th>
                <th style="text-align:center;"><strong>Top-1
                Accuracy</strong></th>
                <th><strong>Overhead(s)</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">cellular
                network</td>
                <td style="text-align:center;">attention 1</td>
                <td style="text-align:center;">0.22</td>
                <td>≈ 600</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">attention 2</td>
                <td style="text-align:center;">0.24</td>
                <td>≈ 1600</td>
              </tr>
              <tr>
                <td style="text-align:center;">mobile
                application</td>
                <td style="text-align:center;">attention 1</td>
                <td style="text-align:center;">0.24</td>
                <td>≈ 2600</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">attention 2</td>
                <td style="text-align:center;">0.27</td>
                <td>≈ 11200</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.6</span> Evaluation
            on User Groups</h3>
          </div>
        </header>
        <p>In order to evaluate the variation of performance gain
        among different users, we cluster them into different
        groups based on three rules: mobility
        entropy&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0008">8</a>], explore ratio, radius of
        gyration&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0010">10</a>]. Mobility entropy calculates the
        entropy of locations in trajectory, which is related to the
        regularity level of human mobility. Explore ratio
        represents the fraction of new locations in test data,
        which do not exist during the training. Thus, one person
        with more regular behaviors should have lower mobility
        entropy and lower explore ratio. The final rule is the
        radius of gyration which describes the spatial range of the
        mobility.</p>
        <p>The evaluation results are presented in Figure <a class=
        "fig" href="#fig9">9</a>, where the vertical axis shows the
        performance gain compared with the baseline method-general
        RNN. There are two interesting insights from the result: 1)
        our model outperforms the baseline method on almost all
        kinds of users; 2) our model predicts non-regular users
        better than baseline method that meets the goal of our
        historical attention module. For example, in the top-left
        image of Figure <a class="fig" href="#fig9">9</a>, our
        model's prediction accuracy increases when the mobility
        entropy of users increases. With the effective usage of
        lengthy historical trajectory, our model captures the
        underlying and deeper periodical patterns of human
        mobility.</p>
        <figure id="fig9">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186058/images/www2018-67-fig9.jpg"
          class="img-responsive" alt="Figure 9" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 9:</span>
            <span class="figure-title">Performance varies with
            mobility entropy/explore ratio/radius of gyration (rg)
            on cellular network data (left) and mobile application
            data (right).</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-25">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> RELATED
          WORK</h2>
        </div>
      </header>
      <p>Works close to our work can be classified into two
      categories: model-based methods and pattern-based methods.
      Besides, we introduce related works on recurrent neural
      network and attention model.</p>
      <p><em>Model-based methods:</em>. Markov model and its
      variations are common models of this approach. In
      Markov-based models&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>], they model the probability of the
      future action by building a transition matrix between several
      locations based on the past trajectories. To capture the
      unobserved characteristics between location transition,
      Mathew et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>] cluster the locations from the
      trajectories and then train a Hidden Markov Model for each
      user. Considering the mobility similarity between user group,
      Zhang et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0043">43</a>] propose GMove: a group-level
      mobility modeling method to share significant movement
      regularity. Different from existing Markov-based models, our
      model can model time-dependent and high order
      transitions.</p>
      <p><em>Pattern-based methods:</em>. Pattern-based
      methods&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0042">42</a>] first discover the popular
      sequential mobility patterns from the trajectory, and then
      try to predict the mobility based on these popular patterns.
      Matrix factorization can also be regarded as a kind of
      pattern discovered method. Matrix factorization
      (MF)&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0018">18</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>] emerges
      from recommendation system and the basic idea of it is to
      factorize the users-items matrix into two latent matrices
      which represent the users and items characteristics. Cheng et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>]
      fuse MF with the geographical influence by modeling the
      location probability as a multi-center Gaussian Model.
      Combining Markov model with matrix factorization, Rendle et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0027">27</a>]
      propose factorized personalized Markov model (FPMC) to do
      item recommendation. Based on FPMC, Cheng et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>]
      propose a matrix factorization method named FPMC-LR to
      capture the sequence transition with Markov chain while
      considering the localized region constraint. Compared with
      pattern-based methods, our model can not only model the
      transitional regularities shared by all the users but also
      model the personal preference based on the user embedding and
      personal historical trajectory.</p>
      <p><em>Recurrent neural network:</em>. Recurrent Neural
      Network (RNN)&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] is a powerful tool to capture the
      long-range dependencies in the sequence and have achieved
      success in Natural Language Processing (NLP)&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0030">30</a>], Image
      Caption&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>], etc. Because of its powerful
      representation ability, RNN have been applied to many fields
      like click prediction&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0044">44</a>], recommendation
      system&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0013">13</a>]
      and mobility prediction&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0038">38</a>]. RNN-based models is also a kind of
      model-based method. Liu et al.&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>] propose Spatial Temporal
      Recurrent Neural Networks (ST-RNN) to model temporal and
      spatial contexts. However, the proposed model is too
      complicated to train and apply with so many parameters.
      Besides, it can not be applied into the discrete location
      prediction scene because of its continuous spatial modeling
      method. Du et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>] propose Recurrent Marked Temporal
      Point Process (RMTPP) to learn the conditional intensity
      function automatically from the history. However, this model
      is not specific for the trajectory prediction and do not
      consider the natural characteristics of trajectory like
      multi-level periodicity. By coupling convolutional and
      recurrent neural network, the Yao et al.&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0039">39</a>] propose DeepSense: a
      unified deep learning framework for mobile sensing data.
      However, this model needs uniform sampling data and also does
      not consider the multi-level periodicity of trajectory.</p>
      <p><em>Attention model:</em>. Based on the seq2seq
      model&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0030">30</a>],
      Bahdanau et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>] introduce attention mechanism into
      neural machine translation task. The attention mechanism
      strengthens not only the ability of RNN in capturing the
      long-range dependencies but also the interpretability.
      Attention mechanism has been applied into many other fields
      such as image caption&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>] and recommendation
      system&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0002">2</a>],
      which achieves satisfactory result. To the best of our
      knowledge, we are the first to propose the attention
      mechanism into mobility prediction to model human
      mobility.</p>
    </section>
    <section id="sec-26">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we investigated the problem of mobility
      prediction from the sparse and lengthy trajectories. We
      proposed an attentional mobility model DeepMove, which enjoys
      two novel characteristics compared to previous methods: 1) a
      multi-modal embedding recurrent neural network for capturing
      multiple factors that govern the transition regularities of
      human mobility; and 2) a historical attention module for
      modeling the multi-level periodicity of human mobility.
      Extensive experiments on three real-life mobility datasets
      show that DeepMove significantly outperforms all the
      baselines. Meanwhile, the visualization of historical
      attention weights shows that DeepMove is able to effectively
      capture meaningful periodicities for mobility prediction.</p>
      <p>There are several future directions for our work. First,
      we currently only predict the next location because we fixed
      the time interval in practice. In the future, we plan to
      expand the location prediction into the spatiotemporal point
      prediction by taking the potential duration into
      consideration. Second, our current work does not consider the
      semantic context in the trajectory like point of interests
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0033">33</a>] and user's
      tweets because of the limitation of data. In the future, we
      plan to add these semantic information into the model to
      predict not only the location but also the underlying
      motivation of user's movement.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-27">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This work is supported by research fund of Tsinghua
      University - Tencent Joint Laboratory for Internet Innovation
      Technology.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Dzmitry Bahdanau,
        Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine
        Translation by Jointly Learning to Align and Translate.
        <em><em>Computer Science</em></em> (2014).</li>
        <li id="BibPLXBIB0002" label="[2]">Jingyuan Chen, Hanwang
        Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-Seng
        Chua. 2017. Attentive Collaborative Filtering: Multimedia
        Recommendation with Item- and Component-Level Attention. In
        <em><em>Proceedings of the 40th International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval</em></em> (SIGIR ’17). ACM, New York, NY, USA,
        335–344. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/3077136.3080797" target="_blank">
          https://doi.org/10.1145/3077136.3080797</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Meng Chen, Yang Liu, and
        Xiaohui Yu. 2014. NLPMM: A Next Location Predictor with
        Markov Modeling. 8444 (2014), 186–197.</li>
        <li id="BibPLXBIB0004" label="[4]">C. Cheng, H. Yang, I.
        King, and M.&nbsp;R. Lyu. 2012. Fused matrix factorization
        with geographical and social influence in location-based
        social networks. In <em><em>AAAI Conference on Artificial
        Intelligence</em></em> .</li>
        <li id="BibPLXBIB0005" label="[5]">Chen Cheng, Haiqin Yang,
        Michael&nbsp;R Lyu, and Irwin King. 2013. Where you like to
        go next: successive point-of-interest recommendation. In
        <em><em>International Joint Conference on Artificial
        Intelligence</em></em> .</li>
        <li id="BibPLXBIB0006" label="[6]">Eunjoon Cho,
        Seth&nbsp;A. Myers, and Jure Leskovec. 2011. Friendship and
        Mobility: User Movement in Location-based Social Networks.
        In <em><em>Proceedings of the 17th ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining</em></em>
        (KDD ’11). ACM, New York, NY, USA, 1082–1090. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2020408.2020579" target="_blank">
          https://doi.org/10.1145/2020408.2020579</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Junyoung Chung, Çaglar
        Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical
        Evaluation of Gated Recurrent Neural Networks on Sequence
        Modeling. <em><em>CoRR</em></em> abs/1412.3555(2014).
        <a class="link-inline force-break" href=
        "http://arxiv.org/abs/1412.3555" target=
        "_blank">http://arxiv.org/abs/1412.3555</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Justin Cranshaw, Eran
        Toch, Jason Hong, Aniket Kittur, and Norman Sadeh. 2010.
        Bridging the Gap Between Physical Location and Online
        Social Networks. In <em><em>Proceedings of the 12th ACM
        International Conference on Ubiquitous Computing</em></em>
        (UbiComp ’10). ACM, New York, NY, USA, 119–128. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/1864349.1864380" target="_blank">
          https://doi.org/10.1145/1864349.1864380</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">Nan Du, Hanjun Dai,
        Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez,
        and Le Song. 2016. Recurrent Marked Temporal Point
        Processes: Embedding Event History to Vector. In
        <em><em>Proceedings of the 22Nd ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining</em></em>
        (KDD ’16). ACM, New York, NY, USA, 1555–1564. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2939672.2939875" target="_blank">
          https://doi.org/10.1145/2939672.2939875</a>
        </li>
        <li id="BibPLXBIB0010" label="[10]">M.&nbsp;C. González,
        C.&nbsp;A. Hidalgo, and A.&nbsp;L. Barabási. 2008.
        Understanding individual human mobility patterns.
        <em><em>Nature</em></em> 453, 7196 (2008), 779.</li>
        <li id="BibPLXBIB0011" label="[11]">Alex Graves. 2008.
        Supervised Sequence Labelling with Recurrent Neural
        Networks. <em><em>Studies in Computational
        Intelligence</em></em> 385 (2008).</li>
        <li id="BibPLXBIB0012" label="[12]">Alex Graves. 2013.
        Generating Sequences With Recurrent Neural Networks.
        <em><em>Computer Science</em></em> (2013).</li>
        <li id="BibPLXBIB0013" label="[13]">Balázs Hidasi,
        Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
        2015. Session-based Recommendations with Recurrent Neural
        Networks. <em><em>Computer Science</em></em> (2015).</li>
        <li id="BibPLXBIB0014" label="[14]">Sepp Hochreiter and
        Jürgen Schmidhuber. 1997. Long Short-Term Memory. <em><em>
          Neural Comput.</em></em> 9, 8 (Nov. 1997), 1735–1780.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1162/neco.1997.9.8.1735" target=
          "_blank">https://doi.org/10.1162/neco.1997.9.8.1735</a>
        </li>
        <li id="BibPLXBIB0015" label="[15]">Jintao Ke Xianfeng Tang
        Yitian Jia Siyu Lu Pinghua Gong Jieping&nbsp;Ye
        Huaxiu&nbsp;Yao, Fei&nbsp;Wu and Zhenhui Li. 2018. Deep
        Multi-View Spatial-Temporal Network for Taxi Demand
        Prediction. In <em><em>Proceedings of 2018 AAAI Conference
        on Artificial Intelligence</em></em> (AAAI ’18).</li>
        <li id="BibPLXBIB0016" label="[16]">Marc&nbsp;Olivier
        Killijian. 2012. Next place prediction using mobility
        Markov chains. In <em><em>The Workshop on Measurement,
        Privacy, and Mobility</em></em> . 3.</li>
        <li id="BibPLXBIB0017" label="[17]">Diederik&nbsp;P. Kingma
        and Jimmy Ba. 2014. Adam: A Method for Stochastic
        Optimization. <em><em>CoRR</em></em> abs/1412.6980(2014).
        <a class="link-inline force-break" href=
        "http://arxiv.org/abs/1412.6980" target=
        "_blank">http://arxiv.org/abs/1412.6980</a>
        </li>
        <li id="BibPLXBIB0018" label="[18]">Yehuda Koren, Robert
        Bell, and Chris Volinsky. 2009. Matrix Factorization
        Techniques for Recommender Systems.
        <em><em>Computer</em></em> 42, 8 (2009), 30–37.</li>
        <li id="BibPLXBIB0019" label="[19]">Zachary&nbsp;C Lipton,
        John Berkowitz, and Charles Elkan. 2015. A Critical Review
        of Recurrent Neural Networks for Sequence Learning.
        <em><em>Computer Science</em></em> (2015).</li>
        <li id="BibPLXBIB0020" label="[20]">Qiang Liu, Shu Wu,
        Liang Wang, and Tieniu Tan. 2016. Predicting the next
        location: a recurrent model with spatial and temporal
        contexts. In <em><em>Thirtieth AAAI Conference on
        Artificial Intelligence</em></em> . 194–200.</li>
        <li id="BibPLXBIB0021" label="[21]">Minh&nbsp;Thang Luong,
        Hieu Pham, and Christopher&nbsp;D Manning. 2015. Effective
        Approaches to Attention-based Neural Machine Translation.
        <em><em>Computer Science</em></em> (2015).</li>
        <li id="BibPLXBIB0022" label="[22]">Wesley Mathew, Ruben
        Raposo, and Bruno Martins. 2012. Predicting future
        locations with hidden Markov models. In <em><em>ACM
        Conference on Ubiquitous Computing</em></em> .
        911–918.</li>
        <li id="BibPLXBIB0023" label="[23]">Hongyuan Mei and Jason
        Eisner. 2017. The Neural Hawkes Process: A Neurally
        Self-Modulating Multivariate Point Process. In
        <em><em>Advances in Neural Information Processing
        Systems</em></em> . Long Beach. <a class=
        "link-inline force-break" href=
        "https://arxiv.org/abs/1612.09328" target=
        "_blank">https://arxiv.org/abs/1612.09328</a>
        </li>
        <li id="BibPLXBIB0024" label="[24]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
        Distributed Representations of Words and Phrases and their
        Compositionality. <em><em>Advances in Neural Information
        Processing Systems</em></em> 26 (2013), 3111–3119.</li>
        <li id="BibPLXBIB0025" label="[25]">Anna Monreale, Fabio
        Pinelli, Roberto Trasarti, and Fosca Giannotti. 2009.
        WhereNext:a location predictor on trajectory pattern
        mining. In <em><em>ACM SIGKDD International Conference on
        Knowledge Discovery and Data Mining, Paris, France, June 28
        - July</em></em> . 637–646.</li>
        <li id="BibPLXBIB0026" label="[26]">Fabio Pinelli, Fabio
        Pinelli, Fabio Pinelli, and Dino Pedreschi. 2007.
        Trajectory pattern mining. In <em><em>ACM SIGKDD
        International Conference on Knowledge Discovery and Data
        Mining</em></em> . 330–339.</li>
        <li id="BibPLXBIB0027" label="[27]">Steffen Rendle,
        Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010.
        Factorizing personalized Markov chains for next-basket
        recommendation. In <em><em>International Conference on
        World Wide Web</em></em> . 811–820.</li>
        <li id="BibPLXBIB0028" label="[28]">C. Song, Z. Qu, N
        Blumm, and A.&nbsp;L. Barabási. 2010. Limits of
        predictability in human mobility. <em><em>Science</em></em>
        327, 5968 (2010), 1018.</li>
        <li id="BibPLXBIB0029" label="[29]">Xiaoyuan Su and
        Taghi&nbsp;M. Khoshgoftaar. 2009. <em><em>A survey of
        collaborative filtering techniques</em></em> . Hindawi
        Publishing Corp.4 pages.</li>
        <li id="BibPLXBIB0030" label="[30]">Ilya Sutskever, Oriol
        Vinyals, and Quoc&nbsp;V Le. 2014. Sequence to sequence
        learning with neural networks. 4 (2014), 3104–3112.</li>
        <li id="BibPLXBIB0031" label="[31]">Hongjian Wang and
        Zhenhui Li. 2017. Region Representation Learning via
        Mobility Flow. In <em><em>CIKM</em></em> .</li>
        <li id="BibPLXBIB0032" label="[32]">Fei Wu and Zhenhui Li.
        2016. Where Did You Go: Personalized Annotation of Mobility
        Records. In <em><em>Proceedings of the 25th ACM
        International on Conference on Information and Knowledge
        Management</em></em> (CIKM ’16). ACM, New York, NY, USA,
        589–598. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/2983323.2983845" target="_blank">
          https://doi.org/10.1145/2983323.2983845</a>
        </li>
        <li id="BibPLXBIB0033" label="[33]">Fei Wu, Zhenhui Li,
        Wang-Chien Lee, Hongjian Wang, and Zhuojie Huang. 2015.
        Semantic Annotation of Mobility Data Using Social Media. In
        <em><em>Proceedings of the 24th International Conference on
        World Wide Web</em></em> (WWW ’15). International World
        Wide Web Conferences Steering Committee, Republic and
        Canton of Geneva, Switzerland, 1253–1263. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2736277.2741675" target="_blank">
          https://doi.org/10.1145/2736277.2741675</a>
        </li>
        <li id="BibPLXBIB0034" label="[34]">Fengli Xu, Zhen Tu,
        Yong Li, Pengyu Zhang, Xiaoming Fu, and Depeng Jin. 2017.
        Trajectory Recovery From Ash: User Privacy Is NOT Preserved
        in Aggregated Mobility Data. In <em><em>International
        Conference on World Wide Web</em></em> . 1241–1250.</li>
        <li id="BibPLXBIB0035" label="[35]">Fengli Xu, Pengyu
        Zhang, and Yong Li. 2016. Context-aware Real-time
        Population Estimation for Metropolis. In
        <em><em>Proceedings of the 2016 ACM International Joint
        Conference on Pervasive and Ubiquitous Computing</em></em>
        (UbiComp ’16). ACM, New York, NY, USA, 1064–1075.
          <a class="link-inline force-break" href=
          "https://doi.org/10.1145/2971648.2971673" target=
          "_blank">https://doi.org/10.1145/2971648.2971673</a>
        </li>
        <li id="BibPLXBIB0036" label="[36]">Kelvin Xu, Jimmy Ba,
        Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
        Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015.
        Show, Attend and Tell: Neural Image Caption Generation with
        Visual Attention. <em><em>Computer Science</em></em>
        (2015), 2048–2057.</li>
        <li id="BibPLXBIB0037" label="[37]">Cheng Yang, Maosong
        Sun, Wayne&nbsp;Xin Zhao, Zhiyuan Liu, and Edward&nbsp;Y.
        Chang. 2017. A Neural Network Approach to Jointly Modeling
        Social Networks and Mobile Trajectories. <em><em>ACM Trans.
        Inf. Syst.</em></em> 35, 4, Article 36 (Aug. 2017),
        28&nbsp;pages. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/3041658" target=
        "_blank">https://doi.org/10.1145/3041658</a>
        </li>
        <li id="BibPLXBIB0038" label="[38]">Di Yao, Chao Zhang,
        Jianhui Huang, and Jingping bi. 2017. SERM: A Recurrent
        Model for Next Location Prediction in Semantic
        Trajectories. In <em><em>ACM International on Conference on
        Information and Knowledge Management</em></em> .
        2411–2414.</li>
        <li id="BibPLXBIB0039" label="[39]">Shuochao Yao, Shaohan
        Hu, Yiran Zhao, Aston Zhang, and Tarek Abdelzaher. 2017.
        DeepSense: A Unified Deep Learning Framework for
        Time-Series Mobile Sensing Data Processing. In
        <em><em>Proceedings of the 26th International Conference on
        World Wide Web</em></em> (WWW ’17). International World
        Wide Web Conferences Steering Committee, Republic and
        Canton of Geneva, Switzerland, 351–360. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/3038912.3052577" target="_blank">
          https://doi.org/10.1145/3038912.3052577</a>
        </li>
        <li id="BibPLXBIB0040" label="[40]">Jia&nbsp;Ching Ying,
        Wang&nbsp;Chien Lee, Tz&nbsp;Chiao Weng, and
        Vincent&nbsp;S. Tseng. 2011. Semantic trajectory mining for
        location prediction. In <em><em>ACM Sigspatial
        International Conference on Advances in Geographic
        Information Systems</em></em> . 34–43.</li>
        <li id="BibPLXBIB0041" label="[41]">Quan Yuan, Wei Zhang,
        Chao Zhang, Xinhe Geng, Gao Cong, and Jiawei Han. 2017.
        PRED: Periodic Region Detection for Mobility Modeling of
        Social Media Users. In <em><em>Proceedings of the Tenth ACM
        International Conference on Web Search and Data
        Mining</em></em> (WSDM ’17). ACM, New York, NY, USA,
        263–272. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/3018661.3018680" target="_blank">
          https://doi.org/10.1145/3018661.3018680</a>
        </li>
        <li id="BibPLXBIB0042" label="[42]">Chao Zhang, Jiawei Han,
        Lidan Shou, Jiajun Lu, and Thomas La&nbsp;Porta. 2014.
        Splitter: Mining Fine-grained Sequential Patterns in
        Semantic Trajectories. <em><em>Proc. VLDB Endow.</em></em>
        7, 9 (May 2014), 769–780. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.14778/2732939.2732949" target="_blank">
          https://doi.org/10.14778/2732939.2732949</a>
        </li>
        <li id="BibPLXBIB0043" label="[43]">C. Zhang, K. Zhang, Q.
        Yuan, L. Zhang, T Hanratty, and J. Han. 2016. GMove:
        Group-Level Mobility Modeling Using Geo-Tagged Social
        Media. In <em><em>ACM SIGKDD International Conference on
        Knowledge Discovery and Data Mining</em></em> .
        1305–1314.</li>
        <li id="BibPLXBIB0044" label="[44]">Yuyu Zhang, Hanjun Dai,
        Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, and
        Tie&nbsp;Yan Liu. 2014. Sequential click prediction for
        sponsored search with recurrent neural networks. In
        <em><em>Twenty-Eighth AAAI Conference on Artificial
        Intelligence</em></em> . 1369–1375.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186058">https://doi.org/10.1145/3178876.3186058</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
