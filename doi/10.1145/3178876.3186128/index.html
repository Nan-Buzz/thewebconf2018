<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Low Rank Spectral Network Alignment</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Low Rank Spectral Network Alignment</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Huda</span>      <span class="surName">Nassar</span>,     Computer Science Department, Purdue University, West Lafayette, Indiana, USA, <a href="mailto:hnassar@purdue.edu">hnassar@purdue.edu</a>     </div>     <div class="author">     <span class="givenName">Nate</span>      <span class="surName">Veldt</span>,     Mathematics Department, Purdue University, West Lafayette, Indiana, USA, <a href="mailto:lveldt@purdue.edu">lveldt@purdue.edu</a>     </div>     <div class="author">     <span class="givenName">Shahin</span>      <span class="surName">Mohammadi</span>,     CSAIL MIT &#x0026;, Broad Institute, Cambridge, Massachusetts, USA, <a href="mailto:mohammadi@broadinstitute.org">mohammadi@broadinstitute.org</a>     </div>     <div class="author">     <span class="givenName">Ananth</span>      <span class="surName">Grama</span>,     Computer Science Department, Purdue University, West Lafayette, Indiana, USA, <a href="mailto:ayg@cs.purdue.edu">ayg@cs.purdue.edu</a>     </div>     <div class="author">     <span class="givenName">David F.</span>      <span class="surName">Gleich</span>,     Computer Science Department, Purdue University, West Lafayette, Indiana, USA, <a href="mailto:dgleich@purdue.edu">dgleich@purdue.edu</a>     </div>                         </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186128" target="_blank">https://doi.org/10.1145/3178876.3186128</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Network alignment or graph matching is the classic problem of finding matching vertices between two graphs with applications in network de-anonymization and bioinformatics. There exist a wide variety of algorithms for it, but a challenging scenario for all of the algorithms is aligning two networks without any information about which nodes might be good matches. In this case, the vast majority of principled algorithms demand quadratic memory in the size of the graphs. We show that one such method&#x2014;the recently proposed and theoretically grounded EigenAlign algorithm&#x2014;admits a novel implementation which requires memory that is linear in the size of the graphs. The key step to this insight is identifying low-rank structure in the node-similarity matrix used by EigenAlign for determining matches. With an exact, closed-form low-rank structure, we then solve a maximum weight bipartite matching problem on that low-rank matrix to produce the matching between the graphs. For this task, we show a new, a-posteriori, approximation bound for a simple algorithm to approximate a maximum weight bipartite matching problem on a low-rank matrix. The combination of our two new methods then enables us to tackle much larger network alignment problems than previously possible and to do so quickly. Problems that take hours with existing methods take only seconds with our new algorithm. We thoroughly validate our low-rank algorithm against the original EigenAlign approach. We also compare a variety of existing algorithms on problems in bioinformatics and social networks. Our approach can also be combined with existing algorithms to improve their performance and speed.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>network alignment; graph matching; low rank matrix; low-rank bipartite matching</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Huda Nassar, Nate Veldt, Shahin Mohammadi, Ananth Grama, and David F. Gleich. 2018. Low Rank Spectral Network Alignment. In <em>WWW 2018: The 2018 Web Conference,       April 23&#x2013;27, 2018,       Lyon, France</em>. ACM, New York, NY, USA, 10 pages. <a href="https://doi.org/10.1145/3178876.3186128" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186128</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction And Motivation</h2>     </div>    </header>    <p>Network alignment is the problem of pairing nodes across two different graphs in a way that preserves edge structure and highlights similarities between the networks. The node pairings can either be one-to-one or many-to-many. While the methods we propose are amenable to both settings with some modification, we focus on the one-to-one case as it has the most extensive literature. Applications of network alignment include (i) finding similar nodes in social networks, which uncovers information about one or both of the paired nodes, and can help with tailoring advertisements and suggesting activities for similar users in a network; (ii) social-network de-anonymization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]; and (iii) pattern matching in graphs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. One very popular example of this problem is the alignment of protein-protein interaction networks in biology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. Often in biology one can extract valuable knowledge about proteins for which little information is known by aligning a protein network with another protein network that has been studied more. By doing so one can draw conclusions about proteins in the first network by understanding their similarities to proteins in the second.</p>    <p>There are two major approaches to network alignment problems&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]: local network alignment, where the goal is to find local regions of the graph that are similar to any given node, and global network alignment, where the goal is to understand how two large graphs would align to each other. Many approaches to network alignment rely on solving an optimization problem to compute what amounts to a topological similarity score between pairs of nodes in the two networks. Here, we focus on global alignment with one-to-one matches between the two graphs.</p>    <p>Some applications also come with prior information about which nodes in one network may be good matches for nodes of another network, which implicitly imposes a restriction on the number of the similarity scores that must be computed and stored in practice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. However, for problems that lack this prior, the data requirement for storing the similarity scores is quadratic, which severely limits the scalability of this class of approaches to solve the problem. For instance, methods such as the Lagrangian relaxation method of Klau et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] require at least quadratic memory. There do exist memory-scalable heuristics for solving network alignment problems with no prior, including the GHOST procedure of Patro et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>], or the GRAAL algorithm of Kuchaieve et al&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] and its variants. However, these usually involve cubic or worse computation in terms of vertex neighborhoods in the graph (e.g. enumeration of all 5-node graphlets within a local region).</p>    <p>One principled approach that avoids the quadratic memory requirement is the Network Similarity Decomposition (NSD)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>], which provides a useful low-rank decomposition of a specific similarity matrix based on the IsoRank method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. This method enables alignments to be computed between extremely large networks. However, there have been many improvements to network alignment methods since the publication of IsoRank.</p>    <p>A recent innovation is a method based on eigenvectors called EigenAlign. The EigenAlign method uses the dominant eigenvector of a matrix related to the product-graph between the two networks in order to estimate the similarity. The eigenvector information is rounded into a matching between the vertices of the graphs by solving a maximum-weight bipartite matching problem on a dense bipartite graph&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. The IsoRank method is also based on eigenvectors, or more specifically, the PageRank vector of the product-graph of the two networks was used for the same purpose&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. In contrast, a key innovation of EigenAlign is that it explicitly models nodes that may not have a match in the network. In this way, it is able to provably align many simple graph models such as Erd&#x0151;s-R&#x00E9;nyi when the graphs do not have too much noise. This gives it a firm theoretical basis <em>although it still suffers from the quadratic memory requirement</em>.</p>    <p>In our manuscript, we highlight a number of innovations that enable the EigenAlign methodology to work without the quadratic memory requirement. We first show that the EigenAlign solution can be expressed via low-rank factors, and we can compute these low-rank factors exactly and explicitly using a simple procedure. A challenge in using the low-rank information provided by our new method is that there are only a few ideas on how to use the low-rank structure of the similarity scores in the matching step&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>]. We contribute a new analysis of a simple idea to use the low-rank structure that gives a computable a-posteriori approximation guarantee. In practice, this approximation guarantee is extremely good: around 1.1. Such a procedure should enable further low-rank applications beyond just network alignment.</p>    <p>     <strong>Our contributions.</strong>    </p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">An explicit expression for the solution of the EigenAlign eigenvector as a low-rank matrix (Theorem&#x00A0;<a class="enc" href="#enc1">3.1</a>).<br/></li>     <li id="list2" label="&#x2022;">An <em>O</em>(<em>n</em>log&#x2009;<em>n</em>) method that will solve a maximum-weight bipartite matching problem on a low-rank matrix with an a-posteriori approximation guarantee (Algorithm&#x00A0;<a class="fig" href="#fig3">3</a>, Theorem&#x00A0;<a class="enc" href="#enc3">4.2</a>). In practice, these approximation guarantees are better than 1.1 for our experiments (Figure &#x00A0;<a class="fig" href="#fig6">7</a>). This improves recent work in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>], which gave a simple <em>k</em>-approximation algorithm, where <em>k</em> is the rank.<br/></li>     <li id="list3" label="&#x2022;">A thorough evaluation of our methodology to show that there appears to be little difference between the results of our low-rank methods and the original EigenAlign (Section&#x00A0;<a class="sec" href="#sec-20">5.1</a>), and our methods are more scalable.<br/></li>     <li id="list4" label="&#x2022;">A demonstration that our low-rank methods can be combined with existing network alignment methods to yield better quality results (Section&#x00A0;<a class="sec" href="#sec-21">5.2</a>).<br/></li>     <li id="list5" label="&#x2022;">A demonstration that the methods are sufficiently scalable to be run for all pairs of networks induced by the vertex neighborhoods of every two connected nodes in a large graph. That is, we seek to align two vertex neighborhoods together whenever the vertices have an edge. To validate the alignments, we show that these track the Jaccard similarity between the set of neighbors. (Figure&#x00A0;<a class="fig" href="#fig10">10</a>).<br/></li>    </ul>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Network alignment formulations and current techniques</h2>     </div>    </header>    <p>We now review the state of network alignment algorithms and our specific setting and objective. A helpful illustration is shown in Figure&#x00A0;<a class="fig" href="#fig1">1</a>. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Our setup for network alignment follows Feizi&#x00A0;et.&#x00A0;al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0005">5</a>], where we seek to align two networks without any other metadata. Possible alignments between pairs of any nodes (i, j) in G<sub>A</sub> and (i&#x2032;, j&#x2032;) in G<sub>B</sub> are scored based on one of three cases and assembled into a massive, but highly structured, alignment matrix <em>M</em>.</span>      </div>     </figure> </p>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> The canonical network alignment problem</h3>     </div>     </header>     <p>For the network alignment problem, we are given two graphs <em>G<sub>A</sub>     </em> and <em>G<sub>B</sub>     </em> with adjacency matrices <em>A</em> and <em>B</em>. The goal is to produce a one-to-one mapping between nodes of <em>G<sub>A</sub>     </em> and <em>G<sub>B</sub>     </em> that preserves topological similarities between the networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. In some cases we additionally receive information about which nodes in one network can be paired with nodes in the other. This additional information is presented in the form of a bipartite graph whose edge weights are stored in a matrix <em>L</em>; if <em>L<sub>uv</sub>     </em> > 0, this indicates outside evidence that node <em>u</em> in <em>G<sub>A</sub>     </em> should be matched to node <em>v</em> in <em>G<sub>B</sub>     </em>. We call this outside evidence <em>a prior</em> on the alignment. When a prior is present, the prior and topological information are taken together to determine an alignment.</p>     <p>More formally, we seek a binary matrix <em>P</em> that encodes a matching between the nodes of the networks and maximizes one of a few possible objective functions discussed below. The matrix <em>P</em> encodes a matching when it satisfies the constraints <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ P_{u,v} = {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}1 &#x0026; u \text{ is matched with } v \\ 0 &#x0026; \text{otherwise}. \end{array}\right.}, \begin{array}{l}\sum _{u} P_{u,v} \le 1 \text{ for all $v$}, \\ \sum _v P_{u,v} \le 1 \text{ for all $u$}. \end{array} \] </span>       <br/>      </div>     </div> The inequality constraints guarantee that a node in the first network is only matched with one or zero nodes in the other network.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Objective functions for network alignment</h3>     </div>     </header>     <p>The classic formulation of the problem seeks a matrix <em>P</em> that maximizes the number of overlapping edges between <em>G<sub>A</sub>     </em> and <em>G<sub>B</sub>     </em>, i.e. the number of adjacent node pairs (<em>i<sub>A</sub>     </em>, <em>j<sub>A</sub>     </em>) in <em>G<sub>A</sub>     </em> that are mapped to an adjacent node pair <span class="inline-equation"><span class="tex">$(i^{\prime }_B,j^{\prime }_B)$</span>     </span> in <em>G<sub>B</sub>     </em>. This results in the following integer quadratic program: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{array}{ll}\displaystyle \mathop{maximize}_{{P}} &#x0026; \sum _{ij}[{P}^T {A}{P}]_{ij} [{B}]_{ij}\\ \text{subject to} &#x0026; \sum _u P_{u,v} \le 1 \text{ for all $v$} \\ &#x0026; \sum _v P_{u,v} \le 1 \text{ for all $u$} \\ &#x0026; P_{u,v} \in \lbrace 0,1\rbrace \end{array} \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> Recent variations of this objective include an extension to overlapping triangles&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], an extension that combines edge overlapping with prior similarity scores&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>], as well as an extension specific to bipartite graphs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>].     </p>    </section>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> The EigenAlign Algorithm</h3>     </div>     </header>     <p>One of the drawbacks to the previous objective functions is there is no downside to matches that do not produce an overlap, i.e. edges in <em>G<sub>A</sub>     </em> that are mapped to non-edges in <em>G<sub>B</sub>     </em> or vice versa. Neither do these objective functions consider the case where non-edges in <em>G<sub>A</sub>     </em> are mapped to non-edges in <em>G<sub>B</sub>     </em>. The first problem was recognized in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>] which proposed an SDP-based method to minimize the number of conflicting matches. More recently, the EigenAlign objective&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] included explicit terms for these three cases: overlaps, non-informative matches, and conflicts, see Figure&#x00A0;<a class="fig" href="#fig1">1</a>. The alignment score corresponding to <em>P</em> in this case is <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{multline} \text{AlignmentScore}({P}) = \\ s_O (\# \text{ overlaps}) + s_N(\# \text{ non-informatives}) + s_C(\# \text{ conflicts})\end{multline} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <em>s<sub>O</sub>     </em>, <em>s<sub>N</sub>     </em>, and <em>s<sub>C</sub>     </em> are weights for overlaps, non-informatives and conflicts. These constants should be chosen such that <em>s<sub>O</sub>     </em> > <em>s<sub>N</sub>     </em> > <em>s<sub>C</sub>     </em>. By setting <em>s<sub>N</sub>     </em> and <em>s<sub>C</sub>     </em> to zero we recover the ordinary notion of maximizing the number of overlaps. Although it may seem strange to maximize the number of conflicts, when the graphs have very different sizes or numbers of edges, this term acts as regularization. The important piece is that <em>non-informatives</em> are more valuable than conflicts.</p>     <p>This objective can be expressed formally by first introducing a massive <em>alignment matrix M</em> defined as follows: for all pairs of nodes <em>i<sub>A</sub>     </em>, <em>j<sub>A</sub>     </em> in <em>G<sub>A</sub>     </em> and all pairs <span class="inline-equation"><span class="tex">${i^{\prime }_B,j^{\prime }_B}$</span>     </span> in <em>G<sub>B</sub>     </em>, if <span class="inline-equation"><span class="tex">${P}(i_A,i^{\prime }_B) = 1$</span>     </span> and <span class="inline-equation"><span class="tex">${P}(j_A,j^{\prime }_B) = 1$</span>     </span>, then <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {M}[(i_A^{},i^{\prime }_B),(j_A^{},j^{\prime }_B)]= {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}s_O ,&#x0026; \text{if } (i_A^{},j_A^{}), (i^{\prime }_B,j^{\prime }_B) \text{ are overlaps} \\ s_N ,&#x0026; \text{if } (i_A^{},j_A^{}), (i^{\prime }_B,j^{\prime }_B) \text{ are noninformatives} \\ s_C ,&#x0026; \text{if } (i_A^{},j_A^{}), (i^{\prime }_B,j^{\prime }_B) \text{ are conflicts.} \end{array}\right.} \] </span>       <br/>      </div>     </div>     </p>     <p>We are abusing notations a bit in this definition and using pairs <span class="inline-equation"><span class="tex">$i_A^{}$</span>     </span> and <span class="inline-equation"><span class="tex">$i_B^{\prime }$</span>     </span> to index the rows and columns of this matrix. For a straightforward, canonical ordering of these pairs <span class="inline-equation"><span class="tex">$i_A, i_B^{\prime }$</span>     </span>, the matrix <em>M</em> can be rewritten in terms of the adjacency matrices of <em>A</em> and <em>B</em>: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {M}= c_1({B}\otimes {A}) + c_2({E}_B \otimes {A}) + c_2({B}\otimes {E}_A) + c_3({E}_B \otimes {E}_A) \] </span>       <br/>      </div>     </div> where &#x2297; denotes the Kronecker product, <em>c</em>     <sub>1</sub> = <em>s<sub>O</sub>     </em> + <em>s<sub>N</sub>     </em> &#x2212; 2<em>s<sub>C</sub>     </em>, <em>c</em>     <sub>2</sub> = <em>s<sub>C</sub>     </em> &#x2212; <em>s<sub>N</sub>     </em>, <em>c</em>     <sub>3</sub> = <em>s<sub>N</sub>     </em> and <em>E<sub>A</sub>     </em>, and <em>E<sub>B</sub>     </em> are the matrices of all ones and of the same size as <em>A</em> and <em>B</em> respectively. The matrix <em>M</em> is symmetric as long as <em>G<sub>A</sub>     </em> and <em>G<sub>B</sub>     </em> are undirected. (There are directed extensions discussed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>], but we don&#x0027;t consider them here.)</p>     <p>Maximizing the alignment score&#x00A0;(<a class="eqn" href="#eq2">2</a>) is then equivalent to the following quadratic assignment problem: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{array}{ll}\displaystyle \mathop{maximize}_{\mathrm{y}} &#x0026; \mathrm{y}^T {M}\mathrm{y}\\ \text{subject to} &#x0026; y_i \in \lbrace 0, 1 \rbrace \\ &#x0026; \sum _{u} y[u,v] \le 1 \text{ for all } v \in V_B \\ &#x0026; \sum _{v} y[u,v] \le 1 \text{ for all } u \in V_A \end{array} \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <em>V<sub>A</sub>     </em> and <em>V<sub>B</sub>     </em> are the node sets of <em>G<sub>A</sub>     </em> and <em>G<sub>B</sub>     </em> respectively. The vector y is really just the <em>vector of data</em> representing the matching matrix <em>P</em>, and the constraints are just the translation of the matching constraints from&#x00A0;(<a class="eqn" href="#eq1">1</a>).</p>     <p>An empirically and theoretically successful method for optimizing this objective is to solve an eigenvector equation instead of the quadratic program. This is exactly the approach of EigenAlign, which computes network alignments using the following two steps:</p>     <ol class="list-no-style">     <li id="list6" label="(1)">Find the eigenvector x of <em>M</em> that corresponds to the eigenvalue of largest magnitude. Note, <em>M</em> is of dimension <em>n<sub>A</sub>n<sub>B</sub>      </em> &#x00D7; <em>n<sub>A</sub>n<sub>B</sub>      </em>, where <em>n<sub>A</sub>      </em> and <em>n<sub>B</sub>      </em> are the number of nodes in <em>G<sub>A</sub>      </em> and <em>G<sub>B</sub>      </em> respectively; so, the eigenvector will be of dimension <em>n<sub>A</sub>n<sub>B</sub>      </em>, and can be reshaped into an <em>n<sub>A</sub>      </em> &#x00D7; <em>n<sub>B</sub>      </em> matrix <em>X</em> where each entry represents a score for every pair of nodes between the two graphs. <em>We call this a similarity matrix because it reflects the topological similarity between vertices of <em>G<sub>A</sub>       </em> and <em>G<sub>B</sub>       </em>.</em>      <br/></li>     <li id="list7" label="(2)">Run a bipartite matching algorithm on the similarity matrix <em>X</em> that maximizes the total weight of the final alignment.<br/></li>     </ol>     <p>     <em>Our contribution.</em>. In our work we extend the foundation laid by EigenAlign by considering improvements to both steps. We first show that the similarity matrix <em>X</em> can be accurately represented through an exact low-rank factorization. This allows us to avoid the quadratic memory requirement of EigenAlign. We then present several new fast techniques for bipartite matching problems on low-rank matrices. Together these improvements yield a low-rank EigenAlign algorithm that is far more scalable in practice.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Summary of other techniques</h3>     </div>     </header>     <p>Our work shares a number of similarities with the Network Similarity Decomposition (NSD)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>], a technique based on a low-rank factorization of a different similarity matrix, the matrix used by the IsoRank algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. The authors of&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] show that this decomposition can be obtained by performing calculations separately on the two graphs, which significantly speeds up the calculation of similarity scores between nodes. Another procedure designed for aligning networks without prior information is the the Graph Alignment tool (GRAAL)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. GRAAL computes the so-called <em>graphlet degree signature</em> for each node, a vector that generalizes node degree and represents the topological structure of a node&#x0027;s local neighborhood. The method measures distances between graphlet degrees to obtain similarity scores, and then uses a greedy <em>seed and extend</em> procedure for matching nodes across two networks based on the scores. A number of algorithms related to this method have been introduced, which extend the original technique by considering other measures of topological similarity as well as different approaches to rounding similarity scores into an alignment&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>]. The seed-and-extend alignment procedure was also employed by the GHOST algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>], which computes topological similarity scores based on a novel spectral signature for each node. Recently,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] introduced the notion of finding an alignment that maximizes the number of preserved higher order structures (such as triangles) across networks. This results in an integer programming problem that can be approximated by the Triangular Alignment algorithm (TAME), which obtains similarity scores by solving a tensor eigenvalue problem that relaxes the original objective.</p>     <p>Alternative approaches to improve network alignment include active methods that allow users to select matches from a host of potential near equal matches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>].</p>    </section>   </section>   <section id="sec-12">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Low Rank Factors of EigenAlign</h2>     </div>    </header>    <p>The first step of the EigenAlign algorithm is to compute the dominant eigenvector of the symmetric matrix <em>M</em>. Feizi et al.&#x00A0;suggest obtaining a similarity matrix <em>X</em> by first forming <em>M</em>, performing a power iteration on this matrix, and reshaping the final output eigenvector x into <em>X</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. Because of the Kronecker structure in <em>M</em>, this can equivalently be formulated directly as the matrix <em>X</em> that satisfies: <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \!\!\!\!\!\!\begin{array}{ll}{\displaystyle \mathop{maximize}_{{X}}} &#x0026; {X}\bullet (c_1 {A}{X}{B}^T + c_2 {A}{X}{E}^T + c_2 {E}{X}{B}^T + c_3 {E}{X}{E}^T) \\ \text{subject to}&#x0026; {\\ \Vert {X} \Vert }_{F} = 1, {X}\in \mathbb {R}^{n_A \times n_B}. \end{array} \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>     </div> In this expression, <em>X</em>&#x2022;<em>Y</em> = &#x2211;<sub>     <em>ij</em>     </sub>     <em>X<sub>ij</sub>Y<sub>ij</sub>     </em> is the matrix inner-product and the translation from the eigenvector of <em>M</em> follows from the Kronecker product property vec(<em>AXB<sup>T</sup>     </em>) = (<em>B</em> &#x2297; <em>A</em>)vec(<em>X</em>). We also dropped the dimensions from the matrices <em>E</em> of all ones. The eigenvector of <em>M</em> is the result of the <em>vec</em> operation on the matrix <em>X</em>, which converts the matrix into a vector by concatenating columns.</p>    <p>Our first major contribution is to show that if the matrix <em>X</em> is estimated with the power-method starting from a rank 1 matrix, then the <em>k</em>th iteration of the power method results in a rank <em>k</em> + 1 matrix that we can explicitly and exactly compute.</p>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> A Four Factor Low-Rank Decomposition</h3>     </div>     </header>     <p>In the matrix form of problem&#x00A0;(<a class="eqn" href="#eq3">4</a>), one step of the power method corresponds to the iteration: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {X}_{k+1} = c_1 {A}{X}_k {B}^T + c_2 {A}{X}_k {E}^T + c_2 {E}{X}_k {B}^T + c_3 {E}{X}_k {E}^T. \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> If we begin with a rank-1 matrix <em>X</em>     <sub>0</sub> = uv<sup>      <em>T</em>     </sup> where <span class="inline-equation"><span class="tex">$\mathrm{u}\in \mathbb {R}^{n_A}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathrm{v}\in \mathbb {R}^{n_B}$</span>     </span> and let <em>U</em>     <sub>0</sub> = u, and <em>V</em>     <sub>0</sub> = v so that <span class="inline-equation"><span class="tex">${X}_0 = {U}_0^{} {V}_0^T$</span>     </span>. We will first prove by induction that <em>X<sub>k</sub>     </em> can be written as <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} {X}_{k} = {U}_{k}^{} {V}_{k}^T \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{gather*} {U}_{k} = [c_1 {A}{U}_{k-1} \mid c_2 {E}{U}_{k-1} \mid c_2 {A}{U}_{k-1} \mid c_3 {E}{U}_{k-1}]\\ {V}_{k} = [{B}{V}_{k-1} \mid {B}{V}_{k-1} \mid {E}{V}_{k-1} \mid {E}{V}_{k-1}].\end{gather*} </span>       <br/>      </div>     </div> The base case of our induction follows directly from our definition of <em>X</em>     <sub>0</sub>. Assume now that the equivalence between&#x00A0;(<a class="eqn" href="#eq4">5</a>) and&#x00A0;(<a class="eqn" href="#eq5">6</a>) holds up to <em>k</em> and we will prove the equivalence for <em>k</em> + 1. We begin with equation&#x00A0;(<a class="eqn" href="#eq4">5</a>) and plug in the decomposition of <em>X<sub>k</sub>     </em> from&#x00A0;(<a class="eqn" href="#eq5">6</a>): <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; {X}_{k+1} \\ &#x0026;\quad = c_1 {A}{X}_{k} {B}^T + c_2 {A}{X}_{k} {E}^T + c_2 {E}{X}_{k} {B}^T + c_3 {E}{X}_{k} {E}^T\\ &#x0026;\quad = c_1 {A}{U}_{k} {V}_k^T {B}^T + c_2 {A}{U}_{k} {V}_k^T {E}^T + c_2 {E}{U}_{k} {V}_k^T {B}^T + c_3 {E}{U}_{k} {V}_k^T {E}^T\\ &#x0026;\quad = [c_1 {A}{U}_{k} \mid c_2 {E}{U}_{k} \mid c_2 {A}{U}_{k} \mid c_3 {E}{U}_{k}] [{B}{V}_{k} \mid {B}{V}_{k} \mid {E}{V}_{k} \mid {E}{V}_{k}]^T\\ &#x0026;\quad = {U}_{k+1} {V}_{k+1}^T.\end{align*} </span>       <br/>      </div>     </div>     </p>     <p>This form of the factorization is not yet helpful, because the matrix <em>U<sub>k</sub>     </em> is of dimension <em>n<sub>A</sub>     </em> &#x00D7; 4<sup>      <em>k</em>     </sup>. To show that this is indeed a rank <em>k</em> + 1 matrix, we show <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {X}_k^{} = {S}_k^{} {C}_k^{} {T}_k^T {R}_k^T \] </span>       <br/>      </div>     </div> where: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {S}_k = [ {A}^k \mathrm{u}\mid {A}^{k-1} \mathrm{e}\mid \dots \mid {A}\mathrm{e}\mid \mathrm{e}] \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {R}_k = [{B}^k \mathrm{v}\mid {B}^{k-1} \mathrm{e}\mid \dots \mid {B}\mathrm{e}\mid \mathrm{e}] \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {C}_k = {\left[\begin{array}{*10c}c_1 {C}_{k-1} &#x0026; {0}&#x0026; c_2 {C}_{k-1} &#x0026; {0}\\ {0}^T &#x0026; c_2\mathrm{r}_k^T {C}_{k-1} &#x0026; {0}^T &#x0026; c_3\mathrm{r}_k^T {C}_{k-1} \end{array}\right]} \] </span>       <br/>      </div>     </div>     <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {T}_{k} = {\left[\begin{array}{*10c}{T}_{k-1} &#x0026;{T}_{k-1} &#x0026; {0}&#x0026;{0}\\ {0}^T &#x0026; {0}^T &#x0026; \mathrm{h}_k^T {T}_{k-1} &#x0026; \mathrm{h}_k^T {T}_{k-1} \end{array}\right]}. \] </span>       <br/>      </div>     </div> In the above, 0 is the all zeros matrix or vector of appropriate size, and e is the all ones vector. Also, <em>C</em>     <sub>0</sub> = <em>T</em>     <sub>0</sub> = 1, and r<sub>      <em>k</em>     </sub> and h<sub>      <em>k</em>     </sub> are defined as follows: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \begin{aligned} \mathrm{r}_k &#x0026; = [ \begin{array}{ccccc}\mathrm{e}^T {A}^{k-1} \mathrm{u}&#x0026; \mathrm{e}^T {A}^{k-2} \mathrm{e}&#x0026; \cdots &#x0026; \mathrm{e}^T {A}^1 \mathrm{e}&#x0026; \mathrm{e}^T {A}^0 \mathrm{e}\end{array}]^T \\\mathrm{h}_k &#x0026; = [ \begin{array}{cccccc}\mathrm{e}^T {B}^{k-1} \mathrm{v}&#x0026; \mathrm{e}^T {B}^{k-2} \mathrm{e}&#x0026; \cdots &#x0026; \mathrm{e}^T {B}^1 \mathrm{e}&#x0026; \mathrm{e}^T {B}^0 \mathrm{e}\end{array}]^T \end{aligned} \] </span>       <br/>      </div>     </div> with r<sub>1</sub> = [e<sup>      <em>T</em>     </sup>     <em>A</em>     <sup>0</sup>u] and h<sub>1</sub> = [e<sup>      <em>T</em>     </sup>     <em>B</em>     <sup>0</sup>v]. Note that this form gives the rank <em>k</em> + 1 decomposition we desire because <em>S<sub>k</sub>     </em> and <em>R<sub>k</sub>     </em> both have <em>k</em> + 1 columns.</p>     <p>To complete our derivation, we show <em>U<sub>k</sub>     </em> = <em>S<sub>k</sub>C<sub>k</sub>     </em> again using induction. The base case <em>k</em> = 0 is immediate from a simple expansion of the initial definitions, so assume that the result holds for up to integer <em>k</em>. Then, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} {U}_{k+1} &#x0026;= [c_1 {A}{U}_k \mid c_2 {E}{U}_k \mid c_2 {A}{U}_k \mid c_3 {E}{U}_k] \\ &#x0026;= [{A}{U}_{k} \mid {E}{U}_k] \left[{{\begin{array}{*10c}c_1 {I}&#x0026; {0}&#x0026; c_2 {I}&#x0026; {0}\\ {0}&#x0026; c_2 {I}&#x0026; {0}&#x0026; c_3 {I} \end{array}}}\right]\\ &#x0026;= [{A}{S}_{k}{C}_k \mid {E}{S}_{k}{C}_k] \left[{{\begin{array}{*10c}c_1 {I}&#x0026; {0}&#x0026; c_2 {I}&#x0026; {0}\\ {0}&#x0026; c_2 {I}&#x0026; {0}&#x0026; c_3 {I} \end{array}}}\right].\end{align*} </span>       <br/>      </div>     </div> Now, note that <span class="inline-equation"><span class="tex">${A}{S}_k = {S}_{k+1} \left[{{\begin{array}{*10c}{I}\\ {0}^T \end{array}}}\right]$</span>     </span> and <span class="inline-equation"><span class="tex">${E}{S}_k = {S}_{k+1} \left[{{\begin{array}{*10c}{0}\\\mathrm{r}_{k+1}^T \end{array}}}\right]$</span>     </span>. Thus <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {U}_{k+1} = {S}_{k+1} \left[{{\begin{array}{*10c}{I}&#x0026; {0}\\ {0}^T &#x0026; \mathrm{r}_{k+1}^T \end{array}}}\right] \left[{{\begin{array}{*10c}{C}_{k} &#x0026; {0}\\ {0}&#x0026; {C}_{k} \end{array}}}\right] \left[{{\begin{array}{*10c}c_1 {I}&#x0026; {0}&#x0026; c_2 {I}&#x0026; {0}\\ {0}&#x0026; c_2 {I}&#x0026; {0}&#x0026; c_3 {I} \end{array}}}\right] = {S}_{k+1} {C}_{k+1} \] </span>       <br/>      </div>     </div> Applying the same set of steps again will yield that <em>V<sub>k</sub>     </em> = <em>R<sub>k</sub>T<sub>k</sub>     </em>.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Three and Two Factor Decompositions</h3>     </div>     </header>     <p>While this four factor decomposition is useful for revealing the rank of <em>X<sub>k</sub>     </em>, we do not wish to work with matrices <em>C<sub>k</sub>     </em> and <em>T<sub>k</sub>     </em> in practice since each has 4<sup>      <em>k</em>     </sup> columns. We now show that their product <span class="inline-equation"><span class="tex">${C}_k {T}_k^T$</span>     </span> yields a simple-to-compute matrix <em>W<sub>k</sub>     </em> of size (<em>k</em> + 1) &#x00D7; (<em>k</em> + 1), giving us a three-factor decomposition (3FD): <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {X}_k = {S}_k {W}_k {R}_k. \] </span>       <br/>      </div>     </div> The matrix <em>W<sub>k</sub>     </em> is defined iteratively by: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ {W}_{k} = {C}_k {T}_k^T = {\left[\begin{array}{*10c}c_1 {W}_{k-1} &#x0026; c_2 {W}_{k-1} \mathrm{h}_k \\ c_2 \mathrm{r}_k^T {W}_{k-1} &#x0026; c_3 \mathrm{r}_k^T {W}_{k-1}\mathrm{h}_k \end{array}\right]}. \] </span>       <br/>      </div>     </div> with <span class="inline-equation"><span class="tex">${W}_{0} = {C}_{0}{T}_0^T = 1\cdot 1 = 1$</span>     </span>. This follows from multiplying <em>C<sub>k</sub>     </em> with <span class="inline-equation"><span class="tex">${T}_k^T$</span>     </span> together.</p>     <p>This decomposition is a step closer to our final goal but suffers from poor scaling of numbers in the factors. Consequently, we can remedy this by using scaling diagonal matrices in order to present our final well-scaled three factor decomposition of <em>X<sub>k</sub>     </em>, which we present as a summarizing theorem:</p>     <div class="theorem" id="enc1">     <Label>Theorem 3.1.</Label>     <p> If &#x2009;<em>X</em>      <sub>0</sub> = uv<sup>       <em>T</em>      </sup> for vectors <span class="inline-equation"><span class="tex">$\mathrm{u}\in \mathbb {R}^{n_A \times 1}$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathrm{v}\in \mathbb {R}^{n_B \times 1}$</span>      </span>, then the <em>k</em>th iteration of update&#x00A0;(<a class="eqn" href="#eq4">5</a>) permits the following low-rank factorization: <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ {X}_k^{} = \tilde{{U}}_k^{} \tilde{{W}}_k^{} \tilde{{V}}_k^T \] </span>        <br/>       </div>      </div> where <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \tilde{{U}}_k = {\left[\begin{array}{*10c}\frac{{A}^k \mathrm{u}}{ {}_{\phantom{\infty }}{\mathrm\Vert{{A}^k \mathrm{u}}\Vert}_{\infty } } &#x0026; \frac{{A}^{k-1} \mathrm{e}}{ {}_{\phantom{\infty }}{\mathrm\Vert{{A}^{k-1} \mathrm{e}}\Vert}_{\infty } } &#x0026; \dots &#x0026; \frac{{A}\mathrm{e}}{ {}_{\phantom{\infty }}{\mathrm\Vert{{A}\mathrm{e}}\Vert}_{\infty } } &#x0026; \frac{\mathrm{e}}{ {}_{\phantom{\infty }}{\mathrm{\mathrm\Vert{e}}\Vert}_{\infty } } \end{array}\right]} \] </span>        <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \tilde{{V}}_k = {\left[\begin{array}{*10c} \frac{{B}^k \mathrm{u}}{ {}_{\phantom{\infty }}{\mathrm\Vert{{B}^k \mathrm{u}}\Vert}_{\infty } } &#x0026; \frac{{B}^{k-1} \mathrm{e}}{ {}_{\phantom{\infty }}{\mathrm\Vert{{B}^{k-1} \mathrm{e}}\Vert}_{\infty } } &#x0026; \dots &#x0026; \frac{{B}\mathrm{e}}{ {}_{\phantom{\infty }}{\mathrm\Vert{{B}\mathrm{e}}\Vert}_{\infty } } &#x0026; \frac{\mathrm{e}}{ {}_{\phantom{\infty }}{\mathrm{\mathrm\Vert{e}}\Vert}_{\infty } } \end{array}\right]} \] </span>        <br/>       </div>      </div>      <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \tilde{{W}}_k = {D}_u {W}_k {D}_v. \] </span>        <br/>       </div>      </div> Here <em>D<sub>u</sub>      </em> is a (<em>k</em> + 1) &#x00D7; (<em>k</em> + 1) diagonal matrix with diagonal entries <span class="inline-equation"><span class="tex">$(\Vert {A}^k \mathrm{u}\Vert , \Vert {A}^{k-1} \mathrm{e}\Vert , \dots , \Vert {{A}\mathrm{e}}\Vert ,\Vert {\mathrm{e}}\Vert)$</span>      </span> and <em>D<sub>v</sub>      </em> is a diagonal matrix with entries <span class="inline-equation"><span class="tex">$(\Vert {B}^k \mathrm{v}\Vert , \Vert {B}^{k-1} \mathrm{e}\Vert , \dots , \Vert {{B}\mathrm{e}}\Vert ,\Vert {\mathrm{e}}\Vert)$</span>      </span>.</p>     </div>     <p>The diagonal matrices in Theorem (<a class="enc" href="#enc1">3.1</a>) are designed specifically to satisfy <span class="inline-equation"><span class="tex">${S}_k^{} {D}_u^{-1} = \tilde{{U}}_k^{}$</span>     </span>, <span class="inline-equation"><span class="tex">${R}_k {D}_v^{-1} = \tilde{{V}}_k$</span>     </span>, so the equivalence between the scaled and unscaled three factor decompositions is straightforward. Note that the result is still unnormalized. However, we can easily normalize in practice by scaling the matrix <span class="inline-equation"><span class="tex">$\tilde{{W}}_k$</span>     </span> as we see fit.</p>     <p>Note that when computing this decomposition in practice, we do not simply construct <em>S</em>, <em>R</em>, and <em>W</em> and then scale with <em>D<sub>u</sub>     </em> and <em>D<sub>v</sub>     </em>. Instead, we form the scaled factors recursively by noting the similarities between each factor at step <em>k</em> and the corresponding factor at step <em>k</em> + 1. A pseudo-code for our implementation that directly computes these is shown in Figure&#x00A0;<a class="fig" href="#fig2">2</a>. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">The pseudocode of the algorithm to decompose <em>X</em> into two low-rank matrices. Note that &#x25CB; refers to the element-wise Hadamard product between two vectors.</span>      </div>     </figure>     </p>     <p>As we shall see in the next section, we would ultimately like to express <em>X<sub>k</sub>     </em> in terms of a just a left and a right low-rank factor in order to apply our techniques for low-rank bipartite matching. It is preferable for our purposes to produce two factors that have roughly equal scaling, so we accomplish this by factorizing <span class="inline-equation"><span class="tex">$\tilde{{W}}_k$</span>     </span> using an SVD decomposition and splitting the pieces of <span class="inline-equation"><span class="tex">$\tilde{{W}}_k$</span>     </span> into the left and right terms. The last steps of the Figure&#x00A0;<a class="fig" href="#fig2">2</a> accomplish this goal.</p>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Low Rank Matching</h2>     </div>    </header>    <p>In this section, we consider the problem of solving a maximum weight bipartite matching problem on a low rank matrix with a useful a-posteriori approximation guarantee. In our network alignment routine, our algorithm will be used on the low-rank matrix from Figure&#x00A0;<a class="fig" href="#fig2">2</a>. In this section, however, we proceed in terms of a general matrix <em>Y</em> with low rank factors <em>Y</em> = <em>UV<sup>T</sup>     </em>. The matrix <em>Y</em> represents the edge-weights of a bipartite graph, and so the max-weight matching problem is: <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{array}{ll}{\displaystyle \mathop{maximize}_{{M}}} &#x0026; {M}\bullet {Y} \\ \text{subject to}&#x0026; M_{i,j} \in \lbrace 0, 1\rbrace \\ &#x0026; \sum _{i} M_{i,j} \le 1 \text{ for all $j$}, \sum _{ij} M_{i,j} \le 1 \text{ for all $i$}, \end{array} \end{equation} </span>      <br/>      <span class="equation-number">(7)</span>     </div>     </div> where &#x2022; is the matrix inner-product (see&#x00A0;(<a class="eqn" href="#eq3">4</a>)). The <em>M</em>     <sub>     <em>i</em>, <em>j</em>     </sub> entries represent a match between node <em>i</em> on one side of the bipartite graph and node <em>j</em> on the other side. We call any <em>M</em> that satisfies the matching constraints a matching matrix.</p>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Optimal Matching on a Rank 1 Matrix</h3>     </div>     </header>     <p>We begin by considering optimal matchings for a rank-1 matrix <em>Y</em> = uv<sup>      <em>T</em>     </sup> where <span class="inline-equation"><span class="tex">$\mathrm{u}, \mathrm{v}\in \mathbb {R}^n$</span>     </span> (these results are easily adapted for vectors of different lengths).</p>     <p>     <em>Case 1: </em>     <span class="inline-equation"><span class="tex">$\mathrm{u}, \mathrm{v}\in \mathbb {R}^n_{\ge 0}$</span>     </span>     <em> or </em>     <span class="inline-equation"><span class="tex">$\mathrm{u}, \mathrm{v}\in \mathbb {R}^n_{\le 0}$</span>     </span> . If u and v contain only non-negative entries, <em>or</em> both contain only non-positive entries, the procedure for finding the optimal matching is the same: we order the entries of both vectors by magnitude and pair up elements as they appear in the sorted list. If any pair contributes a 0 weight, we do not bother to match that pair since it doesn&#x0027;t improve the overall matching score. The optimality of this matching for these special cases can be seen as a direct result of the rearrangement inequality.</p>     <p>     <em>Case 2: General </em>     <span class="inline-equation"><span class="tex">$\mathrm{u}, \mathrm{v}\in \mathbb {R}^n$</span>     </span>. If u and v have entries that can be positive, negative, or zero, we require a slightly more sophisticated method for finding the optimal matching on <em>Y</em>. In this case, define <span class="inline-equation"><span class="tex">$\tilde{{Y}}$</span>     </span> to be the matrix obtained by copying <em>Y</em> and deleting all negative entries. To find the optimal matching of <em>Y</em> we would never pair elements giving a negative weight, so the optimal matching for <span class="inline-equation"><span class="tex">$\tilde{{Y}}$</span>     </span> is the same as for <em>Y</em>. Now let u<sub>+</sub> and u<sub>&#x2212;</sub> be the vectors that contain the strictly positive and negative elements in u respectively, and define v<sub>+</sub>, and v<sub>&#x2212;</sub> similarly for v. Then, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \tilde{{Y}} = \tilde{{Y}}_1 + \tilde{{Y}}_2 \] </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$ \tilde{{Y}}_1 = \mathrm{u}_{+} \mathrm{v}_{+}^T$</span>     </span> and <span class="inline-equation"><span class="tex">$\tilde{{Y}}_2 = \mathrm{u}_{-} \mathrm{v}_{-}^T$</span>     </span>. Let <em>M</em>     <sub>1</sub> and <em>M</em>     <sub>2</sub> be the optimal matching matrices for <span class="inline-equation"><span class="tex">$\tilde{{Y}}_1$</span>     </span> and <span class="inline-equation"><span class="tex">$\tilde{{Y}}_2$</span>     </span> respectively, obtained using the sorting techniques for case 1. Since u<sub>+</sub>, u<sub>&#x2212;</sub>, v<sub>+</sub> and v<sub>&#x2212;</sub> will contain some entries that are zero, both <em>M</em>     <sub>1</sub> and <em>M</em>     <sub>2</sub> may leave certain nodes unmatched. The following lemma shows that combining these matchings yields the optimal result for <span class="inline-equation"><span class="tex">$\tilde{{Y}}$</span>     </span>:</p>     <div class="lemma" id="enc2">     <Label>Lemma 4.1.</Label>     <p> The set of nodes matched by <em>M</em>      <sub>1</sub> will be disjoint from the set of nodes matched by <em>M</em>      <sub>2</sub>. The matching <span class="inline-equation"><span class="tex">$\tilde{{M}}$</span>      </span> defined by combining these two matchings will be optimal for <em>Y</em>.</p>     </div>     <div class="proof" id="proof1">     <Label>Proof.</Label>     <p> We will prove by contradiction that there are no conflicts between <em>M</em>      <sub>1</sub> and <em>M</em>      <sub>2</sub>. Assume that <em>M</em>      <sub>1</sub> contains the match (<em>i</em>, <em>j</em>) and <em>M</em>      <sub>2</sub> contains a conflicting match (<em>i</em>, <em>k</em>). Since <em>M</em>      <sub>1</sub> contains the match (<em>i</em>, <em>j</em>), <span class="inline-equation"><span class="tex">$\tilde{{Y}}_1(i,j)$</span>      </span> must be nonzero, implying that u<sub>+</sub>(<em>i</em>) and v<sub>+</sub>(<em>j</em>) are both positive. Similarly, <em>M</em>      <sub>2</sub> contains the pair (<em>i</em>, <em>k</em>), so u<sub>&#x2212;</sub>(<em>i</em>) and v<sub>&#x2212;</sub>(<em>k</em>) are both negative. This is a contradiction, since at least one of u<sub>+</sub>(<em>i</em>) and u<sub>&#x2212;</sub>(<em>i</em>) must be zero.</p>     <p>We just need to show that <span class="inline-equation"><span class="tex">$\tilde{M}$</span>      </span> is an optimal matching for <em>Y</em>. If this were not the case, there would exist some matching <em>M</em> such that <span class="inline-equation"><span class="tex">${M}\bullet \tilde{{Y}} {\gt} \tilde{{M}} \bullet \tilde{{Y}}$</span>      </span>. If such an <em>M</em> existed, we would have that <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ {M}\bullet \tilde{{Y}}_1 + {M}\bullet \tilde{{Y}}_2 {\gt} \tilde{{M}} \bullet \tilde{{Y}}_1 + \tilde{{M}} \bullet \tilde{{Y}}_2 \] </span>        <br/>       </div>      </div> However, <span class="inline-equation"><span class="tex">$\tilde{{M}} \bullet \tilde{{Y}}_1 = {M}_1 \bullet \tilde{{Y}}_1 \ge {M}\bullet \tilde{{Y}_1}$</span>      </span>, and <span class="inline-equation"><span class="tex">$\tilde{{M}} \bullet \tilde{{Y}}_2 = {M}_2 \bullet \tilde{{Y}}_2 \ge {M}\bullet \tilde{{Y}}_2$</span>      </span>. Thus, a contradiction; <span class="inline-equation"><span class="tex">$\tilde{{M}}$</span>      </span> is an optimal matching of <span class="inline-equation"><span class="tex">$\tilde{{Y}}$</span>      </span>. &#x25A1;</p>     </div>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Matchings on Low Rank Factors</h3>     </div>     </header>     <p>Now we address the problem of finding a good matching for a matrix <em>Y</em> = <em>UV<sup>T</sup>     </em>, where <span class="inline-equation"><span class="tex">${Y}\in \mathbb {R}^{m\times n}$</span>     </span>, <span class="inline-equation"><span class="tex">${U}\in \mathbb {R}^{m\times k}$</span>     </span>, and <span class="inline-equation"><span class="tex">${V}\in \mathbb {R}^{n\times k}$</span>     </span>. Let u<sub>      <em>i</em>     </sub> and v<sub>      <em>i</em>     </sub> be the <em>i</em>th columns in <em>U</em> and <em>V</em>, and let <span class="inline-equation"><span class="tex">${Y}_i^{} = \mathrm{u}_i^{} \mathrm{v}_i^T$</span>     </span>, then <span class="inline-equation"><span class="tex">${Y}= \sum _{i=1}^{k} {Y}_i$</span>     </span>.</p>     <p>We can find the optimal matching on each <em>Y<sub>i</sub>     </em> using the results from Section&#x00A0;<a class="sec" href="#sec-16">4.1</a>. Let <em>M<sub>i</sub>     </em> be the matching matrix corresponding to <em>Y<sub>i</sub>     </em>, and let <em>M</em>     <sup>*</sup> be a matching matrix that achieves an optimal maximum weight on <em>Y</em>. Note that <em>M</em>     <sup>*</sup>&#x2022;<em>Y<sub>i</sub>     </em> &#x2264; <em>M<sub>i</sub>     </em>&#x2022;<em>Y<sub>i</sub>     </em>, and thus, <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \textstyle {M}^* \bullet {Y}\le \sum _{i =1}^{k} {M}_i \bullet {Y}_i. \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> To analyze how good of a matching each <em>M<sub>i</sub>     </em> is on the entire matrix <em>Y</em>, define the following terms: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \textstyle d_{i,j} = \frac{{M}_i \bullet {Y}_i}{{M}_j \bullet {Y}_i} \quad d_{j} = \max \limits _{i} d_{i,j} \quad D = \min \limits _{j} d_{j} \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> and let <span class="inline-equation"><span class="tex">$j^* = \operatorname{argmin}\limits _{j} d_j$</span>     </span>, i.e. <span class="inline-equation"><span class="tex">$D = d_{j^*}$</span>     </span>. Note that for any fixed indices <em>i</em>, <em>j</em>, we have <em>d</em>     <sub>      <em>i</em>, <em>j</em>     </sub> &#x2264; <em>d<sub>j</sub>     </em>. Applying this to <em>j</em> = <em>j</em>     <sup>*</sup> we have that for all <em>i</em>, <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \textstyle \frac{{M}_{i} \bullet {Y}_{i}}{{M}_{j^*} \bullet {Y}_{i}} = d_{i,j^*} \le d_{j^*} = D \Rightarrow {{M}_{i} \bullet {Y}_{i}} \le D ({M}_{j^*} \bullet {Y}_{i}) \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> By combining&#x00A0;(<a class="eqn" href="#eq6">8</a>) and&#x00A0;(<a class="eqn" href="#eq8">10</a>) we have the following result.</p>     <div class="theorem" id="enc3">     <Label>Theorem 4.2.</Label>     <p> We can achieve a <em>D</em>-approximation for the bipartite matching problem by selecting an optimal matching for one of the low-rank factors of <em>Y</em>.</p>     </div>     <div class="proof" id="proof2">     <Label>Proof.</Label>     <p>      <span class="inline-equation"><span class="tex">${M}^* \hspace{-1.66656pt}\bullet \hspace{-1.66656pt}{Y}\le \sum _{i =1}^{k} {M}_i \hspace{-1.66656pt}\bullet \hspace{-1.66656pt}{Y}_i \le \sum _{i =1}^{k} D {M}_{j^*} \hspace{-1.66656pt}\bullet \hspace{-1.66656pt}{Y}_i = D ({M}_{j^*} \hspace{-1.66656pt}\bullet \hspace{-1.66656pt}{Y}).$</span>      </span>      &#x25A1;</p>     </div>     <p>This procedure (Figure&#x00A0;<a class="fig" href="#fig3">3</a>) runs in <span class="inline-equation"><span class="tex">$\mathcal {O}(k^2n + k n \log n)$</span>     </span> where <em>k</em> is the rank, and <em>U</em> and <em>V</em> have <em>O</em>(<em>n</em>) rows. The space requirement is <span class="inline-equation"><span class="tex">$\mathcal {O}(nk)$</span>     </span>. In practice, the approximation factors <em>D</em> are less than 1.1 for our problems (see Figure&#x00A0;<a class="fig" href="#fig7">7</a>). Figure&#x00A0;<a class="fig" href="#fig3">3</a> shows pseudocode to implement this matching algorithm. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Pseudocode for finding a <em>D</em>-approximate matching from a low rank matrix.</span>      </div>     </figure>     </p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Improved practical variations</h3>     </div>     </header>     <p>Our method (Figure&#x00A0;<a class="fig" href="#fig3">3</a>) can be improved without substantially changing its runtime or memory requirement. The key idea is to create a sparse max-weight bipartite matching problem that include the matching <span class="inline-equation"><span class="tex">${M}_{j^*}$</span>     </span> and other helpful edges. By optimally solving these, we will only improve the approximation. These incur the cost of solving those problems optimally, but sparse max-weight matching solvers are practical and fast for problems with millions of edges.</p>     <p>     <strong>Union of matchings.</strong> The simplest improvement is to create a sparse graph based on the full set of matches <em>M</em>     <sub>1</sub>, &#x2026;, <em>M<sub>k</sub>     </em>. We can do this by transforming the complete bipartite network defined by <em>Y</em> into a sparsified network <span class="inline-equation"><span class="tex">$\hat{{Y}}$</span>     </span> where edge (<em>j</em>, <em>k</em>) is nonzero with weight <em>Y</em>     <sub>      <em>j</em>, <em>k</em>     </sub> only if nodes (<em>j</em>, <em>k</em>) were matched by some <em>M<sub>i</sub>     </em>. Then, we solve a maximum bipartite matching problem on the sparse matrix <span class="inline-equation"><span class="tex">$\hat{{Y}}$</span>     </span> with <em>O</em>(<em>nk</em>) non-zeros or edges. This only improves the approximation because we included the matching <span class="inline-equation"><span class="tex">${M}_{j^*}$</span>     </span>.</p>     <p>     <strong>Expanding non-matchings on rank-1 factors.</strong> Since algorithm&#x00A0;<a class="fig" href="#fig3">3</a> relies on a sorting procedure when building <em>M<sub>i</sub>     </em> from the rank-1 factors, and since these numbers may very likely be close to each other, we can choose to expand the set of possible matchings and let each node pair up with <em>c</em> closest values to it. By way of example, if <em>c</em> = 3 and we had sorted indices <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \begin{array}{lccccc}\text{sorted } \mathrm{u}: &#x0026; i_1 &#x0026; i_2 &#x0026; i_3 &#x0026; i_4 &#x0026; i_5 \\ \text{sorted } \mathrm{v}: &#x0026;j_1 &#x0026; j_2 &#x0026; j_3 &#x0026; j_4 &#x0026; j_5 \end{array} \text{ } \begin{array}{l}\text{then we} \\ \text{add edges} \end{array} \text{ } \begin{array}{ccc}(i_1, j_1) &#x0026; (i_1, j_2)&#x0026; \\ (i_2, j_1)&#x0026; (i_2, j_2)&#x0026; (i_2, j_3) \\ (i_3, j_2) &#x0026; \ldots \end{array} \] </span>       <br/>      </div>     </div> We add all these edges to the sparse matrix <span class="inline-equation"><span class="tex">$\hat{{Y}}$</span>     </span> with their true values from <em>Y</em> and solve a maximum bipartite matching problem on the resulting matrix. Again, this includes all edges from <span class="inline-equation"><span class="tex">${M}_{j^*}$</span>     </span>. After adding all the edges from set u<sub>      <em>i</em>     </sub>, v<sub>      <em>i</em>     </sub>, the final number of edges is <em>O</em>(<em>kcn</em>), and thus, the resulting union of matchings matrix is a sparse matrix when <em>kc</em> is <em>o</em>(<em>n</em>).</p>    </section>   </section>   <section id="sec-19">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>     </div>    </header>    <p>To evaluate our method, we first study the relationship between Low Rank EigenAlign and the original EigenAlign algorithm. The goal of these initial experiments is to show (i) that we need about 8 iterations, which gives a rank 9 matrix, to get equivalent results to EigenAlign (Figure&#x00A0;<a class="fig" href="#fig4">4</a>), (ii) our method performs the same over a variety of graph models (Figure&#x00A0;<a class="fig" href="#fig5">5</a>), (iii) the method scales better (Figure&#x00A0;6), and (iv) the computed approximation bounds are better than 1.1 (Figure&#x00A0;<a class="fig" href="#fig7">7</a>). We also compare against other scalable techniques in Figure&#x00A0;<a class="fig" href="#fig8">8</a>, and see that our approach is the best. Next, we use a test-set of networks with known alignments from biology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] to evaluate our algorithms (Section&#x00A0;<a class="sec" href="#sec-21">5.2</a>). Finally, we end our experiments with a study on a collaboration network where we seek to align vertex neighborhoods (Section&#x00A0;<a class="sec" href="#sec-22">5.3</a>).</p>    <p>     <strong>Our low-rank EigenAlign</strong> In all of these experiments, our low-rank techniques use the expanded matching with <em>c</em> = 3 (Section&#x00A0;<a class="sec" href="#sec-18">4.3</a>) and set the initial rank-1 factors to be all uniform: v = e, u = e. Let <span class="inline-equation"><span class="tex">$\alpha = 1 + \frac{\text{nnz}(A) \text{nnz}(B)}{ \text{nnz}(A)(n_B^2 - \text{nnz}(B)) + \text{nnz}(B)(n_A^2 - \text{nnz}(A))}$</span>     </span>. This equals one plus the ratio of possible overlaps divided by possible conflicts. Let <em>&#x03B3;</em> = 0.001, then <em>s<sub>O</sub>     </em> = <em>&#x03B1;</em> + <em>&#x03B3;</em>, <em>s<sub>N</sub>     </em> = 1 + <em>&#x03B3;</em>, <em>s<sub>C</sub>     </em> = <em>&#x03B3;</em>. These parameters correspond to those used in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] as well. Finally, we set the number of iterations to be 8 for all experiments except those where we explicitly vary the number of iterations. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">At left, the number of overlapped edges in the alignment computed by our low-rank method relative to EigenAlign&#x0027;s alignment. A value of 1.0 means that we get the same number as EigenAlign&#x0027;s solution. At right, the same ratio but with respect to the recovery. The recovery results stop improving after around 8 iterations, so we fix this value in the rest of our experiments.</span>      </div>     </figure> <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Thick lines are the median recovery fractions over 200 trials and dashed lines are the 20th and 80th percentiles. These figures show that there appears to be small and likely insignificant differences between the alignment quality of EigenAlign and our low rank method.</span>      </div>     </figure> 				 </p>    <p>     <strong>Theoretical runtime.</strong> When we combine our low-rank computation and the subsequent expanded low-rank matching, the runtime of our method is <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ O(\underbrace{nk^2}_{\begin{array}{c}\scriptstyle \text{low-rank factors} \\ \scriptstyle \text{compute $d_{ij}$} \end{array}} + \underbrace{k^3}_{\text{SVD}} + \underbrace{kn\log n}_{\text{sorting}} + \text{matching with $ckn$ edges}) \] </span>      <br/>     </div>     </div> and <em>O</em>(<em>nck</em>) memory. (Note that <em>k</em> = 8 and <em>c</em> = 3 in our experiments.)</p>    <p>     <strong>EigenAlign baseline.</strong> For EigenAlign, we use the same set of parameters <em>s<sub>O</sub>     </em>, <em>s<sub>N</sub>     </em>, <em>s<sub>C</sub>     </em> and use the power method with starting with the all ones vector. We run the power method with normalization as described in&#x00A0;(<a class="eqn" href="#eq4">5</a>) until we reach an eigenvalue-eigenvector pair that achieves a residual value 10<sup>&#x2212; 12</sup>. This usually occurs after a 15-20 iterations.</p>    <section id="sec-20">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Erd&#x0151;s-R&#x00E9;nyi and preferential attachment</h3>     </div>     </header>     <p>The goal of our first experiment is to assess the performance of our method compared to EigenAlign. These experiments are all done with respect to synthetic problems with a known alignment between the graphs. The metric we use to assess the performance is <em>recovery</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>], where we want <em>large recovery values</em>. Recovery is between 0 and 1 and is defined <div class="table-responsive" id="Xeq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \text{recovery}({M}) = 1 - \tfrac{1}{2n} \Vert {M}- {M}_{\text{true}} \Vert _F. \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div> In words, recovery is the fraction of correct alignments.</p>     <p>     <strong>Graph models.</strong> To generate the starting undirected network in the problem (<em>G<sub>A</sub>     </em>), we use either Erd&#x0151;s-R&#x00E9;nyi with average degree <em>&#x03C1;</em> (where the edge probability is <em>&#x03C1;</em>/<em>n</em>) or preferential attachment with a random 6-node initial graph and adding <em>&#x03B8;</em> edges with each vertex.</p>     <p>     <strong>Noise Model.</strong> Given a network <em>G<sub>A</sub>     </em>, we add some noise to generate our second network <em>G<sub>B</sub>     </em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]. With probability <span class="inline-equation"><span class="tex">$p_{e_1}$</span>     </span>, we remove an edge, and with probability <span class="inline-equation"><span class="tex">$p_{e_2}$</span>     </span> we add an edge. Then, algebraically, <em>B</em> can be written as <em>A</em>&#x25CB;(1 &#x2212; <em>Q</em>     <sub>1</sub>) + (1 &#x2212; <em>A</em>)&#x25CB;<em>Q</em>     <sub>2</sub>, where <em>Q</em>     <sub>1</sub> and <em>Q</em>     <sub>2</sub> are undirected Erd&#x0151;s-R&#x00E9;nyi graphs with density <span class="inline-equation"><span class="tex">$p_{e_1}$</span>     </span> and <span class="inline-equation"><span class="tex">$p_{e_2}$</span>     </span> respectively and &#x25CB; is the Hadamard (element-wise) product. We fix <span class="inline-equation"><span class="tex">$p_{e_2} = pp_{e_1}/(1-p)$</span>     </span> where <em>p</em> is the density of <em>G<sub>A</sub>     </em>. Because some algorithms have a bias in the presence of multiple possible solutions, after <em>B</em> is generated, we relabel the nodes in <em>B</em> in reverse order.</p>     <p>     <strong>Eight iterations are enough.</strong> We first study the change in results with the number of iterations. We use Erd&#x0151;s-R&#x00E9;nyi graphs with average degree 20 and analyze the performance of our method as iterations vary. Figure&#x00A0;<a class="fig" href="#fig4">4</a> shows the recovery (left) and overlap (right) relative to the EigenAlign result so a value of 1.0 means the same number as EigenAlign. After 8 iterations, the recovery stops increasing, and so we perform the rest of our experiments with only 8 iterations.     </p>     <p>     <strong>Our Low-rank EigenAlign matches EigenAlign for Erd&#x0151;s-R&#x00E9;nyi and preferential attachment.</strong> We next test a variety of graphs as the noise level <span class="inline-equation"><span class="tex">$p_{e_1}$</span>     </span> varies. For these experiments, we create Erd&#x0151;s-R&#x00E9;nyi graphs with average degree 5 and 20 and preferential attachment graphs with <em>&#x03B8;</em> = 4 and <em>&#x03B8;</em> = 6 for graphs with 50 nodes. Figure&#x00A0;<a class="fig" href="#fig5">5</a> shows these results in terms of the recovery of the true alignment. In the figure, the experimental results over 200 trials are essentially indistinguishable.     </p>     <p>     <strong>Our low-rank method is far more scalable.</strong> We next consider what happens to the runtime of the two algorithms as the graphs get larger. Figure&#x00A0;<a class="fig" href="#fig6">6</a> shows these results where we let each method run up to two minutes. We look at preferential attachment graphs with <em>&#x03B8;</em> = 4 and <span class="inline-equation"><span class="tex">$p_{e_1} = 0.5/n$</span>     </span>. EigenAlign requires a little more than two minutes to solve a problem of size 1000, whereas our low rank formulation can solve a problem that is an order of magnitude bigger in the same amount of time. <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">These show the average time over 10 trials. The dashed line is the time required for the matching step. The time required for EigenAlign is an order of magnitude larger than our low-rank formulation. Our low rank EigenAlign solves 10,000 node problems in about two minutes whereas EigenAlign requires the same amount of time to solve a 1000 node problem.</span>      </div>     </figure>     </p>     <p>     <strong>Our matching approximations are high quality.</strong> We also evaluate the effectiveness of our D-approximation computed in Section&#x00A0;<a class="sec" href="#sec-17">4.2</a>. Here, we compare the computed bound <em>D</em> we get to the actual approximation value of our algorithm and to the actual approximation ration of a greedy matching algorithm. The greedy algorithm <em>can</em> be implemented in a memory scalable fashion with a <em>O</em>(<em>n</em>     <sup>3</sup>) runtime (or <em>O</em>(<em>n</em>     <sup>2</sup>log&#x2009;<em>n</em>) with quadratic memory) and guarantees a 2-approximation whereas our D value gives better theoretical bounds. Figure&#x00A0;<a class="fig" href="#fig7">7</a> shows these results. Our guaranteed approximation factors are always less than 1.1 when the low-rank factors arise from the problems in Figure&#x00A0;<a class="fig" href="#fig6">6</a>. Surprisingly, greedy matching does exceptionally well in terms of approximation, prompting our next experiment. <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Over the experiments from Figure <a class="fig" href="#fig6">6</a>, the top figure shows the guaranteed D-approximation value computed by our algorithm. The bound appears to be strong and gives a better-than 1.1 approximation. The middle figure shows the true approximation value after solving the optimal matching using Lowrank EigenAlign (LR), and the bottom figure shows the true approximation value for a greedy matching (GM) strategy, which guarantees a 2-approximation.</span>      </div>     </figure> 				 </p>     <p>     <strong>Our matching greatly outperforms greedy matching and other low-rank techniques.</strong> NSD&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] is another network alignment algorithm which solves the network alignment problem via low-rank factors. In the previous experiment, we saw that greedy matching consistently gave better than expected approximation ratios. Here, we compare the low-rank EigenAlign formulations with our low-rank matching scheme to greedy matching in terms of <em>recovery</em>. The results are shown in Figure&#x00A0;<a class="fig" href="#fig8">8</a> and show that the low-rank EigenAlign strategy with our low-rank matching outperforms the other scalable alternatives. <figure id="fig8">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">Recovery scores achieved by Low Rank EigenAlign and NSD&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0008">8</a>]. We use a greedy matching (GM) and our low rank matching algorithm (LR) on the low rank factors of the similarity matrix from both algorithms.</span>      </div>     </figure>     </p>    </section>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Biological networks</h3>     </div>     </header>     <p>The MultiMagna dataset is a test case in bioinformatics that involves network alignment&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>]. It consists of a base yeast network that has been modified in different ways to produce five related networks, which we can think of as different edge sets on the same set of 1004 nodes. This results in 15 pairs of networks to align (6 choose 2). One unique aspect of this data is that there is no side information provided to guide the alignment process, which is exactly where our methods are most useful. In Figure&#x00A0;<a class="fig" href="#fig9">9</a>, we show results for aligning MultiMagna networks using low-rank EigenAlign, EigenAlign , belief propagation (BP)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>], and Klau&#x0027;s method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>] in terms of two biologically relevant measures:</p>     <p>     <strong>F-Node Correctness (F-NC).</strong> This is the F-score (harmonic mean) of the precision and recall of the alignment.</p>     <p>     <strong>NCV-Generalized S3.</strong> This shows how well the network structure correlates. Let <em>M</em> be a matching matrix for graphs with <em>n<sub>A</sub>     </em> and <em>n<sub>B</sub>     </em> nodes. The node coverage value of an alignment is NCV = 2nnz(<em>M</em>)/(<em>n<sub>A</sub>     </em> + <em>n<sub>B</sub>     </em>), where nnz(<em>M</em>) counts the number of nonzero entries in <em>M</em>. Let <em>E<sub>O</sub>     </em> be the set of overlapping edges for an alignment <em>M</em> and <em>E<sub>C</sub>     </em> be the set of conflicts, and define GS3 = |<em>E<sub>O</sub>     </em>|/(|<em>E<sub>O</sub>     </em>| + |<em>E<sub>C</sub>     </em>|). The NCV-GS3 score is the geometric mean of NCV and GS3.</p>     <p>In this experiment, we find that standard network alignment algorithms (BP and Klau) perform dreadfully (F-NC) without any guidance about which nodes might be good matches. Towards that end, we can take the output from our expanded matchings from the low-rank factors and run the Klau and BP methods <em>on this restricted set of matchings</em>. This enables them to run in a reasonable amount of time with improved results. The idea here is that we are treating Klau and BP as the matching algorithm rather than using bipartite matching for this step. This picks a matching that also yields a good alignment. Our results are comparable with the results in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>], which is a recent paper that uses a number of other algorithms on the same data. The timing results from these experiments are shown in Table&#x00A0;<a class="tbl" href="#tab1">1</a>.     </p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Time required for methods on the MultiMagna data.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Algorithm</th>        <th style="text-align:left;" colspan="3">Time (sec)</th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;">min</th>        <th style="text-align:left;">median</th>        <th style="text-align:left;">max</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">LR</td>        <td style="text-align:left;">1.9553</td>        <td style="text-align:left;">2.1971</td>        <td style="text-align:left;">2.9173</td>       </tr>       <tr>        <td style="text-align:left;">EA</td>        <td style="text-align:left;">83.6777</td>        <td style="text-align:left;">96.9938</td>        <td style="text-align:left;">194.363</td>       </tr>       <tr>        <td style="text-align:left;">BP</td>        <td style="text-align:left;">1985.2</td>        <td style="text-align:left;">2216.3</td>        <td style="text-align:left;">2744.3</td>       </tr>       <tr>        <td style="text-align:left;">Klau</td>        <td style="text-align:left;">3031.4</td>        <td style="text-align:left;">3856.0</td>        <td style="text-align:left;">4590.2</td>       </tr>       <tr>        <td style="text-align:left;">LR+BP</td>        <td style="text-align:left;">174.06</td>        <td style="text-align:left;">182.58</td>        <td style="text-align:left;">190.44</td>       </tr>       <tr>        <td style="text-align:left;">LR+Klau</td>        <td style="text-align:left;">257.59</td>        <td style="text-align:left;">301.86</td>        <td style="text-align:left;">318.83</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-22">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Collaboration network</h3>     </div>     </header>     <p>We now use Low Rank EigenAlign to perform a study on a collaboration network to understand what would be possible in terms of a fully anonymized network problem. We show that we could use our network alignment technique to identify edges where the endpoints have a high Jaccard similarity. We do so by aligning the node neighborhoods of each of the end points of each edge and observe that a high overlap implies a high Jaccard similarity score.</p>     <p>In more detail, recall that the Jaccard similarity of two nodes (<em>a</em> and <em>b</em>) is defined as <span class="inline-equation"><span class="tex">$\frac{|N(a) \cap N(b)|}{|N(a) \cup N(b)|}$</span>     </span>, where <em>N</em>(<em>a</em>) are the neighboring nodes of <em>a</em>. The vertex neighborhood of node <em>a</em> is the induced subgraph of the node and all of its neighbors. Given an edge (<em>i</em>, <em>j</em>), we then compute the Jaccard similarity between <em>i</em> and <em>j</em>, and also align the vertex neighborhood of <em>i</em> to the vertex neighborhood of <em>j</em> using our technique.</p>     <p>We use the DBLP collaboration network from&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] and consider pairs of nodes that have a sufficiently big neighborhood and are connected by an edge. Specifically, we consider nodes that have 100 or more neighbors. In total, we end up with 15187 such pairs. This is an easy experiment with our fast codes and it takes less than five minutes. The results are in Figure&#x00A0;<a class="fig" href="#fig10">10</a>. We score the network alignments in terms of normalized overlap, which is the number of overlapped edges to the maximum possible number for a pair of neighborhoods. What we observe is that large Jaccard similarities and large overlap scores are equivalent. This means we <em>could</em> have identified these results without any information on the actual identity of the vertices. <figure id="fig9">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig9.jpg" class="img-responsive" alt="Figure 9"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 9:</span>       <span class="figure-title">These are violin plots over the results of 15 problems. Klau and BP are strong algorithms for network alignment but they only perform well when given a sparsified set of possible matches from our expanded low-rank matchings (LR+BP, LR+Klau). Larger scores are better.</span>      </div>     </figure> <figure id="fig10">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186128/images/www2018-137-fig10.jpg" class="img-responsive" alt="Figure 10"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 10:</span>       <span class="figure-title">The edge overlap (normalized so the maximum value is 1.0) of alignments between vertex neighborhoods of nodes in the DBLP dataset. This shows that the Jaccard score of two connected nodes in the network is correlated to the overlap size.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-23">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion &#x0026; Discussion</h2>     </div>    </header>    <p>The low-rank spectral network alignment framework we introduce here offers a number of exciting possibilities in new uses of network alignment methods. First, it enables a new level of high-quality results with a scalable, principled method as illustrated by our experiments. This is because it has near-linear runtime and memory requirements in the size of the input networks. Second, in the course of this application, we developed a novel matching routine with high-quality a-posteriori approximation guarantees that will likely be useful in other areas as well.</p>    <p>That said, there are a number of areas that merit further exploration. First, the resulting low-rank factorization uses the matrix <em>S<sub>k</sub>     </em>, which is related to graph diffusions. There are results in computational geometry that prove rigorous results about using diffusions to align manifolds&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. There are likely to be useful connections to further explore here. Second, there are strong relationships between our low-rank methods and fast algorithms for Sylvester and multi-term matrix equations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] of the form <em>C</em>     <sub>1</sub>     <em>XD</em>     <sub>1</sub> + <em>C</em>     <sub>2</sub>     <em>XD</em>     <sub>2</sub> + &#x22C5;&#x22C5;&#x22C5; = <em>F</em>. These connections offer new possibilities to improve our methods.</p>   </section>   <section id="sec-24">    <header>     <div class="title-info">     <h2>Acknowledgements</h2>     </div>    </header>    <p>The authors were supported by NSF CCF-1149756, IIS-1422918, IIS-1546488, CCF-0939370, DARPA SIMPLEX, and the Sloan Foundation.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Nir Atias and Roded Sharan. 2012. Comparative analysis of protein networks: hard problems, practical solutions. <em>      <em>Commun. ACM</em>     </em>55, 5 (May 2012), 88&#x2013;97. <a class="link-inline force-break" href="https://doi.org/10.1145/2160718.2160738"      target="_blank">https://doi.org/10.1145/2160718.2160738</a></li>     <li id="BibPLXBIB0002" label="[2]">Mohsen Bayati, David&#x00A0;F. Gleich, Amin Saberi, and Ying Wang. 2013. Message-Passing Algorithms for Sparse Network Alignment. <em>      <em>ACM Trans. Knowl. Discov. Data</em>     </em>7, 1, Article 3 (March 2013), 31&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2435209.2435212"      target="_blank">https://doi.org/10.1145/2435209.2435212</a></li>     <li id="BibPLXBIB0003" label="[3]">D. Conte, P. Foggia, C. Sansone, and M. Vento. 2004. Third Years of Graph Matching in Pattern Recognition. (2004), 265-298&#x00A0;pages. arXiv:<a href="http://www.worldscientific.com/doi/pdf/10.1142/S0218001404003228" target="_blank">http://www.worldscientific.com/doi/pdf/10.1142/S0218001404003228</a><a class="link-inline force-break"      href="http://www.worldscientific.com/doi/abs/10.1142/S0218001404003228"      target="_blank">http://www.worldscientific.com/doi/abs/10.1142/S0218001404003228</a></li>     <li id="BibPLXBIB0004" label="[4]">Pooya Esfandiar, Francesco Bonchi, David&#x00A0;F. Gleich, Chen Greif, Laks V.&#x00A0;S. Lakshmanan, and Byung-Won On. 2010. <em>      <em>Fast Katz and Commuters: Efficient Estimation of Social Relatedness in Large Networks</em>     </em>. Springer Berlin Heidelberg, Berlin, Heidelberg, 132&#x2013;145. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-642-18009-5_13"      target="_blank">https://doi.org/10.1007/978-3-642-18009-5_13</a></li>     <li id="BibPLXBIB0005" label="[5]">Soheil Feizi, Gerald Quon, Mariana&#x00A0;Recamonde Mendoza, Muriel M&#x00E9;dard, Manolis Kellis, and Ali Jadbabaie. 2016. Spectral Alignment of Networks. <em>      <em>arXiv</em>     </em>cs.DS(2016), 1602.04181. <a class="link-inline force-break" href="http://arxiv.org/abs/1602.04181"      target="_blank">http://arxiv.org/abs/1602.04181</a></li>     <li id="BibPLXBIB0006" label="[6]">Brian&#x00A0;P. Kelley, Bingbing Yuan, Fran Lewitter, Roded Sharan, Brent&#x00A0;R. Stockwell, and Trey Ideker. 2004. PathBLAST: a tool for alignment of protein interaction networks. <em>      <em>Nucl. Acids Res.</em>     </em>32(2004), W83&#x2013;88. <a class="link-inline force-break" href="https://doi.org/10.1093/nar/gkh411"      target="_blank">https://doi.org/10.1093/nar/gkh411</a></li>     <li id="BibPLXBIB0007" label="[7]">Gunnar&#x00A0;W Klau. 2009. A new graph-based method for pairwise global network alignment. <em>      <em>BMC Bioinformatics</em>     </em>10, 1 (2009), S59.</li>     <li id="BibPLXBIB0008" label="[8]">Giorgos Kollias, Shahin Mohammadi, and Ananth Grama. 2012. Network Similarity Decomposition (NSD): A Fast and Scalable Approach to Network Alignment. <em>      <em>IEEE Trans. on Knowl. and Data Eng.</em>     </em>24, 12 (December 2012), 2232&#x2013;2243. <a class="link-inline force-break" href="https://doi.org/10.1109/TKDE.2011.174"      target="_blank">https://doi.org/10.1109/TKDE.2011.174</a></li>     <li id="BibPLXBIB0009" label="[9]">Giorgos Kollias, Madan Sathe, Olaf Schenk, and Ananth Grama. 2014. Fast parallel algorithms for graph similarity and matching. <em>      <em>J. Parallel and Distrib. Comput.</em>     </em>74, 5 (2014), 2400 &#x2013; 2410. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.jpdc.2013.12.010"      target="_blank">https://doi.org/10.1016/j.jpdc.2013.12.010</a></li>     <li id="BibPLXBIB0010" label="[10]">Nitish Korula and Silvio Lattanzi. 2014. An Efficient Reconciliation Algorithm for Social Networks. <em>      <em>Proc. VLDB Endow.</em>     </em>7, 5 (January 2014), 377&#x2013;388. <a class="link-inline force-break" href="https://doi.org/10.14778/2732269.2732274"      target="_blank">https://doi.org/10.14778/2732269.2732274</a></li>     <li id="BibPLXBIB0011" label="[11]">D. Koutra, H. Tong, and D. Lubensky. 2013. BIG-ALIGN: Fast Bipartite Graph Alignment. In <em>      <em>2013 IEEE 13th International Conference on Data Mining</em>     </em>. 389&#x2013;398. <a class="link-inline force-break" href="https://doi.org/10.1109/ICDM.2013.152"      target="_blank">https://doi.org/10.1109/ICDM.2013.152</a></li>     <li id="BibPLXBIB0012" label="[12]">Oleksii Kuchaiev, Tijana Milenkovi&#x0107;, Vesna Memi&#x0161;evi&#x0107;, Wayne Hayes, and Nata&#x0161;a Pr&#x017E;ulj. 2010. Topological network alignment uncovers biological function and phylogeny. <em>      <em>Journal of The Royal Society Interface</em>     </em>7, 50 (2010), 1341&#x2013;1354. <a class="link-inline force-break" href="https://doi.org/10.1098/rsif.2010.0063"      target="_blank">https://doi.org/10.1098/rsif.2010.0063</a></li>     <li id="BibPLXBIB0013" label="[13]">Oleksii Kuchaiev and Nata&#x0161;a Pr&#x017E;ulj. 2011. Integrative network alignment reveals large regions of global network similarity in yeast and human. <em>      <em>Bioinformatics</em>     </em>27, 10 (2011), 1390&#x2013;1396. <a class="link-inline force-break"      href="https://doi.org/10.1093/bioinformatics/btr127"      target="_blank">https://doi.org/10.1093/bioinformatics/btr127</a></li>     <li id="BibPLXBIB0014" label="[14]">Chung-Shou Liao, Kanghao Lu, Michael Baym, Rohit Singh, and Bonnie Berger. 2009. IsoRankN: spectral methods for global alignment of multiple protein networks. <em>      <em>Bioinformatics</em>     </em>25, 12 (2009), i253&#x2013;i258. <a class="link-inline force-break"      href="https://doi.org/10.1093/bioinformatics/btp203"      target="_blank">https://doi.org/10.1093/bioinformatics/btp203</a></li>     <li id="BibPLXBIB0015" label="[15]">Xingwu Liu and Shang-Hua Teng. 2016. Maximum Bipartite Matchings with Low Rank Data. <em>      <em>Theor. Comput. Sci.</em>     </em>621, C (March 2016), 82&#x2013;91. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.tcs.2016.01.033"      target="_blank">https://doi.org/10.1016/j.tcs.2016.01.033</a></li>     <li id="BibPLXBIB0016" label="[16]">Eric Malmi, Aristides Gionis, and Evimaria Terzi. 2017. Active Network Alignment: A Matching-Based Approach. In <em>      <em>Proceedings of the International Conference on Information and Knowledge Management</em>     </em>. In presss. <a class="link-inline force-break" href="https://arxiv.org/abs/1610.05516"      target="_blank">https://arxiv.org/abs/1610.05516</a></li>     <li id="BibPLXBIB0017" label="[17]">No&#x00EB;l Malod-Dognin and Nata&#x0161;a Pr&#x017E;ulj. 2015. L-GRAAL: Lagrangian graphlet-based network aligner. <em>      <em>Bioinformatics</em>     </em>31, 13 (2015), 2182&#x2013;2189. <a class="link-inline force-break"      href="https://doi.org/10.1093/bioinformatics/btv130"      target="_blank">https://doi.org/10.1093/bioinformatics/btv130</a></li>     <li id="BibPLXBIB0018" label="[18]">Vesna Memisevic and Nata&#x0161;a Pr&#x017E;ulj. 2012. C-GRAAL: Common-neighbors-based global GRAph ALignment of biological networks. <em>      <em>Integrative biology : quantitative biosciences from nano to macro</em>     </em>4, 7 (07 2012), 734&#x2013;43.</li>     <li id="BibPLXBIB0019" label="[19]">Lei Meng, Aaron Striegel, and Tijana Milenkovi&#x0107;. 2016. Local versus global biological network alignment. <em>      <em>Bioinformatics</em>     </em>32, 20 (2016), 3155&#x2013;3164. <a class="link-inline force-break"      href="https://doi.org/10.1093/bioinformatics/btw348"      target="_blank">https://doi.org/10.1093/bioinformatics/btw348</a></li>     <li id="BibPLXBIB0020" label="[20]">Tijana Milenkovic, Weng&#x00A0;Leong Ng, Wayne Hayes, and Nata&#x0161;a Pr&#x017E;ulj. 2010. Optimal Network Alignment with Graphlet Degree Vectors. <em>      <em>Cancer Informatics</em>     </em>9 (06 2010), 121&#x2013;37.</li>     <li id="BibPLXBIB0021" label="[21]">Shahin Mohammadi, David&#x00A0;F Gleich, Tamara&#x00A0;G Kolda, and Ananth Grama. 2016. Triangular alignment (TAME): A tensor-based approach for higher-order network alignment. <em>      <em>IEEE/ACM transactions on computational biology and bioinformatics</em>     </em>Online (2016), 1&#x2013;14. <a class="link-inline force-break"      href="https://doi.org/10.1109/TCBB.2016.2595583"      target="_blank">https://doi.org/10.1109/TCBB.2016.2595583</a></li>     <li id="BibPLXBIB0022" label="[22]">Huda Nassar and David&#x00A0;F. Gleich. 2017. Multimodal Network Alignment. In <em>      <em>Proceedings of the 2017 SIAM International Conference on Data Mining</em>     </em>. SIAM, 615&#x2013;623. <a class="link-inline force-break"      href="https://doi.org/10.1137/1.9781611974973.69"      target="_blank">https://doi.org/10.1137/1.9781611974973.69</a></li>     <li id="BibPLXBIB0023" label="[23]">Maks Ovsjanikov, Quentin M&#x00E9;rigot, Facundo M&#x00E9;moli, and Leonidas Guibas. 2010. One Point Isometric Matching with the Heat Kernel. <em>      <em>Computer Graphics Forum</em>     </em>29, 5 (2010), 1555&#x2013;1564. <a class="link-inline force-break"      href="https://doi.org/10.1111/j.1467-8659.2010.01764.x"      target="_blank">https://doi.org/10.1111/j.1467-8659.2010.01764.x</a></li>     <li id="BibPLXBIB0024" label="[24]">Rob Patro and Carl Kingsford. 2012. Global network alignment using multiscale spectral signatures. <em>      <em>Bioinformatics</em>     </em>28, 23 (2012), 3105&#x2013;3114.</li>     <li id="BibPLXBIB0025" label="[25]">Christian Schellewald and Christoph Schn&#x00F6;rr. 2005. Probabilistic Subgraph Matching Based on Convex Relaxation. In <em>      <em>Energy Minimization Methods in Computer Vision and Pattern Recognition</em>     </em>. Springer Berlin / Heidelberg, Berlin, Heidelberg, 171&#x2013;186. <a class="link-inline force-break" href="https://doi.org/10.1007/11585978_12"      target="_blank">https://doi.org/10.1007/11585978_12</a></li>     <li id="BibPLXBIB0026" label="[26]">V. Simoncini. 2016. Computational Methods for Linear Matrix Equations. <em>      <em>SIAM Rev.</em>     </em>58, 3 (2016), 377&#x2013;441. <a class="link-inline force-break" href="https://doi.org/10.1137/130912839"      target="_blank">https://doi.org/10.1137/130912839</a>arXiv:<a href="https://doi.org/10.1137/130912839" target="_blank">https://doi.org/10.1137/130912839</a></li>     <li id="BibPLXBIB0027" label="[27]">Rohit Singh, Jinbo Xu, and Bonnie Berger. 2008. Global alignment of multiple protein interaction networks with application to functional orthology detection. <em>      <em>PNAS</em>     </em>105, 35 (2008), 12763&#x2013;12768. <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.0806627105"      target="_blank">https://doi.org/10.1073/pnas.0806627105</a></li>     <li id="BibPLXBIB0028" label="[28]">V. Vijayan and T. Milenkovi&#x0107;. 2017. Multiple network alignment via multiMAGNA++. <em>      <em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em>     </em>PP, 99(2017), 1&#x2013;1. <a class="link-inline force-break"      href="https://doi.org/10.1109/TCBB.2017.2740381"      target="_blank">https://doi.org/10.1109/TCBB.2017.2740381</a></li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW 2018, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/>ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186128">https://doi.org/10.1145/3178876.3186128</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
