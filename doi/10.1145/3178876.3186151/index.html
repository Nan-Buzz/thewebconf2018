<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Learning Causal Effects From Many Randomized Experiments Using Regularized Instrumental Variables</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186151"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186151'>https://doi.org/10.1145/3178876.3186151</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186151'>https://w3id.org/oa/10.1145/3178876.3186151</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Learning Causal Effects From Many Randomized Experiments Using Regularized Instrumental Variables</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Alexander</span> <span class="surName">Peysakhovich</span>, Facebook Artificial Intelligence Research, New York, NY, <a href="mailto:alexpeys@fb.com">alexpeys@fb.com</a>
        </div>
        <div class="author">
          <span class="givenName">Dean</span> <span class="surName">Eckles</span>, Massachusetts Institute of Technology, Cambridge, MA, <a href="mailto:eckles@mit.edu">eckles@mit.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186151" target="_blank">https://doi.org/10.1145/3178876.3186151</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Scientific and business practices are increasingly resulting in large collections of randomized experiments. Analyzed together multiple experiments can tell us things that individual experiments cannot. We study how to learn causal relationships between variables from the kinds of collections faced by data scientists: experiments are numerous, many experiments have very small effects, and the analyst lacks metadata (e.g., descriptions of the interventions). We use experimental groups as instrumental variables (IV) and show that a standard method (two-stage least squares) is biased even when the number of experiments is infinite. We show how a sparsity-inducing <em>l</em> <sub>0</sub> regularization can (in a reversal of the standard bias–variance tradeoff) reduce bias (and thus error) of interventional predictions. We are interested in estimating causal effects, rather than just predicting outcomes, so we also propose a modified cross-validation procedure (IVCV) to feasibly select the regularization parameter. We show, using a trick from Monte Carlo sampling, that IVCV can be done using summary statistics instead of raw data. This makes our full procedure simple to use in many real-world applications.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>General and reference</strong> → <strong>Experimentation;</strong> • <strong>Mathematics of computing</strong> → <strong>Probability and statistics;</strong> • <strong>Computing methodologies</strong> → <strong>Machine learning;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>causality</small>,</span> <span class="keyword"><small>experimentation</small>,</span> <span class="keyword"><small>instrumental variables</small>,</span> <span class="keyword"><small>machine learning</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Alexander Peysakhovich and Dean Eckles. 2018. Learning Causal Effects From Many Randomized Experiments Using Regularized Instrumental Variables. In <em>WWW 2018: The 2018 Web Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 9 Pages. <a href="https://doi.org/10.1145/3178876.3186151" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3178876.3186151</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Randomized experiments (i.e. A/B tests, randomized controlled trials) are a popular practice in medicine, business, and public policy [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>]. When decision-makers employ experimentation they have a far greater chance of learning true causal relationships and making good decisions than via observation alone [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]. However, a single experiment is often insufficient to learn about the causal mechanisms linking multiple variables. Learning such multivariate causal structures is important for both theory building and making decisions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>].</p>
      <p>Consider the situation of a internet service for watching videos. The firm is interested in how watching different types of videos (e.g., funny vs. serious, short vs. long) affects user behaviors (e.g., by increasing time spent on the site, inducing subscriptions, etc.). Such knowledge will inform decisions about content recommendation or content acquisition. Even though the firm can measure all relevant variables, training a model on observational data will likely be misleading. Existing content recommendation systems and heterogeneous user dispositions will produce strong correlations between exposure and time spent or subscription, but the magnitude of this correlation will, in general, not match what would occur if the decision-maker <em>intervened</em> and changed the promotion or availability of various video types. Thus, we are interested not just in prediction but prediction under intervention [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>].</p>
      <p>The standard solution is to run a randomized experiment exposing some users to more of some type of video. However, a single test will likely change many things in the complex system. It is hard to change the number of views of funny videos without affecting the number of views of serious videos or short videos. This is sometimes called the problem of ‘fat hand’ interventions because such interventions touch multiple causal variables at once and so the effect on a single variable is not identified. To solve this issue the company would need to experiment with several factors simultaneously, perhaps conducting new experiments specifically to measure effects via each mechanism [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>].</p>
      <p>However, because routine product experimentation is common in internet companies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>], this firm has likely already run many A/B tests, including on the video recommendation algorithm. The method proposed in this paper can either be applied to a new set of experiments run explicitly to learn a causal effect vector [as in, e.g., <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], or can be applied to repurpose already run tests by treating them as random perturbations injected into the system and using that randomness in a smart way.</p>
      <p>Our contributions arise from adapting the econometric method of instrumental variables [IV; <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] to this setting. It is well known that a standard IV estimator — two-stage least squares (TSLS) — is biased in finite samples [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>]. For our case, it also has asymptotic bias. We show that this bias depends on the distribution of the treatment effects in the set of experiments under consideration.</p>
      <p>Our main technical contribution is to introduce a multivariate <em>l</em> <sub>0</sub> regularization into the first stage of the TSLS procedure and show that it can reduce the bias of estimated causal effects. Because in finite samples this regularization procedure reduces bias but adds variance, we introduce a method to trade these off and select a regularization parameter. We call this procedure <em>instrumental variables cross-validation</em> (IVCV). In an empirical evaluation that combines simulation and data from hundreds of real randomized experiments, we show that the <em>l</em> <sub>0</sub> regularization with IVCV outperforms TSLS and a Bayesian random effects model.</p>
      <p>Finally, we show how to perform this estimation in a computationally and practically efficient way. Our regularization and cross-validation procedures only require summary statistics at the level of experimental groups. This is advantageous when using raw data is computationally or practically burdensome, e.g., in the case of internet companies. This means the computational and data storage complexities of the method are actually quite low. In addition, standard A/B testing platforms [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>] should already compute and store all the required statistics, so the method here can be thought of as an “upcycling” of existing statistics.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Confounding and the Basic IV Model</h2>
        </div>
      </header>
      <p>Suppose we have some (potentially vector valued) random variable <em>X</em> and a scalar valued outcome variable <em>Y</em>. We want to ask: what happens to <em>Y</em> if we change some component of <em>X</em> by one unit, holding the rest constant? Formally, we study a linear structural (i.e. data generating) equation pair</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ X = U\psi + \epsilon _{X} \]</span><br />
        </div>
      </div>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ Y = X\beta + U\gamma + \epsilon _{Y} \]</span><br />
        </div>
      </div>where <em>U</em>, ϵ <sub><em>X</em></sub> , and ϵ <sub><em>Y</em></sub> are independent random variables with mean 0, without loss of generality. Note that in A/B testing we are often interested in relatively small changes to the system, and thus we can just think about locally linear approximations to the true function. We can also consider basis expansions. We refer to <em>X</em> as the causal variables (in our motivating example this would be a vector of time spent on each video type), <em>Y</em> as the outcome variables (here overall user satisfaction), <em>U</em> as the unobserved confounders, ϵ as noise, and <em>β</em> as the causal effects.
      <p></p>
      <p>In general, we are interested in estimating the causal effect <em>β</em> because we are interested in intervention, e.g., one which will change our data-generating model to</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ X = U\psi + \epsilon _{X} + a. \]</span><br />
        </div>
      </div>
      <p></p>
      <p>In the presence of unobserved confounders trying to learn causal relationships using predictive models naively can lead us astray [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]. Suppose that we have observational data of the form (<em>X</em>, <em>Y</em>) with <em>U</em> completely unobserved. If we use this data to estimate the causal effect <em>β</em> we can, due to the influence of the unobserved confounder, get an estimate that is (even in infinite samples) larger, smaller or even the opposite sign of the true causal effect <em>β</em>.</p>
      <p>To see this consider the linear structure equation model above and suppose that we only observe (<em>X</em>, <em>Y</em>) where both are scalar. Since the underlying model is linear, we can try to estimate it using a linear regression. However, not including the confounder <em>U</em> in the regression yields the estimator:</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \hat{\beta }_\text{obs} = (X^{\prime } X)^{-1} (X^{\prime } Y) \]</span><br />
        </div>
      </div>
      <p></p>
      <p>When all variables are scalar algebra yields</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \mathbb {E}[\hat{\beta }_\text{obs}] = \beta + \gamma \frac{\text{Cov}(X,U)}{\text{Var}(X)}. \]</span><br />
        </div>
      </div>
      <p></p>
      <p>Thus, the best linear predictor of <em>Y</em> given <em>X</em> (<span class="inline-equation"><span class="tex">$\hat{\beta }_\text{obs}$</span></span> ) may not be lead to a good estimate of what would happen to <em>Y</em> if we <em>intervened</em> (<em>β</em>).</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186151/images/www2018-160-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">DAG representing our structural equations, in which the relationship between <em>X</em> and <em>Y</em> is confounded by <em>U</em>, and including the instrumental variable <em>Z</em>. Crosses represent causal relationships that are ruled out by the IV assumptions.</span>
        </div>
      </figure>
      <p></p>
      <p>We now discuss instrumental variable (IV) estimator as a method for learning the causal effects. Suppose that we have some variable <em>Z</em> that has two properties (see Figure <a class="fig" href="#fig1">1</a> for the directed acyclic graph which represents these assumptions):</p>
      <ol class="list-no-style">
        <li id="list1" label="(1)"><em>Z</em> is not caused by anything in the (<em>X</em>, <em>U</em>, <em>Y</em>) system; that is, <em>Z</em> is as good as randomly assigned.<br /></li>
        <li id="list2" label="(2)">
          <em>Z</em> affects <em>Y</em> only via <em>X</em>. This is often called the exclusion restriction or complete mediation assumption [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>]<br />
        </li>
      </ol>
      <p>In terms of the structural equations, this modifies the equation for <em>X</em> to be</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ X = Z \mu + U \psi + \epsilon _{X} \]</span><br />
        </div>
      </div>with the appropriate independence assumptions.
      <p></p>
      <p>The standard IV estimator for <em>β</em> is two-stage least squares (TSLS) and works off the principle that the variance in <em>X</em> can be broken down into two components. The first component is confounded with the true causal effect (i.e. comes from <em>U</em>). The second component, on the other hand, is independent of <em>U</em>. Thus, if we could regress <em>Y</em> only on the random component, we could recover the causal effect <em>β</em>. Knowing <em>Z</em> allows us to do nearly this (i.e. by using only the variation in <em>X</em> caused by <em>Z</em> not <em>U</em>).</p>
      <p>TSLS can be thought of as follows: in the first stage we regress <em>X</em> on <em>Z</em>. We then replace <em>X</em> by the predicted values from the regression. In the second stage, we regress <em>Y</em> on these fitted values. It is straightforward to show that as <em>n</em> approaches infinity this estimator converges to the true causal effect <em>β</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>, Theorem 5.1].</p>
      <p>All IV methods make a full rank assumption. In order to estimate the effect of each variable <em>X<sub>j</sub></em> on <em>Y</em> with the other <em>X</em>’s held constant it must be the case that <em>Z</em> is such that it causes independent variation in all dimensions of <em>X</em>. This implies that we must, at least, have as many instruments as the dimension of <em>β</em> for TSLS to work. An interesting and fruitful direction for future work is what to do when some subspace of <em>X</em> is well spanned by our instruments but some some subspace is not.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> IV with Test Groups without Metadata</h2>
        </div>
      </header>
      <p>In our setting of interest, randomly assigned groups from a large collection of experiments are the instruments.</p>
      <p>Formally, here the IV is a categorical variable indicating which of <em>K</em> test groups a unit (e.g., user) was assigned to in one of many experiments. For simplicity of notation, we assume that each treatment group <em>g</em> ∈ {1, ..., <em>K</em>} has exactly <em>n<sub>g</sub></em> = <em>n</em> <sub>per</sub> units assigned to it at random.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Computational Properties</h3>
          </div>
        </header>
        <p>The way to represent the first stage regression of the TSLS is to use the <em>one-hot representation</em> (or dummy-variable encoding) of the group which each unit is assigned to, such that <em>Z<sub>i</sub></em> is a <em>K</em>-dimensional vector of 0s and a single 1 indicating the randomly assigned group.</p>
        <p>In this setup the TSLS estimator has a very convenient form. The first stage regression of <em>X</em> on <em>Z</em> simply yields estimates that are group level means of <em>X</em> in each group. This means that if each group has the same number of units (e.g., users) and the same error variance, the second stage has a convenient form as well: we can recover <em>β</em> by simply regressing group level averages of <em>X</em> on <em>Y</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, section 4.1.3].</p>
        <p>Thus, to estimate causal effects from large meta-analyses practitioners do not need to retain or compute with the raw data (which can span millions or billions of rows in the context of A/B testing at a medium or large internet company), but rather can retain and compute with sample means of <em>X</em> and <em>Y</em> in each A/B test group (this is now just thousands of rows of data). These are quantities that are recorded already in many A/B testing systems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>]. Working with summary statistics simplifies computation enormously and allows us to reuse existing pipelines and data.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Asymptotic Bias in the Grouped IV Estimator</h3>
          </div>
        </header>
        <p>There are now multiple ways to think about the asymptotic properties of this “groups as IVs” estimator. Either we increase the size of each experiment (<em>n</em> <sub>per</sub> → ∞) or we get more experiments (<em>K</em> → ∞). The former is the standard asymptotic sequence, but for meta-analysis of a growing collection of experiments, the latter is the more natural asymptotic series, so we fix <em>n</em> <sub>per</sub> but we raise <em>K</em>.</p>
        <p>We fix ideas with the case where <em>X</em>, <em>Y</em>, <em>Z</em>, <em>U</em> are scalar. We denote the group level means of our variables with bars (e.g., <span class="inline-equation"><span class="tex">$\bar{X}$</span></span> to be the random variable that is the group-level means of <em>X</em>). Recall that our TSLS is, in the group case, a regression of <span class="inline-equation"><span class="tex">$\bar{Y}$</span></span> on <span class="inline-equation"><span class="tex">$\bar{X}$</span></span> .</p>
        <p>Decompose the causal variable group level average into</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{X} = \bar{Z} + \bar{U} \psi + \epsilon _{\bar{X}}, \]</span><br />
          </div>
        </div>where
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{Z} \equiv Z \mu = \mathbb {E}[X | Z] \]</span><br />
          </div>
        </div>is the true first stage of the IV model (i.e. what we are trying to learn in the first stage of the TSLS). In the case of experiments as instruments this term has a nice interpretation: it is the true average value of the causal variables when assigned to that experimental group. If we assume (without loss of generality) that the mean of <em>X</em> is 0 then this first-stage can also be interpreted as the true treatment effect of the experiment.
        <p></p>
        <p>While we are not considering asymptotic series where <em>n</em> <sub>per</sub> goes to infinity, <em>n</em> <sub>per</sub> will generally also be large enough that so that we can use the normality of sample means guaranteed by the central limit theorem. Thus, <span class="inline-equation"><span class="tex">$\bar{U}$</span></span> and <span class="inline-equation"><span class="tex">$\bar{\epsilon }_X$</span></span> are normal with mean 0 and variance proportional to <em>n<sub>per</sub></em> <sup>− 1</sup>.</p>
        <p>With finite <em>n</em> <sub>per</sub> we can show that, even as <em>K</em> → ∞, TSLS will be biased [cf. <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>]. Suppose for intuition that <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> has mean 0 and finite variance <span class="inline-equation"><span class="tex">$\sigma ^2_{\bar{Z}}$</span></span> this bias has the closed form which can be derived as follows. First, denote <span class="inline-equation"><span class="tex">$\bar{A}$</span></span> as the group level mean of variable <em>A</em>. From the structural equations we know that:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{X} = \bar{Z} + \bar{U} \psi + \epsilon _{\bar{X}} \]</span><br />
          </div>
        </div>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{Y} = \bar{X} \beta + \bar{U} \gamma + \epsilon _{\bar{Y}} \]</span><br />
          </div>
        </div>
        <p></p>
        <p>Since the TSLS estimator in this case is a regression of <span class="inline-equation"><span class="tex">$\bar{X}$</span></span> on <span class="inline-equation"><span class="tex">$\bar{Y}$</span></span> we can use the equation derived above for the scalar case to rewrite</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathbb {E}[\beta _{TSLS}] = \beta + \gamma \frac{\text{Cov}(\bar{X}, \bar{U})}{\text{Var}(\bar{X})}. \]</span><br />
          </div>
        </div>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathop{plim}_{K \rightarrow \infty } \hat{\beta }_\text{TSLS} = \beta + \frac{\gamma \psi \frac{ \sigma ^2_{U}}{{n_{\text{per}}}}}{\psi ^{2} \frac{\sigma ^2_U}{{n_{\text{per}}}} + \frac{\sigma ^2_{\epsilon _X}}{{n_{\text{per}}}} + \sigma ^2_{\bar{Z}}}. \]</span><br />
          </div>
        </div>To understand where this bias comes from, think about the case where <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> is always 0. The instrument does nothing, however the group-level averages still include group-level confounding noise; that is, for finite <em>n</em> <sub>per</sub>, <span class="inline-equation"><span class="tex">$\bar{U}$</span></span> has positive variance.
        <p></p>
        <p>Thus, we simply recover the original observational estimate that we have already discussed as including omitted variable bias. When <em>Z</em> is not degenerate, <span class="inline-equation"><span class="tex">$\bar{X}$</span></span> and <span class="inline-equation"><span class="tex">$\bar{Y}$</span></span> include variation from both <span class="inline-equation"><span class="tex">$\bar{U}$</span></span> and <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> . As <em>n</em> <sub>per</sub> increases the influence of <span class="inline-equation"><span class="tex">$\bar{U}$</span></span> decreases and so <span class="inline-equation"><span class="tex">$\hat{\beta }_{\text{TSLS}}$</span></span> is consistent for <em>β</em>.</p>
        <p>While in many cases, where variation induced by instrumental variables is large, this bias can be safely ignored, in the case of online A/B testing this is likely not the case. Since much of online experimentation involves hill climbing and small improvements (on the order of a few percent or less) that add up, the TSLS estimator can be quite biased in practice (more on this below).</p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Bias-Reducing Regularization</h2>
        </div>
      </header>
      <p>We now introduce a regularization procedure that can decrease bias in the TSLS estimator. We show that, in this setting a <em>l</em> <sub>0</sub>-regularized first stage is computationally feasible and can help reduce this bias under some conditions on the distribution of the latent treatment effects.</p>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Intuition via a Mixture Model</h3>
          </div>
        </header>
        <p>There are many types of A/B tests conducted — some are micro-optimizations at the margin and some are larger explorations of the action space. Consider the stylized case with two types of tests calling the smaller variance type ‘weak’ tests while the larger variance ones are ‘strong.’ Here we can model the first stage <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> as being drawn from a two-component mixture distribution:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{Z} = {E}[X | Z] \sim \left\lbrace \begin{array}{@{}l@{\quad }l@{}}\mathcal {N} (0, \sigma ^2_{weak}) &amp; \text{with probability } p\\ \mathcal {N} (0, \sigma ^2_{strong}) &amp; \text{with probability } (1-p)\\ \end{array}\right. \]</span><br />
          </div>
        </div>
        <p></p>
        <p>If we knew which group was drawn from which component and ran two separate TSLS procedures using only groups whose <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> is drawn from the same component, we would asymptotically get two estimators:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathop{plim}_{K \rightarrow \infty } \hat{\beta }_{\text{TSLS}, j} = \beta + \gamma \frac{ \psi \frac{\sigma ^2_{U}}{{n_{\text{per}}}}}{\psi ^{2} \frac{\sigma ^2_U}{{n_{\text{per}}}} + \frac{\sigma ^2_{\epsilon _{X}}}{{n_{\text{per}}}} + \sigma ^2_{j}} \]</span><br />
          </div>
        </div>Here <em>j</em> ∈ {weak, strong} representing the component from which a particular group's <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> is drawn. Because <span class="inline-equation"><span class="tex">$\sigma ^2_\text{strong} {\gt} \sigma ^2_\text{weak}$</span></span> we will have that <span class="inline-equation"><span class="tex">$\hat{\beta }_{\text{TSLS, strong}}$</span></span> is a less asymptotically biased (and thus asymptotically better) estimator than <span class="inline-equation"><span class="tex">$\hat{\beta }_{\text{TSLS, weak}}.$</span></span> Thus, if we could choose, we would choose to only use strong tests for our estimation of the causal effect.
        <p></p>
        <p>In reality, we would likely not know which component each group is drawn from and if simply ran a TSLS on the full data set, this estimator will be a weighted combination of the two estimators.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186151/images/www2018-160-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Comparison of stagewise vs. IVCV method. X-axis is the strength of regularization (lower <em>p</em>-value implies stronger regularization). Optimizing for stagewise loss would imply using almost no regularization whereas optimizing for IVCV loss implies strong regularization. Causal loss coincides much more with IVCV loss than stagewise loss.</span>
          </div>
        </figure>
        <p></p>
        <p>Within this discrete mixture model, we are limited to how much we can reduce bias (since <span class="inline-equation"><span class="tex">$\mathop{plim}_{K \rightarrow \infty } \hat{\beta }_\text{TSLS, strong} \ne \beta$</span></span> ). However if the treatment effects are drawn from a distribution which is an infinite mixture of normals that has full support on normals of all variances (for example a <em>t</em> distribution) then we can asymptotically reduce the bias below any ϵ by using only observations which come from components with arbitrarily large variances.</p>
        <p>We now introduce a regularization procedure that tries to perform this selection. Because using this regularization effectively decreases our dataset size, decreasing the bias increases the variance. Thus, afterwards we will turn to a procedure to set the regularization parameter to make good finite-sample bias–variance tradeoffs.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Formalizing First Stage Regularization</h3>
          </div>
        </header>
        <p>Consider a data set <span class="inline-equation"><span class="tex">$(\bar{X}_g, \bar{Y}_g)$</span></span> of vectors of group-level averages. Let</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ p(x) = \Pr (|\bar{U}_g + \bar{\epsilon }_{x,g}| {\gt} |x|) \]</span><br />
          </div>
        </div>be the <em>p</em>-value for a group-level observation <em>x</em> under a ‘no intervention’ null with <em>Z</em> = 0. Under this null, <span class="inline-equation"><span class="tex">$\bar{X}$</span></span> is normally distributed so that computing <em>p</em> is straightforward and requires simply the observational (within-condition) covariance matrix of <em>X</em>.
        <p></p>
        <p>For a given threshold <em>q</em> ∈ (0, 1], let</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{X}^q_g \equiv \left\lbrace \begin{array}{@{}l@{\quad }l@{}}\bar{X}_g&amp; \text{if } p(\bar{X}_g) {\lt} q\\ 0 &amp; \text{otherwise}. \end{array}\right. \]</span><br />
          </div>
        </div>We then define the regularized IV estimator as
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \hat{\beta }_q = (\bar{X}^{q^{\prime }} {\bar{X}}^q)^{-1} (\bar{X}^{q^{\prime }} \bar{Y}). \]</span><br />
          </div>
        </div>
        <p></p>
        <p>Thus, this procedure is equivalent to an <em>l</em> <sub>0</sub> regularization in the first stage of the TSLS regression. In particular, when <span class="inline-equation"><span class="tex">$\bar{U}_g + \bar{\epsilon }_{x,g}$</span></span> has a normal distribution, as in the present case, then this is equivalent to <em>l</em> <sub>0</sub>-regularized least squares.</p>
        <p>Recall that in the binary mixture example above, this regularization would preferentially retain groups that come from the higher variance (strong) component. This extends to infinite mixtures, such as the <em>t</em>, where this procedure will preferentially set <span class="inline-equation"><span class="tex">$\bar{X}_g$</span></span> to zero for groups where <span class="inline-equation"><span class="tex">$\bar{Z}_g$</span></span> is drawn from a lower variance component.</p>
        <p>So far we have focused on scalar <em>X</em>. This procedure naturally extends to multidimensional settings. Just as with the single dimension we compute the ‘null’ distribution from no-intervention conditions. We then compute <span class="inline-equation"><span class="tex">$p(\bar{X}_g)$</span></span> for each group and threshold all dimensions of the experimental group <em>g</em>; that is, if this probability is above a threshold <em>q</em> we set the whole vector <span class="inline-equation"><span class="tex">$\bar{X}_g$</span></span> to 0. This gives us the multi-dimensional, group-based <em>l</em> <sub>0</sub> regularizer which we will apply in our experiments.</p>
        <p>This group-<em>l</em> <sub>0</sub> regularization can be inefficient in certain regiments of treatment effects — for example, in a regime where each A/B test explicitly only moves a single dimension of <em>X</em> (i.e. ‘skinny hand’ interventions). We show how this can go wrong in our synthetic data experiment but we also see that real A/B tests appear not to fall into this regime.</p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Causal Cross-Validation</h2>
        </div>
      </header>
      <p>We now turn to an important practical question: because there is a bias–variance tradeoff how should one set the regularization parameter when <em>K</em> is finite to optimize for prediction under intervention?</p>
      <p>First, let us suppose that we have access to the raw data where a row is a (<em>X<sub>i</sub></em> , <em>Z<sub>i</sub></em> , <em>Y<sub>i</sub></em> ) which is a unit <em>i</em>’s, <em>X</em>, <em>Y</em> and treatment assignment <em>Z</em>. We propose a procedure to set our hyperparameter <em>q</em>. We describe 2-fold version as it conveys the full intuition, but extension to <em>k</em>-folds is straightforward.</p>
      <p><strong>Instrumental variables cross-validation algorithm</strong> (IVCV):</p>
      <ol class="list-no-style">
        <li id="list3" label="(1)">Split <em>each treatment</em> in the data set into 2 folds, call these new data sets <span class="inline-equation"><span class="tex">$\lbrace (X^1_i, Y^1_i, Z^1_i) \rbrace$</span></span> and <span class="inline-equation"><span class="tex">$\lbrace (X^2_i, Y^2_i, Z^2_i) \rbrace$</span></span> .<br /></li>
        <li id="list4" label="(2)">Compute treatment level averages <span class="inline-equation"><span class="tex">$\lbrace (\bar{X}^1_g, \bar{Y}^1_g) \rbrace$</span></span> and <span class="inline-equation"><span class="tex">$\lbrace (\bar{X}^2_g, \bar{Y}^2_g) \rbrace$</span></span> as described above where <em>g</em> now indexes experimental groups.<br /></li>
        <li id="list5" label="(3)">Compute <span class="inline-equation"><span class="tex">$\hat{\beta }_{q}$</span></span> for a variety of thresholds <em>q</em> using <span class="inline-equation"><span class="tex">$\lbrace (\bar{X}^1_g, \bar{Y}^1_g) \rbrace$</span></span> .<br /></li>
        <li id="list6" label="(4)">Compute treatment level predictions of <em>Y</em> using fold 1 for each level of <em>q</em>: <span class="inline-equation"><span class="tex">$\hat{Y}^q_g = \bar{X}^1_g \hat{\beta }_q$</span></span> .<br /></li>
        <li id="list7" label="(5)">Choose <em>q</em> which minimizes <span class="inline-equation"><span class="tex">$\text{IVCV}(q) = \sum _{j} (\bar{Y}^2_g - \hat{Y}^q_g)^2.$</span></span><br /></li>
      </ol>
      <p>The intuition behind IVCV is similar to the main idea behind IV in general. Recall that our objective is to use variation in <em>X</em> that is not caused by <em>U</em>. The IVCV algorithm uses the <em>X</em> value from fold 1 and compares the prediction to the <em>Y</em> value in fold 2 because fold 1 and fold 2 share a <em>Z</em> but differ in <em>U</em> (since <em>U</em> is independent across units but <em>Z</em> is the same within group). This intuition has been exploited in split-sample based estimators [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>].</p>
      <p>We can demonstrate the importance of using the full causal loss by comparing the IVCV procedure to other two candidates. The first is simply applying naive CV in the second stage (i.e., splitting each group into 2, training a model on fold 1 and computing the CV loss naively as <span class="inline-equation"><span class="tex">$\Vert Y_2 - X_2 \hat{\beta }_q \Vert ^2$</span></span> ). The second is stagewise, in which the regularization parameter is chosen to minimize MSE in the first stage, and then the second stage is fit conditional on the selected model [as in <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>]. We compare these approaches in a simple linear model with scalar <em>X</em>, such that</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \bar{Y} = \bar{X} + \bar{U} \gamma \]</span><br />
        </div>
      </div>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \bar{X} = \bar{Z} + \bar{U}. \]</span><br />
        </div>
      </div>We set the first stage to have fat tails
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \bar{Z} = \mathbb {E}[X \mid Z] \sim t(df=3, scale=.4). \]</span><br />
        </div>
      </div>So there is confounding in the observational case set <em>γ</em> = 10, <em>n</em> <sub>per</sub> = 100 and <em>K</em> = 2500.
      <p></p>
      <p>Recall that our goal is to find a hyperparameter (regularization threshold) which gives us the best prediction <span class="inline-equation"><span class="tex">$\hat{\beta }_q$</span></span> of the causal parameter. Formally, we write this as</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \text{CausalLoss}(q) = \sum _{k} (\hat{\beta }_{qk} - \beta _k)^2. \]</span><br />
        </div>
      </div>The causal loss is an unobserved quantity in the IV problem and thus we need to choose a surrogate loss to try to approximate it in our cross-validation procedure. A good choice of CV procedure is one whose loss function tracks closely with the true causal loss function.
      <p></p>
      <p>This gives us 3 candidate cross-validation loss functions to compare to the true casual loss in our simulations</p>
      <ol class="list-no-style">
        <li id="list8" label="(1)">The second stage CV loss
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \sum _{g} (\bar{Y}^2_g - \bar{X}^2_g \hat{\beta }_q)^2 \]</span><br />
            </div>
          </div><br />
        </li>
        <li id="list9" label="(2)">The first stage CV loss
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \sum _{g} (\bar{X}^2_g - \hat{\bar{X}}^{2q}_g)^2 \]</span><br />
            </div>
          </div><br />
        </li>
        <li id="list10" label="(3)">The IVCV loss
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \sum _{g} (\bar{Y}^2_g -\bar{X}^1_g \hat{\beta }_q)^2 \]</span><br />
            </div>
          </div><br />
        </li>
      </ol>
      <p>Figure <a class="fig" href="#fig2">2</a> shows these losses as a function of the parameter <em>q</em> averaged over 500 simulations of the model above. We see that both the first stage loss curve and the second stage loss curve look very different from the causal loss curve. However, the IVCV loss curve matches almost exactly. Thus, either stage error naively yields a very different objective function from minimizing the causal error. In particular, we see that making the bias–variance tradeoffs for the first stage need not coincide with a desirable bias-variance tradeoff for inferring <em>β</em>.</p>
      <figure id="fig3">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186151/images/www2018-160-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 3:</span> <span class="figure-title">Left: Two dimensions of the multivariate means for sampled test groups (<span class="inline-equation"><span class="tex">$\bar{X}_g$</span></span> ). The joint distribution exhibits many more outliers than a multivariate normal would suggest. Right: QQ-plots for the dimensions of the sampled test groups (<span class="inline-equation"><span class="tex">$\bar{X}_g$</span></span> ). For each variable the marginal distributions are notably non-normal.</span>
        </div>
      </figure>
      <p></p>
      <p>The <em>l</em> <sub>0</sub>-regularized IV estimator only requires the kinds of summary statistics per experimental group that are already recorded in the course of running A/B tests, which has practical and computational utility. However, the cross-validation procedure above requires the use of raw data. We now turn to the following question: if the raw data is unavailable, but summary statistics are, can we use these summary statistics to choose a threshold <em>q</em>?</p>
      <p>Suppose that we have access to summary means <span class="inline-equation"><span class="tex">$\lbrace (\bar{X}_g, \bar{Y}_g) \rbrace$</span></span> for each treatment <em>j</em> and the covariance matrix of <span class="inline-equation"><span class="tex">$(\bar{X}, \bar{Y})$</span></span> conditional on <em>Z</em> = 0 which we denote by <em>τ</em>. We note that <em>τ</em> can be estimated very precisely from observational data or, in the case of the experimental meta-analysis just looking at covariances among known control groups. We assume that <em>n</em> <sub>per</sub> is large enough such that the distributions of <em>U</em> and ϵ in groups of size <span class="inline-equation"><span class="tex">$\frac{{n_{\text{per}}}}{2}$</span></span> are well approximated by the Gaussian <span class="inline-equation"><span class="tex">$\mathcal {N}(0, \frac{\sigma ^2_i}{\frac{{n_{\text{per}}}}{2}}).$</span></span></p>
      <p>To perform IVCV under these assumptions, we use a result from the literature on Monte Carlo [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>, ch. 8]. If some vector <em>X</em> is distributed</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ X \sim \mathcal {N} (\mu , \Sigma) \]</span><br />
        </div>
      </div>then any linear combination <em>T</em> = <em>θX</em> has a normal distribution. Moreover, conditional on <em>T</em> = <em>t</em> the distribution of <em>X</em> is also normal and can be written explicitly as
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ X | T=t \sim \mathcal {N} (\mu + \Sigma \Theta ^{\prime } (t - \theta \mu)) ,\Sigma - \Sigma \Theta ^{\prime } (\Theta \Sigma \Theta ^{\prime })^{-1} \Theta \Sigma). \]</span><br />
        </div>
      </div>
      <p></p>
      <p>This means if we know the observational covariance matrix <em>τ</em> then for every group <em>g</em> we can take the group level averages <span class="inline-equation"><span class="tex">$(\bar{X}_g, \bar{Y}_g)$</span></span> and sample using the equation above to get <span class="inline-equation"><span class="tex">$\bar{X}^1_g$</span></span> and <span class="inline-equation"><span class="tex">$\bar{X}^2_g$</span></span> such that <span class="inline-equation"><span class="tex">$\bar{X}^1_g + \bar{X}^2_g = 2\bar{X}_g$</span></span> . Since by the central limit theorem the generating Gaussian model is approximately correct, this procedure simulates the split required by IVCV without having access to the raw data. The algorithm is as follows:</p>
      <p><strong>Summary statistics instrumental variables cross-validation algorithm</strong> (sIVCV):</p>
      <ol class="list-no-style">
        <li id="list11" label="(1)">Start with data comprising of treatment group means <span class="inline-equation"><span class="tex">$\lbrace (\bar{X}_g, \bar{Y}_g) \rbrace$</span></span> .<br /></li>
        <li id="list12" label="(2)">Use the covariance matrix to perform Monte Carlo sampling to simulate groups
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \lbrace (X^1_i, Y^1_i, Z^1_i) \rbrace \]</span><br />
            </div>
          </div>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \lbrace (X^2_i, Y^2_i, Z^2_i) \rbrace \]</span><br />
            </div>
          </div><br />
        </li>
        <li id="list13" label="(3)">Use the IVCV algorithm to set the hyperparameter using the simulated splits.<br /></li>
        <li id="list14" label="(4)">Estimate <em>β</em> using the selected hyperparameters on the full data set.<br /></li>
      </ol>
      <figure id="fig4">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186151/images/www2018-160-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class="figure-title">Left: Causal error (relative to a naive observational estimator) for the full <em>l</em> <sub>0</sub>-regularization path (solid black), TSLS (solid red), IVCV selected parameters (dashed purple) and Bayesian random effects model (dashed teal). IVCV outperforms all other estimation techniques. Right: Error in estimating causal effects for varying numbers of test groups <em>K</em>. IVCV is useful even with a relatively small meta-analysis, while TSLS exhibits asymptotic bias. With a very small number of test groups, the Oracle can actually underperform TSLS because of near collinearity.</span>
        </div>
      </figure>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Evaluation</h2>
        </div>
      </header>
      <p>We now evaluate the IVCV procedure empirically. True causal effects in real IV data are generally unobservable, so comparisons of methods usually lack a single number to validate against. Examples of the kinds of evaluations usually done in work on IV include comparing different procedures and showing that one yields estimates which are more ‘reasonable.’</p>
      <p>Simulations allow us to know the true causal effects, but can lack realism. We strike a middle ground by using simulations where we set the causal effects ourselves but use real data to generate distributions for other variables. In our simulations we use a model given by</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \bar{X} = \bar{Z} + \bar{U} \]</span><br />
        </div>
      </div>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \bar{Y} = X \beta + \bar{U} \gamma . \]</span><br />
        </div>
      </div>Thus, in this case all the variance in <em>X</em> that is not driven by our instruments is confounding variance.
      <p></p>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span> Real A/B Tests</h3>
          </div>
        </header>
        <p>The multivariate case is made difficult and interesting when <em>U</em> has a non-diagonal covariance matrix and <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> has some unknown underlying distribution, so we generate these distributions from real data derived from 798 randomly assigned test groups from a sample of A/B tests run on a recommendation algorithm at Facebook. We define our endogenous, causal <em>X</em>s as 7 key performance indicators (i.e. intermediate outcomes examined by decision-makers and analysts); we standardize these to have mean 0 and variance 1. As the distribution of <em>U</em> we use the estimated covariance matrix among these outcomes in observational data. Third, we take the experiment-level empirical means of the <em>X</em>s as the true <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> , to which we add the confounding noise according to the distribution of <em>U</em>.</p>
        <p>We show a projection of these <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> onto 2 of the <em>X</em> dimensions in Figure <a class="fig" href="#fig3">3</a>(A). We see that the A/B tests appear to have correlated effects but do span both dimensions independently, many groups are retained even with strong first stage regularization, and the distribution has much more pronounced extremes than would be expected under a Gaussian model. Figure <a class="fig" href="#fig3">3</a>(B) compares the observed and Gaussian quantiles, illustrating that all dimensions are notably non-normal (Shapiro–Wilk tests of normality reject normality for all the variables at <em>p</em>s &lt; 10<sup>− 39</sup>).</p>
        <p>We set <em>β</em> as the vector of ones and <em>γ</em> as a diagonal matrix with alternating elements 1 and − 1, so that there is both positive and negative confounding. For each simulated data set, we compute the <em>causal MSE loss</em> for <em>β</em>; that is, the expected risk from intervening on one of the causal variables at random. If <span class="inline-equation"><span class="tex">$\hat{\beta }$</span></span> is our estimated <em>β</em> vector then recall that this is given by</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \text{CausalLoss}(\hat{\beta }) = \sum _{k} (\hat{\beta }_k - \beta _k)^2. \]</span><br />
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span> Results</h3>
          </div>
        </header>
        <p>In addition to the <em>l</em> <sub>0</sub>-regularized IV method and TSLS, we examine a Bayesian random effects model, as in Chamberlain and Imbens [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] but with a <em>t</em>, rather than Gaussian, distribution for the instruments. Formally we let</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \bar{Z} \sim t(d) \]</span><br />
          </div>
        </div>with a standard choice of prior
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ d \sim \text{Gamma}(2, .2). \]</span><br />
          </div>
        </div>To tilt the evaluation against our procedure we also give the Bayesian model the true covariance matrix for <span class="inline-equation"><span class="tex">$\bar{U}$</span></span> . To fit the Bayesian model we use Stan&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]. We compare the Bayesian random effects model and our regularized IV model to the infeasible Oracle estimator where the estimate of the first stage <span class="inline-equation"><span class="tex">$\mathbb {E}[\bar{X} \mid \bar{Z}]$</span></span> is known with certainty.
        <p></p>
        <p>Figure <a class="fig" href="#fig4">4</a>(A) shows the results for various dimensions of <em>X</em> for 1,000 simulations. Because of the high level of confounding in the observational data, the observational (OLS) estimates of the causal effect are highly biased, such that even the standard TSLS decreases our causal MSE by over <span class="inline-equation"><span class="tex">$70\%.$</span></span></p>
        <p>We see that the <em>l</em> <sub>0</sub>-regularization path (black line) reduces error compared with TSLS and, with high regularization, approaches the Oracle estimator. Furthermore, feasible selection of this hyperparameter using IVCV leads to near optimal performance (purple line). The Bayesian random effects model can reduce bias, but substantially increases variance and thus MSE.</p>
        <p>We also look at how large the collection of experimental groups needs to be to see advantages of a regularized estimator relative to a TSLS procedure.</p>
        <p>We repeat the TSLS, Oracle, and <em>l</em> <sub>0</sub>-regularization with IVCV analyses in 100 simulations with smaller <em>K</em> (Figure <a class="fig" href="#fig4">4</a>(B)) for the case of the 7 dimensional <em>X</em>. Intuitively, what is important is the relative size of the tails of the distribution of the latent treatment effects <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> . As the tails get fatter, fewer experiments are required to get draws from the more extreme components of the mixture. We see that in this realistic case where <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> is determined using a sampled set of Facebook A/B tests, feasible selection of the <em>l</em> <sub>0</sub>-regularization hyperparameter using IVCV outperforms TSLS substantially for many values of <em>K</em>. Thus, meta-analyses of even relatively small collections of experiments can be improved by the first-stage <em>l</em> <sub>0</sub> regularization.</p>
        <figure id="fig5">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186151/images/www2018-160-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span> <span class="figure-title">Performance of various IV estimation techniques under various first stage data generating assumptions (top = independent <em>t</em>, middle = Wishart <em>t</em>, bottom = correlated variances). We see that when the <em>Z</em> induced components of <em>X</em> are independent even for moderate dimensionality that the <em>l</em> <sub>0</sub> regularization performs less well. However, as soon as there is any correlation the IVCV procedure performs much better than TSLS and can both under or over-perform the Bayesian random effects model.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Fully Simulated Data</h2>
        </div>
      </header>
      <p>In addition to the evaluation using a real data set. We also consider the IVCV procedure in several completely synthetic data sets. The completely synthetic experiments allows us to elucidate the important assumptions for our procedure to work while the real data-based experiment shows that these assumptions are indeed satisfied in real world conditions.</p>
      <p>We consider the same exact model as in the previous section except that we generate the first stage effects <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> from a known parametric distribution and let <em>U</em> be normal. First, we consider</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ X = \bar{Z} + U \]</span><br />
        </div>
      </div>and we vary the distribution that <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> is drawn from. We consider
      <p></p>
      <ol class="list-no-style">
        <li id="list15" label="(1)"><span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> drawn from independent <em>t</em> with 3 degrees of freedom<br /></li>
        <li id="list16" label="(2)"><span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> drawn from <em>t</em> with 3 degrees of freedom and covariance matrix drawn from inverse Wishart (a standard prior for covariance matrices) with 10 × dim(<em>X</em>) degrees of freedom<br /></li>
        <li id="list17" label="(3)"><span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> generated by first drawing <em>σ</em> <sup>2</sup> from an inverse Gamma distribution and then <span class="inline-equation"><span class="tex">$\bar{Z}$</span></span> drawn from multivariate normal with mean 0 and covariance matrix <em>σ</em> <sup>2</sup> <em>I</em><br /></li>
      </ol>
      <p>Note that in case 1 effects are axis aligned while in the next two larger values of one dimension can predict more extreme values of <em>Z</em> (and <em>X</em>) on another dimension. The final setup corresponds to the case where components are mean-uncorrelated but have correlated variance. This is the multivariate analog of our motivating example where some A/B tests are strong explorations of the parameter spaces and others are micro-optimizations at the margin. Note that the marginal distribution for each dimension is, in all cases, a <em>t</em> distribution with 3 degrees of freedom (since the <em>t</em> can be written as a mixture of normals drawn from the inverse gamma).</p>
      <p>Figure <a class="fig" href="#fig5">5</a> shows the results of applying IVCV to the data generating processes above (top = independent <em>t</em>, middle = Wishart <em>t</em>, bottom = correlated variances). We restrict to dim(<em>X</em>) ∈ {2, 4} because it is sufficient to illustrate our main points. We see that in the independent <em>t</em> case the IVCV procedure (and indeed our multivariate <em>l</em> <sub>0</sub> regularization) can underperform the Bayesian random effects model and fail to substantially improve on TSLS. This happens because in the independent <em>t</em> case there is a high probability that a single dimension is extreme enough to pass the regularization threshold and thus even strong regularization does not necessarily remove bias. On the other hand, when outcomes are correlated (or their variances are) we see that multivariate IVCV performs well because being extreme in one <em>X</em> component predicts having extreme outcomes in other components. Note that the fact that IVCV performs well in the distribution generated by real A/B tests suggests that real world A/B test effect variance is correlated within A/B test — ie. if a test moves one metric by a large amount, it likely moves others by a large amount.</p>
    </section>
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span> Conclusion</h2>
        </div>
      </header>
      <p>Most analysis of randomized experiments, whether in academia, business, or public policy tends to look at each test in isolation. When meta-analyses of experiments are conducted, the goal is usually either to pool data about multiple instances of the same intervention or to find heterogeneity in the effects of interventions across settings. We instead propose that combining many experiments can help us learn richer causal structures. IV methods give a way of doing this pooling. We have shown that in such situations using easily-implemented <em>l</em> <sub>0</sub> regularization in the first stage can lead to much better estimates of the causal effects, and thus better predictions about interventions, than using standard TSLS methods.</p>
      <p>We expand on the literature which uses multi-condition experiments as instruments [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>]. Such analyses usually feature a smaller number of experimental groups and a single causal variable. Our work contributes to the growing literature merging experimental techniques with methods from machine learning to allow deeper analyses than has been traditionally possible [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]. In addition, our procedure is intended to be simple to implement and not require strong assumptions about the data-generating process.</p>
      <p>Our work is also related to research on IV estimation with weak instruments [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>]. In addition, we also contribute to existing research on regularized IV estimation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>]. In the case of univariate <em>X</em> and disjoint groups as instruments, the post-lasso method in Belloni et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>] coincides with the proposed <em>l</em> <sub>0</sub> regularization, however in the case where <em>X</em> is a vector, it does not.</p>
      <p>Recently, active learning in the form of bandit optimization [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>] and reinforcement learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>] have become quite popular in the AI community. Such approaches can be used in many of the same contexts as the IV analysis we have discussed here, and so may appear to be substitutes. However, we note there are many important differences the largest of which is that RL and bandit approaches try to perform policy optimization rather than learning of a causal graph. For this reason, often <em>why</em> RL/bandit estimated policies work can be hard to understand. This is in contrast to explicit causal models (e.g., the linear model described above) which can be stated explicitly and in terms that are more natural to human decision-makers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>]. On the other hand, RL/bandit approaches have advantages in that they are explicitly online whereas many causal inference procedures (including the one we have described here) are ‘batch’ procedures that assume that data collection is a passive enterprise, separate from analysis. There is growing interest in combining these approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] and we think that future work would benefit greatly from this fusion of thought.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Joshua&nbsp;D Angrist, Guido&nbsp;W Imbens, and Donald&nbsp;B Rubin. 1996. Identification of causal effects using instrumental variables. <em><em>J. Amer. Statist. Assoc.</em></em> 91, 434 (1996), 444–455.</li>
        <li id="BibPLXBIB0002" label="[2]">Joshua&nbsp;D Angrist and Alan&nbsp;B Krueger. 1995. Split-sample instrumental variables estimates of the return to schooling. <em><em>Journal of Business &amp; Economic Statistics</em></em> 13, 2 (1995), 225–235.</li>
        <li id="BibPLXBIB0003" label="[3]">Joshua&nbsp;D Angrist and Jörn-Steffen Pischke. 2008. <em><em>Mostly Harmless Econometrics: An Empiricist's Companion</em></em> . Princeton university press.</li>
        <li id="BibPLXBIB0004" label="[4]">Susan Athey and Guido Imbens. 2016. Recursive partitioning for heterogeneous causal effects. <em><em>Proceedings of the National Academy of Sciences</em></em> 113, 27(2016), 7353–7360.</li>
        <li id="BibPLXBIB0005" label="[5]">E. Bakshy, D. Eckles, and M.&nbsp;S. Bernstein. 2014. Designing and Deploying Online Field Experiments. In <em><em>Proceedings of the 23rd ACM conference on the World Wide Web</em></em> . ACM.</li>
        <li id="BibPLXBIB0006" label="[6]">Abhijit Banerjee and Esther Duflo. 2012. <em><em>Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty</em></em> . PublicAffairs.</li>
        <li id="BibPLXBIB0007" label="[7]">Paul&nbsp;A Bekker. 1994. Alternative approximations to the distributions of instrumental variable estimators. <em><em>Econometrica: Journal of the Econometric Society</em></em> (1994), 657–681.</li>
        <li id="BibPLXBIB0008" label="[8]">Alexandre Belloni, Daniel Chen, Victor Chernozhukov, and Christian Hansen. 2012. Sparse models and methods for optimal instruments with an application to eminent domain. <em><em>Econometrica</em></em> 80, 6 (2012), 2369–2429.</li>
        <li id="BibPLXBIB0009" label="[9]">Léon Bottou. 2014. From machine learning to machine reasoning. <em><em>Machine Learning</em></em> 94, 2 (2014), 133–149.</li>
        <li id="BibPLXBIB0010" label="[10]">Léon Bottou, Jonas Peters, Joaquin&nbsp;Quinonero Candela, Denis&nbsp;Xavier Charles, Max Chickering, Elon Portugaly, Dipankar Ray, Patrice&nbsp;Y Simard, and Ed Snelson. 2013. Counterfactual reasoning and learning systems: The example of computational advertising. <em><em>Journal of Machine Learning Research</em></em> 14, 1 (2013), 3207–3260.</li>
        <li id="BibPLXBIB0011" label="[11]">Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Michael&nbsp;A Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2016. Stan: A probabilistic programming language. <em><em>Journal of Statistical Software</em></em> (2016).</li>
        <li id="BibPLXBIB0012" label="[12]">Gary Chamberlain and Guido Imbens. 2004. Random effects estimators with many instrumental variables. <em><em>Econometrica</em></em> 72, 1 (2004), 295–306.</li>
        <li id="BibPLXBIB0013" label="[13]">Dean Eckles, René&nbsp;F Kizilcec, and Eytan Bakshy. 2016. Estimating peer effects in networks with peer encouragement designs. <em><em>Proceedings of the National Academy of Sciences</em></em> 113, 27(2016), 7316–7322.</li>
        <li id="BibPLXBIB0014" label="[14]">Ziv Epstein, Alexander Peysakhovich, and David&nbsp;G Rand. 2016. The good, the bad, and the unflinchingly selfish: Cooperative decision-making can be predicted with high accuracy when using only three behavioral types. In <em><em>Proceedings of the 2016 ACM Conference on Economics and Computation</em></em> . ACM, 547–559.</li>
        <li id="BibPLXBIB0015" label="[15]">John&nbsp;C Gittins. 1979. Bandit processes and dynamic allocation indices. <em><em>Journal of the Royal Statistical Society. Series B (Methodological)</em></em> (1979), 148–177.</li>
        <li id="BibPLXBIB0016" label="[16]">Mathew Goldman and Justin&nbsp;M Rao. 2014. Experiments as Instruments: Heterogeneous Position Effects in Sponsored Search Auctions. <em><em>Available at SSRN 2524688</em></em> (2014).</li>
        <li id="BibPLXBIB0017" label="[17]">Donald&nbsp;P Green, Shang&nbsp;E Ha, and John&nbsp;G Bullock. 2010. Enough already about “black box” experiments: Studying mediation is more difficult than most scholars suppose. <em><em>The Annals of the American Academy of Political and Social Science</em></em> 628, 1(2010), 200–208.</li>
        <li id="BibPLXBIB0018" label="[18]">Justin Grimmer, Solomon Messing, and Sean&nbsp;J Westwood. 2014. Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods. <em><em>Unpublished manuscript, Stanford University, Stanford, CA</em></em> (2014).</li>
        <li id="BibPLXBIB0019" label="[19]">Christian Hansen and Damian Kozbur. 2014. Instrumental variables estimation with many weak instruments using regularized JIVE. <em><em>Journal of Econometrics</em></em> 182, 2 (2014), 290–308.</li>
        <li id="BibPLXBIB0020" label="[20]">Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2016. Counterfactual Prediction with Deep Instrumental Variables Networks. <em><em>arXiv preprint arXiv:1612.09596</em></em> (2016).</li>
        <li id="BibPLXBIB0021" label="[21]">Lars&nbsp;G Hemkens, Despina&nbsp;G Contopoulos-Ioannidis, and John&nbsp;PA Ioannidis. 2016. Agreement of treatment effects for mortality from routinely collected data and subsequent randomized trials: Meta-epidemiological survey. <em><em>British Medical Journal</em></em> 352 (2016).</li>
        <li id="BibPLXBIB0022" label="[22]">Kosuke Imai, Dustin Tingley, and Teppei Yamamoto. 2013. Experimental designs for identifying causal mechanisms. <em><em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em></em> 176, 1 (2013), 5–51.</li>
        <li id="BibPLXBIB0023" label="[23]">Guido Imbens, Joshua Angrist, and Alan Krueger. 1999. Jackknife Instrumental Variables Estimation. <em><em>Journal of Applied Econometrics</em></em> 14, 1 (1999).</li>
        <li id="BibPLXBIB0024" label="[24]">Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, and Daniel&nbsp;G Goldstein. 2017. Simple rules for complex decisions. <em><em>arXiv preprint arXiv:1702.04690</em></em> (2017).</li>
        <li id="BibPLXBIB0025" label="[25]">Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann. 2013. Online controlled experiments at large scale. In <em><em>Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em> . ACM, 1168–1176.</li>
        <li id="BibPLXBIB0026" label="[26]">Robert&nbsp;J LaLonde. 1986. Evaluating the econometric evaluations of training programs with experimental data. <em><em>The American Economic Review</em></em> (1986), 604–620.</li>
        <li id="BibPLXBIB0027" label="[27]">Finnian Lattimore, Tor Lattimore, and Mark&nbsp;D Reid. 2016. Causal Bandits: Learning Good Interventions via Causal Inference. In <em><em>Advances in Neural Information Processing Systems</em></em> . 1181–1189.</li>
        <li id="BibPLXBIB0028" label="[28]">Lihong Li, Wei Chu, John Langford, and Robert&nbsp;E Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In <em><em>Proceedings of the 19th international conference on World wide web</em></em> . ACM, 661–670.</li>
        <li id="BibPLXBIB0029" label="[29]">Michelle&nbsp;N Meyer. 2015. Two cheers for corporate experimentation: The A/B illusion and the virtues of data-driven innovation. <em><em>J. on Telecomm. &amp; High Tech. L.</em></em> 13 (2015), 273.</li>
        <li id="BibPLXBIB0030" label="[30]">Art&nbsp;B. Owen. 2016. <em><em>Monte Carlo Theory, Methods and Examples</em></em> . <a href="http://statweb.stanford.edu/~owen/mc/" target="_blank">http://statweb.stanford.edu/~owen/mc/</a>
        </li>
        <li id="BibPLXBIB0031" label="[31]">Judea Pearl. 2009. <em><em>Causality</em></em> . Cambridge University Press.</li>
        <li id="BibPLXBIB0032" label="[32]">Alexander Peysakhovich and Jeffrey Naecker. 2017. Using methods from machine learning to evaluate behavioral models of choice under risk and ambiguity. <em><em>Journal of Economic Behavior &amp; Organization</em></em> 133 (2017), 373–384.</li>
        <li id="BibPLXBIB0033" label="[33]">Olav Reiersöl. 1945. <em>Confluence analysis by means of instrumental sets of variables</em>. Ph.D. Dissertation. Stockholm College.</li>
        <li id="BibPLXBIB0034" label="[34]">Uri Shalit, Fredrik Johansson, and David Sontag. 2016. Bounding and Minimizing Counterfactual Error. <em><em>arXiv preprint arXiv:1606.03976</em></em> (2016).</li>
        <li id="BibPLXBIB0035" label="[35]">Douglas Staiger and James&nbsp;H Stock. 1997. Instrumental Variables Regression with Weak Instruments. <em><em>Econometrica</em></em> (1997), 557–586.</li>
        <li id="BibPLXBIB0036" label="[36]">James&nbsp;H Stock, Jonathan&nbsp;H Wright, and Motohiro Yogo. 2012. A survey of weak instruments and weak identification in generalized method of moments. <em><em>Journal of Business &amp; Economic Statistics</em></em> (2012).</li>
        <li id="BibPLXBIB0037" label="[37]">James&nbsp;H Stock and Motohiro Yogo. 2005. Testing for weak instruments in linear IV regression. In <em><em>Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg</em></em> . Cambridge University Press, 80–108.</li>
        <li id="BibPLXBIB0038" label="[38]">Richard&nbsp;S Sutton and Andrew&nbsp;G Barto. 1998. <em><em>Reinforcement learning: An introduction</em></em> . Vol.&nbsp;1. MIT press Cambridge.</li>
        <li id="BibPLXBIB0039" label="[39]">Hal Varian. 2016. Intelligent Technology. <em><em>Finance and Development</em></em> 53, 3 (2016).</li>
        <li id="BibPLXBIB0040" label="[40]">Jeffrey&nbsp;M Wooldridge. 2010. <em><em>Econometric Analysis of Cross Section and Panel Data</em></em> . MIT Press.</li>
        <li id="BibPLXBIB0041" label="[41]">Philip&nbsp;Green Wright. 1928. <em><em>The Tariff on Animal and Vegetable Oils</em></em> . The Macmillan Co.</li>
        <li id="BibPLXBIB0042" label="[42]">Ya Xu, Nanyu Chen, Addrian Fernandez, Omar Sinno, and Anmol Bhasin. 2015. From infrastructure to culture: A/B testing challenges in large scale social networks. In <em><em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> . ACM, 2227–2236.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3178876.3186151">https://doi.org/10.1145/3178876.3186151</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
