<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Chenyi</span>     <span class="surName">Zhuang</span>,     Department of Informatics, Kyoto University, Kyoto, Japan, <a href="mailto:zhuang@db.soc.i.kyoto-u.ac.jp">zhuang@db.soc.i.kyoto-u.ac.jp</a>    </div>    <div class="author">     <span class="givenName">Qiang</span>     <span class="surName">Ma</span>,     Department of Informatics, Kyoto University, Kyoto, Japan, <a href="mailto:qiang@i.kyoto-u.ac.jp">qiang@i.kyoto-u.ac.jp</a>    </div>        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186116" target="_blank">https://doi.org/10.1145/3178876.3186116</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>The problem of extracting meaningful data through graph analysis spans a range of different fields, such as the internet, social networks, biological networks, and many others. The importance of being able to effectively mine and learn from such data continues to grow as more and more structured data become available. In this paper, we present a simple and scalable semi-supervised learning method for graph-structured data in which only a very small portion of the training data are labeled. To sufficiently embed the graph knowledge, our method performs graph convolution from different views of the raw data. In particular, a dual graph convolutional neural network method is devised to jointly consider the two essential assumptions of semi-supervised learning: (1) <em>local consistency</em> and (2) <em>global consistency</em>. Accordingly, two convolutional neural networks are devised to embed the local-consistency-based and global-consistency-based knowledge, respectively. Given the different data transformations from the two networks, we then introduce an unsupervised temporal loss function for the ensemble. In experiments using both unsupervised and supervised loss functions, our method outperforms state-of-the-art techniques on different datasets.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Graph convolutional networks</small>, </span>     <span class="keyword">      <small> Semi-supervised learning</small>, </span>     <span class="keyword">      <small> Graph diffusion</small>, </span>     <span class="keyword">      <small> Adjacency matrix</small>, </span>     <span class="keyword">      <small> Pointwise mutual information</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Chenyi Zhuang, Qiang Ma. 2018. Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       Lyon, France. ACM, New York, NY, USA 10 Pages. <a href="https://doi.org/10.1145/3178876.3186116" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186116</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>The explosion in the availability of structured and semi-structured data (e.g., from the internet, social science, physics, biology, and computer science) has led to a wealth of research focusing on the analysis of graphs. Moreover, modern companies often wish to store information in the form of a corporate knowledge graph and apply a variety of machine learning and data mining techniques (e.g., graph mining [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>], relational learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], security [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], knowledge embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]) for different applications.</p>    <p>One problem of significant interest is the classification of graph nodes from only a small portion of labeled training data and the graph structure. In the context of machine learning, graph-based semi-supervised learning is one such technique that aims to use the graph structure to train models with higher accuracy when only a limited set of labeled data is available. Accordingly, in this paper, we propose a new general semi-supervised learning algorithm that can be applied to different kinds of graphs, such as social networks, knowledge graphs, citation networks, World Wide Web, and so on.</p>    <p>Conventionally, graph-based semi-supervised learning could be defined through the following loss function: <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \mathcal {L}=\mathcal {L}_{0}+\lambda \mathcal {L}_{reg}, \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div> where <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}$</span>    </span> denotes the supervised loss with respect to the labeled data and <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}$</span>    </span> denotes the regularizer with respect to the graph structure. (Note that, in Eqs. (<a class="eqn" href="#eq1">1</a>), (<a class="eqn" href="#eq2">2</a>), and (<a class="eqn" href="#eq3">3</a>), we omit a model&#x0027;s own parameter regularization terms for simplicity.) Using an explicit graph-based regularizer <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}$</span>    </span>, the formulation of Eq. (<a class="eqn" href="#eq1">1</a>) smooths the label information in <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}$</span>    </span> over the whole graph. For instance, a graph Laplacian regularization term is typically used in <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}$</span>    </span> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], which relies on the prior assumption that connected nodes in the graph are likely to share the same label. However, this assumption might restrict the modeling capacity, as graph edges need not encode the node similarity, but could instead contain additional information.</p>    <p>To avoid this limitation, recent studies have attempted to consider both label and graph structure information in a convolutional manner [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. In Eq. (<a class="eqn" href="#eq2">2</a>), instead of using an explicit graph-based regularizer in the loss function, a convolutional function <em>Conv</em> is derived to encode the graph structure directly. <div class="table-responsive" id="eq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \mathcal {L}=\mathcal {L}_{0}(Conv). \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>    </div> In approximate terms, the structure encoding <em>Conv</em> could be conducted in two domains: (1) the graph vertex domain [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]; and (2) the graph spectral domain [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>], the authors discussed the relationship between these two domains.</p>    <p>However, by using Eqs. (<a class="eqn" href="#eq1">1</a>) and (<a class="eqn" href="#eq2">2</a>), most of the related work have only considered the <em>local consistency</em> of a graph for knowledge embedding. To sufficiently embed the graph knowledge, we find that the <em>global consistency</em> of a graph has not been well investigated yet. Hence, in this paper, we propose a dual graph convolutional neural network method to jointly take both of them into consideration. The form of the loss function in the proposed strategy is <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \mathcal {L}=\mathcal {L}_{0}(Conv_{A})+\lambda (t) \mathcal {L}_{reg}(Conv_{A}, Conv_{P}). \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>    </div>    </p>    <p>The idea of our method is simple. First, during the convolutional process, sample input data (e.g., a feature vector of a node) pass through two kinds of convolutional networks: <em>Conv<sub>A</sub>    </em> and <em>Conv<sub>P</sub>    </em>. Using the graph adjacency matrix and positive pointwise mutual information matrix, the two convolutional networks encode the local and global structure information. Corresponding to the two essential assumptions in semi-supervised learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], <em>Conv<sub>A</sub>    </em> embeds the <em>local-consistency</em>-based knowledge (i.e., nearby data points are likely to have the same label), whereas <em>Conv<sub>P</sub>    </em> embeds the <em>global-consistency</em>-based knowledge (i.e., data points that occur in similar contexts tend to have the same label).</p>    <p>During training, a sample makes multiple passes through the two convolutional networks and the random layer-wise dropout [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. Thus, different transformations of the sample are obtained. The output of either <em>Conv<sub>A</sub>    </em> or <em>Conv<sub>P</sub>    </em> is then used for supervised learning, e.g., <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_{A})$</span>    </span>. However, to give better predictions, an ensemble-oriented regularizer <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}(Conv_{A}, Conv_{P})$</span>    </span> for these transformations is derived. By minimizing the difference between predictions from different transformations of an input sample, the regularizer combines the opinions of <em>Conv<sub>A</sub>    </em> and <em>Conv<sub>P</sub>    </em>. Accordingly, <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}(Conv_{A}, Conv_{P})$</span>    </span> is an unsupervised loss function.</p>    <p>Overall, the main contributions of this work can be summarized as follows:</p>    <ol class="list-no-style">    <li id="list1" label="(1)">In addition to the graph adjacency matrix-based convolution <em>Conv<sub>A</sub>     </em>, we propose a new convolutional neural network <em>Conv<sub>P</sub>     </em> that depends on the positive pointwise mutual information (PPMI) matrix. Unlike <em>Conv<sub>A</sub>     </em>, which embeds local-consistency-based knowledge, we employ a random walk to construct a PPMI matrix that further embeds semantic information, i.e., global-consistency-based knowledge.<br/></li>    <li id="list2" label="(2)">In addition to the supervised learning on a small portion of labeled training data (i.e., <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}$</span>     </span> in Eq. (<a class="eqn" href="#eq3">3</a>)), an unsupervised loss function (i.e., <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}$</span>     </span> in Eq. (<a class="eqn" href="#eq3">3</a>)) is proposed as a kind of regularizer to combine the output of different convolved data transformations. In a series of experiments considering both <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>, our method is shown to yield better predictions than a single convolutional network.<br/></li>    </ol>   </section>   <section id="sec-3">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Background to Graph-based Semi-supervised Learning</h2>    </div>    </header>    <p>Semi-supervised learning considers the general problem of learning from labeled and unlabeled data. Given a set of data points <span class="inline-equation"><span class="tex">$\mathcal {X}=\lbrace x_1, ..., x_l, x_{l+1}, ..., x_{n} \rbrace$</span>    </span> and a set of labels <span class="inline-equation"><span class="tex">$\mathcal {C}=\lbrace 1, ..., c\rbrace$</span>    </span>, the first <em>l</em> points have labels <span class="inline-equation"><span class="tex">$\lbrace y_1, ..., y_l\rbrace \in \mathcal {C}$</span>    </span> and the remaining points are unlabeled. The goal is to predict the labels of the unlabeled points.</p>    <p>In addition to the labeled and unlabeled points, graph-based semi-supervised learning also involves a given graph, denoted as an <em>n</em> &#x00D7; <em>n</em> matrix <em>A</em>. Each entry <em>a</em>    <sub>     <em>i</em>, <em>j</em>    </sub> &#x2208; <em>A</em> indicates the <em>similarity</em> between data points <em>x<sub>i</sub>    </em> and <em>x<sub>j</sub>    </em>. The <em>similarity</em> can be derived by calculating the distances among data points [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], or may be explicitly given by structured data, such as knowledge graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>], citation graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], hyperlinks between documents [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], and so on. Therefore, the key problem of graph-based semi-supervised learning concerns how to embed the additional information of the graph for better label prediction. In approximate terms, we classify the different graph knowledge embeddings into two groups, i.e., explicit and implicit graph-based semi-supervised learning.</p>    <section id="sec-4">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Explicit Graph-Based Semi-Supervised Learning</h3>     </div>    </header>    <p>Explicit graph-based semi-supervised learning uses a graph-based regularizer (i.e., <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}$</span>     </span> in Eq. (<a class="eqn" href="#eq1">1</a>)) to incorporate the information in the graph. Conventionally, a graph Laplacian regularizer is defined so as to incur a large penalty when similar data points <em>x<sub>i</sub>     </em> and <em>x<sub>j</sub>     </em> with a large <em>a</em>     <sub>      <em>i</em>, <em>j</em>     </sub> are predicted to have different labels <em>f</em>(<em>x<sub>i</sub>     </em>) &#x2260; <em>f</em>(<em>x<sub>j</sub>     </em>). <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathcal {L}_{reg} =\sum _{i,j}a_{i,j}||f(x_i)-f(x_j)||^2 =\bf f^T \Delta f. \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> Eq. (<a class="eqn" href="#eq4">4</a>) presents an instance of the graph Laplacian regularizer, where <em>f</em>(&#x00B7;) is the label prediction function (e.g., a neural network). The unnormalized graph Laplacian is defined as <em>&#x0394;</em> = <em>A</em> &#x2212; <em>D</em>, where <em>A</em> is the adjacency matrix and <em>D</em> is a diagonal matrix with each entry defined as <em>d</em>     <sub>      <em>i</em>, <em>i</em>     </sub> = &#x2211;<sub>      <em>j</em>     </sub>     <em>a</em>     <sub>      <em>i</em>, <em>j</em>     </sub>.</p>    <p>Many related methods have been proposed as variants of Eq. (<a class="eqn" href="#eq4">4</a>). For instance, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], the authors proposed a label propagation algorithm based on Gaussian Random Fields. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], a PageRank-like algorithm is proposed to account for both local and global consistency in the graph. Recently, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], the authors proposed a sampling-based method. Instead of using the graph Laplacian <em>&#x0394;</em>, they derived a random walk-based sampling algorithm to obtain the positive and negative contexts for each data point. A feed-forward neural network method was then used for knowledge embedding.</p>    </section>    <section id="sec-5">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Implicit Graph-Based Semi-Supervised Learning</h3>     </div>    </header>    <p>As mentioned in Section <a class="sec" href="#sec-2">1</a>, a convolutional process for implicit graph-based semi-supervised learning could be conducted in either the graph vertex domain or the graph spectral domain. We now introduce some work related to each case.</p>    <p>     <strong>Convolution in the vertex domain.</strong> To apply convolution in the vertex domain, a data point <em>x<sub>i</sub>     </em> will usually be transformed in a diffusive manner. A simple example of a <em>k</em>-hop localized linear transform is <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} Conv(x_i)=b_{i, i}x_i+\sum _{j \in \mathcal {N}(i, k)}b_{i, j}x_j, \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where {<em>b</em>     <sub>      <em>i</em>, <em>j</em>     </sub>} are some weights for filtering and <span class="inline-equation"><span class="tex">$\mathcal {N}(i, k)$</span>     </span> denotes the set of neighbors connected to <em>x<sub>i</sub>     </em> by a path of <em>k</em> or fewer edges. Such a convolution is built on the idea of a diffusion kernel, which can be thought of as a measure of the level of connectivity between any two nodes in a graph. For example, by introducing a damping ratio, longer paths will be discounted more than shorter paths. A recent survey paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] compared several diffusion kernels on graphs.</p>    <p>Recently, several methods have used diffusion-based convolution. For instance, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>], the authors proposed a diffusion-convolutional neural network. In their method, the <em>k</em>-hop diffusion-convolutional results (i.e., a large tensor) are directly used as the input to a neural network. As a result, significant amounts of memory are required to record the input. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], the authors proposed a scalable method that conducts 1-hop diffusion on each layer of a neural network. This approach obtained state-of-the-art performance in semi-supervised classification.</p>    <p>     <strong>Convolution in the spectral domain.</strong> We first consider the most simple situation of scalar <em>x<sub>i</sub>     </em>. In this context, the input <span class="inline-equation"><span class="tex">$X \in \mathbb {R}^{n \times 1}$</span>     </span> is considered as a signal defined on the graph with <em>n</em> nodes. As shown in Eq. (<a class="eqn" href="#eq6">6</a>), the spectral convolution on a graph can then be defined as the multiplication of the signal <em>X</em> with a filter <em>g<sub>&#x03B8;</sub>     </em> = <em>diag</em>(<em>&#x03B8;</em>) parametrized by <span class="inline-equation"><span class="tex">$\theta \in \mathbb {R}^n$</span>     </span> in the graph Fourier domain. <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} Conv(X)=g_{\theta } * X=Ug_{\theta }U^TX, \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <em>U</em> is the matrix of eigenvectors of the normalized Laplacian matrix <span class="inline-equation"><span class="tex">$\Delta =I_n-D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$</span>     </span>, which plays the role of the Fourier transform. When <em>x<sub>i</sub>     </em> has more than one feature, we can regard <em>X</em> as a signal with multiple input channels. The transformation <em>U</em> is then applied to each channel [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>].</p>    <p>In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>], the authors discussed different ways to define graph spectral domains and explained the graph Fourier transform in detail. As Eq. (<a class="eqn" href="#eq6">6</a>) is computationally expensive for large graphs, recent studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] have attempted to reduce the computational complexity. For instance, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>], the authors approximated <em>g<sub>&#x03B8;</sub>     </em> using a truncated expansion in terms of Chebyshev polynomials.</p>    <p>     <strong>Relation between the two domains.</strong> In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>], the authors identified the relation between convolutions in the vertex and spectral domains. That is, when the filter function <em>g<sub>&#x03B8;</sub>     </em> is approximated as an order-<em>k</em> polynomial, the spectral convolution can be interpreted as a <em>k</em>-hop diffusion convolution. This conclusion was later verified [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. Thus, by approximating <em>g<sub>&#x03B8;</sub>     </em> as an order-1 Chebyshev polynomial, after some derivation, it is equivalent to a 1-hop diffusion.</p>    <p>Our work is mainly inspired by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. In addition to adjacency matrix-based convolution, we further calculate a PPMI matrix for encoding the semantic information during the convolution. Furthermore, a new regularizer is proposed to combine the different convolutional results for better label prediction.</p>    </section>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Dual Graph Convolutional Networks</h2>    </div>    </header>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Problem Definition and an Example</h3>     </div>    </header>    <p>Following the notation in Sections <a class="sec" href="#sec-2">1</a> and <a class="sec" href="#sec-3">2</a>, the input to our model includes a set of data points <span class="inline-equation"><span class="tex">$\mathcal {X}=\lbrace x_1, ..., x_l, x_{l+1}, ..., x_{n} \rbrace$</span>     </span>, the labels {<em>y</em>     <sub>1</sub>, ..., <em>y<sub>l</sub>     </em>} for the first <em>l</em> points, and the graph structure. Assuming that each point has at most <em>k</em> features, the dataset is denoted as a matrix <span class="inline-equation"><span class="tex">$X \in \mathbb {R}^{n \times k}$</span>     </span>. As in previous studies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], the graph structure is represented by the adjacency matrix <span class="inline-equation"><span class="tex">$A \in \mathbb {R}^{n \times n}$</span>     </span>.<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> Given the input <em>X</em>, {<em>y</em>     <sub>1</sub>, ..., <em>y<sub>l</sub>     </em>}, and <em>A</em>, our model aims to predict the labels of the unlabeled points.</p>    <p>As shown in Figure <a class="fig" href="#fig1">1a</a>, we use the Karate club network [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>] as an example to visualize some intermediate results. In this example, as the data points (i.e., graph nodes) do not have any features, we initialize <em>X</em> as the identity matrix. Namely, each node is described by a different one-hot vector of length 34. Our objective is to classify all the nodes into two groups (labeled red and green) correctly. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Illustration of our proposed method. (a) shows the Karate club network [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0031">31</a>]; (b) and (c) present heatmaps of the normalized adjacency matrix <span class="inline-equation"><span class="tex">$\tilde{A}$</span>       </span> and the PPMI matrix <em>P</em>, respectively; (d), (e), and (f) present the t-stochastic neighbor embedding (SNE) distributions of the different convolved results obtained by <em>Conv<sub>A</sub>       </em> and <em>Conv<sub>P</sub>       </em>. Note that no training was conducted.</span>      </div>     </figure> </p>    <p>In the remainder of this section, we introduce our method in three steps. First, for <em>local consistency</em>, we introduce the convolutional method using the graph adjacency matrix <em>A</em>. Another convolutional method based on a random walk is then proposed to encode the semantic information for <em>global consistency</em>. Finally, we introduce a regularizer for the ensemble.</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Local Consistency Convolution: <em>Conv<sub>A</sub>       </em>      </h3>     </div>    </header>    <p>By directly utilizing the state-of-the-art method proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], we formulate the graph-structure-based convolution <em>Conv<sub>A</sub>     </em> as a type of feed-forward neural network. Given the input feature matrix <em>X</em> and adjacency matrix <em>A</em>, the output of the <em>i</em>-th hidden layer of the network <em>Z</em>     <sup>(<em>i</em>)</sup> is defined as: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} Conv_{A}^{(i)}(X)=Z^{(i)}=\sigma (\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}Z^{(i-1)}W^{(i)}). \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>     <span class="inline-equation"><span class="tex">$\tilde{A} = A+I_n$</span>     </span>, where <span class="inline-equation"><span class="tex">$I_n \in \mathbb {R}^{n \times n}$</span>     </span> is the identity matrix, is the adjacency matrix with self-loops and <span class="inline-equation"><span class="tex">$\tilde{D}_{i, i}=\sum _{j}\tilde{A}_{i, j}$</span>     </span>. Accordingly, <span class="inline-equation"><span class="tex">$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$</span>     </span> is the normalized adjacency matrix. <em>Z</em>     <sup>(<em>i</em> &#x2212; 1)</sup> is the output of the (<em>i</em> &#x2212; 1)-th layer, and <em>Z</em>     <sup>(0)</sup> = <em>X</em>. <em>W</em>     <sup>(<em>i</em>)</sup> are the trainable parameters of the network, and <em>&#x03C3;</em>(&#x00B7;) denotes an activation function (e.g., ReLU, Sigmoid).</p>    <p>In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], the authors derived Eq. (<a class="eqn" href="#eq7">7</a>) from the spectral-domain-based convolution in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>], i.e., Eq. (<a class="eqn" href="#eq6">6</a>). In this context, the parameters <em>W</em>     <sup>(<em>i</em>)</sup> in Eq. (<a class="eqn" href="#eq7">7</a>) correspond to the parameters <em>&#x03B8;</em> of the filtering function <em>g<sub>&#x03B8;</sub>     </em>. A detailed explanation is presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>].</p>    <p>The role of <span class="inline-equation"><span class="tex">$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}Z^{(i-1)}$</span>     </span> in Eq. (<a class="eqn" href="#eq7">7</a>) is to exactly conduct a 1-hop diffusion process in each layer. Namely, a node&#x0027;s feature vector is enriched by linearly adding all the feature vectors of its neighbors. This discovery inspired the proposed concept. That is, this method can be further improved by reducing the exceptions to <em>local consistency</em> in semi-supervised learning: nearby points are likely to have the same label. For example, from Figure <a class="fig" href="#fig1">1a</a>, as the directly connected data points <em>x</em>     <sub>8</sub> and <em>x</em>     <sub>30</sub> have different labels, their convolved feature vectors should not be similar. However, Eq. (<a class="eqn" href="#eq7">7</a>) cannot deal with such an exception in an effective manner.</p>    <p>To verify our idea, we visualize the Karate club network&#x0027;s convolved result, i.e., the output of <em>Conv<sub>A</sub>     </em>, using t-stochastic neighbor embeddings (SNEs) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>]. Given <em>X</em> (an identity matrix) and the normalized <span class="inline-equation"><span class="tex">$\tilde{A}$</span>     </span> (see Figure <a class="fig" href="#fig1">1b</a>), a neural network with two hidden layers is constructed and all the parameters, i.e., <em>W</em>     <sup>(1)</sup> and <em>W</em>     <sup>(2)</sup>, are randomly initialized. No training is conducted.</p>    <p>From the visualized results in Figure <a class="fig" href="#fig1">1d</a>, as expected, <em>x</em>     <sub>8</sub> and <em>x</em>     <sub>30</sub> are close together. However, they belong to different groups. To verify the proposed concept, we manually delete the edge between <em>x</em>     <sub>8</sub> and <em>x</em>     <sub>30</sub>, i.e., setting <em>A</em>[8, 30] = <em>A</em>[30, 8] = 0. As a result, Figure <a class="fig" href="#fig1">1e</a> presents the new t-SNE distribution of all 34 data points, where <em>x</em>     <sub>8</sub> and <em>x</em>     <sub>30</sub> are far apart. Hence, the attendant problem is how to automatically reduce the number of such exceptions.</p>    <p>In the next subsection, we introduce a PPMI-based convolution method. By encoding semantic information, this method allows different latent representations to be learnt for each data point. By devising an ensemble, we then automatically reduce the number of exceptions while avoiding the introduction of additional noise.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Global Consistency Convolution: <em>Conv<sub>P</sub>       </em>      </h3>     </div>    </header>    <p>In addition to the graph structure information defined by the adjacency matrix <em>A</em>, we further apply PPMI to encode the semantic information, which is denoted as a matrix <span class="inline-equation"><span class="tex">$P \in \mathbb {R}^{n \times n}$</span>     </span>. We first calculate a frequency matrix <em>F</em> using a random walk. Based on <em>F</em>, we then calculate <em>P</em> and explain why it leverages knowledge from the frequency to semantics. Finally, we define the <em>P</em>-based convolution function <em>Conv<sub>P</sub>     </em>.</p>    <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>     <strong>Calculating frequency matrix <em>F</em>.</strong> The Markov chain describing the sequence of nodes visited by a random walker is called a random walk. If the random walker is on node <em>x<sub>i</sub>     </em> at time <em>t</em>, we define the state as <em>s</em>(<em>t</em>) = <em>x<sub>i</sub>     </em>. The transition probability of jumping from the current node <em>x<sub>i</sub>     </em> to one of its neighbors <em>x<sub>j</sub>     </em> is denoted as <em>p</em>(<em>s</em>(<em>t</em> + 1) = <em>x<sub>j</sub>     </em>|<em>s</em>(<em>t</em>) = <em>x<sub>i</sub>     </em>). In our problem setting, given the adjacency matrix <em>A</em>, we assign: <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} p(s(t+1)=x_j|s(t)=x_i)=A_{i, j}/\sum _{j}A_{i, j}. \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> Algorithm 1 describes the process of calculating <em>F</em> using a random walk. The time complexity is <em>O</em>(<em>n&#x03B3;q</em>     <sup>2</sup>); as the parameters <em>&#x03B3;</em> and <em>q</em> are small integers, <em>F</em> can be calculated quickly. Furthermore, the algorithm could be parallelized by conducting several random walks simultaneously on different parts of a graph.</p>    <p>Random walks have been used as a similarity measure for a variety of problems in recommendation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>], graph classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], and semi-supervised learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. In our method, we use a random walk to calculate the semantic similarity between nodes.</p>    <p>     <strong>Calculating PPMI.</strong> After calculating the frequency matrix <em>F</em>, the <em>i</em>-th row in <em>F</em> is the row vector <em>F</em>     <sub>      <em>i</em>, :</sub> and the <em>j</em>-th column in <em>F</em> is the column vector <em>F</em>     <sub>:, <em>j</em>     </sub>. <em>F</em>     <sub>      <em>i</em>, :</sub> corresponds to a node <em>x<sub>i</sub>     </em> and <em>F</em>     <sub>:, <em>j</em>     </sub> corresponds to a context <em>c<sub>j</sub>     </em>. Based on Algorithm 1, the contexts are defined as all nodes in <span class="inline-equation"><span class="tex">$\mathcal {X}$</span>     </span>. The value of an entry <em>F</em>     <sub>      <em>i</em>, <em>j</em>     </sub> is the number of times that <em>x<sub>i</sub>     </em> occurs in context <em>c<sub>j</sub>     </em>. Based on <em>F</em>, we calculate the PPMI matrix <span class="inline-equation"><span class="tex">$P \in \mathbb {R}^{n \times n}$</span>     </span> as: <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \begin{split} p_{i, j} &#x0026; = \frac{F_{i, j}}{\sum _{i, j}F_{i, j}}; \\ p_{i, *} &#x0026;= \frac{\sum _{j}F_{i, j}}{\sum _{i, j}F_{i, j}}; \\ p_{*, j} &#x0026;= \frac{\sum _{i}F_{i, j}}{\sum _{i, j}F_{i, j}}; \\ P_{i, j} &#x0026;=max\lbrace pmi_{i, j} = log(\frac{p_{i, j}}{p_{i, *} \; p_{*, j}}), 0\rbrace . \end{split} \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>    </p>    <p>Applying Eq. (<a class="eqn" href="#eq9">9</a>) encodes the semantic information in <em>P</em>. That is, <em>p</em>     <sub>      <em>i</em>, <em>j</em>     </sub> is the estimated probability that node <em>x<sub>i</sub>     </em> occurs in context <em>c<sub>j</sub>     </em>; <em>p</em>     <sub>      <em>i</em>, *</sub> is the estimated probability of node <em>x<sub>i</sub>     </em>; and <em>p</em>     <sub>*, <em>j</em>     </sub> is the estimated probability of context <em>c<sub>j</sub>     </em>. Based on the definition of statistical independence, if <em>x<sub>i</sub>     </em> and <em>c<sub>j</sub>     </em> are independent (i.e., <em>x<sub>i</sub>     </em> occurs in <em>c<sub>j</sub>     </em> by pure random chance), then <em>p</em>     <sub>      <em>i</em>, <em>j</em>     </sub> = <em>p</em>     <sub>      <em>i</em>, *</sub>     <em>p</em>     <sub>*, <em>j</em>     </sub>, and thus <em>pmi</em>     <sub>      <em>i</em>, <em>j</em>     </sub> = 0. Accordingly, if there is a semantic relation between <em>x<sub>i</sub>     </em> and <em>c<sub>j</sub>     </em>, then <em>p</em>     <sub>      <em>i</em>, <em>j</em>     </sub> is expected to be greater than if <em>x<sub>i</sub>     </em> and <em>c<sub>j</sub>     </em> are independent. Hence, when <em>p</em>     <sub>      <em>i</em>, <em>j</em>     </sub> > <em>p</em>     <sub>      <em>i</em>, *</sub>     <em>p</em>     <sub>*, <em>j</em>     </sub>, <em>pmi</em>     <sub>      <em>i</em>, <em>j</em>     </sub> should be positive. If node <em>x<sub>i</sub>     </em> is unrelated to context <em>c<sub>j</sub>     </em>, <em>pmi</em>     <sub>      <em>i</em>, <em>j</em>     </sub> may be negative. As we are focusing on pairs (<em>x<sub>i</sub>     </em>, <em>c<sub>j</sub>     </em>) that have a semantic relation, our method uses a nonnegative <em>pmi</em>.</p>    <p>PPMI has been extensively investigated in terms of natural language processing (NLP) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>]. Indeed, the PPMI metric is known to perform well on semantic similarity tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. However, to the best of our knowledge, we are the first to introduce PPMI to the field of graph-based semi-supervised learning. Furthermore, using a novel PPMI-based convolution, our method applies the concept of <em>global consistency</em>: graph nodes that occur in similar contexts tend to have the same label.</p>    <p>Figure <a class="fig" href="#fig1">1c</a> visualizes the normalized PPMI matrix <em>P</em> of the Karate club network. Compared with the adjacency matrix of this network (shown in Figure <a class="fig" href="#fig1">1b</a>), there are at least two obvious differences: (1) <em>P</em> has reduced the effect of the hub nodes, e.g., <em>x</em>     <sub>0</sub> and <em>x</em>     <sub>33</sub>; and (2) <em>P</em> has initiated more latent relations among different data points, which cannot be characterized by the adjacency matrix <em>A</em>.</p>    <p>     <strong>PPMI-based convolution.</strong> In addition to the convolution <em>Conv<sub>A</sub>     </em>, which is based on the similarity defined by the adjacency matrix <em>A</em>, another feed-forward neural network <em>Conv<sub>P</sub>     </em> is derived from the similarity defined by the PPMI matrix <em>P</em>. This convolutional neural network is given by: <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{equation} Conv_{P}^{(i)}(X)=Z^{(i)}=\sigma (D^{-\frac{1}{2}}PD^{-\frac{1}{2}}Z^{(i-1)}W^{(i)}), \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div>    </p>    <p> where <em>P</em> is the PPMI matrix and <em>D</em>     <sub>      <em>i</em>, <em>i</em>     </sub> = &#x2211;<sub>      <em>j</em>     </sub>     <em>P</em>     <sub>      <em>i</em>, <em>j</em>     </sub> for normalization. Obviously, applying diffusion based on such a node-contextual matrix <em>P</em> ensures <em>global consistency</em>. Additionally, by using the same neural network structure as <em>Conv<sub>A</sub>     </em>, the two can be combined very concisely.</p>    <p>Figure <a class="fig" href="#fig1">1f</a> presents the t-SNE distribution of the output of <em>Conv<sub>P</sub>     </em> applied to the Karate club network. We find that <em>x</em>     <sub>8</sub> and <em>x</em>     <sub>30</sub> are now slightly farther away from each other. A more encouraging result is that, compared with the results shown in Figures <a class="fig" href="#fig1">1d</a> and <a class="fig" href="#fig1">1e</a>, <em>Conv<sub>P</sub>     </em> has correctly classified data points <em>x</em>     <sub>0</sub> and <em>x</em>     <sub>14</sub>. However, <em>x</em>     <sub>16</sub> is not assigned an ideal latent representation by <em>Conv<sub>P</sub>     </em>. Accordingly, in the next subsection, we introduce a novel ensemble method to jointly consider both the local and global consistencies.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Ensemble of Local and Global Consistencies</h3>     </div>    </header>    <p>To jointly consider the <em>local consistency</em> and <em>global consistency</em> for semi-supervised learning, we must overcome the challenge of having very few labeled training data. That is, as the training data are limited, a general ensemble method (e.g., by concatenating the output of <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>) cannot be utilized. In Appendix <a class="sec" href="#sec-18">A</a>, we discuss this issue in detail. The lack of training data causes the ensemble method introduced in Appendix <a class="sec" href="#sec-18">A</a> to achieve worse performance than non-ensemble methods. Hence, in addition to supervised learning using training data, we further derive an unsupervised regularizer for the ensemble.</p>    <p>Figure <a class="fig" href="#fig2">2</a> presents the architecture of our dual graph convolutional networks method. In addition to training <em>Conv<sub>A</sub>     </em> using the labeled data (i.e., <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_A)$</span>     </span> in Eq. (<a class="eqn" href="#eq3">3</a>)), an unsupervised regularizer (i.e., <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}(Conv_A, Conv_P)$</span>     </span> in Eq. (<a class="eqn" href="#eq3">3</a>)) is introduced to train <em>Conv<sub>P</sub>     </em> against the posterior probabilities of a previously trained model, i.e., the trained <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_A)$</span>     </span>. To explain this self-ensembling method, the remainder of this section describes the calculation of <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_A)$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}(Conv_A, Conv_P)$</span>     </span> sequentially. The learning algorithm is then introduced. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Architecture of our method. Inputs are: <span class="inline-equation"><span class="tex">$X \in \mathbb {R}^{n \times k}$</span>       </span>, masked <span class="inline-equation"><span class="tex">$Y \in \mathbb {R}^{n \times c}$</span>       </span>, <span class="inline-equation"><span class="tex">$\tilde{A} \in \mathbb {R}^{n \times n}$</span>       </span>, <span class="inline-equation"><span class="tex">$P \in \mathbb {R}^{n \times n}$</span>       </span>, and time <em>t</em>.</span>      </div>     </figure> </p>    <p>     <strong>Calculating </strong>     <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_A)$</span>     </span>     <strong>.</strong> Assuming there are <em>c</em> different labels for prediction, the <em>softmax</em> activation function is applied row-wise to the output <span class="inline-equation"><span class="tex">$Z^A \in \mathbb {R}^{n \times c}$</span>     </span> given by <em>Conv<sub>A</sub>     </em>. The output of the <em>softmax</em> layer is denoted as <span class="inline-equation"><span class="tex">$\hat{Z}^A \in \mathbb {R}^{n \times c}$</span>     </span>. <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_A)$</span>     </span>, which evaluates the cross-entropy error over all labeled data points, is calculated as: <div class="table-responsive" id="eq11">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathcal {L}_{0}(Conv_A)=-\frac{1}{\mid \mathcal {Y}_L \mid }\sum _{l \in \mathcal {Y}_L}\sum _{i=1}^{c}Y_{l, i}ln\hat{Z}^{A}_{l, i}, \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathcal {Y}_L$</span>     </span> is the set of data indices whose labels are observed for training and <span class="inline-equation"><span class="tex">$Y \in \mathbb {R}^{n \times c}$</span>     </span> is the ground truth.</p>    <p>     <strong>Calculating </strong>     <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}(Conv_A, Conv_P)$</span>     </span>     <strong>.</strong> The calculation of <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}$</span>     </span> is given by: <div class="table-responsive" id="eq12">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathcal {L}_{reg}(Conv_A, Conv_P)=\frac{1}{n}\sum _{i=1}^{n}\parallel \hat{Z}^P_{i, :}-\hat{Z}^A_{i, :}\parallel ^2. \end{equation} </span>       <br/>       <span class="equation-number">(12)</span>      </div>     </div> Similar to <em>Conv<sub>A</sub>     </em>, after applying the <em>softmax</em> activation function, the output of <em>Conv<sub>P</sub>     </em> is denoted as <span class="inline-equation"><span class="tex">$\hat{Z}^P \in \mathbb {R}^{n \times c}$</span>     </span>. Over all <em>n</em> data points, we introduce an unsupervised loss function that minimizes the mean squared differences between <span class="inline-equation"><span class="tex">$\hat{Z}^P$</span>     </span> and <span class="inline-equation"><span class="tex">$\hat{Z}^A$</span>     </span>.</p>    <p>By looking at the formulation of Eq. (<a class="eqn" href="#eq12">12</a>), we could regard the unsupervised loss function as training <em>Conv<sub>P</sub>     </em> against <em>Conv<sub>A</sub>     </em>. That is, after the <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}$</span>     </span>-based training (i.e., Eq. (<a class="eqn" href="#eq11">11</a>)), the softmaxed scores in <span class="inline-equation"><span class="tex">$\hat{Z}^A \in \mathbb {R}^{n \times c}$</span>     </span> are then interpreted as a posterior distribution over the <em>c</em> labels. By minimizing the loss function in Eq. (<a class="eqn" href="#eq12">12</a>), despite different transformations by <em>Conv<sub>A</sub>     </em>, <em>Conv<sub>P</sub>     </em>, and random layer-wise dropout, the final predictions given by each model should then be the same.</p>    <p>As shown in Figure <a class="fig" href="#fig2">2</a>, the key to our model is to share the model parameters (i.e., neural network weights <em>W</em> in Eqs. (<a class="eqn" href="#eq7">7</a>) and (<a class="sec" href="#sec-9">10</a>)) in <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>. By doing so, our model can jointly consider the opinions of both <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>. Although sharing the same parameters <em>W</em>, the different diffusions (i.e., <em>A</em> and <em>P</em>) and random dropout may cause the predictions of <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em> (i.e., <span class="inline-equation"><span class="tex">$\hat{Z}^A$</span>     </span> and <span class="inline-equation"><span class="tex">$\hat{Z}^P$</span>     </span>) to differ. However, we know that each data point is assigned to only one class. Therefore, the model (which is characterized by the parameters <em>W</em>) is expected to give the same prediction from <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>, i.e., minimizing Eq. (<a class="eqn" href="#eq12">12</a>). As a result, the trained parameters <em>W</em> have considered the opinions from both <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>.</p>    <p>We are aware that a recently proposed transformation/stability loss [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>] is based on a similar principle as our ensemble method. By explicitly incorporating the prior knowledge (i.e., the diffusion matrices <em>A</em> and <em>P</em> in our method) during the data transformation stage, our ensemble method can be regarded as a further extension of theirs. Namely, by using multiple neural networks, different prior knowledge can be embedded during the data transformation stage.</p>    <p>     <strong>The final model.</strong> Algorithm 2 describes the training process of our dual graph convolutional networks method. The loss function is defined as a weighted sum of <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}(Conv_A)$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {L}_{reg}(Conv_A, Conv_P)$</span>     </span>. A dynamic weight function is devised to implement the idea described above. That is, at the beginning of the training process (i.e., small <em>t</em>), the loss function is mainly dominated by the supervised entry <span class="inline-equation"><span class="tex">$\mathcal {L}_{0}$</span>     </span>. After obtaining a posterior distribution over the labels using <em>Conv<sub>A</sub>     </em>, increasing <em>&#x03BB;</em>(<em>t</em>) forces our model to simultaneously consider the knowledge encoded in <em>Conv<sub>P</sub>     </em>.</p>    <p>As our method consists of two simple feed-forward neural networks, conventional parameter update strategies can be applied. Instead of Stochastic Gradient Decent (SGD), which updates the parameters for each training example, our implementation uses Batch Gradient Decent (BGD), in which the full training dataset is used for every training iteration. Although BGD is relatively slow, it is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces. In the case of very large training datasets that cannot be fully loaded into memory, SGD or Minibatch Gradient Decent are good memory-efficient extensions. A recent survey [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>] discusses the different gradient decent methods in detail.</p>    </section>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>    </div>    </header>    <p>In this section, we present the results from several experiments to verify the performance of our method in graph-based semi-supervised learning tasks. We first introduce the five datasets used in the experiments, and then list the comparative methods and their implementation details. Finally, we present the experimental results and discuss the advantages and limitations of our method. Furthermore, in Appendix 1, we present and discuss some variants of our method for reference.</p>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>    </header>    <p>For comparison, we use the same datasets employed in previous studies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. Specifically, there are three citation network datasets (i.e., Citeseer, Cora, and Pubmed) and one knowledge graph dataset (i.e., NELL). In addition, we constructed a simplified NELL dataset for further verification. Table <a class="tbl" href="#tab1">1</a> presents an overview of the five datasets; detailed descriptions are given below.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Overview of the Five Datasets.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Dataset</th>       <th style="text-align:center;">#Nodes</th>       <th style="text-align:center;">#Features</th>       <th style="text-align:center;">#Edges</th>       <th style="text-align:center;">#Classes</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Citeseer</td>       <td style="text-align:center;">3,327</td>       <td style="text-align:center;">3,703</td>       <td style="text-align:center;">4,732</td>       <td style="text-align:center;">6</td>       </tr>       <tr>       <td style="text-align:center;">Cora</td>       <td style="text-align:center;">2,708</td>       <td style="text-align:center;">1,433</td>       <td style="text-align:center;">5,429</td>       <td style="text-align:center;">7</td>       </tr>       <tr>       <td style="text-align:center;">Pubmed</td>       <td style="text-align:center;">19,717</td>       <td style="text-align:center;">500</td>       <td style="text-align:center;">44,338</td>       <td style="text-align:center;">3</td>       </tr>       <tr>       <td style="text-align:center;">NELL</td>       <td style="text-align:center;">65,755</td>       <td style="text-align:center;">61,278</td>       <td style="text-align:center;">266,144</td>       <td style="text-align:center;">210</td>       </tr>       <tr>       <td style="text-align:center;">Simplified NELL</td>       <td style="text-align:center;">9,891</td>       <td style="text-align:center;">5,414</td>       <td style="text-align:center;">13,142</td>       <td style="text-align:center;">210</td>       </tr>      </tbody>     </table>    </div>    <p>    <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-img2.svg" class="img-responsive" alt="" longdesc=""/>    </p>     <strong>Citeseer.</strong> The Citeseer dataset contains 3,327 scientific publications classified into one of six classes. The citation network consists of 4,732 links. Each publication in this dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from a dictionary consisting of 3,703 unique words. Only 3.6% of the nodes are labeled for training.</p>    <p>     <strong>Cora.</strong> Similar to the Citeseer dataset, Cora contains 2,708 scientific publications classified into one of seven classes. The citation network consists of 5,429 links. Each node is described by a 1,433-dimensional 0/1-valued vector. Only 5.2% of the nodes are labeled for training.</p>    <p>     <strong>Pubmed.</strong> The Pubmed dataset contains 19,717 scientific publications classified into one of three classes. The citation network consists of 44,338 links. Each publication is described by a Term Frequency&#x2013;Inverse Document Frequency (TF-IDF) vector drawn from a dictionary with 500 terms. Only 0.3% of the nodes are labeled for training.</p>    <p>     <strong>NELL.</strong> The NELL dataset is extracted from the Never Ending Language Learning (NELL) knowledge graph [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]. By linking the selected NELL entities (9,891 in total) with text descriptions in ClueWeb09 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>], each relation in NELL is described as a triplet (<em>e<sub>h</sub>     </em>, <em>r</em>, <em>e<sub>t</sub>     </em>). <em>e<sub>h</sub>     </em> and <em>e<sub>t</sub>     </em> are the head and tail entity vectors, respectively, and <em>r</em> indicates the relation between them. By splitting each (<em>e<sub>h</sub>     </em>, <em>r</em>, <em>e<sub>t</sub>     </em>) into two edges (<em>e<sub>h</sub>     </em>, <em>r</em>     <sub>1</sub>) and (<em>r</em>     <sub>2</sub>, <em>e<sub>t</sub>     </em>), we obtain a graph of 65,755 nodes (i.e., the total number of both entity and relation nodes) and 266,144 edges. By assigning a different one-hot vector to each relation node, the length of each feature vector is 5, 414 + 55, 864 = 61, 278. Only a single data point per class is labeled for training.</p>    <p>     <strong>Simplified NELL.</strong> In simplified NELL, the relation information (i.e., <em>r</em>) has been removed and edges among entities have been directly added. By counting the co-occurrences of each (<em>e<sub>h</sub>     </em>, <em>e<sub>t</sub>     </em>) pair in all triplets, a weighted adjacency matrix <em>A</em> is constructed. After removing edges with small weights, we obtain a graph of 9,891 nodes (i.e., the number of all entities) and 13,142 edges. The simplified NELL dataset is intended to further verify that our dual convolutional networks-based method is more robust than the baselines (see Section <a class="sec" href="#sec-14">4.3</a>).</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Methods for Comparison</h3>     </div>    </header>    <p>We require several state-of-the-art baselines for comparison. As most of the baselines have several hyper-parameters requiring fine-grained tuning, we selected baselines with public source code. As different baselines use different strategies to embed the graph knowledge, we ensured our baseline set had sufficient diversity. As a result, the following methods were selected.</p>    <ul class="list-no-style">     <li id="list3" label="&#x2022;"><strong>DGCN.</strong> This is the proposed method, as described in Algorithm 2 . In our Dual Graph Convolutional Networks (DGCN) implementation, both <em>Conv<sub>A</sub>      </em> and <em>Conv<sub>P</sub>      </em> have two hidden layers. Namely, there are two separate <em>W</em> vectors, <em>W</em>      <sup>(1)</sup> and <em>W</em>      <sup>(2)</sup>, that need training in Algorithm 2 . Table <a class="tbl" href="#tab2">2</a> presents detailed information about the implementation of our method for each dataset, including (1) size of the hidden layer; (2) layer-wise dropout rate; (3) window size <em>w</em> in Algorithm 1 ; and (4) learning rate <em>&#x03B7;</em>. A detailed discussion of the temporal regularization weight <em>&#x03BB;</em>(<em>t</em>) in Algorithm 2 is given in Section <a class="sec" href="#sec-15">4.4</a>. Our source code is available<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> for reference.<br/></li>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Values of the Hyper-Parameters in our DGCN.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Dataset</th>       <th style="text-align:center;">        <em>W</em>        <sup>(1)</sup>       </th>       <th style="text-align:center;">        <em>W</em>        <sup>(2)</sup>       </th>       <th style="text-align:center;">Dropout rate</th>       <th style="text-align:center;">        <em>w</em>       </th>       <th>        <em>&#x03B7;</em>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Citeseer, Cora,</td>       <td/>       <td/>       <td/>       <td/>       <td/>       </tr>       <tr>       <td style="text-align:center;">Pubmed</td>       <td style="text-align:center;">32</td>       <td style="text-align:center;">6, 7, 3</td>       <td style="text-align:center;">10%</td>       <td style="text-align:center;">2</td>       <td>0.05</td>       </tr>       <tr>       <td style="text-align:center;">NELL</td>       <td style="text-align:center;">64</td>       <td style="text-align:center;">210</td>       <td style="text-align:center;">10%</td>       <td style="text-align:center;">2</td>       <td>0.002</td>       </tr>       <tr>       <td style="text-align:center;">Simplified NELL</td>       <td style="text-align:center;">96</td>       <td style="text-align:center;">210</td>       <td style="text-align:center;">30%</td>       <td style="text-align:center;">2</td>       <td>0.001</td>       </tr>      </tbody>     </table>    </div> 			     <li id="list4" label="&#x2022;"><strong>GCN.</strong> The Graph Convolutional Networks (GCN) method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>] is a state-of-the-art technique that has obtained the highest performance among the baselines. It is derived from the related work of conducting graph convolutions in the spectral domain (i.e., Eq. (<a class="eqn" href="#eq6">6</a>)) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>]. Our DGCN method is inspired by GCN. Obviously, DGCN would collapse into GCN if we set <em>&#x03BB;</em>(<em>t</em>) = 0. The source code for GCN is publicly available<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>.<br/></li>     <li id="list5" label="&#x2022;"><strong>PLANETOID.</strong> Inspired by the Skipgram model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0018">18</a>] from NLP, PLANETOID [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0030">30</a>] embeds the graph information using positive and negative samplings. During sampling, both the label information and graph structure are considered. As PLANETOID can be conducted in inductive and transductive manners, we report the better results. The source code for PLANETOID is publicly available<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>.<br/></li>     <li id="list6" label="&#x2022;"><strong>DeepWalk.</strong> By taking random walks on a graph, different paths are generated. By regarding the paths as &#x201C;sentences,&#x201D; DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0021">21</a>] generalizes language modeling techniques from sequences of words to paths in a graph. As our method also uses a random walk to calculate the PPMI matrix, this method represents an important comparison. The source code for DeepWalk source code is publicly available<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>.<br/></li>    </ul>    <p>In addition to the state-of-the-art baselines described above, we compared some variants of our proposed method. For brevity, these self-comparisons are given in Appendix <a class="sec" href="#sec-18">A</a>.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Results</h3>     </div>    </header>    <p>Similar to the studies describing the baselines [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], our comparison uses the classification accuracy metric for quantitative evaluation. Table <a class="tbl" href="#tab3">3</a> summarizes the experimental results over the three citation network datasets (Citeseer, Cora, and Pubmed) and the knowledge graph dataset (NELL). Using the public source code of the comparative methods, we directly executed their programs and obtained the classification results reported in the table.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Validation accuracies of all Methods: DGCN, GCN, PLANETOID, and DeepWalk.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">        <strong>Method</strong>       </th>       <th style="text-align:center;">        <strong>Citeseer</strong>       </th>       <th style="text-align:center;">        <strong>Cora</strong>       </th>       <th style="text-align:center;">        <strong>Pubmed</strong>       </th>       <th>        <strong>NELL</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0021">21</a>]</td>       <td style="text-align:center;">43.2%</td>       <td style="text-align:center;">67.2%</td>       <td style="text-align:center;">65.3%</td>       <td>58.1%</td>       </tr>       <tr>       <td style="text-align:center;">PLANETOID [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0030">30</a>]</td>       <td style="text-align:center;">64.7%</td>       <td style="text-align:center;">75.7%</td>       <td style="text-align:center;">77.2%</td>       <td>61.9%</td>       </tr>       <tr>       <td style="text-align:center;">GCN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0015">15</a>]</td>       <td style="text-align:center;">70.3%</td>       <td style="text-align:center;">81.5%</td>       <td style="text-align:center;">79.0%</td>       <td>66.0%</td>       </tr>       <tr>       <td style="text-align:center;">DGCN (our method)</td>       <td style="text-align:center;">        <strong>72.6%</strong>       </td>       <td style="text-align:center;">        <strong>83.5%</strong>       </td>       <td style="text-align:center;">        <strong>80.0%</strong>       </td>       <td>        <strong>74.2%</strong>       </td>       </tr>      </tbody>     </table>    </div>    <p>On the basis of Table <a class="tbl" href="#tab3">3</a>, the results are encouraging. That is, over all four datasets, our DGCN method outperformed all of the baselines. Specifically, the random walk based method (i.e., DeepWalk) did not perform well on graphs with a relatively low average degree. For example, on the Citeseer dataset (average degree of 2.84), DeepWalk achieved low classification accuracy. Comparing PLANETOID and GCN, although they use a similar strategy of jointly considering both the graph structure and label information for knowledge embeddings, GCN produced better performance over all the datasets. One likely explanation is that, because of its sampling strategy, PLANETOID cannot fully embed all of the knowledge. For example, as the NELL dataset has 210 classes, the label-related knowledge derived by sampling would be sparse. As a result, PLANETOID achieved low accuracy on the NELL dataset.</p>    <p>In addition to the graph structure (i.e., local consistency) and label information, DGCN further considers the semantic context (i.e., global consistency) for each graph node. Compared with GCN, a PPMI matrix-based graph convolution improves the classification performance. By introducing an unsupervised loss function (Eq. (<a class="eqn" href="#eq12">12</a>)), two convolutional classifiers are combined in an ensemble manner. As a result, the model is more robust. The experimental results verify our claim, especially on the highly sparse NELL dataset, where our method outperformed the other baselines by a margin of 8.2%.</p>    <p>To further verify the robustness of our method, Table <a class="tbl" href="#tab4">4</a> compares DGCN and GCN on the new simplified NELL dataset. As discussed in Section <a class="sec" href="#sec-12">4.1</a>, the relation information has been removed in the simplified NELL dataset. Furthermore, by directly adding edges among entities, there are many more noisy edges. Thus, the performance with this dataset will verify which method is more resistant to noise and sparsity.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Comparison of DGCN and GCN on Simplified NELL.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">% Labeled</th>       <th style="text-align:center;">1%</th>       <th style="text-align:center;">5%</th>       <th style="text-align:center;">10%</th>       <th style="text-align:center;">15%</th>       <th style="text-align:center;">20%</th>       <th style="text-align:center;">25%</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">DGCN</td>       <td style="text-align:center;">26.0%</td>       <td style="text-align:center;">62.6%</td>       <td style="text-align:center;">70.4%</td>       <td style="text-align:center;">70.6%</td>       <td style="text-align:center;">72.0%</td>       <td style="text-align:center;">72.8%</td>       </tr>       <tr>       <td style="text-align:center;">GCN</td>       <td style="text-align:center;">20.4%</td>       <td style="text-align:center;">53.0%</td>       <td style="text-align:center;">63.4%</td>       <td style="text-align:center;">64.6%</td>       <td style="text-align:center;">68.6%</td>       <td style="text-align:center;">69.8%</td>       </tr>       <tr>       <td style="text-align:center;">Margin</td>       <td style="text-align:center;">+5.6%</td>       <td style="text-align:center;">+9.6%</td>       <td style="text-align:center;">+7.0%</td>       <td style="text-align:center;">+6.0%</td>       <td style="text-align:center;">+3.4%</td>       <td style="text-align:center;">+3.0%</td>       </tr>      </tbody>     </table>    </div>    <p>In Table <a class="tbl" href="#tab4">4</a>, the classification accuracies are presented with respect to different percentages of data points that were labeled for training. For a labeling rate of 1%, more than half of the 210 classes have no training data, and both DGCN and GCN obtained low accuracies. As the labeling percentage increased, the accuracy margin between GCN and DGCN became smaller. Namely, when there are not enough training data, DGCN performed much better than GCN. In other words, by introducing the PPMI matrix-based convolutional network and the unsupervised ensemble regularizer, our method is more robust in difficult situations, such as few training data, noise, and high sparsity.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Effect of Regularization Weight <em>&#x03BB;</em>(<em>t</em>)</h3>     </div>    </header>    <p>In line 14 of Algorithm 2, DGCN uses a temporal weight function <em>&#x03BB;</em>(<em>t</em>) to balance the trade-off between the supervised and unsupervised loss functions. In our implementations, we devised several different weight functions. Through a series of comparisons, we attempted to (1) identify the best classification performance and (2) verify the idea introduced in Section <a class="sec" href="#sec-10">3.4</a>.    </p>    <p>Figure <a class="fig" href="#fig3">3</a> shows the shapes of the different weight functions. The X-axis represents the function variable <em>t</em> and the Y-axis represents the function value. During training, <em>t</em> is defined as the number of epochs. The maximum function value is fixed to be proportional to the training rate. Functions <em>f</em>1, <em>f</em>2, and <em>f</em>3 have different incremental gradients, but reach the maximum value at the same epoch. Function <em>f</em>4 reaches the maximum value later than <em>f</em>1, <em>f</em>2, and <em>f</em>3. Unlike the other functions, the value of <em>f</em>5 decreases as the number of epochs increases. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Five different implementations of the weight function <em>&#x03BB;</em>(<em>t</em>) in Algorithm 2.</span>      </div>     </figure> </p>    <p>Figure <a class="fig" href="#fig4">4</a> shows the classification accuracy using the different weight functions as a function of the training epoch. From the similar performance obtained by <em>f</em>1, <em>f</em>2, and <em>f</em>3, generally speaking, our temporal ensemble method is stable enough to cope with different definitions of the weight. From the performance of all five functions, the classification accuracy clearly has a positive correlation with the function value. Namely, our regularizer embeds additional knowledge learned from the PPMI-based convolution <em>Conv<sub>P</sub>     </em>.     <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Classification accuracy using different weight functions for training. Dataset: Citeseer.</span>      </div>     </figure> </p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Effect of a Shifted PPMI Matrix <em>P</em>      </h3>     </div>    </header>    <p>A further evaluation was conducted to examine whether the performance of our method could be further improved by a shifted PPMI matrix <em>P</em> and to verify that our ensemble method can effectively embed the knowledge from <em>P</em>. <div class="table-responsive" id="eq13">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \forall P_{i, j} \in P, \; \; \; P_{i, j}=max\lbrace P_{i, j}-log(k), 0\rbrace . \end{equation} </span>       <br/>       <span class="equation-number">(13)</span>      </div>     </div>    </p>    <p>Eq. (<a class="eqn" href="#eq13">13</a>) presents the calculation of a shifted PPMI matrix, first introduced in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] for word embedding. On the basis of the derivation in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>], the value of <em>k</em> indicates the number of negative samplings required to calculate each entry of <em>P</em>. In research on semi-supervised learning, we are the first to verify whether such a shift can also be applied to understand a graph. Interesting results are observed in our experiments.</p>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Classification Accuracy Using Different Shifted PPMI Matrixes <em>P</em> in DGCN Method.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;"/>       <th style="text-align:center;">        <strong>Citeseer</strong>       </th>       <th style="text-align:center;">        <strong>Cora</strong>       </th>       <th style="text-align:center;">        <strong>Pubmed</strong>       </th>       <th>        <strong>NELL</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <em>k</em> = 1.0</td>       <td style="text-align:center;">72.6%</td>       <td style="text-align:center;">83.5%</td>       <td style="text-align:center;">80.0%</td>       <td>74.2%</td>       </tr>       <tr>       <td style="text-align:left;">        <em>k</em> = 2.0</td>       <td style="text-align:center;">72.8%</td>       <td style="text-align:center;">83.6%</td>       <td style="text-align:center;">80.1%</td>       <td>74.4%</td>       </tr>       <tr>       <td style="text-align:left;">        <em>k</em> = 5.0</td>       <td style="text-align:center;">72.9%</td>       <td style="text-align:center;">83.4%</td>       <td style="text-align:center;">79.9%</td>       <td>        <strong>75.2%</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">        <em>k</em> = 10.0</td>       <td style="text-align:center;">        <strong>73.2%</strong>       </td>       <td style="text-align:center;">83.3%</td>       <td style="text-align:center;">79.8%</td>       <td>74.5%</td>       </tr>       <tr>       <td style="text-align:left;">        <em>k</em> = 100.0</td>       <td style="text-align:center;">72.3%</td>       <td style="text-align:center;">83.2%</td>       <td style="text-align:center;">79.7%</td>       <td>73.7%</td>       </tr>      </tbody>     </table>    </div>    <p>Table <a class="tbl" href="#tab5">5</a> presents the classification accuracy on all datasets for different values of <em>k</em>. When <em>k</em> = 1.0, no shifts are conducted on <em>P</em>. An interesting observation is that, compared with the Cora and Pubmed datasets, the shifted PPMI matrix allows our method to obtain significant improvements on the Citeseer and NELL datasets. As the classification performance on these datasets is relatively low (around 70% accuracy), there is a high probability that their given graphs have more noise than those associated with the Cora and Pubmed datasets (around 80% accuracy). By shifting the PPMI matrices of the Citeseer and NELL graphs (namely, conducting more negative samplings), the noise is reduced to a certain degree.</p>    <p>This observation proves that our ensemble method embeds useful knowledge from <em>Conv<sub>P</sub>     </em>. In related work, the adjacency matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], positive &#x0026; negative sampling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], and random walks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] are utilized to understand a graph. In our work, we have verified that the PPMI matrix can further be used to encode the global consistency of a graph.</p>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusions</h2>    </div>    </header>    <p>In this paper, we have proposed a Dual Graph Convolutional Network method for graph-based semi-supervised learning. In addition to the often-considered local consistency, our method uses the PPMI to encode the global consistency. To jointly consider both the local and global consistencies, a dual neural network structure has been devised for the ensemble. Experiments on a variety of public datasets illustrate the effectiveness of our method for solving classification tasks. In the appendix, some variants of DGCN are derived to provide a deeper insight into our idea.</p>    <p>Our work provides a solution for combining the prior knowledge learned from different views of raw data. As a reasonable extension, in future work, we will seek more ways of understanding a graph. In other research, we will also investigate whether our method could be applied in research fields such as domain adaptation learning. The source and target domains&#x2019; knowledge could be jointly embedded in our dual convolutional networks for adaptation.</p>   </section>  </section>  <section class="back-matter">   <Appendix>    <section id="sec-18">    <header>     <div class="title-info">      <h2>       <span class="section-number">A</span> Variants of Our Method</h2>     </div>    </header>    <p>In this appendix, some variants of our method are derived for further comparison. The methods are as follows.</p>    <ul class="list-no-style">     <li id="list7" label="&#x2022;"><strong>DGCN:</strong> The original method used in our experiments.<br/></li>     <li id="list8" label="&#x2022;"><strong>DGCN-1:</strong> When the neural network parameters (i.e., <em>W</em>      <sup>(<em>h</em>)</sup>s) are not shared between <em>Conv<sub>A</sub>      </em> and <em>Conv<sub>P</sub>      </em>.<br/></li>     <li id="list9" label="&#x2022;"><strong>DGCN-2:</strong> Instead of Eq. (<a class="eqn" href="#eq12">12</a>), we use concatenation to form the ensemble. Namely, the final predictions are derived from the latent representations: <em>Conv<sub>A</sub>      </em>&#x2295;<em>Conv<sub>P</sub>      </em>. Before <em>softmax</em>, a dense layer is added to ensure the same shape as the label matrix <span class="inline-equation"><span class="tex">$Y \in \mathbb {R}^{n \times c}$</span>      </span>.<br/></li>     <li id="list10" label="&#x2022;"><strong>DGCN-3:</strong> When the parameters are not shared between <em>Conv<sub>A</sub>      </em> and <em>Conv<sub>P</sub>      </em>, and concatenation is used.<br/></li>     <li id="list11" label="&#x2022;"><strong>DGCN-4:</strong> Without ensemble, when only <em>Conv<sub>P</sub>      </em> is used.<br/></li>    </ul>    <p>Figure <a class="fig" href="#fig5">5</a> shows the training processes of all the methods on the NELL dataset. For DGCN-1 and DGCN-2, there is a significant decrease in accuracy during training. Such a decline is very likely to have been caused by the conflicts between <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em>. In other words, both the sharing parameters and unsupervised regularizer are necessary for the ensemble when the number of training data is limited. As the parameters are not shared, DGCN-3 can be regarded as a linear combination of GCN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>] and <em>Conv<sub>P</sub>     </em>. DGCN-3 obtained similar performance to GCN. Accordingly, this observation has verified that jointly considering both <em>Conv<sub>A</sub>     </em> and <em>Conv<sub>P</sub>     </em> during training (i.e., DGCN) can significantly improve the linear DGCN-3. By only using <em>Conv<sub>P</sub>     </em>, DGCN-4 obtained a bit of better performance than DGCN-3.    <figure id="fig5">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186116/images/www2018-125-fig5.jpg" class="img-responsive" alt="Figure 5"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Training processes of the different variants of our method. Dataset: NELL.</span>     </div>    </figure> 			 </p>    <p>Regarding the number of hidden layers in <em>Conv</em>     <sub>*</sub>, as the effect of this parameter has been investigated elsewhere in detail [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], we omit this discussion here.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work is partly supported by JSPS KAKENHI (16K12532, 15J01402) and MIC SCOPE (172307001).</p>    </section>   </Appendix>   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning using pagerank vectors. In <em>      <em>the 47th Annual IEEE Symposium on Foundations of Computer Science</em>     </em>. 475&#x2013;486.</li>    <li id="BibPLXBIB0002" label="[2]">James Atwood and Don Towsley. 2016. Diffusion-convolutional neural networks. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 1993&#x2013;2001.</li>    <li id="BibPLXBIB0003" label="[3]">Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. <em>      <em>Journal of machine learning research</em>     </em>7, Nov (2006), 2399&#x2013;2434.</li>    <li id="BibPLXBIB0004" label="[4]">John&#x00A0;A Bullinaria and Joseph&#x00A0;P Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. <em>      <em>Behavior research methods</em>     </em>39, 3 (2007), 510&#x2013;526.</li>    <li id="BibPLXBIB0005" label="[5]">Deng Cai, Xiaofei He, Jiawei Han, and Thomas&#x00A0;S Huang. 2011. Graph regularized nonnegative matrix factorization for data representation. <em>      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>     </em>33, 8(2011), 1548&#x2013;1560.</li>    <li id="BibPLXBIB0006" label="[6]">Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam&#x00A0;R Hruschka&#x00A0;Jr, and Tom&#x00A0;M Mitchell. 2010. Toward an Architecture for Never-Ending Language Learning. In <em>      <em>Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence</em>     </em>, Vol.&#x00A0;5.</li>    <li id="BibPLXBIB0007" label="[7]">Bhavana Dalvi, Aditya Mishra, and William&#x00A0;W Cohen. 2016. Hierarchical semi-supervised classification with incomplete class hierarchies. In <em>      <em>Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</em>     </em>. 193&#x2013;202.</li>    <li id="BibPLXBIB0008" label="[8]">Maximilien Danisch, T-H&#x00A0;Hubert Chan, and Mauro Sozio. 2017. Large Scale Density-friendly Graph Decomposition via Convex Programming. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web</em>     </em>. 233&#x2013;242.</li>    <li id="BibPLXBIB0009" label="[9]">Micha&#x00EB;l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 3844&#x2013;3852.</li>    <li id="BibPLXBIB0010" label="[10]">Fran&#x00E7;ois Fouss, Kevin Francoisse, Luh Yen, Alain Pirotte, and Marco Saerens. 2012. An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification. <em>      <em>Neural networks</em>     </em>31(2012), 53&#x2013;72.</li>    <li id="BibPLXBIB0011" label="[11]">Francois Fouss, Alain Pirotte, Jean-Michel Renders, and Marco Saerens. 2007. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. <em>      <em>IEEE Transactions on knowledge and data engineering</em>     </em>19, 3(2007), 355&#x2013;369.</li>    <li id="BibPLXBIB0012" label="[12]">David&#x00A0;K Hammond, Pierre Vandergheynst, and R&#x00E9;mi Gribonval. 2011. Wavelets on graphs via spectral graph theory. <em>      <em>Applied and Computational Harmonic Analysis</em>     </em>30, 2 (2011), 129&#x2013;150.</li>    <li id="BibPLXBIB0013" label="[13]">Mikael Henaff, Joan Bruna, and Yann LeCun. 2015. Deep convolutional networks on graph-structured data. <em>      <em>arXiv preprint arXiv:1506.05163</em>     </em>(2015).</li>    <li id="BibPLXBIB0014" label="[14]">Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. 2010. Graph regularized transductive classification on heterogeneous information networks. In <em>      <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>     </em>. 570&#x2013;586.</li>    <li id="BibPLXBIB0015" label="[15]">Thomas&#x00A0;N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In <em>      <em>Proceedings of the 5th International Conference on Learning Representations</em>     </em>. 1&#x2013;14.</li>    <li id="BibPLXBIB0016" label="[16]">Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In <em>      <em>Advances in neural information processing systems</em>     </em>. 2177&#x2013;2185.</li>    <li id="BibPLXBIB0017" label="[17]">Laurens van&#x00A0;der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. <em>      <em>Journal of Machine Learning Research</em>     </em>9, Nov (2008), 2579&#x2013;2605.</li>    <li id="BibPLXBIB0018" label="[18]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&#x00A0;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em>      <em>Advances in neural information processing systems</em>     </em>. 3111&#x2013;3119.</li>    <li id="BibPLXBIB0019" label="[19]">Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2016. A review of relational machine learning for knowledge graphs. <em>      <em>Proc. IEEE</em>     </em>104, 1 (2016), 11&#x2013;33.</li>    <li id="BibPLXBIB0020" label="[20]">Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. <em>      <em>The PageRank citation ranking: Bringing order to the web.</em>     </em>Technical Report. Stanford InfoLab.</li>    <li id="BibPLXBIB0021" label="[21]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In <em>      <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>     </em>. 701&#x2013;710.</li>    <li id="BibPLXBIB0022" label="[22]">Sebastian Ruder. 2016. An overview of gradient descent optimization algorithms. <em>      <em>arXiv preprint arXiv:1609.04747</em>     </em>(2016).</li>    <li id="BibPLXBIB0023" label="[23]">Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. 2016. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In <em>      <em>Advances in Neural Information Processing Systems</em>     </em>. 1163&#x2013;1171.</li>    <li id="BibPLXBIB0024" label="[24]">David&#x00A0;I Shuman, Sunil&#x00A0;K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. 2013. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. <em>      <em>IEEE Signal Processing Magazine</em>     </em>30, 3 (2013), 83&#x2013;98.</li>    <li id="BibPLXBIB0025" label="[25]">Milivoj Simeonovski, Giancarlo Pellegrino, Christian Rossow, and Michael Backes. 2017. Who Controls the Internet?: Analyzing Global Threats using Property Graph Traversals. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web</em>     </em>. 647&#x2013;656.</li>    <li id="BibPLXBIB0026" label="[26]">Nitish Srivastava, Geoffrey&#x00A0;E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting.<em>      <em>Journal of machine learning research</em>     </em>15, 1 (2014), 1929&#x2013;1958.</li>    <li id="BibPLXBIB0027" label="[27]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In <em>      <em>Proceedings of the 24th International Conference on World Wide Web</em>     </em>. 1067&#x2013;1077.</li>    <li id="BibPLXBIB0028" label="[28]">Peter&#x00A0;D Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. <em>      <em>Journal of artificial intelligence research</em>     </em>37 (2010), 141&#x2013;188.</li>    <li id="BibPLXBIB0029" label="[29]">Derry Wijaya, Partha&#x00A0;Pratim Talukdar, and Tom Mitchell. 2013. Pidgin: ontology alignment using web text as interlingua. In <em>      <em>Proceedings of the 22nd ACM international conference on Information &#x0026; Knowledge Management</em>     </em>. 589&#x2013;598.</li>    <li id="BibPLXBIB0030" label="[30]">Zhilin Yang, William&#x00A0;W Cohen, and Ruslan Salakhutdinov. 2016. Revisiting semi-supervised learning with graph embeddings. In <em>      <em>Proceedings of the 33rd International Conference on Machine Learning</em>     </em>. 1&#x2013;9.</li>    <li id="BibPLXBIB0031" label="[31]">Wayne&#x00A0;W Zachary. 1977. An information flow model for conflict and fission in small groups. <em>      <em>Journal of anthropological research</em>     </em>33, 4 (1977), 452&#x2013;473.</li>    <li id="BibPLXBIB0032" label="[32]">Denny Zhou, Olivier Bousquet, Thomas&#x00A0;N Lal, Jason Weston, and Bernhard Sch&#x00F6;lkopf. 2004. Learning with local and global consistency. In <em>      <em>Advances in neural information processing systems</em>     </em>. 321&#x2013;328.</li>    <li id="BibPLXBIB0033" label="[33]">Xiaojin Zhu, Zoubin Ghahramani, and John&#x00A0;D Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In <em>      <em>Proceedings of the 20th International conference on Machine learning</em>     </em>. 912&#x2013;919.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>If the edges have attributes, we can add additional edge nodes for encoding. For detailed pre-processing, please refer to the NELL dataset in our experiments.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a> <a href="https://github.com/ZhuangCY/Coding-NN" class="link-inline force-break"       target="_blank">https://github.com/ZhuangCY/Coding-NN</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a> <a href="https://github.com/tkipf/gcn" class="link-inline force-break"       target="_blank">https://github.com/tkipf/gcn</a></p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a> <a href="https://github.com/kimiyoung/planetoid" class="link-inline force-break"       target="_blank">https://github.com/kimiyoung/planetoid</a></p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a> <a href="https://github.com/phanein/deepwalk" class="link-inline force-break"       target="_blank">https://github.com/phanein/deepwalk</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW' 18, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/>ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186116">https://doi.org/10.1145/3178876.3186116</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
