<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Text-Enriched Representations for News Image
  Classification</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186948'>https://doi.org/10.1145/3184558.3186948</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186948'>https://w3id.org/oa/10.1145/3184558.3186948</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Text-Enriched Representations for
          News Image Classification</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Elias</span> <span class=
          "surName">Moons</span> Department of Computer Science KU
          Leuven, <a href=
          "mailto:elias.moons@cs.kuleuven.be">elias.moons@cs.kuleuven.be</a>
        </div>
        <div class="author">
          <span class="givenName">Tinne</span> <span class=
          "surName">Tuytelaars</span> Department of Electrical
          Engineering KU Leuven, <a href=
          "mailto:tinne.tuytelaars@kuleuven.be">tinne.tuytelaars@kuleuven.be</a>
        </div>
        <div class="author">
          <span class="givenName">Marie-Francine</span>
          <span class="surName">Moens</span> Department of Computer
          Science KU Leuven, <a href=
          "mailto:sien.moens@kuleuven.be">sien.moens@kuleuven.be</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186948"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186948</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Images have a prominent role in the communication
        of news on the Web. We propose a novel method for image
        classification with subject categories when limited
        annotated images are available for training the classifier.
        A neural network based encoder learns image representations
        from paired news images and their texts. Once trained, this
        encoder transforms any image to a text-enriched
        representation of the image, which is then used as input
        for the classifier that categorizes an image according to
        its subject category. We have trained classifiers with
        different amounts of annotated images and found that the
        image classifier that uses the text-enriched image
        representations outperforms a baseline model that only uses
        image features especially in cases with limited training
        examples.</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Elias Moons, Tinne Tuytelaars, and Marie-Francine Moens.
          2018. Text-Enriched Representations for News Image
          Classification. In <em>WWW '18 Companion: The 2018 Web
          Conference Companion,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 2 Pages.
          <a href="https://doi.org/10.1145/3184558.3186948" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186948</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Web news companies receive a lot of images that could be
      used in news articles, but these are often not accompanied by
      text and even less by subject categories. For instance, on
      social media users post large amounts of images of local news
      that could be picked up by journalists. We need technology
      that classifies and filters these images into subject
      categories. The constraints are that within-class variability
      of news image patterns is huge and the number of annotated
      training data is limited. Automated classification of Web
      content has a long-standing tradition, but is mostly
      restricted to the classification of textual documents where
      SVM's form an established technique. The availability of
      large scale benchmark datasets such as
      ImageNet&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>] has boosted image classification
      based on deep learning. Classifiers trained on these
      benchmarks are used directly to classify Web images/video
      (e.g.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>]).
      They are also used as a way to obtain a powerful image
      representation on top of which a final classifier can be
      trained&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>], or as initialization of (part of)
      another neural network (finetuning)&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0009">9</a>]. Such transfer learning
      techniques have drastically reduced the amount of
      application-specific annotations needed compared to training
      models from scratch, but are still insufficient for dealing
      with categories with large within-class variability. There
      are some works that integrate context when classifying Web
      visual content. Examples are the work of Guermazi et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>]
      or Song et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>], who exploit textual descriptions
      that accompany images or video. These methods need images and
      their context information as input for the classifier, while
      we want to build a subject classifier that uses solely images
      as input. Various unsupervised image representations based on
      spatial or temporal (e.g., &nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0008">8</a>]) structural information
      have been proposed, which proved to reduce the number of
      training data in visual classification. We follow a similar
      philosophy, but instead we propose to enrich news image
      representations or embeddings with textual information. We
      propose a neural encoder which is trained on the large
      image-text dataset (i.e., news articles with text and images)
      and which at test time builds a text-enriched representation
      of any input image. The actual subject neural classifier is
      trained on labeled image data. It takes the text-enriched
      image representation as input and produces membership
      predictions for all subject classes. The contribution is a
      novel news image classifier that uses the text-enriched image
      representations, which proves to be advantageous compared to
      a traditional image classifier which only uses image
      features.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Methods</h2>
        </div>
      </header>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Text-enriched image classifier</h3>
          </div>
        </header>
        <section id="sec-7">
          <p><em>2.1.1 Encoder.</em> The input of the encoder is an
          image vector representation <strong><em>x</em></strong>
          <sup><em>enc</em></sup> of a training example. This
          vector passes through two fully connected layers with
          transformation matrices <strong><em>W</em></strong>
          <sup><em>enc</em>, 1</sup> and
          <strong><em>W</em></strong> <sup><em>enc</em>, 2</sup>,
          obtaining respectively a hidden representation
          <strong><em>h</em></strong> <sup><em>enc</em></sup> and
          an output <strong><em>y</em></strong>
          <sup><em>enc</em></sup> . The state of one hidden node
          <span class="inline-equation"><span class=
          "tex">$h^{enc}_{i} \in {\bf {\it h}}^{enc}$</span></span>
          is calculated as follows (with <em>F</em> being a softmax
          activation function):</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} h^{enc}_{i}
              = \sum ^{m}_{j = 1}F(w^{enc,1}_{i,j} \cdot
              x^{enc}_{j}) \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>with <em>m</em> = 4096. The state of an output node
          <span class="inline-equation"><span class=
          "tex">$y^{enc}_{i} \in {\bf {\it y}}^{enc}$</span></span>
          becomes:
          <div class="table-responsive" id="eq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} y^{enc}_{i}
              = \sum ^{n}_{j = 1}F(w^{enc,2}_{i,j} \cdot
              h^{enc}_{j}) \end{equation}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>with <em>n</em> = 1024. At training time, the
          encoder learns to predict the corresponding text
          representation for a given image (both 4096-dimensional
          vectors). At test time, it produces a corresponding
          text-enriched image representation for a given image.
          <p></p>
        </section>
        <section id="sec-8">
          <p><em>2.1.2 Classifier.</em> The input of the classifier
          is an enriched image vector representation
          <em><strong>x</strong> <sup>enr</sup></em> . This vector
          passes through two fully connected layers with
          transformation matrices <strong><em>W</em></strong>
          <sup><em>enr</em>, 1</sup> and
          <strong><em>W</em></strong> <sup><em>enr</em>, 2</sup>,
          obtaining respectively a hidden representation
          <strong><em>h</em></strong> <sup><em>enr</em></sup> and
          an output <strong><em>y</em></strong>
          <sup><em>enr</em></sup> . The state of one hidden node
          <span class="inline-equation"><span class=
          "tex">$h^{enr}_{i} \in {\bf {\it h}}^{enr}$</span></span>
          is calculated as:</p>
          <div class="table-responsive" id="eq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} h^{enr}_{i}
              = \sum ^{o}_{j = 1}F(w^{enr,1}_{i,j} \cdot
              x^{enr}_{j}) \end{equation}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>with <em>o</em> = 4096. The state of an output node
          <span class="inline-equation"><span class=
          "tex">$y^{enr}_{i} \in {\bf {\it y}}^{enr}$</span></span>
          becomes:
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} y^{enr}_{i}
              = \sum ^{p}_{j = 1}F(w^{enr,2}_{i,j} \cdot
              h^{enr}_{j}) \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>with <em>p</em> = 256. For a given (enriched) image
          (representation), the output of the network will be a
          vector with membership predictions for each of the
          subject categories. During training the two networks are
          considered separately. The encoder is trained on the news
          image-text dataset. The image classifier is trained only
          on annotated images. At test time an image is
          subsequently sent through the encoder and then through
          the image classifier to obtain subject category
          membership predictions.
          <p></p>
        </section>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Image-only
            baseline</h3>
          </div>
        </header>
        <p>The architecture of the traditional image-only model is
        identical to the image classifier described above. This
        model is trained to predict memberships for the different
        classes solely based on the image features without first
        sending these features through an encoder.</p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experiments and
          results</h2>
        </div>
      </header>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Data</h3>
          </div>
        </header>
        <p>The data used to evaluate the models comes from the
        Webhose<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a> dataset, which classifies news
        articles into 7 subject categories (entertainment, finance,
        politics, sports, technology, travel and world news)
        defining a multiclass and multilabel categorization. A
        preprocessing step is shared between the proposed model and
        the baseline. An image is described by 4096 features
        generated by a VGG-16 network, a deep convolutional network
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0006">6</a>]. To train
        the encoder, a text is represented as a bag-of-words using
        the 4096 most informative words calculated by the
        Chi-Square statistic [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>] on a labeled text dataset. A test
        set of 500 samples is drawn uniformly at random from the
        Webhose dataset keeping only the images and their
        ground-truth subject labels and making sure that its images
        do not occur in the rest of the dataset, leaving an actual
        test set of 378 samples. The training data for the encoder
        (7384 images and their co-occurring texts) and the training
        data for the image classifier (various sizes, images with
        their classification label) are obtained from the remainder
        of the Webhose dataset.<a class="fn" href="#fn2" id=
        "foot-fn2"><sup>2</sup></a> In all experiments 20% of the
        training data is used for validation and parameter tuning.
        An output score threshold of 0.5 is used for category
        prediction. We report micro-averaged precision, recall and
        F1 scores.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Training
            with limited annotated images</h3>
          </div>
        </header>
        <p>For all multiples of 1000 smaller than the size of the
        image classifier training set, training sets are selected
        uniformly at random of that specific size. This experiment
        was repeated 5 times in total and the scores are averaged
        over all the experiments to obtain more qualitative
        results. The F1 scores in Figure <a class="fig" href=
        "#fig1">1</a> show that using text-enriched image
        representations in news image classification outperforms
        the image-only baseline classifier for all considered sizes
        of the training data. The performance gain of the
        text-enriched classifier is more prominent when the
        annotated training set is smaller. Table <a class="tbl"
        href="#tab1">1</a> gives detailed results for a dataset of
        2000 and 5000 annotated training images. In case of only
        2000 training images, the text-enriched classifier improves
        the baseline by 4.93% (p-value of 0.023, 2-tailed t-test).
        For the case of 5000 samples the difference in F1 score is
        1.84% but still significant (p-value of 0.040).</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186948/images/www18companion-188-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">F1 score of the image
            classifiers in function of the fraction of the
            annotated image dataset.</span>
          </div>
        </figure>
        <p></p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Accuracy, precision, recall and F1 scores
            for the image classification models trained on 2000 and
            5000 annotated training instances</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">Model</td>
                <td style="text-align:center;">Size</td>
                <td style="text-align:center;">Acc</td>
                <td style="text-align:center;">Prec</td>
                <td style="text-align:center;">Rec</td>
                <td>F1</td>
              </tr>
              <tr>
                <td style="text-align:left;">Image</td>
                <td style="text-align:center;">2000</td>
                <td style="text-align:center;">87.18%</td>
                <td style="text-align:center;">60.47%</td>
                <td style="text-align:center;">29.84%</td>
                <td>39.93%</td>
              </tr>
              <tr>
                <td style="text-align:left;">Text-enriched</td>
                <td style="text-align:center;">2000</td>
                <td style="text-align:center;">87.21%</td>
                <td style="text-align:center;">58.34%</td>
                <td style="text-align:center;">36.46%</td>
                <td><strong>44.86</strong>%</td>
              </tr>
              <tr>
                <td style="text-align:left;">Image</td>
                <td style="text-align:center;">5000</td>
                <td style="text-align:center;">87.41%</td>
                <td style="text-align:center;">60.48%</td>
                <td style="text-align:center;">34.55%</td>
                <td>43.92%</td>
              </tr>
              <tr>
                <td style="text-align:left;">Text-enriched</td>
                <td style="text-align:center;">5000</td>
                <td style="text-align:center;">87.10%</td>
                <td style="text-align:center;">57.34%</td>
                <td style="text-align:center;">38.10%</td>
                <td><strong>45.76</strong>%</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Conclusion</h2>
        </div>
      </header>
      <p>The proposed text-enriched image classifier outperforms a
      purely image-feature based classifier on a Web news image
      dataset, which is characterized by a large variety of visual
      patterns and limited labeled training data.</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span>
          Acknowledgments</h2>
        </div>
      </header>
      <p>The research was funded by the Google DNI grant
      MetaHaven.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Jia Deng, Wei Dong,
        Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.
        ImageNet: A large-scale hierarchical image database. In
        <em>CVPR</em> 2009. IEEE, 248-255.</li>
        <li id="BibPLXBIB0002" label="[2]">Persi Diaconis and
        Bradley Efron. 1985. Testing for independence in a two-way
        table: New interpretations of the chi-square statistic. The
        Annals of Statistics 13, 3, 845-874.
        http://www.jstor.org/stable/2241103</li>
        <li id="BibPLXBIB0003" label="[3]">Radhouane Guermazi,
        Mohamed Hammami, and Abdelmajid Ben Hamadou. 2010.
        Classification of violent Web images using context based
        analysis. In SAC. Sierre, Switzerland, March 22-26, 2010.
        1768-1773.</li>
        <li id="BibPLXBIB0004" label="[4]">Tao Mei and Cha Zhang.
        2017. Deep learning for intelligent video analysis. In
        <em>MM, Mountain View, CA, USA, October23-27, 2017.</em>
        1955-1956. https://doi.org/10. 1145/3123266.3130141</li>
        <li id="BibPLXBIB0005" label="[5]">Ali Sharif Razavian,
        Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
        2014. CNN features off-the-shelf: an astounding baseline
        for recognition. In <em>CVPR.</em> 806-813.</li>
        <li id="BibPLXBIB0006" label="[6]">Karen Simonyan and
        Andrew Zisserman. 2014. Very deep convorational networks
        for large-scale image recognition. <em>CoRR</em>
        abs/1409.1556. arXiv:1409.1556</li>
        <li id="BibPLXBIB0007" label="[7]">Yang Song, Ming Zhao,
        Jay Yagnik, and Xiaoyun Wu. 2010. Taxonomic classification
        for web-based videos. In <em>CVPR, 2010.</em> IEEE,
        871-878.</li>
        <li id="BibPLXBIB0008" label="[8]">Xiaolong Wang and
        Abhinav Gupta. 2015. Unsupervised learning of visual
        representations using videos. In <em>ICCV.</em>
        2794-2802.</li>
        <li id="BibPLXBIB0009" label="[9]">Jason Yosinski, Jeff
        Clune, Yoshua Bengio, and Hod Lipson. 2014. How
        transferable are features in deep neural networks?. In
        <em>NIPS.</em> 3320-3328.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href=
    "#foot-fn1"><sup>1</sup></a>https://webhose.io/datasets.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Data splits can
    be found at: http://liir.cs.kuleuven.be/software.php.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186948">https://doi.org/10.1145/3184558.3186948</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
