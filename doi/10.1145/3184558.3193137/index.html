<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Distributed Optimization of All-in-one SVMs for Extreme Classfication</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3193137'>https://doi.org/10.1145/3184558.3193137</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3193137'>https://w3id.org/oa/10.1145/3184558.3193137</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Distributed Optimization of All-in-one SVMs for Extreme Classfication</span>      <br/>      <span class="subTitle">       <SubTitle>Workshop Keynote Talk</SubTitle>      </span>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Marius</span>     <span class="surName">Kloft</span>     University of Kaiserslautern, Kaiserslautern, Germany, <a href="mailto:kloft@cs.uni-kl.de">kloft@cs.uni-kl.de</a>    </div>        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3193137" target="_blank">https://doi.org/10.1145/3184558.3193137</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Training of multi-class or multi-label classification machines are embarrassingly parallelizable via the one-vs.-rest approach. However, training of all-in-one multi-class learning machines such as multinomial logistic regression or all-in-one multi-class SVMs (MC-SVMs) is not parallelizable out of the box. In my talk, I present optimization strategies to distribute the training of all-in-one multi-class SVMs over the classes, which makes them appealing for the use in extreme classification.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Theory of computation </strong>&#x2192; <strong>Distributed algorithms;</strong> <strong>Support vector machines;</strong> &#x2022;<strong> Mathematics of computing </strong>&#x2192; <em>Solvers;</em> <em>Mathematical optimization;</em> <em>Quadratic programming;</em> &#x2022;<strong> Computing methodologies </strong>&#x2192; <em>Supervised learning by classification;</em> Structured outputs;</small> </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Marius Kloft. 2018. Distributed Optimization of All-in-one SVMs for Extreme Classfication. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 1 Pages. <a href="https://doi.org/10.1145/3184558.3193137" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3193137</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>    <div class="title-info">     <h2>Bio</h2>    </div>    </header>    <p>Since 2017 Marius Kloft is a professor of machine learning at the Department of Computer Science of TU Kaiserslautern. Prior to joining TUK, he was a junior professor at HU Berlin (2014-2017) and a joint postdoctoral fellow at Courant Institute and MSKCC, working with M. Mohri, C. Cortes, and G. R&#x00E4;tsch. MK is interested in theory and algorithms of statistical machine learning and its applications, especially in statistical genetics. He has been working on, e.g., multiple kernel learning, multi-task learning, anomaly detection, extreme classification, and adversarial learning for computer security. He has co-organized workshops on multiple kernel learning, multitask learning, anomaly detection, and extreme classication at NIPS (2010, 2013, 2014, 2017), ICML (2016), and Dagstuhl (2015,2018). For his research, MK received the Google Most Influential Papers 2013 award.</p>   </section>  </section> </body> </html> 
