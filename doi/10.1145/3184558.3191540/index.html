<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>A Corpus for Hybrid Question Answering Systems</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">A Corpus for Hybrid Question Answering Systems</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Brigitte</span>      <span class="surName">Grau</span>     LIMSI, CNRS, ENSIIE, University Paris-Saclay, Rue John von NeumannOrsay, FranceF-91405, <a href="mailto:brigitte.grau@limsi.fr">brigitte.grau@limsi.fr</a>     </div>     <div class="author">     <span class="givenName">Anne-Laure</span>      <span class="surName">Ligozat</span>     LIMSI, CNRS, ENSIIE, University Paris-Saclay, Rue John von NeumannOrsay, FranceF-91405, <a href="mailto:anne-laure.ligozat@limsi.fr">anne-laure.ligozat@limsi.fr</a>     </div>            </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191540" target="_blank">https://doi.org/10.1145/3184558.3191540</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Question answering has been the focus of a lot of researches and evaluation campaigns, either for text-based systems (TREC and CLEF evaluation campaigns for example), or for knowledge-based systems (QALD, BioASQ). Few systems have effectively combined both types of resources and methods in order to exploit the fruitfulness of merging the two kinds of information repositories. The only evaluation QA track that focuses on hybrid QA is QALD since 2014. As it is a recent task, few annotated data are available (around 150 questions). In this paper, we present a question answering dataset that was constructed to develop and evaluate hybrid question answering systems. In order to create this corpus, we collected several textual corpora and augmented them with entities and relations of a knowledge base by retrieving paths in the knowledge base which allow to answer the questions. The resulting corpus contains 4300 question-answer pairs and 1600 have a true link with DBpedia.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Hybrid Question Answering</small>, </span>     <span class="keyword">      <small> Corpus</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Brigitte Grau and Anne-Laure Ligozat. 2018. A Corpus for Hybrid Question Answering Systems. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 6 Pages. <a href="https://doi.org/10.1145/3184558.3191540" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191540</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Question answering (QA) systems provide a user-friendly tool for seeking different kinds of resources, as they allow the user to enter questions written in natural language. Such systems have to extract the answers from relevant documents or to query a database or even to exploit both kinds of resources.</p>    <p>Question answering has been the focus of a lot of researches and evaluation campaigns, either for text-based systems (TREC and CLEF evaluation campaigns for example), or for knowledge-based systems (QALD, BioASQ). Few systems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] have effectively combined both types of resources and methods in order to exploit the fruitfulness of merging the two kinds of information repositories. For example, if presented with the question <em>At which college did the only American actor that received the C&#x00E9;sar Award study?</em>, a system will find the actor more probably in texts and his college in a knowledge base. Textual resources contain a great amount of information, but require complex natural language processing (NLP) tools for extracting answers. On the contrary, knowledge bases contain structured information, which makes it possible to query them directly once the question has been translated into the appropriate query language. Yet, if knowledge bases are much more reliable, they remain incomplete and do not contain as much information as texts. Moreover, KB are not dedicated to store contextual information or information about all the entities. Only the most famous entities have entries in a KB. Hybrid QA systems aim at exploiting both types of information.</p>    <p>The only QA evaluation track that focuses on hybrid QA is QALD, since 2014 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. As it is a recent task, few annotated data are available for learning. Moreover, in this track, all answers are KB entities. Other question answering datasets exist, but they are built for developing systems dedicated to search in one resource only: Trec, CLEF and SQuAD datasets for textual QA, QALD and WebQuestions datasets for knowledge base QA. Thus they are not suitable for training or evaluating a hybrid QA system. Textual datasets do not provide the answer URIs, when they exist, which are required to evaluate the results of a knowledge base search. Concerning KB datasets, QALD dataset contains too few examples, and WebQuestions contains mostly simple questions, i.e. questions that can be solved by a single triple and do not require a hybrid approach.</p>    <p>In this paper, we present a question answering dataset that was constructed to develop and evaluate hybrid question answering systems. It contains both question and answer pairs in textual form, and references to the KB. The textual mentions of entities have a reference to their entity in the knowledge base, and useful relations of the KB are added to the pairs enabling to align text and structured representations. The questions must be complex enough for needing the resort to a hybrid solution.</p>    <p>In order to create this corpus, we collected several textual corpora and augmented them with entities and relations of a knowledge base by retrieving the paths in the knowledge base which allow to answer the questions. The resulting corpus contains 4300 question-answer pairs where 1600 have a true link with DBpedia and can be used for learning and testing hybrid QA systems as well as improving KB systems on complex questions<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related work</h2>     </div>    </header>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Question answering systems</h3>     </div>     </header>     <p>Most QA systems are dedicated to search for an answer either in text or in a knowledge base, but not both. Textual QA systems rely mainly on methods able to recognize the similarity between a question and a sentence, or more generally a textual entailment relation between them. Their goal is mainly to model the recognition of lexical and syntactic overlaps that take into account lexical variations. Methods range from feature based learning methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>] to neural network methods, for example [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>]<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>, that show better performances when dealing with lexical similarity.</p>     <p>The extraction of the exact answer involves criteria based on the determination of the expected answer type and its matching in the candidate sentences, based on named entity recognition and syntactic features. It also ranges from feature based learning approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] to neural network approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>     </p>     <p>One of the main challenges when querying a KB given a NL question concerns the alignment of a question with the KB triples, which needs to overcome lexical gap and to adapt the question parsing to the KB schema in order to determine which phrases are entity or relation mentions (see Diefenbach et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] for a recent survey). The query can be generated by representing the question based on semantic graphs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] or patterns [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] and transforming this representation into a query. Yao and Van&#x00A0;Durme adopt a less sequential approach and extract a subgraph of the KB around the entities recognized in the question. Deep neural network methods are similar to those applied on text and compare a question representation learned from word embeddings to triple representations. The triple representation are learned from the KB triples [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>] or from their label [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>].</p>     <p>Some hybrid QA approaches were developed. Besides former methods that use in parallel both resources for searching an answer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>] , [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] also use them in a complementary strategy for verifying a candidate answer type. Some recent works develop a collaborative strategy. Yahya et&#x00A0;al. developed query extension and relaxation techniques to search for information in the text contexts associated to triples. Park et&#x00A0;al., on the contrary, first search for information in texts annotated with KB entities, and use SPARQL queries if the text strategy is not successful. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>], a hybrid search is really performed with the decomposition of the questions into subparts that are searched in both kinds of resources and provided answers are aggregated for the final answer selection.</p>     <p>The exploration of hybrid approaches needs to enlarge the available datasets and to provide them with information that allow to learn and evaluate the alignment of text and KB data. It will also benefit the mono-source QA systems.</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> QA datasets</h3>     </div>     </header>     <section id="sec-7">     <p><em>2.2.1 Text QA corpora.</em> QA evaluation campaigns have been organized at TREC between 1999 and 2007. TREC questions are built from logs of search engines and answers have to be extracted from texts, that are mainly newswires. Datasets of question-answer pairs are freely available, but without their supporting passages, i.e. passages that support and justify the answer given. Around 500 factual questions<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> were released each year.</p>     <p>CLEF QA campaigns took place between 2003 and 2009 with an important evolution over the time concerning the type of questions and the documents to search. Documents are newswires and articles from Wikipedia. As CLEF propose evaluations for the European languages, the questions are conceived so that they can be answered from texts in each studied language <a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>. Each year, 200 questions were provided and their textual answers were released.</p>     <p>Questions in these corpora concern news that were present in newspapers. Thus, some questions are closely linked to the actuality at the time of the newspapers. Others are timeless and concerns encyclopedic information.</p>     <p>Apart from evaluation campaigns, some corpora have been distributed. The Microsoft Research Question-Answering Corpus<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> is a corpus of 400 factual questions on Encarta 98.</p>     <p>The Stanford Question Answering Dataset (SQuAD)<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>] a corpus of more than 100 000 question-answer pairs concerning more than 500 articles of Wikipedia that were collected by crowdsourcing. This corpus is now largely used for developing textual QA systems based on deep learning.</p>     </section>     <section id="sec-8">     <p><em>2.2.2 Knowledge base corpora.</em> QALD evaluation campaign exists since 2011, whose objective is to evaluate the performances of QA systems over a knowledge base. The datasets are made of about 200 textual questions every year. They must be answered using DBpedia and are provided with a SPARQL query. The answers are DBpedia URIs. One limitation of this corpus is the low number of questions, which makes it hard to use in learning methods. Another limitation is the bias due to the fact that this corpus was built specifically to be answerable on a knowledge base.</p>     <p>WebQuestions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0003">3</a>] is a question-answer dataset on Freebase. The textual questions have been build using Google Suggest API queries and Amazon Mechanical Turk to filter them. Answers are the Freebase URIs. It is constituted of 3778 examples for training and 2032 for test. The questions are rather simple in their form, as most of them can be represented as a single triple of the knowledge base and do not seem complex enough to evaluate a hybrid question answering system.</p>     </section>     <section id="sec-9">     <p><em>2.2.3 Hybrid corpora.</em> QALD has created a task for hybrid QA since 2014. The objective of the task is to answer questions by using both triples from DBpedia and the abstract of each relevant Wikipedia article. Until now, it has provided around 150 question-answer pairs that can be used to evaluate a question answering hybrid system, but the number of questions remains low for learning.</p>     </section>    </section>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Requirements of the new dataset</h2>     </div>    </header>    <p>In a first step, we explore which kinds of phenomena occurring in QA could benefit a hybrid approach, so that it will allow us to define the content of our new corpus.</p>    <p>We consider a knowledge base which is made of triplets and stores binary relations (subject, <em>predicate</em>, object) about instances, such as DBpedia or Freebase, and a text corpus. We also define a hybrid search as follows: finding the answer would require to search for information in the two sources and aggregate them.</p>    <p>The possibility to find answers using either text or a knowledge base, or both, is related to the question content, &#x2013; are they about an entity, an event, a concept &#x2013; and the type of answer that is expected &#x2013; a definition, a factual information, an entity, an explanation, etc. &#x2013;. As soon as the question mentions an entity or the answer is a known entity, a KB search can be fired in addition or in place of a textual search. However, we wanted to examine more closely which cases would need a hybrid resolution. A preliminary corpus study <a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a> lead us to define the following cases:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">The answer is a property value of an entity given in the question (direct relationship): <em>Who is Bill Clinton&#x0027;s daughter&#x0027;s husband?</em>. The answer can be searched directly in both sources, or be solved by hybrid search because it requires finding three entities and two relations which could not be both present in a KB: (Bill Clinton, <em>daughter</em>, Chelsea Clinton) and (Chelsea Clinton, <em>married</em>, Marc Mervinsky) ;<br/></li>     <li id="list2" label="&#x2022;">The answer is about an event, either about a role or the name of the event: <em>Who is the assassin of Martin Luther King?</em>. In general the answer will come from texts, especially if the event involves unknown entities as events are often not represented in KB;<br/></li>     <li id="list3" label="&#x2022;">A combination of the two preceding cases, for example a direct relation with an entity having a role in an event (composition of relations): <em>Where was the assassin of Martin Luther King born?</em>. A hybrid search should be done.<br/></li>     <li id="list4" label="&#x2022;">The answer is either an instance or a concept : <em>What animal lays blue eggs ?</em>. The answer will certainly come from text, with for example &#x201D;The Collonca are without tail and lay blue eggs&#x201D;, and the verification that Collonca are animals can be operated on one or the other source.<br/></li>     <li id="list5" label="&#x2022;">The relation with the answer is contextual; it can be an opinion or related to an event, for example as in <em>Which country bought petroleum to Irak during the embargo?</em> (about the <em>embargo</em> event). The answer can only be found in texts;<br/></li>     <li id="list6" label="&#x2022;">A definition: <em>What is an atom?</em>. The answer is in texts ;<br/></li>     <li id="list7" label="&#x2022;">A result coming from an operator of aggregation (comparison, sorting, counting): <em>Give the ten bigger French companies.</em> The answers can come from texts, as long as the searched information is explicit, but they are easier to deduce from a knowledge base.<br/></li>    </ul>    <p>In conclusion, if we want a corpus to be helpful for hybrid research, we must ensure that: 1) at least one entity is present in the question or the answer; 2) there exists at least one relation in the KB relevant for answering the question; 3) the question often contains additional information so that its whole meaning cannot be represented by a unique relation.</p>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Construction of the dataset</h2>     </div>    </header>    <p>In order to build hybrid systems, it is better to have a dataset with a large number of questions, that are long enough so that they would more probably require a hybrid reasoning. It is thus required that the questions be linked to a reference in a knowledge base.</p>    <p>Our intent is to augment an existing corpus, and not to build a new one from scratch. To obtain such a corpus, two approaches are possible. The first one consists in using a corpus of questions for knowledge base QA systems and adding texts related to the pairs. The available datasets of that type contain either too short questions (WebQuestions) or too few questions (QALD). The second approach consists in using a textual QA dataset and complementing it with information that come from the knowledge base. We chose this second approach because the corpus of questions on texts that are available (TREC and CLEF) are often about information present in knowledge bases (DBpedia). Part of these questions are also complex and long enough to make it possible to build a hybrid system.</p>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Corpus sources</h3>     </div>     </header>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Corpus sources</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;">Name</td>        <td style="text-align:center;">Number of questions</td>       </tr>       <tr>        <td style="text-align:center;">Clef 2004 : MultiEight</td>        <td style="text-align:center;">700</td>       </tr>       <tr>        <td style="text-align:center;">Clef 2005 : MultiNine</td>        <td style="text-align:center;">200</td>       </tr>       <tr>        <td style="text-align:center;">Trec 1999 -> 2007</td>        <td style="text-align:center;">3400</td>       </tr>       <tr>        <td style="text-align:center;">total</td>        <td style="text-align:center;">4300</td>       </tr>      </tbody>     </table>     </div>     <p>We selected factual questions in CLEF and TREC datasets. We kept the datasets of CLEF 2004 and 2005 (cf [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>]) that concern newspapers from 1994 and 1995. They enclose several kinds of manually written and translated questions: factual questions, &#x201D;how&#x201D; questions and definition questions. The factual questions types are: TIME, MEASURE, PERSON, ORGANISATION, LOCATION, OBJECT, MANNER, OTHER. Definition questions are limited to persons and organizations.</p>     <p>The TREC datasets (cf [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]) are based on similar documents. The questions were written manually in Trec 1999 and conceived from logs for the campaigns of 2000 to 2004. They cover factual and definition questions.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Augmentation of the corpus</h3>     </div>     </header>     <p>In order to determine if questions are hybrid, we can try to determine whether it is possible to solve them only with the knowledge base. A first step consists in determining 1) if an entity of the knowledge base answers the question; 2) if an entity of the knowledge base is the main entity (the focus) in the question; 3) if a path of the KB links the two entities.</p>     <p>Several cases can then be possible:</p>     <ul class="list-no-style">     <li id="list8" label="&#x2022;">the answer can be identified as a knowledge base entity, but no path can be found with an entity of the question: the knowledge base and the texts could be combined to find the answer;<br/></li>     <li id="list9" label="&#x2022;">an entity can be identified in the question, but the entity of the answer cannot be identified: then the entities could help the resolution by using the knowledge base, but the resolution must be done with text;<br/></li>     <li id="list10" label="&#x2022;">a path can be found: then the question might be solved completely by the knowledge base.<br/></li>     </ul>     <p>These cases are all potentially hybrid: the third case can be solved using only the knowledge base but in practice information from texts can help to find an answer.</p>     <p>Thus the next step is to augment each question-answer pair with data from DBpedia: answer URI, question entity URIs, paths between these two entities (named KB path in the following).</p>     <p>In order to distribute a more self-contained corpus, we also augment the pairs with a related Wikipedia paragraph if found. To do that, we use the code released by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>].</p>     <p>We will now define more closely a KB path. First, a path that leads to the answer, called <em>answer path</em>, is a finite sequence ((<em>e<sub>n</sub>     </em>, <em>r<sub>n</sub>     </em>, <em>e</em>     <sub>      <em>n</em> + 1</sub>))<sub>      <em>n</em>     </sub> with:</p>     <ul class="list-no-style">     <li id="list11" label="&#x2022;"><em>n</em> &#x2208; [0, <em>N</em>[, N is the length of the path,<br/></li>     <li id="list12" label="&#x2022;"><em>e<sub>n</sub>      </em> is either an entity of the KB <span class="inline-equation"><span class="tex">$e^K_n$</span>      </span> represented as an URI, or an entity of a text <span class="inline-equation"><span class="tex">$e^T_n$</span>      </span> represented as a sequence of words,<br/></li>     <li id="list13" label="&#x2022;"><em>r<sub>n</sub>      </em> is either a relation of the knowledge base <span class="inline-equation"><span class="tex">$r^K_n$</span>      </span> represented as an URI, or a relation of a text <span class="inline-equation"><span class="tex">$r^T_n$</span>      </span> represented as a sequence of words,<br/></li>     <li id="list14" label="&#x2022;"><em>e</em>      <sub>0</sub> is the focus entity in the question while <em>e<sub>N</sub>      </em> is the answer.<br/></li>     </ul>     <p>The answer path allows to answer the question but note that an answer path does not necessarily contain all the information given in the question; it is not a representation of the meaning of the question, neither a full justification of the answer and does not correspond to a query as in QALD corpora. The supplementary information in the question will help to find it, using text or KB.</p>     <p>A hybrid answer path contains both kinds of entity: at least one entity from a knowledge base and one from a text.</p>     <p>Given these definitions, a hybrid pair (<em>q</em>, <em>a</em>), with <em>q</em> a question and <em>a</em> an answer, which potentially requires a hybrid resolution is a pair that is associated to a hybrid answer path. Thus, a hybrid resolution could consists in finding a part of the answer path in KB, i.e. a KB path, and complete missing information using texts or in using textual information outside the answer path for selecting the answer path. We will give some examples below.</p>     <p>A KB path links a question entity to the answer entity, so it has the same extremities as the answer path. For evaluating its relevance, we will do it in reference to the answer path.</p>     <p>In order to annotate pairs with the KB information, firstly answers and questions are associated with URIs of the entities they mention by using DBpedia Spotlight. We then compute the KB paths at one and two steps between the pairs of entities by querying DBpedia.</p>     <ul class="list-no-style">     <li id="list15" label="&#x2022;">A one-step KB path (<em>e<sub>q</sub>      </em>, <em>p</em>, <em>e<sub>r</sub>      </em>) is made of one entity of the question, <em>e<sub>q</sub>      </em>, the answer entity, <em>e<sub>r</sub>      </em>, and a predicate, <em>p</em> between the two. For example in the question <em>Who is Shimon Peres?</em> whose answer is <em>Israeli Foreign Affairs Minister</em>, the entities <em>e<sub>q</sub> res:Shimon_Peres</em> and <em>e<sub>r</sub> res:Ministry_of_Foreign_Affairs_(Israel)</em> are identified respectively in the question and the answer and the path with the predicate <em>dbp:office</em> is found and allows to answer the question.<br/></li>     <li id="list16" label="&#x2022;">A two-step KB path is made of 2 entities <em>e<sub>q</sub>      </em> and <em>e<sub>r</sub>      </em>, 2 predicates <em>p</em>      <sub>1</sub> and <em>p</em>      <sub>2</sub> and an intermediary entity <em>e<sub>i</sub>      </em> : ((<em>e<sub>q</sub>      </em>, <em>p</em>      <sub>1</sub>, <em>e<sub>i</sub>      </em>), (<em>e<sub>i</sub>      </em>, <em>p</em>      <sub>2</sub>, <em>e<sub>r</sub>      </em>)). A 2-step path means that the question is complex enough and needs some reasoning. We limit the paths to a length of 2 for a computational reason and because we think that, in general, paths longer than 2 steps are not relevant and are not part of the answer path.<br/></li>     </ul>     <p>In order to examine the relevance of these annotations, we validated them manually. We first validated the annotation of the answers with the found URIs. Then, for every validated entities, we examined the KB paths. We do not explicit the answer path, as it would have to be done manually. Thus, when we decide the validity of a KB path as a possible sub-path of an answer path, we have to decide if some words of the question can be associated with some predicates found in the KB path, i.e. if they could be a mention of the relation.</p>     <p>The kinds of possible annotation of one-step or two-step KB paths are:</p>     <ul class="list-no-style">     <li id="list17" label="&#x2022;">Correct: the KB path is an answer path. Several cases can occur:<br/>      <ul class="list-no-style">       <li id="list18" label="&#x2022;">the KB path is an answer path, i.e. the predicates correspond to question words, and it is fully justified, i.e. contains all the information given in the question. For example (CERN, headquarters, Canton_of_Geneva), (Canton_of_Geneva, capital, Geneva) correctly answers the question <em>Where is the CERN located?</em> with Geneva as answer<br/></li>       <li id="list19" label="&#x2022;">the KB path is an answer path and does not cover all the information given in the question. For example, (James_Bond, portrayer, Pierce_Brosnan) correctly answer <em>Who plays James Bond in the latest film of the 007 series?</em> with Pierce Brosnan as answer&#x00A0;<a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a>, however the fact that it is the last one is not in the path.<br/></li>      </ul></li>     <li id="list20" label="&#x2022;">partial path: the KB path is a sub-path of the answer path: the predicate is not precise enough and requires to be completed by other information. For example, (Kim_Il-sung, allegiance, North_Korea) partially answers <em>Who was the president of North Korea before 1994?</em> with Kim Il-sung as answer. The information that Kim Il-sung is a president is more precise than allegiance.<br/></li>     <li id="list21" label="&#x2022;">related path: the predicates found are related to the question words but do not correspond to a sub-path of the answer path.<br/>      <ul class="list-no-style">       <li id="list22" label="&#x2022;">the relation matches some information given in the question, but does not belong to the answer path. For example (Java, designer, Sun_Microsystems) is relevant to the question <em>What will Microsoft license from SUN?</em> with answer Java, as it makes the connection between Java and SUN, but it does not answer the question about Microsoft license.<br/></li>       <li id="list23" label="&#x2022;">as the preceding case, but with a too vague relation or a relation topically related to the question, i.e. the relation is not completely out of topic.<br/></li>      </ul></li>     <li id="list24" label="&#x2022;">incorrect path: the KB path is not in relation with the answer path, i.e. all the other cases. For example (Mississippi, flower, Magnolia) does not answer <em>What is the nickname of the state of Mississippi?</em> with answer Magnolia, as flower cannot be matched with some question words. This relation could be useful in some reasoning, for example if we know that often a nickname of a state is its associated flower, however we can expect that we will find the direct relation in texts. Thus incorrect path means that the question is better solved with textual information.<br/></li>     </ul>     <p>A lot of two-step paths have been found but only a small part is correct. In order to shorten the annotation time, we only examine two-steps paths of questions that do not have a validated one-step path. Moreover, some irrelevant relations have been removed: subject, align, direction, seeAlso which are not semantic relations and often yield false paths.</p>    </section>   </section>   <section id="sec-14">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Analysis of the annotated corpus</h2>     </div>    </header>    <p>We computed some statistics on the corpus after validation that are given in Table <a class="tbl" href="#tab2">2</a>.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Corpus statistics</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:center;">Kind of question</td>       <td style="text-align:center;">Number of questions</td>      </tr>      <tr>       <td style="text-align:center;">2-step path</td>       <td style="text-align:center;">290</td>      </tr>      <tr>       <td style="text-align:center;">1-step path</td>       <td style="text-align:center;">269</td>      </tr>      <tr>       <td style="text-align:center;">the answer is a URI</td>       <td style="text-align:center;">1699</td>      </tr>      <tr>       <td style="text-align:center;">total</td>       <td style="text-align:center;">4300</td>      </tr>     </tbody>     </table>    </div>    <p>There are around 1700 questions that have a connection with DBpedia and text. Among them, around 560 are annotated with a useful KB path for their resolution.</p>    <p>Our corpus can be used for hybrid QA but also for KB QA in case of correct KB paths. As 1-length answer paths are usually found in corpus for KB QA, we wanted to know whether the 2-path questions bring new interesting cases that are not usually part of the existing KB corpora.</p>    <p>We found three categories of such questions:</p>    <ol class="list-no-style">     <li id="list25" label="(1)">the answer requires to do some inferences;<br/></li>     <li id="list26" label="(2)">the answer requires some geographical reasoning;<br/></li>     <li id="list27" label="(3)">the resolution needs a complex adaptation to the schema of the knowledge base.<br/></li>    </ol>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Inferences</h3>     </div>     </header>     <p>Some questions can only be answered by human-made inference obtained by relation composition according to the meaning of the relations involved.</p>     <ul class="list-no-style">     <li id="list28" label="&#x2022;">Question : Name the children of Sani Abacha<br/></li>     <li id="list29" label="&#x2022;">Answer : <span style="text-decoration: underline;">Mohammed Abacha</span>      <br/></li>     <li id="list30" label="&#x2022;">Path :<br/>      <ul class="list-no-style">       <li id="list31" label="&#x2022;">(Sani_Abacha, <em>spouse</em>, Maryam_Abacha)<br/></li>       <li id="list32" label="&#x2022;">(Maryam_Abacha, <em>child</em>, Mohammed_Abacha)<br/></li>      </ul></li>     </ul>     <p>This kind of question is usually answered by a single triple: the entity of the person, the relation <em>child</em> and the entity of its child. Our DBpedia version does not contain this information for Sani Abacha.</p>     <p>However the path from <em>Sani Abacha</em> to <em>Mohammed Abacha</em> that goes through the spouse relation of <em>Sani Abacha</em> has been found. Answering that question with the KB means inferring that the children of his wife are also his children.</p>     <ul class="list-no-style">     <li id="list33" label="&#x2022;">Question : What is Jane Goodall known for?<br/></li>     <li id="list34" label="&#x2022;">Answer : <span style="text-decoration: underline;">London-born primatologist</span>      <br/></li>     <li id="list35" label="&#x2022;">Path :<br/>      <ul class="list-no-style">       <li id="list36" label="&#x2022;">(Dian_Fossey, <em>influences</em>, Jane_Goodall)<br/></li>       <li id="list37" label="&#x2022;">(Dian_Fossey, <em>fields</em>, Primatology)<br/></li>      </ul></li>     </ul>     <p>The field of Jane Goodal is not directly available in DBpedia, but the fact that she was influenced by Dian Fossey who is a primatologist makes it possible to infer she is a primatologist too.</p>     <p>In these cases, the question words do not explicitly refer to the two relations, and requires a complex treatment for finding them.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Geographical relations</h3>     </div>     </header>     <p>For some questions that expect a type of location as answer, some geographical inference may be needed to find a precise location.</p>     <ul class="list-no-style">     <li id="list38" label="&#x2022;">Question : Where is the Leaning Tower?<br/></li>     <li id="list39" label="&#x2022;">Answers : <span style="text-decoration: underline;">Pisa</span>      <br/></li>     <li id="list40" label="&#x2022;">Path :<br/>      <ul class="list-no-style">       <li id="list41" label="&#x2022;">(Leaning_Tower_of_Pisa, <em>province</em>, Province_of_Pisa)<br/></li>       <li id="list42" label="&#x2022;">(Province_of_Pisa, <em>seat</em>, Pisa)<br/></li>      </ul></li>     </ul>     <p>To link Pisa to the <em>Leaning_Tower_of_Pisa</em>, the path goes through <em>Province_of_Pisa</em> which has a relation to <em>Pisa</em>. Province of Pisa could be an accepted answer, but being able to find the Pisa entity is more precise. This example does not picture a general rule for geographical inference (the reasoning for finding the precise location of the Leaning Tower will not apply in other cases)</p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Adaptation to a complex knowledge base schema</h3>     </div>     </header>     <p>One of the main challenges in KB QA is to match question words to the relevant relations. This is particularly difficult when this matching involves a one to many or a many to one correspondence.</p>     <ul class="list-no-style">     <li id="list43" label="&#x2022;">Question : What is the name of the chairman of the Federal Reserve Board?<br/></li>     <li id="list44" label="&#x2022;">Answers : <span style="text-decoration: underline;">Alan Greenspan</span>      <br/></li>     <li id="list45" label="&#x2022;">Path :<br/>      <ul class="list-no-style">       <li id="list46" label="&#x2022;">(Federal_Reserve_Board_of_Governors, <em>leaderTitle</em>, Chair_of_the_Federal_Reserve)<br/></li>       <li id="list47" label="&#x2022;">(Alan_Greenspan, <em>title</em>, Chair_of_the_Federal_Reserve)<br/></li>      </ul></li>     </ul>     <p>The answer path goes through <em>Chair_of_the_Federal_Reserve</em> to find <em>Alan Greenspan</em>. Chairman is involved in an entity name, Chair_of_the_Federal_Reserve and in relation names, leaderTitle and title, which makes it hard to find.</p>     <ul class="list-no-style">     <li id="list48" label="&#x2022;">Question : What party does Edouard Balladur represent?<br/></li>     <li id="list49" label="&#x2022;">Answers : <span style="text-decoration: underline;">conservative</span>      <br/></li>     <li id="list50" label="&#x2022;">Path :<br/>      <ul class="list-no-style">       <li id="list51" label="&#x2022;">(Edouard_Balladur, <em>party</em>, Union_for_a_Popular_Movement)<br/></li>       <li id="list52" label="&#x2022;">(Union_for_a_Popular_Movement, <em>ideology</em>, Conservatism)<br/></li>      </ul></li>     </ul>     <p>The expected answer for this question is <em>conservative</em>. In fact, it is not the name of the party but its ideology. A direct triple between <em>&#x00C9;douard_Balladur</em> and <em>Conservatism</em> cannot be found but it is possible to find a path between these 2 entities by going through the <em>Union_for_a_Popular_Movement</em> entity. According to the granularity of the expected answer, the word party has to be linked to two relations.</p>    </section>   </section>   <section id="sec-18">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>     </div>    </header>    <p>QA systems are generally dedicated to search in a single kind of source, a knowledge base or texts. However, using the two kinds of resources would lead to build more powerful systems. A challenge is then to study which kinds of phenomena occur, which kind of cooperation can be useful and how to leverage QA methods. For these purposes, we enrich automatically a textual corpus conceived for textual QA with KB annotations (KB entities and a path between them of length 1 or 2) in order to complement textual triples (question, answer, passage) with relevant KB material. While paths made of one relation are often correct, 2-length paths need to be curated. After annotation, the corpus contains around 1700 questions that have a connection with DBpedia and text. Among them, around 560 are annotated with a KB path useful for their resolution. We also showed that searching for a KB solution to questions that have been asked in a text retrieval context leads to propose new kinds of questions that require complex reasoning. For future work, we envisage to explore the automatic curating of the corpus, in order to define a distant supervision process that does not generate too much noise and allows for building larger corpora.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-19">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>The work is partly supported by the ANR project GoAsQ (ANR-15-CE23-0022) and the FUI project Pulsar.</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Ricardo Baeza-Yates, Berthier Ribeiro-Neto, <em>et al.</em> 1999. <em>      <em>Modern information retrieval</em>     </em>. Vol.&#x00A0;463. ACM press New York.</li>     <li id="BibPLXBIB0002" label="[2]">Romain Beaumont, Brigitte Grau, and Anne-Laure Ligozat. 2015. SemGraphQA@QALD5: LIMSI participation at QALD5@CLEF. <em>      <em>In Working Notes of CLEF 2015</em>     </em>.</li>     <li id="BibPLXBIB0003" label="[3]">Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. <em>      <em>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>     </em>. 1533&#x2013;1544.</li>     <li id="BibPLXBIB0004" label="[4]">Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. <em>      <em>In Advances in Neural Information Processing Systems</em>     </em>. 2787&#x2013;2795.</li>     <li id="BibPLXBIB0005" label="[5]">Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. <em>      <em>In Proceedings of ACL 2017</em>     </em>. 1870&#x2013;1879.</li>     <li id="BibPLXBIB0006" label="[6]">Jennifer Chu-Carroll, James Fan, BK Boguraev, David Carmel, Dafna Sheinwald, and Chris Welty. 2012. Finding needles in the haystack: Search and candidate generation. <em>      <em>IBM Journal of Research and Development</em>     </em>56, 3.4 (2012), 6&#x2013;1.</li>     <li id="BibPLXBIB0007" label="[7]">Silviu Cucerzan and Eugene Agichtein. 2005. Factoid Question Answering over Unstructured and Structured Web Content.. <em>      <em>In TREC Proceedings</em>     </em>, Vol.&#x00A0;72. 90.</li>     <li id="BibPLXBIB0008" label="[8]">Dennis Diefenbach, Vanessa Lopez, Kamal Singh, and Pierre Maret. 2017. Core techniques of question answering systems over knowledge bases: a survey. <em>      <em>Knowledge and Information Systems</em>     </em>(2017), 1&#x2013;41.</li>     <li id="BibPLXBIB0009" label="[9]">Martin Gleize and Brigitte Grau. 2015. A Unified Kernel Approach for Learning Typed Sentence Rewritings. <em>      <em>In Proceedings of ACL-IJCNLP</em>     </em>.</li>     <li id="BibPLXBIB0010" label="[10]">Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco, Anne-Laure Ligozat, Isabelle Robba, and Anne Vilnat. 2011. Selecting answers to questions from Web documents by a robust validation process. <em>      <em>In Proceedings of Web Intelligence Conference (WI)</em>     </em>.</li>     <li id="BibPLXBIB0011" label="[11]">Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement. <em>      <em>In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>     </em>. Association for Computational Linguistics, San Diego, California, 937&#x2013;948. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/N16-1108"      target="_blank">http://www.aclweb.org/anthology/N16-1108</a></li>     <li id="BibPLXBIB0012" label="[12]">Wesley Hildebrandt, Boris Katz, and Jimmy&#x00A0;J Lin. 2004. Answering Definition Questions Using Multiple Knowledge Sources.. <em>      <em>In HLT-NAACL</em>     </em>. 49&#x2013;56.</li>     <li id="BibPLXBIB0013" label="[13]">Denis Lukovnikov, Asja Fischer, Jens Lehmann, and S&#x00F6;ren Auer. 2017. Neural network-based question answering over knowledge graphs on word and character level. <em>      <em>In Proceedings of the 26th international conference on World Wide Web</em>     </em>. International World Wide Web Conferences Steering Committee, 1211&#x2013;1220.</li>     <li id="BibPLXBIB0014" label="[14]">Bernardo Magnini, Alessandro Vallin, Christelle Ayache, Gregor Erbach, Anselmo Pe&#x00F1;as, Maarten De&#x00A0;Rijke, Paulo Rocha, Kiril Simov, and Richard Sutcliffe. 2004. Overview of the CLEF 2004 multilingual question answering track. <em>      <em>In Workshop of the Cross-Language Evaluation Forum for European Languages</em>     </em>. Springer, 371&#x2013;391.</li>     <li id="BibPLXBIB0015" label="[15]">Seonyeong Park, Soonchoul Kwon, Byungsoo Kim, and Gary&#x00A0;Geunbae Lee. 2015. ISOFT at QALD-5: Hybrid question answering system over linked data and text data. <em>      <em>In Working Notes of CLEF 2015</em>     </em>.</li>     <li id="BibPLXBIB0016" label="[16]">Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. <em>      <em>arXiv preprint arXiv:1606.05250</em>     </em>(2016).</li>     <li id="BibPLXBIB0017" label="[17]">Christina Unger, Lorenz B&#x00FC;hmann, Jens Lehmann, Axel-Cyrille Ngonga&#x00A0;Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. <em>      <em>In Proceedings of the 21st international conference on World Wide Web</em>     </em>. ACM, 639&#x2013;648.</li>     <li id="BibPLXBIB0018" label="[18]">Christina Unger, Corina Forascu, Vanessa Lopez, ACN Ngomo, E Cabrio, Philipp Cimiano, and Sebastian Walter. 2014. Question Answering over Linked Data (QALD-4). CLEF Conference.</li>     <li id="BibPLXBIB0019" label="[19]">Alessandro Vallin, Bernardo Magnini, Danilo Giampiccolo, Lili Aunimo, Christelle Ayache, Petya Osenova, Anselmo Pe&#x00F1;as, Maarten De&#x00A0;Rijke, Bogdan Sacaleanu, Diana Santos, <em>et al.</em> 2005. Overview of the CLEF 2005 multilingual question answering track. <em>      <em>In Workshop of the Cross-Language Evaluation Forum for European Languages</em>     </em>. Springer, 307&#x2013;331.</li>     <li id="BibPLXBIB0020" label="[20]">Ellen&#x00A0;M Voorhees. 2001. Overview of TREC 2001 Question Answering Track.. <em>      <em>In TREC Conference</em>     </em>.</li>     <li id="BibPLXBIB0021" label="[21]">Ellen&#x00A0;M Voorhees. 2003. Overview of the TREC 2003 Question Answering Track.. <em>      <em>In TREC Conference</em>     </em>, Vol.&#x00A0;2003. 54&#x2013;68.</li>     <li id="BibPLXBIB0022" label="[22]">Ellen&#x00A0;M Voorhees. 2004. Overview of the TREC 2004 Question Answering Track. (2004).</li>     <li id="BibPLXBIB0023" label="[23]">Mengqiu Wang and Christopher&#x00A0;D Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. <em>      <em>In COLING</em>     </em>.</li>     <li id="BibPLXBIB0024" label="[24]">Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Hybrid Question Answering over Knowledge Base and Free Text. <em>      <em>In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em>     </em>. The COLING 2016 Organizing Committee, Osaka, Japan, 2397&#x2013;2407. <a class="link-inline force-break" href="http://aclweb.org/anthology/C16-1226"      target="_blank">http://aclweb.org/anthology/C16-1226</a></li>     <li id="BibPLXBIB0025" label="[25]">Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Question Answering on Freebase via Relation Extraction and Textual Evidence. <em>      <em>In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>     </em>. Association for Computational Linguistics, Berlin, Germany, 2326&#x2013;2336. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/P16-1220"      target="_blank">http://www.aclweb.org/anthology/P16-1220</a></li>     <li id="BibPLXBIB0026" label="[26]">Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2013. Robust question answering over the web of linked data. <em>      <em>In Proceedings of CIKM</em>     </em>.</li>     <li id="BibPLXBIB0027" label="[27]">Xuchen Yao and Benjamin Van&#x00A0;Durme. 2014. Information extraction over structured data: Question answering with freebase. <em>      <em>In Proceedings of ACL</em>     </em>.</li>     <li id="BibPLXBIB0028" label="[28]">Xuchen Yao, Benjamin Van&#x00A0;Durme, Chris Callison-Burch, and Peter Clark. 2013. A Lightweight and High Performance Monolingual Word Aligner.. <em>      <em>In Proceedings of ACL</em>     </em>. 702&#x2013;707.</li>     <li id="BibPLXBIB0029" label="[29]">Xuchen Yao, Benjamin Van&#x00A0;Durme, Chris Callison-Burch, and Peter Clark. 2013. Semi-Markov Phrase-Based Monolingual Alignment.. <em>      <em>In Proceedings of EMNLP</em>     </em>. 590&#x2013;600.</li>     <li id="BibPLXBIB0030" label="[30]">Lei Zou, Ruizhe Huang, Haixun Wang, Jeffer&#x00A0;Xu Yu, Wenqiang He, and Dongyan Zhao. 2014. Natural language question answering over RDF: a graph data driven approach. <em>      <em>In Proceedings of SIGMOD</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>The corpus can be found at <a class="link-inline force-break"     href="https://zenodo.org/record/1186300#.Wpbj-eYiE5s">https://zenodo.org/record/1186300#.Wpbj-eYiE5s</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>More references are given at <a class="link-inline force-break"     href="http://aclweb.org/aclwiki/index.php?title=Question_Answering_State_of_the_art">http://aclweb.org/aclwiki/index.php?title=Question_Answering_State_of_the_art</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>Results of several Deep Neural Network models using the SQUAD Dataset can be found on the leaderboard <a class="link-inline force-break"     href="https://rajpurkar.github.io/SQuAD-explorer/">https://rajpurkar.github.io/SQuAD-explorer/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break" href="http://trec.nist.gov/data/qamain.html">http://trec.nist.gov/data/qamain.html</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break"     href="http://nlp.uned.es/clef-qa/repository/qa.php">http://nlp.uned.es/clef-qa/repository/qa.php</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class="link-inline force-break"     href="https://www.microsoft.com/en-us/download/details.aspx?id=52318">https://www.microsoft.com/en-us/download/details.aspx?id=52318</a>   </p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break"     href="https://rajpurkar.github.io/SQuAD-explorer/">https://rajpurkar.github.io/SQuAD-explorer/</a>   </p>   <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>on 9&#x00A0;227 questions for text QA that were annotated by a named entity recognizer, about 59 % of questions mention an entity</p>   <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a>Some questions implicitly involve a time stamp, as with latest, and the answer may not be correct anymore.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191540">https://doi.org/10.1145/3184558.3191540</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
