<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Prediction of Sparse User-Item Consumption Rates with
  Zero-Inflated Poisson Regression</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186153'>https://doi.org/10.1145/3178876.3186153</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186153'>https://w3id.org/oa/10.1145/3178876.3186153</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Prediction of Sparse User-Item
          Consumption Rates with Zero-Inflated Poisson
          Regression</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Moshe</span> <span class=
          "surName">Lichman</span><a class="fn" href="#fn1" id=
          "foot-fn1"><sup>⁎</sup></a>, Department of Computer
          Science University of California, Irvine, <a href=
          "mailto:lichman@gmail.com">lichman@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Padhraic</span> <span class=
          "surName">Smyth</span>, Department of Computer Science
          University of California, Irvine, <a href=
          "mailto:smyth@ics.uci.edu">smyth@ics.uci.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186153"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186153</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>In this paper we address the problem of building
        user models that can predict the rate at which individuals
        consume items from a finite set, including items they have
        consumed in the past and items that are new. This
        combination of repeat and new item consumption is common in
        applications such as listening to music, visiting web
        sites, and purchasing products. We use zero-inflated
        Poisson (ZIP) regression models as the basis for our
        modeling approach, leading to a general framework for
        modeling user-item consumption rates over time. We show
        that these models are more flexible in capturing user
        behavior than alternatives such as well-known latent factor
        models based on matrix factorization. We compare the
        performance of ZIP regression and latent factor models on
        three different data sets involving music, restaurant
        reviews, and social media. The ZIP regression models are
        systematically more accurate across all three data sets and
        across different prediction metrics.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Consumption Rate
          Modeling</small>,</span> <span class=
          "keyword"><small>Repeat Consumption</small>,</span>
          <span class=
          "keyword"><small>Explore-Exploit</small>,</span>
          <span class="keyword"><small>Zero-Inflated
          Poisson</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Moshe Lichman and Padhraic Smyth. 2018. Prediction of
          Sparse User-Item Consumption Rates with Zero-Inflated
          Poisson Regression. In <em>WWW 2018: The 2018 Web
          Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186153" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186153</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>In many aspects of our daily lives the way we consume
      products and items has evolved from interactions in a
      physical world to interactions in digital worlds. We purchase
      books online instead of shopping at brick-and-mortar stores,
      stream music and movies online instead of purchasing physical
      copies, and so on. The digital nature of our consumption
      provides the opportunity for tailoring of individual user
      experiences that can benefit both the consumer and the
      provider. As a consequence, the ability to develop predictive
      individual-level models for user-item consumption from past
      observations is increasingly important across a variety of
      applications.</p>
      <p>Building accurate models of consumption in a typical
      digital environment is challenging for multiple reasons. In
      particular, as an individual moves forward through time, the
      items an individual consumes are a combination of (a) items
      that they have consumed in the past (i.e., repeat
      consumption), and (b) novel items that they have not consumed
      in the past (i.e., new consumption). User models in this
      context must balance these two aspects of behavior.</p>
      <p>Individual heterogeneity, in the form of significant
      variability in behavior across users, further complicates the
      modeling process. In particular, when the set of possible
      items to be consumed is large, different users may have very
      different consumption patterns. Another significant challenge
      is data sparsity, given that the number of items a user
      typically consumes is often a very small fraction of the
      total number of available items.</p>
      <p>In this paper we focus on the problem of predicting
      <em>rates</em> of item consumption per unit time (days,
      weeks, months) for individual users. The prediction of rates
      is broadly useful in a variety of applications since it
      allows us to predict not only which items a user will
      consume, but also how often those items will be consumed. For
      example, prediction of rates of consumption for specific
      items and specific sets of users is important in the design
      and engineering of proxy-caching systems for online streaming
      media content&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>]. For contexts where items have
      different costs associated with them, predictions of the
      rates at which a user will consume specific items can be used
      for estimating the expected value of a customer from the
      provider perspective. Rates also can be used to help evaluate
      the expected benefit of interventions such as providing
      incentives to a user. For example, if some users have a high
      rate of usage for a particular app on their mobile phones and
      other users have low rates of usage for the same app, the
      latter group is likely to be a better target for
      incentivization than the former&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0008">8</a>].</p>
      <p>As mentioned above, in many real-world applications
      consumption behavior is characterized by a combination of
      repeat and new consumption. For example, some users’
      behaviors may be highly repetitive in nature, e.g., they tend
      to visit the same restaurants or listen to the same music
      artists, and rarely try new items. Other users may have
      behavior at the other extreme, continuously exploring new
      items and rarely returning to old items. This trade-off
      between exploration and exploitation is well known in
      computer science in the context of reinforcement learning,
      and is also well-established in cognitive science as a basic
      trait of how humans interact with the world around them
      (e.g., [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0024">24</a>]).</p>
      <p>These observations suggest that in addition to handling
      significant heterogeneity in terms of individual behavior,
      the notion that there is a steady-state behavior for many
      users may be a fallacy in the sense that users are continuing
      over time to both exploit and explore the choice of items
      available to them. Rather than having user models that are
      represented as fixed distributions over items, individual
      behavior can be thought as a dynamic process over time that
      is driven by feedback from past item consumption, both
      positive and negative. To capture these ideas we develop
      individual-level Poisson-based regression models where the
      predicted rate that a user will consume an item in the next
      time period is modeled as a function of an individual's past
      behavior. In addition, the models use global contextual
      information (such as item popularity) in order to better
      generalize to prediction of new items.</p>
      <p>The primary contribution of this paper is the development
      of a systematic approach for modeling user-item consumption
      rates over time using Poisson-based regression models with
      zero-inflation. Through a systematic investigation of several
      user-item consumption data sets from multiple domains, we
      demonstrate that this modeling approach can capture
      individual-level user preferences for both old and new items
      as a function of past behavior and contextual information. We
      compare the proposed approach to state-of-the-art
      alternatives both empirically and qualitatively and also show
      that the proposed approach is scalable to large-scale data
      sets.</p>
      <p>On the surface the problem we address looks very similar
      to that of the classic recommendation system problem.
      However, it is important to note that the modeling goals and
      evaluation criteria in our work are significantly different.
      Recommender systems focus only on prediction and ranking of
      new items that a user has not consumed in the past, e.g., for
      items such as movies or books, where typically an item is
      only consumed once by a user. In contrast we specifically
      focus on problems where consumption is a mix of repeated and
      novel item consumption. In this context a natural approach is
      to predict the <em>rates</em> at which items are consumed and
      to evaluate how well these rates are predicted, rather than
      just evaluating the likelihood of whether a user will consume
      an item or not.</p>
      <p>The remainder of this paper proceeds as follows. In
      Section&nbsp;<a class="sec" href="#sec-4">2</a> we explore
      different user-item consumption sequence data sets, and
      provide motivation for our modeling approach in
      Section&nbsp;<a class="sec" href="#sec-5">3</a>. In
      Section&nbsp;<a class="sec" href="#sec-8">4</a> we describe
      the proposed ZIP model for understanding and predicting
      user-item consumption rates and we show how this model is
      learned using user-item consumption observations in
      Section&nbsp;<a class="sec" href="#sec-11">5</a>.
      Section&nbsp;<a class="sec" href="#sec-12">6</a> provides an
      overview of the existing approaches for modeling user-item
      consumption data. In Section&nbsp;<a class="sec" href=
      "#sec-13">7</a> we compare our proposed model to a variety of
      state-of-the-art alternatives and interpret the results.
      Section <a class="sec" href="#sec-17">8</a> discusses the
      scalability of the approach and we conclude with a brief
      discussion in Section&nbsp;<a class="sec" href=
      "#sec-18">9</a>.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Problem
          Statement and User-Item Consumption Data</h2>
        </div>
      </header>
      <p>In this paper we consider user-item consumption counts
      measured in discrete time intervals (by day, week, month,
      etc.). We define <span class="inline-equation"><span class=
      "tex">$y_{ij}^t \in \lbrace 0, 1, 2, \ldots
      \rbrace$</span></span> as the number of consumptions of item
      <em>j</em> ∈ {1, 2, …, <em>M</em>} by user <em>i</em> ∈ {1,
      2, …, <em>N</em>} in time window <em>t</em> ∈ {1, 2, …,
      <em>T</em>}. In this context the goal of our work is to
      predict the expected number of items of type <em>j</em> that
      user <em>i</em> will consume during time <em>t</em> + 1,
      <span class="inline-equation"><span class="tex">$E[
      y_{ij}^{t+1} | \ldots ]$</span></span> given the history of
      all user-item consumption up through time <em>t</em>. It
      should be relatively straightforward to extend the approach
      to continuous time, where each consumption event has its own
      time-stamp—here we focus on the discrete-time case.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Summary of the three datasets used in this
          paper: number of unique users <em>N</em>, unique items
          <em>M</em>, time-window <em>t</em>, number of windows
          <em>T</em>, and the percentage of data points that are
          non-zero.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">
              <strong>Dataset</strong></th>
              <th style="text-align:center;"><em>N</em></th>
              <th style="text-align:center;"><em>M</em></th>
              <th style="text-align:center;"><em>t</em></th>
              <th style="text-align:center;"><em>T</em></th>
              <th>% <strong>non-zero</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr style="border-top: solid 2px">
              <td style="text-align:center;">reddit</td>
              <td style="text-align:center;">1000</td>
              <td style="text-align:center;">1000</td>
              <td style="text-align:center;">week</td>
              <td style="text-align:center;">52</td>
              <td style="text-align:center;">2.5</td>
            </tr>
            <tr>
              <td style="text-align:center;">lastfm</td>
              <td style="text-align:center;">931</td>
              <td style="text-align:center;">19997</td>
              <td style="text-align:center;">month</td>
              <td style="text-align:center;">50</td>
              <td style="text-align:center;">0.5</td>
            </tr>
            <tr>
              <td style="text-align:center;">Yelp</td>
              <td style="text-align:center;">2836</td>
              <td style="text-align:center;">203</td>
              <td style="text-align:center;">2 months</td>
              <td style="text-align:center;">12</td>
              <td style="text-align:center;">1.7</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>Our work is motivated by the challenge of creating a
      general framework for consumer behavior data across different
      domains. To that end, we investigate multiple publicly
      available data sets that represent different types of items
      and consumption activities. The 3 data sets are summarized
      and compared in Table&nbsp;<a class="tbl" href=
      "#tab1">1</a>.</p>
      <p><strong>Reddit</strong>: reddit is a popular social
      network with on the order of 1 million topic-focused
      subgroups (known as subreddits) where users can post,
      comment, and vote on content. In this work we considered data
      from a sample of <em>N</em> = 1000 users with high activity
      and <em>M</em> = 1000 highly active subreddits throughout
      2015. The value of <span class="inline-equation"><span class=
      "tex">$y_{ij}^t$</span></span> is defined as the number of
      times user <em>i</em> posted (or commented) in subreddit
      <em>j</em> during a given week <em>t</em>.</p>
      <p><strong>Lastfm</strong>: lastfm is an online music
      streaming service that allows users to listen to a selected
      song or playlist. The particular dataset we use contains the
      listening actions over time of nearly <em>N</em> = 1000
      users<a class="fn" href="#fn2" id=
      "foot-fn2"><sup>1</sup></a>. We consider artists as items and
      retain the top <em>M</em> = 20<em>K</em> artists that are
      most frequently listened to during the period of time of
      February 2005 to June 2009. In the lastfm dataset,
      <span class="inline-equation"><span class=
      "tex">$y_{ij}^t$</span></span> represents the number of times
      that user <em>i</em> listened to a song performed by artist
      <em>j</em> during a month <em>t</em>.</p>
      <p><strong>Yelp</strong>: Yelp is a popular review platform
      that allows users to share their experience with different
      service providers such as restaurants. The dataset that we
      use has been widely used as a benchmark across recommendation
      system studies<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>2</sup></a>. For our experiments we focused
      on the histories of <em>N</em> = 2836 unique users and their
      reviews of <em>M</em> = 203 types of restaurants (e.g. fast
      food, Mexican, sushi, etc.) in the Scottsdale and Phoenix
      (Arizona, USA) metropolitan areas between June 2014 and June
      2016. <span class="inline-equation"><span class=
      "tex">$y_{ij}^t$</span></span> is the number of times user
      <em>i</em> reviewed a restaurant of type <em>j</em> during
      every two months <em>t</em>.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Excess Zeros
          and Heterogeneity</h2>
        </div>
      </header>
      <p>Two typical characteristics of sparse user-item data sets
      are (1) an excess of zeros and (2) heterogeneity across both
      users and items. We discuss both characteristics below in
      turn and describe our approach to handling each from a
      modeling perspective.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186153/images/www2018-162-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The distribution of item-consumption rates
          for a sample of user-item pairs with similar
          <em>average</em> rates.</span>
        </div>
      </figure>
      <p></p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Excess
            Zeros</h3>
          </div>
        </header>
        <p>A common feature of user-item consumption data sets,
        particularly when the number of items is large, is a very
        high rate of zeros, i.e., most users do not consume the
        vast majority of items. This is certainly true of the 3
        data sets we analyze in this paper where roughly 98% to 99%
        of the entries are zero across the datasets. This is not
        surprising: for high-dimensional data sets each user will
        only be exposed to or be aware of a relatively small
        fraction of the potential items that they could interact
        with. In addition, there are practical limits (e.g., from
        cognitive and economic perspectives) in terms of how many
        items a user can realistically interact with.</p>
        <p>In statistical modeling an overabundance of zeros is
        often referred to as “zero-inflation”&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0015">15</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>], to reflect the
        phenomenon that the frequency of zeros in the data is
        significantly higher than what a typical parametric model
        for count data (such as a Poisson model) can handle. This
        observation has been made in application contexts as
        diverse as epidemiology, economics, and manufacturing
        (e.g., see [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>]), but has seen relatively little
        application to the type of high-dimensional user-item
        consumption data that we investigate in this
        pape—exceptions are [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>], which we discuss in more detail
        later in the paper.</p>
        <p>To illustrate the phenomenon of zero-inflation
        Figure&nbsp;<a class="fig" href="#fig1">1</a> shows a
        histogram of the <span class="inline-equation"><span class=
        "tex">$y_{ij}^t$</span></span> values for a sample of
        user-item pairs from all datasets with an average
        consumption rate between 5 and 6 across time (values of
        <span class="inline-equation"><span class="tex">$y_{ij}^t =
        0$</span></span> were excluded in computing the average).
        The histogram illustrates the variability of <span class=
        "inline-equation"><span class=
        "tex">$y_{ij}^t$</span></span> values across all time
        windows. We can see that the consumption rate has a bimodal
        distribution with one mode at <span class=
        "inline-equation"><span class=
        "tex">$y_{ij}^t=0$</span></span> and additional mode at
        <span class="inline-equation"><span class=
        "tex">$y_{ij}^t=6$</span></span> . We selected the average
        rate of 5 to 6 for illustration—similar bimodal patterns
        occur for different values of average number of user-item
        consumption. The bimodal nature of this data suggests that
        user-item rate can be represented as a mixture of two
        processes: an <em>exposure process</em> and a <em>rate
        process</em>.</p>
        <p><strong>Exposure Process</strong>: The exposure process
        describes whether or not a user <em>i</em> has been
        <em>exposed</em> to item <em>j</em> at time <em>t</em>. The
        concept of exposure captures the idea that for large item
        sets a typical user is likely to be unaware of (or
        unexposed to) most items in the “item vocabulary” (see also
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0009">9</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0016">16</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0018">18</a>]), e.g.,
        in music-listening many artists are unknown to many users.
        We define <span class="inline-equation"><span class=
        "tex">$z_{ij}^t \in \lbrace 0, 1\rbrace$</span></span> as
        an indicator variable to indicate if user <em>i</em> was
        exposed to item <em>j</em> at time <em>t</em>. We can model
        <span class="inline-equation"><span class="tex">$P(z_{ij}^t
        = 1)$</span></span> via a Bernoulli distribution with
        parameter <span class="inline-equation"><span class=
        "tex">$\pi _{ij}^t$</span></span> , where the Bernoulli
        parameter will be a function of the past history of user
        <em>i</em> and item <em>j</em>.</p>
        <p><strong>Rate Process</strong>: Conditioned on exposure,
        i.e., <span class="inline-equation"><span class=
        "tex">$z_{ij}^t = 1$</span></span> , the rate process
        accounts for the number of times user <em>i</em> consumes
        item <em>j</em> at time <em>t</em>. A natural and simple
        distribution for the rate process is the Poisson model,
        parameterized by the expected consumption rate <span class=
        "inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> :</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} P(y_{ij}^t = k|
            \lambda _{ij}^t) = \frac{{\lambda _{ij}^t}^k
            e^{-\lambda _{ij}^t}}{k!} \end{align}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>where <em>k</em> = 0, 1, 2, … is the number of
        consumptions. There are a number of other alternatives for
        defining probability distributions over count data that we
        could have used in our modeling approach, such as the
        non-negative Binomial distribution (NBD). We chose to use
        the Poisson distribution since it is straightforward to
        interpret the model parameters and is simple to implement.
        Using an NBD model, within the same general modeling
        framework that we propose here, could in principle lead to
        more accurate predictive models than the Poisson.
        <p></p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Data
            Heterogeneity</h3>
          </div>
        </header>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186153/images/www2018-162-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Average number of unique
            consumed items (left) and total number of consumed
            items (right) per week for each user across each of the
            3 data sets.</span>
          </div>
        </figure>
        <p>Another common feature of high-dimensional user-item
        data sets is heterogeneity. Figure&nbsp;<a class="fig"
        href="#fig2">2</a> shows boxplots of the average number of
        unique items each user consumes (left panel) and the
        average number of total consumed items for each user (right
        panel), per week, for each of the 3 data sets. The figure
        clearly indicates (a) significant variability across users,
        as well as (b) significant variability across the different
        data sets. A plausible explanation for user variability is
        that different users can have significantly different
        <em>budgets</em>, either monetary or non-monetary (e.g.
        time), for consuming items. In addition there can be
        significant variation in the <em>domain-specific cost</em>
        for the consumption of a typical item across different data
        sets, leading to significant differences in the scale of
        user-item consumption across different domains, i.e.,
        <em>domain-specific cost offsets</em>. For example, the
        effective cost to a user of listening to a song (lastfm) is
        significantly less than the cost of visiting a restaurant
        for a meal (Yelp).</p>
        <p>Another contribution to data heterogeneity is the
        natural variation across users (and datasets) of some users
        to <em>explore</em> new items compared to their tendency to
        <em>exploit</em> known items. For example, a user who has a
        low tendency for exploration will naturally tend to repeat
        their behavior and the number and identity of unique items
        this user consumes is likely to remain relatively small and
        static over time. On the other hand a different user could
        have a tendency to be easily bored with items (a state that
        could be detected from recent activity&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0012">12</a>]) and a corresponding
        tendency to often explore new items, where the new items
        are perhaps strongly influenced by global popularity and
        trends in the data.</p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Poisson
          Regression with Zero-Inflation</h2>
        </div>
      </header>
      <p>Given the prevalence of zero-inflation and heterogeneity
      in user-item data sets we propose to model the observations
      <span class="inline-equation"><span class=
      "tex">$y_{ij}^t$</span></span> for user <em>i</em>, item
      <em>j</em>, at time <em>t</em>, as</p>
      <ol class="list-no-style">
        <li id="list1" label="(1)">a mixture of an exposure process
        and a rate process, where <span class=
        "inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> is the mixture weight and where
        <span class="inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> is the expected rate that user
        <em>i</em> will consume item <em>j</em> at time <em>t</em>
        (conditioned on being exposed to the item), and<br /></li>
        <li id="list2" label="(2)">regression models for each of
        <span class="inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> and <span class=
        "inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> , conditioned on features
        <span class="inline-equation"><span class=
        "tex">$x_{ij}^t$</span></span> .<br /></li>
      </ol>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Zero-Inflated Poisson Models</h3>
          </div>
        </header>
        <p>The <em>exposure</em> and <em>rate</em> processes are
        modeled via a mixture of two components: (a) a delta
        function at zero and (b) a Poisson distribution. The
        mixture model weights and Poisson rate parameters,
        <span class="inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> and <span class=
        "inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> respectively, are user and
        item-dependent and are implicit functions of the features
        <span class="inline-equation"><span class=
        "tex">$x_{ij}^t$</span></span> —we provide more details on
        the conditional models for these parameters later in this
        section.</p>
        <p>We can write the probability of <span class=
        "inline-equation"><span class="tex">$P_{zip}(y_{ij}^t = k|
        \pi _{ij}^t, \lambda _{ij}^t)$</span></span> as:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} P_{zip}(y_{ij}^t
            = k) = &amp;{\left\lbrace \begin{array}{@{}l@{\quad
            }l@{}}(1 - \pi _{ij}^t) + \pi _{ij}^t P_\lambda (k |
            \lambda _{ij}^t), &amp; k=0 \\\pi _{ij}^t P_\lambda (k
            | \lambda _{ij}^t), &amp; k = 1, 2, \ldots
            \end{array}\right.} \\
            \nonumber\end{align}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$P_\lambda (k|\lambda _{ij}^t)$</span></span> is the
        Poisson probability defined in Equation&nbsp;<a class="eqn"
        href="#eq1">1</a>. The model above is known as the
        zero-inflated Poisson (ZIP) regression model, where the
        regression aspect arises through the conditioning of
        <span class="inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> and <span class=
        "inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> on the features [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0015">15</a>]. In the ZIP model,
        zeros can be generated either by (a) the Bernoulli random
        variable <span class="inline-equation"><span class=
        "tex">$\pi _{ij}^t$</span></span> taking value 0 or (b)
        <span class="inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> taking value 1 and the Poisson model
        generating a value <em>k</em> = 0. From a generative
        perspective these two “routes” for generating zeros can be
        interpreted as either (a) the user <em>i</em> not being
        exposed to item <em>j</em>, or (b) the user being exposed
        but deciding not to consume the item (by drawing a zero
        from the Poisson distribution).
        <p></p>
        <p>An alternative to the ZIP model is to use a shifted
        Poisson process for the rate that has a minimum value of
        <em>k</em> = 1 (rather than <em>k</em> = 0). In the
        statistical literature this is known as a <em>hurdle
        model</em> in the sense that the Poisson model is invoked
        if the count is greater than the “hurdle” (where here the
        hurdle value is 0). We empirically compared the hurdle and
        the ZIP model (results not shown) and found that the ZIP
        model systematically outperformed the hurdle variant for
        our 3 data sets in terms of modeling and predicting
        user-item consumption rates. For this reason we focus on
        the ZIP model in the rest of the paper.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Regression
            Modeling of Mixture Parameters</h3>
          </div>
        </header>
        <p>We model heterogeneity across users and items via
        generalized linear regression models for both <span class=
        "inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> and <span class=
        "inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> , where the regression models depend
        on feature vectors <span class=
        "inline-equation"><span class=
        "tex">$x_{ij}^t$</span></span> that vary by user
        <em>i</em>, item <em>j</em> and time <em>t</em>. The
        regression models use two constant intercepts,
        globally-shared and individual-specific, capturing
        (respectively) the effect of <em>global domain costs</em>
        and heterogeneity in <em>individual-specific budgets</em>.
        In addition, we use four data-driven features, defined in
        Table <a class="tbl" href="#tab2">2</a>, that are computed
        from each individual user's historical data and from
        contextual information.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Definition of features used in our
            regression models, based on user and item historical
            data.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">
                <strong>Covariate</strong></th>
                <th style="text-align:center;">
                <strong>Notation</strong></th>
                <th style="text-align:center;">
                <strong>Value</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">Global domain
                costs</td>
                <td style="text-align:center;"><em>x</em>
                <sub>0</sub></td>
                <td style="text-align:center;">1</td>
              </tr>
              <tr>
                <td style="text-align:center;">User-specific
                Budget</td>
                <td style="text-align:center;"><em>x</em>
                <sub><em>i</em>0</sub></td>
                <td style="text-align:center;">1</td>
              </tr>
              <tr>
                <td style="text-align:center;">Past user-item
                preference</td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class=
                "tex">$x_{ij}^{\bar{t}}$</span></span></td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class="tex">$\log \left(1 +
                \frac{\sum _{\tau = 1}^{t} y_{ij}^\tau
                }{t}\right)$</span></span></td>
              </tr>
              <tr>
                <td style="text-align:center;">Current user-item
                activity</td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class=
                "tex">$x_{ij}^t$</span></span></td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class="tex">$\log (1 +
                y_{ij}^{t})$</span></span></td>
              </tr>
              <tr>
                <td style="text-align:center;">Historical item
                popularity</td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class=
                "tex">$x_{j}^{\bar{t}}$</span></span></td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class="tex">$\log (1 +
                \frac{\sum _i \sum _{\tau = 1}^{t} y_{ij}^\tau
                }{tN})$</span></span></td>
              </tr>
              <tr>
                <td style="text-align:center;">Current item
                popularity</td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class=
                "tex">$x_{j}^t$</span></span></td>
                <td style="text-align:center;"><span class=
                "inline-equation"><span class="tex">$\log (1 +
                \frac{\sum _i y_{ij}^{t}}{N})$</span></span></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The features capture different aspects of user and item
        histories and allow the model to capture the balance
        between <em>explore</em> and <em>exploit</em> for each
        individual. <strong>Past user-item preference</strong>,
        <span class="inline-equation"><span class=
        "tex">$x_{ij}^{\bar{t}}$</span></span> , represents the
        average rate that user <em>i</em> consumes item <em>j</em>
        over time and can capture the behavior of repetitive users
        who have a high probability of <em>exploitation</em>.
        <strong>Current user-item activity</strong>, <span class=
        "inline-equation"><span class=
        "tex">$x_{ij}^t$</span></span> , captures (on a log-scale)
        the recent activity of user <em>i</em> with item
        <em>j</em>, motivated by recent studies on the effect of
        recency and boredom in item consumption&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0001">1</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0010">10</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0011">11</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0012">12</a>].
        <strong>Historical item popularity</strong>, <span class=
        "inline-equation"><span class=
        "tex">$x_j^{\bar{t}}$</span></span> , reflects the overall
        popularity of an item and is expected to capture the
        behavior of users whose <em>exploration</em> preferences
        are affected by conformity&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0023">23</a>]. <strong>Current item
        popularity</strong>, <span class=
        "inline-equation"><span class="tex">$x_{j}^t$</span></span>
        , captures current trends in item popularity, allowing the
        model to reflect the behavior of users driven by trends
        such as hype as a result of a sale, or the “death” of an
        item.</p>
        <p>The use of features based on a user's past observations
        to predict the future behavior of the individual is an
        instance of an observation-driven time-series modeling
        approach (which we discuss further in the related-work
        section below). In particular, this allows for an
        individual's behavior to change over time in a
        non-stationary fashion. For example some individuals could
        be permanently in exploration mode to the extent that their
        future behavior is always different to their past (in terms
        of specific item consumption). More typical is the case
        where future behavior is a combination of repeat and novel
        item consumption, to varying degrees across different
        individuals.</p>
        <p>The features used in this paper (in Table&nbsp;<a class=
        "tbl" href="#tab2">2</a>) are somewhat general and other
        features could be used depending on the application. For
        example, more specific domain-dependent features could also
        be incorporated, such as static features that provide
        side-information about users and items&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0018">18</a>], or exogenous
        time-varying features such as seasonality or calendar
        effects&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>].</p>
        <p>Given the regression features we model the exposure and
        rate processes parameters in the following way:</p>
        <p><strong>Exposure Process:</strong> The value of
        <span class="inline-equation"><span class="tex">$\pi
        _{ij}^t$</span></span> is estimated using logistic
        regression, conditioned on the globally-shared and the
        individual-specific intercept coefficients <em>η</em>
        <sub>0</sub> and <em>η</em> <sub><em>i</em>0</sub>
        respectively, as well as the individual-based feature
        coefficient vector <em>η<sub>i</sub></em> = {<em>η</em>
        <sub><em>i</em>1</sub>, <em>η</em> <sub><em>i</em>2</sub>,
        <em>η</em> <sub><em>i</em>3</sub>, <em>η</em>
        <sub><em>i</em>4</sub>}. We denote the data-driven feature
        vector as <span class="inline-equation"><span class=
        "tex">$x_{ij}^t = \lbrace x_{ij}^{\bar{t}}, x_{ij}^{t},
        x_{j}^{\bar{t}}, x_{j}^t\rbrace$</span></span> and write
        the logistic function as:</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \pi _{ij}^t
            &amp;= \frac{1}{1 + e ^{ -(\eta _0 x_0 +\eta _{i0}
            x_{i0} + \eta _i x_{ij}^t)}} \end{align}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div><strong>Rate Process:</strong> Similarly, the value
        of <span class="inline-equation"><span class="tex">$\lambda
        _{ij}^t$</span></span> is modeled via Poisson regression
        with a globally-shared and individual-specific coefficient
        <em>β</em> <sub>0</sub> and <em>β</em>
        <sub><em>i</em>0</sub>, as well as an individual-specific
        coefficient vector <em>β<sub>i</sub></em> = {<em>β</em>
        <sub><em>i</em>1</sub>, <em>β</em> <sub><em>i</em>2</sub>,
        <em>β</em> <sub><em>i</em>3</sub>, <em>β</em>
        <sub><em>i</em>4</sub>}. In addition, as proposed
        in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0006">6</a>]
        and&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>], we added an additional intercept
        (<em>x</em> <sub><em>j</em>0</sub> = 1) with an
        item-specific offset <em>β</em> <sub><em>j</em>0</sub> to
        accommodate heterogeneity across items.
        <p></p>
        <p>The resulting Poisson regression model can be written
        as</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \log \lambda
            _{ij}^t = \beta _0 x_0 + \beta _{i0} x_{i0} +\beta
            _{j0} x_{j0} + \beta _i x_{ij}^t
            \end{align}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where the feature vector <span class=
        "inline-equation"><span class=
        "tex">$x_{ij}^t$</span></span> is defined in the same way
        as in Equation&nbsp;<a class="eqn" href="#eq3">3</a>.
        <p></p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Learning
          Algorithms</h2>
        </div>
      </header>
      <p>Since the ZIP regression model can be expressed as a
      two-component mixture model, with <span class=
      "inline-equation"><span class="tex">$P_\lambda
      (y_{ij}^t|\lambda = 0)$</span></span> as the zero-inflation
      component, the model parameters can be estimated via a
      standard application of the Expectation-Maximization (EM)
      algorithm. EM is a general procedure for iterative
      optimization of a likelihood function with missing
      information. For mixture models the missing information for
      each data point <span class="inline-equation"><span class=
      "tex">$y_{ij}^t$</span></span> is the identity of which
      component generated that data point. In particular, for ZIP
      mixtures this information is missing for all the zeros in the
      data set, <span class="inline-equation"><span class=
      "tex">$y_{ij}^t = 0$</span></span> , since these data points
      could have been generated by either component. For values
      <span class="inline-equation"><span class="tex">$y_{ij}^t
      {\gt} 0$</span></span> the data are unambiguously assigned to
      the rate component <span class="inline-equation"><span class=
      "tex">$P_\lambda (y_{ij}^t|\lambda _{ij}^t)$</span></span>
      .</p>
      <p>The E-step computes the membership probability (equivalent
      to the expected value of the binary membership indicator) for
      each data point <span class="inline-equation"><span class=
      "tex">$y_{ij}^t = 0$</span></span> , conditioned on current
      estimates of the model parameters. The M-step generates
      maximum likelihood estimates of the parameters conditioned on
      the membership probabilities provided by the E-step. Under
      fairly broad conditions, repeated application of E and M
      steps is guaranteed to converge to a (local) maximum of the
      likelihood function.</p>
      <p><strong>E-step</strong>: In the E-step, for each of the
      zero-valued data points <span class=
      "inline-equation"><span class="tex">$y_{ij}^t =
      0$</span></span> , we compute the membership probability
      <span class="inline-equation"><span class=
      "tex">$w_{ij}^t$</span></span> , namely the probability that
      this zero was generated by the rate component. These
      membership probabilities can be computed by applying Bayes
      rule to the definition of the mixture model above,
      <span class="inline-equation"><span class=
      "tex">$P_{zip}(y_{ij}^t=k|\pi _{ij}^t, \lambda
      _{ij}^t)$</span></span> , where the parameters <span class=
      "inline-equation"><span class="tex">$\pi _{ij}^t, \lambda
      _{ij}^t$</span></span> are the current parameter estimates
      (from the most recent M-step or their initial values at the
      first iteration).</p>
      <div class="table-responsive" id="eq5">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} w_{ij}^t =
          \frac{\pi _{ij}^t P_\lambda (y_{ij}^t| \lambda
          _{ij}^t)}{(1 - \pi _{ij}^t) P_\lambda (y_{ij}^t|\lambda
          =0) + \pi _{ij}^t P_\lambda (y_{ij}^t|\lambda _{ij}^t)}
          \end{align}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>Data points with membership weights closer to 1 are
      more likely (according to the current parameters) to have
      been generated by the rate component and, conversely, data
      with weights closer to 0 are more likely to have been
      generated by the zero-inflated component.
      <p></p>
      <p><strong>M-step</strong>: The M-step optimizes the
      parameters of the model conditioned on the current estimates
      of the <span class="inline-equation"><span class=
      "tex">$w_{ij}^t$</span></span> membership values. Our ZIP
      model has two sets of parameters, the logistic regression
      parameters for the mixture weights <em>η</em> = {<em>η</em>
      <sub>0</sub>, <em>η<sub>i</sub></em> }, and the rate
      parameters for the Poisson rate component in the mixture
      model, <em>β</em> = {<em>β</em> <sub>0</sub>,
      <em>β<sub>j</sub></em> , <em>β<sub>i</sub></em> }. The
      logistic regression uses the membership weights as targets
      and the Poisson regression uses weighted regression with the
      weights being the membership weights.</p>
      <p>Neither the logistic or Poisson regression can be
      performed in closed-form, so we use gradient descent within
      each M-step to estimate the coefficients for each model. The
      gradients in both cases (logistic and Poisson) involve dense
      sums over all <em>N</em> × <em>M</em> × <em>T</em> data
      values, where <em>N</em>, <em>M</em> and <em>T</em> are the
      number of users, items and time-windows respectively. This is
      in contrast to sparse estimation methods such as Poisson
      matrix factorization that can ignore the zeros in the data,
      effectively working with only a tiny fraction of the full
      data matrix for highly sparse data. Thus, in order to achieve
      a scalable algorithm, we use <em>stochastic gradient
      descent</em> (SGD) instead of full gradient methods, inspired
      by the success of SGD in training of large-scale deep neural
      networks on large data sets. SGD approximates the exact
      gradient at each gradient update by estimating the gradient
      in a stochastic manner using a small randomly-selected subset
      of rows (“mini-batches”) from the data matrix. We discuss the
      convergence of our EM + SGD method in more detail in
      Section&nbsp;<a class="sec" href="#sec-17">8</a> later in the
      paper—at this point it is sufficient to note that our
      implementation is as fast (or faster) in wall-clock time when
      compared to publicly-available implementations of other
      competing approaches.</p>
      <p>The step size in each SGD step was determined via the ADAM
      algorithm&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>] which provides a systematic way of
      conditioning the step size on the level of confidence in the
      gradient. We found empirically that ADAM worked well for our
      SGD-based optimization problems (and that convergence could
      be difficult to attain without it) in agreement with work in
      deep learning where the combination of SGD and adaptive
      step-size (such as ADAM) is essential to the success of
      training models on large data sets.</p>
      <p>One final note is that rather than maximizing the
      likelihood we maximized the likelihood times a prior, i.e.,
      maximum a posteriori EM estimation. In log-space this
      corresponds to maximizing (in the M-step) the log-likelihood
      plus a regularization term corresponding to the log prior. In
      our experiments we found that empirically-determined MAP
      priors were particularly effective. To compute the empirical
      prior we trained the model using global coefficients
      (assuming all data belong to a single user) with L2
      regularization. The learned coefficients were then used as a
      common prior for all users.</p>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Related
          Work</h2>
        </div>
      </header>
      <p>The conceptual basis of our work builds from a rich
      literature in statistics on modeling of count data [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0002">2</a>]. For
      example, within the framework of generalized linear models
      the Poisson mean is modeled as
      exp (∑<em>β<sub>k</sub>x<sub>k</sub></em> ) where the
      <em>β<sub>k</sub></em> ’s are regression coefficients and the
      <em>x<sub>k</sub></em> ’s are the inputs to the model. In the
      context of longitudinal data (data across multiple
      individuals) it is common to use fixed and random effects to
      account for individual-level heterogeneity, e.g., by allowing
      for individual-specific intercept terms in the mean such as
      exp (<em>β<sub>i</sub></em> +
      ∑<em>β<sub>k</sub>x<sub>k</sub></em> ) where
      <em>β<sub>i</sub></em> is the offset for individual
      <em>i</em> (e.g., [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>], ch.7). The incorporation of
      time-dependence into such models can typically be categorized
      into one of two general categories ([<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>], ch 7.2):
      <em>observation-driven</em> models where the counts are
      modeled directly as functions of past counts (such as
      autoregressive models for count data), or
      <em>parameter-driven</em> models where the counts depend on a
      latent state-space process (such as a hidden Markov model or
      a linear-Gaussian filter). The models we propose in this
      paper are in the <em>observation-driven</em> category, while
      the dynamic matrix factorization methods (discussed below)
      that we compare to in our experiments are in the
      <em>parameter-driven</em> category. The use of zero-inflation
      is also well-known in statistical modeling of count data
      (e.g., [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0007">7</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0015">15</a>]) and can
      be combined with other modeling components (as we do in our
      proposed approach) such as temporal dependence and fixed
      effects.</p>
      <p>While our approach builds on much of the above prior work
      in statistics, a significant difference is that we model
      <em>high-dimensional count vectors</em> (i.e., a large number
      of items). These count vectors are orders of magnitude larger
      in dimensionality than the low-dimensional (often scalar)
      count data that is often the primary focus in the statistical
      literature for modeling of count time-series [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>]. To handle the
      optimization challenges of parameter estimation for
      high-dimensional counts we use techniques from stochastic
      gradient optimization, which have not (to date at least) seen
      much application in the statistical literature for count
      modeling.</p>
      <p>Another significant line of related work is in matrix
      factorization of user-item consumption data. The most
      well-known approach in this context over the past decade has
      been the bilinear Gaussian model based on an SVD
      decomposition (e.g., [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>]). The expected target value
      <em>y<sub>ij</sub></em> is usually represented in such models
      as</p>
      <div class="table-responsive" id="eq6">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} E [y_{ij} ] =
          \theta ^{\prime }_i \phi _j + \beta _0 + \beta _i + \beta
          _j, \end{align}</span><br />
          <span class="equation-number">(6)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\theta ^{\prime }_i \phi _j$</span></span> is the
      inner product of low-dimensional latent vector
      representations for user <em>u</em> and item <em>i</em>, and
      <em>β</em> <sub>0</sub>, <em>β<sub>i</sub></em> , and
      <em>β<sub>j</sub></em> are constant, user, and item offsets
      respectively. In this framework the latent vectors
      <em>θ<sub>i</sub></em> and <em>ϕ<sub>j</sub></em> and
      parameters <em>β</em> <sub>0</sub>, <em>β<sub>i</sub></em> ,
      <em>β<sub>j</sub></em> are typically estimated from the data
      using least-squares. This is equivalent to maximizing the
      likelihood of a Gaussian model for the
      <em>y<sub>ij</sub></em> ’s (e.g., [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0022">22</a>]). This is a useful
      approach for data that can be approximated by a symmetric
      distribution but is not ideal for the types of highly skewed
      count data we are focusing on in this paper.
      <p></p>
      <p>More recent work in matrix factorization has built on
      ideas from non-negative matrix factorization to develop
      models that are more appropriate for count data, e.g., where
      the expectation in Equation <a class="eqn" href="#eq6">6</a>
      above represents the mean of a Poisson model for the
      <em>y<sub>ij</sub></em> ’s—known as Poisson matrix
      factorization (PMF). A typical approach is to estimate the
      parameters within a Bayesian framework (such as variational
      inference) and to place priors (such as Gamma priors) on the
      parameters <em>θ<sub>i</sub></em> and <em>ϕ<sub>j</sub></em>
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0006">6</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>].</p>
      <p>Of particular relevance to this paper is the
      recently-introduced dynamic Poisson matrix factorization
      model (DPMF)&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>] which models the expected counts as a
      function of time <em>t</em> as:</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\begin{align*} E [y_{ij}^t ] =
          \theta ^{\prime }_{it} \phi _{jt} +
          \ldots\end{align*}</span><br />
        </div>
      </div>where <em>t</em> is a discrete time index (such as
      days, weeks, etc). Here the latent user and item vectors are
      allowed to evolve dynamically over time, such that
      predictions for time <em>t</em> + 1 are a functions of the
      latent vectors estimated at time <em>t</em>. This DPMF
      approach (and matrix factorization in general) can be viewed
      as an instantiation of a <em>parameter-driven</em>
      latent-space model, in contrast to the
      <em>observation-driven</em> model that we pursue here.
      <p></p>
      <p>Another recent strand of related work (in the non-dynamic
      PMF context) is the use of zero-inflated models in
      probabilistic matrix factorization. Liang et al.
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0016">16</a>]
      proposed the framework of <em>exposure matrix
      factorization</em> (ExpoMF) which uses zero-inflation to
      explicitly account for exposure effects in matrix
      factorization of large binary user-item data sets. Liang et
      al. found that ExpoMF systematically outperformed traditional
      PMF methods that did not account for exposure. In a similar
      vein, Jain et al. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>] developed a probabilistic matrix
      factorization framework with zero-inflation to handle
      exposure effects, for multi-label classification with very
      large numbers of labels, also finding that explicit modeling
      of exposure systematically outperforms methods that do not
      include it. Finally, Liu and Blei&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>] recently proposed a
      zero-inflated exponential family embedding approach for
      sparse binary and count matrices, which from the perspective
      of this paper could be effectively viewed as a “cousin” of
      traditional matrix factorization with its low-dimensional
      embedding representation of the data.</p>
      <p>Our work differs from the matrix factorization and
      embedding approaches described above, in terms of our focus
      on (a) prediction of <em>consumption rates</em> rather than
      ratings or binary data, (b) modeling both repeat and novel
      consumption over time, and (c) the use of user- and
      item-specific regression models rather than low-dimensional
      factorizations or embeddings.</p>
      <p>There has also been recent work on <em>continuous-time
      modeling</em> of time-stamped user-item data, using Markov
      approaches &nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>], Poisson point
      processes&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>], and neural networks&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0010">10</a>]. While
      these papers share a common motivation with our work in terms
      of analyzing explore/exploit aspects of user consumption, the
      focus and methodologies are significantly different to what
      we pursue in this paper, with less emphasis on user-item rate
      prediction and without the use of zero-inflation or
      regression models.</p>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Experiments and
          Results</h2>
        </div>
      </header>
      <p>Below we describe the results of comparing the ZIP model
      to baselines and to a number of well-known approaches from
      the literature for modeling sparse user-item count data. For
      all of the experiments described below the model parameters
      were estimated using data up to time <em>t</em> − 2, with
      hyperparameter tuning via grid search using data at time
      <em>t</em> − 1, and then evaluated on holdout test data from
      time <em>t</em>. This was repeated for <em>t</em> =
      <em>T</em> − 4 to <em>t</em> = <em>T</em> and the prediction
      metrics for the 5 test sets were then averaged.</p>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.1</span> Performance
            Metrics</h3>
          </div>
        </header>
        <p>We evaluated our models using four different
        metrics.</p>
        <p><strong>Log-Loss</strong>: The log-loss is the average
        of the negative log-probability (or negative
        log-likelihood) of each user-item consumption rate in the
        test data:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} -\log P =
            -\frac{1}{N_{\mbox{test}}} \sum _i \sum _j \log
            P(y^t_{ij})\end{align*}</span><br />
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$P(y^t_{ij})$</span></span> is the probability of the
        observed count <span class="inline-equation"><span class=
        "tex">$y^t_{ij}$</span></span> , under the model being
        evaluated, and where <em>N</em> <sub>test</sub> is the
        total number of test points. The log-loss metric is bounded
        below by zero (attainable only by perfect predictions) and
        is widely used in the evaluation of machine learning
        algorithms that produce probabilistic predictions. A model
        that assigns higher probability, or lower negative
        log-probability, to the observed test data is preferred
        over a model that assigns lower probability (or a higher
        negative log-probability).
        <p></p>
        <p><strong>Precision, Recall, and F1</strong>: Let
        <span class="inline-equation"><span class=
        "tex">$\hat{y}^t_{ij}$</span></span> denote the expected
        number of times (according to a particular model) that user
        <em>i</em> will consume item <em>j</em> during time-window
        <em>t</em>. For the ZIP model, by the linearity of
        expectation we have that <span class=
        "inline-equation"><span class="tex">$\hat{y}^t_{ij} =
        \hat{\pi }^t_{ij} \hat{\lambda }^t_{ij}$</span></span>
        where <span class="inline-equation"><span class=
        "tex">$\hat{\pi }^t_{ij}$</span></span> and <span class=
        "inline-equation"><span class="tex">$ \hat{\lambda
        }^t_{ij}$</span></span> are point (MAP) parameter estimates
        learned by the model on the training data.</p>
        <p>For each pair <em>i</em>, <em>j</em> we can compute the
        precision and recall of a prediction <span class=
        "inline-equation"><span class=
        "tex">$\hat{y}^t_{ij}$</span></span> , relative to the
        observed value <span class="inline-equation"><span class=
        "tex">${y}^t_{ij}$</span></span> , as follows. Precision
        can be defined in the context of count data as <span class=
        "inline-equation"><span class="tex">$\frac{\min \lbrace
        y_{ij}^t, \hat{y}_{ij}^t\rbrace
        }{\hat{y}_{ij}^t}$</span></span> , i.e., it is the fraction
        of user-item consumptions that the model predicted would
        occur that actually did occur. Similarly, recall can be
        defined as <span class="inline-equation"><span class=
        "tex">$\frac{\min \lbrace y_{ij}^t, \hat{y}_{ij}^t\rbrace
        }{y_{ij}^t}$</span></span> , which is the fraction of
        observed user-item consumptions that did occur that the
        model predicted would occur. These pairwise user-item
        precision and recall values can be averaged over all
        user-item pairs to obtain overall precision and recall
        numbers. Models that systematically underestimate
        <span class="inline-equation"><span class=
        "tex">${y}^t_{ij}$</span></span> (e.g., that predict all
        zeros) will have high precision but low recall, and
        vice-versa for models that systematically overestimate
        <span class="inline-equation"><span class=
        "tex">${y}^t_{ij}$</span></span> . For our experimental
        results below we report the <em>F1</em> score, which
        combines both precision and recall, in the standard fashion
        as:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} F1 = 2 \times
            \frac{\mbox{Prec} \times \mbox{Rec}}{\mbox{Prec} +
            \mbox{Rec}}\end{align*}</span><br />
          </div>
        </div><strong>MAE</strong>: Mean absolute error between the
        expected number of times that a user <em>i</em> will
        consume item <em>j</em> during time-window <em>t</em> and
        the observed value <span class=
        "inline-equation"><span class=
        "tex">$y_{ij}^T$</span></span> : MAE <span class=
        "inline-equation"><span class="tex">$=
        \frac{1}{N_{\mbox{test}}} \sum _i \sum _j \left| y_{ij}^t -
        \hat{y}_{ij}^t\right|$</span></span>
        <p></p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.2</span> Poisson
            Regression with and without Zero-Inflation</h3>
          </div>
        </header>
        <p>We first compare the ZIP Poisson regression model
        (<strong>ZIP</strong>) to a Poisson regression model
        (<strong>PR</strong>) without a zero-inflation component.
        We use the same features for both models.
        Table&nbsp;<a class="tbl" href="#tab3">3</a> shows that the
        ZIP model systematically outperforms the PR model on
        holdout data, for all three metrics across all three data
        sets.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Log-Loss, F1 measure and MAE on the test
            data for the PR and ZIP models across different data
            sets. Lower values are better for Log-Loss and MAE and
            higher values are better for F1. Best performing
            methods indicated in bold font.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <em>Log-Loss</em>
                  <hr />
                </th>
                <th colspan="2" style="text-align:center;">
                  <em>F1</em>
                  <hr />
                </th>
                <th colspan="2" style="text-align:center;">
                  <em>MAE</em>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">PR</th>
                <th style="text-align:center;">ZIP</th>
                <th style="text-align:center;">PR</th>
                <th style="text-align:center;">ZIP</th>
                <th style="text-align:center;">PR</th>
                <th>ZIP</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">reddit</td>
                <td style="text-align:center;">0.30</td>
                <td style="text-align:center;">
                <strong>0.14</strong></td>
                <td style="text-align:center;">0.70</td>
                <td style="text-align:center;">
                <strong>0.82</strong></td>
                <td style="text-align:center;">0.40</td>
                <td><strong>0.23</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">lastfm</td>
                <td style="text-align:center;">0.20</td>
                <td style="text-align:center;">
                <strong>0.08</strong></td>
                <td style="text-align:center;">0.08</td>
                <td style="text-align:center;">
                <strong>0.25</strong></td>
                <td style="text-align:center;">0.08</td>
                <td><strong>0.04</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">Yelp</td>
                <td style="text-align:center;">0.09</td>
                <td style="text-align:center;">
                <strong>0.07</strong></td>
                <td style="text-align:center;">0.09</td>
                <td style="text-align:center;">
                <strong>0.15</strong></td>
                <td style="text-align:center;">0.07</td>
                <td><strong>0.04</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>To further analyze the contribution of the zero-inflated
        component we evaluated each of the models in terms of their
        ability to predict the zeros. We focused on user-item pairs
        with <span class="inline-equation"><span class=
        "tex">$y_{ij}^t=0$</span></span> and computed the Log-Loss
        for those pairs under each model. The Log-Loss values for
        the PR model are 0.064, 0.035 and 0.042 in the reddit,
        lastfm and Yelp data sets respectively. The corresponding
        values for the ZIP model are an order of magnitude lower:
        0.004, 0.004 and 0.017. This significant improvement is
        directly attributable to the presence of the zero-inflation
        component in the ZIP model.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.3</span> Comparing
            ZIP to Baselines and Matrix Factorization</h3>
          </div>
        </header>
        <p>Below we describe results obtained from comparing the
        ZIP model to a set of simple baselines and to several
        well-known approaches in the literature based on matrix
        factorization and embeddings for count data.</p>
        <p><strong>GR (Global Rate)</strong>: This is defined as
        the global rate at which each item is consumed in the
        training data, computed by averaging across all users and
        time-stamps:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \hat{\lambda
            }^{GR}_{ij} = \hat{\lambda }_j = \frac{1}{N}
            \frac{1}{t} \sum _{i=1}^N \sum _{\tau = 1}^t
            y_{ij}^\tau \ \ \ \ \ 1 \le j \le
            M\end{align*}</span><br />
          </div>
        </div><strong>MPE (Mean Posterior Estimate)</strong>: This
        is the mean posterior estimate (MPE) of each user-item
        rate, with a conjugate Bayesian <em>Gamma</em>(<em>γ</em>
        <sub>0</sub>, <em>γ</em> <sub>1</sub>) prior, based on the
        counts in the training data:
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \hat{\lambda
            }^{MPE}_{ij} = \frac{\sum _ {\tau = 1}^t y_{ij}^\tau +
            \gamma _0}{t + \gamma _1}\end{align*}</span><br />
          </div>
        </div>where <em>γ</em> <sub>0</sub> and <em>γ</em>
        <sub>1</sub> are determined via grid search to optimize the
        log-loss of the validation data for the MPE model.
        <p></p>
        <p><strong>PMF (Poisson Matrix Factorization)</strong>:
        This is a latent factor matrix factorization model with a
        Poisson distribution for the observed counts. In our
        results we used a state-of-the-art Bayesian PMF version
        implementation by Liang et al.&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0017">17</a>]. We fit the model to
        the aggregated data across all time windows. At prediction
        time the predicted rates from the model were divided by the
        number of time windows in the training data set to scale
        the rate for prediction for a single time window.</p>
        <p><strong>DPMF (Dynamic Poisson Matrix
        Factorization)</strong>: This is an extension of the PMF
        model that learns latent-space decomposition from a
        sequence of user-item consumption counts&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0003">3</a>]. The
        latent-space vectors for users and items are estimated for
        each time-window jointly by modeling the change in
        <span class="inline-equation"><span class="tex">$\theta
        _i^t$</span></span> and <span class=
        "inline-equation"><span class="tex">$\phi
        _j^t$</span></span> between different time steps <em>t</em>
        using a Kalman filter.</p>
        <p><strong>ZIE (Zero-Inflated Exponential Family
        Embeddings)</strong>: This is an exponential family
        embedding algorithm that uses a zero-inflation component to
        model exposure and effectively downweight zero counts when
        learning item embeddings from sparse binary or count
        data&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>]. We fit the model to the
        aggregated data across all time windows using Poisson
        distribution and scaled to a single time window at
        prediction time. The exposure covariates in this model are
        individual-specific and represent external information
        (such as demographic variables). In the absence of
        additional meta-data about the individuals for the data
        sets used in this paper, we used a single intercept for
        each item.</p>
        <p>Hyperparameters for PMF, DPMF, and ZIE were determined
        via grid search on the validation data. One hyperparameter
        of particular interest is the number of factors or
        dimensions used by these models. We found that the matrix
        factorization techniques, PMF and DPMF, had the best
        predictive performance when using extremely high numbers of
        factors, to the point of almost memorizing the data. Rather
        than using very high dimensional representations, in
        keeping with typical matrix factorization experiments in
        the literature, we limited the number of factors for the
        models to a moderate range of 200 to 500 dimensions. For
        the ZIE method, grid search on the validation resulted in
        50 to 100 embedding dimensions being approximately optimal
        for prediction across the different data sets, with little
        to no improvement above 100.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Log-Loss on the test data for different
            algorithms across different data sets. Lower scores are
            better. Best-performing methods indicated in bold
            font.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">GR</th>
                <th style="text-align:center;">MPE</th>
                <th style="text-align:center;">PMF</th>
                <th style="text-align:center;">DPMF</th>
                <th style="text-align:center;">ZIE</th>
                <th style="text-align:center;">ZIP</th>
              </tr>
            </thead>
            <tbody>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">
                <strong>reddit</strong></td>
                <td style="text-align:center;">2.984</td>
                <td style="text-align:center;">0.248</td>
                <td style="text-align:center;">0.362</td>
                <td style="text-align:center;">0.845</td>
                <td style="text-align:center;">4.214</td>
                <td><strong>0.136</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>lastfm</strong></td>
                <td style="text-align:center;">0.213</td>
                <td style="text-align:center;">0.132</td>
                <td style="text-align:center;">0.145</td>
                <td style="text-align:center;">0.154</td>
                <td style="text-align:center;">0.171</td>
                <td><strong>0.075</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>Yelp</strong></td>
                <td style="text-align:center;">0.078</td>
                <td style="text-align:center;">0.084</td>
                <td style="text-align:center;">0.076</td>
                <td style="text-align:center;">0.076</td>
                <td style="text-align:center;">0.074</td>
                <td><strong>0.069</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">F1-scores on the test data for different
            algorithms across different data sets. Higher scores
            are better. Best-performing methods indicated in bold
            font.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">GR</th>
                <th style="text-align:center;">MPE</th>
                <th style="text-align:center;">PMF</th>
                <th style="text-align:center;">DPMF</th>
                <th style="text-align:center;">ZIE</th>
                <th style="text-align:center;">ZIP</th>
              </tr>
            </thead>
            <tbody>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">
                <strong>reddit</strong></td>
                <td style="text-align:center;">0.07</td>
                <td style="text-align:center;">0.62</td>
                <td style="text-align:center;">0.63</td>
                <td style="text-align:center;">0.57</td>
                <td style="text-align:center;">0.01</td>
                <td><strong>0.82</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>lastfm</strong></td>
                <td style="text-align:center;">0.04</td>
                <td style="text-align:center;">0.21</td>
                <td style="text-align:center;">0.12</td>
                <td style="text-align:center;">0.18</td>
                <td style="text-align:center;">0.02</td>
                <td><strong>0.25</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>Yelp</strong></td>
                <td style="text-align:center;">0.10</td>
                <td style="text-align:center;">0.11</td>
                <td style="text-align:center;">0.13</td>
                <td style="text-align:center;">0.10</td>
                <td style="text-align:center;">0.11</td>
                <td><strong>0.15</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class=
            "table-title">MAE on the test data for different
            algorithms across different data sets. Lower scores are
            better. Best-performing methods indicated in bold
            font.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">GR</th>
                <th style="text-align:center;">MPE</th>
                <th style="text-align:center;">PMF</th>
                <th style="text-align:center;">DPMF</th>
                <th style="text-align:center;">ZIE</th>
                <th style="text-align:center;">ZIP</th>
              </tr>
            </thead>
            <tbody>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">
                <strong>reddit</strong></td>
                <td style="text-align:center;">0.996</td>
                <td style="text-align:center;">0.401</td>
                <td style="text-align:center;">0.342</td>
                <td style="text-align:center;">0.473</td>
                <td style="text-align:center;">0.661</td>
                <td><strong>0.228</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>lastfm</strong></td>
                <td style="text-align:center;">0.057</td>
                <td style="text-align:center;">0.049</td>
                <td style="text-align:center;">0.051</td>
                <td style="text-align:center;">0.043</td>
                <td style="text-align:center;">0.136</td>
                <td><strong>0.038</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>Yelp</strong></td>
                <td style="text-align:center;">0.042</td>
                <td style="text-align:center;">
                <strong>0.035</strong></td>
                <td style="text-align:center;">0.040</td>
                <td style="text-align:center;">0.049</td>
                <td style="text-align:center;">0.041</td>
                <td><strong>0.035</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Tables&nbsp;<a class="tbl" href=
        "#tab4">4</a>,&nbsp;<a class="tbl" href="#tab5">5</a>
        and&nbsp;<a class="tbl" href="#tab6">6</a> show the
        <em>Log-Loss</em>, <em>F1</em>, and <em>MAE</em> scores on
        the test data, for each of the baselines and PMF, DPMF and
        ZIE models, compared to the ZIP model. The ZIP model is
        significantly more accurate than the other methods for all
        metrics for all data sets, except for the MAE score for the
        MPE model on the Yelp data set.</p>
        <p>For both the reddit and lastfm data sets the margins of
        improvement of the ZIP model over the PMF, DPMF and ZIE
        models are quite large. There are two likely reasons for
        this improvement. The first is that the zero-inflation
        component in the ZIP model provides a more flexible way to
        handle excess zeros than PMF or DPMF. The second reason is
        that the the user-specific features in the regression
        approach (such as the history variable of what specific
        items a user consumed in the past) allows the regression
        model to more accurately model individual-level details
        than the matrix factorization (MF) or embedding approaches.
        These approaches are constrained by the dimensionality of
        their latent spaces, limiting the level of detail (e.g.,
        specific combinations of items) available for modeling
        individual users. We explore both of these in more detail
        below.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186153/images/www2018-162-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">The ratio of probability for
            the number of consumptions for selected users from
            reddit (top) and lastfm (bottom) at time <em>T</em>
            assigned by the evaluated models compared to the ground
            truth (GT). The number of consumptions predicted by the
            ZIE model were all lower than 1. As a result the ratios
            of probabilities for the ZIE model are omitted from the
            plot for clarity. Best viewed in color.</span>
          </div>
        </figure>
        <p></p>
        <p><strong>Modeling excess zeros</strong>: Modeling the
        zeros provides the ZIP model with a principled way of
        down-weighting the zeros in the process of learning the
        rate parameter. As a result the rate-process coefficients
        are free to fit a larger range of <span class=
        "inline-equation"><span class=
        "tex">$y_{ij}^t$</span></span> values (in particular, high
        numbers of consumptions). In Figure&nbsp;<a class="fig"
        href="#fig3">3</a> we plot the ratio between (a) the
        predicted number of <span class=
        "inline-equation"><span class=
        "tex">$\hat{y}_{ij}^t$</span></span> counts, across
        different ranges of <em>y</em>, for the different models,
        and (b) the ground truth number for those values in the
        test data. These plots are for users with high variance in
        their <span class="inline-equation"><span class=
        "tex">$y_{ij}^t$</span></span> values, for both the reddit
        (top) and lastfm (bottom) data sets. We can see that in
        order to fit the excess of zeros, the baselines tend to
        systematically and significantly overestimate the low rates
        and to underestimate the high rates, relative to ground
        truth. In contrast, the proposed ZIP regression model
        (green squares) tends to be much more accurate (i.e., much
        closer to 1 than the other methods across both data
        sets).</p>
        <p><strong>Balancing Explore-Exploit</strong>: By
        separately modeling the <em>exposure</em> and <em>rate</em>
        processes, the ZIP model is able to capture the
        heterogeneity across users in terms of their
        <em>explore/exploit</em> behavior. In particular, the
        exposure process coefficient <em>η</em>
        <sub><em>i</em>0</sub> corresponds to the estimated
        user-specific budget and captures the number of unique
        items a user will consume. As <em>η</em>
        <sub><em>i</em>0</sub> increases, the probability that a
        user will be exposed to an item and consume it is also
        predicted to increase, corresponding to a higher predicted
        tendency for <em>exploration</em>. To illustrate this, in
        Figure&nbsp;<a class="fig" href="#fig4">4</a> we plot the
        correlation between the value of <em>η</em>
        <sub><em>i</em>0</sub> and the number of unique items the
        user consumed in the lastfm (left) and reddit (right) data
        sets. The clear positive correlation between the two (shown
        as the regression line in red) demonstrates the ability of
        our model to achieve an appropriate balance between
        <em>exploration</em> and <em>exploitation</em>, resulting
        in better individual-level predictive models. In addition,
        the individual-level coefficients provide interpretable
        detail about each specific user, quantifying their
        individual tendency for exploration within the context of a
        broad rate-prediction model.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186153/images/www2018-162-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Number of unique items each
            user consumed as a function of the user-specific budget
            coefficient (<em>η</em> <sub><em>i</em>0</sub>) in the
            <em>lastfm</em> (left) and <em>reddit</em> (right) data
            sets. The red line indicates the exponential curve
            fitted to the scatter plot.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span>
          Scalability</h2>
        </div>
      </header>
      <figure id="fig5">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186153/images/www2018-162-fig5.jpg"
        class="img-responsive" alt="Figure 5" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 5:</span> <span class=
          "figure-title">Optimization cost value (negative
          log-Likelihood) at each SGD iteration for each dataset.
          Markers denote the point in the iterative process where
          E-steps were performed. Best viewed in color.</span>
        </div>
      </figure>
      <p>In fitting our regression models our dense data matrix can
      be thought of as having <em>N</em> × <em>M</em> × <em>T</em>
      rows and <em>d</em> columns (for the features), where
      <em>d</em> is the number of coefficients in the regression,
      <em>N</em>, <em>M</em> and <em>T</em> are the number of
      users, items and time-windows respectively. Thus, direct
      gradient optimization would be <span class=
      "inline-equation"><span class="tex">$\mathcal {O}(d N M
      T)$</span></span> per gradient computation. Using SGD, the
      time complexity of a single gradient step is <span class=
      "inline-equation"><span class="tex">$\mathcal {O}(d \times
      R)$</span></span> where <em>R</em> is the minibatch size
      (i.e., the number of data points selected for computing each
      stochastic estimate of the full gradient).</p>
      <p>If we think of <em>NMT</em> as the effective total number
      of rows in the full data set, then to gain the benefits of
      SGD we need to select <em>R</em> such that <em>R</em> &lt;
      &lt; <em>NMT</em>. In typical applications of SGD with dense
      data the minibatch size can be quite small, e.g., <em>R</em>
      = 10, <em>R</em> = 100. However, with highly skewed data (as
      in the user-item data sets of interest here), the minibatch
      sizes need to be significantly larger to ensure that there
      are enough non-zeros in each minibatch. We found that a
      minibatch size of <em>R</em> = 50, 000 worked well in terms
      of relatively fast and reliable convergence. The time
      complexity of a single E-step is <span class=
      "inline-equation"><span class="tex">$\mathcal {O}(d \times N
      \times M \times T)$</span></span> , i.e., proportional to the
      number of rows, making it the most expensive part in the
      algorithm in terms of time complexity. It is possible that
      some efficiency could be gained here via an approximate
      E-step but we did not investigate this here given that we
      execute far more gradient steps (within the M-step) than
      E-steps.</p>
      <p>Figure&nbsp;<a class="fig" href="#fig5">5</a> shows
      convergence plots, where the y-axis is the cost function
      (Log-Loss) at each iteration. Each iteration on the
      <em>x</em>-axis marks a single stochastic gradient
      (minibatch) step and the markers indicate the point in the
      algorithm where E-steps occurred (each M-step consists of
      multiple gradient steps). We see from the convergence plots
      that the algorithm converges quickly for each of the three
      data sets in our experiments. We implemented our algorithm in
      Python (with Cython to speed-up)<a class="fn" href="#fn4" id=
      "foot-fn4"><sup>3</sup></a>. Each mini-batch iteration in our
      implementation ran in a matter of few milliseconds and the
      relatively expensive E-step took 7, 62, and 1 seconds on
      average for the reddit, lastfm, and Yelp data sets
      respectively. Our implementation used a single core—it is
      relatively straightforward to distribute the computation of
      the gradients and membership weights by using multiple cores,
      rendering the algorithm scalable to much larger data sets
      than what we used here.</p>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">9</span>
          Conclusions</h2>
        </div>
      </header>
      <p>We proposed and investigated a framework using
      zero-inflated Poisson regression for prediction of
      consumption rates in high-dimensional user-item data sets.
      The approach is motivated by applications where user
      consumption is a mix of repeat (exploitation) and novel
      (exploration) behavior over time. The regression component of
      the model allows for detailed modeling of individual users
      based on their histories and provides an alternative to more
      widely-used latent variable models such as matrix
      factorization. Experimental results indicate that the
      proposed approach can systematically outperform existing
      alternatives such as PMF, DPMF and ZIE for the problem of
      predicting the rates at which specific users consume specific
      items. There are a number of natural directions for further
      explorations of models of this type, including for example
      modeling of the dynamic changes between time windows within
      the regression framework, potentially further enhancing the
      predictive capabilities of the proposed ZIP regression
      model.</p>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">10</span>
          Acknowledgements</h2>
        </div>
      </header>
      <p>The work in this paper was supported in part by the
      National Science Foundation award IIS-1320527 and by a Google
      Faculty Award.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Austin&nbsp;R Benson,
        Ravi Kumar, and Andrew Tomkins. 2016. Modeling user
        consumption sequences. In <em><em>Proceedings of the 25th
        International Conference on World Wide Web</em></em> . ACM
        Press, 519–529.</li>
        <li id="BibPLXBIB0002" label="[2]">A&nbsp;Colin Cameron and
        Pravin&nbsp;K Trivedi. 2013. <em><em>Regression Analysis of
        Count Data</em></em> . Cambridge University Press.</li>
        <li id="BibPLXBIB0003" label="[3]">Laurent Charlin, Rajesh
        Ranganath, James McInerney, and David&nbsp;M Blei. 2015.
        Dynamic Poisson factorization. In <em><em>Proceedings of
        the 9th ACM Conference on Recommender Systems</em></em> .
        ACM Press, 155–162.</li>
        <li id="BibPLXBIB0004" label="[4]">Nathaniel&nbsp;D Daw,
        John&nbsp;P O'Doherty, Peter Dayan, Ben Seymour, and
        Raymond&nbsp;J Dolan. 2006. Cortical substrates for
        exploratory decisions in humans. <em><em>Nature</em></em>
        441(2006), 876–879.</li>
        <li id="BibPLXBIB0005" label="[5]">Peter Diggle, Patrick
        Heagerty, Kung-Yee Liang, and Scott Zeger. 2002.
        <em><em>Analysis of Longitudinal Data</em></em> . Oxford
        University Press.</li>
        <li id="BibPLXBIB0006" label="[6]">Prem Gopalan,
        Jake&nbsp;M Hofman, and David&nbsp;M Blei. 2015. Scalable
        recommendation with hierarchical Poisson factorization. In
        <em><em>Proceedings of the 31st Conference on Uncertainty
        in Artificial Intelligence</em></em> . 326–335.</li>
        <li id="BibPLXBIB0007" label="[7]">Daniel&nbsp;B Hall.
        2000. Zero-inflated Poisson and binomial regression with
        random effects: a case study. <em><em>Biometrics</em></em>
        56, 4 (2000), 1030–1039.</li>
        <li id="BibPLXBIB0008" label="[8]">Seyed&nbsp;Abbas
        Hosseini, Keivan Alizadeh, Ali Khodadadi, Ali Arabzadeh,
        Mehrdad Farajtabar, Hongyuan Zha, and Hamid&nbsp;R Rabiee.
        2017. Recurrent Poisson factorization for temporal
        recommendation. In <em><em>Proceedings of the 23rd ACM
        SIGKDD International Conference on Knowledge Discovery and
        Data Mining</em></em> . ACM Press, 847–855.</li>
        <li id="BibPLXBIB0009" label="[9]">Vikas Jain, Nirbhay
        Modhe, and Piyush Rai. 2017. Scalable generative models for
        multi-label learning with missing labels. In
        <em><em>Proceedings of the 34th International Conference on
        Machine Learning</em></em> . PMLR, 1636–1644.</li>
        <li id="BibPLXBIB0010" label="[10]">How Jing and
        Alexander&nbsp;J. Smola. 2017. Neural survival recommender.
        In <em><em>Proceedings of the Tenth ACM International
        Conference on Web Search and Data Mining</em></em> . ACM
        Press, 515–524.</li>
        <li id="BibPLXBIB0011" label="[11]">Komal Kapoor, Vikas
        Kumar, Loren Terveen, Joseph&nbsp;A. Konstan, and Paul
        Schrater. 2015. “I like to explore sometime”: adapting to
        dynamic user novelty preferences. In <em><em>Proceedings of
        the 9th ACM Conference on Recommender Systems</em></em> .
        ACM Press, 19–26.</li>
        <li id="BibPLXBIB0012" label="[12]">Komal Kapoor, Karthik
        Subbian, Jaideep Srivastava, and Paul Schrater. 2015. Just
        in time recommendations: modeling the dynamics of boredom
        in activity streams. In <em><em>Proceedings of the Eighth
        ACM International Conference on Web Search and Data
        Mining</em></em> . ACM Press, 233–242.</li>
        <li id="BibPLXBIB0013" label="[13]">Diederik Kingma and
        Jimmy Ba. 2015. Adam: A method for stochastic optimization.
        In <em><em>International Conference on Learning
        Representations (ICLR)</em></em> .</li>
        <li id="BibPLXBIB0014" label="[14]">Y. Koren, R. Bell, and
        C. Volinsky. 2009. Matrix factorization techniques for
        recommender systems. <em><em>Computer</em></em> 42, 8
        (2009), 30–37.</li>
        <li id="BibPLXBIB0015" label="[15]">Diane Lambert. 1992.
        Zero-inflated Poisson regression, with an application to
        defects in manufacturing. <em><em>Technometrics</em></em>
        34, 1 (1992), 1–14.</li>
        <li id="BibPLXBIB0016" label="[16]">Dawen Liang, Laurent
        Charlin, James McInerney, and David&nbsp;M. Blei. 2016.
        Modeling user exposure in recommendation. In
        <em><em>Proceedings of the 25th International Conference on
        World Wide Web</em></em> . ACM Press, 951–961.</li>
        <li id="BibPLXBIB0017" label="[17]">Dawen Liang,
        John&nbsp;William Paisley, and Dan Ellis. 2014.
        Codebook-based scalable music tagging with Poisson matrix
        factorization. In <em><em>Proceedings of the Fifteenth
        International Society for Music Information Retrieval
        Conference</em></em> . 167–172.</li>
        <li id="BibPLXBIB0018" label="[18]">Li-Ping Liu and
        David&nbsp;M Blei. 2017. Zero-inflated exponential family
        embeddings. In <em><em>International Conference on Machine
        Learning</em></em> . PMLR, 2140–2148.</li>
        <li id="BibPLXBIB0019" label="[19]">Reza Rejaie, Haobo Yu,
        Mark Handley, and Deborah Estrin. 2000. Multimedia proxy
        caching mechanism for quality adaptive streaming
        applications in the internet. In <em><em>Proceedings of the
        Nineteenth Annual Joint Conference of the IEEE Computer and
        Communications Societies (INFOCOM 2010)</em></em> ,
        Vol.&nbsp;2. IEEE Press, 980–989.</li>
        <li id="BibPLXBIB0020" label="[20]">Martin Ridout,
        Clarice&nbsp;GB Demétrio, and John Hinde. 1998. Models for
        count data with many zeros. In <em><em>Proceedings of the
        XIXth International Biometric Conference</em></em> .
        179–192.</li>
        <li id="BibPLXBIB0021" label="[21]">Matthias&nbsp;W Seeger,
        David Salinas, and Valentin Flunkert. 2016. Bayesian
        intermittent demand forecasting for large inventories. In
        <em><em>Advances in Neural Information Processing Systems
        29</em></em> . Curran Associates, Inc., 4646–4654.</li>
        <li id="BibPLXBIB0022" label="[22]">Ajit&nbsp;P Singh and
        Geoffrey&nbsp;J Gordon. 2008. A unified view of matrix
        factorization models. In <em><em>Proceedings of the
        European Conference on Machine Learning and Knowledge
        Discovery in Databases</em></em> . Springer Berlin
        Heidelberg, 358–373.</li>
        <li id="BibPLXBIB0023" label="[23]">Yingzi Wang,
        Nicholas&nbsp;Jing Yuan, Defu Lian, Linli Xu, Xing Xie,
        Enhong Chen, and Yong Rui. 2015. Regularity and Conformity:
        Location Prediction Using Heterogeneous Mobility Data. In
        <em><em>Proceedings of the 21th ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining</em></em>
        . ACM, 1275–1284.</li>
        <li id="BibPLXBIB0024" label="[24]">Robert&nbsp;C Wilson,
        Andra Geana, John&nbsp;M White, Elliot&nbsp;A Ludvig, and
        Jonathan&nbsp;D Cohen. 2014. Humans use directed and random
        exploration to solve the explore–exploit dilemma.
        <em><em>Journal of Experimental Psychology:
        General</em></em> 143, 6(2014), 2074–2081.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Current
    affiliation: Google Inc.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html">http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "http://www.yelp.com/dataset_challenge">http://www.yelp.com/dataset_challenge</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/UCIDataLab/ZIP-Regression">https://github.com/UCIDataLab/ZIP-Regression</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186153">https://doi.org/10.1145/3178876.3186153</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
