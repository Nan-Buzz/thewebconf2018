<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186347'>https://doi.org/10.1145/3184558.3186347</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186347'>https://w3id.org/oa/10.1145/3184558.3186347</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Hogun</span> <span class="surName">Park</span><a class="fn" href="#fn1" id="foot-fn1"><sup>⁎</sup></a>, Purdue University, West Lafayette, IN, USA, <a href="mailto:hogun@purdue.edu">hogun@purdue.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Hamid Reza Motahari</span> <span class="surName">Nezhad</span>, IBM Almaden Research Center, San Jose, CA, USA, <a href="mailto:hamid.motahari@ieee.org">hamid.motahari@ieee.org</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186347" target="_blank">https://doi.org/10.1145/3184558.3186347</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>A lot of knowledge about procedures and how-tos are described in text. Recently, extracting semantic relations from the procedural text has been actively explored. Prior work mostly has focused on finding relationships among verb-noun pairs or clustering of extracted pairs. In this paper, we investigate the problem of learning individual procedure-specific relationships (e.g. <em>is<span style="text-decoration: underline;">&nbsp;</span>method<span style="text-decoration: underline;">&nbsp;</span>of, is<span style="text-decoration: underline;">&nbsp;</span>alternative<span style="text-decoration: underline;">&nbsp;</span>of</em>, or <em>is<span style="text-decoration: underline;">&nbsp;</span>subtask<span style="text-decoration: underline;">&nbsp;</span>of</em>) among sentences. To identify the relationships, we propose an end-to-end neural network architecture, which can selectively learn important procedure-specific relationships. Using this approach, we could construct a how-to knowledge base from the largest procedure sharing-community, <em>wikihow.com</em>. The evaluation of our approach shows that it outperforms the existing entity relationship extraction algorithms.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Computing methodologies</strong> → <strong>Artificial intelligence;</strong> <strong>Neural networks;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Procedure Learning</small>,</span> <span class="keyword"><small>How-to-Knowledge</small>,</span> <span class="keyword"><small>Neural Network</small>,</span> <span class="keyword"><small>Relationship Learning</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Hogun Park and Hamid Reza Motahari Nezhad. 2018. Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3186347" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186347</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Procedural knowledge has been an important component for many semantic applications such as question answering&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>], information retrieval&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>], and conversational agents&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. A lot of knowledge about procedures are described in text. A procedure is comprised of a nested set of inter-related tasks to achieve a goal. For a given procedure, there may be a few alternative methods of achieving the goal, each comprised of a set of tasks. For example, the goal ”how to cook Kimchi” is composed of tasks such as ”preparing ingredients” and ”making the sauce.” The task has a set of sub-tasks like ”wash your hands” and ”dissolve salt,” and they are sequentially related to each other. In addition, a task or subtask could include conditional statements and be under specific temporal/spatial constraints. For the given sentence, ”If the cabbage is not preserved, keep it in the brine at least 12 hours,” we can represent it as &lt;”keep it in the brine at least 12 hours”&gt;-<em>conditional<span style="text-decoration: underline;">&nbsp;</span>of</em>-&lt;”the cabbage is not preserved”&gt;. Figure &nbsp;<a class="fig" href="#fig1">1</a> shows an example of procedural text (left) and the corresponding procedure graph (right). The graph has many procedure-specific relationships (e.g. <em>is<span style="text-decoration: underline;">&nbsp;</span>method<span style="text-decoration: underline;">&nbsp;</span>of, is<span style="text-decoration: underline;">&nbsp;</span>alternative<span style="text-decoration: underline;">&nbsp;</span>of</em>, or <em>is<span style="text-decoration: underline;">&nbsp;</span>subtask<span style="text-decoration: underline;">&nbsp;</span>of</em>.) In this paper, we focus on such procedure-specific relationship extraction for learning a procedure knowledge base.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186347/images/www18companion-109-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Example of procedural text and Extracted Procedure Graph from the Text.</span>
        </div>
      </figure>
      <p></p>
      <p>Learning procedures described in text is a challenging problem. This problem is investigated in prior work through approaches such as extracting or learning patterns&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>], learning probabilistic models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], or hierarchical clustering&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]. Relationship extraction in these works has mostly been in the form of identifying verb/noun pairs in a sentence and their relationship. For example, a relation, <em>hasNextAction</em>, could be found among verb/noun pairs like (pump, brake pedal) and (take, spare tire). Thus, the extracted relationships are defined at term-level within a sentence, which are found based on syntactic patterns and rules. In this paper, we investigate ways to identify action-level relationships within and across sentences in text describing procedures. In addition, our proposed method allows finding richer procedural representation by allowing action arguments to model conditions or contexts associated with actions in the text. In addition, our method could prohibit the error propagation from preprocessing steps such as POS Tagging and Dependency Parsing, which are used in the above work.</p>
      <p>In this paper, we propose an end-to-end neural relationship extraction model for procedural text, which is the core element of constructing a procedure knowledge base. Recently, neural network models have shown their effectiveness in many different kinds of relationship classification tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]. Most models are based on Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN) for extracting either word- or sentence-level relationships. Different from the previously proposed neural network architectures, this paper proposes a hierarchical attention mechanism, which can select out the most important part at the action-level, and at cross-sentence level.</p>
      <p>We also propose an alternative architecture based on MemoryNet-augmented neural network to enable task relationships learning utilizing high-level contextual information. Previously, MemoryNet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] has been widely used in question-answering and reading comprehension. In this paper, we employ the concept of MemoryNet's associated memory to learn the underlying structure of procedures and propose a new method to model and classify relationships. For this, our hierarchical attention encoder is incorporated into the MemoryNet in order to encode the input sentences and contextual information (e.g goal or previous task).</p>
      <p>As the dataset to evaluate our proposed architecture, we obtained procedure description data from <em>wikihow.com</em> for procedures with relatively long and complete tasks/actions. The proposed methods and architectures are compared to a select set of baselines and prior art alternatives, which demonstrate superior performance of the proposed methods on identifying and classifying procedure-specific relationships.</p>
      <p>The rest of paper is structured as follows. Related work is discussed in Section 2. The research problem is defined in Section 3, and the proposed architectures for procedural relationship classification is presented in Section 4. We explain experimental results in Section 5. Finally, we conclude and discuss future work in Section 6.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Relation Classification</h3>
          </div>
        </header>
        <p>For entity-relationship extraction and classification, many feature/kernel-based methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>] have been proposed. Recently, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>] showed that the shortest dependency paths between relation arguments are still useful in neural network architectures like convolutional neural network (CNN) and long short-term memory (LSTM). In particular, it was shown that LSTM-based approach is worse than CNN-based model in their framework [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>]. Later, many existing models have been tried to extract entity relationship in a sentence, and most of them rely on complex natural language processing pipelines. To reduce the burden, there were some attempts at extracting relations between entities (e.g., [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]) by utilizing neural networks. However, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] still uses part-of-speech tags, and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] do not take full sentences as their input.</p>
        <p>Recently, more variants of CNN and RNN have been proposed for relation classification. For example, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] uses CNN to get the vector representations of sentences by splitting them for in-sentence entity relationship detection. In the work, relationships are learned by three different CNNs,and their output vectors are concatenated before the softmax layer to classify relationships. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>] additionally uses positional vectors to represent the sentence through the convolutional neural network. The positional vectors are attached to the existing word embedding layers and used to indicate target entities for classification. For our objective, it can not be used directly, but, instead, we can still model each sentence using the convolutional neural network without using splitting or positional vector. In this paper, we implemented these architectures for comparison with our proposed methods. Similarly, the architecture proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>], although much simpler in principle, was implemented and used for comparison, as discussed in Section 5.</p>
        <p>Discourse relationship classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] is also relevant work to our paper. The task of implicit discourse relationship classification is to recognize how two adjacent sentences are associated without explicit discourse markers (e.g. ”because”). The paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] proposed a recurrent neural model for identifying implicit discourse relations in Chinese text. Their model is based on the single-level bi-directional LSTM model with attention. Our hierarchical attention model is a more generalized neural network by allowing multiple bi-directional LSTM layers. We have compared our models and theirs in our experiments for procedural text. Paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] utilizes document graph&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] and learns a Graph LSTM to classify the relationships. However, it is required to have the document graph for the input and use distant supervision from the existing knowledge base, which is not appropriate for our purpose and quite heavy for learning both neural networks. Paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] also proposed interesting attention-based models for the implicit discourse relationship, but they were also needed to learn a neural model using the external knowledge base to overcome the lack of training data.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Procedure Knowledge Mining</h3>
          </div>
        </header>
        <p>Constructing procedural knowledge base has been investigated in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>] extracts procedural knowledge from <em>eHow.com</em> and <em>wikiHow.com</em>. They proposed a framework to detect verbs/actions and objects, and their relationships to build a situation ontology. However, it is required to go through many pre-processing steps like normalization and pattern-matching. Their relationship extraction is also limited to simpler types of relationships such as ”hasTopic” and ”hasAction”. Paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] uses Open-IE 4.2 software, which is the successor to ReVerb[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] and Ollie[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>] to extract noisy candidates and accomplish hierarchical clustering to find similar tasks. However, they also rely on preprocessing for extracting subject-predicate-object tuples and time/location information. Therefore, its constructed knowledge base is still prone to inaccuracies and rigidity of the information extraction pipeline.</p>
        <p>Workflow detection is also related to the procedure knowledge mining. Paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>] attempted to detect tasks, products, task facets, and control flows based on information extraction pipeline and rules. They also suggested both term- and frame-based approaches that deal with workflow elements. The term-based approach uses Java Pattern Annotation Engine to utilize hand-crafted and domain-specific rules. The frame-based approach is based on predefined patterns to detect a new workflow. Paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>] also proposed to detect sequences among tasks and control flow. Both are much depending on hand-written rules and patterns to detect workflow and could be limited to previously seen patterns.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> MemoryNet-Augmented Neural Network</h3>
          </div>
        </header>
        <p>MemoryNet was proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>]. They designed new types of memory representation for modeling and storing a set of sentences which are sequentially related to each other. The earliest recent work proposed an external memory component [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], and they has been applied to many language processing tasks. Their input module computes sentence vectors independently and store/load when it is needed. However, it has a limitation that strong supervision for training stage is needed. To overcome the problem, authors in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] proposed an end-to-end MemoryNet that is composed of two main components: supporting memories and predicting answers. Supporting memories get a set of inputs and then generate an output with memory cells, which is stored and updated using external embedding matrices. In this paper, we define a new approach to modeling the underlying structure of procedures using the architecture of MemoryNet. The memory representation is used for augmenting our proposed hierarchical attention neural network.</p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Problem Definition</h2>
        </div>
      </header>
      <p>Let us assume that target sentences, <em>S</em>, which contain actions, are identified before relationship classification. In this paper, we have leveraged our earlier work on action identification&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>] for identification of target sentences, and the corresponding method is described in Section 5.1. To represent the identified relationships and sentences, we propose a process-goal meta model.</p>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Process-Goal Meta Model</h3>
          </div>
        </header>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186347/images/www18companion-109-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Process-Goal Meta Model.</span>
          </div>
        </figure>
        <p>Procedure&nbsp;<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> text describes different alternative methods, each consisting of their own tasks/actions, arbitrary levels of nesting/hierarchy of tasks, and may describe various context when each method or action is applicable. For modeling a procedure, we propose a process-goal meta model as depicted in Figure <a class="fig" href="#fig2">2</a>. Each procedure document can be converted into the connected graph representation. Each procedure has a goal, which is represented as the head of the graph. This is the highest level element in the procedure. It is composed of (alternative) methods, tasks, and sub-tasks in turn. Each goal, method, task, and subtask can have contexts such as time, location, and actor. The contextual information can be extracted by many information extraction tools like named-entity recognition modules and semantic role labeling (out of scope in this paper). In this paper, we focus on the relations in the process-goal meta model. The relations of interest include: <em>is<span style="text-decoration: underline;">&nbsp;</span>method<span style="text-decoration: underline;">&nbsp;</span>of, is<span style="text-decoration: underline;">&nbsp;</span>alternative<span style="text-decoration: underline;">&nbsp;</span>of, is<span style="text-decoration: underline;">&nbsp;</span>task<span style="text-decoration: underline;">&nbsp;</span>of, is<span style="text-decoration: underline;">&nbsp;</span>subtask<span style="text-decoration: underline;">&nbsp;</span>of, conditional<span style="text-decoration: underline;">&nbsp;</span>of</em>, and <em>is<span style="text-decoration: underline;">&nbsp;</span>next<span style="text-decoration: underline;">&nbsp;</span>of</em> (Total 6 types.) Even though many previous semantic knowledge bases like ConceptNet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>] include some similar relationships such as ”hassubevent” and ”hasprerequisite,” our process-goal meta model can represent more process-specific, sequential, or hierarchical relationships among tasks.</p>
        <p>In the process-goal-meta model, a node is presented by a sentence&nbsp;<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a> except actor, time, and location. The reason why the relationship among them is important is that a sentence in procedural text usually describes the step(s) with its pre-condition and post-condition. Therefore, it is reasonable to get sentence representations for nodes. In particular, business procedure (process) for accounting, recruitment, call center, and technical support are often described by domain experts to document the procedures for other workers or for automation. The main challenge is learning the structure of a process, which is expressed in terms of relationships among different tasks/subtasks of the process, as described in the text as a series of steps using sentences. In this paper, we assume that each sentence represents a (sub)task or a method. Moreover, a goal is to understand the procedure including all (alternative) methods, their (sub)tasks, and the relationships among them in the document describing the procedure.</p>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186347/images/www18companion-109-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">The proposed hierarchical attention neural network.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Neural Network Architectures for Task Relation Classification</h2>
        </div>
      </header>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Hierarchical Attention Encoder (HAE)</h3>
          </div>
        </header>
        <p>LSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] is a neural network architecture which can model the state of sequences. The LSTM has a way of ”remembering” important information while disregarding the superfluous information through its gating mechanisms. LSTMs have the advantage that it can model variable-sized inputs, while traditional MLPs and other forms of neural networks are only designed for fixed-length inputs.</p>
        <p>In this paper, we propose a hierarchical attention neural network using LSTM as in Figure <a class="fig" href="#fig3">3</a>. Assume that <em>w<sub>it</sub></em> with <em>t</em> ∈ [1, <em>T</em>] represents the <em>t</em> <sub>th</sub> word in the <em>i</em> <sub>th</sub> sentence, <em>s<sub>i</sub></em>. In this architecture, we always have two sentences for relations. The proposed model encodes the raw sentences into a vector representation, which will be used for relationship classification. In the following subsections, we will describe how we encode each word and relation in detail.</p>
        <p><em>Word Encoder</em>. Firstly, the architecture gets words to have word vectors through an embedding matrix, <em>W<sub>e</sub></em>. For the embedding, we use the pre-trained Glove embedding matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>]. To get the representation from the word vectors, a bi-directional LSTM layer is used. The bi-directional LSTM contains the forward LSTM that reads from the first word, <em>w</em> <sub><em>i</em>1</sub> to the last word, <em>w<sub>iT</sub></em>. The backward LSTM reads the word in another direction.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} &amp;x_{it} = W_{e}{w_{it}, t} \in [1, T],\\ &amp;\overset{\rightarrow }{h}_{it} = \overrightarrow{LSTM}(x_{it}), t \in [1, T],\\ &amp;\overset{\leftarrow }{h}_{it} = \overleftarrow{LSTM}(x_{it}), t \in [T, 1]\\\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>It concatenates the forward hidden state <span class="inline-equation"><span class="tex">$\overset{\rightarrow }{h}_{it}$</span></span> and the backward hidden state <span class="inline-equation"><span class="tex">$\overset{\leftarrow }{h}_{it}$</span></span> . The concatenated vectors, <em>h<sub>it</sub></em> = [<span class="inline-equation"><span class="tex">$\overset{\rightarrow }{h}_{it}$</span></span> , <span class="inline-equation"><span class="tex">$\overset{\leftarrow }{h}_{it}$</span></span> ] where <em>t</em> ∈ [1, <em>T</em>], summarize the meaning of a sentence, <em>s<sub>i</sub></em>. This will be summed using our proposed attention mechanism as the following subsection.</p>
        <p><em>Word Attention</em>. Attention mechanism is to automatically focus on the words that have a decisive effect on classification. In other words, it is to capture the most important semantic information in a sentence, without using extra knowledge and NLP systems. In this attention layer, it produces a weight vector, <em>u<sub>w</sub></em> , and merge word-level features from each time step into a sentence-level feature vector, by multiplying the weight vector.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} &amp;u_{it} = t (W_{w}{h_{it}} + b_w), \\ &amp;\alpha _{it} = \frac{exp(u_{it}^{T} u_w)}{{\sum _{t} {exp(u_{it}^{T} u_w)}}}\\ &amp;k_i = \sum _{t}{\alpha _{it} h_{it}}\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>As in the above equation, we measure the importance of the word as the similarity of <em>u<sub>it</sub></em> with a word level context vector <em>u<sub>w</sub></em> and get the normalized weight <em>α<sub>it</sub></em> through a softmax function. After that, the weighted sum of words is computed to have the representation for <em>k<sub>i</sub></em>. The word context vector <em>u<sub>w</sub></em> is initialized randomly. This attention mechanism is similar to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>], but we use Glove embedding for handling sparse terminologies and applied the attention mechanism only to the word encoder.</p>
        <p><em>Sentence Encoder</em>. Sentence encoder is to learn the representation of relationship using the sentence vectors in a similar way.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} &amp;\overset{\rightarrow }{h}_{i} = \overrightarrow{LSTM}(k_{i}), i \in [1, 2],\\ &amp;\overset{\leftarrow }{h}_{i} = \overleftarrow{LSTM}(k_{i}), i \in [2, 1]\\ &amp;v = \sum _{i}^{2} {h_{i}}\\ &amp;y=\text{softmax}(W_{c}{v} + b_c)\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p><em>h<sub>i</sub></em> is the concatenated variable as in [<span class="inline-equation"><span class="tex">$\overset{\rightarrow }{h}_{i}$</span></span> , <span class="inline-equation"><span class="tex">$\overset{\leftarrow }{h}_{i}$</span></span> ], and then summed to feed to a softmax layer. The output dimension of <em>W<sub>c</sub></em> is the number of relationships.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} &amp;L=-\sum _{r}{y_{r}\log {\hat{y}_{r}}}\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>Its loss function is defined as above using the output of softmax layer. We use the categorical cross-entropy as the loss function. The loss function will be calculated upon the relation, <em>r</em>. The <em>r</em> is one of relationship labels. Its optimization is accomplished through Adam optimizer&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>].</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> MemoryNet-augmented Relation Classifier (MARC)</h3>
          </div>
        </header>
        <p>The introduced Hierarchical Attention Network in Section 4.1 models each relation. To encode procedure information, though, we need to model not only the sentence-level relations but also higher-level relation information such as context or hierarchical action information (e.g. Parent Task or Previous Task). In order to do this, we use MemoryNet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] to augment the contextual, hierarchical and underlying intrinsic information for relations. The underlying intrinsic information could be domain knowledge or shared common sense. For example, the intrinsic information for recipe could be basic ingredient preparation in commonsense and apply different importance to each words depending on the domain. In this paper, we use the architecture of end-to-end Hierarchical Attention Network&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] as an encoder (HAE) and utilize it to store the encoded vectors in the associated memory in MemoryNet.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} &amp;u = {\text{HAE}}_{1}(s_i, s_j)\\ &amp;m_i = {\text{HAE}}_{2}(f_i)\\ &amp;c_i = {\text{HAE}}_{3}(f_i)\\ &amp;p_i = \text{softmax}(u^{T}{m_i})\\ &amp;o = \sum _{i=1}^{k}{p_{i}c_{i}}\\ &amp;\hat{u}= o+u\\ &amp;\hat{y}=\text{softmax}(W_{m}{\hat{u}} + b_m)\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>Assume that we are given input sentences, <em>S</em> = {<em>s<sub>i</sub></em> , <em>s<sub>j</sub></em> }, and the contextual information of the sentences, <em>s<sub>i</sub></em> , is <em>f<sub>i</sub></em> = {<em>f</em> <sub><em>i</em>1</sub>, <em>f</em> <sub><em>i</em>2</sub>, ..., <em>f<sub>in</sub></em> } that is composed of contextual sentences. Again, the contextual information could be goal for method or goal + method for task, for example. Figure <a class="fig" href="#fig4">4</a> shows the architecture of MemoryNet-augmented Relation Classifier (MARC). The above equations also show the internal computation.</p>
        <p>The input sentences, <em>S</em> = {<em>s<sub>i</sub></em> , <em>s<sub>j</sub></em> }, are converted into memory vectors, <em>u</em>. The contextual sentences, <em>f<sub>i</sub></em> are also converted into <em>m<sub>i</sub></em>. We compute the match between <em>u</em> and the corresponding memory <em>m<sub>i</sub></em> by taking the inner product followed by a softmax. HAE<sub>3</sub> returns output memory representation of <em>f<sub>i</sub></em>. The response vector, <em>c<sub>i</sub></em> will be weighted by the probability vector, <em>p<sub>i</sub></em>.</p>
        <p>The weighted output, <em>o</em> will be added to the input vector, <em>u</em>, and the softmax activation of <span class="inline-equation"><span class="tex">$W_{m}{\hat{u}} + b_m$</span></span> will be followed. The optimization and loss function are same as those in Hierarchical Attention Encoder (HAE). As in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>], it could have more layers for MemoryNet. In other words, the HAE<sub>2</sub> and HAE<sub>3</sub> can be stacked up multiple times. In the figure <a class="fig" href="#fig4">4</a>, [1, ..., N] indicates that the two layers could be used multiple times. In this paper, we use a 2-layer memory network architecture. As the architecture has multiple layers, each HAN is shared at each layer.</p>
        <figure id="fig4">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186347/images/www18companion-109-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span> <span class="figure-title">MemoryNet-augmented Relation Classifier.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experiments</h2>
        </div>
      </header>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Data</h3>
          </div>
        </header>
        <p>In the experiment, we use data from an online community, <em>wikihow.com</em> which has been contributed by thousands of users. Wikihow includes abundant informative contents about procedures in many domains such as recipes, finance, cars, education, health, hobbies, personal care, and etc. In total, 198,163 articles were downloaded using the wikiTeam <a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a> data processing interface. We only used articles which have at least one of each of these entities of interest: goal, method, task, and subtask. ”Goal” is from the first sentence of the main description in a procedural description in an article. Similarly, sentences for method, task, and subtask were extracted. The identification of method, task, subtask could be solved using inherent mediaWiki structural conventions (e.g. starting of new child section).</p>
        <p>The first sentences of methods and tasks are presented as ”verb object (*)” in general. Subtasks may contain many conditional statements or details about the task. Table <a class="tbl" href="#tab1">1</a> shows the number of identified relation triples with their underlying statistics. As in the first row of the table, the number of extracted relation triples is 78,217. Here, we counted every triple of sentences and relationships, which is extracted from our Hierarchical Attention Neural Network. For example, &lt;”Dissolve 3 table spoons salt in the water”&gt;- <em>is<span style="text-decoration: underline;">&nbsp;</span>next<span style="text-decoration: underline;">&nbsp;</span>of</em>-&lt;”Wash your hands and utensils”&gt; is a triple. To compute the averages in the table <a class="tbl" href="#tab1">1</a>, each score is summed and averaged over the number of WikiHow's articles. For extracting the <em>conditional<span style="text-decoration: underline;">&nbsp;</span>of</em>, Stanford dependency parser [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] is used for detecting conditional and consequent clauses. From the parsed tree, sub-nodes of SBAR are utilized to detect the clauses, and sentences were considered if they have the following connectives: IF, UNLESS, EVEN IF, AS (SO) LONG AS, ASSUMING/SUPPOSING, IN CASE, and ONLY IF.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Data Statistics: Extracted Relations.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"><strong>Data Statistics</strong></th>
                <th style="text-align:center;"><strong>Counts</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Number of Extracted Relation Triples</td>
                <td style="text-align:center;">78,217</td>
              </tr>
              <tr>
                <td style="text-align:left;">Avg Number of Methods per Goal</td>
                <td style="text-align:center;">2.15</td>
              </tr>
              <tr>
                <td style="text-align:left;">Avg Number of Tasks per Method</td>
                <td style="text-align:center;">7.57</td>
              </tr>
              <tr>
                <td style="text-align:left;">Avg Number of sub-Tasks per Tasks</td>
                <td style="text-align:center;">3.43</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Methods</h3>
          </div>
        </header>
        <p>To evaluate our method, alternative methods below are considered. As an evaluation metric, we use accuracy, which is calculated by counting how many correct labels are predicted over the total number of possible relation triples. For training and testing below models, we use 70%, 10%, and 20% of procedural articles for training, validation, and testing, respectively. For all neural network models, max epochs are set to 100, and if accuracy on validation dataset does not increase during 15 epochs, it stops training. We also use dropout regularization (0.2) and rectified linear units (ReLU) for activation functions. For optimization, <em>Adam</em> optimizer[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] is used to update variables. In the rest of this subsection, models that we used for the experiment are described.</p>
        <p><em>Convolutional Neural Network</em>. The approach in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] takes a sentence as an input and transforms it into the vector representation through Convolutional Neural Network (CNN) by splitting the sentence into left, middle, and right phrases. They learn three different CNNs and concatenate their outputs at the end to classify entity-to-entity relationship. Authors in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>] additionally employ the concept of positional vector to represent the relationship in the sentence through CNN. In our case, we would like to find the relations among sentences, so we could not use the same approach. Instead, we model the whole sentence using the convolutional neural network without using splitting and positional vector. This approach is well-studied and has shown good performance for sentence classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>]. We also use max pooling and concatenate in output. The concatenated output feeds fully-connected layer with softmax activation. For parameters for CNN, the number of feature maps is searched over [8, 16, 32, 64], and the size of feature maps is chosen from [3, 5, 7]. Max pooling size is selected from [4, 8, 16].</p>
        <p><em>Bi-directional LSTM</em>. We also use LSTM for the purpose of comparison. As in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>], we use the bi-directional LSTM for modeling sentence. Again, because the method in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] was also designed for detecting relationship among entities in a sentence, we use natural sentences as inputs of LSTM. The number of hidden nodes is chosen by grid search. The grid search is selected from [50, 100, 200].</p>
        <p><em>Hierarchical Bi-directional LSTM</em>. This is to verify how the attention matrix works for relation classification. This is the architecture which does not have attention-based activation. To search its appropriate parameters, the number of hidden nodes is searched from [50, 100, 200].</p>
        <p><em>Hierarchical Bi-directional LSTM with Attention</em>. This is one of our proposed models, which is labeled as (Our Model1) in the result section below. This model uses the word attention, and its performance is related and compared for effectiveness. Here, to search appropriate parameters, the number of hidden nodes for LSTM is also searched from [50, 100, 200].</p>
        <p><em>End-to-end MemoryNet</em>. This model is adapted to our procedure learning task to evaluate the performance of the end-to-end MemoryNet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>]-like neural model. The original end-to-end MemoryNet is designed for reading comprehension. Instead of the given background sentences in the task, we use all the high-level information, <em>f<sub>i</sub></em> , and the first sentence, <em>s<sub>i</sub></em> , to train embedding and attention variables. In addition, the second sentence, <em>s<sub>j</sub></em> , is used for input query, instead of the question in the reading comprehension task. For a fair comparison, we used HAE encoders for embedding matrices and trained using same parameters with our MemoryNet-augmented Relational Classifier's ones.</p>
        <p><em>MemoryNet-augmented Relational Classifier</em>. This classifier is the other proposed model in this paper (Our Model 2). This model employs the encoder, which is proposed as <em>Hierarchical Bi-directional LSTM with Attention</em>, to model each sentence for feeding inputs or embedding vectors in MemoryNet. The three different encoders (HAE<sub>1</sub>, HAE<sub>2</sub>, and HAE<sub>3</sub>) are trained separately. For their parameter search, the number of hidden nodes for LSTM is also selected from [50, 100, 200].</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Results</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Result of Relation Classification.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"><strong>Approaches</strong></th>
                <th style="text-align:center;"><strong>Accuracy</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Convolutional Neural Network</td>
                <td style="text-align:center;">0.73</td>
              </tr>
              <tr>
                <td style="text-align:left;">Bi-directional LSTM</td>
                <td style="text-align:center;">0.75</td>
              </tr>
              <tr>
                <td style="text-align:left;">Hierarchical Bi-directional LSTM</td>
                <td style="text-align:center;">0.869</td>
              </tr>
              <tr>
                <td style="text-align:left;">20cmHierarchical Bi-directional LSTM</td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">with Attention (Our Model 1)</td>
                <td style="text-align:center;">0.878</td>
              </tr>
              <tr>
                <td style="text-align:left;">20cmMemoryNet-augmented Relation Classifier</td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">(Our Model 2)</td>
                <td style="text-align:center;">0.874</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Table <a class="tbl" href="#tab2">2</a> shows results of experiments. In the experiment, CNN-based relation classifier showed the worst result. When we used the Bi-directional LSTM, it was able to get a better result than the CNN's one. In addition to the Bi-directional LSTM, a layer of word-level LSTM was additionally considered, the neural network got the additional significant gain. This means that the idea of hierarchy is crucial to have better classification results.</p>
        <p>In the experiment, our proposed model (Hierarchical Bi-directional LSTM with Attention) showed the best result for the classification. It indicates that the attention model is useful to detect the important words for modeling each relation triple. We also tested whether the additional attention matrix for the second Bi-directional LSTM layer is needed or not, but it did not show any improvement. MemoryNet-augmented Relation Classifier shows 0.874 in accuracy in the dataset, but slightly less than Hierarchical Bi-directional LSTM with Attention (Our model 1). This implies that it is enough to model the procedural text using one HAN encoder when we have enough training data.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">Result of Relation Classification on Smaller Procedures for Training and Testing (10 Percent of Randomly Sampled Procedural Articles).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"><strong>Approaches</strong></th>
                <th style="text-align:center;"><strong>Accuracy</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">20cmHierarchical Bi-directional LSTM</td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">with Attention (Our Model 1)</td>
                <td style="text-align:center;">0.829</td>
              </tr>
              <tr>
                <td style="text-align:left;">20cmEnd-to-end Neural Network</td>
                <td style="text-align:center;">0.74</td>
              </tr>
              <tr>
                <td style="text-align:left;">20cmMemoryNet-augmented Relation Classifier</td>
                <td></td>
              </tr>
              <tr>
                <td style="text-align:left;">(Our Model 2)</td>
                <td style="text-align:center;">0.849</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>However, when we used 10 percent of total procedural articles, which are randomly chosen, MemoryNet-augmented Relation Classifier got better than the model 1 (Hierarchical Bi-directional LSTM with Attention) as shown in Table <a class="tbl" href="#tab3">3</a>. This means MemoryNet-augmented Relation Classifier has an advantage when it is trained by smaller training data. This result aligns with the previous observation, i.e [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>]. Authors in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>] reported that the MemoryNet-Driven Neural Network is effective at one-shot learning, and multiple MemoryNets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], which maximize the effect of single MemoryNets, also showed a better performance on Question/Answering Tasks. When our Model 2 was compared to End-to-end Neural Network, it was also significantly better in accuracy. This shows that our HAE encodes the relation well and is more effective only if the contextual information is stored in the memory.</p>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion and Future Work</h2>
        </div>
      </header>
      <p>In this paper, we presented a neural network architecture for extracting procedure-specific relationships. Unlike the previous work, our methodology does not rely on information extraction pipeline for identifying task relationships. Rather, it learns rich procedure-specific relations like the sequential, conditional and hierarchical relationship among goals, methods, and tasks by directly feeding sentences in the text of procedure descriptions (also in a hybrid, attention-aware manner) into a novel neural network architecture designed for this purpose. In experiments, the proposed methods yielded superior results compared to the state of the art methods for relationship extraction. In particular, MemoryNet-augmented Classifier demonstrates great performance using a relatively small size of training data.</p>
      <p>As future work, we plan to investigate methods that use the proposed architectures for learning procedures knowledge bases in question-answering scenarios. Moreover, our work can be extended to learn more complex and chained action relationships from process and procedure descriptions.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Heike Adel, Benjamin Roth, and Hinrich Schütze. 2016. Comparing convolutional neural networks to traditional models for slot filling. <em><em>arXiv preprint arXiv:1603.05157</em></em> (2016).</li>
        <li id="BibPLXBIB0002" label="[2]">Razvan&nbsp;C. Bunescu and Raymond&nbsp;J. Mooney. 2005. A Shortest Path Dependency Kernel for Relation Extraction. In <em><em>Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</em></em> (<em>HLT ’05</em>). Association for Computational Linguistics, Stroudsburg, PA, USA, 724–731. <a class="link-inline force-break" href="https://doi.org/10.3115/1220575.1220666" target="_blank">https://doi.org/10.3115/1220575.1220666</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In <em><em>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em></em>. 740–750.</li>
        <li id="BibPLXBIB0004" label="[4]">Cuong&nbsp;Xuan Chu, Niket Tandon, and Gerhard Weikum. 2017. Distilling Task Knowledge from How-To Communities. In <em><em>Proceedings of the 26th International Conference on World Wide Web</em></em> (<em>WWW ’17</em>). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 805–814. <a class="link-inline force-break" href="https://doi.org/10.1145/3038912.3052715" target="_blank">https://doi.org/10.1145/3038912.3052715</a>
        </li>
        <li id="BibPLXBIB0005" label="[5]">Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open Information Extraction: The Second Generation.. In <em><em>IJCAI</em></em> , Vol.&nbsp;11. 3–10.</li>
        <li id="BibPLXBIB0006" label="[6]">Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. <em><em>Neural Comput.</em></em> 9, 8 (Nov. 1997), 1735–1780. <a class="link-inline force-break" href="https://doi.org/10.1162/neco.1997.9.8.1735" target="_blank">https://doi.org/10.1162/neco.1997.9.8.1735</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Konrad Höffner, Sebastian Walter, Edgard Marx, Ricardo Usbeck, Jens Lehmann, and Axel-Cyrille Ngonga&nbsp;Ngomo. 2017. Survey on challenges of question answering in the semantic web. <em><em>Semantic Web</em></em> 8, 6 (2017), 895–920.</li>
        <li id="BibPLXBIB0008" label="[8]">Yuchul Jung, Jihee Ryu, Kyung-min Kim, and Sung-Hyon Myaeng. 2010. Automatic Construction of a Large-scale Situation Ontology by Mining How-to Instructions from the Web. <em><em>Web Semant.</em></em> 8, 2-3 (July 2010), 110–124. <a class="link-inline force-break" href="https://doi.org/10.1016/j.websem.2010.04.006" target="_blank">https://doi.org/10.1016/j.websem.2010.04.006</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">Yoon Kim. 2014. Convolutional neural networks for sentence classification. In <em><em>Proceedings of Conference on Empirical Methods in Natural Language Processing</em></em> (<em>EMNLP ’14</em>). 1746–1751.</li>
        <li id="BibPLXBIB0010" label="[10]">Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <em><em>arXiv preprint arXiv:1412.6980</em></em> (2014).</li>
        <li id="BibPLXBIB0011" label="[11]">Man Lan, Jianxiang Wang, Yuanbin Wu, Zheng-Yu Niu, and Haifeng Wang. 2017. Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification. In <em><em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em></em>. 1299–1308.</li>
        <li id="BibPLXBIB0012" label="[12]">Lijun Mei, Qicheng Li, Rangachari Anand, Juhnyoung Lee, Feng Li, and Shaochun Li. 2014. Enabling customizable service-based procedural knowledge applications—An service-orientation for Dialog Manager. In <em><em>Service Operations and Logistics, and Informatics (SOLI), 2014 IEEE International Conference on</em></em>. IEEE, 412–417.</li>
        <li id="BibPLXBIB0013" label="[13]">Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. <em><em>arXiv preprint arXiv:1606.03126</em></em> (2016).</li>
        <li id="BibPLXBIB0014" label="[14]">Makoto Miwa and Mohit Bansal. 2016. End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures. <em><em>Proceedings of ACL</em></em> (2016).</li>
        <li id="BibPLXBIB0015" label="[15]">Hamid R.&nbsp;Motahari Nezhad, Kalpa Gunaratna, and Juan Cappi. 2017. eAssistant: Cognitive Assistance for Identification and Auto-Triage of Actionable Conversations. In <em><em>Proceedings of the 26th International Conference on World Wide Web Companion</em></em> (<em>WWW ’17 Companion</em>). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 89–98. <a class="link-inline force-break" href="https://doi.org/10.1145/3041021.3054147" target="_blank">https://doi.org/10.1145/3041021.3054147</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-Sentence N-ary Relation Extraction with Graph LSTMs. <em><em>Transactions of the Association for Computational Linguistics</em></em> 5 (2017), 101–115.</li>
        <li id="BibPLXBIB0017" label="[17]">Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In <em><em>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em></em>. 1532–1543.</li>
        <li id="BibPLXBIB0018" label="[18]">Chris Quirk and Hoifung Poon. 2017. Distant supervision for relation extraction beyond the sentence boundary. <em><em>Proceedings of ACL</em></em> (2017), 1171–1182.</li>
        <li id="BibPLXBIB0019" label="[19]">Samuel Rönnqvist, Niko Schenk, and Christian Chiarcos. 2017. A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations. In <em><em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em></em>. Association for Computational Linguistics, 256–262. <a class="link-inline force-break" href="https://doi.org/10.18653/v1/P17-2040" target="_blank">https://doi.org/10.18653/v1/P17-2040</a>
        </li>
        <li id="BibPLXBIB0020" label="[20]">Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. 2016. One-shot learning with memory-augmented neural networks. <em><em>arXiv preprint arXiv:1605.06065</em></em> (2016).</li>
        <li id="BibPLXBIB0021" label="[21]">Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, <em>et al.</em> 2012. Open language learning for information extraction. In <em><em>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</em></em>. Association for Computational Linguistics, 523–534.</li>
        <li id="BibPLXBIB0022" label="[22]">Pol Schumacher and Mirjam Minor. 2014. Extracting control-flow from text. In <em><em>Information Reuse and Integration (IRI), 2014 IEEE 15th International Conference on</em></em>. IEEE, 203–210.</li>
        <li id="BibPLXBIB0023" label="[23]">Pol Schumacher, Mirjam Minor, Kirstin Walter, and Ralph Bergmann. 2012. Extraction of Procedural Knowledge from the Web: A Comparison of Two Workflow Extraction Approaches. In <em><em>Proceedings of the 21st International Conference on World Wide Web</em></em> (<em>WWW ’12 Companion</em>). ACM, New York, NY, USA, 739–747. <a class="link-inline force-break" href="https://doi.org/10.1145/2187980.2188194" target="_blank">https://doi.org/10.1145/2187980.2188194</a>
        </li>
        <li id="BibPLXBIB0024" label="[24]">Robert Speer and Catherine Havasi. 2013. ConceptNet 5: A large semantic network for relational knowledge. In <em><em>The People's Web Meets NLP</em></em>. Springer, 161–176.</li>
        <li id="BibPLXBIB0025" label="[25]">Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, <em>et al.</em> 2015. End-to-end memory networks. In <em><em>Advances in neural information processing systems</em></em>. 2440–2448.</li>
        <li id="BibPLXBIB0026" label="[26]">Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. <em><em>arXiv preprint arXiv:1410.3916</em></em> (2014).</li>
        <li id="BibPLXBIB0027" label="[27]">Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2015. Semantic relation classification via convolutional neural networks with simple negative sampling. <em><em>arXiv preprint arXiv:1506.07650</em></em> (2015).</li>
        <li id="BibPLXBIB0028" label="[28]">Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths. In <em><em>EMNLP</em></em>. 1785–1794.</li>
        <li id="BibPLXBIB0029" label="[29]">Zi Yang and Eric Nyberg. 2015. Leveraging Procedural Knowledge for Task-oriented Search. In <em><em>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> (<em>SIGIR ’15</em>). ACM, New York, NY, USA, 513–522. <a class="link-inline force-break" href="https://doi.org/10.1145/2766462.2767744" target="_blank">https://doi.org/10.1145/2766462.2767744</a>
        </li>
        <li id="BibPLXBIB0030" label="[30]">Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander&nbsp;J Smola, and Eduard&nbsp;H Hovy. 2016. Hierarchical Attention Networks for Document Classification. In <em><em>HLT-NAACL</em></em>. 1480–1489.</li>
        <li id="BibPLXBIB0031" label="[31]">Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel Methods for Relation Extraction. <em><em>J. Mach. Learn. Res.</em></em> 3 (March 2003), 1083–1106. <a class="link-inline force-break" href="http://dl.acm.org/citation.cfm?id=944919.944964" target="_blank">http://dl.acm.org/citation.cfm?id=944919.944964</a>
        </li>
        <li id="BibPLXBIB0032" label="[32]">Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao, <em>et al.</em> 2014. Relation Classification via Convolutional Deep Neural Network.. In <em><em>COLING</em></em>. 2335–2344.</li>
        <li id="BibPLXBIB0033" label="[33]">Shu Zhang, Dequan Zheng, Xinchen Hu, and Ming Yang. 2015. Bidirectional Long Short-Term Memory Networks for Relation Classification.. In <em><em>PACLIC</em></em>.</li>
        <li id="BibPLXBIB0034" label="[34]">Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. In <em><em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em></em>, Vol.&nbsp;1. 1227–1236.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>The work had been done during an internship at IBM Almaden Research Center</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>we use the pair of ”process” and ”procedure”, and the pair of ”action” and ”task” interachangeably</p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a>In case of ”conditional_of”, the node could be a phrase.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="https://github.com/WikiTeam/wikiteam" target="_blank">https://github.com/WikiTeam/wikiteam</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04…15.00 .<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186347">https://doi.org/10.1145/3184558.3186347</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
