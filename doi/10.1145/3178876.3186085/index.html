<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Manifold Learning for Rank Aggregation</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Manifold Learning for Rank Aggregation</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author"><span class="givenName">Shangsong</span>      <span class="surName">Liang</span>,     KAUST, Thuwal, Saudi Arabia, <a href="mailto:shangsong.liang@kaust.edu.sa">shangsong.liang@kaust.edu.sa</a>     </div>     <div class="author"><span class="givenName">Ilya</span>      <span class="surName">Markov</span>,     University of Amsterdam, Amsterdam, The Netherlands, <a href="mailto:i.markov@uva.nl">i.markov@uva.nl</a>     </div>     <div class="author"><span class="givenName">Zhaochun</span>      <span class="surName">Ren</span>,     JD AI, Beijing, China, <a href="mailto:renzhaochun@jd.com">renzhaochun@jd.com</a>     </div>     <div class="author"><a href="https://orcid.org/0000-0002-1086-0202" ref="author"><span class="givenName">Maarten</span>      <span class="surName">de Rijke</span></a>,     University of Amsterdam, Amsterdam, The Netherlands, <a href="mailto:derijke@uva.nl">derijke@uva.nl</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186085" target="_blank">https://doi.org/10.1145/3178876.3186085</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>We address the task of fusing ranked lists of documents that are retrieved in response to a query. Past work on this task of rank aggregation often assumes that documents in the lists being fused are independent and that only the documents that are ranked high in many lists are likely to be relevant to a given topic. We propose manifold learning aggregation approaches, ManX and v-ManX, that build on the cluster hypothesis and exploit inter-document similarity information. ManX regularizes document fusion scores, so that documents that appear to be similar within a manifold, receive similar scores, whereas v-ManX first generates virtual adversarial documents and then regularizes the fusion scores of both original and virtual adversarial documents. Since aggregation methods built on the cluster hypothesis are computationally expensive, we adopt an optimization method that uses the top-<em>k</em> documents as anchors and considerably reduces the computational complexity of manifold-based methods, resulting in two efficient aggregation approaches, a-ManX and a-v-ManX. We assess the proposed approaches experimentally and show that they significantly outperform the state-of-the-art aggregation approaches, while a-ManX and a-v-ManX run faster than ManX, v-ManX, respectively.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Rank aggregation;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Dimensionality reduction and manifold learning;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Ad hoc retrieval; rank aggregation; manifold learning</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Shangsong Liang, Ilya Markov, Zhaochun Ren, and Maarten de Rijke. 2018. Manifold Learning for Rank Aggregation. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA, 11 pages. <a href="https://doi.org/10.1145/3178876.3186085" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186085</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Rank aggregation, also known as data fusion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], is an important technique in information retrieval. Rank aggregation combines multiple ranked lists of documents retrieved from a corpus in response to a query by multiple retrieval algorithms. The ranked lists can be produced by any retrieval approach, using different ranking functions and multiple query and/or document representations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. The combination of several retrieval approaches is assumed to improve retrieval effectiveness of the final fused ranked list of documents.</p>    <p>Past work on data fusion mostly assumes that documents in the lists being combined are independent and that only documents that are ranked high in many lists, are likely to be relevant to a given query&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. However, the cluster hypothesis states that <em>documents in the same intrinsic structure, i.e., cluster or manifold, are likely to have a similar degree of relevance to the same information need underlying a given query</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. This idea has been successfully applied to many ranking problems in information retrieval and data mining&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>]. In data fusion the cluster hypothesis has only been used to a limited extent&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] and with a negative impact on efficiency.</p>    <p>We propose a novel <strong>Man</strong>ifold-based data fusion approach, ManX, which (1)&#x00A0;builds on a generic data fusion method <strong>X</strong>, and (2)&#x00A0;lets similar documents provide support to each other by using inter-document similarities within a global manifold of documents being fused. Our manifold-based data fusion technique, ManX, is computationally demanding at two stages: in graph construction and in fusion score regularization. Therefore, we adopt an efficient design of the adjacency matrix for graph construction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], which supports document fusion for large datasets. Using this adjacency matrix, we propose a more efficient version of ManX, a-ManX, where the top-<em>k</em> documents from the ranking produced by an underlying fusion method X are assumed to be relevant and are used as <strong>a</strong>nchors to represent fusion scores of other documents.</p>    <p>Many machine learning models are vulnerable to adversarial data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. To further improve the performance of rank aggregation, we propose a virtual adversarial manifold learning algorithm, v-ManX, and an efficient version that utilizes anchor documents, a-v-ManX. Our proposed virtual adversarial manifold learning algorithms first generate a virtual adversarial document for each original document, then regularize the model so that given a document, the models will produce the same output distribution as they produce on an adversarial perturbation of that document. They improve the robustness to virtual adversarial documents and the generalization performance for original documents, thus enhancing the performance of manifold learning for rank aggregation.</p>    <p>To evaluate the effectiveness and efficiency of ManX and a-ManX, we conduct experiments using retrieval runs (at the Text REtrieval Conference (TREC),<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> a ranked list of documents is also called a run) submitted to TREC-3&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>], TREC-10&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] and TREC-12&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>]. Experimental results confirm the theoretical findings, the effectiveness and efficiency of the proposed methods.</p>    <p>Our contributions in this paper are:</p>    <ol class="list-no-style">     <li id="list1" label="(1)">We propose novel rank aggregation approaches, ManX, a-ManX, v-ManX and a-v-ManX, that exploit the manifold structure of documents being fused.<br/></li>     <li id="list2" label="(2)">We propose two virtual adversarial learning algorithms, v-ManX and a-v-ManX for rank aggregation.<br/></li>     <li id="list3" label="(3)">We propose a new virtual adversarial construction algorithm in our v-ManX and a-v-ManX algorithms.<br/></li>     <li id="list4" label="(4)">We propose an efficient design of the adjacency matrix and the anchor-based method to reduce the computational complexity of a-ManX and a-v-ManX.<br/></li>     <li id="list5" label="(5)">Through extensive experiments we show that the proposed ManX method outperforms the state-of-the-art data fusion techniques in terms of effectiveness, while a-ManX also considerably improves data fusion efficiency.<br/></li>    </ol>    <p>The remainder of the paper is organized as: &#x00A7;&#x00A0;<a class="sec" href="#sec-6">2</a> reviews existing data fusion techniques and manifold-based algorithms. &#x00A7;&#x00A0;<a class="sec" href="#sec-10">3</a> lists preliminaries. &#x00A7;&#x00A0;<a class="sec" href="#sec-13">4</a> presents our manifold-based data fusion methods, ManX and a-ManX. &#x00A7;&#x00A0;<a class="sec" href="#sec-20">5</a> describes the experimental setup, while &#x00A7;&#x00A0;<a class="sec" href="#sec-25">6</a> discusses our experimental results. We conclude in &#x00A7;&#x00A0;<a class="sec" href="#sec-31">7</a>.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>Three types of research relate to our work: rank aggregation, manifold-based algorithms and adversarial learning algorithms.</p>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Rank aggregation</h3>     </div>     </header>     <p>A large number of rank aggregation algorithms have been proposed. Well-known examples include the CombSUM data fusion family&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], Borda data fusion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], supervised rank aggregation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>], <em>&#x03BB;</em>-Merge&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>], cluster-based data fusion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>], fusion for diversification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>], and, more recently, an aggregation algorithm that learns joint models on both lists and object features&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] and rule-based aggregation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>].</p>     <p>The state-of-the-art fusion method that we use for comparison is the cluster-based approach, ClustFuse, proposed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. ClustFuse uses a combination of a fusion method, like CombSUM, and cluster-based retrieval, thus building on the cluster hypothesis. However, ClustFuse utilizes the nearest-neighbor clustering approach, which considers only the <em>local</em> similarity between documents&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>]. We argue that considering the <em>global</em> similarity within a document manifold will allow us to use the full power of the cluster hypothesis, which will further improve the performance of data fusion. To validate this intuition, we propose a number of manifold-based aggregation methods. To the best of our knowledge, ours is the first attempt to utilize manifolds in rank aggregation.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Manifold-based algorithms</h3>     </div>     </header>     <p>Many manifold-based algorithms have been proposed, for a range of problems in applications. Recent ones include neural networks-based manifold learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>]. In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>], manifold-based algorithms are used for document classification. In&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>], the authors use manifold-based algorithms for recognizing handwritten digits. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] manifold-based algorithms are used for video prediction, in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>] for detecting collective motion, and in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] for face recognition.</p>     <p>We propose to use manifolds to regularize document scores in data fusion. Our manifold algorithms differ from previous ones&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>] by introducing virtual perturbation to documents, which allows us to significantly improve the performance. Since the computation of regularized scores is expensive we also propose an efficient version of manifold-based data fusion that uses the top-<em>k</em> documents as anchors. To the best of our knowledge, we are the first to utilize top-<em>k</em> ranked documents for efficient manifold learning.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Virtual adversarial learning algorithms</h3>     </div>     </header>     <p>Adversarial learning is the process of training a model to correctly label both unmodified data and adversarial data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. It improves not only robustness to adversarial data, but also generalization performance for original data.</p>     <p>Virtual adversarial learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>] extends the idea of adversarial learning to the semi-supervised regime and unlabeled data. This is done by regularizing the model so that given an example, the model will produce the same output distribution as it produces on an adversarial perturbation of that example. One key to the success of virtual adversarial learning is the way proper virtual adversarial data is generated. Miyato et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>], ,<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>] resort to an iteration method and finite difference method to approximately generate <em>local</em> virtual adversarial data, where &#x201C;local&#x201D; indicates that each virtual adversarial example is generated by considering its own original example only but not other data. Unlike previous adversarial learning algorithms where each adversarial example of the data is generated locally, our virtual adversarial algorithms generate each virtual adversarial document <em>globally</em> by considering not only the original itself but all the documents for adversarial perturbation construction. In addition, our two virtual adversarial manifold learning algorithms are unsupervised, compared to any of the existing adversarial learning algorithms that are either supervised or semi-supervised&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]. See&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] for a more thorough review of adversarial learning methods. To the best of our knowledge, we are the first to globally generate virtual adversarial data in adversarial learning, and the first to utilize virtual adversarial perturbation with manifolds for rank aggregation.</p>    </section>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Preliminaries</h2>     </div>    </header>    <p>We detail the task we address and recall standard fusion algorithms that most state-of-the-art fusion methods, including ours, build on.</p>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Problem formulation</h3>     </div>     </header>     <p>We begin by defining the data fusion task that we address. The task is: given a query <em>q</em> and a set of ranked lists of documents <span class="inline-equation"><span class="tex">$\mathcal {L}_1, \ldots , \mathcal {L}_m$</span>     </span>, produced in response to a query <em>q</em> by <em>m</em> different retrieval systems, combine documents contained in the given lists by a data fusion method X into a single ranked list <span class="inline-equation"><span class="tex">$\mathcal {L}_f$</span>     </span>. The aggregation algorithm X is essentially a function <em>f</em>     <sub>X</sub> that satisfies: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} q, \mathcal {L}_1, \mathcal {L}_2, \ldots , \mathcal {L}_m \stackrel{f_{\mathrm{X}}}{\longrightarrow } \mathcal {L}_f.\end{equation*} </span>       <br/>      </div>     </div> The goal of the task is to improve the performance of the final fused list <span class="inline-equation"><span class="tex">$\mathcal {L}_f$</span>     </span> over the expected performance of the input lists.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Standard data fusion</h3>     </div>     </header>     <p>The CombSUM family is a set of simple unsupervised standard methods for data fusion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. They assume that documents that are ranked high in many input result lists should also be ranked high in the final fused list. The methods of the CombSUM family are among the best performing unsupervised data fusion techniques, reaching performance levels of sophisticated supervised approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>].</p>     <p>Given a document <em>d</em> and a query <em>q</em>, CombSUM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] calculates the fusion score <em>f</em>     <sub>X</sub>(<em>d</em>; <em>q</em>) based on the retrieval score (or rank if no retrieval score is available) of <em>d</em> in given lists <span class="inline-equation"><span class="tex">$\mathcal {L}_1, \ldots , \mathcal {L}_m$</span>     </span>. CombSUM sums list-specific document scores: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} f_{\mathrm{CombSUM}}(d;q) :=\sum _{\mathcal {L}_i} \mathrm{score}_{\mathcal {L}_i}(d), \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathrm{score}_{\mathcal {L}_i}(d)$</span>     </span> is the retrieval score of <em>d</em> produced by list <span class="inline-equation"><span class="tex">$\mathcal {L}_i$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathrm{score}_{\mathcal {L}_i}$</span>     </span> (<em>d</em>) = 0 if <span class="inline-equation"><span class="tex">$d \notin \mathcal {L}_i$</span>     </span>. In addition to summing scores, CombMNZ rewards documents that are ranked high in many lists: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{equation*} f_{\mathrm{CombMNZ}}(d;q) :=|\lbrace \mathcal {L}_i: d \in \mathcal {L}_i \rbrace |\cdot f_{\mathrm{CombSUM}}(d;q),\end{equation*} </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$|\lbrace \mathcal {L}_i: d \in \mathcal {L}_i \rbrace |$</span>     </span> is the number of input lists in which document <em>d</em> appears.</p>    </section>   </section>   <section id="sec-13">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Proposed Methods</h2>     </div>    </header>    <p>The cluster hypothesis states that similar documents should have a similar degree of relevance to a given information need. A common approach to exploring this hypothesis in its full power is to consider a document manifold and regularize scores based on global inter-document similarities within this manifold&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>].</p>    <p>Consider the example in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>. Here, relevant documents are shown in the lower moon and non-relevant documents in the upper moon. The top-ranked relevant documents are indicated by red markers &#x2018;+&#x2019;, while other documents are marked with blue circles &#x2018;o&#x2019;, where the size of a circle is proportional to the rank of the corresponding document (larger size denotes higher rank). As we see in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>(a), if we rank documents based on local similarities such as Euclidean distances, many non-relevant documents in the upper moon that are close to the red crosses will be ranked higher than relevant documents in the lower moon that are further away. However, as we see in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>(b), if we rank documents using global similarities within the document manifold, all relevant documents can be ranked higher than all of the non-relevant documents. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186085/images/www2018-94-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Illustration of manifold-based fusion model. (a) Ranking generated by a nearest-neighbor-based technique. (b) Ideal ranking generated by a manifold-base technique.</span>     </div>     </figure>    </p>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Manifold-based fusion</h3>     </div>     </header>     <p>In this subsection, we propose our manifold-based data fusion algorithm, ManX, which integrates with a standard unsupervised data fusion method X such as CombSUM. Recall that we are given a set of ranked lists <span class="inline-equation"><span class="tex">$\mathcal {L}_1, \ldots , \mathcal {L}_m$</span>     </span>, returned in response to a query <em>q</em> by <em>m</em> retrieval systems. Our aim is to calculate a fusion score <em>f</em>(<em>d</em>; <em>q</em>) for each document <span class="inline-equation"><span class="tex">$d \in \mathcal {C}_\mathcal {L}$</span>     </span>, where <span class="inline-equation"><span class="tex">$\mathcal {C}_\mathcal {L}:=\bigcup _{i=1}^m \mathcal {L}_i$</span>     </span> is a set of documents appearing in the input result lists to be fused, and then rank these documents by their fusion scores to form a single fused result list <span class="inline-equation"><span class="tex">$\mathcal {L}_f$</span>     </span>.</p>     <p>Our first goal is to consider inter-document similarities of all documents in <span class="inline-equation"><span class="tex">$\mathcal {C}_\mathcal {L}$</span>     </span> for regularizing fusion scores <strong>f</strong>     <sub>X</sub> = [<em>f</em>     <sub>X</sub>(<em>d</em>     <sub>1</sub>; <em>q</em>), ..., <em>f</em>     <sub>X</sub>(<em>d<sub>n</sub>     </em>; <em>q</em>)], produced by an unsupervised data fusion method&#x00A0;X, like CombSUM (see (<a class="eqn" href="#eq1">1</a>)). Here, we let <em>n</em> denote the number of documents appearing in the input lists being fused, i.e., <span class="inline-equation"><span class="tex">$n=|\mathcal {C}_{\mathcal {L}}|$</span>     </span>. We define a adjacency matrix: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {W} \in \mathbb {R}^{n \times n} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> of inter-document similarities, where <em>W<sub>ij</sub>     </em> = sim(<em>d<sub>i</sub>     </em>, <em>d<sub>j</sub>     </em>) for all pairs of documents in <span class="inline-equation"><span class="tex">$\mathcal {C}_\mathcal {L}$</span>     </span> for <em>i</em> &#x2260; <em>j</em> and <em>W<sub>ii</sub>     </em> = 0 (required by all manifold models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>]). We compute sim(<em>d<sub>i</sub>     </em>, <em>d<sub>j</sub>     </em>) as: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} W_{ij} = \mathrm{sim}(d_i, d_j) = \exp \left\lbrace - \frac{1}{2}\left(\mathrm{KL}(\mathbf {d}_i \Vert \mathbf {d}_j) + \mathrm{KL}(\mathbf {d}_j \Vert \mathbf {d}_i) \right) \right\rbrace , \end{equation} </span>       <br />       <span class="equation-number">(3)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathbf {d}_i=[\theta _{d_{i1}}, \theta _{d_{i2}},\ldots , \theta _{d_{is}}]$</span>     </span> is a vector representation for document <em>d<sub>i</sub>     </em> with <span class="inline-equation"><span class="tex">$\theta _{d_{ij}}$</span>     </span> being the <em>j</em>-th word <em>v<sub>j</sub>     </em>&#x2019;s probability in <em>d</em> computed by an unsupervised language model&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>], <em>s</em> is the size of the vocabulary, and KL(&#x00B7; &#x2016; &#x00B7;) is the Kullback-Leibler divergence. We obtain <span class="inline-equation"><span class="tex">$\theta _{d_{ij}}$</span>     </span>, the element in the vector <strong>d</strong>     <sub>      <em>i</em>     </sub> of document <em>d</em>, by an unsupervised language model with Dirichlet smoothing as: <div class="table-responsive" id="Xeq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \theta _{d_{ij}} = \frac{c(v_j ; d) + \delta \cdot p(v_j \mid \mathcal {C})}{\sum _v c(v; d) + \delta }, \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <em>c</em>(<em>v</em>; <em>d</em>) is the total number of times the word <em>v</em> appearing in document <em>d</em>, <span class="inline-equation"><span class="tex">$p(v \mid \mathcal {C})$</span>     </span> is the probability of the word <em>v</em> appearing in the whole corpus, and <em>&#x03B4;</em> is the smoothing parameter that is set to the average length of the documents in the corpus&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>]. Then, according to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>], we can compute regularized scores <strong>f</strong>     <sub>ManX</sub> of our manifold-based fusion method, ManX, by minimizing the following objective function: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbf {f}^*_{\mathrm{ManX}} &#x0026; = \mathop{\arg \min }_{\mathbf {f}_{\mathrm{ManX}}} \mathcal {Q}(\mathbf {f}_{\mathrm{ManX}}) \nonumber \\ &#x0026; = \mathop{\arg \min }_{\mathbf {f}_{\mathrm{ManX}}} \frac{1}{2} \sum _{i,j=1}^n W_{ij} \left\Vert \frac{\mathbf {f}_{\mathrm{ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {f}_{\mathrm{ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 \nonumber \\ &#x0026; \hspace{10.11775pt} + \frac{1}{2} \mu \sum _{i=1}^n\left\Vert \mathbf {f}_{\mathrm{ManX}} - \mathbf {f}_{\mathrm{X}} \right\Vert ^2 , \end{align} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where <em>&#x03BC;</em> is a regularization parameter, <em>D<sub>ii</sub>     </em> is an element in the diagonal matrix <strong>D</strong> = diag(<em>D</em>     <sub>11</sub>, &#x2026;, <em>D<sub>nn</sub>     </em>) defined as <span class="inline-equation"><span class="tex">$D_{ii}=\sum _{j=1}^{n}W_{ij}$</span>     </span> (note that <em>W<sub>ij</sub>     </em> is an element in <strong>W</strong> computed by&#x00A0;(<a class="eqn" href="#eq3">3</a>)), and &#x2016; &#x00B7; &#x2016; is the 2-norm. The first component in the middle line in&#x00A0;(<a class="eqn" href="#eq4">5</a>) smoothes the fusion score vector <strong>f</strong>     <sub>ManX</sub> by assigning similar scores to similar documents. The second component ||<strong>f</strong>     <sub>ManX</sub> &#x2212; <strong>f</strong>     <sub>X</sub>||<sup>2</sup> forces the ManX fusion scores to be close to the original scores <strong>f</strong>     <sub>X</sub> obtained by an unsupervised fusion method such as CombSUM. The amount of regularization is controlled by the parameter&#x00A0;<em>&#x03BC;</em>. The final fused list <span class="inline-equation"><span class="tex">$\mathcal {L}_f$</span>     </span> is constructed by ranking documents <span class="inline-equation"><span class="tex">$d \in \mathcal {C_L}$</span>     </span> according to their regularized fusion scores <span class="inline-equation"><span class="tex">$f^*_{\mathrm{ManX}}(d; q)$</span>     </span>. The solution of the optimization problem&#x00A0;(<a class="eqn" href="#eq4">5</a>) can be found either iteratively or in closed form&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>]. The iterative solution is the following: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {f}_{\mathrm{ManX}} (t+1) = \alpha \mathbf {S} \mathbf {f}_{\mathrm{ManX}}(t) + (1-\alpha) \mathbf {f}_{\mathbf {X}}, \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <strong>f</strong>     <sub>ManX</sub>(<em>t</em>) is the vector of regularized fusion scores at iteration <em>t</em>, <em>&#x03B1;</em> = 1/(1 + <em>&#x03BC;</em>) and <strong>S</strong> = <strong>D</strong>     <sup>&#x2212; 1/2</sup>     <strong>W</strong>     <strong>D</strong>     <sup>&#x2212; 1/2</sup>. This can be rewritten as: <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \textstyle \mathbf {f}_{\mathrm{ManX}} (t+1) = (\alpha \mathbf {S})^{t} \mathbf {f_X} + (1-\alpha)\sum _{i=0}^t (\alpha \mathbf {S})^{i} \mathbf {f}_\mathrm{X}. \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> The time complexity of this iterative process is equal to the complexity of matrix multiplication, i.e., <em>O</em>(<em>n</em>     <sup>3</sup>). The closed form solution of&#x00A0;(<a class="eqn" href="#eq4">5</a>) can be written as follows: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {f^*_{\mathrm{ManX}}} = (1 - \alpha) (\mathbf {I}_n - \alpha \mathbf {S})^{-1}\mathbf {f}_\mathrm{X}, \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> where <strong>I</strong>     <sub>      <em>n</em>     </sub> is an <em>n</em> &#x00D7; <em>n</em> identity matrix. This means that in order to calculate regularized fusion scores in closed form, one needs to inverse the matrix <strong>I</strong> &#x2212; <em>&#x03B1;</em>     <strong>S</strong>, which also requires <em>O</em>(<em>n</em>     <sup>3</sup>) time. Thus, both the iterative and closed form approaches to computing regularized scores <strong>f</strong>     <sub>ManX</sub> have cubic complexity. To make our ManX technique applicable in practice, we develop an efficient version below.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Efficient manifold-based fusion</h3>     </div>     </header>     <p>We propose <em>a-ManX</em>, a revised ManX aggregation method that utilizes <em>anchors</em> for efficient improvement, to reduce the complexity of ManX. It differs from previous manifold-based algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>] in the way we design the adjacency matrix used in manifolds, the anchor definition and how we reduce the computational time. We first discuss the way anchor-documents can be chosen and used to represent all documents. We then show how the optimization problem&#x00A0;(<a class="eqn" href="#eq4">5</a>) and its optimal solution&#x00A0;(<a class="eqn" href="#eq7">8</a>) should be adjusted to anchors.</p>     <section id="sec-16">     <p><em>4.2.1 Defining anchors.</em> In a variety of real world information retrieval applications, including web search, users mainly pay attention to the top-<em>k</em> documents and ignore documents that are ranked low&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0036">36</a>]. Following this idea, a-ManX assumes the top-<em>k</em> documents (<em>k</em> &#x226A; <em>n</em>) produced by a basic unsupervised data fusion method&#x00A0;X to be relevant and considers them as anchors. We denote the (unknown) regularized fusion scores of these documents as <strong>a</strong>      <sub>a-ManX</sub> = [<em>f</em>      <sub>a-ManX</sub>(<em>a</em>      <sub>1</sub>; <em>q</em>), &#x2026;, <em>f</em>      <sub>a-ManX</sub>(<em>a<sub>k</sub>      </em>; <em>q</em>)].</p>     <p>Then, we represent regularized fusion score <em>f</em>      <sub>a-ManX</sub>(<em>d<sub>i</sub>      </em>; <em>q</em>) for document <em>d<sub>i</sub>      </em> as a linear combination of scores in <strong>a</strong>      <sub>a-ManX</sub>: <div class="table-responsive" id="Xeq2">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \textstyle f_\mathrm{a\text{-}ManX}(d_i;q) = \sum _{j = 1}^k Z_{ij} f_\mathrm{a\text{-}ManX}(a_j;q), \end{equation} </span>        <br/>        <span class="equation-number">(9)</span>       </div>      </div> where <em>k</em> is the total number of top-<em>k</em> documents acting as anchors, and <em>Z<sub>ij</sub>      </em> are the weights discussed below. In matrix form for all documents this can be written as follows: <div class="table-responsive" id="eq8">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathbf {f}_\mathrm{a\text{-}ManX} = \mathbf {Z} \mathbf {a}_\mathrm{a\text{-}ManX}, \end{equation} </span>        <br/>        <span class="equation-number">(10)</span>       </div>      </div> where a good design principle for the weight matrix <strong>Z</strong> is to have <span class="inline-equation"><span class="tex">$\sum _{j=1}^k Z_{ij}=1$</span>      </span> and <em>Z<sub>ij</sub>      </em> &#x2265; 0. Therefore, we define <em>Z<sub>ij</sub>      </em> as: <div class="table-responsive" id="eq9">       <div class="display-equation">        <span class="tex mytex">\begin{equation} Z_{ij}=\frac{\mathrm{sim}(d_i, a_j)}{\sum _{l=1}^k \mathrm{sim}(d_i, a_l)}. \end{equation} </span>        <br/>        <span class="equation-number">(11)</span>       </div>      </div> Hence, the more similar document <em>d<sub>i</sub>      </em> and anchor <em>a<sub>j</sub>      </em> are, the higher the weight <em>Z<sub>ij</sub>      </em>. Thus, documents similar to anchors will have higher regularized scores, which is a desired property as we assume anchor documents to be relevant.</p>     <p>We need to redefine the similarity matrix <strong>W</strong> and propose a new design based on the anchors for graph construction: <div class="table-responsive" id="eq10">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathbf {W}=\mathbf {Z} \mathbf {Z}^\top . \end{equation} </span>        <br/>        <span class="equation-number">(12)</span>       </div>      </div> According to this definition, two documents <em>d<sub>i</sub>      </em> and <em>d<sub>j</sub>      </em> have positive similarity <em>W<sub>ij</sub>      </em> > 0 if they share at least one anchor-document <em>d<sub>l</sub>      </em>: <span class="inline-equation"><span class="tex">$Z_{il} \ne 0\ \&#x0026;\ Z_{jl} \ne 0$</span>      </span>. The more anchors are shared, the more similar the documents are. Compared to the original adjacency matrix defined in&#x00A0;(<a class="eqn" href="#eq2">2</a>), where an <em>n</em> &#x00D7; <em>n</em> matrix <strong>W</strong> needs to be kept in memory, the adjacency matrix <strong>W</strong> in&#x00A0;(<a class="eqn" href="#eq10">12</a>) is scalable for ranking large datasets, as it only needs to save the <em>n</em> &#x00D7; <em>k</em> matrix <strong>Z</strong>.</p>     </section>     <section id="sec-17">     <p><em>4.2.2 Efficient optimal solution.</em> Instead of solving the optimization problem&#x00A0;(<a class="eqn" href="#eq4">5</a>) for all regularized scores <strong>f</strong>      <sub>a-ManX</sub>, we need to solve it only for <strong>a</strong>      <sub>a-ManX</sub>: <div class="table-responsive" id="eq11">       <div class="display-equation">        <span class="tex mytex">\begin{align} \mathbf {a}^*_\mathrm{a\text{-}ManX} &#x0026; = \mathop{\arg \min }_{\mathbf {a}_\mathrm{a\text{-}ManX}} \mathcal {Q}(\mathbf {a}_\mathrm{a\text{-}ManX}) \nonumber \\ &#x0026; = \mathop{\arg \min }_{\mathbf {a}_{\mathrm{a\text{-}ManX}}} \frac{1}{2} \sum _{i,j=1}^n W_{ij} \left\Vert \frac{\mathbf {Z}\mathbf {a}_{\mathrm{a\text{-}ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {Z}\mathbf {a}_{\mathrm{a\text{-}ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 \nonumber \\ &#x0026; \hspace{10.11775pt}+ \frac{1}{2} \mu \sum _{i=1}^n\left\Vert \mathbf {Z}\mathbf {a}_{\mathrm{a\text{-}ManX}} - \mathbf {f}_{\mathrm{X}} \right\Vert ^2 , \end{align} </span>        <br/>        <span class="equation-number">(13)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$D_{ii}=\sum _{j=1}^n W_{ij}$</span>      </span> and <strong>Z</strong>      <strong>a</strong>      <sub>a-ManX&#x2009;<em>i</em>      </sub> is the <em>i</em>-th element in <strong>Z</strong>      <strong>a</strong>      <sub>a-ManX</sub> = <strong>f</strong>      <sub>a-ManX</sub>, i.e.,&#x00A0;(<a class="eqn" href="#eq8">10</a>). Then, optimal regularized scores can be obtained as follows: <div class="table-responsive" id="Xeq3">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathbf {f}^*_\mathrm{a\text{-}ManX} = \mathbf {Z} \mathbf {a}^*_\mathrm{a\text{-}ManX}. \end{equation} </span>        <br/>        <span class="equation-number">(14)</span>       </div>      </div> Then, the optimal regularized fusion scores for anchor-documents can be calculated in closed form as follows: <div class="table-responsive" id="eq12">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathbf {f}^*_\mathrm{a\text{-}ManX} = \mathbf {Za}^*_{\mathrm{a\text{-}ManX}} = (\mathbf {I}_n - \alpha \mathbf {S})^{-1}\mathbf {f}_\mathrm{X}, \end{equation} </span>        <br/>        <span class="equation-number">(15)</span>       </div>      </div> where <strong>S</strong> = <strong>D</strong>      <sup>&#x2212; 1/2</sup>      <strong>W</strong>      <strong>D</strong>      <sup>&#x2212; 1/2</sup> = <strong>D</strong>      <sup>&#x2212; 1/2</sup>      <strong>ZZ<sup>&#x22A4;</sup>      </strong>      <strong>D</strong>      <sup>&#x2212; 1/2</sup>. Note that here we drop the constant (1 &#x2212; <em>&#x03B1;</em>), because it does not affect the final ranking of documents (see&#x00A0;(<a class="eqn" href="#eq7">8</a>)).</p>     <p>The matrix <strong>I</strong>      <sub>       <em>n</em>      </sub> &#x2212; <em>&#x03B1;</em>      <strong>S</strong> still has dimensions <em>n</em> &#x00D7; <em>n</em>. However, if we set <span class="inline-equation"><span class="tex">$\mathbf {P} = \mathbf {Z}^\top \mathbf {D}^{-\frac{1}{2}}$</span>      </span>, then we can rewrite&#x00A0;(<a class="eqn" href="#eq12">15</a>) as follows: <div class="table-responsive" id="eq13">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathbf {f}^*_\mathrm{a\text{-}ManX} = (\mathbf {I}_n - \mathbf {P}^\top (\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha } \mathbf {I}_k)^{-1} \mathbf {P})\mathbf {f}_\mathrm{X}, \end{equation} </span>        <br/>        <span class="equation-number">(16)</span>       </div>      </div> where matrix <span class="inline-equation"><span class="tex">$\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha } \mathbf {I}_k$</span>      </span> has dimensions <em>k</em> &#x00D7; <em>k</em> and, thus, requires only <em>O</em>(<em>k</em>      <sup>3</sup>) rather than <em>O</em>(<em>n</em>      <sup>3</sup>) to calculate the inverse. &#x00A0;(<a class="eqn" href="#eq12">15</a>) and&#x00A0;(<a class="eqn" href="#eq13">16</a>) are equivalent, as when we multiply matrix <strong>I</strong>      <sub>       <em>n</em>      </sub> &#x2212; <em>&#x03B1;</em>      <strong>S</strong> (not inverse) from&#x00A0;(<a class="eqn" href="#eq12">15</a>) by the matrix from&#x00A0;(<a class="eqn" href="#eq13">16</a>), we get the identity matrix <strong>I</strong>      <sub>       <em>n</em>      </sub>. The proof that&#x00A0;(<a class="eqn" href="#eq12">15</a>) and&#x00A0;(<a class="eqn" href="#eq13">16</a>) are equivalent is included in Appendix&#x00A0;<a class="sec" href="#sec-33">A</a>. In fact, in&#x00A0;(<a class="eqn" href="#eq13">16</a>) for efficient computations, we do not need to save the newly designed adjacency matrix <strong>W</strong> in memory but only the matrix <strong>Z</strong>. For the diagonal matrix <strong>D</strong> with <span class="inline-equation"><span class="tex">$D_{ii}=\sum _{j=1}^{n} W_{ij}$</span>      </span> used in&#x00A0;(<a class="eqn" href="#eq13">16</a>), we obtain <strong>D</strong> without using <strong>W</strong> as well, as <strong>W</strong> = <strong>Z</strong>      <sup>&#x22A4;</sup>      <strong>Z</strong> and <span class="inline-equation"><span class="tex">$D_{ii}=\sum _{j=1}^n \mathbf {z}_i^\top \mathbf {z}_j$</span>      </span>, where <strong>z</strong>      <sub>       <em>i</em>      </sub> is the <em>i</em>-th column vector of matrix <strong>Z</strong>.</p>     </section>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Virtual manifold learning</h3>     </div>     </header>     <p>The manifold-based fusion model proposed in&#x00A0;&#x00A7;<a class="sec" href="#sec-14">4.1</a> lacks the ability to rank adversarial examples correctly. In this subsection, we propose a <em>virtual</em> manifold learning algorithm, v-ManX, that trains manifolds to correctly rank both unmodified (original) and virtual adversarial documents. It improves not only robustness to adversarial documents, but also generalization performance of original documents.</p>     <p>For each original document <span class="inline-equation"><span class="tex">$d_i\in \mathcal {C}_{\mathcal {L}}$</span>     </span>, we find its virtual adversarial document <span class="inline-equation"><span class="tex">$d_i^\mathrm{v}$</span>     </span>. Let&#x0027;s denote the vector representation of <span class="inline-equation"><span class="tex">$d_i^\mathrm{v}$</span>     </span> as <span class="inline-equation"><span class="tex">$\mathbf {d}_i^\mathrm{v}$</span>     </span> and let <span class="inline-equation"><span class="tex">$\mathbf {d}_i^\mathrm{v} = \mathbf {d}_i+\mathbf {r}_{i}$</span>     </span>, which is built based on original document <em>d</em>&#x2019;s vector representation <strong>d</strong>     <sub>      <em>i</em>     </sub> and its virtual adversarial perturbation <strong>r</strong>     <sub>      <em>i</em>     </sub>. Here <strong>r</strong>     <sub>      <em>i</em>     </sub> is the virtual adversarial perturbation making to the original document <em>d<sub>i</sub>     </em>. We propose to obtain <strong>r</strong>     <sub>      <em>i</em>     </sub> as: <div class="table-responsive" id="eq14">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbf {r}^*_i = &#x0026; \mathop{\arg \min }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \mathrm{sim}(\mathbf {d}_i + \mathbf {r}_i, \mathbf {d}_j) \\= &#x0026; \mathop{\arg \min }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \exp \left\lbrace -\frac{1}{2} \left(\mathrm{KL}(\mathbf {d}_i + \mathbf {r}_{i} \Vert \mathbf {d}_j) + \mathrm{KL}(\mathbf {d}_j \Vert \mathbf {d}_i + \mathbf {r}_i) \right) \right\rbrace , \nonumber\end{align} </span>       <br/>       <span class="equation-number">(17)</span>      </div>     </div> where &#x03F5; > 0 is a constant parameter that controls the amount of the perturbation. The motivation behind&#x00A0;(<a class="eqn" href="#eq14">17</a>) is that the virtual adversarial document needs to be as far as possible from any of the original documents while making the perturbation such that it can significantly increase the loss incurred by our manifold learning model (see below), and thus improves the robustness of the model. Given a set of original documents <span class="inline-equation"><span class="tex">$\mathcal {C}_{\mathcal {L}} = \lbrace d_j \rbrace _{j=1}^n$</span>     </span> being fused, we can obtain the optimal <span class="inline-equation"><span class="tex">$\mathbf {r}^*_i$</span>     </span> for each document <em>d<sub>i</sub>     </em> so as to obtain <span class="inline-equation"><span class="tex">$\mathbf {d}_i^\mathrm{v}$</span>     </span> by <em>globally</em> considering all other documents as: <div class="table-responsive" id="Xeq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {r}^*_i = \epsilon \times \overline{n\mathbf {d}_i - \sum _{j=1}^n \mathbf {d}_j}, \end{equation} </span>       <br/>       <span class="equation-number">(18)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\overline{\mathbf {x}}$</span>     </span> denotes an operator acting on an arbitrary non-zero vector <strong>x</strong> that returns a unit vector in the direction of <strong>x</strong>. The time complexity of computing <span class="inline-equation"><span class="tex">$\mathbf {r}^*_{i}$</span>     </span> is <em>O</em>(<em>n</em>) for each document, which is faster than any of the state-of-the-art algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>]: their time complexity is at least <em>O</em>(<em>n</em>     <sup>2</sup>) and they approximately generate <em>local</em> virtual adversarial data, i.e., obtaining the perturbation by considering the example itself but not the others. See Appendix&#x00A0;<a class="sec" href="#sec-34">B</a> for our derivation of getting <span class="inline-equation"><span class="tex">$\mathbf {r}^*_i$</span>     </span>.</p>     <p>After obtaining a virtual adversarial perturbation for each original document, in total we double the number of documents for manifold learning. Thus, we have more documents and, hence, more information to regularize fusion scores in our virtual manifold model. The document set in manifold learning becomes <span class="inline-equation"><span class="tex">$\lbrace d_1, \ldots , d_n, d_1^\mathrm{v}, \ldots ,$</span>     </span>      <span class="inline-equation"><span class="tex">$d_n^\mathrm{v}\rbrace$</span>     </span>. Let <strong>f</strong>     <sub>v-ManX</sub> be the regularized score for both original documents and virtual adversarial documents with <strong>f</strong>     <sub>v-ManX&#x2009;<em>i</em>     </sub> being the regularized score shared by both <em>d<sub>i</sub>     </em> and its virtual adversarial document <span class="inline-equation"><span class="tex">$d_i^{\mathrm{v}}$</span>     </span>. Note that the size of <strong>f</strong>     <sub>v-ManX</sub> is still <em>n</em>, as each &#x201C;virtual and original document pair&#x201D; needs to share the same regularized score. Compared to&#x00A0;(<a class="eqn" href="#eq2">2</a>), the adjacency matrix (in <span class="inline-equation"><span class="tex">$\mathbb {R}^{2n \times 2n}$</span>     </span>) becomes: <div class="table-responsive" id="eq15">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} {\mathbf {W} = {\begin{array}{cc}&#x0026; {\begin{array}{@{}ccccccc@{}}&#x0026; d_1 &#x0026;\,\,\,\,\,\,\cdots &#x0026;\,\,\,\,\,\,\,\,\,d_n &#x0026;\,\,\,\,\,\,\,\,\,\,\,\,d_1^\mathrm{v} &#x0026;\,\,\,\,\,\,\,\cdots &#x0026;\,\,\,\,\,\,\,d_n^\mathrm{v}\,\,\,\,\\\end{array}} \nonumber \\[2pt] {\begin{array}{c}d_1\\[2pt] \vdots \\[2pt] d_n \\[2pt] d_1^\mathrm{v} \\[2pt] \vdots \\[2pt] d_n^\mathrm{v}\\\end{array}} &#x0026; \left[ \begin{array}{@{}c@{\quad }c@{\quad }c@{\quad }c@{\quad }c@{\quad }cc@{}}W_{11} &#x0026; \cdots &#x0026; W_{1n} &#x0026; W_{1\, n+1} &#x0026; \cdots &#x0026; W_{1\, 2n} \\[2pt] \vdots &#x0026; \ddots &#x0026; \vdots &#x0026; \vdots &#x0026; \cdots &#x0026; \vdots \\[2pt] W_{n1} &#x0026; \cdots &#x0026; W_{nn} &#x0026; W_{n\, n+1} &#x0026; \cdots &#x0026; W_{n\, 2n} \\[2pt] W_{n+1\,1} &#x0026; \cdots &#x0026; W_{n+1\, n} &#x0026; W_{n+1\, n+1} &#x0026; \cdots &#x0026; W_{n+1\, 2n} \\[2pt] \vdots &#x0026; \ddots &#x0026; \vdots &#x0026; \vdots &#x0026; \cdots &#x0026; \vdots \\[2pt] W_{2n\, 1} &#x0026; \cdots &#x0026; W_{2n\, n} &#x0026; W_{2n\, n+1} &#x0026; \cdots &#x0026; W_{2n\, 2n} \\\end{array}\right] \end{array}}.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(19)</span>      </div>     </div> To simplify the discussion, we write <span class="inline-equation"><span class="tex">$\mathbf {W}={\left[\begin{array}{*10c}\mathbf {W}_{11} &#x0026; \mathbf {W}_{12} \\ \mathbf {W}_{21} &#x0026; \mathbf {W}_{22}\end{array}\right]}$</span>     </span>, where <strong>W</strong>     <sub>11</sub>, <strong>W</strong>     <sub>12</sub>, <strong>W</strong>     <sub>21</sub> and <strong>W</strong>     <sub>22</sub> are the corresponding <em>n</em> &#x00D7; <em>n</em> sub-matrixes in <strong>W</strong>. We write <strong>D</strong> for the diagonal matrix for <strong>W</strong>: <span class="inline-equation"><span class="tex">$\mathbf {D}={\left[\begin{array}{*10c}\mathbf {D}_1 \\ \mathbf {D}_2 \end{array}\right]}$</span>     </span>, where <strong>D</strong>     <sub>1</sub> = diag(<em>D</em>     <sub>11</sub>, <em>D</em>     <sub>22</sub>, &#x2026;, <em>D<sub>nn</sub>     </em>) and <strong>D</strong>     <sub>2</sub> = diag(<em>D</em>     <sub>      <em>n</em> + 1&#x2009;<em>n</em> + 1</sub>, <em>D</em>     <sub>      <em>n</em> + 2&#x2009;<em>n</em> + 2</sub>, ..., <em>D</em>     <sub>2<em>n</em>&#x2009;2<em>n</em>     </sub>). In our proposed virtual adversarial manifold learning based aggregation method, v-ManX, we obtain the optimal <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{v\text{-}ManX}}$</span>     </span> for aggregation by minimizing the following objective function: <div class="table-responsive" id="eq16">      <div class="display-equation">       <span class="tex mytex">\begin{align} {\mathbf {f}^*_{\mathrm{v\text{-}ManX}} = \mathop{\arg \min }_{\mathbf {f}_{\mathrm{v\text{-}ManX}}} \mathcal {Q}(\mathbf {f}_{\mathrm{v\text{-}ManX}}) = {}}\\&#x0026;\frac{1}{4} \sum _{i,j=1}^{2n} W_{ij} \left\Vert \frac{\mathbf {c}_{\mathrm{v\text{-}ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {c}_{\mathrm{v\text{-}ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 + \frac{\mu }{4} \sum _{i=1}^{2n}\Vert \mathbf {c}_{\mathrm{v\text{-}ManX}\, i} - \mathbf {h}_{\mathrm{X}\, i} \Vert ^2, \nonumber\end{align} </span>       <br/>       <span class="equation-number">(20)</span>      </div>     </div> where <strong>c</strong>     <sub>v-ManX</sub> = <strong>f</strong>     <sub>v-ManX</sub>&#x2295;<strong>f</strong>     <sub>v-ManX</sub> and <strong>h</strong>     <sub>X</sub> = <strong>f</strong>     <sub>X</sub>&#x2295;<strong>f</strong>     <sub>X</sub> (<strong>h</strong>     <sub>X</sub> can be obtained in an unsupervised way, as <strong>f</strong>     <sub>X</sub> can be obtained by an unsupervised fusion method like CombSUM), and &#x2295; is the concatenation operator for two vectors; <strong>c</strong>     <sub>v-ManX&#x2009;<em>i</em>     </sub> and <strong>h</strong>     <sub>X&#x2009;<em>i</em>     </sub> are the <em>i</em>-th element in <strong>c</strong>     <sub>v-ManX</sub> and <strong>h</strong>     <sub>X</sub>, respectively. Unlike&#x00A0;(<a class="eqn" href="#eq4">5</a>), which smoothes original documents only, (<a class="eqn" href="#eq16">20</a>) smoothes both original and virtual adversarial ones &#x2013; between original documents themselves (<strong>W</strong>     <sub>11</sub>), between virtual adversarial documents themselves (<strong>W</strong>     <sub>22</sub>), between original and virtual adversarial documents (<strong>W</strong>     <sub>12</sub> and <strong>W</strong>     <sub>21</sub>), by the first term on the right-hand side of&#x00A0;(<a class="eqn" href="#eq16">20</a>), while still forcing the v-Manx fusion scores to be close to the original scores <strong>f</strong>     <sub>X</sub> by the last term in&#x00A0;(<a class="eqn" href="#eq16">20</a>). The closed form solution of&#x00A0;(<a class="eqn" href="#eq16">20</a>) is <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{v\text{-}ManX}} ={}$</span>     </span>      <div class="table-responsive" id="eq17">      <div class="display-equation">       <span class="tex mytex">\begin{equation} (1 - \alpha) \left(\mathbf {I} - \alpha \left(\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22} \right) \right)^{-1} \mathbf {f}_{\mathrm{X}}, \end{equation} </span>       <br/>       <span class="equation-number">(21)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathbf {S}_{11} = \mathbf {D}_1^{-1/2} \mathbf {W}_{11} \mathbf {D}_1^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{12} = \mathbf {D}_1^{-1/2} \mathbf {W}_{12} \mathbf {D}_1^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{21} = \mathbf {D}_2^{-1/2}$</span>     </span>      <span class="inline-equation"><span class="tex">$\mathbf {W}_{21} \mathbf {D}_2^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{22} = \mathbf {D}_2^{-1/2} \mathbf {W}_{22} \mathbf {D}_2^{-1/2}$</span>     </span>, and again <span class="inline-equation"><span class="tex">$\alpha =\frac{\mu }{1 + \mu }$</span>     </span>. The derivation of the closed form solution in&#x00A0;(<a class="eqn" href="#eq17">21</a>) is included in Appendix&#x00A0;<a class="sec" href="#sec-35">C</a>.</p>     <p>The iterative solution of&#x00A0;(<a class="eqn" href="#eq16">20</a>) is <strong>f</strong>     <sub>v-ManX</sub>(<em>t</em> + 1) = <div class="table-responsive" id="eq18">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \alpha \left(\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22} \right) \mathbf {f}_{\mathrm{v\text{-}ManX}}(t) + (1 - \alpha) \mathbf {f}_{\mathrm{X}}, \end{equation} </span>       <br/>       <span class="equation-number">(22)</span>      </div>     </div> where <strong>f</strong>     <sub>v-ManX</sub>(<em>t</em>) is the vector of regularized fusion scores at iteration <em>t</em>. After <em>t</em> &#x2192; +&#x221E; iterations, the optimal <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{v\text{-}ManX}}$</span>     </span> can be set to be <strong>f</strong>     <sub>v-ManX</sub>(+ &#x221E;) and is the closed form solution, i.e.,&#x00A0;(<a class="eqn" href="#eq17">21</a>). See Appendix&#x00A0;<a class="sec" href="#sec-36">D</a> for the proof that&#x00A0;(<a class="eqn" href="#eq18">22</a>) is equivalent to&#x00A0;(<a class="eqn" href="#eq17">21</a>) when <em>t</em> &#x2192; +&#x221E;. The time complexity of both&#x00A0;(<a class="eqn" href="#eq17">21</a>) and&#x00A0;(<a class="eqn" href="#eq18">22</a>) is <em>O</em>(<em>n</em>     <sup>3</sup>).</p>    </section>    <section id="sec-19">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Efficient virtual manifold learning</h3>     </div>     </header>     <p>Similar to &#x00A7;<a class="sec" href="#sec-15">4.2</a>, we boost the efficiency of our virtual manifold learning algorithm proposed in&#x00A0;&#x00A7;<a class="sec" href="#sec-18">4.3</a> to arrive at a method called a-v-ManX, a revised v-ManX utilizing anchors for efficiency. As before, we denote the (unknown) regularized fusion scores of the top-<em>k</em> documents as <strong>a</strong>     <sub>a-v-ManX</sub> = [<em>f</em>     <sub>a-v-ManX</sub>(<em>a</em>     <sub>1</sub>; <em>q</em>), &#x2026;, <em>f</em>     <sub>a-v-ManX</sub>(<em>a<sub>k</sub>     </em>; <em>q</em>)]. Then, we represent the regularized fusion score <em>f</em>     <sub>a-v-ManX</sub>(<em>d<sub>i</sub>     </em>; <em>q</em>) for both document <em>d<sub>i</sub>     </em> and its virtual document <span class="inline-equation"><span class="tex">$d_i^\mathrm{v}$</span>     </span> as a linear combination of scores in <strong>a</strong>     <sub>a-v-ManX</sub>: <div class="table-responsive" id="Xeq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \textstyle f_\mathrm{a\text{-}v\text{-}ManX}(d_i;q) = \sum _{j = 1}^k Z_{ij} f_\mathrm{a\text{-}v\text{-}ManX}(a_j;q), \end{equation} </span>       <br/>       <span class="equation-number">(23)</span>      </div>     </div> where <em>Z<sub>ij</sub>     </em> is the weight computed by&#x00A0;(<a class="eqn" href="#eq9">11</a>).</p>     <p>Instead of solving the optimization problem&#x00A0;(<a class="eqn" href="#eq16">20</a>) for all regularized scores <strong>f</strong>     <sub>a-v-ManX</sub>, we need to solve it only for <strong>a</strong>     <sub>a-v-ManX</sub>: <div class="table-responsive" id="eq19">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbf {a}^*_\mathrm{a\text{-}v\text{-}ManX} &#x0026; = \mathop{\arg \min }_{\mathbf {a}_\mathrm{a\text{-}v\text{-}ManX}} \mathcal {Q}(\mathbf {a}_\mathrm{a\text{-}v\text{-}ManX}) \nonumber \\ &#x0026; \hspace{-13.00806pt} = \mathop{\arg \min }_{\mathbf {a}_{\mathrm{a\text{-}v\text{-}ManX}}} \frac{1}{4} \sum _{i,j=1}^n W_{ij} \left\Vert \frac{\mathbf {Z}_{11}\mathbf {a}_{\mathrm{a\text{-}ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {Z}_{11}\mathbf {a}_{\mathrm{a\text{-}ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 + \nonumber \\ &#x0026; \hspace{-3.61371pt} \frac{1}{4} \sum _{i=1}^n \sum _{j=n+1}^{2n} W_{ij} \left\Vert \frac{\mathbf {Z}_{12}\mathbf {a}_{\mathrm{a\text{-}ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {Z}_{12}\mathbf {a}_{\mathrm{a\text{-}ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 + \nonumber \\ &#x0026; \hspace{-3.61371pt} \frac{1}{4} \sum _{i=n+1}^{2n} \sum _{j=1}^{n} W_{ij} \left\Vert \frac{\mathbf {Z}_{21}\mathbf {a}_{\mathrm{a\text{-}ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {Z}_{21}\mathbf {a}_{\mathrm{a\text{-}ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 + \nonumber \\ &#x0026; \hspace{-3.61371pt} \frac{1}{4} \sum _{i=n+1}^{2n} \sum _{j=n+1}^{2n} W_{ij} \left\Vert \frac{\mathbf {Z}_{22}\mathbf {a}_{\mathrm{a\text{-}ManX}\, i}}{\sqrt {D_{ii}}} - \frac{\mathbf {Z}_{22}\mathbf {a}_{\mathrm{a\text{-}ManX}\, j}}{\sqrt {D_{jj}}} \right\Vert ^2 + \nonumber \\ &#x0026; \hspace{-3.61371pt} \frac{\mu }{4} \sum _{i=1}^{2n} \left\Vert \mathbf {c}_{\mathrm{a\text{-}v\text{-}ManX}\, i} - \mathbf {h}_{\mathrm{X}\, i}\right\Vert ^2 . \end{align} </span>       <br/>       <span class="equation-number">(24)</span>      </div>     </div> Here, <strong>c</strong>     <sub>a-v-ManX</sub> = <strong>f</strong>     <sub>a-v-ManX</sub>&#x2295;<strong>f</strong>     <sub>a-v-ManX</sub>, <span class="inline-equation"><span class="tex">$\mathbf {Z}={\left[\begin{array}{*10c}\mathbf {Z}_{11} &#x0026; \mathbf {Z}_{12} \\ \mathbf {Z}_{21} &#x0026; \mathbf {Z}_{22}\end{array}\right]}$</span>     </span> with each sub-matrix of size <em>n</em> &#x00D7; <em>n</em> and the element <em>Z<sub>ij</sub>     </em> computed by&#x00A0;(<a class="eqn" href="#eq9">11</a>), and <span class="inline-equation"><span class="tex">$\mathbf {W}={\left[\begin{array}{*10c}\mathbf {W}_{11} &#x0026; \mathbf {W}_{12} \\ \mathbf {W}_{21} &#x0026; \mathbf {W}_{22}\end{array}\right]} = {\left[\begin{array}{*10c}\mathbf {Z}_{11} \mathbf {Z}_{11}^\top &#x0026; \mathbf {Z}_{12} \mathbf {Z}_{12}^\top \\ \mathbf {Z}_{21} \mathbf {Z}_{21}^\top &#x0026; \mathbf {Z}_{22} \mathbf {Z}_{22}^\top \end{array}\right]}$</span>     </span>.</p>     <p>Similar to&#x00A0;(<a class="eqn" href="#eq17">21</a>), the closed form solution of&#x00A0;(<a class="eqn" href="#eq19">24</a>) is <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{a\text{-}v\text{-}ManX}} ={}$</span>     </span>      <div class="table-responsive" id="eq20">      <div class="display-equation">       <span class="tex mytex">\begin{equation} (1 - \alpha) \left(\mathbf {I} - \alpha \left(\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22} \right) \right)^{-1} \mathbf {f}_{\mathrm{X}}, \end{equation} </span>       <br/>       <span class="equation-number">(25)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\mathbf {S}_{11} = \mathbf {D}_1^{-1/2} \mathbf {Z}_{11} \mathbf {Z}_{11}^\top \mathbf {D}_1^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{12} = \mathbf {D}_1^{-1/2} \mathbf {Z}_{12} \mathbf {Z}_{12}^\top \mathbf {D}_1^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{21} = \mathbf {D}_2^{-1/2} \mathbf {Z}_{21} \mathbf {Z}_{21}^\top \mathbf {D}_2^{-1/2}, \mathbf {S}_{22} = \mathbf {D}_2^{-1/2} \mathbf {Z}_{22} \mathbf {Z}_{22}^\top \mathbf {D}_2^{-1/2}$</span>     </span>. Again, if we set <span class="inline-equation"><span class="tex">$\mathbf {P}_{11} = \mathbf {Z}_{11}^\top \mathbf {D}_1^{-\frac{1}{2}}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {P}_{12} = \mathbf {Z}_{12}^\top \mathbf {D}_1^{-\frac{1}{2}}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {P}_{21} = \mathbf {Z}_{21}^\top \mathbf {D}_2^{-\frac{1}{2}}$</span>     </span>, and <span class="inline-equation"><span class="tex">$\mathbf {P}_{22} = \mathbf {Z}_{22}^\top \mathbf {D}_2^{-\frac{1}{2}}$</span>     </span>, the time complexity of&#x00A0;(<a class="eqn" href="#eq20">25</a>) will reduce from <em>O</em>(<em>n</em>     <sup>3</sup>) to <em>O</em>(<em>k</em>     <sup>3</sup>) (<em>k</em> &#x226A; <em>n</em>), which makes a-v-ManX more efficient than v-ManX.</p>    </section>   </section>   <section id="sec-20">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Experimental Setup</h2>     </div>    </header>    <section id="sec-21">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Research questions</h3>     </div>     </header>     <p>The main research questions guiding the paper are:</p>     <ul class="list-no-style">     <li id="uid42" label="RQ1">Do manifold-based fusion methods outperform state-of-the-art methods?<br/></li>     <li id="uid43" label="RQ2">Does adding virtual adversarial perturbation into manifold learning methods improve performance?<br/></li>     <li id="uid44" label="RQ3">Does our way of globally generating virtual adversarial perturbation outperform the local ways in rank aggregation?<br/></li>     <li id="uid45" label="RQ4">Do efficient manifold-based fusion methods, a-ManX and a-v-ManX, run faster? Does our way of generating virtual adversarial documents run faster than the state-of-the-art?<br/></li>     <li id="uid46" label="RQ5">What is the impact of the number of anchors used in efficient manifold-based methods?<br/></li>     <li id="uid47" label="RQ6">What is the effect of the number of lists to be fused on manifold-based methods compared to state-of-the-art methods?<br/></li>     </ul>    </section>    <section id="sec-22">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Datasets</h3>     </div>     </header>     <p>In order to evaluate the proposed manifold-based data fusion methods and answer the research questions stated in &#x00A7;&#x00A0;<a class="sec" href="#sec-21">5.1</a>, we need a set of queries and a number of ranked lists of documents for each query. To this end, We work with three publicly available large document datasets from the Text REtrieval Conference (TREC).<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> We use the titles of TREC topics as queries (150 queries in total) and the runs submitted by participants as ranked lists of documents to be fused. We focus on the ad hoc track at TREC-3&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>], web track at TREC-10&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] and robust retrieval track at TREC-12&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>]. Table&#x00A0;<a class="tbl" href="#tab1">1</a> summarizes the key statistics of the datasets. Participants produced 40, 97 and 78 runs for the tracks of TREC-3, TREC-10 and TREC-12, respectively. The precision at rank 20 (p@20) is the official evaluation metric of these three tracks. The precision at rank 20 of the submitted runs varies dramatically; see Table&#x00A0;<a class="tbl" href="#tab1">1</a>.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Summary of the datasets used for our experiments.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">Track dataset</th>        <th style="text-align:center;">#queries</th>        <th style="text-align:right;">#documents</th>        <th style="text-align:center;">#runs</th>        <th style="text-align:center;">p@20</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">TREC-3 ad hoc</td>        <td style="text-align:center;">50</td>        <td style="text-align:right;">741,856</td>        <td style="text-align:center;">40</td>        <td style="text-align:center;">0.062&#x2013;0.674</td>       </tr>       <tr>        <td style="text-align:left;">TREC-10 web</td>        <td style="text-align:center;">50</td>        <td style="text-align:right;">1,692,096</td>        <td style="text-align:center;">97</td>        <td style="text-align:center;">0.001&#x2013;0.473</td>       </tr>       <tr>        <td style="text-align:left;">TREC-12 robust retrieval</td>        <td style="text-align:center;">50</td>        <td style="text-align:right;">528,155</td>        <td style="text-align:center;">78</td>        <td style="text-align:center;">0.090&#x2013;0.393</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-23">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Baselines and evaluation measures</h3>     </div>     </header>     <p>We include comparisons among the following algorithms:</p>     <p>     <strong>Standard unsupervised fusion methods:</strong> CombSUM and CombMNZ&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>].</p>     <p>     <strong>Learning-to-rank method:</strong>      <em>&#x03BB;</em>-Merge&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>] &#x2013; a supervised, state-of-the-art learning-to-rank-based rank aggregation method.</p>     <p>     <strong>Clustering-based fusion methods:</strong> ClustX&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] and a-ClustX that build on a standard fusion method, i.e., either X = CombSUM or X = CombMNZ. Here, ClustX is the original supervised clustering-based fusion method that creates clusters for each document, and a-ClustX is an efficient version of ClustX applying our efficiency framework from&#x00A0;&#x00A7;<a class="sec" href="#sec-15">4.2</a>.</p>     <p>     <strong>Virtual adversarial learning methods:</strong> v-LDSX&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>] (Local Distributional Smoothness), a-v-LDSX and our v-ManX (&#x00A7;<a class="sec" href="#sec-18">4.3</a>) and a-v-ManX (&#x00A7;<a class="sec" href="#sec-19">4.4</a>). The only difference between v-LDSX and our v-ManX (&#x00A7;<a class="sec" href="#sec-18">4.3</a>) is the way they generate the virtual adversarial perturbation. LDSX and a-v-LDSX use an iteration and finite difference method to generate local virtual adversarial perturbations.</p>     <p>     <strong>Efficient fusion methods:</strong> a-ClustX, our a-ManX (&#x00A7;<a class="sec" href="#sec-15">4.2</a>) and a-v-ManX (&#x00A7;<a class="sec" href="#sec-19">4.4</a>). a-ClustX is an efficient version that utilizes our proposed efficient fusion framework in&#x00A0;&#x00A7;<a class="sec" href="#sec-15">4.2</a>.</p>     <p>     <strong>Manifold-based fusion methods:</strong> ManX (&#x00A7;<a class="sec" href="#sec-14">4.1</a>), a-ManX (&#x00A7;<a class="sec" href="#sec-15">4.2</a>), v-ManX (&#x00A7;<a class="sec" href="#sec-18">4.3</a>), a-v-ManX (&#x00A7;<a class="sec" href="#sec-19">4.4</a>), v-LDSX, and a-v-LDSX.</p>     <p>For convenience, we write <em>M</em>SUM for <em>M</em>CombSUM, and <em>M</em>MNZ for <em>M</em>CombSUM, respectively, where <em>M</em> = {Man, a-Man, v-Man, a-v-Man, Clust, a-Clust}. For instance, a-ManSUM is the abbreviation for a-ManCombSUM when <em>M</em> = a-Man. To measure performance, we use MAP&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>], p@<em>k</em> (precision@<em>k</em>) and nDCG@<em>k</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], <em>k</em> = {5, 10, 20}, all of which are the official metrics in these three TREC tracks.</p>    </section>    <section id="sec-24">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Parameters and settings</h3>     </div>     </header>     <p>For fusion methods <em>&#x03BB;</em>-Merge, ManX, a-ManX, v-ManX, a-v-ManX, LDSX, a-LDSX, ClustX, and a-ClustX, we randomly split queries into three parts: 70% queries are used to train a fusion model, 20% queries are used to validate the model during training, and 10% queries are used for testing the model. We train a fusion model by varying the values of its parameters and then choose the best values during validation. The training, validation, test splits are permuted until all queries were chosen once for the test set. Statistical significance of observed differences between two results is tested using a two-tailed paired t-test and is denoted using <sup>&#x25B2;</sup> (or <sup>&#x25BC;</sup>) for significant differences for <em>&#x03B1;</em> = .01, or (and <sup>&#x25BD;</sup>) for <em>&#x03B1;</em> = .05.</p>    </section>   </section>   <section id="sec-25">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Results and Analysis</h2>     </div>    </header>    <section id="sec-26">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Effectiveness of proposed methods</h3>     </div>     </header>     <p>     <strong>RQ1:</strong> We compare the ranking performance of all of our manifold learning algorithms to that of the baselines. We use these algorithms to aggregate the top-5 best retrieval runs in each TREC dataset.</p>     <p>Tables&#x00A0;<a class="tbl" href="#tab2">2</a>,&#x00A0;<a class="tbl" href="#tab3">3</a> and&#x00A0;<a class="tbl" href="#tab4">4</a> show the results; we also show the performance of the best run in each TREC dataset, i.e., runs inq102, iit01m, and pricRBa2, respectively.<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>     </p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Performance of the methods and the best run on the TREC-3 dataset. The best performance per metric is given in bold. Statistically significant differences between ManX and ClustX, between ManX and a-ManX are marked in the upper and lower right hand corner of ManX&#x0027;s score, respectively. Statistically significant differences between any virtual adversarial learning method and ManX are marked in the upper right hand corner of the virtual adversarial learning method. Statistically significant differences between v-LDSX and a-v-LDSX, between v-ManX and a-v-ManX, are marked in the lower right hand corner of v-LDSX and v-ManX, respectively.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;"/>        <th style="text-align:center;" colspan="3">p@        </th>        <th style="text-align:center;" colspan="3">nDCG@        </th>        <th/>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;"/>        <th style="text-align:center;">MAP</th>        <th style="text-align:center;">5</th>        <th style="text-align:center;">10</th>        <th style="text-align:center;">20</th>        <th style="text-align:center;">5</th>        <th style="text-align:center;">10</th>        <th style="text-align:center;">20</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">inq102</td>        <td style="text-align:center;">.1039</td>        <td style="text-align:center;">.7440</td>        <td style="text-align:center;">.7220</td>        <td style="text-align:center;">.6740</td>        <td style="text-align:center;">.7423</td>        <td style="text-align:center;">.7275</td>        <td style="text-align:center;">.6925</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">CombSUM</td>        <td style="text-align:center;">.1073</td>        <td style="text-align:center;">.8040</td>        <td style="text-align:center;">.7620</td>        <td style="text-align:center;">.6960</td>        <td style="text-align:center;">.8009</td>        <td style="text-align:center;">.7736</td>        <td style="text-align:center;">.7245</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>&#x03BB;</em>-Merge</td>        <td style="text-align:center;">.1081</td>        <td style="text-align:center;">.8052</td>        <td style="text-align:center;">.7701</td>        <td style="text-align:center;">.7120</td>        <td style="text-align:center;">.8031</td>        <td style="text-align:center;">.7765</td>        <td style="text-align:center;">.7321</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ClustSUM</td>        <td style="text-align:center;">.1093</td>        <td style="text-align:center;">.8075</td>        <td style="text-align:center;">.7741</td>        <td style="text-align:center;">.7257</td>        <td style="text-align:center;">.8102</td>        <td style="text-align:center;">.7815</td>        <td style="text-align:center;">.7421</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ClustSUM</td>        <td style="text-align:center;">.1199</td>        <td style="text-align:center;">.8200</td>        <td style="text-align:center;">.8020</td>        <td style="text-align:center;">.7430</td>        <td style="text-align:center;">.8136</td>        <td style="text-align:center;">.8038</td>        <td style="text-align:center;">.7638</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ManSUM</td>        <td style="text-align:center;">.1312</td>        <td style="text-align:center;">.8440</td>        <td style="text-align:center;">.8120</td>        <td style="text-align:center;">.7840</td>        <td style="text-align:center;">.8313</td>        <td style="text-align:center;">.8136</td>        <td style="text-align:center;">.7958</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ManSUM</td>        <td style="text-align:center;">.1317<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8480<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8260<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.7960<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8336<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8238<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8054<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-LDSSUM</td>        <td style="text-align:center;">.1421<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8513</td>        <td style="text-align:center;">.8345</td>        <td style="text-align:center;">.8139<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8423<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8328<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8247<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-LDSSUM</td>        <td style="text-align:center;">.1453<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8645<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8458<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8267<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8535<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8439<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8432<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-ManSUM</td>        <td style="text-align:center;">.1470<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8683<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8537<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8364<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8621<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8543<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8532<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;">TREC-3</td>        <td style="text-align:left;">v-ManSUM</td>        <td style="text-align:center;">.1571<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8857<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8639<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8437<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8756<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8673<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.8660</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">CombMNZ</td>        <td style="text-align:center;">.1065</td>        <td style="text-align:center;">.8080</td>        <td style="text-align:center;">.7700</td>        <td style="text-align:center;">.6970</td>        <td style="text-align:center;">.8021</td>        <td style="text-align:center;">.7781</td>        <td style="text-align:center;">.7254</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ClustMNZ</td>        <td style="text-align:center;">.1107</td>        <td style="text-align:center;">.8113</td>        <td style="text-align:center;">.7834</td>        <td style="text-align:center;">.7251</td>        <td style="text-align:center;">.8056</td>        <td style="text-align:center;">.7821</td>        <td style="text-align:center;">.7534</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ClustMNZ</td>        <td style="text-align:center;">.1236</td>        <td style="text-align:center;">.8240</td>        <td style="text-align:center;">.8040</td>        <td style="text-align:center;">.7630</td>        <td style="text-align:center;">.8127</td>        <td style="text-align:center;">.8031</td>        <td style="text-align:center;">.7766</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ManMNZ</td>        <td style="text-align:center;">.1305</td>        <td style="text-align:center;">.8310</td>        <td style="text-align:center;">.8200</td>        <td style="text-align:center;">.7910</td>        <td style="text-align:center;">.8120</td>        <td style="text-align:center;">.8173</td>        <td style="text-align:center;">.8001</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ManMNZ</td>        <td style="text-align:center;">.1324</td>        <td style="text-align:center;">.8334</td>        <td style="text-align:center;">.8240</td>        <td style="text-align:center;">.7930<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8148</td>        <td style="text-align:center;">.8183<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8004<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-LDSSUM</td>        <td style="text-align:center;">.1434<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8538<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8362<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8140<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8451<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8334<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.8252<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-LDSSUM</td>        <td style="text-align:center;">.1454<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8656<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8463<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8279<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8552<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8447<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8442<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-ManSUM</td>        <td style="text-align:center;">.1482<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8673<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8548<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8375<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8642<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8552<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8530<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-ManSUM</td>        <td style="text-align:center;">        <strong>.1572</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.8862</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.8642</strong>        <span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.8448</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.8767</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.8679</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.8658<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Performance on the TREC-10 dataset. Notational conventions for statistical significant test results are as in Table&#x00A0;<a class="tbl" href="#tab2">2</a>.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;"/>        <th style="text-align:center;" colspan="3">p@<hr/>        </th>        <th style="text-align:center;" colspan="3">nDCG@<hr/>        </th>        <th/>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;"/>        <th style="text-align:center;">MAP</th>        <th style="text-align:center;">5</th>        <th style="text-align:center;">10</th>        <th style="text-align:center;">20</th>        <th style="text-align:center;">5</th>        <th style="text-align:center;">10</th>        <th style="text-align:center;">20</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">TREC-10</td>        <td style="text-align:left;">iit01m</td>        <td style="text-align:center;">.2145</td>        <td style="text-align:center;">.6320</td>        <td style="text-align:center;">.5880</td>        <td style="text-align:center;">.4730</td>        <td style="text-align:center;">.5650</td>        <td style="text-align:center;">.5707</td>        <td style="text-align:center;">.5369</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">CombSUM</td>        <td style="text-align:center;">.1988</td>        <td style="text-align:center;">.6480</td>        <td style="text-align:center;">.5620</td>        <td style="text-align:center;">.4650</td>        <td style="text-align:center;">.5870</td>        <td style="text-align:center;">.5679</td>        <td style="text-align:center;">.5409</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>&#x03BB;</em>-Merge</td>        <td style="text-align:center;">.2052</td>        <td style="text-align:center;">.6480</td>        <td style="text-align:center;">.5731</td>        <td style="text-align:center;">.4721</td>        <td style="text-align:center;">.5824</td>        <td style="text-align:center;">.5731</td>        <td style="text-align:center;">.5435</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ClustSUM</td>        <td style="text-align:center;">.1902</td>        <td style="text-align:center;">.6483</td>        <td style="text-align:center;">.5738</td>        <td style="text-align:center;">.4842</td>        <td style="text-align:center;">.5810</td>        <td style="text-align:center;">.5764</td>        <td style="text-align:center;">.5543</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ClustSUM</td>        <td style="text-align:center;">.2351</td>        <td style="text-align:center;">.6500</td>        <td style="text-align:center;">.5980</td>        <td style="text-align:center;">.5370</td>        <td style="text-align:center;">.5821</td>        <td style="text-align:center;">.5945</td>        <td style="text-align:center;">.5955</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ManSUM</td>        <td style="text-align:center;">.2732</td>        <td style="text-align:center;">.6880</td>        <td style="text-align:center;">.6500</td>        <td style="text-align:center;">.6004</td>        <td style="text-align:center;">.6018</td>        <td style="text-align:center;">.6210</td>        <td style="text-align:center;">.6293</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ManSUM</td>        <td style="text-align:center;">.2734<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.7040<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6580<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6020<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6293<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6369<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6447<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-LDSSUM</td>        <td style="text-align:center;">.2814<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.7124<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6625<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6247<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6275<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6347<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6545<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-LDSSUM</td>        <td style="text-align:center;">.2924<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.7232<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6741<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6345<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6357<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6472<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6759<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-ManSUM</td>        <td style="text-align:center;">.2941<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.7263<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6941<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6455<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6478<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6567<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6836<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-ManSUM</td>        <td style="text-align:center;">        <strong>.3110</strong>        <span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.7451</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.7040</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6537</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6548</strong>        <span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6653</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6945</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Performance on the TREC-12 dataset. Notational conventions for statistical significant test results are as in Table&#x00A0;<a class="tbl" href="#tab2">2</a>.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;"/>        <th style="text-align:center;" colspan="3">p@<hr/>        </th>        <th style="text-align:center;" colspan="3">nDCG@<hr/>        </th>        <th/>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;"/>        <th style="text-align:center;">MAP</th>        <th style="text-align:center;">5</th>        <th style="text-align:center;">10</th>        <th style="text-align:center;">20</th>        <th style="text-align:center;">5</th>        <th style="text-align:center;">10</th>        <th style="text-align:center;">20</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">TREC-12</td>        <td style="text-align:left;">pircRBa2</td>        <td style="text-align:center;">.1849</td>        <td style="text-align:center;">.5280</td>        <td style="text-align:center;">.4880</td>        <td style="text-align:center;">.3930</td>        <td style="text-align:center;">.5004</td>        <td style="text-align:center;">.4892</td>        <td style="text-align:center;">.4580</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">CombSUM</td>        <td style="text-align:center;">.1899</td>        <td style="text-align:center;">.5480</td>        <td style="text-align:center;">.4870</td>        <td style="text-align:center;">.3980</td>        <td style="text-align:center;">.5082</td>        <td style="text-align:center;">.4866</td>        <td style="text-align:center;">.4632</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">        <em>&#x03BB;</em>-Merge</td>        <td style="text-align:center;">.1899</td>        <td style="text-align:center;">.5485</td>        <td style="text-align:center;">.4870</td>        <td style="text-align:center;">.4025</td>        <td style="text-align:center;">.5082</td>        <td style="text-align:center;">.4873</td>        <td style="text-align:center;">.4657</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ClustSUM</td>        <td style="text-align:center;">.1903</td>        <td style="text-align:center;">.5488</td>        <td style="text-align:center;">.4907</td>        <td style="text-align:center;">.4113</td>        <td style="text-align:center;">.5082</td>        <td style="text-align:center;">.4883</td>        <td style="text-align:center;">.4724</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ClustSUM</td>        <td style="text-align:center;">.2213</td>        <td style="text-align:center;">.5720</td>        <td style="text-align:center;">.5420</td>        <td style="text-align:center;">.4655</td>        <td style="text-align:center;">.5225</td>        <td style="text-align:center;">.5258</td>        <td style="text-align:center;">.5197</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-ManSUM</td>        <td style="text-align:center;">.2445</td>        <td style="text-align:center;">.6102</td>        <td style="text-align:center;">.6000</td>        <td style="text-align:center;">.5045</td>        <td style="text-align:center;">.5452</td>        <td style="text-align:center;">.5603</td>        <td style="text-align:center;">.5507</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">ManSUM</td>        <td style="text-align:center;">.2498<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6140<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5910<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5085<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5527<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5637<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5611<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-LDSSUM</td>        <td style="text-align:center;">.2571<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6245<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.6037<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.5342<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.5589<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.5674<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>        <td style="text-align:center;">.5678<span class="inline-equation"><span class="tex">$^\triangle$</span>        </span></td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-LDSSUM</td>        <td style="text-align:center;">.2610<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6342<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6152<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5541<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5672<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5867<sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5782<sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">a-v-ManSUM</td>        <td style="text-align:center;">.2654<span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6413<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.6262<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5821<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5747<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5894<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">.5849<sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">v-ManSUM</td>        <td style="text-align:center;">        <strong>.2724</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6435</strong>        <span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6347</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6054</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.5841</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.5973</strong>        <span class="inline-equation"><span class="tex">$_\triangle$</span>        </span>        <sup>&#x25B2;</sup>        </td>        <td style="text-align:center;">        <strong>.6052</strong>        <sub>&#x25B2;</sub>        <sup>&#x25B2;</sup>        </td>       </tr>      </tbody>     </table>     </div>     <p>There are several trends worth noting. (1)&#x00A0;<em>Fusion vs. best single run</em>: All data fusion methods statistically significantly outperform the best single run, which underlines the value of data fusion for improving the performance of document ranking. (2)&#x00A0;<em>Manifold-based fusion methods (ManX, v-LDSX, v-ManX and their efficient versions) vs. all other fusion methods</em>: Compared to other state-of-the-art fusion methods (CombSUM, CombMNZ, <em>&#x03BB;</em>-Merge, ClustX), manifold-based methods are among the best performing fusion methods in terms of all metrics, and the performance differences are usually statistically significant. Thus, fusing documents via manifold algorithms can enhance the performance of data fusion. (3)&#x00A0;<em>Manifold-based vs. clustering-based (ClustX and a-ClustX)</em>: Tables&#x00A0;<a class="tbl" href="#tab2">2</a>,&#x00A0;<a class="tbl" href="#tab3">3</a> and&#x00A0;<a class="tbl" href="#tab4">4</a> show that both manifold-based methods outperform cluster-based methods on all datasets and most improvements are statistically significant. Thus exploiting global inter-document similarities in manifolds helps to enhance performance. (4)&#x00A0;<em>Efficient method vs. its original aggregation method, such as a-ManX vs. ManX</em>: The efficient method does not perform significantly worse than the original method, although it considers only the top-20 documents as anchors. Thus, our anchor-based approach maintains the effectiveness of manifold-based data fusion while considerably reducing its computational cost (see &#x00A7;<a class="sec" href="#sec-28">6.3</a>).</p>    </section>    <section id="sec-27">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Contribution of virtual adversarial perturbation to manifold learning</h3>     </div>     </header>     <p>     <strong>RQ2:</strong> According to Tables&#x00A0;<a class="tbl" href="#tab2">2</a>,&#x00A0;<a class="tbl" href="#tab3">3</a> and&#x00A0;<a class="tbl" href="#tab4">4</a>, all virtual adversarial learning methods statistically significantly outperform methods without virtual adversarial documents. E.g., v-LDSSUM outperforms all non-virtual adversarial learning methods, and v-ManSUM outperforms ManX. Thus, adding virtual adversarial perturbation into manifold learning methods improves the performance of aggregation.</p>     <p>     <strong>RQ3:</strong> Tables&#x00A0;<a class="tbl" href="#tab2">2</a>,&#x00A0;<a class="tbl" href="#tab3">3</a> and&#x00A0;<a class="tbl" href="#tab4">4</a> show that our virtual adversarial manifold learning methods statistically significantly outperform other virtual adversarial manifold methods that use local information for generating virtual adversarial perturbation. E.g., v-ManSUM outperforms v-LDSSUM, and a-v-ManSUM outperforms a-v-LDSSUM. This highlights another merit of our virtual adversarial perturbation construction method: it globally generates virtual adversarial perturbation for each original document by considering not only the original document itself but all other documents to be fused, and thus makes a positive contribution to the performance of manifold learning. Due to space limitations we only discuss CombSUM as a basic data fusion method below; results for CombMNZ are similar.</p>    </section>    <section id="sec-28">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Efficiency of proposed methods</h3>     </div>     </header>     <p>     <strong>RQ4:</strong> To show the efficiency of our proposed efficient manifold learning methods, we randomly choose <em>m</em> = {3, 5, 9, 15, 23} runs from the ad hoc track of TREC-3. We combine these runs using the rank aggregation techniques and measure the running time. We repeat the experiment 20 times for each fusion method and report the average running time in seconds.</p>     <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Running time (in sec.) comparisons among the methods.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;" colspan="5">Number of runs<hr/>        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:left;">3</th>        <th style="text-align:left;">5</th>        <th style="text-align:left;">9</th>        <th style="text-align:left;">15</th>        <th style="text-align:left;">23</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">CombSUM</td>        <td style="text-align:left;">3.98e&#x2013;4</td>        <td style="text-align:left;">8.07e&#x2013;4</td>        <td style="text-align:left;">1.69e&#x2013;3</td>        <td style="text-align:left;">2.86e&#x2013;3</td>        <td style="text-align:left;">3.96e&#x2013;3</td>       </tr>       <tr>        <td style="text-align:left;">a-ManSUM</td>        <td style="text-align:left;">1.46e&#x2013;1</td>        <td style="text-align:left;">5.09e&#x2013;1</td>        <td style="text-align:left;">1.79</td>        <td style="text-align:left;">4.48</td>        <td style="text-align:left;">11.17</td>       </tr>       <tr>        <td style="text-align:left;">a-v-ManSUM</td>        <td style="text-align:left;">2.30e&#x2013;1</td>        <td style="text-align:left;">1.21</td>        <td style="text-align:left;">3.32</td>        <td style="text-align:left;">7.58</td>        <td style="text-align:left;">18.52</td>       </tr>       <tr>        <td style="text-align:left;">a-v-LDSSUM</td>        <td style="text-align:left;">2.55e&#x2013;1</td>        <td style="text-align:left;">1.47</td>        <td style="text-align:left;">3.75</td>        <td style="text-align:left;">8.14</td>        <td style="text-align:left;">21.39</td>       </tr>       <tr>        <td style="text-align:left;">a-ClustSUM</td>        <td style="text-align:left;">6.83e&#x2013;1</td>        <td style="text-align:left;">1.86</td>        <td style="text-align:left;">7.30</td>        <td style="text-align:left;">18.15</td>        <td style="text-align:left;">43.12</td>       </tr>       <tr>        <td style="text-align:left;">ManSUM</td>        <td style="text-align:left;">3.08</td>        <td style="text-align:left;">8.18</td>        <td style="text-align:left;">33.29</td>        <td style="text-align:left;">73.47</td>        <td style="text-align:left;">170.40</td>       </tr>       <tr>        <td style="text-align:left;">v-ManSUM</td>        <td style="text-align:left;">5.83</td>        <td style="text-align:left;">16.30</td>        <td style="text-align:left;">61.72</td>        <td style="text-align:left;">130.59</td>        <td style="text-align:left;">317.23</td>       </tr>       <tr>        <td style="text-align:left;">v-LDSSUM</td>        <td style="text-align:left;">6.21</td>        <td style="text-align:left;">17.52</td>        <td style="text-align:left;">65.42</td>        <td style="text-align:left;">134.23</td>        <td style="text-align:left;">325.38</td>       </tr>       <tr>        <td style="text-align:left;">ClustSUM</td>        <td style="text-align:left;">3.59</td>        <td style="text-align:left;">11.27</td>        <td style="text-align:left;">42.91</td>        <td style="text-align:left;">117.45</td>        <td style="text-align:left;">267.36</td>       </tr>      </tbody>     </table>     </div>     <p>As can be seen in Table&#x00A0;<a class="tbl" href="#tab5">5</a>, all the efficient aggregation methods significantly run faster than the corresponding non-efficient methods. For instance, a-ManSUM runs faster than ManSUM. This demonstrates the merit of the proposed efficient manifold learning methods: they have lower computational costs and run faster, while still achieving a comparable performance to original non-efficient methods. Also, a-v-ManSUM runs faster than a-v-LDSSUM, and v-ManSUM runs faster than v-LDSSUM. The only differences between a-v-ManSUM and a-v-LDSSUM, and between v-ManSUM and v-LDSSUM, are the ways of obtaining the virtual documents. This demonstrates another merit of our way of generating virtual adversarial documents: our virtual adversarial document generation method runs faster than that of LDS method.</p>    </section>    <section id="sec-29">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.4</span> Number of anchor documents</h3>     </div>     </header>     <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186085/images/www2018-94-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Performance on MAP and p@20 for varying number of anchor documents. The top-5 runs from TREC-3 dataset are fused.</span>     </div>     </figure>     <p>     <strong>RQ5:</strong> Next, we examine the effect of the number of anchors on the performance of the proposed manifold learning algorithms that utilize anchors for efficiency. We use the efficient manifold methods, a-v-ManX, a-ManX and a-ClustX, as representatives, as the corresponding non-efficient methods perform better.</p>     <p>Fig.&#x00A0;<a class="fig" href="#fig2">2</a> depicts the MAP and p@20 performance for the aggregation methods when fusing the top-5 runs from the TREC-3 ad hoc track. The performance of all efficient methods increases with the number of anchor documents: with more anchor documents come more information to regularize scores of other documents. In contrast, the performance of a-ClustSUM also increases with the number of anchor documents, but cannot top that of ClustSUM.</p>     <p>a-ManSUM usually outperforms ClustSUM when the top-<em>k</em> (<em>k</em> &#x2265; 15) documents are used as anchors. This shows that considering only a small number of the top-<em>k</em> documents can still improve data fusion performance in manifold-based approach. Still, a-v-ManSUM always significantly outperforms a-ManSUM. This, again, illustrates that adding virtual adversarial documents into efficient manifold learning improves the performance. Also, the performance of a-v-ManSUM and a-ManSUM seems to level off when more than 20 anchors are used. We conjecture that this is because the more top documents are considered, there is less room to make improvement.</p>    </section>    <section id="sec-30">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.5</span> Number of fused lists</h3>     </div>     </header>     <p>     <strong>RQ6:</strong> Finally, we explore the effect of the number of lists being aggregated on the performance. We use a-v-ManSUM and a-ManSUM as representatives, as their non-efficient versions work better. We randomly choose <em>m</em> &#x2208; {3, 5, &#x2026;, 25} runs from the TREC-3 dataset and fuse them. For each <em>m</em>, we repeat the experiment 20 times and report the average performance and the standard deviations. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186085/images/www2018-94-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Performance on MAP and p@20 for varying number of anchor documents. The runs are sampled from the TREC-3 dataset.</span>      </div>     </figure>     </p>     <p>Fig.&#x00A0;<a class="fig" href="#fig3">3</a> shows the performance for varying <em>m</em>, using MAP and p@20. The plots also show the best-performing single run (&#x201C;Maximum&#x201D;) and the average performance of input runs (&#x201C;Average&#x201D;). Data fusion performance usually increases for <em>m</em> &#x2264; 15 and then stays almost flat, while the average performance of input runs fluctuates around the same value. This result is in line with other studies on data fusion, showing that the more individual lists are fused the better the performance. Fig.&#x00A0;<a class="fig" href="#fig3">3</a> also shows that a-v-ManSUM and a-ManSUM always outperform ClustSUM (here we use ClustSUM only, as ClustSUM outperforms a-ClustSUM). This confirms that utilizing manifolds does boost the performance of data fusion. Here, again, the manifold method, a-v-ManSUM, that utilizes virtual adversarial documents, always outperforms the manifold method, a-ManSUM, that does not integrate virtual adversarial documents. Clearly, most fusion methods beat the average performance and the best input run in most cases.</p>    </section>   </section>   <section id="sec-31">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusions</h2>     </div>    </header>    <p>We have studied the problem of rank aggregation. To enhance the performance of aggregation, we have proposed manifold-based aggregation methods. In our manifold learning methods, we let similar documents across the lists being fused provide support to each other by using inter-document similarities, and let documents in the same intrinsic structure enhance each other&#x0027;s relevance score by considering manifolds of the documents being fused. Since the manifold-based technique that we introduce first, ManX, suffers from high computational costs, we propose an efficient version that can handle large-scale datasets for data fusion, reduce the running time and achieve comparable performance. To improve performance and robustness, we propose a virtual adversarial manifold learning method where we generate virtual adversarial documents by adding perturbation to the original documents. We have conducted experiments on three datasets; the results show the effectiveness and efficiency of the proposed manifold learning methods and our way of generating virtual adversarial documents for manifolds.</p>    <p>There are many unexplored avenues. For instance, can we apply the proposed manifold learning to other information retrieval applications, such as clustering documents in streams&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] and diversifying search results&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]? Are there other virtual adversarial perturbation generation methods for manifold learning?</p>   </section>   <section id="sec-32">    <header>     <div class="title-info">     <h2>Acknowledgments</h2>     </div>    </header>    <p>This research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the Criteo Faculty Research Award program, Elsevier, the European Community&#x0027;s Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Google Faculty Research Awards program, the Microsoft Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.001.116, CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.</p>   </section>  </section>  <section class="back-matter">   <Appendix>    <section id="sec-33">     <header>     <div class="title-info">      <h2>       <span class="section-number">A</span> proof of the equality</h2>     </div>     </header>     <p>All notations below are defined in the body of the paper. Analogous proof is found in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]. We multiply matrix <strong>I</strong>     <sub>      <em>n</em>     </sub> &#x2212; <em>&#x03B1;</em>     <strong>S</strong> (not inverse) from&#x00A0;(<a class="eqn" href="#eq12">15</a>) by the matrix from&#x00A0;(<a class="eqn" href="#eq13">16</a>) and should get the identity matrix <strong>I</strong>     <sub>      <em>n</em>     </sub> if they are equivalent, as a result: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray*} &#x0026;&#x0026;(\mathbf {I}_n - \alpha {\bf S}) \times (\mathbf {I}_n - \mathbf {P}^\top (\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha } \mathbf {I}_k)^{-1} \mathbf {P})\\ &#x0026;&#x0026;= (\mathbf {I}_n - \alpha \mathbf {P}^\top \mathbf {P}) \times (\mathbf {I}_n - \mathbf {P}^\top (\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha } \mathbf {I}_k)^{-1} \mathbf {P})\\ &#x0026;&#x0026;= \mathbf {I}_n - \mathbf {P}^\top (\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha }\mathbf {I}_k)^{-1}\mathbf {P} - \alpha \mathbf {P}^\top \mathbf {P} + \alpha \mathbf {P}^\top \mathbf {P}\mathbf {P}^\top (\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha })^{-1}\mathbf {P}\\ &#x0026;&#x0026;= \mathbf {I}_n - (\mathbf {P}^\top - \alpha \mathbf {P}^\top \mathbf {P}\mathbf {P}^\top)(\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha }\mathbf {I}_k)^{-1}\mathbf {P} - \alpha \mathbf {P}^\top \mathbf {P}\\ &#x0026;&#x0026;= \mathbf {I}_n - \alpha \mathbf {P}^\top (\frac{1}{\alpha }\mathbf {I}_k - \mathbf {P}\mathbf {P}^\top)(\mathbf {P}\mathbf {P}^\top - \frac{1}{\alpha }\mathbf {I}_k)^{-1}\mathbf {P} - \alpha \mathbf {P}^\top \mathbf {P}\\ &#x0026;&#x0026;= \mathbf {I}_n + \alpha \mathbf {P}^\top \mathbf {P} - \alpha \mathbf {P}^\top \mathbf {P} =\mathbf {I}_n\end{eqnarray*} </span>       <br/>      </div>     </div>     </p>    </section>    <section id="sec-34">     <header>     <div class="title-info">      <h2>       <span class="section-number">B</span> Derivation of the perturbation <span class="inline-equation"><span class="tex">$\mathbf {r}_i^*$</span>       </span>      </h2>     </div>     </header>     <p>The derivation of the optimal perturbation <strong>r</strong>     <sub>      <em>i</em>     </sub> is as follows: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \mathbf {r}^*_i = &#x0026; \mathop{\arg \min }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \mathrm{sim}(\mathbf {d}_i + \mathbf {r}_i, \mathbf {d}_j) \nonumber \\ = &#x0026; \mathop{\arg \min }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \exp \left\lbrace -\frac{1}{2} \left(\mathrm{KL}(\mathbf {d}_i + \mathbf {r}_{i} \Vert \mathbf {d}_j) + \mathrm{KL}(\mathbf {d}_j \Vert \mathbf {d}_i + \mathbf {r}_i) \right) \right\rbrace . \nonumber\end{align*} </span>       <br/>      </div>     </div> As <span class="inline-equation"><span class="tex">$\sum _{j=1}^n \exp \lbrace -x_j\rbrace \ge n \exp \lbrace -\sum _{j=1}^n x_j\rbrace \ge \exp \lbrace -\sum _{j=1}^n x_j\rbrace$</span>     </span> when <em>x<sub>j</sub>     </em> &#x2265; 0 and KL(&#x00B7; &#x2016; &#x00B7;) &#x2265; 0, the above becomes: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray*} &#x0026; =&#x0026; \mathop{\arg \min }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \exp \left\lbrace -\frac{1}{2} \sum _{j=1}^n \left(\mathrm{KL}(\mathbf {d}_i + \mathbf {r}_{i} \Vert \mathbf {d}_j) + \mathrm{KL}(\mathbf {d}_j \Vert \mathbf {d}_i + \mathbf {r}_i) \right) \right\rbrace \\ &#x0026; =&#x0026; \mathop {\mathrm{arg}\,\max }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \exp \left\lbrace \sum _{j=1}^n \left(\mathrm{KL}(\mathbf {d}_i + \mathbf {r}_{i} \Vert \mathbf {d}_j) + \mathrm{KL}(\mathbf {d}_j \Vert \mathbf {d}_i + \mathbf {r}_i) \right) \right\rbrace .\end{eqnarray*} </span>       <br/>      </div>     </div> According to Pinsker&#x0027;s inequality&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] <span class="inline-equation"><span class="tex">$\mathrm{KL}(\mathbf {x} \Vert \mathbf {y}) \ge \frac{1}{2\ln 2} \Vert \mathbf {x} - \mathbf {y}\Vert ^2_1 = \frac{1}{2\ln 2} \Vert \mathbf {y} - \mathbf {x}\Vert ^2_1$</span>     </span>, where <strong>x</strong> and <strong>y</strong> are two distributions and &#x2016; &#x00B7; &#x2016;<sub>1</sub> is the 1-norm, the above becomes: <div class="table-responsive" id="eq21">      <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026; = \mathop {\mathrm{arg}\,\max }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \exp \left\lbrace \sum _{j=1}^n \frac{1}{2\ln 2} \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1+ \frac{1}{2\ln 2} \Vert \mathbf {d}_j - (\mathbf {r}_i + \mathbf {d}_i) \Vert ^2_1\right\rbrace \nonumber \\ &#x0026; = \mathop {\mathrm{arg}\,\max }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \exp \left\lbrace \sum _{j=1}^n \frac{2}{2\ln 2} \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1 \right\rbrace \nonumber \\ &#x0026; = \mathop {\mathrm{arg}\,\max }_{\mathbf {r}_i, \Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \exp \left\lbrace \sum _{j=1}^n \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1 \right\rbrace \nonumber \\ &#x0026; = \arg _{\mathbf {r}_i} \exp \left\lbrace \max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1 \right\rbrace . \end{align} </span>       <br/>       <span class="equation-number">(26)</span>      </div>     </div> In the above equation&#x00A0;(<a class="eqn" href="#eq21">26</a>), <span class="inline-equation"><span class="tex">$\max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1$</span>     </span> is derived as: <div class="table-responsive" id="eq22">      <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026; \max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1 = \max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon }\sum _{j=1}^n \Vert \mathbf {r}_{i}\Vert _1^2 + 2 \Vert \mathbf {r}_i (\mathbf {d}_i - \mathbf {d}_j)\Vert _1 + \Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1^2 \nonumber \\ &#x0026; = \max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \left\lbrace n \Vert \mathbf {r}_{i}\Vert ^2 + 2 \Vert \mathbf {r}_{i}\Vert _1 \sum _{j=1}^n\Vert \mathbf {d}_i - \mathbf {d}_j\Vert + \sum _{j=1}^n \Vert \mathbf {d}_i - \mathbf {d}_j\Vert ^2\right\rbrace \nonumber \\ &#x0026; \le n\epsilon + \sum _{j=1}^n \Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1^2 + \max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } 2 \Vert \mathbf {r}_{i}\Vert _1 \sum _{j=1}^n\Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1 \nonumber \\ &#x0026; \le n\epsilon + \sum _{j=1}^n \Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1^2 + \left(\sum _{j=1}^n \Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1 \right)^2 + \max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \Vert \mathbf {r}_{i}\Vert _1^2 \nonumber \\ &#x0026; \le n\epsilon + \sum _{j=1}^n \Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1^2 + \left(\sum _{j=1}^n \Vert \mathbf {d}_i - \mathbf {d}_j\Vert _1 \right)^2 + \epsilon = \mathrm{constant}. \end{align} </span>       <br/>       <span class="equation-number">(27)</span>      </div>     </div> If we let <span class="inline-equation"><span class="tex">$\mathbf {r}_i = \epsilon \times \overline{\sum _{j=1}^n (\mathbf {d}_i - \mathbf {d}_j)} = \epsilon \times \overline{n\mathbf {d}_i - \sum _{j=1}^n \mathbf {d}_j}$</span>     </span>, <span class="inline-equation"><span class="tex">$\max _{\Vert \mathbf {r}_{i} \Vert ^2 \le \epsilon } \sum _{j=1}^n \Vert \mathbf {d}_i + \mathbf {r}_i - \mathbf {d}_j \Vert ^2_1$</span>     </span>, i.e.,&#x00A0;(<a class="eqn" href="#eq22">27</a>), reaches its maximum. Thus, by combining&#x00A0;(<a class="eqn" href="#eq21">26</a>) and&#x00A0;(<a class="eqn" href="#eq22">27</a>), we have the final virtual adversarial perturbation <span class="inline-equation"><span class="tex">$\mathbf {r}^*_i = \epsilon \times \overline{n\mathbf {d}_i - \sum _{j=1}^n \mathbf {d}_j}$</span>     </span>.</p>    </section>    <section id="sec-35">     <header>     <div class="title-info">      <h2>       <span class="section-number">C</span> Derivation of the closed form <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{v\text{-}manx}}$</span>       </span>      </h2>     </div>     </header>     <p>Writing <em>&#x0394;</em> for <span class="inline-equation"><span class="tex">$\Vert \frac{\mathbf {f}_{\mathrm{v\text{-}ManX}\,i}}{\sqrt {D_{ii}}} - \frac{\mathbf {f}_{\mathrm{v\text{-}ManX}\,j}}{\sqrt {D_{jj}}} \Vert ^2$</span>     </span>, the cost function in&#x00A0;(<a class="eqn" href="#eq16">20</a>) is: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \mathcal {Q}(\mathbf {f}_{\mathrm{v\text{-}ManX}})= \frac{1}{4} \sum _{i,j=1}^n W_{ij} \Delta + \frac{1}{4}\sum _{i=1}^n \sum _{j=n+1}^{2n} W_{ij} \Delta \nonumber \\ &#x0026;+ \frac{1}{4}\sum _{i,j=n+1}^{2n} W_{ij} \Delta + \frac{1}{4}\sum _{i=n+1}^{2n} \sum _{j=1}^{n} W_{ij} \Delta + \frac{\mu }{2} \sum _{i=1}^n \left\Vert \mathbf {f}_{\mathrm{v\text{-}ManX}\,i} - \mathbf {f}_{\mathrm{X}\,i} \right\Vert ^2 . \nonumber\end{align*} </span>       <br/>      </div>     </div> We differentiate <span class="inline-equation"><span class="tex">$\mathcal {Q}(\mathbf {f}_{\mathrm{v\text{-}ManX}})$</span>     </span> with respect to <strong>f</strong>     <sub>v-ManX</sub> and have: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \hspace{-7.22743pt} \frac{\partial \mathcal {Q}(\mathbf {f}_{\mathrm{v\text{-}ManX}})}{\partial \mathbf {f}_{\mathrm{v\text{-}ManX}}} = \mathbf {f}_{\mathrm{v\text{-}ManX}} - \frac{1}{2} \mathbf {D}_1^{-1/2} \mathbf {W}_{11} \mathbf {D}_1^{-1/2} \mathbf {f}_{\mathrm{v\text{-}ManX}} - \frac{1}{2} \mathbf {D}_1^{-1/2} \mathbf {W}_{12} \mathbf {D}_1^{-1/2} \mathbf {f}_{\mathrm{v\text{-}ManX}} \nonumber \\ &#x0026; \hspace{-7.22743pt} - \frac{1}{2} \mathbf {D}_2^{-1/2} \mathbf {W}_{21} \mathbf {D}_2^{-1/2} \mathbf {f}_{\mathrm{v\text{-}ManX}} - \frac{1}{2} \mathbf {D}_2^{-1/2} \mathbf {W}_{22} \mathbf {D}_2^{-1/2} \mathbf {f}_{\mathrm{v\text{-}ManX}} + \mu (\mathbf {f}_{\mathrm{v\text{-}ManX}} - \mathbf {f}_{\mathrm{X}}). \nonumber\end{align*} </span>       <br/>      </div>     </div> To obtain the closed form solution <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{v\text{-}ManX}}$</span>     </span>, we set <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {Q}(\mathbf {f}_{\mathrm{v\text{-}ManX}})}{\partial \mathbf {f}_{\mathrm{v\text{-}ManX}}} = 0$</span>     </span> and have: <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; (1+\mu) \mathbf {f}^*_{\mathrm{v\text{-}ManX}} - \left(\frac{1}{2} \mathbf {D}_1^{-1/2} \mathbf {W}_{11} \mathbf {D}_1^{-1/2} + \frac{1}{2} \mathbf {D}_1^{-1/2} \mathbf {W}_{12} \mathbf {D}_1^{-1/2} + \right. \nonumber \\ &#x0026; \left. \frac{1}{2} \mathbf {D}_2^{-1/2} \mathbf {W}_{21} \mathbf {D}_2^{-1/2} + \frac{1}{2} \mathbf {D}_2^{-1/2} \mathbf {W}_{22} \mathbf {D}_2^{-1/2} \right) \mathbf {f}^*_{\mathrm{v\text{-}ManX}} - \mu \mathbf {f}_{\mathrm{X}} =0, \nonumber\end{align*} </span>       <br/>      </div>     </div> which results in the following closed form solution: <div class="table-responsive" id="eq23">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathbf {f}^*_{\mathrm{v\text{-}ManX}} = (1 - \alpha) \left(\mathbf {I} - \alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^{-1} \mathbf {f}_{\mathrm{X}}, \end{equation} </span>       <br/>       <span class="equation-number">(28)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\alpha =\frac{\mu }{1 + \mu }$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{11} = \mathbf {D}_1^{-1/2} \mathbf {W}_{11} \mathbf {D}_1^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{12} = \mathbf {D}_1^{-1/2} \mathbf {W}_{12} \mathbf {D}_1^{-1/2}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathbf {S}_{21} = \mathbf {D}_2^{-1/2} \mathbf {W}_{21} \mathbf {D}_2^{-1/2}$</span>     </span>, and <span class="inline-equation"><span class="tex">$\mathbf {S}_{22} = \mathbf {D}_2^{-1/2} \mathbf {W}_{22} \mathbf {D}_2^{-1/2}$</span>     </span>.</p>    </section>    <section id="sec-36">     <header>     <div class="title-info">      <h2>       <span class="section-number">D</span> Derivation of the iteration form</h2>     </div>     </header>     <p>Without loss of generalization, we suppose <strong>f</strong>     <sub>v-ManX</sub>(0) = <strong>f</strong>     <sub>X</sub>. According to the iteration <span class="inline-equation"><span class="tex">$\mathbf {f}_{\mathrm{v\text{-}ManX}}(t+1) = \alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \mathbf {f}_{\mathrm{v\text{-}ManX}}(t) + (1 - \alpha) \mathbf {f}_{\mathrm{X}}$</span>     </span>, we have: <div class="table-responsive" id="eq24">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbf {f}_{\mathrm{v\text{-}ManX}}(t) = &#x0026; \left(\alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^{t-1} \mathbf {f}_{\mathrm{X}} + \nonumber \\ &#x0026; (1 - \alpha) \sum _{i=0}^{t-1} \left(\alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^i \mathbf {f}_{\mathrm{X}}. \end{align} </span>       <br/>       <span class="equation-number">(29)</span>      </div>     </div> Because 0 < <em>&#x03B1;</em> < 1 and the eigenvalues of <span class="inline-equation"><span class="tex">$\alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22})$</span>     </span> is within [ &#x2212; 1, 1], the the optimal solution <span class="inline-equation"><span class="tex">$\mathbf {f}^*_{\mathrm{v\text{-}ManX}}$</span>     </span> is: <div class="table-responsive" id="eq25">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbf {f}^*_{\mathrm{v\text{-}ManX}} = &#x0026; \lim _{t \rightarrow + \infty } \mathbf {f}_{\mathrm{v\text{-}ManX}}(t) = \lim _{t \rightarrow + \infty } \left(\alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^{t-1} \mathbf {f}_{\mathrm{X}} \nonumber \\ &#x0026; + \lim _{t \rightarrow + \infty }(1 - \alpha) \sum _{i=0}^{t-1} \left(\alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^i \mathbf {f}_{\mathrm{X}} \nonumber \\ = &#x0026; \; \mathbf {0} + (1 - \alpha) \mathbf {f}_{\mathrm{X}} \frac{\mathbf {I} - \left(\alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^{+ \infty }}{\mathbf {I} - \alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22})} \nonumber \\ = &#x0026; (1 - \alpha) \left(\mathbf {I} - \alpha (\frac{1}{2} \mathbf {S}_{11} + \frac{1}{2} \mathbf {S}_{12} + \frac{1}{2} \mathbf {S}_{21}+ \frac{1}{2} \mathbf {S}_{22}) \right)^{-1} \mathbf {f}_{\mathrm{X}}, \end{align} </span>       <br/>       <span class="equation-number">(30)</span>      </div>     </div> which is the same as that in&#x00A0;(<a class="eqn" href="#eq23">28</a>). Thus, the iteration form holds.</p>    </section>   </Appendix>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">J. A. Aslam and M. Montague. Models for metasearch. In <em>      <em>SIGIR</em>     </em>, pages 276&#x2013;284, New Orleans, Louisiana, USA, 2001. ACM.</li>     <li id="BibPLXBIB0002" label="[2]">A. Bhowmik and J. Ghosh. Letor methods for unsupervised rank aggregation. In <em>      <em>WWW</em>     </em>, pages 1331&#x2013;1340, 2017.</li>     <li id="BibPLXBIB0003" label="[3]">I. Caragiannis, X. Chatzigeorgiou, G. A. Krimpas, and A. A. Voudouris. Optimizing positional scoring rules for rank aggregation. In <em>      <em>AAAI</em>     </em>, pages 430&#x2013;436, 2017.</li>     <li id="BibPLXBIB0004" label="[4]">W. B. Croft, D. Metzler, and T. Strohman. <em>      <em>Search engines: Information retrieval in practice</em>     </em>. Addison-Wesley Reading, 2015.</li>     <li id="BibPLXBIB0005" label="[5]">I. Csiszar and J. K&#x00F6;rner. <em>      <em>Information theory: coding theorems for discrete memoryless systems</em>     </em>. Cambridge University Press, 2011.</li>     <li id="BibPLXBIB0006" label="[6]">J. S. Culpepper, M. Petri, and F. Scholer. Efficient in-memory top-k document retrieval. In <em>      <em>SIGIR</em>     </em>, pages 225&#x2013;234, Portland, USA, 2012. ACM.</li>     <li id="BibPLXBIB0007" label="[7]">F. Diaz. Regularizing ad hoc retrieval scores. In <em>      <em>CIKM</em>     </em>, pages 672&#x2013;679, Bremen, Germany, 2005. ACM.</li>     <li id="BibPLXBIB0008" label="[8]">Z. Dong, S. Jia, C. Zhang, M. Pei, and Y. Wu. Deep manifold learning of symmetric positive definite matrices with application to face recognition. In <em>      <em>AAAI</em>     </em>, pages 4009&#x2013;4015, 2017.</li>     <li id="BibPLXBIB0009" label="[9]">I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In <em>      <em>ICLR</em>     </em>, 2015.</li>     <li id="BibPLXBIB0010" label="[10]">A. Griffiths, H. C. Luckhurst, and P. Willett. Adaptive manifold learning. <em>      <em>IEEE Transaction on Pattern Analysis and Machine Intelligence</em>     </em>, 34(2):253&#x2013;265, 2012.</li>     <li id="BibPLXBIB0011" label="[11]">K. Grosse, N. Papernot, et al.Adversarial perturbations against deep neural networks for malware classification. <em>      <em>arXiv preprint arXiv:1606.04435</em>     </em>, 2016.</li>     <li id="BibPLXBIB0012" label="[12]">D. Harman. Overview of the third text retrieval conference (TREC-3). In <em>      <em>TREC</em>     </em>, page 398, Gaithersburg, Maryland, USA, 1994. NIST.</li>     <li id="BibPLXBIB0013" label="[13]">D. Hawking and N. Craswell. Overview of the trec-2001 web track. In <em>      <em>the TREC</em>     </em>, pages 1&#x2013;8, Gaithersburg, Maryland, USA, 2002. NIST.</li>     <li id="BibPLXBIB0014" label="[14]">K. Hofmann, S. Whiteson, and M. de&#x00A0;Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. <em>      <em>ACM Trans. on Inf. Sys.</em>     </em>, pages 1&#x2013;43, 2013.</li>     <li id="BibPLXBIB0015" label="[15]">K. J&#x00E4;rvelin and J. Kek&#x00E4;l&#x00E4;inen. Cumulated gain-based evaluation of IR techniques. <em>      <em>ACM Trans. Inf. Syst.</em>     </em>, 20(4):422&#x2013;446, 2002.</li>     <li id="BibPLXBIB0016" label="[16]">A. Khudyak-Kozorovitsky and O. Kurland. Cluster-based fusion of retrieved lists. In <em>      <em>SIGIR</em>     </em>, pages 893&#x2013;902, Beijing, China, 2011. ACM.</li>     <li id="BibPLXBIB0017" label="[17]">S. Liang and M. de&#x00A0;Rijke. Burst-aware data fusion for microblog search. <em>      <em>Information Processing &#x0026; Management</em>     </em>, pages 89&#x2013;113, 2015.</li>     <li id="BibPLXBIB0018" label="[18]">S. Liang, Z. Ren, and M. de&#x00A0;Rijke. Fusion helps diversification. In <em>      <em>SIGIR</em>     </em>, pages 303&#x2013;312, Gold Coast, Australia, 2014. ACM.</li>     <li id="BibPLXBIB0019" label="[19]">S. Liang, F. Cai, Z. Ren, and M. de&#x00A0;Rijke. Efficient structured learning for personalized diversification. <em>      <em>IEEE Transactions on Knowledge and Data Engineering</em>     </em>, 28(11):2958&#x2013;2973, 2016a.</li>     <li id="BibPLXBIB0020" label="[20]">S. Liang, E. Yilmaz, and E. Kanoulas. Dynamic clustering of streaming short documents. In <em>      <em>KDD</em>     </em>, pages 995&#x2013;1004, 2016b.</li>     <li id="BibPLXBIB0021" label="[21]">S. Liang, Z. Ren, E. Yilmaz, and E. Kanoulas. Collaborative user clustering for short text streams. In <em>      <em>AAAI</em>     </em>, pages 3504&#x2013;3510, 2017a.</li>     <li id="BibPLXBIB0022" label="[22]">S. Liang, Z. Ren, Y. Zhao, J. Ma, E. Yilmaz, and M. de&#x00A0;Rijke. Inferring dynamic user interests in streams of short texts for user clustering. <em>      <em>ACM Trans. Inf. Syst.</em>     </em>, 36(1):10:1&#x2013;10:37, 2017b.</li>     <li id="BibPLXBIB0023" label="[23]">S. Liang, E. Yilmaz, H. Shen, M. de&#x00A0;Rijke, and W. B. Croft. Search result diversification in short text streams. <em>      <em>ACM Trans. Inf. Syst.</em>     </em>, 36(1):8:1&#x2013;8:35, 2017c.</li>     <li id="BibPLXBIB0024" label="[24]">W. Liu, J. He, and S.-F. Chang. Large graph construction for scalable semi-supervised learning. In <em>      <em>ICML</em>     </em>, pages 679&#x2013;686, Haifa, Israel, 2010. Omnipress.</li>     <li id="BibPLXBIB0025" label="[25]">W. Liu, J. Wang, and S.-F. Chang. Robust and scalable graph-based semisupervised learning. <em>      <em>Proceedings of the IEEE</em>     </em>, 100(9):2624&#x2013;2638, 2012.</li>     <li id="BibPLXBIB0026" label="[26]">Y. Liu, Z. Gu, Y.-m. Cheung, and K. A. Hua. Multi-view manifold learning for media interestingness prediction. In <em>      <em>ICMR</em>     </em>, pages 308&#x2013;314, 2017.</li>     <li id="BibPLXBIB0027" label="[27]">Y.-T. Liu, T.-Y. Liu, et al.Supervised rank aggregation. In <em>      <em>WWW</em>     </em>, pages 481&#x2013;489, Banff, Alberta, Canada, 2007. ACM.</li>     <li id="BibPLXBIB0028" label="[28]">T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual adversarial training. In <em>      <em>ICLR</em>     </em>, 2016.</li>     <li id="BibPLXBIB0029" label="[29]">T. Miyato, A. M. Dai, and I. Goodfellow. Adversarial training methods for semi-supervised text classification. In <em>      <em>ICLR</em>     </em>, 2017.</li>     <li id="BibPLXBIB0030" label="[30]">J. A. Shaw and E. A. Fox. Combination of multiple searches. In <em>      <em>TREC</em>     </em>, pages 243&#x2013;252, Gaithersburg, Maryland, USA, 1994. NIST.</li>     <li id="BibPLXBIB0031" label="[31]">D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. LambdaMerge: merging the results of query reformulations. In <em>      <em>WSDM</em>     </em>, pages 795&#x2013;804. ACM, 2011.</li>     <li id="BibPLXBIB0032" label="[32]">M. D. Smucker and J. Allan. A new measure of the cluster hypothesis. In <em>      <em>ICTIR</em>     </em>, pages 281&#x2013;288. Springer-Verlag, 2009.</li>     <li id="BibPLXBIB0033" label="[33]">C. Szegedy, W. Zaremba, et al.Intriguing properties of neural networks. In <em>      <em>ICLR</em>     </em>, 2014.</li>     <li id="BibPLXBIB0034" label="[34]">E. M. Voorhees. Overview of the TREC 2005 robust retrieval track. In <em>      <em>the TREC</em>     </em>, pages 1&#x2013;9, Gaithersburg, Maryland, USA, 2005. NIST.</li>     <li id="BibPLXBIB0035" label="[35]">Q. Wang, M. Chen, and X. Li. Quantifying and detecting collective motion by manifold learning. In <em>      <em>AAAI</em>     </em>, pages 4292&#x2013;4298, 2017.</li>     <li id="BibPLXBIB0036" label="[36]">B. Xu, J. Bu, et al.Efficient manifold ranking for image retrieval. In <em>      <em>SIGIR</em>     </em>, pages 525&#x2013;534. ACM, 2011.</li>     <li id="BibPLXBIB0037" label="[37]">C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In <em>      <em>SIGIR</em>     </em>, 2001.</li>     <li id="BibPLXBIB0038" label="[38]">D. Zhou, O. Bousquet, et al.Learning with local and global consistency. In <em>      <em>NIPS</em>     </em>, pages 321&#x2013;328. MIT Press, 2004a.</li>     <li id="BibPLXBIB0039" label="[39]">D. Zhou, J. Weston, et al.Ranking on data manifolds. In <em>      <em>NIPS</em>     </em>, pages 169&#x2013;176, 2004b.</li>     <li id="BibPLXBIB0040" label="[40]">X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In <em>      <em>ICML</em>     </em>, 2003.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://trec.nist.gov">http://trec.nist.gov</a>. </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Publicly available from <a class="link-inline force-break" href="http://trec.nist.gov">http://trec.nist.gov</a>. </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>In Tables&#x00A0;<a class="tbl" href="#tab3">3</a> and&#x00A0;<a class="tbl" href="#tab4">4</a>, we only report the results for CombSUM as a basic fusion method as the results for CombMNZ are similar.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; '18; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/>ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186085">https://doi.org/10.1145/3178876.3186085</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
