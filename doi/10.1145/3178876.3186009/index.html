<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Short-Text Topic Modeling via Non-negative Matrix Factorization Enriched with Local Word-Context Correlations</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186009'>https://doi.org/10.1145/3178876.3186009</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186009'>https://w3id.org/oa/10.1145/3178876.3186009</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Short-Text Topic Modeling via Non-negative Matrix Factorization Enriched with Local Word-Context Correlations</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Tian</span> <span class="surName">Shi</span>, Virginia Tech, <a href="mailto:tshi@vt.edu">tshi@vt.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Kyeongpil</span> <span class="surName">Kang</span>, Korea University, <a href="mailto:rudvlf0413@korea.ac.kr">rudvlf0413@korea.ac.kr</a>
        </div>
        <div class="author">
          <span class="givenName">Jaegul</span> <span class="surName">Choo</span><a class="fn" href="#fn1" id="foot-fn1"><sup>*</sup></a>, Korea University, <a href="mailto:jchoo@korea.ac.kr">jchoo@korea.ac.kr</a>
        </div>
        <div class="author">
          <span class="givenName">Chandan K.</span> <span class="surName">Reddy</span>, Virginia Tech, <a href="mailto:reddy@cs.vt.edu">reddy@cs.vt.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186009" target="_blank">https://doi.org/10.1145/3178876.3186009</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Being a prevalent form of social communications on the Internet, billions of short texts are generated everyday. Discovering knowledge from them has gained a lot of interest from both industry and academia. The short texts have a limited contextual information, and they are sparse, noisy and ambiguous, and hence, automatically learning topics from them remains an important challenge. To tackle this problem, in this paper, we propose a semantics-assisted non-negative matrix factorization (SeaNMF) model to discover topics for the short texts. It effectively incorporates the word-context semantic correlations into the model, where the semantic relationships between the words and their contexts are learned from the skip-gram view of the corpus. The SeaNMF model is solved using a block coordinate descent algorithm. We also develop a sparse variant of the SeaNMF model which can achieve a better model interpretability. Extensive quantitative evaluations on various real-world short text datasets demonstrate the superior performance of the proposed models over several other state-of-the-art methods in terms of topic coherence and classification accuracy. The qualitative semantic analysis demonstrates the interpretability of our models by discovering meaningful and consistent topics. With a simple formulation and the superior performance, SeaNMF can be an effective standard topic model for short texts.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Document topic models;</strong> <em>Document representation;</em> • <strong>Computing methodologies</strong> → <strong>Topic modeling;</strong> <strong>Non-negative matrix factorization;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Topic modeling</small>,</span> <span class="keyword"><small>short texts</small>,</span> <span class="keyword"><small>non-negative matrix factorization</small>,</span> <span class="keyword"><small>word embedding.</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Tian Shi, Kyeongpil Kang, Jaegul Choo, and Chandan K. Reddy. 2018. Short-Text Topic Modeling via Non-negative Matrix Factorization Enriched with Local Word-Context Correlations. In <em>WWW 2018: The 2018 Web Conference,</em> <em>April 23–27, 2018, Lyon, France</em>. ACM, New York, NY, USA, 9 pages. <a href="https://doi.org/10.1145/3178876.3186009" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3178876.3186009</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Everyday, large amounts of short texts are generated, such as tweets, search queries, questions, image tags, ad keywords, headlines, and others. They have played an important role in our daily lives. Discovering knowledge from them becomes an interesting yet challenging research task which has gained a lot of attention [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>]. Since short texts have only a few words, they can be arbitrary, noisy and ambiguous. All these factors make it difficult to effectively represent short texts and discover knowledge from them.</p>
      <p>Traditionally, topic modeling has been widely used to automatically uncover the hidden thematic information from the documents with rich content [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>]. Generally speaking, there are two groups of topic models, i.e., generative probabilistic models, such as latent Dirichlet allocation (LDA) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>], and non-negative matrix factorization (NMF) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]. The NMF-based models learn topics by directly decomposing the term-document matrix, which is a bag-of-word matrix representation of a text corpus, into two low-rank factor matrices. The NMF based models have shown outstanding performance in dimension reduction and clustering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] for the high-dimensional data.</p>
      <p>Although the conventional topic models have achieved great success for regular-sized documents, they do not work well on short text collections. Since a short text only contains a few meaningful keywords, the word co-occurrence information is difficult to be captured [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>]. In the last few years, many efforts have been dedicated to tackle this challenge. A popular strategy is to aggregate short texts to the pseudo-documents and uncover the cross-document word co-occurrence [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]. However, the topics discovered by these models may be biased by the pseudo-documents generated heuristically. More specifically, many irrelevant short texts may be aggregated into the same pseudo-document.</p>
      <p>Another strategy is to use the internal semantic relationships of the words to overcome the problem of lacking word co-occurrence. This strategy is proposed due to the fact that the semantic information of words has been effectively captured by the deep-neural-network-based word embedding techniques, such as word2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] and Glove [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>]. Several attempts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] have been made to discover topics for short texts by leveraging semantic information of the words from the existing sources, such as the word embeddings based on GoogleNews<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> and WiKipedia<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>. However, since there are many differences between the Wikipedia articles and the short texts, such word semantic representations may introduce the noise and bias to the topics.</p>
      <p>Generally speaking, the word embedding can be useful for short text topic modeling because the words with similar semantic attributes are projected into the same region in the continuous vector space which will improve the clustering performance of the topic models. However, we find another way to boost the performance of the topic models using the skip-gram model with the negative sampling (SGNS). It is well known that SGNS can successfully capture the relationships between a word and its context in a small sliding window [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>]. Interestingly, for a short text corpus, each document can naturally be selected as a window. Therefore, the word-context semantic correlations will be effectively captured by SGNS. These correlations can be viewed as an alternative form of the word co-occurrence. It potentially overcomes the problem that arises due to the data sparsity.</p>
      <p>There are a few recent studies which show that the SGNS algorithm is equivalent to factorizing a term correlation matrix [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>]. Thus, we raise some natural questions: <strong>1)</strong> Can we convert the matrix factorization problem to a non-negative matrix factorization problem? <strong>2)</strong> Can we incorporate this result into the conventional NMF for term-document matrix? <strong>3)</strong> Will the proposed model perform well on discovering topics for short texts? Motivated by these questions, we propose a novel semantics-assisted NMF (SeaNMF) model for short-text topic modeling which is outlined in Fig.&nbsp;<a class="fig" href="#fig1">1</a>. In this figure, the documents, words and contexts are denoted as <em>D<sub>i</sub></em> , <em>w<sub>i</sub></em> and <em>c<sub>i</sub></em> , respectively. The proposed SeaNMF model can capture the semantics from the short text corpus based on word-document and word-context correlations, and our objective function combines the advantages of both the NMF model for topic modeling and the skip-gram model for capturing word-context semantic correlations. In the figure, <em>H</em>, <em>W<sub>c</sub></em> and <em>W</em> are the vector representations of documents, contexts and words in the latent space. Each column of <em>W</em> represents a topic. We use a block coordinate descent algorithm to solve the optimizations. To achieve better interpretability, we also introduce a sparse version of the SeaNMF model.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186009/images/www2018-18-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">The overview of the proposed SeaNMF model for learning topics from the short text corpus, which is represented by a bi-relational matrix with both word-document and word-context correlations.</span>
        </div>
      </figure>
      <p></p>
      <p>The proposed models are compared with the other state-of-the-art methods on four real-world short text datasets. The quantitative experiments demonstrate the superiority of our models over several other existing methods in terms of topic coherence and document classification accuracy. The stability and consistency of SeaNMF are testified by parameter sensitivity analysis. Finally, we design an experiment to investigate the interpretability of the SeaNMF model. By visualizing the top keywords of different topics and analyzing their networks, we demonstrate that the topics discovered by SeaNMF are meaningful and their representative keywords are more semantically correlated. Hence, the proposed SeaNMF is an effective topic model for short texts.</p>
      <p>The rest of this paper is organized as follows. In Section <a class="sec" href="#sec-6">2</a>, we present related work. In Section <a class="sec" href="#sec-7">3</a>, we propose the SeaNMF model and explain the optimization method used for learning the model. In Section <a class="sec" href="#sec-19">4</a>, we introduce the datasets, comparison methods and evaluation metrics, as well as analyze the experimental results. Finally, we conclude our work in Section <a class="sec" href="#sec-28">5</a>.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>Topic modeling for short texts is a challenging research area and many models have been proposed to overcome the lack of contextual information. Most of the current studies are based on the generative probabilistic model, i.e., LDA [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>]. Basically, there are three strategies to tackle the problem. The first strategy can capture the cross-document word co-occurrence via aggregating the short texts to the pseudo-documents. To aggregate the documents, some studies leverage the rich auxiliary contextual information, like authors, time, locations, etc. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>]. For example, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], tweets posted by the same user are aggregated to a pseudo-document. However, this method cannot be applied to the corpus without auxiliary information. To overcome this disadvantage, another aggregation method is proposed, where the so-called latent pseudo-document is generated using the short texts according to their own topics [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>].</p>
      <p>The second strategy considers to the word semantic information from a external corpus, like Wikipedia and Google news [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>]. It benefits a lot from the recently developed word embedding approaches based on neural networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>], which are efficient in uncovering the syntactic and semantic information of the words. For example, Xun et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] train the word embeddings upon Wikipedia and use the semantic information as supplementary sources for their topic model. The third strategy directly makes use of word co-occurrence patterns in documents, i.e., short texts. It is also known as the Biterm model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], since word-pairs co-occurring in the same short text are extracted during the topic modeling. All the above strategies have been demonstrated to be useful in discovering topics for short texts.</p>
      <p>Although the NMF based methods have been successfully applied to topic modeling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>], very few of them are designed to discover topics for the short texts. In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>], Yan et al. propose a NMF model to learn topics for short texts by directly factorizing a symmetric term correlation matrix. However, since they formulate a quartic non-convex loss function, the algorithm proposed in the work is not reliable and stable. The recently proposed SymNMF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] can overcome this problem. However, it does not provide any good intuition for topic modeling. In addition, we cannot get the document representation from SymNMF directly. Therefore, the proposed method in this paper is the first work that considers to build a standard NMF-based topic model for the short texts.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Proposed Method</h2>
        </div>
      </header>
      <p>In this section, we will first provide some preliminaries along with the block coordinate descent method and its applications in NMF for topic modeling. Then, we will propose our SeaNMF model, and a block-coordinate descent algorithm to estimate latent representations of terms and short documents.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Notations</h3>
          </div>
        </header>
        <p>The frequently used notations in this section are summarized in Table&nbsp;<a class="tbl" href="#tab1">1</a>.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Notations used in this paper.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Name</th>
                <th style="text-align:center;">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>A</em></td>
                <td style="text-align:center;">Term-document (word-document) matrix.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>S</em></td>
                <td style="text-align:center;">Word-context (semantic) correlation matrix.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>W</em></td>
                <td style="text-align:center;">Latent factor matrix of words.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>W<sub>c</sub></em></td>
                <td style="text-align:center;">Latent factor matrix of contexts.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>H</em></td>
                <td style="text-align:center;">Latent factor matrix of documents.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><span class="inline-equation"><span class="tex">$\vec{w}_j$</span></span></td>
                <td style="text-align:center;">Vector representation of word <em>w<sub>j</sub></em> .</td>
              </tr>
              <tr>
                <td style="text-align:center;"><span class="inline-equation"><span class="tex">$\vec{c}_j$</span></span></td>
                <td style="text-align:center;">Vector representation of context <em>c<sub>j</sub></em> .</td>
              </tr>
              <tr>
                <td style="text-align:center;"><span class="inline-equation"><span class="tex">$\mathbb {R}_+$</span></span></td>
                <td style="text-align:center;">Non-negative real numbers.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>N</em></td>
                <td style="text-align:center;">Number of documents in the corpus.</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>M</em></td>
                <td style="text-align:center;">Number of distinct words in the vocabulary.</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Preliminaries</h3>
          </div>
        </header>
        <section id="sec-10">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.2.1</span> NMF for Topic Modeling</h4>
            </div>
          </header>
          <p>The NMF method has been successfully applied to topic modeling, due to its superior performance in clustering high-dimensional data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]. Given a corpus with <em>N</em> documents and <em>M</em> distinct words/terms/keywords in the vocabulary <span class="inline-equation"><span class="tex">$\mathbb {V}$</span></span> , we can use a term-document matrix <span class="inline-equation"><span class="tex">$A\in \mathbb {R}_+^{M\times N}$</span></span> to represent it, where <span class="inline-equation"><span class="tex">$\mathbb {R}_+$</span></span> denotes non-negative real numbers. Each column vector <span class="inline-equation"><span class="tex">$A_{(:,j)}\in \mathbb {R}^{M\times 1}_+$</span></span> corresponds to a bag-of-word representation of document <em>j</em> in terms of <em>M</em> keywords. The term-document matrix can be approximated by two lower-rank matrices <span class="inline-equation"><span class="tex">$W\in \mathbb {R}^{M\times K}_+$</span></span> and <span class="inline-equation"><span class="tex">$H\in \mathbb {R}_+^{N\times K}$</span></span> , i.e., <em>A</em> ≈ <em>WH<sup>T</sup></em> , where <em>K</em> ≪ min (<em>M</em>, <em>N</em>) is the number of latent factors (i.e., topics). Usually, this approximation can be formulated as follows:</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \min _{W,H\ge 0}\Vert A-WH^T\Vert _F^2. \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>In topic models, the column vector <span class="inline-equation"><span class="tex">$W_{(:,k)}\in \mathbb {R}_+^{M\times 1}$</span></span> represents the <em>k</em>-th topic in terms of <em>M</em> keywords, and its elements are the weights of the corresponding keywords. The row vector <span class="inline-equation"><span class="tex">$H_{(j,:)}\in \mathbb {R}_+^{1\times K}$</span></span> is the latent representation for document <em>j</em> in terms of <em>K</em> topics. Similarly, we can view the row vector <span class="inline-equation"><span class="tex">$W_{(i,:)}\in \mathbb {R}_+^{1\times K}$</span></span> as the latent semantic representation of word <em>i</em>. It is worth mentioning that there are many other divergences, which can be found in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>].
          <p></p>
        </section>
        <section id="sec-11">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.2.2</span> Problem Statement</h4>
            </div>
          </header>
          <p>Due to the data sparsity, the short texts are too short for the conventional topic models to effectively capture document-level word co-occurrence, which leads to the poor performance in topic learning. To tackle this problem, we first investigate the algorithms for estimating the factor matrices in NMF. For example, in the block coordinate descent (BCD) algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>], the updating rules for <em>W</em> and <em>H</em> are shown as follows:</p>
          <p></p>
          <ul class="list-no-style">
            <li id="list1a" label="•">
              <strong>Update <em>W</em></strong>.
              <div class="table-responsive" id="eq2">
                <div class="display-equation">
                  <span class="tex mytex">\begin{equation} \begin{split}W_{(:,k)}&amp;\leftarrow \left[W_{(:,k)}+\frac{(AH)_{(:,k)}-(WH^TH)_{(:,k)}}{(H^TH)_{(k,k)}}\right]_+ \end{split}\end{equation}</span><br />
                  <span class="equation-number">(2)</span>
                </div>
              </div>
            </li>
            <li id="list1b" label="•">
              <strong>Update <em>H</em></strong>.
              <div class="table-responsive" id="eq3">
                <div class="display-equation">
                  <span class="tex mytex">\begin{equation} \begin{split}H_{(:,k)}\leftarrow \left[H_{(:,k)}+\frac{(A^TW)_{(:,k)}-(HW^TW)_{(:,k)}}{(W^TW)_{(k,k)}}\right]_+ \end{split}\end{equation}</span><br />
                  <span class="equation-number">(3)</span>
                </div>
              </div>
            </li>
          </ul>
          <p></p>where <span class="inline-equation"><span class="tex">$[x]_+=\max (x, 0), \forall x\in \mathbb {R}$</span></span> .
          <p></p>
          <p>From the algorithm, we observe that the following lemma holds.</p>
          <div class="lemma" id="enc1">
            <label>Lemma 3.1.</label>
            <p>For the BCD algorithm, within each iteration:</p>
            <p></p>
            <ol class="list-no-style">
              <li id="list1" label="(1)">The keyword-vector <span class="inline-equation"><span class="tex">$W_{(i,:)}^{t+1}$</span></span> is independent of vector <span class="inline-equation"><span class="tex">$W_{(j,:)}^{t}$</span></span> , when 1 ≤ <em>j</em> ≠ <em>i</em> ≤ <em>M</em>.<br /></li>
              <li id="list2" label="(2)">The document-vector <span class="inline-equation"><span class="tex">$H_{(i,:)}^{t+1}$</span></span> is independent of vector <span class="inline-equation"><span class="tex">$H_{(j,:)}^{t}$</span></span> , when 1 ≤ <em>j</em> ≠ <em>i</em> ≤ <em>N</em>.<br /></li>
            </ol>
            <p></p>
            <p><em>where t represents the t-th iteration.</em></p>
          </div>
          <div class="proof" id="proof1">
            <label>Proof.</label>
            <p>To prove that <span class="inline-equation"><span class="tex">$W_{(i,:)}^{t+1}$</span></span> is independent of <span class="inline-equation"><span class="tex">$W_{(j,:)}^{t}$</span></span> , ∀<em>j</em> ≠ <em>i</em>, we only need to prove that (<em>WH<sup>T</sup>H</em>)<sub>(<em>i</em>, <em>k</em>)</sub> is independent of <em>W</em> <sub>(<em>j</em>, :)</sub>, ∀1 ≤ <em>k</em> ≤ <em>K</em>. To simplify the proof, we use a symmetric matrix <span class="inline-equation"><span class="tex">$B\in \mathbb {R}^{K\times K}_+$</span></span> to represent <em>H<sup>T</sup>H</em>. Thus, we get (<em>WH<sup>T</sup>H</em>)<sub>(<em>i</em>, <em>k</em>)</sub> = (<em>WB</em>)<sub>(<em>i</em>, <em>k</em>)</sub> = <em>W</em> <sub>(<em>i</em>, :)</sub> · <em>B</em> <sub>(:, <em>k</em>)</sub> which only depends on <em>W</em> <sub>(<em>i</em>, :)</sub>. Hence, <span class="inline-equation"><span class="tex">$W_{(i,:)}^{t+1}$</span></span> is independent of <span class="inline-equation"><span class="tex">$W_{(j,:)}^{t},\forall j\ne i$</span></span> . Similarly, we can also prove that <span class="inline-equation"><span class="tex">$H_{(i,:)}^{t+1}$</span></span> is independent of <span class="inline-equation"><span class="tex">$H_{(j,:)}^{t}$</span></span> . □</p>
          </div>
          <p>We also have the same conclusion for the gradient descent (GD) algorithm. Generally speaking, the relationship between different keywords strongly depends on the documents and vice-versa (see Fig.&nbsp;<a class="fig" href="#fig1">1</a>). However, due to the data sparsity, i.e., each document has only several keywords, the relationships of keywords are biased by a lot of unrelated documents which results in poor clustering performance. Moreover, the relationships between the keywords and their contexts, i.e., semantic relationships, are not directly discovered by the BCD or GD algorithms in NMF. Therefore, a standard NMF model cannot effectively capture the word co-occurrence for short texts. In this paper, we will overcome this drawback by introducing additional dependence of the keywords on their contexts via neural word embedding (see Fig.&nbsp;<a class="fig" href="#fig1">1</a>).</p>
        </section>
        <section id="sec-12">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.2.3</span> Neural Word Embedding</h4>
            </div>
          </header>
          <p>Word embedding has been demonstrated to be an effective tool in capturing semantic relationships of the words. Represented by dense vectors, words with similar semantic and syntatic attributes can be found in the same area in the continuous vector space. One of the most successful word embedding methods is proposed by Mikolov et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>], known as Skip-Gram with Negative-Sampling (SGNS). The objective function of SGNS is expressed as:</p>
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \log \sigma (\vec{w}\cdot \vec{c})+\kappa \cdot \mathbb {E}_{c_{neg}\sim p(c)}[\log \sigma (-\vec{w}\cdot \vec{c}_{neg})], \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>where <em>w</em> and <em>c</em> represent word and one of its contexts in a sliding window, respectively. <span class="inline-equation"><span class="tex">$\vec{w}\in \mathbb {R}^K$</span></span> and <span class="inline-equation"><span class="tex">$\vec{c}\in \mathbb {R}^K$</span></span> are vector representations of them. <span class="inline-equation"><span class="tex">$\sigma (\vec{w}\cdot \vec{c})={1}/{(1+e^{-\vec{w}\cdot \vec{c}})}$</span></span> . <em>c<sub>neg</sub></em> is the sampled contexts, known as negative samples, drawn based on a unigram distribution <em>p</em>(<em>c</em>). <em>κ</em> is the number of negative samples.
          <p></p>
          <p>Recently, Levy et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>] have proven that SGNS is equivalent to factorizing a (shifted) word correlation matrix:</p>
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \vec{w}\cdot \vec{c}=\log \left(\frac{\#(w,c)\cdot \mathcal {D}}{\#(w)\cdot \#(c)}\right)-\log \kappa \end{equation}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>where <span class="inline-equation"><span class="tex">$\#(w,c)$</span></span> denotes the number of (<em>w</em>, <em>c</em>) pairs in a corpus. The total number of word-context pairs is <span class="inline-equation"><span class="tex">$\mathcal {D}=\sum _{w,c\in \mathbb {V}}\#(w,c)$</span></span> . Similarly, <span class="inline-equation"><span class="tex">$\#(w)=\sum _{c\in V}\#(w,c)$</span></span> and <span class="inline-equation"><span class="tex">$\#(c)=\sum _{w\in \mathbb {V}}\#(w,c)$</span></span> represent the number of times <em>w</em> and <em>c</em> occur in all possible word-context pairs, respectively. <em>p</em>(<em>c</em>) in Eq. (<a class="eqn" href="#eq4">4</a>) is expressed as <span class="inline-equation"><span class="tex">$p(c)=\#(c)/\mathcal {D}$</span></span> . It is worth mentioning that the <span class="inline-equation"><span class="tex">$\log (({\#(w,c)\cdot \mathcal {D}})/({\#(w)\cdot \#(c)}))$</span></span> is known as the pointwise mutual information (PMI). Therefore, based on this concern, an alternative word representation method was proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>], where the positive constraint is applied to the PMI matrix (PPMI), and then it is factorized by a singular value decomposition method. The Eq. (<a class="eqn" href="#eq5">5</a>) reveals the internal relationships between the word and its context, which is critical to overcome the problem of lacking word co-occurrence. In this paper, we will leverage the word-context semantic relationships to boost the performance of our models.
          <p></p>
        </section>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> The SeaNMF Model</h3>
          </div>
        </header>
        <p>In this section, we propose a novel semantics-assisted NMF (SeaNMF) model to learn topics from the short texts. Our model incorporates the semantic information using the word embeddings into the model training, which enable SeaNMF to recover word co-occurrence from semantic relationships between keywords and their contexts (see Fig.&nbsp;<a class="fig" href="#fig1">1</a>).</p>
        <section id="sec-14">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.3.1</span> Model Formulation</h4>
            </div>
          </header>
          <p>One challenge of our work is to appropriately introduce the word semantics to NMF. Since the latent matrix <span class="inline-equation"><span class="tex">$W\in \mathbb {R}_+^{M\times K}$</span></span> (The elements of <em>W</em> are non-negative), we apply the non-negative constraints on both word and context vectors. Therefore, <span class="inline-equation"><span class="tex">$\vec{w}\in \mathbb {R}^K_+$</span></span> and <span class="inline-equation"><span class="tex">$\vec{c}\in \mathbb {R}^K_+$</span></span> hold. Given a keyword <span class="inline-equation"><span class="tex">$w_i\in \mathbb {V}$</span></span> , we set <span class="inline-equation"><span class="tex">$W_{(i,:)}=\vec{w}_i$</span></span> . To reveal the semantic relationships between the keywords and their context, a matrix <em>W<sub>c</sub></em> is defined for the words in contexts. Thus, <span class="inline-equation"><span class="tex">$W_c{(j,:)}=\vec{c}_j$</span></span> for <span class="inline-equation"><span class="tex">$c_j\in \mathbb {V}$</span></span> .</p>
          <p>With the word and context representations, we can define a semantic (word-context) correlation matrix <em>S</em> which reveals relationships between the keyword and their contexts. Hence, we have</p>
          <div class="table-responsive" id="Xeq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} S\approx WW_c^T. \end{equation}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>The matrix <em>S</em> can be obtained from the skip-gram view of the corpus. Here, we define each element <em>S<sub>ij</sub></em> as follows:
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} S_{ij}=\left[\log \left(\frac{\#(w_i,c_j)}{\#(w_i)\cdot p(c_j)}\right)-\log \kappa \right]_+, \end{equation}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>where <em>p</em>(<em>c<sub>j</sub></em> ) is a unigram distribution for sampling a context <em>c<sub>j</sub></em> . Different from Eq. (<a class="eqn" href="#eq5">5</a>), it is defined as
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} p(c_j)=\frac{\#(c_j)^\gamma }{\sum _{c_j\in \mathbb {V}}\#(c_j)^\gamma }, \end{equation}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>where <em>γ</em> is a smoothing factor. It should be noted that <em>S</em> need not necessarily be symmetric. Specifying the sliding windows is a critical component of the skip-gram model. However, for the short texts, this work turns out to be simple. That is, we can naturally view each short document as a window, since each window will have only a few words. Therefore, the total number of windows is equal to the number of documents. Finally, <span class="inline-equation"><span class="tex">$\#(w_i,c_j)$</span></span> , <span class="inline-equation"><span class="tex">$\#(w_i)$</span></span> , <span class="inline-equation"><span class="tex">$\#(c_j)$</span></span> and <span class="inline-equation"><span class="tex">$\mathcal {D}$</span></span> will be calculated accordingly.
          <p></p>
          <div class="remark" id="enc2">
            <label>REMARK 1.</label>
            <p>The semantic correlation matrix <em>S</em> is not required to be symmetric.</p>
          </div>
          <div class="remark" id="enc3">
            <label>REMARK 2.</label>
            <p>In this paper, each short text is viewed as a window. Therefore, the size of each window in the skip-gram model is equal the length of the corresponding short text. The total number of windows is equal to the number of short texts.</p>
          </div>
          <p>With the term-document matrix and the semantic correlation matrix, the objective function is expressed as follows:</p>
          <div class="table-responsive" id="eq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \min _{W,W_c,H\ge 0}\left\Vert {\left(\begin{array}{*10c}A^T \\ \sqrt {\alpha }S^T\end{array}\right)}-{\left(\begin{array}{*10c}H \\ \sqrt {\alpha }W_c\end{array}\right)}W^T\right\Vert _F^2+\psi (W,W_c,H), \end{equation}</span><br />
              <span class="equation-number">(9)</span>
            </div>
          </div>where <span class="inline-equation"><span class="tex">$\alpha \in \mathbb {R}_+$</span></span> is a scale parameter. <em>ψ</em>(<em>W</em>, <em>W<sub>c</sub></em> , <em>H</em>) is a penalty function for SeaNMF, which will be specified for a different purpose, such as the sparsity. In this paper, we will primarily demonstrate that SeaNMF is an effective topic model for the short texts.
          <p></p>
        </section>
        <section id="sec-15">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.3.2</span> Optimization</h4>
            </div>
          </header>
          <p>Suppose <em>ψ</em>(<em>W</em>, <em>W<sub>c</sub></em> , <em>H</em>) = 0, a block coordinate descent (BCD) algorithm can be used to solve Eq. (<a class="eqn" href="#eq7">9</a>). We take the derivatives of the objective function with respect to the vectors <em>W</em> <sub>(:, <em>k</em>)</sub>, <em>W</em> <sub><em>c</em>(:, <em>k</em>)</sub> and <em>H</em> <sub>(:, <em>k</em>)</sub>. By setting them to zero, we get the updating rules as follows:</p>
          <p></p>
          <ul class="list-no-style">
            <li id="list2a" label="•">
              <strong>Update <em>W</em></strong>
              <div class="table-responsive" id="eq8">
                <div class="display-equation">
                  <span class="tex mytex">\begin{equation} \begin{split}&amp;W_{(:,k)}\leftarrow [W_{(:,k)}\\ &amp;+\frac{(AH)_{(:,k)}+\alpha (SW_c)_{(:,k)}-(WH^TH)_{(:,k)}-\alpha (WW_c^TW_c)_{(:,k)}}{(H^TH)_{(k,k)}+\alpha (W_c^TW_c)_{(k,k)}}]_+ \end{split}\end{equation}</span><br />
                  <span class="equation-number">(10)</span>
                </div>
              </div>
            </li>
            <li id="list2b" label="•">
              <strong>Update <em>W<sub>c</sub></em></strong>
              <div class="table-responsive" id="eq9">
                <div class="display-equation">
                  <span class="tex mytex">\begin{equation} \begin{split}W_{c(:,k)}&amp;\leftarrow \left[W_{c(:,k)}+\frac{(SW)_{(:,k)}-(W_cW^TW)_{(:,k)}}{(W^TW)_{(k,k)}}\right]_+ \end{split}\end{equation}</span><br />
                  <span class="equation-number">(11)</span>
                </div>
              </div>
            </li>
          </ul>
          <p></p>
          <p>From lemma <a class="enc" href="#enc1">3.1</a>, the document representation <em>H</em> is independent of <em>W<sub>c</sub></em> and <em>S</em>, therefore, the update rule for <em>H</em> is the same as Eq. (<a class="eqn" href="#eq3">3</a>).</p>
          <p>The BCD algorithm for SeaNMF is summarized in Algorithm 1 . We first build the term-document matrix <em>A</em> using the bag-of-word representation. Then, we calculate the semantic correlation matrix <em>S</em> by Eq. (<a class="eqn" href="#eq6">7</a>). The latent factor matrices <em>W</em>, <em>W<sub>c</sub></em> and <em>H</em> are initialized randomly with non-negative real numbers. Then, within each iteration, their coordinates will be updated column-wise. After each update, <em>W</em> <sub>(:, <em>k</em>)</sub> and <em>W</em> <sub><em>c</em>(:, <em>k</em>)</sub> will be normalized to have a unit ℓ<sub>2</sub>-norm. We will repeat this iteration until the algorithm converges.</p>
          <p><img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186009/images/www2018-18-img1.svg" class="img-responsive" alt="" longdesc="" /></p>
        </section>
        <section id="sec-16">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.3.3</span> Intuitive Explanation</h4>
            </div>
          </header>
          <p>We further demonstrate that Eq. (<a class="eqn" href="#eq8">10</a>) is equivalent to the following three updating procedures.</p>
          <div class="table-responsive" id="eq10">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} W_{(:,k)}^1\leftarrow W_{(:,k)}+ \frac{(AH)_{(:,k)}-(WH^TH)_{(:,k)}}{(H^TH)_{(k,k)}} \end{equation}</span><br />
              <span class="equation-number">(12)</span>
            </div>
          </div>
          <div class="table-responsive" id="eq11">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} W_{(:,k)}^2\leftarrow W_{(:,k)}+\frac{(SW_c)_{(:,k)}-(WW_c^TW_c)_{(:,k)}}{(W_c^TW_c)_{(k,k)}} \end{equation}</span><br />
              <span class="equation-number">(13)</span>
            </div>
          </div>
          <div class="table-responsive" id="eq12">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} W_{(:,k)}\leftarrow \left[\lambda W_{(:,k)}^1+(1-\lambda)W_{(:,k)}^2\right]_+ \end{equation}</span><br />
              <span class="equation-number">(14)</span>
            </div>
          </div>where <span class="inline-equation"><span class="tex">$\lambda =\frac{(H^TH)_{(k,k)}}{(H^TH)_{(k,k)}+\alpha (W_c^TW_c)_{(k,k)}}\in [0,1]$</span></span>. □
          <p></p>
          <p>As we can see, Eq. (<a class="eqn" href="#eq10">12</a>) is the same as Eq. (<a class="eqn" href="#eq2">2</a>) for the standard NMF. It tries to project the words in the same documents into the same region of the space using the term-document matrix. On the other hand, the Eq. (<a class="eqn" href="#eq11">13</a>) tries to move the words close to each other if they share the common context keywords. Therefore, it increases the coherence of the topics. For example, in Fig.&nbsp;<a class="fig" href="#fig1">1</a>, <em>w</em> <sub>1</sub> and <em>w</em> <sub>4</sub> do not appear in the same document. However, since they both have <em>w</em> <sub>2</sub> as context keyword, they may be semantically correlated. Take two short texts ”iphone ios system” and ”galaxy android system” as an example. ”iphone” and ”ios” do not appear in the second sentence, and ”galaxy” and ”android” do not appear in the first sentence. Thus, the correlations between ”iphone, ios” and ”galaxy, android” are minor in the standard NMF. However, in SeaNMF, the correlations are enhanced by Eq. (<a class="eqn" href="#eq11">13</a>) using the fact that they share the common keywords ”system”. The overall updating procedure, given in Eq. (<a class="eqn" href="#eq12">14</a>), is a linear combination of Eq. (<a class="eqn" href="#eq10">12</a>) and (<a class="eqn" href="#eq11">13</a>) which guarantees the top keywords in each topic are highly correlated.</p>
        </section>
        <section id="sec-17">
          <header>
            <div class="title-info">
              <h4><span class="section-number">3.3.4</span> Computational Complexity</h4>
            </div>
          </header>We have noticed that the proposed SeaNMF model maintains the same formation (Eq. (<a class="eqn" href="#eq7">9</a>)) as that of the standard NMF (Eq. (<a class="eqn" href="#eq1">1</a>)), therefore, its computational complexity is <em>O</em>((<em>M</em> + <em>N</em>)<em>MK</em>) within a single iteration of updating factor matrices. Since for short text corpus, the number of keywords is usually less than the number of documents, i.e, <em>M</em> &lt; <em>N</em>, we have <em>M</em> + <em>N</em> &lt; 2<em>N</em>. Therefore, the computational complexity of SeaNMF for short texts is reduced to <em>O</em>(<em>NMK</em>), which is the same as that of standard NMF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>]. However, due to the data sparsity for short texts, this complexity can be further reduced. In details, it can be seen from Eqs. (<a class="eqn" href="#eq8">10</a>), (<a class="eqn" href="#eq9">11</a>) and (<a class="eqn" href="#eq3">3</a>), the complexity is dominated by the calculations of <em>AH</em>, <em>SW<sub>c</sub></em> , <em>A<sup>T</sup>W</em>. Without considering the sparsity, their computational costs are <em>O</em>(<em>MNK</em>), <em>O</em>(<em>MMK</em>), <em>O</em>(<em>NMK</em>), respectively. However, since <em>A</em> and <em>S</em> are sparse matrices, which can be seen in Table <a class="tbl" href="#tab2">2</a>, we only need to multiply the non-zero elements with factor matrices. Suppose the numbers of non-zero elements in <em>A</em> and <em>S</em> are <em>z<sub>A</sub></em> and <em>z<sub>S</sub></em> , the complexity of calculating <em>AH</em>, <em>SW<sub>c</sub></em> , <em>A<sup>T</sup>W</em> will be <em>O</em>(<em>z<sub>A</sub>K</em>), <em>O</em>(<em>z<sub>S</sub>K</em>), <em>O</em>(<em>z<sub>A</sub>K</em>), respectively. Therefore, the proposed SeaNMF model has the complexity of <em>O</em>(max (<em>z<sub>A</sub></em> , <em>z<sub>S</sub></em> )<em>K</em>), where max (<em>z<sub>A</sub></em> , <em>z<sub>S</sub></em> ) ≪ <em>NM</em> and <em>K</em> ≪ min (<em>N</em>, <em>M</em>), which is much cheaper than the standard NMF.
          <p></p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span> <span class="table-title">Basic statistics of the datasets used in this paper.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Data Set</th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\#\text{docs}$</span></span></th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\#$</span></span> terms</th>
                  <th style="text-align:center;">density(A)</th>
                  <th style="text-align:center;">density(S)</th>
                  <th style="text-align:center;">doc-length</th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\#$</span></span> cats</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">Tag.News</td>
                  <td style="text-align:center;">28658</td>
                  <td style="text-align:center;">11525</td>
                  <td style="text-align:center;">1.2861<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">0.1369<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">18.14</td>
                  <td style="text-align:center;">7</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Yahoo.Ans</td>
                  <td style="text-align:center;">40754</td>
                  <td style="text-align:center;">4334</td>
                  <td style="text-align:center;">0.1997<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">0.0973<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">4.30</td>
                  <td style="text-align:center;">10</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Tweets</td>
                  <td style="text-align:center;">43413</td>
                  <td style="text-align:center;">10279</td>
                  <td style="text-align:center;">0.2744<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">0.0713<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">7.73</td>
                  <td style="text-align:center;">15</td>
                </tr>
                <tr>
                  <td style="text-align:center;">DBLP</td>
                  <td style="text-align:center;">15001</td>
                  <td style="text-align:center;">2447</td>
                  <td style="text-align:center;">0.7693%</td>
                  <td style="text-align:center;">0.2677<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">6.64</td>
                  <td style="text-align:center;">4</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Yahoo.CA</td>
                  <td style="text-align:center;">30686</td>
                  <td style="text-align:center;">4334</td>
                  <td style="text-align:center;">5.0532<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">0.7754<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">42.61</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;">ACM.IS</td>
                  <td style="text-align:center;">36392</td>
                  <td style="text-align:center;">2447</td>
                  <td style="text-align:center;">4.2667<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">1.9494<span class="inline-equation"><span class="tex">$\%$</span></span></td>
                  <td style="text-align:center;">77.49</td>
                  <td style="text-align:center;">-</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> The Sparse SeaNMF Model</h3>
          </div>
        </header>
        <p>In standard topic models, words are represented by dense vectors in a continuous real space. Specifically, in SeaNMF, we use the low-rank factor matrix <em>W</em> to encode the words. Introducing sparsity to <em>W</em> will reduce the active components of the word vectors, which will make it easy to interpret the topics.</p>
        <p>Considering a better interpretability of the model, we introduce the Sparse SeaNMF (SSeaNMF) model, where we apply the sparsity constraint to <em>W</em> and express the penalty function as follows:</p>
        <div class="table-responsive" id="Xeq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \psi (W,W_c,H)=\beta \Vert W\Vert _1^2, \end{equation}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>where ‖ · ‖<sub>1</sub> represents the ℓ<sub>1</sub>-norm. Since the sparsity is only applied to <em>W</em>, the BCD algorithm for updating <em>W</em> is modified to
        <div class="table-responsive" id="eq13">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \begin{split}&amp;W_{(:,k)}\leftarrow [W_{(:,k)}+\\ &amp;\frac{(AH)_{(:,k)}+\alpha (SW_c)_{(:,k)}-(WH^TH)_{(:,k)}-\alpha (WW_c^TW_c)_{(:,k)}+\beta \cdot 1_K}{(H^TH)_{(k,k)}+\alpha (W_c^TW_c)_{(k,k)}+\beta }]_+ \end{split}\end{equation}</span><br />
            <span class="equation-number">(16)</span>
          </div>
        </div>where <span class="inline-equation"><span class="tex">$1_K\in \mathbb {R}^{M\times 1}$</span></span> and <span class="inline-equation"><span class="tex">$1_{K(i,:)}=-\sum _{k=1}^{K}W_{(i,k)},\forall 1\le i\le M$</span></span> .
        <p></p>
        <p>Updating procedures for <em>W<sub>c</sub></em> and <em>H</em> remain the same as in Eq. (<a class="eqn" href="#eq9">11</a>) and Eq. (<a class="eqn" href="#eq3">3</a>), respectively. Compared with standard SeaNMF, calculating 1 <sub><em>K</em></sub> will not significantly increase the computational complexity of the algorithm.</p>
      </section>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experimental Results</h2>
        </div>
      </header>
      <p>In this section, we will demonstrate the promising performance of our models by conducting extensive experiments on different real-world datasets. We will introduce the datasets, evaluation metrics and baseline methods, and then explain different sets of results.</p>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Datasets Used</h3>
          </div>
        </header>
        <p>Our experiments are carried out on four real-world short text datasets corresponding to four types of applications, i.e., News, Questions&amp;Answers, Microblogs and Article headlines.</p>
        <p>•<strong>Tag.News</strong>. This data set is a part of the TagMyNews dataset<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>, which is composed of news, snippets and tweets. After removing the stopwords, we only keep the news with at most 25 keywords. The articles in the dataset belong to one of the following 7 categories: Business, Entertainment, Health, Sci&amp;Tech, Sport, US and World.</p>
        <p>•<strong>Yahoo.Ans</strong>. This dataset is a subset extracted from the Yahoo! Answers Manner Questions, version 2.0<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a>. In our dataset, we collect the subjects of the Questions from 10 different categories, including Financial Service, Diet&amp;Fitness, etc.</p>
        <p>•<strong>Tweets</strong>. The original Tweets dataset is collected and labeled by Zubiaga et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]. We select 15 different categories from the dataset, i.e., Arts, Business, Computers, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping, Society, Sports and World. For each category, we sample 2500 ∼ 3000 distinct tweets with at least two keywords.</p>
        <p>•<strong>DBLP</strong>. The raw DBLP dataset is available at <a class="fn" href="#fn6" id="foot-fn6"><sup>5</sup></a>. In our dataset, we collect the titles of the conference papers from the following 4 categories: Machine Learning, Data Mining, Information Retrieval and Database.</p>
        <p>•<strong>GoogleNews(300d)</strong>. This dataset is obtained from <a class="fn" href="#fn7" id="foot-fn7"><sup>6</sup></a>. It contains 3 million English words which are embedded into 300 dimensional latent space by performing the word2vec model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] on Google News corpus which consists of 3 billion running words. It is used to train the comparison method GPUDMM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>]</p>
        <p>Some basic statistics of these datasets are shown in Table&nbsp;<a class="tbl" href="#tab2">2</a>. In this table, ‘<span class="inline-equation"><span class="tex">$\#$</span></span> docs’ represents the number of documents in each dataset. ‘<span class="inline-equation"><span class="tex">$\#$</span></span> terms’ is the number of keywords in the vocabulary. ‘density’ is defined as <span class="inline-equation"><span class="tex">$\frac{\#\text{non-zero}}{\#\text{docs}\cdot \#\text{terms}}$</span></span> , where <span class="inline-equation"><span class="tex">$\#$</span></span> non-zero is the number of non-zero elements in the matrix. The ‘density(A)’ and ‘density(S)’ represent the density of term-document matrix (<em>A</em>) and semantic correlation matrix (S), respectively. ‘doc-length’ represents the average length of the documents. ‘<span class="inline-equation"><span class="tex">$\#$</span></span> cats’ denotes the number of distinct categories. In our experiments, we also leverage the following two datasets as external sources in the evaluations. It should be noted that they are NOT used to train the models.</p>
        <p>• <strong>Yahoo.CA</strong>. From the Yahoo! Answers Manner Questions, version 2.0, we collect the content and best answer for each question, and construct a new regular-sized document sets, namely, Yahoo.CA.</p>
        <p>• <strong>ACM.IS</strong>. This dataset is part of ACM IS abstract dataset<a class="fn" href="#fn8" id="foot-fn8"><sup>7</sup></a>, which contains the abstracts of ACM information system papers published between 2002 and 2011.</p>
      </section>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Evaluation Metrics</h3>
          </div>
        </header>
        <p>In this paper, we will use the topic coherence and document classification accuracy for our evaluation.</p>
        <p>• <strong>Topic Coherence</strong>. Given a topic <em>k</em>, the PMI score is calculated by the following equation:</p>
        <div class="table-responsive" id="eq14">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} C_k=\frac{2}{\mathcal {N}(\mathcal {N}-1)}\sum _{1\le i{\lt} j\le \mathcal {N}}\log \frac{p(w_i,w_j)}{p(w_i)p(w_j)} \end{equation}</span><br />
            <span class="equation-number">(17)</span>
          </div>
        </div>where <span class="inline-equation"><span class="tex">$\mathcal {N}$</span></span> is the number of most probable words in this topic.
        <p></p>
        <p><span class="inline-equation"><span class="tex">$p(w_i,w_j)={\#(w_i,w_j)}/{\mathcal {D}}$</span></span> is the probability of the words <em>w<sub>i</sub></em> and <em>w<sub>j</sub></em> co-occurring in the same document. <span class="inline-equation"><span class="tex">$p(w_i)={\#(w_i)}/{\mathcal {D}}$</span></span> and <span class="inline-equation"><span class="tex">$p(w_j)={\#(w_j)}/{\mathcal {D}}$</span></span> are the marginal probabilities. The average PMI score over all the topics will be used to evaluate the quality of the topic models. However, Quan et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>] have shown that the average PMI score, that works well for regular-sized documents, is still problematic for short texts, which means a gold-standard topic may be assigned with a low PMI score. In this paper, we leverage the following strategy to overcome this problem. First, we calculate the PMI score based on the four short text datasets as usual. Second, for Yahoo.Ans and DBLP datasets, we calculate the PMI score based on the external corpus, i.e., Yahoo.CA and ACM.IS, which are composed of regular documents. The results in both experiments will be used to demonstrate the effectiveness of our models. We emphasize that Yahoo.CA and ACM.IS do not participate in the training of our models. In our experiments, we set <span class="inline-equation"><span class="tex">$\mathcal {N}=10$</span></span> . It also should be noted that the difference between Eq. (<a class="eqn" href="#eq14">17</a>) and the PMI score used in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>] is that we do not consider the co-occurrence of the same word.</p>
        <p>• <strong>Document Classification</strong>. Another popular way to evaluate the effectiveness of the topic models is to leverage the latent document representations for external tasks. In our experiments, we will conduct short text classification on all the datasets, whose documents have been labeled. A five-fold cross validation is used to evaluate the performance of the classification, where each corpus is randomly split into training and testing sets with a ratio of 4: 1. Then, the documents are classified by LIBLINEAR package <a class="fn" href="#fn9" id="foot-fn9"><sup>8</sup></a> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. Finally, the quality of the classification is measured by averaged precision, recall and F-score.</p>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Comparison Methods</h3>
          </div>
        </header>
        <p>We compare the performance of our models with the following state-of-the-art methods.</p>
        <p>• <strong>Latent Dirichlet Allocation (LDA)</strong>. LDA [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] is a well-known baseline method in the topic modeling which performs well on the regular-sized documents. In this paper, we use a Python implementation<a class="fn" href="#fn10" id="foot-fn10"><sup>9</sup></a> of LDA with a collapsed Gibbs sampling.</p>
        <p>• <strong>Non-negative Matrix Factorization (NMF)</strong>. NMF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] is an unsupervised method that can perform dimension reduction and clustering simultaneously. It has found applications in a range of areas, including topic modeling. In our experiments, the NMF<a class="fn" href="#fn11" id="foot-fn11"><sup>10</sup></a> is implemented in Python with a block coordinate descent algorithm.</p>
        <p>• <strong>Pseudo-document-based Topic Model (PTM)</strong>. PTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>] introduces <em>pseudo-documents</em> into the topic model, which implicitly aggregates short texts without auxiliary information. It is one of the most recent methods for discovering topics from the short text corpus.</p>
        <p>• <strong>GPUDMM</strong>. The GPUDMM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>] for short-text topic modeling is based on the Dirichlet Multinomial Mixture model. During the sampling process using the generalized Pólya urn model, it promotes the semantically related words in each topic by leveraging the external word semantic knowledge, i.e., word vectors, from very large corpus. In this paper, we will use the GoogleNews(300d) dataset as the external resource. In our experiments, the default number of topics is set to <em>K</em> = 100. For LDA, we set parameters <em>α</em> = 0.1 and <em>β</em> = 0.01, since the weak prior can give a better performance for short texts [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]. For PTM and GPUDMM, we use the default hyper-parameter settings. In details, we set parameters <em>α</em> = 0.1, <em>λ</em> = 0.1 and <em>β</em> = 0.01 for PTM. For GPUDMM, we set parameters <em>β</em> = 0.1. In LDA, PTM and GPUDMM, Gibbs sampling is run for 2000 iterations. For SeaNMF, we set <em>α</em> = 1.0 for Tag.News and Tweets and <em>α</em> = 0.1 for Yahoo.Ans and DBLP. To calculate <em>S</em>, we set <em>κ</em> = 1.0 and <em>γ</em> = 1.0. In SSeaNMF, we set <em>β</em> = 0.1. We also set the seed for the random number generator to 0 for NMF, SeaNMF and SSeaNMF to make sure the results are consistent and independent of random initial states. The codes for SeaNMF has been publicly available at <a class="fn" href="#fn12" id="foot-fn12"><sup>11</sup></a>.</p>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Results</h3>
          </div>
        </header>
        <section id="sec-24">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.4.1</span> Topic Coherence Results</h4>
            </div>
          </header>
          <p>We first present the topic coherence results of our models and other comparison methods in Tables <a class="tbl" href="#tab3">3</a> and <a class="tbl" href="#tab4">4</a>. We use the bold font to show the best performance values and the underline to highlight the second best values.</p>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span> <span class="table-title">Topic coherence results in terms of PMI.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"></th>
                  <th style="text-align:center;">Tag.News</th>
                  <th style="text-align:center;">Yahoo.Ans</th>
                  <th style="text-align:center;">Tweets</th>
                  <th style="text-align:center;">DBLP</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">LDA</td>
                  <td style="text-align:center;">1.5048</td>
                  <td style="text-align:center;">1.2957</td>
                  <td style="text-align:center;">1.1637</td>
                  <td style="text-align:center;">0.9346</td>
                </tr>
                <tr>
                  <td style="text-align:left;">NMF</td>
                  <td style="text-align:center;">1.6414</td>
                  <td style="text-align:center;">1.1394</td>
                  <td style="text-align:center;">1.8045</td>
                  <td style="text-align:center;">0.9184</td>
                </tr>
                <tr>
                  <td style="text-align:left;">PTM</td>
                  <td style="text-align:center;">1.6628</td>
                  <td style="text-align:center;">1.1311</td>
                  <td style="text-align:center;">1.3745</td>
                  <td style="text-align:center;">0.8505</td>
                </tr>
                <tr>
                  <td style="text-align:left;">GPUDMM</td>
                  <td style="text-align:center;">0.9751</td>
                  <td style="text-align:center;">0.5798</td>
                  <td style="text-align:center;">0.9213</td>
                  <td style="text-align:center;">0.2815</td>
                </tr>
                <tr>
                  <td style="text-align:left;">SeaNMF</td>
                  <td style="text-align:center;"><strong>3.6318</strong></td>
                  <td style="text-align:center;"><strong>1.7553</strong></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">4.1477</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">1.6137</span></td>
                </tr>
                <tr>
                  <td style="text-align:left;">SSeaNMF</td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">3.6053</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">1.6081</span></td>
                  <td style="text-align:center;"><strong>4.1979</strong></td>
                  <td style="text-align:center;"><strong>1.6239</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab4">
            <div class="table-caption">
              <span class="table-number">Table 4:</span> <span class="table-title">Topic coherence results with Yahoo.CA and ACM.IS.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;"></th>
                  <th style="text-align:center;">Yahoo.Ans/Yahoo.CA</th>
                  <th style="text-align:center;">DBLP/ACM.IS</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">LDA</td>
                  <td style="text-align:center;">0.6540</td>
                  <td style="text-align:center;">0.4282</td>
                </tr>
                <tr>
                  <td style="text-align:center;">NMF</td>
                  <td style="text-align:center;">0.5261</td>
                  <td style="text-align:center;">0.3626</td>
                </tr>
                <tr>
                  <td style="text-align:center;">PTM</td>
                  <td style="text-align:center;">0.6504</td>
                  <td style="text-align:center;">0.4431</td>
                </tr>
                <tr>
                  <td style="text-align:center;">GPUDMM</td>
                  <td style="text-align:center;">0.3302</td>
                  <td style="text-align:center;">-0.0159</td>
                </tr>
                <tr>
                  <td style="text-align:center;">SeaNMF</td>
                  <td style="text-align:center;"><strong>1.1094</strong></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6641</span></td>
                </tr>
                <tr>
                  <td style="text-align:center;">SSeaNMF</td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">1.0188</span></td>
                  <td style="text-align:center;"><strong>0.6447</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>From Table <a class="tbl" href="#tab3">3</a>, we observe that our models outperform the standard NMF, which indicates that SeaNMF is effective for learning topics from short texts. Compared with LDA and recent PTM, SeaNMF shows significant improvements, which implies that our models discover more coherent topics. To better understand the poor performance of GPUDMM in all cases, we visualize the top keywords in each topic, where we find that many top keywords (e.g. ‘extraction’, ‘extracting’ and ‘extract’) are semantically correlated, but they do not tend to appear in the same document. Another possible reason is that the word semantic relationships in Google News and other datasets are different, so that the general semantics knowledge from Google News may not work well on discovering topics from these datasets.</p>
          <p>As discussed in topic coherence section, since the PMI scores are problematic for short texts, we also evaluate topic coherence based on external corpus which are composed of long documents. After training different models on Yahoo.Ans, we extract the top keywords from each topic, and then calculate the PMI scores based on the Yahoo.CA corpus. Similarly, for DBLP, the PMI scores are calculated based on ACM.IS dataset. The results obtained on these external corpus are presented in Table <a class="tbl" href="#tab4">4</a>. From the table, we find that SeaNMF outperforms the other baseline methods. Therefore, from our topic coherence results, we demonstrate that by leveraging the word semantic correlations, SeaNMF can capture more coherent topics from short texts.</p>
        </section>
        <section id="sec-25">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.4.2</span> Document Classification Results</h4>
            </div>
          </header>
          <p>In addition to the topic coherence, we also compared the document classification performance of different methods. As we can see from Table <a class="tbl" href="#tab5">5</a>, both the best and the second best results are achieved by our models on Tag.News, Yahoo.Ans and Tweets. This demonstrates that our models are effective in the document classification for short texts. Compared with the conventional topic models, such as LDA and NMF, SeaNMF has a significant improvement in terms of different classification measures. The SeaNMF models also perform better than PTM, which attempts to capture the cross-document word correlations by aggregating similar short texts into pseudo documents. This comparison demonstrates that the word correlations obtained from skip-gram view of the corpus play an important role in capturing high quality semantics, given the performance of standard NMF is not as good as that of LDA. In Table&nbsp;<a class="tbl" href="#tab5">5</a>, we also observe that the GPUDMM model performs better than the other baseline methods. The difference between GPUDMM and SeaNMF is that GPUDMM explicitly makes use of the term correlations obtained from the pre-trained word representations on the external large corpus, while SeaNMF is only based on the short text corpus itself. Thus, given an external resource, like Google News, the performance of GPUDMM cannot be guaranteed across different short texts. In summary, the classification results have shown that SeaNMF is a superior topic model for short texts, even without using the auxiliary information or external sources, or aggregating the short texts.</p>
          <div class="table-responsive" id="tab5">
            <div class="table-caption">
              <span class="table-number">Table 5:</span> <span class="table-title">Performance comparison of various methods on document classification.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;"></th>
                  <th colspan="3" style="text-align:center;">Tag.News</th>
                  <th colspan="3" style="text-align:center;">Yahoo.Ans</th>
                  <th colspan="3" style="text-align:center;">Tweets</th>
                  <th colspan="3" style="text-align:center;">DBLP</th>
                </tr>
                <tr>
                  <th style="text-align:center;"></th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F-score</th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F-score</th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F-score</th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F-score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">LDA</td>
                  <td style="text-align:center;">0.7323</td>
                  <td style="text-align:center;">0.7184</td>
                  <td style="text-align:center;">0.7239</td>
                  <td style="text-align:center;">0.5929</td>
                  <td style="text-align:center;">0.5738</td>
                  <td style="text-align:center;">0.5659</td>
                  <td style="text-align:center;">0.3827</td>
                  <td style="text-align:center;">0.3867</td>
                  <td style="text-align:center;">0.3758</td>
                  <td style="text-align:center;">0.6081</td>
                  <td style="text-align:center;">0.5973</td>
                  <td style="text-align:center;">0.5994</td>
                </tr>
                <tr>
                  <td style="text-align:center;">NMF</td>
                  <td style="text-align:center;">0.6763</td>
                  <td style="text-align:center;">0.6371</td>
                  <td style="text-align:center;">0.6507</td>
                  <td style="text-align:center;">0.6303</td>
                  <td style="text-align:center;">0.5470</td>
                  <td style="text-align:center;">0.5706</td>
                  <td style="text-align:center;">0.3677</td>
                  <td style="text-align:center;">0.3517</td>
                  <td style="text-align:center;">0.3506</td>
                  <td style="text-align:center;">0.6393</td>
                  <td style="text-align:center;">0.6226</td>
                  <td style="text-align:center;">0.6273</td>
                </tr>
                <tr>
                  <td style="text-align:center;">PTM</td>
                  <td style="text-align:center;">0.7525</td>
                  <td style="text-align:center;">0.7396</td>
                  <td style="text-align:center;">0.7444</td>
                  <td style="text-align:center;">0.6390</td>
                  <td style="text-align:center;">0.6038</td>
                  <td style="text-align:center;">0.6026</td>
                  <td style="text-align:center;">0.3941</td>
                  <td style="text-align:center;">0.3838</td>
                  <td style="text-align:center;">0.3786</td>
                  <td style="text-align:center;">0.6424</td>
                  <td style="text-align:center;">0.6367</td>
                  <td style="text-align:center;">0.6379</td>
                </tr>
                <tr>
                  <td style="text-align:center;">GPUDMM</td>
                  <td style="text-align:center;">0.7843</td>
                  <td style="text-align:center;">0.7712</td>
                  <td style="text-align:center;">0.7760</td>
                  <td style="text-align:center;">0.5954</td>
                  <td style="text-align:center;">0.6308</td>
                  <td style="text-align:center;">0.5995</td>
                  <td style="text-align:center;">0.3985</td>
                  <td style="text-align:center;">0.4066</td>
                  <td style="text-align:center;">0.3903</td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6670</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6573</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6586</span></td>
                </tr>
                <tr>
                  <td style="text-align:center;">SeaNMF</td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.7868</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.7786</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.7821</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6566</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6338</span></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.6366</span></td>
                  <td style="text-align:center;"><strong>0.4648</strong></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.4555</span></td>
                  <td style="text-align:center;"><strong>0.4527</strong></td>
                  <td style="text-align:center;">0.6648</td>
                  <td style="text-align:center;">0.6552</td>
                  <td style="text-align:center;">0.6575</td>
                </tr>
                <tr>
                  <td style="text-align:center;">SSeaNMF</td>
                  <td style="text-align:center;"><strong>0.7894</strong></td>
                  <td style="text-align:center;"><strong>0.7801</strong></td>
                  <td style="text-align:center;"><strong>0.7841</strong></td>
                  <td style="text-align:center;"><strong>0.6603</strong></td>
                  <td style="text-align:center;"><strong>0.6369</strong></td>
                  <td style="text-align:center;"><strong>0.6401</strong></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.4592</span></td>
                  <td style="text-align:center;"><strong>0.4568</strong></td>
                  <td style="text-align:center;"><span style="text-decoration: underline;">0.4516</span></td>
                  <td style="text-align:center;"><strong>0.6700</strong></td>
                  <td style="text-align:center;"><strong>0.6613</strong></td>
                  <td style="text-align:center;"><strong>0.6636</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>It should be noted that the results based on the Tweets dataset are more reliable because the number of tweets in different categories is almost the same, which avoids the problems caused by the so-called ‘imbalanced classes’. As we can see in Tables <a class="tbl" href="#tab5">5</a>, SeaNMF has on an average more than 12% improvements over the other baseline methods with respect to precision, recall, F-score.</p>
        </section>
      </section>
      <section id="sec-26">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Parameter Sensitivity</h3>
          </div>
        </header>
        <p>In this section, we will demonstrate the stability and consistency of SeaNMF by varying the parameters <em>α</em>, <em>κ</em> and <em>γ</em>.</p>
        <p>The parameter <em>α</em> is the weight for factorizing the word semantic correlation matrix. Here, we study the effects of <em>α</em> on the topic coherence and classification accuracy on DBLP. It can be seen from Fig. <a class="fig" href="#fig3">3</a> that the topic coherence increases rapidly as we increase the weight when <em>α</em> ∈ (0, 1]. However, it stays almost constant after <em>α</em> &gt; 1. This clearly shows that SeaNMF is effective for short texts just because it leverages the word semantic correlations.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186009/images/www2018-18-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Network Visualizations of the keywords obtained by the NMF and SeaNMF models on Yahoo.Ans and DBLP datasets.</span>
          </div>
        </figure>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186009/images/www2018-18-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Topic coherence and classification performance by varying <em>α</em>, <em>κ</em> and <em>γ</em>.</span>
          </div>
        </figure>
        <p></p>
        <p>We also observe that a better topic coherence does not imply better document classification performance. As we can see in Fig.&nbsp;<a class="fig" href="#fig3">3</a>, the F-score decreases as <em>α</em> increasing. Therefore, for a short text collection, a highly coherent topic is not the same as a high quality topic which is consistent with the findings of others in the literature [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>]. We also notice that the F-score does not significantly change with <em>α</em>, i.e., the change is less than 0.02. Hence, SeaNMF is a stable topic model for short texts.</p>
        <p>The parameters <em>κ</em> and <em>α</em> play an important role in constructing the semantic correlation matrix <em>S</em>. <em>κ</em> affects the sparsity of <em>S</em>. Large <em>κ</em> leads to very sparse <em>S</em> and sparse <em>S</em> implies that the words are less correlated. As shown in Fig. <a class="fig" href="#fig3">3</a>, the F-score is reduced when we increase <em>κ</em>. <em>γ</em> is a smoothing factor for the probability of sampling a context. From the figure, the F-score is slightly improved when <em>γ</em> is increased. To summarize, both parameters affect the quality of topics by changing the semantic correlation matrix. It implies that the word semantic correlations are critical to SeaNMF.</p>
      </section>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.6</span> Semantic Analysis of Topics</h3>
          </div>
        </header>
        <p>In this section, we show that the topics discovered by SeaNMF are meaningful by visualizing the top keywords. They will be compared with the top keywords given by the standard NMF method.</p>
        <p>After training the NMF model on Yahoo.Ans and DBLP datasets, we select the topics with high PMI scores. Then, we find the most similar topic obtained from SeaNMF for each of them based on the top keywords. The lists of the top keywords in the selected topics obtained are shown in Table <a class="tbl" href="#tab6">6</a>. As we can see, two topics for Yahoo.Ans are about cooking and the technical problems on downloading or transferring songs. The two topics selected from DBLP are on publications related with machine learning and data mining.</p>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class="table-title">Discovered topics by the proposed method. The word is colored in red if its degree is less than 2. The numbers in the parentheses represent the frequency of the word in the corpus. NMF-<em>k</em> corresponds to the <em>k</em>-th topic discovered by the NMF model.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="4" style="text-align:center;">Yahoo.Ans</th>
                <th colspan="4" style="text-align:center;">DBLP</th>
              </tr>
              <tr>
                <th style="text-align:center;">Category</th>
                <th colspan="2" style="text-align:center;">Cooking and Recipes</th>
                <th colspan="2" style="text-align:center;">Blues</th>
                <th colspan="2" style="text-align:center;">Machine Learning</th>
                <th colspan="2" style="text-align:center;">Data Mining</th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">NMF-24</th>
                <th style="text-align:center;">SeaNMF-47</th>
                <th style="text-align:center;">NMF-54</th>
                <th style="text-align:center;">SeaNMF-50</th>
                <th style="text-align:center;">NMF-100</th>
                <th style="text-align:center;">SeaNMF-45</th>
                <th style="text-align:center;">NMF-72</th>
                <th style="text-align:center;">SeaNMF-98</th>
              </tr>
              <tr>
                <th style="text-align:center;">PMI</th>
                <th style="text-align:center;">2.7291</th>
                <th style="text-align:center;">3.1713</th>
                <th style="text-align:center;">2.6674</th>
                <th style="text-align:center;">3.3517</th>
                <th style="text-align:center;">1.4570</th>
                <th style="text-align:center;">1.7215</th>
                <th style="text-align:center;">1.2636</th>
                <th style="text-align:center;">1.9810</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Top-10 keywords</td>
                <td style="text-align:center;">cook(381) chicken(168) turkey(72) roast(54) rice(80) oven(67) beef(56) pork(40) steak(50) microwave(51)</td>
                <td style="text-align:center;">cook(381) roast(54) oven(67) pork(40) beef(56) grill(50) turkey(72) steak(50) tender(11) ribs(16)</td>
                <td style="text-align:center;">songs(257) ipod(143) download(179) computer(216) itunes(54) player(94) limewire(70) transfer(75) add(138) convert(118)</td>
                <td style="text-align:center;">songs(257) ipod(143) computer(216) download(179) transfer(75) onto(51) itunes(54) limewire(70) video(71) nano(31)</td>
                <td style="text-align:center;">support(228) vector(150) machines(95) machine(116) regression(127) class(104) training(79) kernel(151) incremental(105) weighted(67)</td>
                <td style="text-align:center;">support(228) vector(150) machines(95) machine(116) regression(127) kernel(151) training(79) confidence(19) reduced(5) weighted(67)</td>
                <td style="text-align:center;">filtering(147) collaborative(122) content(166) scalable(130) combining(118) spam(37) recommendation(47) personalized(62) item(29) techniques(115)</td>
                <td style="text-align:center;">filtering(147) collaborative(122) recommendation(47) personalized(62) spam(37) recommender(27) injection(5) style(15) rating(8) ratings(6)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>To demonstrate the topics discovered by SeaNMF are more semantically correlated, we use the selected top keywords in each topic to construct the word networks. More specifically, suppose the top keyword list is denoted as <span class="inline-equation"><span class="tex">$\lbrace w_i\rbrace _{i=1}^{10}$</span></span> , we first find 30 most correlated words <span class="inline-equation"><span class="tex">$\lbrace v_j\rbrace _{j=1}^{30}$</span></span> for each keyword <span class="inline-equation"><span class="tex">$w_{i_0}$</span></span> based on the positive PMI matrix. If a keyword <span class="inline-equation"><span class="tex">$w_{i_1}\in \lbrace w_i\rbrace \cap \lbrace v_j\rbrace$</span></span> , <em>i</em> <sub>1</sub> ≠ <em>i</em> <sub>0</sub>, we draw an edge from <span class="inline-equation"><span class="tex">$w_{i_0}$</span></span> to <span class="inline-equation"><span class="tex">$w_{i_1}$</span></span> .</p>
        <p>As we can see from Fig. <a class="fig" href="#fig2">2</a>, all the graphs for the standard NMF model are very sparse. Some keywords with higher frequency in the corpus have lower degree which means that they are less correlated with the other words. For example, the frequency of ‘chicken’ is high, however, its most correlated words do not contain the other keywords and it is not in the most correlated word lists of the other keywords. In the standard topic modeling, these keywords might be viewed as noise. In Table <a class="tbl" href="#tab6">6</a>, the keywords with degree less than two are colored in red. We can see that the topics obtained from the standard NMF model are noisy. On the other hand, we conduct the same experiments on our SeaNMF model. From Table <a class="tbl" href="#tab6">6</a> and Fig. <a class="fig" href="#fig2">2</a>, we can see that topics discovered by our SeaNMF model have less noisy words and the top keywords are more correlated. Therefore, these semantic analysis results demonstrate that the SeaNMF model can discover meaningful and consistent topics for short texts.</p>
      </section>
    </section>
    <section id="sec-28">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we introduce a semantics-assisted NMF (SeaNMF) model to discover topics for the short texts. The proposed model leverages the word-context semantic correlations in the training, which potentially overcomes the problem of lacking context that arises due to the data sparsity. The semantic correlations between the words and their contexts are learned from the skip-gram view of the corpus, which was demonstrated to be effective for revealing word semantic relationships. We use a block coordinate descent algorithm to solve our SeaNMF model. To achieve a better model interpretability, a sparse SeaNMF model is also developed. We compared the performance of our models with several other state-of-the-art methods on four real-world short text datasets. The quantitative evaluations demonstrate that our models outperform other methods with respect to widely used metrics such as the topic coherence and document classification accuracy. The parameter sensitivity results demonstrate the stability and consistency of the performance of our SeaNMF model. The qualitative results show that the topics discovered by SeaNMF are meaningful and their top keywords are more semantically correlated. Hence, we conclude that the proposed SeaNMF is an effective topic model for short texts.</p>
    </section>
    <section id="sec-29">
      <header>
        <div class="title-info">
          <h2>Acknowledgments</h2>
        </div>
      </header>
      <p>This work was supported in part by the National Science Foundation grants IIS-1619028, IIS-1707498 and IIS-1646881, and the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIP) (No. NRF-2016R1C1B2015924).</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">David&nbsp;M Blei, Andrew&nbsp;Y Ng, and Michael&nbsp;I Jordan. 2003. Latent dirichlet allocation. <em><em>Journal of machine Learning research</em></em> 3, Jan (2003), 993–1022.</li>
        <li id="BibPLXBIB0002" label="[2]">Jaegul Choo, Changhyun Lee, Chandan&nbsp;K Reddy, and Haesun Park. 2013. Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization. <em><em>IEEE transactions on visualization and computer graphics</em></em> 19, 12(2013), 1992–2001.</li>
        <li id="BibPLXBIB0003" label="[3]">Jaegul Choo, Changhyun Lee, Chandan&nbsp;K Reddy, and Haesun Park. 2015. Weakly supervised nonnegative matrix factorization for user-driven clustering. <em><em>Data Mining and Knowledge Discovery</em></em> 29, 6 (2015), 1598–1621.</li>
        <li id="BibPLXBIB0004" label="[4]">Andrzej Cichocki, Rafal Zdunek, Anh&nbsp;Huy Phan, and Shun-ichi Amari. 2009. <em><em>Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation</em></em> . John Wiley &amp; Sons.</li>
        <li id="BibPLXBIB0005" label="[5]">Scott Deerwester, Susan&nbsp;T Dumais, George&nbsp;W Furnas, Thomas&nbsp;K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. <em><em>Journal of the American society for information science</em></em> 41, 6(1990), 391–407.</li>
        <li id="BibPLXBIB0006" label="[6]">Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. <em><em>Journal of machine learning research</em></em> 9, Aug (2008), 1871–1874.</li>
        <li id="BibPLXBIB0007" label="[7]">Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In <em><em>Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</em></em> . ACM, 50–57.</li>
        <li id="BibPLXBIB0008" label="[8]">Liangjie Hong and Brian&nbsp;D Davison. 2010. Empirical study of topic modeling in twitter. In <em><em>Proceedings of the first workshop on social media analytics</em></em> . ACM, 80–88.</li>
        <li id="BibPLXBIB0009" label="[9]">Hannah Kim, Jaegul Choo, Jingu Kim, Chandan&nbsp;K Reddy, and Haesun Park. 2015. Simultaneous discovery of common and discriminative topics via joint nonnegative matrix factorization. <em>In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> . ACM, 567–576.</li>
        <li id="BibPLXBIB0010" label="[10]">Jingu Kim, Yunlong He, and Haesun Park. 2014. Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework. <em><em>Journal of Global Optimization</em></em> 58, 2 (2014), 285–319.</li>
        <li id="BibPLXBIB0011" label="[11]">Da Kuang, Jaegul Choo, and Haesun Park. 2015. Nonnegative matrix factorization for interactive topic modeling and document clustering. <em>In <em>Partitional Clustering Algorithms</em></em> . Springer, 215–243.</li>
        <li id="BibPLXBIB0012" label="[12]">Da Kuang, Chris Ding, and Haesun Park. 2012. Symmetric nonnegative matrix factorization for graph clustering. <em>In <em>Proceedings of the 2012 SIAM international conference on data mining</em></em> . SIAM, 106–117.</li>
        <li id="BibPLXBIB0013" label="[13]">Da Kuang, Sangwoon Yun, and Haesun Park. 2015. SymNMF: nonnegative low-rank approximation of a similarity matrix for graph clustering. <em><em>Journal of Global Optimization</em></em> 62, 3 (2015), 545–574.</li>
        <li id="BibPLXBIB0014" label="[14]">Daniel&nbsp;D Lee and H&nbsp;Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. <em><em>Nature</em></em> 401, 6755 (1999), 788–791.</li>
        <li id="BibPLXBIB0015" label="[15]">Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. <em>In <em>Advances in Neural Information Processing Systems 27</em></em> . Curran Associates, Inc., 2177–2185.</li>
        <li id="BibPLXBIB0016" label="[16]">Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. <em><em>Transactions of the Association for Computational Linguistics</em></em> 3 (2015), 211–225.</li>
        <li id="BibPLXBIB0017" label="[17]">Chenliang Li, Haoran Wang, Zhiqian Zhang, Aixin Sun, and Zongyang Ma. 2016. Topic Modeling for Short Texts with Auxiliary Word Embeddings. <em>In <em>Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</em></em> . ACM, 165–174.</li>
        <li id="BibPLXBIB0018" label="[18]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. <em><em>arXiv preprint arXiv:1301.3781</em></em> (2013).</li>
        <li id="BibPLXBIB0019" label="[19]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&nbsp;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. <em>In <em>Advances in neural information processing systems</em></em> . 3111–3119.</li>
        <li id="BibPLXBIB0020" label="[20]">Jeffrey Pennington, Richard Socher, and Christopher&nbsp;D. Manning. 2014. GloVe: Global Vectors for Word Representation. <em>In <em>Empirical Methods in Natural Language Processing (EMNLP)</em></em> . 1532–1543.</li>
        <li id="BibPLXBIB0021" label="[21]">Xiaojun Quan, Chunyu Kit, Yong Ge, and Sinno&nbsp;Jialin Pan. 2015. Short and Sparse Text Topic Modeling via Self-aggregation. <em>In <em>Proceedings of the 24th International Conference on Artificial Intelligence</em></em> (IJCAI’15). AAAI Press, 2270–2276.</li>
        <li id="BibPLXBIB0022" label="[22]">Vivek Kumar&nbsp;Rangarajan Sridhar. 2015. Unsupervised topic modeling for short texts using distributed representations of words. <em>In <em>Proceedings of NAACL-HLT</em></em> . 192–200.</li>
        <li id="BibPLXBIB0023" label="[23]">Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Ferhatosmanoglu, and Murat Demirbas. 2010. Short text classification in twitter to improve information filtering. <em>In <em>Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</em></em> . ACM, 841–842.</li>
        <li id="BibPLXBIB0024" label="[24]">Zhongyuan Wang and Haixun Wang. 2016. Understanding Short Texts. <em>In <em>ACL 2016 Tutorial</em></em> . ACL, 1–18.</li>
        <li id="BibPLXBIB0025" label="[25]">Guangxu Xun, Vishrawas Gopalakrishnan, Fenglong Ma, Yaliang Li, Jing Gao, and Aidong Zhang. 2016. Topic Discovery for Short Texts Using Word Embeddings. <em>In <em>Data Mining (ICDM), 2016 IEEE 16th International Conference on</em></em> . IEEE, 1299–1304.</li>
        <li id="BibPLXBIB0026" label="[26]">Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2013. A biterm topic model for short texts. <em>In <em>Proceedings of the 22nd international conference on World Wide Web</em></em> . ACM, 1445–1456.</li>
        <li id="BibPLXBIB0027" label="[27]">Xiaohui Yan, Jiafeng Guo, Shenghua Liu, Xueqi Cheng, and Yanfeng Wang. 2013. Learning topics in short texts by non-negative matrix factorization on term correlation matrix. <em>In <em>Proceedings of the 2013 SIAM International Conference on Data Mining</em></em> . SIAM, 749–757.</li>
        <li id="BibPLXBIB0028" label="[28]">Wayne&nbsp;Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011. Comparing twitter and traditional media using topic models. <em>In <em>European Conference on Information Retrieval</em></em> . Springer, 338–349.</li>
        <li id="BibPLXBIB0029" label="[29]">Arkaitz Zubiaga and Heng Ji. 2013. Harnessing web page directories for large-scale classification of tweets. <em>In <em>Proceedings of the 22nd International Conference on World Wide Web</em></em> . ACM, 225–226.</li>
        <li id="BibPLXBIB0030" label="[30]">Yuan Zuo, Junjie Wu, Hui Zhang, Hao Lin, Fei Wang, Ke Xu, and Hui Xiong. 2016. Topic Modeling of Short Texts: A Pseudo-Document View. In <em><em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> . ACM, 2105–2114.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>*</sup></a>Jaegul Choo is the corresponding author.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="http://nlp.stanford.edu/projects/glove/">http://nlp.stanford.edu/projects/glove/</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="http://acube.di.unipi.it/datasets/">http://acube.di.unipi.it/datasets/</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class="link-inline force-break" href="https://webscope.sandbox.yahoo.com/catalog.php?datatype=l">https://webscope.sandbox.yahoo.com/catalog.php?datatype=l</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class="link-inline force-break" href="http://dblp.uni-trier.de/">http://dblp.uni-trier.de/</a></p>
    <p id="fn7"><a href="#foot-fn7"><sup>6</sup></a><a class="link-inline force-break" href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>7</sup></a><a class="link-inline force-break" href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27695">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27695</a></p>
    <p id="fn9"><a href="#foot-fn9"><sup>8</sup></a><a class="link-inline force-break" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></p>
    <p id="fn10"><a href="#foot-fn10"><sup>9</sup></a><a class="link-inline force-break" href="https://github.com/shuyo/iir/tree/master/lda">https://github.com/shuyo/iir/tree/master/lda</a></p>
    <p id="fn11"><a href="#foot-fn11"><sup>10</sup></a><a class="link-inline force-break" href="https://github.com/kimjingu/nonnegfac-python">https://github.com/kimjingu/nonnegfac-python</a></p>
    <p id="fn12"><a href="#foot-fn12"><sup>11</sup></a><a class="link-inline force-break" href="https://github.com/Text-Analytics/SeaNMF">https://github.com/Text-Analytics/SeaNMF</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23–27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3178876.3186009">https://doi.org/10.1145/3178876.3186009</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
