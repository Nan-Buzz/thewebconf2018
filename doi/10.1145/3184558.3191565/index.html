<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>MELL: Effective Embedding Method for Multiplex Networks</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191565'>https://doi.org/10.1145/3184558.3191565</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191565'>https://w3id.org/oa/10.1145/3184558.3191565</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">MELL: Effective Embedding Method for Multiplex Networks</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Ryuta</span> <span class="surName">Matsuno</span> Department of Computer Science, School of Computing, Tokyo Institute of Technology, W8-59 2-12-1 OokayamaMeguro, Tokyo 152-8552Japan, <a href="mailto:ryutamatsuno@net.c.titech.ac.jp">ryutamatsuno@net.c.titech.ac.jp</a>
        </div>
        <div class="author">
          <span class="givenName">Tsuyoshi</span> <span class="surName">Murata</span> Department of Computer Science, School of Computing, Tokyo Institute of Technology, W8-59 2-12-1 OokayamaMeguro, Tokyo 152-8552Japan, <a href="mailto:murata@c.titech.ac.jp">murata@c.titech.ac.jp</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191565" target="_blank">https://doi.org/10.1145/3184558.3191565</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Network embedding is a method for converting nodes in a network into low dimensional vectors, preserving its structure and the similarities among the nodes. Embedding is widely used in many applications, e.g., social network analysis and knowledge discovery. Because of its wide usage, many studies have been proposed, such as DeepWalk, LINE and node2vec. These works are designed for single-layer networks, however, real world networks often possess not just one, but multiple types of connections. Hence it is more appropriate to represent them as multiplex networks, which consist of multiple layers each of which represents one type of relationship. Embedding multiplex networks is difficult because all layer structures have to be taken into consideration.</small></p>
        <p><small>In this paper, we propose MELL, a novel embedding method for multiplex networks, which incorporates an idea of layer vector that captures and characterizes each layer's connectivity. This method exploits the overall structure effectively, and embeds both directed and undirected multiplex networks, whether their layer structures are similar or complementary. We focus on link prediction tasks and test our method and other baseline methods using five data sets from different domains. The results show that our method outperforms all of the baseline methods for all of the data sets.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Computing methodologies</strong> → <strong>Unsupervised learning;</strong> <strong>Learning latent representations;</strong> • <strong>Theory of computation</strong> → <em>Social networks;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Multiplex network</small>,</span> <span class="keyword"><small>Network embedding</small>,</span> <span class="keyword"><small>Link prediction</small>,</span> <span class="keyword"><small>Network analysis</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Ryuta Matsuno and Tsuyoshi Murata. 2018. MELL: Effective Embedding Method for Multiplex Networks. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3191565" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3191565</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Network embedding aims at converting nodes in a network to lower dimensional vectors. The embedding vectors are learned in order to preserve some particular information such as structural connectivity, structural distances or similarities among nodes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. Due to its wide usage, many methods are proposed recently, such as DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], node2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] and APP [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>]. The common tasks to test embedding methods are visualization, multi-class labeling and link prediction. Link prediction is useful for many applications, e.g., friend recommendation in online social networks and knowledge discovery in semantic networks. Thus we focus on link prediction task in this paper.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191565/images/www18companion-304-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">A toy image of undirected multiplex network. This multiplex network has three layers, and they share four nodes. Continuous lines illustrate the edges in each layer. Two nodes connected by a dashed line are the same corresponding nodes; they are counterparts of each other.</span>
        </div>
      </figure>
      <p></p>
      <p>Existing embedding methods are mainly for single-layer networks, however, networks in real world are often represented as multiplex networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>]. Multiplex network is a multi-layer network, where the layers share the same set of nodes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]. A multiplex network represents different types of connectivity between one single set of nodes. For example, online social networks used by the same people are expressed as a multiplex network where each layer illustrates one online social network service. In the case of genetic networks, the multiple types of interactions among genes are described as a multiplex network.</p>
      <p>In multiplex networks, connectivity in layers affects each other and that makes it difficult to analyze each layer independently. <strong>Figure <a class="fig" href="#fig1">1</a></strong> shows a toy image of an undirected multiplex network. This multiplex network has three layers and the layers share four nodes. The structure of layer 2 and layer 3 are almost equal, so it is natural to predict a potential link between node A and node B in layer 3. This prediction is easy and reasonable, however, without layer 2, it is difficult to predict that link because of the limited structural information. Therefore, existing embedding methods for single-layer networks do not work for multiplex networks.</p>
      <p>MTNE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>] is an existing embedding method for multiplex networks. This method jointly learns embedding vectors for each node in each layer via enforcing an extra information-sharing embedding. It predicts links in multiplex networks, but the problem is that their model seems to work only for multiplex networks where layers are similar to each other. Another approach for link prediction in multiplex networks is generative model. MULTITENSOR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] is the one that decomposes adjacency tensor of a multiplex network using the Poisson distribution. Based on the decomposition, this method regenerates the adjacency tensor and uses the tensor for link prediction. However, it is not always the Poisson distribution that generates multiplex networks.</p>
      <p>In this paper, we propose MELL (an abbreviation of <span style="text-decoration: underline;">M</span>ultiplex network <span style="text-decoration: underline;">E</span>mbedding via <span style="text-decoration: underline;">L</span>earning <span style="text-decoration: underline;">L</span>ayer vectors). It embeds each layer into a lower dimensional embedding space, and enforces these embeddings to be close to each other in order to share each layer's connectivity among embeddings. We also incorporate a novel idea of layer vector that captures and characterizes each layer's connectivity. MELL learns embedding vectors and layer vectors at the same time using all of the layer structures, and based on these embeddings and layer vectors, this method calculates edge probabilities for link prediction. Thanks to the enforcing and the layer vectors, MELL works well for link prediction whether layer structures are similar or complementary. We test our method and four other baseline methods for link prediction tasks using five data sets from different domains : social, co-authorship, transportation, neuronal and genetic domains. The experimental results show that our method outperforms all of the baseline methods in all of the data sets. At maximum, the AUC score of our method is 25% higher than that of the baseline methods.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related work</h2>
        </div>
      </header>
      <p>In this section, we introduce related works: network embedding and generative model.</p>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Network Embedding</h3>
          </div>
        </header>
        <p>Network embedding is a method for embedding a network to a lower dimensional space by converting nodes in the network to vectors in the embedding space. One of the well-known approaches is the one based on random walks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], e.g., DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], node2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] and APP [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>]. These methods use sampled paths obtained by random walks for updating and optimizing embedding vectors. We use APP as a representative single-layer embedding method for the experiments because APP is the latest work among these works, and APP is the best method for link prediction. Also APP can embed both undirected and directed networks.</p>
        <p>For multiplex networks, MTNE, Multi-Task Network Embedding method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>] is one of the representative methods. It extends a simple embedding method to multiplex networks, and jointly learns embedding vectors for each layer in a multiplex network via enforcing an extra information-sharing embedding. The authors of MTNE assume that the same node will expose similar or complementary characteristics in layers. Enforcing information-sharing embedding is the way to share the characteristics in all layers. The embedding process is as follows: i) MTNE embeds each layer to the lower dimensional space, ii) it calculates the information-sharing embedding, iii) it optimizes embedding vectors using the information-sharing embedding, iv) it repeats the step ii) and iii) until embedding vectors converge. The authors propose two ways for the information-sharing embedding: MTNE-C, which shares a common embedding, and MTNE-R, which shares a consensus embedding.</p>
        <p>They test MTNE methods through network visualization, link prediction and multi-label classification. MTNE-R and MTNE-C work better than other methods including DeepWalk and LINE. They also show that the best method for link prediction is “MTNE-R Consensus” (MTNE-R Cons. for short), a variant of MTNE-R. MTNE-R uses the consensus embedding merely for optimizing each layer's embedding, and uses each layer's embedding for predicting links in each layer. On the other hand, MTNE-R Cons. uses the consensus embedding to predict links in all layers. The consensus embedding is similar to the average embedding among all layers’ embeddings, and it predicts links well in the experiments. However, MTNE-R Cons. ignores the differences among layers because it uses exactly the same embedding vectors for link prediction in all layers. For multiplex networks whose layer structures are different, MTNE-R Cons. does not work well. MTNE-C and MTNE-R also do not work because the information-sharing embedding cannot improve the overall embeddings. The problem of MTNE methods is that they assume similar layer structures. Indeed, social networks tends to have similar layer structures. So MTNE methods seem to work well for social networks but not for other networks where the structural similarity is not guaranteed, e.g., transportation networks and genetic networks. Another problem is that MTNE methods are designed only for undirected multiplex networks, so these methods cannot be used for directed multiplex networks.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Generative Model</h3>
          </div>
        </header>
        <p>Another approach for link prediction in multiplex networks is generative model. Generative models regard adjacency matrices for a multiplex network as one tensor. Then the models try to find the hidden parameters that generate the tensor assuming a particular probabilistic distribution.</p>
        <p>MULTITENSOR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] is a generative model used for multiplex networks. The authors extend the mixed-membership stochastic block model to multiplex networks. They assume that the layers share underlying common community structures, which determine the connectivity in all layers.</p>
        <p>Formally, MULTITENSOR is similar to Poisson tensor factorization models. MULTITENSOR decomposes the adjacency tensor into three matrices, two membership matrices and one affinity matrix. After the decomposition, it regenerates the adjacency tensor, and uses the tensor for the tasks such as link prediction. This method can be used for directed and undirected networks. Link prediction experiments are conducted on three real networks, and MULTITENSOR performs the best in the experiment.</p>
        <p>However, there is no guarantee that there are underlying community structures in any multiplex networks. Also there are no guarantee that any multiplex networks are generated based on the Poisson distribution.</p>
      </section>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Our Method</h2>
        </div>
      </header>
      <p>In this section, we explain our method, MELL. Firstly, we define notations and terms, and then we explain a basic embedding method for single-layer networks. After that, we illustrate the idea of our method and its algorithm.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Notation</h3>
          </div>
        </header>
        <p>In this paper, we use <em>v</em> for nodes, lower case letters (i.e., <em>x</em> except for <em>v</em>) for scalar values, lower case roman-bold letters (i.e., <strong>x</strong>) for vectors, upper case roman-bold letters (i.e., <span class="inline-equation"><span class="tex">$\bf X$</span></span> ) for matrices or tensors, upper case calligraphic letters (i.e., <span class="inline-equation"><span class="tex">$\mathcal {X}$</span></span> ) for sets. The notation ‖ · ‖ <sub><em>F</em></sub> denotes the Frobenius norm. A term <strong>a</strong> · <strong>b</strong> denotes the inner product of the vectors <strong>a</strong> and <strong>b</strong>. A term denotes the transpose of the vector <strong>a</strong>.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Terminology Definition</h3>
          </div>
        </header>
        <p>The networks studied in this paper are all multiplex networks. Multiplex network is defined as <em>Definition <a class="enc" href="#enc1">3.1</a></em> .</p>
        <div class="definition" id="enc1">
          <label>Definition 3.1 (Multiplex Network).</label>
          <p>A multiplex network consists of a set of networks. Each of the networks is called a “layer” and all layers have the same set of nodes. Specifically, a multiplex network <span class="inline-equation"><span class="tex">$\mathcal {G}= \lbrace G^1, ..., G^l, ..., G^L \rbrace$</span></span> , where <span class="inline-equation"><span class="tex">$G^l = (\mathcal {V}^l, \mathcal {E}^l)$</span></span> denotes the <em>l</em>-th layer in <span class="inline-equation"><span class="tex">$\mathcal {G}$</span></span> . <em>L</em> is the total number of layers. <span class="inline-equation"><span class="tex">$\mathcal {V}^l$</span></span> denotes the set of nodes and <span class="inline-equation"><span class="tex">$\mathcal {E}^l$</span></span> denotes the set of edges in the <em>l</em>-th layer. In other words, <span class="inline-equation"><span class="tex">$\mathcal {E}^l \subset \mathcal {V}^l \times \mathcal {V}^l$</span></span> and <span class="inline-equation"><span class="tex">$\mathcal {V}^l = \lbrace v^l_1, ..., v^l_i, ..., v^l_N\rbrace$</span></span> , where <span class="inline-equation"><span class="tex">$v^l_i$</span></span> denotes the <em>i</em>-th node in the <em>l</em>-th layer. <em>N</em> denotes the number of nodes. For <span class="inline-equation"><span class="tex">$v^l_i$</span></span> , all <em>i</em>-th nodes in all other layers are its counterparts, e.g., <span class="inline-equation"><span class="tex">$v^1_i, ..., v^L_i$</span></span> are the counterparts of each other. We use <em>M</em> for the total number of all existing edges, i.e., <span class="inline-equation"><span class="tex">$M = \sum _l^L \left| \mathcal {E}^l \right|$</span></span> . For undirected networks, <span class="inline-equation"><span class="tex">$(v^l_i,v^l_j) = (v^l_j,v^l_i)$</span></span> , while for directed networks, <span class="inline-equation"><span class="tex">$(v^l_i,v^l_j) \ne (v^l_j,v^l_i)$</span></span> . We use the term “head” for a node where a directed edge is from, and we use “tail” for a node where a directed edge is to, e.g., for <span class="inline-equation"><span class="tex">$(v^l_i,v^l_j)$</span></span> , <span class="inline-equation"><span class="tex">$v^l_i$</span></span> is the head, and <span class="inline-equation"><span class="tex">$v^l_j$</span></span> is the tail.</p>
        </div>
        <p>In this paper, we study a method for predicting missing or potential edges in multiplex networks. This problem is defined as follows.</p>
        <div class="definition" id="enc2">
          <label>Definition 3.2 (Link Prediction in Multiplex Network Problem).</label>
          <p>Given a multiplex network <span class="inline-equation"><span class="tex">$\mathcal {G}$</span></span> , “link prediction in multiplex network problem” aims at inferring the missing or potential edges in any layers in <span class="inline-equation"><span class="tex">$\mathcal {G}$</span></span> . Formally, based on the given multiplex network <span class="inline-equation"><span class="tex">$\mathcal {G}$</span></span> , the objective is to generate an edge probability function <span class="inline-equation"><span class="tex">$p:\mathcal {V}^l \times \mathcal {V}^l \rightarrow [0, 1] \ (l = 1, ..., L)$</span></span> . The output value of <em>p</em> shows how likely the edge between input nodes will be formed.</p>
        </div>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Inner Product Method</h3>
          </div>
        </header>
        <p>We introduce one of the basic embedding methods for single-layer networks, the inner product method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>], which our method in this paper is based on. This method is also adopted in MTNE.</p>
        <p>In the inner product method, the edge probability that there exists an edge between node <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em> is calculated as follows:</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} p({v}_i, {v}_j) = \frac{1}{1 + \exp (- \cdot {\bf v}_j)}, \end{align}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <p>where <strong>v</strong> <sub><em>i</em></sub> and <strong>v</strong> <sub><em>j</em></sub> denote the embedding vectors for <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em> respectively. Given a network, output values of <em>p</em> for connected node pairs should be close to 1, and output values of <em>p</em> for unconnected node pairs should be 0. Thus this method learns the embedding vectors to maximize the likelihood of generating the original network. The likelihood is as follows:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \prod _{({v}_i, {v}_j) \in \mathcal {E}} p({v}_i, {v}_j) \prod _{({v}_i, {v}_j) \not\in \mathcal {E}}\left(1- p({v}_i, {v}_j) \right), \end{align}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where <span class="inline-equation"><span class="tex">$\mathcal {E}$</span></span> denotes the existing edge set. Usually for easy calculation, the log likelihood is used for the loss function, and a regularization term for the embedding vectors is added to it. The final loss function, which should be minimized, is as follows:
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} Loss\left(\mathcal {E} ; \lambda \right) = \nonumber &amp; - \sum _{({v}_i, {v}_j) \in \mathcal {E}} \log p({v}_i, {v}_j) \nonumber \\ &amp; - \sum _{({v}_i, {v}_j) \not\in \mathcal {E}} \log \left(1- p({v}_i, {v}_j) \right) \nonumber \\ &amp; + \lambda \Vert \bf V \Vert _F^2 , \end{align}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>where <span class="inline-equation"><span class="tex">$\bf V$</span></span> denotes the matrix for embedding vectors, the <em>i</em>-th row in <span class="inline-equation"><span class="tex">$\bf V$</span></span> is the embedding vector for <em>v<sub>i</sub></em> , <em>λ</em> denotes a regularization coefficient.
        <p></p>
        <p>The inner product method embeds all nodes in the given single-layer network into the <em>d</em>-dimensional embedding space by minimizing Eq. (<a class="eqn" href="#eq3">3</a>). After the embedding, this method calculates <em>p</em>(<em>v<sub>i</sub></em> , <em>v<sub>j</sub></em> ) by Eq. (<a class="eqn" href="#eq1">1</a>) to predict whether the edge between <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em> will be formed or not.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191565/images/www18companion-304-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">The overall processes for predicting links by MELL. Given a multiplex network, MELL embeds nodes in each layer into a lower dimensional space with learning layer vectors. After learning the embeddings, it predicts links by calculating the edge probabilities by Eq. (<a class="eqn" href="#eq6">6</a>).</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> MELL</h3>
          </div>
        </header>
        <p>We propose MELL, a <span style="text-decoration: underline;">M</span>ultiplex network <span style="text-decoration: underline;">E</span>mbedding via <span style="text-decoration: underline;">L</span>earning <span style="text-decoration: underline;">L</span>ayer vectors. MELL is based on the inner product method explained above, and we incorporate the following new ideas:</p>
        <ul class="list-no-style">
          <li id="list1" label="•">MELL embeds nodes in each layer into a <em>d</em>-dimensional space, and enforces embedding vectors for the same node among layers to be close to each other in order to share all layer structures among embeddings.<br /></li>
          <li id="list2" label="•">MELL learns layer vectors that characterize the layers’ connectivity in order to differentiate the edge probabilities in each layer.<br /></li>
          <li id="list3" label="•">For undirected multiplex networks, MELL uses two vectors for each node; the one is an embedding vector as a head, the other is an embedding vector as a tail.<br /></li>
        </ul>
        <p>The overall processes of our method are shown in <strong>Figure <a class="fig" href="#fig2">2</a></strong> . MELL embeds nodes in each layer with learning layer vectors. After the embedding, it predicts links by calculating the edge probabilities.</p>
        <p>The first idea is to embed each layer into the embedding space and enforce them to be close. By this enforcing, the embedding vectors are learned not only from one layer's connectivity but also from all other layers’ connectivity as well. Specifically, MELL applies the inner product method to each layer. The standard inner product method merely learns the structural connections in one layer. Thus, in order to exploit all layers’ connectivity for each embedding, MELL enforces the embedding vectors for the same node to be close to each other. MELL realizes this idea by adding a regularization term to the loss function. Formally, this term is designed as the variance of embedding vectors. The enforcing term is as follows:</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \textrm {(The enforcing term)} =&amp; \beta \mathbb {V}[\bf V], \\\textrm {where } \mathbb {V}[\bf V] =&amp; \frac{1}{L} \sum _{l = 1}^{L} \Vert \bf V[l] - \mathbb {E}[\bf V] \Vert _F^2 , \nonumber \\\mathbb {E}[\bf V] =&amp; \frac{1}{L} \sum _{l = 1}^{L} \bf V[l], \nonumber\end{align}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>and <em>β</em> denotes the regularization coefficient, <span class="inline-equation"><span class="tex">$\bf V \in \mathbb {R}^{ L \times N \times d }$</span></span> denotes the embedding tensor, which contains all embedding vectors for the given multiplex network. <span class="inline-equation"><span class="tex">$\bf V[l]$</span></span> denotes the <em>N</em> × <em>d</em> embedding matrix for the nodes in the <em>l</em>-th layer. <em>L</em>, <em>N</em>, <em>d</em> denote the number of layers, nodes and the embedding space's dimension, respectively. <span class="inline-equation"><span class="tex">$\mathbb {E}[\bf V]$</span></span> calculates the average embedding matrix among layers and <span class="inline-equation"><span class="tex">$\mathbb {V}[\bf V]$</span></span> calculates the variance of embedding vectors.
        <p></p>
        <p>However, even though layer structures in a multiplex network are different from each other, the enforcing prevents the embedding vectors from being different. This causes poor link predictions because even if some layers have different structures, a method based on the embedding will predict an edge between the same pair of nodes in all layers with almost the same probabilities. In order to differentiate edge probabilities in each layer, we incorporate the idea of layer vector. Layer vector is a vector which determines the edge probabilities for all node pairs in the layer. Formally, the edge probability between <span class="inline-equation"><span class="tex">$v^l_i$</span></span> and <span class="inline-equation"><span class="tex">$v^l_j$</span></span> is calculated as follows using a layer vector <strong>r</strong> <sub><em>l</em></sub> :</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} p(v^l_i, v^l_j) = \frac{1}{1 + \exp \left(- \cdot {{\bf v}}^l_j \right)}, \end{align}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p>where <span class="inline-equation"><span class="tex">${{\bf v}}_i^l$</span></span> and <span class="inline-equation"><span class="tex">${{\bf v}}_j^l$</span></span> denote the embedding vector for <span class="inline-equation"><span class="tex">$v^l_i$</span></span> and <span class="inline-equation"><span class="tex">$v^l_j$</span></span> respectively. <strong>r</strong> <sub><em>l</em></sub> denotes the layer vector for the <em>l</em>-th layer. The layer vectors actually learn the layers’ connectivity. By the enforcing and Eq. (<a class="eqn" href="#eq5">5</a>), if layers are similar to each other, the layer vectors are also similar to each other, and if they are different, the layer vectors are different as well. Therefore, layer vectors capture and characterize connectivity in each layer, comparing the structure with other layer structures.</p>
        <p>The disadvantage of the layer vector is its asymmetric nature. The original inner product method is for undirected network because <em>p</em> in Eq. (<a class="eqn" href="#eq1">1</a>) is symmetric, i.e., <em>p</em>(<em>v<sub>i</sub></em> , <em>v<sub>j</sub></em> ) = <em>p</em>(<em>v<sub>j</sub></em> , <em>v<sub>i</sub></em> ). In Eq. (<a class="eqn" href="#eq5">5</a>), <span class="inline-equation"><span class="tex">$p(v^l_i, v^l_j)$</span></span> is not equal to <span class="inline-equation"><span class="tex">$ p(v^l_j, v^l_i)$</span></span> because the layer vector is added to the head embedding vector. This asymmetric nature is not a problem for directed networks because they require an asymmetric edge probability. To make our method applicable to undirected networks as well, we introduce two vectors for each node, one is as a head and the other is as a tail. This idea is inspired by APP [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>], which employ an asymmetric edge probability like Eq. (<a class="eqn" href="#eq5">5</a>). Since two embedding vectors should be used in order to incorporate this idea, we redefine <span class="inline-equation"><span class="tex">$p(v^l_i, v^l_j)$</span></span> as follows:</p>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} p(v^l_i, v^l_j) = \frac{1}{1 + \exp \left(- \cdot {{\bf v}_T}^l_j \right)}, \end{align}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <p>where <span class="inline-equation"><span class="tex">${{\bf v}_H}^l_i$</span></span> denotes the head embedding vector for <span class="inline-equation"><span class="tex">$v_i^l$</span></span> and <span class="inline-equation"><span class="tex">${{\bf v}_T}^l_j$</span></span> denotes the tail embedding vector for <span class="inline-equation"><span class="tex">$v_j^l$</span></span> . In directed networks, a vector as a head and that as a tail are always the same, i.e., <span class="inline-equation"><span class="tex">${{\bf v}_H}^l_i = {{\bf v}_T}^l_i$</span></span> in any <em>l</em> and any <em>i</em>.</p>
        <p>Considering these three ideas: enforcing, incorporating layer vector, and using two vectors for undirected multiplex networks, the loss function to optimize the embedding vectors and layer vectors is designed as follows:</p>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} Loss(\mathcal {E}_{pos},\mathcal {E}_{neg};\lambda ,\beta ,\gamma) = &amp; - \sum _{(v^l_i, v^l_j) \in \mathcal {E}_{pos}} \log p(v^l_i, v^l_j) \nonumber \\ &amp; - \sum _{(v^l_i, v^l_j) \in \mathcal {E}_{neg}} \log \left(1- p(v^l_i, v^l_j) \right) \nonumber \\ &amp; + \beta \left(\mathbb {V}[\bf V_H] + \mathbb {V}[\bf V_T] \right) \nonumber \\ &amp; + \frac{\lambda }{N} (\Vert \bf V_H \Vert _F^2 + \Vert \bf V_T \Vert _F^2) + \gamma \Vert \bf R \Vert _F^2, \end{align}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>where <span class="inline-equation"><span class="tex">$\bf V_H, \bf V_T \in \mathbb {R}^{ L \times N \times d }$</span></span> denote the head and tail embedding tensor respectively, which contain all head and tail embedding vectors. <span class="inline-equation"><span class="tex">$\bf R \in \mathbb {R}^{L\times d}$</span></span> denotes the layer vectors. <span class="inline-equation"><span class="tex">$\mathcal {E}_{pos}$</span></span> denotes the all existing edges in the given multiplex network, i.e., <span class="inline-equation"><span class="tex">$\mathcal {E}_{pos} = \mathcal {E}^1 \cup ... \cup \mathcal {E}^L$</span></span> . <span class="inline-equation"><span class="tex">$\mathcal {E}_{neg}$</span></span> is the set of negative samples. They are uniformly and randomly sampled from all unconnected node pairs in the given multiplex network. The number of negative samples is <span class="inline-equation"><span class="tex">$k|\mathcal {E}_{pos}|$</span></span> using a parameter <em>k</em>. The third term is basically designed as the enforcing term in Eq. (<a class="eqn" href="#eq4">4</a>), but is modified for using <span class="inline-equation"><span class="tex">$\bf V_H$</span></span> and <span class="inline-equation"><span class="tex">$\bf V_T$</span></span> . <em>β</em> denotes the regularization coefficient for this enforcing term. <em>λ</em> and <em>γ</em> denote regularization coefficients for embedding vectors and for layer vectors respectively. The fourth term is divided by <em>N</em> to normalize the impact of <em>λ</em>.
        <p></p>
        <p>In order to minimize <em>Loss</em>, our method learns the optimal <span class="inline-equation"><span class="tex">$\bf V_H, \bf V_T$</span></span> and <span class="inline-equation"><span class="tex">$\bf R$</span></span> . After learning, we use Eq. (<a class="eqn" href="#eq6">6</a>) to predict edges. Due to the layer vectors, our method is basically for directed multiplex networks. Thus we introduce additional processes for undirected networks; we regard one undirected edge as two directed edges. Firstly, for learning embedding vectors, <span class="inline-equation"><span class="tex">$\mathcal {E}_{pos}$</span></span> is set as <span class="inline-equation"><span class="tex">$\mathcal {E}^1 \cup ... \cup \mathcal {E}^L \cup \mathcal {E}^{\prime 1} \cup ... \cup \mathcal {E}^{\prime L}$</span></span> , where <span class="inline-equation"><span class="tex">$\mathcal {E}^{\prime l} = \left\lbrace (v^l_j, v^l_i) \middle | (v^l_i, v^l_j) \in \mathcal {E}^l \right\rbrace$</span></span> . Secondly, after learning embedding vectors, we use the average value of <span class="inline-equation"><span class="tex">$p(v^l_i, v^l_j)$</span></span> and <span class="inline-equation"><span class="tex">$p(v^l_j, v^l_i)$</span></span> for predicting an edge <span class="inline-equation"><span class="tex">$(v^l_i, v^l_j)$</span></span> .</p>
        <p>Our method has five parameters altogether: embedding space's dimension <em>d</em>, a negative sampling rate <em>k</em>, regularization coefficient for embedding vectors <em>λ</em>, regularization coefficient for the variance <em>β</em> and regularization coefficient for layer vectors <em>γ</em>. These parameters should be decided by a grid search.</p>
        <p>We implement our method using Python and TensorFlow. We use Adam [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] for fast optimizing, but the other standard optimization methods, such as gradient descent, can be used as well. The overall MELL processes to learn <span class="inline-equation"><span class="tex">$\bf V_H, \bf V_T$</span></span> and <span class="inline-equation"><span class="tex">$\bf R$</span></span> are shown in <strong>Algorithm 1</strong> .</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span> Complexity Analysis</h3>
          </div>
        </header>
        <p>The total time complexity of MELL is <em>O</em>(<em>td</em>(<em>kM</em> + <em>NL</em>)), where <em>t</em> denotes the number of iterations, <em>k</em> denotes the negative sampling rate, <em>L</em>, <em>N</em>, <em>M</em> denotes the number of layers, nodes, edges in a given multiplex network, respectively.</p>
        <p><img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191565/images/www18companion-304-img1.svg" class="img-responsive" alt="" longdesc="" /></p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experiments</h2>
        </div>
      </header>
      <p>In this section, we explain the experiments we conducted. First, we explain details of data sets, experimental setting, baseline methods to be compared and the evaluation metric. After that we show the experimental results. We also report the parameter sensitivity of our method.</p>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Data Set</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Properties of the data sets. L, N, M denotes the number of layers, the number of the nodes, the total number of all existing edges, respectively. Type shows the type of the data set.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">Data set</td>
                <td style="text-align:center;">L</td>
                <td style="text-align:center;">N</td>
                <td style="text-align:center;">M</td>
                <td style="text-align:left;">type</td>
                <td style="text-align:left;">directed</td>
              </tr>
              <tr>
                <td style="text-align:left;">CS-Aarhus</td>
                <td style="text-align:center;">5</td>
                <td style="text-align:center;">61</td>
                <td style="text-align:center;">620</td>
                <td style="text-align:left;">social</td>
                <td style="text-align:left;">undirected</td>
              </tr>
              <tr>
                <td style="text-align:left;">Pierre Auger Collaboration</td>
                <td style="text-align:center;">16</td>
                <td style="text-align:center;">514</td>
                <td style="text-align:center;">7153</td>
                <td style="text-align:left;">co-authorship</td>
                <td style="text-align:left;">undirected</td>
              </tr>
              <tr>
                <td style="text-align:left;">EU Air Transportation</td>
                <td style="text-align:center;">37</td>
                <td style="text-align:center;">450</td>
                <td style="text-align:center;">3588</td>
                <td style="text-align:left;">transportation</td>
                <td style="text-align:left;">undirected</td>
              </tr>
              <tr>
                <td style="text-align:left;">C.Elegans Connectome</td>
                <td style="text-align:center;">3</td>
                <td style="text-align:center;">279</td>
                <td style="text-align:center;">5863</td>
                <td style="text-align:left;">neural</td>
                <td style="text-align:left;">directed</td>
              </tr>
              <tr>
                <td style="text-align:left;">Xenopus</td>
                <td style="text-align:center;">5</td>
                <td style="text-align:center;">461</td>
                <td style="text-align:center;">620</td>
                <td style="text-align:left;">genetic</td>
                <td style="text-align:left;">directed</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>We test our method and baseline methods using five data sets from different domains: social, co-authorship, transportation, neuronal and genetic domains. The basic statistics of the data sets are shown in <strong>Table <a class="tbl" href="#tab1">1</a></strong> . L, N, M denotes the number of layers, nodes and all exiting edges, respectively. All data sets are obtained from CoMuNe lab's web site<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. Explanations of the data sets are as follows:</p>
        <ul class="list-no-style">
          <li id="list4" label="•">CS-Aarhus [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] : This undirected multiplex social network consists of five kinds of online and offline relationships (Facebook, Leisure, Work, Co-authorship, Lunch) between the employees of Computer Science department at Aarhus.<br />
          </li>
          <li id="list5" label="•">Pierre Auger Collaboration [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] : This undirected co-authorship multiplex network consists of 16 types of different working tasks within the Pierre Auger Collaboration. The layers represent Neutrinos, Detector, Enhancements and other tasks. This network is originally weighted, but we ignore the weights for our experiments.<br />
          </li>
          <li id="list6" label="•">EU Air Transportation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] : This undirected transportation multiplex network is composed by 37 different layers each one corresponding to a different airline operating in Europe. The nodes are airports, and the edges are routes, respectively.<br />
          </li>
          <li id="list7" label="•">C.Elegans Connectome [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] : This directed neuronal multiplex network, Caenorhabditis Elegans Connectome, consists of three different synaptic junctions: electric, chemical monadic, and polyadic.<br />
          </li>
          <li id="list8" label="•">Xenopus [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] : This directed genetic multiplex network consists of five types of different interactions for organisms: association, direct interaction, physical association, colocalization, and suppressive genetic interaction defined by inequality. The authors use the Biological General Repository for Interaction Data sets 3.2.108 (BioGRID, thebiogrid.org, updated 1 Jan 2014) for Xenopus laevis.<br />
          </li>
        </ul>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Experimental Setting</h3>
          </div>
        </header>
        <p>Given a multiplex network from the data sets, first we randomly split all existing edges into five parts each of which has 20% of the edges of each layer. We vary training edge rate <em>α<sub>t</sub></em> from 20% to 80% in 20% increments. Based on <em>α<sub>t</sub></em> , we pick one to four parts of the split edge parts and use them as training edges. The other parts are used as positive testing edges. This positive testing edges are hidden with other unconnected node pairs, so that methods can not tell the testing edges and truly unconnected node pairs apart. For testing, we sample truly unconnected node pairs as negative testing edges. These edges are sampled uniformly and randomly, and the number of positive testing edges and negative testing edges are the same. We use both the positive and negative testing edges for testing. We change the picking combination for the training edge parts uniformly five times, and report the average score as the final results. When <em>α<sub>t</sub></em> is 20%, this experimental setting is the same as the five-fold cross validation.</p>
        <p>For the evaluation, we use AUC score, a well-known and standard metric for link prediction tasks.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Baseline Method</h3>
          </div>
        </header>
        <p>In this experiments, we test 4 existing methods and our method: APP, MTNE-R, MTNE-R Cons., MULTITENSOR and MELL. Except for MULTITENSOR, which is not an embedding method, we use the same dimension for embedding, <em>d</em> = 128 for fair comparison. For other parameters, such as regularization coefficient, we do grid search for each data set to find optimal parameters. The explanations of the baseline methods are as follows:</p>
        <ul class="list-no-style">
          <li id="list9" label="•">APP [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] : This method embeds directed and undirected single-layer networks. In this experiment, we naively apply this method to each layer, and thus this method only uses one layer, ignoring the other layers.<br />
          </li>
          <li id="list10" label="•">MTNE-R [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>] : This method is for embedding undirected multiplex networks. It embeds each layer to the embedding space, and then it calculates a consensus embedding. Using the consensus embedding, it optimizes each layer's embedding, and uses each embedding for link prediction in each layer. We use all unconnected node pairs as negative samples for learning.<br />
          </li>
          <li id="list11" label="•">MTNE-R Cons. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>] : This method is a variant of MTNE-R. Different from MTNE-R, this method uses the consensus embedding for link prediction for all layers, ignoring differences among layers.<br />
          </li>
          <li id="list12" label="•">MULTITENSOR [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] : This is a generative model based on the Poisson distribution. It decomposes the adjacency tensor and uses obtained matrices for link prediction. Originally, it uses a part of layers for link prediction, however, all layers are used in this experiment for a fair comparison.<br />
          </li>
        </ul>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Experimental Results. The values in the table are AUC scores. <em>α<sub>t</sub></em> is the rate of training edges to all existing edges. Since MTNE-R and MTNE-R Cons. are for undirected multiplex networks, the scores are set to “-” in the table. The bold face is the best value for the same <em>α<sub>t</sub></em> in each data set.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;"></th>
                <th style="text-align:center;" colspan="4">
                  training rate <em>α<sub>t</sub></em>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;">data set</th>
                <th style="text-align:left;">method</th>
                <th style="text-align:center;">20%</th>
                <th style="text-align:center;">40%</th>
                <th style="text-align:center;">60%</th>
                <th style="text-align:center;">80%</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">APP</td>
                <td style="text-align:center;">0.5461</td>
                <td style="text-align:center;">0.6630</td>
                <td style="text-align:center;">0.7376</td>
                <td style="text-align:center;">0.7757</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MTNE-R</td>
                <td style="text-align:center;">0.6977</td>
                <td style="text-align:center;">0.7788</td>
                <td style="text-align:center;">0.7921</td>
                <td style="text-align:center;">0.8492</td>
              </tr>
              <tr>
                <td style="text-align:left;">CS-Aarhus</td>
                <td style="text-align:left;">MTNE-R Cons.</td>
                <td style="text-align:center;">0.6943</td>
                <td style="text-align:center;">0.7645</td>
                <td style="text-align:center;">0.8189</td>
                <td style="text-align:center;">0.8370</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MULTITENSOR</td>
                <td style="text-align:center;">0.6645</td>
                <td style="text-align:center;">0.7827</td>
                <td style="text-align:center;">0.8862</td>
                <td style="text-align:center;">0.9071</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MELL</td>
                <td style="text-align:center;"><strong>0.8173</strong></td>
                <td style="text-align:center;"><strong>0.8877</strong></td>
                <td style="text-align:center;"><strong>0.9224</strong></td>
                <td style="text-align:center;"><strong>0.9417</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">APP</td>
                <td style="text-align:center;">0.6060</td>
                <td style="text-align:center;">0.7913</td>
                <td style="text-align:center;">0.8894</td>
                <td style="text-align:center;">0.9434</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MTNE-R</td>
                <td style="text-align:center;">0.9214</td>
                <td style="text-align:center;">0.9525</td>
                <td style="text-align:center;">0.9765</td>
                <td style="text-align:center;">0.9873</td>
              </tr>
              <tr>
                <td style="text-align:left;">Pierre Auger Collaboration</td>
                <td style="text-align:left;">MTNE-R Cons.</td>
                <td style="text-align:center;">0.9217</td>
                <td style="text-align:center;">0.9542</td>
                <td style="text-align:center;">0.9775</td>
                <td style="text-align:center;">0.9874</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MULTITENSOR</td>
                <td style="text-align:center;">0.9451</td>
                <td style="text-align:center;">0.9705</td>
                <td style="text-align:center;">0.9817</td>
                <td style="text-align:center;">0.9893</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MELL</td>
                <td style="text-align:center;"><strong>0.9811</strong></td>
                <td style="text-align:center;"><strong>0.9918</strong></td>
                <td style="text-align:center;"><strong>0.9956</strong></td>
                <td style="text-align:center;"><strong>0.9979</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">APP</td>
                <td style="text-align:center;">0.4364</td>
                <td style="text-align:center;">0.4904</td>
                <td style="text-align:center;">0.5536</td>
                <td style="text-align:center;">0.5828</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MTNE-R</td>
                <td style="text-align:center;">0.5535</td>
                <td style="text-align:center;">0.6891</td>
                <td style="text-align:center;">0.7446</td>
                <td style="text-align:center;">0.7817</td>
              </tr>
              <tr>
                <td style="text-align:left;">EU Air Transportation</td>
                <td style="text-align:left;">MTNE-R Cons.</td>
                <td style="text-align:center;">0.5550</td>
                <td style="text-align:center;">0.7010</td>
                <td style="text-align:center;">0.7440</td>
                <td style="text-align:center;">0.7683</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MULTITENSOR</td>
                <td style="text-align:center;">0.8516</td>
                <td style="text-align:center;">0.9041</td>
                <td style="text-align:center;">0.9344</td>
                <td style="text-align:center;">0.9463</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MELL</td>
                <td style="text-align:center;"><strong>0.9806</strong></td>
                <td style="text-align:center;"><strong>0.9897</strong></td>
                <td style="text-align:center;"><strong>0.9941</strong></td>
                <td style="text-align:center;"><strong>0.9944</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">APP</td>
                <td style="text-align:center;">0.5875</td>
                <td style="text-align:center;">0.6800</td>
                <td style="text-align:center;">0.7287</td>
                <td style="text-align:center;">0.7518</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MTNE-R</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
              </tr>
              <tr>
                <td style="text-align:left;">C.Elegans Connectome</td>
                <td style="text-align:left;">MTNE-R Cons.</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MULTITENSOR</td>
                <td style="text-align:center;">0.7167</td>
                <td style="text-align:center;">0.8019</td>
                <td style="text-align:center;">0.8434</td>
                <td style="text-align:center;">0.8836</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MELL</td>
                <td style="text-align:center;"><strong>0.8360</strong></td>
                <td style="text-align:center;"><strong>0.9144</strong></td>
                <td style="text-align:center;"><strong>0.9430</strong></td>
                <td style="text-align:center;"><strong>0.9701</strong></td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">APP</td>
                <td style="text-align:center;">0.5135</td>
                <td style="text-align:center;">0.5260</td>
                <td style="text-align:center;">0.5536</td>
                <td style="text-align:center;">0.5442</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MTNE-R</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
              </tr>
              <tr>
                <td style="text-align:left;">Xenopus</td>
                <td style="text-align:left;">MTNE-R Cons.</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
                <td style="text-align:center;">-</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MULTITENSOR</td>
                <td style="text-align:center;">0.5469</td>
                <td style="text-align:center;">0.5921</td>
                <td style="text-align:center;">0.6441</td>
                <td style="text-align:center;">0.6664</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">MELL</td>
                <td style="text-align:center;"><strong>0.7575</strong></td>
                <td style="text-align:center;"><strong>0.8159</strong></td>
                <td style="text-align:center;"><strong>0.8818</strong></td>
                <td style="text-align:center;"><strong>0.9130</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Results</h3>
          </div>
        </header>
        <p>We set <em>α<sub>t</sub></em> to 20%, 40%, 60% and 80%. For each <em>α<sub>t</sub></em> , we test baseline methods and our method using all data sets. Experimental results are shown in <strong>Table <a class="tbl" href="#tab2">2</a></strong> . The values in the table are AUC scores. For MTNE-R and MTNE-R Cons., the results on C.Elegans Connectome and Xenopus are set to “-” because they are directed multiplex networks.</p>
        <p>First of all, APP, which totally ignores other layer structures, shows the worst scores in all data sets. For EU Air Transportation network, APP predicts links almost randomly. On the other hand, other methods, that use structural connectivity in other layers, performs better than APP.</p>
        <p>MELL outperforms all existing methods, APP, MTNE-R, MTNE-R Cons. and MULTITENSOR. For directed networks, C.Elegans Connectome and Xenopus, MELL shows significantly higher AUC scores, compared to only one baseline method for multiplex networks. Especially for Xenopus data sets, the AUC scores of MELL are at least 20% higher than MULTITENSOR, and 25% higher at <span class="inline-equation"><span class="tex">$\alpha _t = 80\%$</span></span> . For undirected network, MELL also shows the higher AUC scores. These results show that MELL effectively exploits the structural connectivity in all layers in the multiplex networks.</p>
        <p>We expect that MTNE models do not work on multiplex networks where the layer structures are not similar with each other. As we expected, MTNE-R and MTNE-R Cons. shows comparable AUC scores on CS-Aarhus and Pierre Auger Collaboration. They are a social network and a co-authorship network, respectively. Probably they have similar layer structures. On the other hand, AUC scores for EU Air Transportation are significantly inferior to MULTITENSOR and MELL. This is because transportation networks are different from social networks, and their layer structures are complementary. MTNE-R Cons., which performs better than MTNE-R in original paper's experiments, shows almost the same AUC scores with MTNE-R, and they are not remarkably different.</p>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191565/images/www18companion-304-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Parameter sensitivity of MELL. These graphs show the AUC scores for link prediction with different parameters, <em>d</em>, <em>k</em>, <em>λ</em>, <em>β</em>, <em>γ</em>. The default parameters are <em>d</em> = 128, <em>k</em> = 4.0, <em>λ</em> = 10.0, <em>β</em> = 1.0, <em>γ</em> = 1.0, and we vary one parameter fixing others.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Parameter Sensitivity</h3>
          </div>
        </header>
        <p>We vary parameters of MELL, in order to test the sensitivity of the parameters. MELL requires five parameters; embedding space's dimension <em>d</em>, negative sampling rate <em>k</em>, and three regularization coefficients : <em>λ</em> for embedding vectors, <em>β</em> for the variance, and <em>γ</em> for layer vectors. We set <em>d</em> = 128, <em>k</em> = 4.0, <em>λ</em> = 10.0, <em>β</em> = 1.0, <em>γ</em> = 1.0 as default parameters, and pick up one parameter and vary that parameter fixing others to check the parameter impact to the link prediction. The results are shown in <strong>Figure <a class="fig" href="#fig3">3</a></strong> . The training rate <em>α<sub>t</sub></em> is fixed to 60% in this experiment. The same as the previous experiment, the AUC scores are the averages of five times results.</p>
        <p>The embedding dimension <em>d</em> does not affect much for AUC scores compared to other parameters. The negative sampling rate <em>k</em> affects AUC scores of Xenopus. Xenopus prefers higher <em>k</em> because Xenopus is the sparsest network in the data sets. Thus in order to increase the true negatives of the link prediction results, Xenopus requires higher <em>k</em>. The regularization coefficient <em>λ</em> greatly affects the AUC scores of CS-Aarhus, C.Elegans Connectome and Xenopus. They show the highest AUC scores at 10, 100 and 1.0 respectively.</p>
        <p>The regularization coefficient <em>β</em> controls how much MELL enforces the embedding vectors among layers to be close. Too small <em>β</em> makes embeddings not to learn the other layer structures, and too big <em>β</em> prevents the embeddings from being different. The results show the highest AUC scores from <em>β</em> = 1 to <em>β</em> = 100, and show lower scores at <em>β</em> = 0.1 and <em>β</em> = 1000. The regularization coefficient <em>γ</em> is for layer vectors, and it affects the results of Xenopus significantly. Xenopus shows the highest at 10, but the score hugely decreases at 100.</p>
        <p>However, no parameters affect the AUC scores for Pierre Auger Collaboration and EU Air Transportation much. Their AUC scores are nearly 1.0, so that MELL can stably predict links in them, regardless of the changes of parameters.</p>
      </section>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we propose a novel embedding method for multiplex networks, named MELL, which incorporates an idea of layer vector that characterizes the connectivity in a layer. MELL embeds nodes in each layer into the lower embedding space using all layer structures, and incorporates layer vectors to differentiate edge probabilities in the layers. We test our method for link prediction tasks, using five data sets from different domains. We evaluate our methods and baseline methods by AUC scores. The experimental results show that our method outperforms all of the baseline methods for all of the data sets, indicating that MELL learns the overall structures of multiplex networks more effectively than the existing methods. For future work, we will extend our method for visualization and multi-label classification tasks.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-20">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This work was supported by Tokyo Tech - Fuji Xerox Cooperative Research (Project Code KY260195), JSPS Grant-in-Aid for Scientific Research(B) (Grant Number 17H01785) and JST CREST (Grant Number JPMJCR1687).</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Alessio Cardillo, Jesús Gómez-Gardeñes, Massimiliano Zanin, Miguel Romance, David Papo, Francisco&nbsp;del Pozo, and Stefano Boccaletti. 2013. Emergence of network features from multiplexity. <em><em>Scientific Reports</em></em> 3 (Feb. 2013), 1344. <a class="link-inline force-break" href="https://doi.org/10.1038/srep01344" target="_blank">https://doi.org/10.1038/srep01344</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">Beth&nbsp;L. Chen, David&nbsp;H. Hall, and Dmitri&nbsp;B. Chklovskii. 2006. Wiring optimization can relate neuronal structure and function. <em><em>Proceedings of the National Academy of Sciences of the United States of America</em></em> 103, 12 (2006), 4723–4728. <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.0506806103" target="_blank">https://doi.org/10.1073/pnas.0506806103</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. 2017. A Survey on Network Embedding. <em><em>ArXiv e-prints</em></em> (Nov. 2017). arxiv:1711.08752</li>
        <li id="BibPLXBIB0004" label="[4]">Caterina De&nbsp;Bacco, Eleanor&nbsp;A. Power, Daniel&nbsp;B. Larremore, and Cristopher Moore. 2017. Community detection, link prediction, and layer interdependence in multilayer networks. <em><em>Physical Review E</em></em> 95, Article 042317 (April 2017), 10&nbsp;pages.Issue 4. <a class="link-inline force-break" href="https://doi.org/10.1103/PhysRevE.95.042317" target="_blank">https://doi.org/10.1103/PhysRevE.95.042317</a>
        </li>
        <li id="BibPLXBIB0005" label="[5]">Manlio De&nbsp;Domenico, Andrea Lancichinetti, Alex Arenas, and Martin Rosvall. 2015. Identifying Modular Flows on Multilayer Networks Reveals Highly Overlapping Organization in Interconnected Systems. <em><em>Physical Review X</em></em> 5, Article 011027 (March 2015), 11&nbsp;pages.Issue 1. <a class="link-inline force-break" href="https://doi.org/10.1103/PhysRevX.5.011027" target="_blank">https://doi.org/10.1103/PhysRevX.5.011027</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">Manlio De&nbsp;Domenico, Mason&nbsp;A. Porter, and Alex Arenas. 2015. MuxViz: a tool for multilayer analysis and visualization of networks. <em><em>Journal of Complex Networks</em></em> 3 (2015), 159–176. Issue 2. <a class="link-inline force-break" href="https://doi.org/10.1093/comnet/cnu038" target="_blank">https://doi.org/10.1093/comnet/cnu038</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Manlio&nbsp;De Domenico, Albert Solé-Ribalta, Emanuele Cozzo, Mikko Kivelä, Yamir Moreno, Mason&nbsp;A. Porter, Sergio Gómez, and Alex Arenas. 2013. Mathematical Formulation of Multilayer Networks. <em><em>Physical Review X</em></em> 3, Article 041022 (Oct. 2013), 15&nbsp;pages.Issue 4. <a class="link-inline force-break" href="https://doi.org/10.1103/PhysRevX.3.041022" target="_blank">https://doi.org/10.1103/PhysRevX.3.041022</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Palash Goyal and Emilio Ferrara. 2017. Graph Embedding Techniques, Applications, and Performance: A Survey. <em><em>ArXiv e-prints</em></em> (May 2017). arxiv:1705.02801</li>
        <li id="BibPLXBIB0009" label="[9]">Aditya Grover and Jure Leskovec. 2016. node2Vec: Scalable Feature Learning for Networks. <em>In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> (<em>KDD ’16</em>). 855–864. <a class="link-inline force-break" href="https://doi.org/10.1145/2939672.2939754" target="_blank">https://doi.org/10.1145/2939672.2939754</a>
        </li>
        <li id="BibPLXBIB0010" label="[10]">Diederik&nbsp;P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. <em><em>Proceedings of the 3rd International Conference on Learning Representations</em></em> (May 2015).</li>
        <li id="BibPLXBIB0011" label="[11]">Matteo Magnani, Barbora Micenkova, and Luca Rossi. 2013. Combinatorial Analysis of Multiple Networks. <em><em>ArXiv e-prints</em></em> (March 2013). arxiv:1303.4986</li>
        <li id="BibPLXBIB0012" label="[12]">Tong Man, Huawei Shen, Shenghua Liu, Xiaolong Jin, and Xueqi Cheng. 2016. Predict Anchor Links Across Social Networks via an Embedding Approach. <em>In <em>Proceedings of the 25th International Joint Conference on Artificial Intelligence</em></em> (<em>IJCAI’16</em>). 1823–1829. <a class="link-inline force-break" href="http://dl.acm.org/citation.cfm?id=3060832.3060876" target="_blank">http://dl.acm.org/citation.cfm?id=3060832.3060876</a>
        </li>
        <li id="BibPLXBIB0013" label="[13]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. <em>In <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> (<em>KDD ’14</em>). 701–710. <a class="link-inline force-break" href="https://doi.org/10.1145/2623330.2623732" target="_blank">https://doi.org/10.1145/2623330.2623732</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Luis Solá, Miguel Romance, Regino Criado, Julio Flores, Alejandro&nbsp;García del Amo, and Stefano Boccaletti. 2013. Eigenvector centrality of nodes in multiplex networks. <em><em>Chaos: An Interdisciplinary Journal of Nonlinear Science</em></em> 23, 3, Article 033131 (Sept. 2013), 10&nbsp;pages.<a class="link-inline force-break" href="https://doi.org/10.1063/1.4818544" target="_blank">https://doi.org/10.1063/1.4818544</a>
        </li>
        <li id="BibPLXBIB0015" label="[15]">Chris Stark, Bobby-Joe Breitkreutz, Teresa Reguly, Lorrie Boucher, Ashton Breitkreutz, and Mike Tyers. 2006. BioGRID: a general repository for interaction datasets. <em><em>Nucleic Acids Research</em></em> 34, suppl_1 (Jan. 2006), D535–D539. <a class="link-inline force-break" href="https://doi.org/10.1093/nar/gkj109" target="_blank">https://doi.org/10.1093/nar/gkj109</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. <em>In <em>Proceedings of the 24th International Conference on World Wide Web</em></em> (<em>WWW ’15</em>). 1067–1077. <a class="link-inline force-break" href="https://doi.org/10.1145/2736277.2741093" target="_blank">https://doi.org/10.1145/2736277.2741093</a>
        </li>
        <li id="BibPLXBIB0017" label="[17]">Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip&nbsp;S. Yu. 2017. Multi-task Network Embedding. <em>In <em>2017 IEEE International Conference on Data Science and Advanced Analytics</em></em> (<em>DSAA ’17</em>). 571–580. <a class="link-inline force-break" href="https://doi.org/10.1109/DSAA.2017.19" target="_blank">https://doi.org/10.1109/DSAA.2017.19</a>
        </li>
        <li id="BibPLXBIB0018" label="[18]">Chang Zhou, Yuqiong Liu, Xiaofei Liu, Zhongyi Liu, and Jun Gao. 2017. Scalable Graph Embedding for Asymmetric Proximity. <em><em>31st AAAI Conference on Artificial Intelligence</em></em> (Feb. 2017). <a class="link-inline force-break" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14696/14500" target="_blank">https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14696/14500</a>
        </li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>https://comunelab.fbk.eu/data.php</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191565">https://doi.org/10.1145/3184558.3191565</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
