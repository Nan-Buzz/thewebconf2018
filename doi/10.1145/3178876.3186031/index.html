<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Creating Crowdsourced Research Talks at Scale</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186031"/></head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186031'>https://doi.org/10.1145/3178876.3186031</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186031'>https://w3id.org/oa/10.1145/3178876.3186031</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Creating Crowdsourced Research Talks at Scale</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Rajan</span>     <span class="surName">Vaish</span><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x002A;</sup></a>,     Stanford University, <a href="mailto:rvaish@cs.stanford.edu">rvaish@cs.stanford.edu</a>    </div>    <div class="author">     <span class="givenName">Shirish</span>     <span class="surName">Goyal</span>,     Stanford University, <a href="mailto:shirish.goyal@stanford.edu">shirish.goyal@stanford.edu</a>    </div>    <div class="author">     <span class="givenName">Amin</span>     <span class="surName">Saberi</span>,     Stanford University, <a href="mailto:saberi@stanford.edu">saberi@stanford.edu</a>    </div>    <div class="author">     <span class="givenName">Sharad</span>     <span class="surName">Goel</span>,     Stanford University, <a href="mailto:scgoel@stanford.edu">scgoel@stanford.edu</a>    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186031" target="_blank">https://doi.org/10.1145/3178876.3186031</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>There has been a marked shift towards learning and consuming information through video. Most academic research, however, is still distributed only in text form, as researchers often have limited time, resources, and incentives to create video versions of their work. To address this gap, we propose, deploy, and evaluate a scalable, end-to-end system for crowdsourcing the creation of short, 5-minute research videos based on academic papers. Doing so requires solving complex coordination and collaborative video production problems. To assist coordination, we designed a structured workflow that enables efficient delegation of tasks, while also motivating the crowd through a collaborative learning environment. To facilitate video production, we developed an online tool with which groups can make micro-audio recordings that are automatically stitched together to create a complete talk. We tested this approach with a group of volunteers recruited from 52 countries through an open call. This distributed crowd produced over 100 video talks in 12 languages based on papers from top-tier computer science conferences. The produced talks consistently received high ratings from a diverse group of non-experts and experts, including the authors of the original papers. These results indicate that our crowdsourcing approach is a promising method for producing high-quality research talks at scale, increasing the distribution and accessibility of scientific knowledge.</small>    </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Rajan Vaish, Shirish Goyal, Amin Saberi, and Sharad Goel. 2018. Creating Crowdsourced Research Talks at Scale. In <em>Proceedings of The Web Conference 2018 (WWW 2018).</em> ACM, New York, NY, USA, 11 pages.       <a href="https://doi.org/10.1145/3178876.3186031" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186031</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>There is growing demand for learning and consuming scientific information through video&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. This demand has in part been met by MOOCs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], which typically focus on in-depth presentations of established areas, and by efforts such as &#x201C;Two Minute Papers&#x201D; and &#x201C;Papers We Love&#x201D;, which distill scientific ideas for viewers with limited technical expertise. But the vast majority of contemporary research is still available only in the form of text, as traditional academic papers, in part because individual researchers often have limited time, resources, and incentives to produce video-based summaries of their work. This gap prompts a challenge: distilling the content of academic papers into short presentations suitable for students and researchers, and doing so at scale.</p>    <p>Here we introduce and evaluate a system for creating an open, multilingual repository of 5-minute lightening talks developed collaboratively by volunteers worldwide. These talks are catered to technically knowledgeable viewers, who after watching the video summary might read the original papers or attend a longer conference presentation. The initial videos are produced by distributed teams of individuals working in close collaboration; the videos can subsequently be edited and improved by any interested participant. Our project increases the accessibility of scientific knowledge by converting English-language research papers into video-based talks produced in multiple languages&#x2014;and all without involving the authors of the paper or other domain experts. In the process of creating this content, volunteer contributors learn collaboratively, furthering educational opportunities and incentivizing participation.</p>    <p>Crowdsourcing such an open-ended expert task poses two key challenges. First it is not immediately clear how to collaboratively produce editable videos. Second, one must facilitate extended and complex coordination between large, distributed groups of individuals of varying expertise. To address the first task, we standardize each talk to consist of slides, a written script, and voice-overs; we then programmatically stitch these components together to produce a complete video presentation. To streamline this process, we created an online tool that lets people collaborate and seamlessly record audio on a slide-by-slide basis. Our modular approach supports efficient editing and reduces retake time, both during and after the initial videos are created. We address the second challenge by designing a structured scaffolding process to coordinate volunteers&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>]. Specifically, we divide the talk creation process into three discrete phases spanning a period of 21 days (three weeks): (1) on-boarding the crowd and forming teams; (2) generating a slide deck that includes both the talk slides and a slide-by-slide script of the talk; and (3) converting the script to slide-by-slide audio recordings, and reviewing the complete video presentation.</p>    <p>To test this system, we issued an open call for participation, attracting 840 people from 52 countries. This crowd of volunteers created 107 lightening talks in 12 languages based on 40 recent papers from top-tier computer science conferences and scientific journals. These talks were entirely created by the crowd, from designing the structure to producing the content. To evaluate the talks, we solicited over 300 responses from outside reviewers, including 73 responses from the authors of the original papers. The talks were rated highly on both presentation quality and utility, receiving a median score of 4 out of 5 on both dimensions.</p>    <p>To explore the applicability of our approach for creating longer and more in-depth content, we experimented with developing technical tutorials on Python and machine learning. These tutorials ranged in length from 30 minutes to 5 hours, and were produced in multiple languages. As with the short research talks, these technical tutorials received high marks by outside evaluators.</p>    <p>Our primary contribution in this work is developing an end-to-end process for producing short, technical research talks from academic papers. To accomplish this, we introduce a new crowdsourcing workflow for achieving open-ended creative goals, built an online tool to facilitate collaborative video production, and analyzed a large-scale, long-term deployment of the method. Our work points to the potential for scalable creation and dissemination of video-based technical content, which we hope will help increase the exposure and accessibility of academic research.</p>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>Our work relates to and builds on several intertwined threads of research. We draw on work in collaborative communities to build the crowd&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], research in organizational behavior to coordinate the crowd&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], ideas in video production to engineer online collaboration tools&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>], and studies in microproductivity to scale the system&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0049">49</a>]. Below we briefly survey these areas.</p>    <section id="sec-7">    <header>     <div class="title-info">      <h3>Communication of scientific ideas</h3>     </div>    </header>    <p>The presentation of scientific ideas&#x2014;for both experts and non-experts&#x2014;is a core function of the academic community. Gaps in communication may be partly responsible for widespread science illiteracy among non-experts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0057">57</a>], and may hinder scientific progress among experts. This communication challenge has been addressed in part by the rise of data journalism and blogging&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>], and by tools for article discovery and summarization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0050">50</a>].</p>    <p>Among experts, the primary mode of science communication is text, in the form of papers. But in many domains, a rapid shift is underway towards consuming information in multimedia formats&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>]. In advertising&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>], education&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>], and health&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], videos are becoming an important means of communication and learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>]. In response to this shift, there are several initiatives to create videos that introduce research papers&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0059">59</a>] and present their findings&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. Those efforts, however, are generally driven by a small group of individuals, and lack the resources necessary to scale. Similarly, conference organizers like the ACM often have limited video production resources, and equipment is provided to conferences on a first-come, first-serve basis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. By utilizing crowdsourcing, our work helps to scale the process of video production and to disseminate research.</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>Crowd workflows and organizational behavior</h3>     </div>    </header>    <p>Crowdsourcing techniques have helped researchers and industry professionals scale a variety of efforts, from microtasks like image labeling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0052">52</a>] and translation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0048">48</a>] to creative expert tasks like producing animations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] and designing software prototypes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]. The successful crowdsourcing of expert tasks often relies on multi-stage crowd workflows and organizational structures to facilitate complex collaboration.</p>    <p>Research utilizing crowd workflows has inspired the design of our approach. Soylent&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] showed that splitting tasks into the find-fix-verify stages can improve the quality and accuracy of crowd workers&#x2019; results. Multi-stage crowdsourcing workflows have also been designed to improve the tone of emails&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0053">53</a>], to provide critiques to designers&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], and to improve the learning experience of existing how-to videos with step-by-step annotations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. These applications have demonstrated that crowd workflows can yield results comparable to those of experts. Our work contributes to this line of research by introducing and evaluating a new three-phased workflow tailored to the production of short research talks, an open-ended expert task.</p>    <p>Along with workflow design, research on organizational behavior offers insights for enabling effective team coordination. We draw on such work to develop our scaffolding process for crowd collaboration&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>]. Past work has identified several obstacles to effective team coordination&#x2014;such as technology-mediated communication, geographic dispersion, and dynamic team membership&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>]&#x2014;and has proposed solutions for mitigating attrition and motivating volunteers. Many of these findings are based on existing platforms with a critical mass of crowd workers, like Wikipedia, NewGround, and oDesk. For our real-world deployment, we built a community from scratch, and thus adapted these findings to our setting. For example, unlike asynchronous collaboration&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] that is popular when a large crowd is available, we rely on synchronous team-based collaboration&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>] to ensure successful task completion. Noting the importance of roles and leadership in online collaborative environments&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], we rely on &#x201C;directly responsible individuals&#x201D; (DRIs)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] to lead the dynamic teams.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>Crowdsourced creative production</h3>     </div>    </header>    <p>Crowdsourcing creative tasks in an online, distributed environment is inherently challenging. Past work has typically utilized a paid-crowdsourcing marketplace, a competition-based approach, or engaged the crowd only for specific, well-scoped subtasks. Projects like Ensemble&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] mobilized a crowd from Amazon Mechanical Turk to create stories, while Flash Teams&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] used a crowd of Upwork workers to design software prototypes and animation videos. On Tongal&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], requesters can crowdsource multiple ideas and detailed pitches for video production. However, unlike collaborative crowd-based idea generation systems like the IdeaHound&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>], Tongal uses a paid, competition-based approach. Rather than collaborating, teams&#x2014;often including professional media studios&#x2014;compete against one another for the best idea, best pitch and a contract to produce the video. Winners for each phase are chosen by the Tongal staff and requesters, not the community. Initiatives like the Johnny Cash Project&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] mobilized volunteers to draw their own portrait of Johnny Cash to be integrated into a collective music video. In this case, though the creative content was crowdsourced, the task was quite specific and there was little collaboration between crowd members. In contrast to past efforts, our work mobilizes a volunteer crowd to collaborate with participants worldwide throughout the entire creative process, from selecting research papers to final video production.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>Collaborative video creation</h3>     </div>    </header>    <p>Despite recent advances in audio and video production technology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0051">51</a>], options for collaborative editing are still limited&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] and are often proprietary&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>]. Most existing video production systems rely on a traditional timeline model, in which contributors make frame-by-frame edits. We introduce a new approach to collaborative video production. By drawing on and advancing research in microtasking&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0049">49</a>], we produce research talks by separately creating talk slides, written scripts, and audio recordings. These components are then programmatically stitched together to create a complete video presentation.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>Online learning</h3>     </div>    </header>    <p>Collaborative, online learning is an active area of research&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0056">56</a>]. Studies show that collaborative learning on a global scale enhances critical thinking&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>] and engagement&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>], and varies along cultural dimensions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0054">54</a>]. Its positive effects have been applied in academia and industry&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. Additionally, research in education and &#x201C;learnersourcing&#x201D;&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0058">58</a>] suggests that presenting learners with subgoals for procedural tasks improves learning. Margulieux et&#x00A0;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] showed that instructions including both specific steps and subgoals resulted in improved learning and transfer compared to those with the specific steps alone. Combining these findings, we set up a collaborative learning environment in Slack, and designed the workflow with subgoals such as paper reading and analysis. The crowd collaboratively worked toward these goals, learning about the latest research in computer science while producing educational content for others.</p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> SYSTEM DESIGN</h2>    </div>    </header>    <p>As outlined in Figure&#x00A0;<a class="fig" href="#fig1">1</a>, our approach to crowdsourcing the creation of research videos proceeds in three phases, spanning 21 days in total. To achieve scale, multiple talks are produced in parallel following the same timeline. The first phase involves on-boarding the crowd, forming teams, and reading the papers to be converted. The second phase entails creating a slide deck and a written script for each paper. In the third phase, the crowd converts the scripts to slide-by-slide audio recordings, and the completed video talks are then reviewed for final improvements. Before the formal talk creation process begins, we recruit a crowd through an open call for participation, as discussed in Section&#x00A0;<a class="sec" href="#sec-21">4.1</a>. Our deployments typically involve creating 10 talks in parallel, with a total of 50&#x2013;100 active participants. We describe this process in detail below. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">We use a three-phase, three-week long structured workflow to convert research papers to video talks.</span>      </div>     </figure> </p>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Phase 1: On-boarding, paper selection, team formation, and paper reading</h3>     </div>    </header>    <p>This first phase spans a period of five days. We start by providing participants documentation about the tools we use, instructions about the workflow, and best practices to follow. As people worldwide participate and contribute, the best practices continue to evolve and grow. The crowd next selects papers to convert into talks. This process proceeds in a free-form fashion, with participants proposing papers and soliciting votes of support from others. The most popular papers are selected for conversion, and participants then choose which paper team to join. Participants are free to join any team, irrespective of their location; there is no cap on team size. Each team works toward one talk, based on one paper. After joining a team, each member begins reading the paper. Communication between team members is primarily carried out on Slack, a popular tool for text-based messaging.</p>    <p>By letting participants select the papers themselves, it helps ensure crowd workers have adequate knowledge in the topic. It also seems like a reasonable policy since participants are volunteering their time. This flexibility, however, can result in favoring papers that are already well-known, exacerbating the so-called Matthew effect in science&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], in which the &#x201C;rich get richer and the poor get poorer&#x201D;. Indeed, some of the papers selected by the crowd (e.g., &#x201C;Mastering the Game of Go with Deep Neural Networks and Tree Search&#x201D;&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>]) had already garnered significant attention in the media. By and large, though, we found that the talks selected by the crowd span a diverse range.</p>    <p>After selecting the papers and joining teams, each team selects two to three <em>directly responsible individuals</em> (DRIs) to oversee the talk creation process&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. DRIs set the tone of the talk, help team members with any problems they might encounter, and ensure the team adheres to the timeline while meeting quality standards. DRIs are a sought-after role, as it confers decision-making power to execute one&#x0027;s vision. Any team member can apply to be a DRI by stating their interest, availability, and expertise in the topic. After a 24-hour window, the team votes and choses a group of DRIs. Having multiple DRIs helps ensure availability across all timezones. To encourage participation, any inactive DRIs are replaced by an active member of the team; this dynamic strategy helps keep DRIs active, while motivating other team members to work consistently to get a chance to become a DRI.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Phase 2: Creation of slide deck and script</h3>     </div>    </header>    <p>In the second phase, participants begin actively collaborating with their teammates. Phase 2 is itself broken down into three steps. First, the team collectively completes a questionnaire to help participants make sense of the paper they selected and read in Phase 1. Second, they work together to put together the talk slides and slide-by-slide written scripts of what will ultimately be converted to audio. Finally, teams offer feedback on one another&#x0027;s work.</p>    <section id="sec-15">     <p><em>Step 1: Paper analysis.</em> To help crowd members&#x2014;who are typically non-experts&#x2014;think critically about the paper they are tasked to present, we require them to collectively answer a series of questions. What is the contribution of the paper? Why is the problem hard? How did authors evaluate the experiment? In total we pose 17 questions, and allot two days to complete this task. Crowd members collaborate and are encouraged to build off of and edit each other&#x0027;s work, and the DRIs in particular help to synthesize answers into a coherent whole. The tone of the talk is largely determined by responses to these questions.</p>    </section>    <section id="sec-16">     <p><em>Step 2: Content creation.</em> This second step, which spans seven days, involves creating the entire slide deck, along with written scripts to accompany each slide. Communication is carried out on Slack, and the slides themselves are created with Google Slides, which allows efficient real-time collaboration. The team is free to organize in any manner they see fit to accomplish the task. To promote high-quality content that is consistent across teams, we encourage crowd members to adhere to the following guidelines.</p>     <ol class="list-no-style">      <li id="list1" label="(1)">Limit talks to approximately five minutes. Based on past literature&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>] and informal pilot studies, we found that short, five-minutes videos were sufficient to convey the key ideas while still maintaining audience engagement and interest.<br/></li>      <li id="list2" label="(2)">Target an audience of viewers with technical expertise comparable to those who might read the paper or attend a conference presentation on the research. The papers often assumed a certain level of technical sophistication, and we mirrored this assumption for the talks, limiting discussion of background material. However, particularly for such short talks, this still meant focusing on the key contributions of the paper rather than on detailed technical descriptions.<br/></li>      <li id="list3" label="(3)">Write the slide-by-slide scripts exactly as they should be spoken. As described below, the audio recordings are created by individuals who may not have been directly involved in creating the slides. As such, it is critical that the written scripts indicate precisely what should be recorded for each slide. These scripts are added to the &#x201C;notes&#x201D; section of each slide of the slide deck.<br/></li>     </ol>     <p>In addition to providing these guidelines, we point participants to past talks produced by the crowd, as it is often easiest to learn by example. We further provide teams with a set of best practices compiled by previous teams. These include, for example, suggestions to use simple language and to explain the key aspects of figures on slides. Finally, we provide crowd members with a pair of articles on creating effective presentations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0055">55</a>].</p>     <p>Team members were encouraged to improve each other&#x0027;s work by editing or commenting. If there are diverging opinions or conflicts, DRIs serve as arbiters and have final decision-making power. To help motivate crowd members through positive reinforcement, we introduced a simple mechanism for peer acknowledgement. Throughout the talk creation phase, anyone could give a &#x201C;+1 thank you&#x201D; to anyone else for their contribution. These acknowledgements were not intended to result in any specific tangible rewards, but rather were meant to show appreciation for a job well done.</p>    </section>    <section id="sec-17">     <p><em>Step 3: Peer review.</em> In the final three days of Phase 2, crowd members offer feedback to other talk teams, and work on addressing the comments that they in turn receive. This review process gets fresh eyes on each presentation, and garners valuable input from individuals who were not involved in creating the talks they are evaluating. Reviewing happens in a free-form iterative process, with talk creators rapidly incorporating feedback and then soliciting more reviews&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>]. Teams that address feedback faster can iterate more times; there is no limit to the number of iterations a team can go through.</p>    </section>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Phase 3: Recording audio and compiling the video presentation</h3>     </div>    </header>    <p>At this point in the process, teams have completed their slides and scripts. As current text-to-speech systems&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] are not yet able to produce fully natural audio, crowd participants record the audio themselves. This recording process spans two days, and is facilitated by an online tool that we built, which we call Audio Studio. Audio Studio is a Django application, with front-end built on AngularJS. As shown in Figure&#x00A0;<a class="fig" href="#fig2">2</a>, Audio Studio starts with the slide deck as input and proceeds in three steps. First, individual slides are extracted and paired with their accompanying scripts. Second, crowd members record audio clips for each slide. Finally, these audio clips are programmatically stitched together with the slides into a complete video presentation. We describe these three steps in more detail below. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Workflow of Audio Studio, a tool to facilitate collaborate video production. Starting from a slide deck and script, Audio Studio first splits the deck into individual slides and accompanying scripts. Any crowd member can then record audio clips for each slide by reading the script. The slides and audio are then programmatically stitched together to create a complete video.</span>      </div>     </figure> </p>    <ol class="list-no-style">     <li id="list4" label="(1)"><strong>Splitting the deck.</strong>Audio Studio is designed to minimize editing and retake time by splitting the slide deck into individual slides for which audio can be recorded in isolation. Given the URL to a presentation in Google Slides, Audio Studio uses the Google Drive API to extract individual slides as PDFs together with the slide-by-slide scripts stored in the &#x201C;notes&#x201D; section of each slide.<br/></li>     <li id="list5" label="(2)"><strong>Audio recording.</strong>Any team member can record audio for any slide by reading off the narrative script displayed next to it (as shown in Figure&#x00A0;<a class="fig" href="#fig3">3</a>). Contributors can replay and re-record audio until they are satisfied with the quality. We do not impose a strict cap on the number of people who can contribute audio to each presentation, but we encourage teams to limit recordings to two people. We further encourage contributors to record continuous slide sections. For example, one individual might record the first half and a second might record the latter half. These contributors are selected by vote using SimplePoll&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0005">5</a>] on Slack after an informal audio audition, where the interested team members record a one-minute long introduction of themselves.<br/> <figure id="fig3">       <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig3.jpg" class="img-responsive" alt="Figure 3"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Audio Studio&#x0027;s recording interface, with slide and script next to one other. Contributors can review and re-record audio clips before submitting.</span>       </div>      </figure> </li>     <li id="list6" label="(3)"><strong>Video creation.</strong>After audio recordings are submitted for every slide in the deck, DRIs review them for quality, and can request improvements if necessary. Our modular approach makes retakes relatively easy. Once the DRIs are satisfied with the recordings, the audio and slides are stitched together with the FFmpeg library to generate a complete video; this compilation step is done automatically within Audio Studio at the click of a button.<br/></li>    </ol>    <p>After the initial video presentation is produced, teams have two final days to review it and incorporate any additional edits. Our modular approach to the creation of slides, scripts, and audio facilitates rapid editing of all talk components. Once the DRIs approve the final talk, the video is published on YouTube and on the platform&#x0027;s website for the general public.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Localization</h3>     </div>    </header>    <p>The initial creation of talk presentations is carried out in English, as described in the three-phase process above. Crowd members can then self-organize to develop localized versions of any previously created talk. The crowd forms teams dynamically based on their language of expertise and interest. They are free to utilize any existing assets&#x2014;including the slides and scripts&#x2014;and can edit these as they see fit. The localization process can be challenging for several reasons: certain scientific terms or phrases in English do not always have suitable translations; some languages (e.g., Arabic and Japanese) had few contributors in our community; and the crowd must translate both the written scripts as well as text on the slides. Localization was typically carried out by individuals working alone or in small teams, peer-reviewed for quality, and was completed in an unstructured fashion without specific deadlines or DRIs.</p>    </section>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Real-World Deployment</h2>    </div>    </header>    <p>To evaluate our system for creating crowdsourced research presentations, we launched an online platform called Stanford Scholar<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> that was open to individuals worldwide.</p>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> An open call for participation</h3>     </div>    </header>    <p>Before the formal talk creation process began, we recruited a crowd through an open call for participation. This global call was made online via social media platforms (e.g., public Facebook and LinkedIn groups and Twitter), public mailing lists (e.g., UW Change and Berkeley TIER), online platforms (e.g., LetMeKnow.in, StudentCompetitions.com), and emails sent to universities worldwide. Such solicitations were repeated periodically. The call sought to motivate potential participants with a chance to help disseminate research ideas while learning about cutting-edge work in a collaborative environment. Contributors were not paid or otherwise compensated, and their participation was voluntary.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Participant demographics</h3>     </div>    </header>    <p>Our open call for volunteers resulted in 840 sign ups from 52 countries on 6 continents. The majority of participants came from the United States (29%) and India (48%). Participants spanned the educational spectrum. The distribution of highest degree attained or in progress was: 11% high school diploma, 59% undergraduate degree, 24% masters, and 6% Ph.D. 25% of participants were women, and the median age of crowd members was 22 years-old. These participants primarily had a background in computer science or other engineering-related fields. Participants typically had moderate technical expertise in the topic of the papers they worked on. Based on self-reports, the mean expertise was 2.7 (median was 3) on a scale from 1 to 5, with 1 being &#x201C;novice&#x201D; and 5 being &#x201C;expert&#x201D;. Two-thirds of participants had no prior experience preparing or giving a research talk before.</p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Outcomes</h3>     </div>    </header>    <p>We carried out two 21-day talk creation rounds following the procedure outlined above. In each iteration, the crowd formed 10 teams to create 10 English-language talks. In preliminary work to develop the process, we ran two additional rounds that approximated our final approach, but which differed in some aspects (e.g., the earlier rounds had a less defined structure). In total, the four iterations of our deployment resulted in the creation of 40 English-language research presentations derived from 40 distinct papers. An additional 67 videos were created in 11 foreign languages, for a total of 107 presentations. To date, these videos have attracted over 50,000 views, 400 shares, and 800 subscribers on YouTube.</p>    <p>The presentations were based on 40 papers published at top-tier computer science conferences and scientific journals in the past two years. These papers spanned the spectrum of computer science, including human-computer interaction, data mining, machine learning, and security. Specifically, the papers were published in: WWW (7), CHI (4), UIST (3), KDD (2), AAAI (2), IJCAI (2) CSCW (2), Nature (1), ICML (3), NIPS (1), SIGMOD (1), CVPR (1), WSDM (1), EuroCrypt (1), OOPSLA (1), VLDB (1), ICWSM (1), SIGIR (1), ECCV (1), IROS (1), ICLR (1), NAACL-HLT (1), and UBICOMP (1). Papers were selected by crowd members, typically from among the best-paper award winners at the conference.</p>    <p>To make scientific knowledge accessible around the world, it is important to distribute research findings in multiple languages. Among the strengths of crowdsourcing is its scalability, and its ability to leverage diverse skills and expertise. Based on the 40 English-language presentations, crowd members created 67 versions in 11 foreign languages: Chinese, Hindi, Spanish, Catalan, Romanian, Oriya, Nepali, Malayalam, Japanese, Filipino and Tamil. As our initiative continues, crowd members have started localizing content into several more languages, including Asante Twi, Albanian, Assamese, Greek, French, Arabic, and Persian. Figure&#x00A0;<a class="fig" href="#fig4">4</a> illustrates some examples of localized content produced by the crowd.    <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Sample slides from talks produced in Japanese, English, Oriya, Chinese, Spanish and Hindi (from left to right, top to bottom). Of the 107 research talks produced, 67 were in foreign languages.</span>     </div>    </figure> </p>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Engagement</h3>     </div>    </header>    <p>A campaign&#x0027;s success depends on the continued participation by its members in multiple capacities. In our initiative, Slack was the primary medium of communication and collaboration. In addition to a common channel, each talk had its own public channel. During the course of the project, over 100,000 messages were posted to Slack.</p>    <p>During each 21-day cycle of video production, the crowd worked on 10 talks in parallel and an average of approximately 1,000 messages were exchanged every day. As shown in Figure&#x00A0;<a class="fig" href="#fig5">5</a>&#x2014;which corresponds to one specific 21-day round&#x2014;participation and discussion about the talk increased over the weekend and subdued during weekdays. This behavior likely stems from the fact that this is a voluntary activity, with participants busy at school and work during the week. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Slack activity during one 21-day video production round, in which 10 talks were simultaneously produced. The vertical lines separate the round into the three development phases. Activity is typically higher on weekends and after a new phase begins.</span>      </div>     </figure>    </p>    <p>On average, talk teams consisted of about 25 crowd members, with about 10 who were regularly active and made substantial contributions. Across the batch of 10 talks created in parallel in a single round, approximately 50 people posted messages to Slack each day, and more than 100 people read the posted messages. Figure &#x00A0;<a class="fig" href="#fig6">6</a> shows a gradual decline in the number of people participating each day, which is typical of voluntary initiatives. Nonetheless, we maintained a large contingent of contributors throughout the talk development process.     <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Slack activity during one video production round, in which 10 videos were simultaneously generated, broken down by number of people reading and writing messages each day. The vertical lines separate the round into the three development phases. Participation gradually declines but there is a critical mass of contributors throughout the development cycle.</span>      </div>     </figure></p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Evaluation</h2>    </div>    </header>    <p>We conducted several surveys to assess the effectiveness and value of our crowdsourcing approach. These surveys measured: (1) presentation quality and educational value for talk viewers; and (2) process quality and learning experiences for talk creators. To do so, we surveyed three separate groups.</p>    <ol class="list-no-style">    <li id="list7" label="(1)"><strong>Paper authors</strong>. We reached out to the authors of the 40 papers on which the presentations were based. As clear subject-matter experts, their feedback provided valuable insight and assessment of the quality of the crowd-generated talks. In total, 73 authors responded to our survey.<br/></li>    <li id="list8" label="(2)"><strong>External evaluators</strong>. Besides authors, we reached out to people not affiliated with the papers, from non-experts to domain experts. We solicited responses from various university groups, and also directed people who visited the project website to take our survey after watching a video. We requested input on overall presentation quality and communication of the research results. In total, we received 260 responses.<br/></li>    <li id="list9" label="(3)"><strong>Participating crowd</strong>. We requested feedback on the initiative from participants who created the talks, and received 95 responses in total. This group provided feedback on the talk creation process and their experiences, but did not evaluate the quality of the completed talks.<br/></li>    </ol>    <section id="sec-26">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Presentation quality and educational value</h3>     </div>    </header>    <p>Overall talk quality was evaluated on a Likert scale from 1 to 5, with 5 being &#x201C;excellent&#x201D; and 1 being &#x201C;poor&#x201D;. Among the 73 paper authors we surveyed, the talks were typically rated highly (mean = 4.1, median = 4). Authors also generally thought the talks were useful for someone trying to get an overview of their papers. Talks received a mean of 4.2 and median of 4 on this dimension, with 5 being &#x201C;extremely useful&#x201D; and 1 being &#x201C;not useful&#x201D;. The quotes below illustrate some of the reactions we received from paper authors.</p>    <Quote>     <p>&#x201C;The talk is extremely thorough despite its brevity. It&#x0027;s certainly better than the talk I gave at WWW.&#x201D;</p>    </Quote>    <Quote>     <p>&#x201C;It does such a great job at motivating the research problem and covers the gist of the paper very well, in language that is engaging to the broader audience.&#x201D;</p>    </Quote>    <p>Not all authors, however, thought the 5-minute format was sufficient to communicate a paper&#x0027;s contribution, as the following quote indicates.</p>    <Quote>     <p>&#x201C;The presentation is overall way too fast, so I am not sure how helpful it is for someone who is not already quite deep into the topic. On the positive side, the presentation might indeed give some intuition for the key contributions of the paper.&#x201D;</p>    </Quote>    <p>External evaluators&#x2014;who were not affiliated with the papers&#x2014;echoed the feedback we received from paper authors. Based on 260 responses from this group, the quality of the talks was again rated highly (mean = 3.7, median = 4). Respondents also indicated that they successfully learned the key contributions of the paper, rating their understanding at a mean of 3.7 and median of 4, with 5 being &#x201C;extremely well&#x201D; and 1 being &#x201C;not at all&#x201D;. Notably, a majority of respondents (65%) indicated that they would rather spend five minutes watching the video than skimming the paper. The quotes below show some of the feedback we received.</p>    <Quote>     <p>&#x201C;I found this paper to be quite abstract and hard to understand, so the video was a helpful summary. This video format is ideal for cases like this where the message of the paper can be hard to skim.&#x201D;</p>    </Quote>    <Quote>     <p>&#x201C;I was pleasantly surprised by how much substance was covered in five minutes. The video seems to have a nice balance between controlling the total duration and still going into a reasonable amount of depth.&#x201D;</p>    </Quote>    <Quote>     <p>&#x201C;Sometimes it&#x0027;s difficult to understand the methodology, flow of system or algorithm while reading paper, but in videos with visualization, using charts, flow diagrams or by showing an example it&#x0027;s easy to understand.&#x201D;</p>    </Quote>    <p>Figure&#x00A0;<a class="fig" href="#fig7">7</a> indicates that ratings by external evaluators were somewhat higher for those with basic technical expertise in the topic, likely because the talks were explicitly tailored to this group. Of the 260 responses from external evaluators, 73% came from people who rated their expertise as 3, 4 or 5. This subgroup of respondents with domain expertise rated the talk quality at a mean of 3.8 (median = 4) and its understandabilty at a mean of 3.8 (median = 4). In contrast, evaluators without such domain expertise rated talk quality at a mean of 3.4 (median = 3) and its understandability at a mean of 3.3 (median = 3). A t-test shows that these group differences are statistically significant at the 1% level (<em>p</em> = 0.005 for quality, and <em>p</em> = 0.0002 for understanding). <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186031/images/www2018-40-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Ratings for video quality and understandability as a function of the evaluator&#x0027;s expertise in the topic.</span>      </div>     </figure> </p>    </section>    <section id="sec-27">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Crowd participation and learning experiences</h3>     </div>    </header>    <p>Our approach was designed to attract and retain volunteers to create high-quality talks in a collaborative, educational environment. It is thus important to measure participants&#x2019; perceptions of the initiative to gauge its sustainability. Overall, participants indicated the initiative was a high-quality experience (mean rating = 4.5, median = 5, with 5 being &#x201C;great&#x201D;). Crowd members also indicated support for various structural elements of the process. For example, participants found iterative talk review very useful (mean rating = 4.4, median = 5, with 5 being &#x201C;extremely useful&#x201D;). And participants rated the Audio Studio recording system as easy to use (mean rating = 4.0, median = 4, with 5 being &#x201C;absolutely simple&#x201D;).</p>    <p>Our survey indicates that crowd volunteers were motivated to participate in the initiative by an opportunity to learn about research while contributing to a public good. Based on a multiple-choice question answered by 75 crowd members, we found that the top three reported motivations were learning about different research topics (93%), collaborating with people worldwide (82%), and increasing research access (69%). These quantitative results are also reflected in the written feedback we received from participants.</p>    <Quote>     <p>&#x201C;I had always wanted to read through research papers on hot topics of computer science. But I could never get started. This program not only inspires me to read through a paper but requires me to understand it enough, so as to create a talk on it. And learning is always fun when more people are learning with us.&#x201D;</p>    </Quote>    <Quote>     <p>&#x201C;Before joining the program, I had wanted to contribute to the cause of education and separately learn about the latest trends and research in Computer Science and Technology. The initiative came as a single opportunity for both. 9 months after joining and having contributed to creation of 3 versions of talk creation, I have gained insights into Machine Learning, Deep Learning and HCI research. I am now in a position to contribute to research in a more informed way. I also learned collaboration and teamwork by working with incredible teams of smart and knowledgeable people working from locations all around the world. I also learned presentation skills as we had to interpret and present the real essence of complex research papers in 5 min talks. All these skills will surely be very valuable to me in the future.&#x201D;</p>    </Quote>    <p>As the quotes above suggest, reading papers and creating talks helped crowd participants learn about research topics. Following established self-assessment techniques in education&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>], we conducted a survey to better understand this learning experience. We found that before working on the talk, participants rated their expertise in the topic of the paper at a mean of 2.6 on a scale from 1 to 5 (median = 3, N = 29), where 1 is &#x201C;novice&#x201D; and 5 is &#x201C;expert&#x201D;. After working on the talk and researching the topic for three weeks, these ratings rose to a mean of 3.4 (median = 4, N = 29). This difference is statistical significant at the 1% level (<em>p</em> = 4e &#x2212; 9) with a paired t-test. Further, among 75 surveyed crowd members, they generally indicated having a positive learning experience, rating their experience at a mean of 4.2 (median = 4), where 1 meant &#x201C;did not learn anything&#x201D; and 5 meant &#x201C;learned a lot&#x201D;.</p>    </section>   </section>   <section id="sec-28">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Creating in-depth lectures</h2>    </div>    </header>    <p>Our primary focus in this work was on developing short research presentations; however, our structured crowdsourcing approach also has promise for producing longer and more in-depth lectures. To investigate this potential, we had the crowd create four extended video tutorials in several languages, ranging from 30 minutes to nearly 5 hours. These tutorial were: (1) &#x201C;A Short Introduction to Python&#x201D; (37 minutes); (2) &#x201C;An Extended Introduction to Python&#x201D; (50 minutes); (3) &#x201C;An Introduction to Algorithms&#x201D; (195 minutes); and (4) &#x201C;Practical Machine Learning with Python&#x201D; (265 minutes). All videos were originally created in English, and the first lecture (&#x201C;A Short Introduction to Python&#x201D;) was additionally translated into Arabic, Hindi, Spanish, and Catalan.</p>    <p>These extended lectures were created following the same basic process as that carried out for the 5-minute research talks, but with two key differences. First, the paper analysis step was replaced with syllabus creation. The goal of this step was two-fold: to determine the overall structure of the lecture, and to partition the lecture into short segments of approximately 5-10 minutes that could be carried out in parallel by different teams. Second, given the inherently interrelated nature of the video content, teams would more regularly interact with one another to ensure consistency and flow. The extended videos were rated by 44 external evaluators. In line with reviews for the research talks, these lectures were typically rated highly, receiving a mean rating of 3.9 and median of 4 out of 5, with 5 being &#x201C;excellent&#x201D;.</p>   </section>   <section id="sec-29">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Discussion &#x0026; Conclusion</h2>    </div>    </header>    <p>There is pressing need to develop new ways to make scientific knowledge available to diverse global audiences. To address this challenge, we developed and evaluated a structured, three-phase approach to crowdsourcing the creation of 5-minute research talks. In the first phase, volunteer crowd members learned about the process, selected papers to present, and formed teams. In the second phase, team members collaborated to critically analyze their selected paper, and to create a slide presentation along with a written slide-by-slide script of the talk. In the third phase, participants used our Audio Studio application to convert the written scripts to audio clips. These audio clips and the slides were programmatically stitched together to create a complete video presentation. In total, volunteers from 52 countries created over 100 talks in 12 languages on papers from top-tier computer science conferences, including WWW, KDD, CHI, and CSCW. Ratings and comments from both the papers&#x2019; authors and outside evaluators indicate the created videos were consistently high-quality. To date, these videos have attracted over 50,000 views, 400 shares, and 800 subscribers on YouTube.</p>    <section id="sec-30">    <header>     <div class="title-info">      <h3>Reflections and lessons from our study</h3>     </div>    </header>    <p>We believe three key design choices contributed to the success of our method: modularity, structure, and community. By decomposing talks into slides, written scripts, and audio recordings, we could successfully overcome common challenges of collaborative video production. These pieces were further separated into micro-segments whose creation could be efficiently parallelized. Though not a universal solution for collaboratively making all types of videos, we demonstrated that such modularity works well for creating research talks and in-depth technical lectures. This general design pattern may prove useful in a variety of open-ended, presentation-based applications, from education to journalism.</p>    <p>A structured, fault-tolerant workflow&#x2014;with a well-defined timeline and dynamic role allocation&#x2014;helped maintain engagement and commitment from the volunteer crowd. This structure helped non-experts quickly learn difficult technical material and produce high-quality content in a relatively short period of time. We note that in informal pilot studies without this structure, it was difficult to achieve high quality&#x2014;or even complete videos&#x2014;as crowd members lacked direction and were often working alone. Our dynamic DRI system ensured that teams were continually led by active members, replacing inactive DRIs with active participants.</p>    <p>Finally, we prioritized creating a vibrant community of participants to facilitate sustainability. For example, we adopted a team-based approach to encourage meaningful interactions between participants. We also allowed the crowd to self-organize and provided freedom for self-expression and control over the final product. Many participants appreciated such design choices, stating that their primary motivation for participating was this opportunity to learn challenging scientific concepts in a supportive community. Understanding and responding to such motivations is critical in the case of voluntary crowd work.</p>    </section>    <section id="sec-31">    <header>     <div class="title-info">      <h3>Future work</h3>     </div>    </header>    <p>By creating an open repository of multilingual research talks, we have sought to advance the dissemination of scientific knowledge while simultaneously providing a collaborative learning environment for individuals across the world. Moving forward, we plan to focus our efforts on three fronts. First, we aim to create more long-form videos, and to refine our process for collaboratively producing complex, interconnected material. Our preliminary efforts in this direction have produced encouraging results. Second, we seek to continue improving talk quality. Although our current approach consistently yields high-quality talks, there is always room for improvement. For example, one might experiment with animation or interaction to increase audience engagement. Finally, we plan to investigate the longer-term sustainability and scalability of our approach. The results of our study demonstrate it is possible to create a vibrant community of hundreds of volunteers who can work together productively with little external oversight; and to date, this community has thrived for over a year. The next challenge is building an even larger community, one that can produce thousands of presentations on diverse topics, both within and beyond computer science. Our approach is one step toward increasing access to and understanding of scientific research; we hope our work spurs further such efforts.</p>    </section>   </section>  </section>  <section class="back-matter">   <section id="sec-32">    <header>    <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>    </div>    </header>    <p>We thank hundreds of participants of the Stanford Scholar community for their contributions. This work was supported by the Office of Naval Research awards N00014-16-1-2893 and N00014-15-1-2711.</p>   </section>   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">2016. Papers We Love. (2016). <a class="link-inline force-break" href="http://paperswelove.org/">http://paperswelove.org/</a>.</li>    <li id="BibPLXBIB0002" label="[2]">2016. WeVideo. (2016). <a class="link-inline force-break" href="https://www.wevideo.com/">https://www.wevideo.com/</a>.</li>    <li id="BibPLXBIB0003" label="[3]">2017. The Johnny Cash Project. (2017). <a class="link-inline force-break" href="http://www.thejohnnycashproject.com/">http://www.thejohnnycashproject.com/</a>.</li>    <li id="BibPLXBIB0004" label="[4]">2017. NovoEd. (2017). <a class="link-inline force-break" href="http://novoed.com/">http://novoed.com/</a>.</li>    <li id="BibPLXBIB0005" label="[5]">2017. SimplePoll. (2017). <a class="link-inline force-break" href="https://simplepoll.rocks/">https://simplepoll.rocks/</a>.</li>    <li id="BibPLXBIB0006" label="[6]">2017. Tongal. (2017). <a class="link-inline force-break" href="https://tongal.com/how">https://tongal.com/how</a>.</li>    <li id="BibPLXBIB0007" label="[7]">ACM. 2017. SIGCHI Live and Recorded Video Policy. (2017). <a class="link-inline force-break"      href="http://www.sigchi.org/conferences/Conferences/Policies/sigchi-live-and-recorded-video-policy">http://www.sigchi.org/conferences/Conferences/Policies/sigchi-live-and-recorded-video-policy</a>.</li>    <li id="BibPLXBIB0008" label="[8]">Catherine Adams, Yin Yin, Luis&#x00A0;Francisco Vargas&#x00A0;Madriz, and C&#x00A0;Scott Mullen. 2014. A phenomenology of learning large: the tutorial sphere of xMOOC video lectures. <em>      <em>Distance Education</em>     </em>35, 2 (2014), 202&#x2013;216.</li>    <li id="BibPLXBIB0009" label="[9]">Lai&#x00A0;Hung Auyeung. 2004. Building a collaborative online learning community: A case study in Hong Kong. <em>      <em>Journal of Educational Computing Research</em>     </em>31, 2 (2004), 119&#x2013;136.</li>    <li id="BibPLXBIB0010" label="[10]">Thomas Balslev, Willem&#x00A0;S De&#x00A0;Grave, Arno&#x00A0;MM Muijtjens, and AJJA Scherpbier. 2005. Comparison of text and video cases in a postgraduate problem-based learning format. <em>      <em>Medical education</em>     </em>39, 11 (2005), 1086&#x2013;1092.</li>    <li id="BibPLXBIB0011" label="[11]">Michael&#x00A0;S Bernstein, Greg Little, Robert&#x00A0;C Miller, Bj&#x00F6;rn Hartmann, Mark&#x00A0;S Ackerman, David&#x00A0;R Karger, David Crowell, and Katrina Panovich. 2015. Soylent: a word processor with a crowd inside. <em>      <em>Commun. ACM</em>     </em>58, 8 (2015), 85&#x2013;94.</li>    <li id="BibPLXBIB0012" label="[12]">David Boud. 2013. <em>      <em>Enhancing learning through self-assessment</em>     </em>. Routledge.</li>    <li id="BibPLXBIB0013" label="[13]">Geoff Brumfiel. 2009. Science journalism: Supplanting the old media?<em>      <em>Nature News</em>     </em>458, 7236 (2009), 274&#x2013;277.</li>    <li id="BibPLXBIB0014" label="[14]">Axel Bruns. 2008. <em>      <em>Blogs, Wikipedia, Second Life, and beyond: From production to produsage</em>     </em>. Vol.&#x00A0;45. Peter Lang.</li>    <li id="BibPLXBIB0015" label="[15]">Catherine&#x00A0;Durnell Cramton. 2001. The mutual knowledge problem and its consequences for dispersed collaboration. <em>      <em>Organization science</em>     </em>12, 3 (2001), 346&#x2013;371.</li>    <li id="BibPLXBIB0016" label="[16]">David&#x00A0;D Curtis and Michael&#x00A0;J Lawson. 2001. Exploring collaborative online learning. <em>      <em>Journal of Asynchronous learning networks</em>     </em>5, 1 (2001), 21&#x2013;34.</li>    <li id="BibPLXBIB0017" label="[17]">Steven Dow, Anand Kulkarni, Scott Klemmer, and Bj&#x00F6;rn Hartmann. 2012. Shepherding the crowd yields better work. In <em>      <em>Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work</em>     </em>. ACM, 1013&#x2013;1022.</li>    <li id="BibPLXBIB0018" label="[18]">David&#x00A0;A Dudas, James&#x00A0;H Kaskade, and Kenneth&#x00A0;W O&#x0027;flaherty. 2007. System and methods for online collaborative video creation. (Jan.&#x00A0;5 2007). US Patent App. 12/159,736.</li>    <li id="BibPLXBIB0019" label="[19]">DukeUniversity. 2016. How to Convert your Paper into a Presentation. (2016). <a class="link-inline force-break"      href="http://twp.duke.edu/uploads/media_items/paper-to-talk.original.pdf">http://twp.duke.edu/uploads/media_items/paper-to-talk.original.pdf</a>.</li>    <li id="BibPLXBIB0020" label="[20]">Philip&#x00A0;J. Guo, Juho Kim, and Rob Rubin. 2014. How Video Production Affects Student Engagement: An Empirical Study of MOOC Videos. In <em>      <em>Proceedings of the First ACM Conference on Learning @ Scale Conference</em>     </em>(L@S &#x2019;14). ACM, New York, NY, USA, 41&#x2013;50. 978-1-4503-2669-8<a class="link-inline force-break" href="https://doi.org/10.1145/2556325.2566239"      target="_blank">https://doi.org/10.1145/2556325.2566239</a></li>    <li id="BibPLXBIB0021" label="[21]">R Hawkins and K Price. 1992. The effects of an education video on patients&#x2019; requests for postoperative pain relief.<em>      <em>The Australian journal of advanced nursing: a quarterly publication of the Royal Australian Nursing Federation</em>     </em>10, 4 (1992), 32&#x2013;40.</li>    <li id="BibPLXBIB0022" label="[22]">William&#x00A0;J Haynie&#x00A0;III <em>et al.</em> 1998. Collaborative Learning Enhances Critical Thinking. <em>      <em>Volume 7 Issue 1 (fall 1995)</em>     </em>(1998).</li>    <li id="BibPLXBIB0023" label="[23]">Pamela Hinds, Lei Liu, and Joachim Lyon. 2011. Putting the global in global work: An intercultural lens on the practice of cross-national collaboration. <em>      <em>Academy of Management Annals</em>     </em>5, 1 (2011), 135&#x2013;188.</li>    <li id="BibPLXBIB0024" label="[24]">Robert&#x00A0;S Huckman, Bradley&#x00A0;R Staats, and David&#x00A0;M Upton. 2009. Team familiarity, role experience, and performance: Evidence from Indian software services. <em>      <em>Management science</em>     </em>55, 1 (2009), 85&#x2013;100.</li>    <li id="BibPLXBIB0025" label="[25]">Robin Kay and Ilona Kletskin. 2012. Evaluating the use of problem-based video podcasts to teach mathematics in higher education. <em>      <em>Computers &#x0026; Education</em>     </em>59, 2 (2012), 619&#x2013;627.</li>    <li id="BibPLXBIB0026" label="[26]">Joy Kim, Justin Cheng, and Michael&#x00A0;S Bernstein. 2014. Ensemble: exploring complementary strengths of leaders and crowds in creative collaboration. In <em>      <em>Proceedings of the 17th ACM conference on Computer supported cooperative work &#x0026; social computing</em>     </em>. ACM, 745&#x2013;755.</li>    <li id="BibPLXBIB0027" label="[27]">Juho Kim, Phu&#x00A0;Tran Nguyen, Sarah Weir, Philip&#x00A0;J Guo, Robert&#x00A0;C Miller, and Krzysztof&#x00A0;Z Gajos. 2014. Crowdsourcing step-by-step information extraction to enhance existing how-to videos. In <em>      <em>Proceedings of the 32nd annual ACM conference on Human factors in computing systems</em>     </em>. ACM, 4017&#x2013;4026.</li>    <li id="BibPLXBIB0028" label="[28]">Aniket Kittur. 2010. Crowdsourcing, Collaboration and Creativity. <em>      <em>XRDS</em>     </em>17, 2 (Dec. 2010), 22&#x2013;26. 1528-4972 <a class="link-inline force-break" href="https://doi.org/10.1145/1869086.1869096"      target="_blank">https://doi.org/10.1145/1869086.1869096</a></li>    <li id="BibPLXBIB0029" label="[29]">Adam Lashinsky. 2012. <em>      <em>Inside Apple</em>     </em>. Hachette Book Group, New York.</li>    <li id="BibPLXBIB0030" label="[30]">Kurt Luther and Amy Bruckman. 2010. Flash collabs: Collaborative innovation networks in online communities of animators. <em>      <em>Procedia-Social and Behavioral Sciences</em>     </em>2, 4 (2010), 6571&#x2013;6581.</li>    <li id="BibPLXBIB0031" label="[31]">Kurt Luther, Kelly Caine, Kevin Ziegler, and Amy Bruckman. 2010. Why it works (when it works): Success factors in online creative collaboration. In <em>      <em>Proceedings of the 16th ACM international conference on Supporting group work</em>     </em>. ACM, 1&#x2013;10.</li>    <li id="BibPLXBIB0032" label="[32]">Kurt Luther, Casey Fiesler, and Amy Bruckman. 2013. Redistributing leadership in online creative collaboration. In <em>      <em>Proceedings of the 2013 conference on Computer supported cooperative work</em>     </em>. ACM, 1007&#x2013;1022.</li>    <li id="BibPLXBIB0033" label="[33]">Kurt Luther, Amy Pavel, Wei Wu, Jari-lee Tolentino, Maneesh Agrawala, Bj&#x00F6;rn Hartmann, and Steven&#x00A0;P Dow. 2014. CrowdCrit: crowdsourcing and aggregating visual design critique. In <em>      <em>Proceedings of the companion publication of the 17th ACM conference on Computer supported cooperative work &#x0026; social computing</em>     </em>. ACM, 21&#x2013;24.</li>    <li id="BibPLXBIB0034" label="[34]">Lauren&#x00A0;E Margulieux, Mark Guzdial, and Richard Catrambone. 2012. Subgoal-labeled instructional material improves performance and transfer in learning to develop mobile applications. In <em>      <em>Proceedings of the ninth annual international conference on International computing education research</em>     </em>. ACM, 71&#x2013;78.</li>    <li id="BibPLXBIB0035" label="[35]">Elaine McCreary and Madge Brochet. 1992. Collaboration in international online teams. In <em>      <em>Collaborative learning through computer conferencing</em>     </em>. Springer, 69&#x2013;85.</li>    <li id="BibPLXBIB0036" label="[36]">Tao Mei, Xian-Sheng Hua, Linjun Yang, and Shipeng Li. 2007. VideoSense: towards effective online video advertising. In <em>      <em>Proceedings of the 15th ACM international conference on Multimedia</em>     </em>. ACM, 1075&#x2013;1084.</li>    <li id="BibPLXBIB0037" label="[37]">Robert&#x00A0;K Merton <em>et al.</em> 1968. The Matthew effect in science. <em>      <em>Science</em>     </em>159, 3810 (1968), 56&#x2013;63.</li>    <li id="BibPLXBIB0038" label="[38]">Jeff Nevid and Alejandro&#x00A0;Franco Jaramillo. 2011. Teaching the millennials. <em>      <em>APS Observer</em>     </em>24(2011), 53&#x2013;56.</li>    <li id="BibPLXBIB0039" label="[39]">Gary&#x00A0;M Olson and Judith&#x00A0;S Olson. 2000. Distance matters. <em>      <em>Human-computer interaction</em>     </em>15, 2 (2000), 139&#x2013;178.</li>    <li id="BibPLXBIB0040" label="[40]">Aaron van&#x00A0;den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. <em>      <em>arXiv preprint arXiv:1609.03499</em>     </em>(2016).</li>    <li id="BibPLXBIB0041" label="[41]">Daniela Retelny, S&#x00E9;bastien Robaszkiewicz, Alexandra To, Walter&#x00A0;S. Lasecki, Jay Patel, Negar Rahmati, Tulsee Doshi, Melissa Valentine, and Michael&#x00A0;S. Bernstein. 2014. Expert Crowdsourcing with Flash Teams. In <em>      <em>Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology</em>     </em>(UIST &#x2019;14). ACM, New York, NY, USA, 75&#x2013;85. 978-1-4503-3069-5<a class="link-inline force-break" href="https://doi.org/10.1145/2642918.2647409"      target="_blank">https://doi.org/10.1145/2642918.2647409</a></li>    <li id="BibPLXBIB0042" label="[42]">Steve Rubin, Floraine Berthouzoz, Gautham&#x00A0;J Mysore, Wilmot Li, and Maneesh Agrawala. 2013. Content-based tools for editing audio stories. In <em>      <em>Proceedings of the 26th annual ACM symposium on User interface software and technology</em>     </em>. ACM, 113&#x2013;122.</li>    <li id="BibPLXBIB0043" label="[43]">Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, <em>et al.</em> 2015. Imagenet large scale visual recognition challenge. <em>      <em>International Journal of Computer Vision</em>     </em>115, 3 (2015), 211&#x2013;252.</li>    <li id="BibPLXBIB0044" label="[44]">SearchEngineLand. 2015. YouTube Ranking Factors: Getting Ranked In The Second Largest Search Engine. (2015). <a class="link-inline force-break" href="https://goo.gl/FS8Ki4">https://goo.gl/FS8Ki4</a>.</li>    <li id="BibPLXBIB0045" label="[45]">Pao Siangliulue, Joel Chan, Steven&#x00A0;P Dow, and Krzysztof&#x00A0;Z Gajos. 2016. IdeaHound: Improving Large-scale Collaborative Ideation with Crowd-powered Real-time Semantic Modeling. In <em>      <em>Proceedings of the 29th Annual Symposium on User Interface Software and Technology</em>     </em>. ACM, 609&#x2013;624.</li>    <li id="BibPLXBIB0046" label="[46]">David Silver, Aja Huang, Chris&#x00A0;J Maddison, Arthur Guez, Laurent Sifre, George Van Den&#x00A0;Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, <em>et al.</em> 2016. Mastering the game of Go with deep neural networks and tree search. <em>      <em>Nature</em>     </em>529, 7587 (2016), 484&#x2013;489.</li>    <li id="BibPLXBIB0047" label="[47]">Dominique Sluijsmans, Filip Dochy, and George Moerkerke. 1998. Creating a learning environment by using self-, peer-and co-assessment. <em>      <em>Learning environments research</em>     </em>1, 3 (1998), 293&#x2013;319.</li>    <li id="BibPLXBIB0048" label="[48]">Gwyneth Sutherlin. 2013. A voice in the crowd: Broader implications for crowdsourcing translation during crisis. <em>      <em>Journal of information science</em>     </em>(2013), 0165551512471593.</li>    <li id="BibPLXBIB0049" label="[49]">Jaime Teevan, Shamsi&#x00A0;T Iqbal, Carrie&#x00A0;J Cai, Jeffrey&#x00A0;P Bigham, Michael&#x00A0;S Bernstein, and Elizabeth&#x00A0;M Gerber. 2016. Productivity Decomposed: Getting Big Things Done with Little Microtasks. In <em>      <em>Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>     </em>. ACM, 3500&#x2013;3507.</li>    <li id="BibPLXBIB0050" label="[50]">Simone Teufel and Marc Moens. 2002. Summarizing Scientific Articles: Experiments with Relevance and Rhetorical Status. <em>      <em>Comput. Linguist.</em>     </em>28, 4 (Dec. 2002), 409&#x2013;445. 0891-2017 <a class="link-inline force-break"      href="https://doi.org/10.1162/089120102762671936"      target="_blank">https://doi.org/10.1162/089120102762671936</a></li>    <li id="BibPLXBIB0051" label="[51]">Anh Truong, Floraine Berthouzoz, Wilmot Li, and Maneesh Agrawala. 2016. Quickcut: An interactive tool for editing narrated video. In <em>      <em>Proceedings of the 29th Annual Symposium on User Interface Software and Technology</em>     </em>. ACM, 497&#x2013;507.</li>    <li id="BibPLXBIB0052" label="[52]">Rajan Vaish, Sascha&#x00A0;T Ishikawa, Sheng Lundquist, Reid Porter, and James Davis. 2013. Human Computation for Object Detection. <em>      <em>Tech Report UCSC-SOE-15-03</em>     </em>(2013).</li>    <li id="BibPLXBIB0053" label="[53]">Rajan Vaish and Andr&#x00E9;s Monroy-Hern&#x00E1;ndez. 2017. CrowdTone: Crowd-powered tone feedback and improvement system for emails. <em>      <em>MSR-TR-2017-1</em>     </em> (2017).</li>    <li id="BibPLXBIB0054" label="[54]">Ravi Vatrapu and Dan Suthers. 2007. Culture and computers: A review of the concept of culture and implications for intercultural collaborative online learning. In <em>      <em>Intercultural Collaboration</em>     </em>. Springer, 260&#x2013;275.</li>    <li id="BibPLXBIB0055" label="[55]">VirginiaTech. 2016. How To Make an Oral Presentation of Your Research. (2016). <a class="link-inline force-break"      href="http://www.virginia.edu/cue/presentationtips.html">http://www.virginia.edu/cue/presentationtips.html</a>.</li>    <li id="BibPLXBIB0056" label="[56]">Mark Warschauer. 1997. Computer-mediated collaborative learning: Theory and practice. <em>      <em>The modern language journal</em>     </em>81, 4 (1997), 470&#x2013;481.</li>    <li id="BibPLXBIB0057" label="[57]">Michael&#x00A0;F Weigold. 2001. Communicating science: A review of the literature. <em>      <em>Science communication</em>     </em>23, 2 (2001), 164&#x2013;193.</li>    <li id="BibPLXBIB0058" label="[58]">Sarah Weir, Juho Kim, Krzysztof&#x00A0;Z Gajos, and Robert&#x00A0;C Miller. 2015. Learnersourcing subgoal labels for how-to videos. In <em>      <em>Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &#x0026; Social Computing</em>     </em>. ACM, 405&#x2013;416.</li>    <li id="BibPLXBIB0059" label="[59]">Karoly Zsolnai-Feher. 2016. Two Minute Papers. (2016). <a class="link-inline force-break"      href="https://www.youtube.com/user/keeroyz/playlists">https://www.youtube.com/user/keeroyz/playlists</a>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x002A;</sup></a>The author&#x0027;s current affiliation is Snap Inc. This work was done while he was a postdoc at Stanford University.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://scholar.stanford.edu">https://scholar.stanford.edu</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.</p> 			<p>ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186031">https://doi.org/10.1145/3178876.3186031</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
