<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Why Reinvent the Wheel &#x2013; Let&#x0027;s Build Question Answering Systems Together</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Why Reinvent the Wheel &#x2013; Let&#x0027;s Build Question Answering Systems Together</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Kuldeep</span>     <span class="surName">Singh</span>,     University of Bonn &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:kuldeep.singh@iais.fraunhofer.de">kuldeep.singh@iais.fraunhofer.de</a>    </div>    <div class="author">     <span class="givenName">Arun Sethupat</span>     <span class="surName">Radhakrishna</span>,     University of Minnesota, USA, <a href="mailto:sethu021@umn.edu">sethu021@umn.edu</a>    </div>    <div class="author">     <span class="givenName">Andreas</span>     <span class="surName">Both</span>,     DATEV eG, Germany, <a href="mailto:contact@andreasboth.de">contact@andreasboth.de</a>    </div>    <div class="author">     <span class="givenName">Saeedeh</span>     <span class="surName">Shekarpour</span>,     University of Dayton, Dayton, USA, <a href="mailto:sshekarpour1@udayton.edu">sshekarpour1@udayton.edu</a>    </div>    <div class="author">     <span class="givenName">Ioanna</span>     <span class="surName">Lytra</span>,     University of Bonn &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:lytra@cs.uni-bonn.de">lytra@cs.uni-bonn.de</a>    </div>    <div class="author">     <span class="givenName">Ricardo</span>     <span class="surName">Usbeck</span>,     University of Paderborn, Germany, <a href="mailto:ricardo.usbeck@uni-paderborn.de">ricardo.usbeck@uni-paderborn.de</a>    </div>    <div class="author">     <span class="givenName">Akhilesh</span>     <span class="surName">Vyas</span>,     University of Bonn &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:akhilesh.vyas@iais.fraunhofer.de">akhilesh.vyas@iais.fraunhofer.de</a>    </div>    <div class="author">     <span class="givenName">Akmal</span>     <span class="surName">Khikmatullaev</span>,     University of Bonn &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:akmal.khikmatullaev@gmail.com">akmal.khikmatullaev@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Dharmen</span>     <span class="surName">Punjani</span>,     University of Athens, Greece, <a href="mailto:dharmen.punjani@gmail.com">dharmen.punjani@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Christoph</span>     <span class="surName">Lange</span>,     University of Bonn &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:christoph.lange@uni-bonn.de">christoph.lange@uni-bonn.de</a>    </div>    <div class="author">     <span class="givenName">Maria Esther</span>     <span class="surName">Vidal</span>,     Leibniz Information Centre For Science and Technology University Library &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:maria.vidal@tib.eu">maria.vidal@tib.eu</a>    </div>    <div class="author">     <span class="givenName">Jens</span>     <span class="surName">Lehmann</span>,     University of Bonn &#x0026; Fraunhofer IAIS, Germany, <a href="mailto:jens.lehmann@iais.fraunhofer.de">jens.lehmann@iais.fraunhofer.de</a>    </div>    <div class="author">     <span class="givenName">S&#x00F6;ren</span>     <span class="surName">Auer</span>,     Leibniz Information Centre For Science and Technology University Library &#x0026; University of Hannover, Germany, <a href="mailto:soeren.auer@tib.eu">soeren.auer@tib.eu</a>    </div>                                                        </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186023" target="_blank">https://doi.org/10.1145/3178876.3186023</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Modern question answering (QA) systems need to flexibly integrate a number of components specialised to fulfil specific tasks in a QA pipeline. Key QA tasks include Named Entity Recognition and Disambiguation, Relation Extraction, and Query Building. Since a number of different software components exist that implement different strategies for each of these tasks, it is a major challenge to select and combine the most suitable components into a QA system, given the characteristics of a question. We study this optimisation problem and train classifiers, which take features of a question as input and have the goal of optimising the selection of QA components based on those features. We then devise a greedy algorithm to identify the pipelines that include the suitable components and can effectively answer the given question. We implement this model within <font style="font-variant: small-caps">Frankenstein</font>, a QA framework able to select QA components and compose QA pipelines. We evaluate the effectiveness of the pipelines generated by <font style="font-variant: small-caps">Frankenstein</font> using the QALD and LC-QuAD benchmarks. These results not only suggest that <font style="font-variant: small-caps">Frankenstein</font> precisely solves the QA optimisation problem but also enables the automatic composition of optimised QA pipelines, which outperform the static Baseline QA pipeline. Thanks to this flexible and fully automated pipeline generation process, new QA components can be easily included in <font style="font-variant: small-caps">Frankenstein</font>&#x00A0; thus improving the performance of the generated pipelines.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Natural language processing;</strong> <strong>Knowledge representation and reasoning;</strong></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Question Answering</small>, </span>     <span class="keyword">      <small> Software Reusability</small>, </span>     <span class="keyword">      <small> Semantic Web</small>, </span>     <span class="keyword">      <small> Semantic Search</small>, </span>     <span class="keyword">      <small> QA Framework</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Kuldeep Singh, Arun Sethupat Radhakrishna, Andreas Both, Saeedeh Shekarpour, Ioanna Lytra, Ricardo Usbeck, Akhilesh Vyas, Akmal Khikmatullaev, Dharmen Punjani, Christoph Lange, Maria Esther Vidal, Jens Lehmann, and S&#x00F6;ren Auer. 2018. Why Reinvent the Wheel &#x2013; Let&#x0027;s Build Question Answering Systems Together. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186023" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186023</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-14">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <figure id="fig1">    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186023/images/www2018-32-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Four natural language questions answered successfully by different pipelines composed of three NED, two RL, and two QB components. The optimal pipelines for each question are highlighted.</span>    </div>    </figure>    <p>Answering questions based on information encoded in knowledge graphs has recently received much attention by the research community. Since 2010, more than 62 systems for question answering (QA) over the Web of Data have been developed [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. These systems typically include components building on Artificial Intelligence, Natural Language Processing, and Semantic Technology; they implement common tasks such as Named Entity Recognition and Disambiguation, Relation Extraction, and Query Building. Evaluation studies have shown that there is no best performing QA system for all types of Natural Language&#x00A0;(NL) questions; instead, there is evidence that certain systems, implementing different strategies, are more suitable for certain types of questions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>]. Hence, modern QA systems need to flexibly integrate a number of components specialised to fulfil specific tasks in a QA pipeline.</p>    <p>Relying on these observations, we devise <font style="font-variant: small-caps">Frankenstein</font>, a framework able to dynamically select QA components in order to exploit the properties of the components to optimise the F-Score. <font style="font-variant: small-caps">Frankenstein</font> implements a classification based learning model, which estimates the performance of QA components for a given question, based on its features. Given a question, the <font style="font-variant: small-caps">Frankenstein</font>&#x00A0;framework implements a greedy algorithm to generate a QA pipeline consisting of the best performing components for this question.</p>    <p>We empirically evaluate the performance of <font style="font-variant: small-caps">Frankenstein</font> using two renowned benchmarks from the Question Answering over Linked Data Challenge<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> (QALD) and the Large-Scale Complex Question Answering Dataset<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> (LC-QuAD). We observe that <font style="font-variant: small-caps">Frankenstein</font> is able to combine QA components to produce optimised QA pipelines outperforming the static Baseline pipeline.</p>    <p>In summary, we provide the following contributions:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;"><font style="font-variant: small-caps">Frankenstein</font> framework relying on machine learning techniques for dynamically selecting suitable QA components and composing QA pipelines based on the input question, thus optimising the overall F-Score.<br/></li>    <li id="list2" label="&#x2022;">A collection of 29 reusable QA components that can be combined to generate 360 distinct QA pipelines, integrated in the <font style="font-variant: small-caps">Frankenstein</font> framework.<br/></li>    <li id="list3" label="&#x2022;">An in-depth analysis of advantages and disadvantages of QA components in QA pipelines after a thorough benchmarking of the performance of the <font style="font-variant: small-caps">Frankenstein</font> pipeline generator using over 3,000 questions from the QALD and LC-QuAD QA benchmarks.<br/></li>    </ul>    <p>As a result of this work, we expect a new class of QA systems to emerge. Currently, QA systems are tailored to a particular domain (mostly common knowledge), a source of background knowledge (most commonly DBpedia&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]) and benchmarking data (most commonly QALD). Based on <font style="font-variant: small-caps">Frankenstein</font>, more flexible, domain-agnostic QA systems can be built and quickly adapted to new domains.</p>    <p>The remainder of this article is structured as follows: We introduce the motivation of our work in Section&#x00A0;<a class="sec" href="#sec-15">2</a>. The problem tackled as well as the proposed solution are discussed in Section&#x00A0;<a class="sec" href="#sec-16">3</a> and the details of <font style="font-variant: small-caps">Frankenstein</font> are presented in Section&#x00A0;<a class="sec" href="#sec-19">4</a>. The Section&#x00A0;<a class="sec" href="#sec-22">5</a> describes the preparation of training datasets followed by performance evaluation of components in Section&#x00A0;<a class="sec" href="#sec-25">6</a>. Section&#x00A0;<a class="sec" href="#sec-28">7</a> reports the empirical evaluation of <font style="font-variant: small-caps">Frankenstein</font> QA pipelines, with subsequent discussions in Section&#x00A0;<a class="sec" href="#sec-31">8</a>. The related work is reviewed in Section&#x00A0;<a class="sec" href="#sec-32">9</a> and finally, conclusions and directions for future work are discussed in Section&#x00A0;<a class="sec" href="#sec-33">10</a>.</p>   </section>   <section id="sec-15">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Motivating Example</h2>    </div>    </header>    <p>A great number of components perform QA tasks &#x2013; either as part of QA systems or standalone&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. Table&#x00A0;<a class="tbl" href="#tab1">1</a> presents several QA components, implementing the QA tasks NED (Named Entity Disambiguation) implemented by (i) DBpedia Spotlight&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>], (ii) Aylien API<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>, and (iii) Tag Me API&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]), RL (Relation Linking) implemented by (i) ReMatch&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] and (ii) RelMatch&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]), and QB (Query Building) implemented by (i) SINA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] and (ii) NLIWOD QB<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>).</p>    <p>For example, given the question <em>&#x201C;What is the capital of Canada?&#x201D;</em>, the ideal NED component is expected to recognise the keyword <em>&#x201C;Canada&#x201D;</em> as a named entity and map it to the corresponding DBpedia resource, i.e.&#x00A0;<tt>dbr:Canada</tt><a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>. Thereafter, a component performing RL finds embedded relations in the given question and links them to appropriate relations of the underlying knowledge graph. In our example, the keyword <em>&#x201C;capital&#x201D;</em> is mapped to the relation <tt>dbo:capital</tt><a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a>. Finally, the QB component generates a formal query (e.g.&#x00A0;expressed in SPARQL), which retrieves all answers from the corresponding knowledge graph (i.e.&#x00A0;<tt>SELECT ?c {dbr:Canada dbo:capital ?c.}</tt>).</p>    <p>Table&#x00A0;<a class="tbl" href="#tab1">1</a> presents precision, recall, and F-Score of the listed components for the QALD-5 benchmark (cf.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] and Section&#x00A0;<a class="sec" href="#sec-23">5.1</a>). We observe that DBpedia Spotlight, ReMatch, and NLIWOD QB achieve the best performance for the tasks NED, RL, and QB, respectively (cf.&#x00A0;Section&#x00A0;<a class="sec" href="#sec-25">6</a> for details). When QA components are integrated into a QA pipeline, the overall performance of the pipeline depends on the individual performance of each component. The fact that a particular component gives superior performance for a task on a given set of questions does not imply that the component is superior for all types of questions. That is, the performance of components varies depending on the type of question.</p>    <div class="table-responsive" id="tab1">    <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Performance of QA components implementing various QA tasks evaluated with the QALD-5 benchmark.</span>    </div>    <table class="table">     <thead>      <tr>       <th style="text-align:left;">       <strong>QA Component</strong>       </th>       <th style="text-align:left;">       <strong>QA Task</strong>       </th>       <th style="text-align:left;">       <strong>Precision</strong>       </th>       <th style="text-align:left;">       <strong>Recall</strong>       </th>       <th style="text-align:left;">       <strong>F-Score</strong>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">       <em>DBpedia Spotlight</em>       </td>       <td style="text-align:left;">NED</td>       <td style="text-align:left;">0.67</td>       <td style="text-align:left;">0.76</td>       <td style="text-align:left;">0.71</td>      </tr>      <tr>       <td style="text-align:left;">       <em>Aylien API</em>       </td>       <td style="text-align:left;">NED</td>       <td style="text-align:left;">0.60</td>       <td style="text-align:left;">0.66</td>       <td style="text-align:left;">0.63</td>      </tr>      <tr>       <td style="text-align:left;">       <em>Tag Me API</em>       </td>       <td style="text-align:left;">NED</td>       <td style="text-align:left;">0.47</td>       <td style="text-align:left;">0.57</td>       <td style="text-align:left;">0.52</td>      </tr>      <tr>       <td style="text-align:left;">       <em>ReMatch</em>       </td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">0.54</td>       <td style="text-align:left;">0.74</td>       <td style="text-align:left;">0.62</td>      </tr>      <tr>       <td style="text-align:left;">       <em>RelMatch</em>       </td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">0.10</td>       <td style="text-align:left;">0.19</td>       <td style="text-align:left;">0.13</td>      </tr>      <tr>       <td style="text-align:left;">       <em>SINA</em>       </td>       <td style="text-align:left;">QB</td>       <td style="text-align:left;">0.38</td>       <td style="text-align:left;">0.41</td>       <td style="text-align:left;">0.39</td>      </tr>      <tr>       <td style="text-align:left;">       <em>NLIWOD QB</em>       </td>       <td style="text-align:left;">QB</td>       <td style="text-align:left;">0.49</td>       <td style="text-align:left;">0.50</td>       <td style="text-align:left;">0.49</td>      </tr>     </tbody>    </table>    </div>    <p>The performance values in Table&#x00A0;<a class="tbl" href="#tab1">1</a> are averaged over the entire query inventory. They are not representative for the specific performance of components for various types of input questions. For example, Figure&#x00A0;<a class="fig" href="#fig1">1</a> illustrates the best performing QA pipelines for four exemplary input questions. We observe that Pipeline P1 is the most efficient for answering Question <em>Q</em>    <sub>1</sub>: <em>&#x201C;What is the capital of Canada?&#x201D;</em> but it fails to answer Question <em>Q</em>    <sub>4</sub>: <em>&#x201C;Which river does the Brooklyn Bridge cross?&#x201D;</em>. This is caused by the fact that the RL component ReMatch in Pipeline P1 does not correctly map the relation <tt>dbo:crosses</tt> in <em>Q</em>    <sub>4</sub> for the input keyword <em>&#x201C;cross&#x201D;</em>, while RelMatch maps this relation correctly. Although the overall precision of ReMatch on QALD-5 is higher than that of RelMatch, for <em>Q</em>    <sub>4</sub>, the performance of RelMatch is higher. Similarly, for Question <em>Q</em>    <sub>2</sub>     <em>&#x201C;Did Socrates influence Aristotle?&#x201D;</em> Pipeline P2 delivers the desired answer, while it fails to answer the similar question <em>Q</em>    <sub>3</sub>     <em>&#x201C;Did Tesla win a nobel prize in physics?&#x201D;</em>. Although questions <em>Q</em>    <sub>2</sub> and <em>Q</em>    <sub>3</sub> have a similar structure (i.e.&#x00A0;Boolean answer type), DBpedia Spotlight NED succeeds for <em>Q</em>    <sub>2</sub>, but on <em>Q</em>    <sub>3</sub> it fails to disambiguate the resource <tt>dbr:Nobel_Prize_in_Physics</tt>. At the same time, Tag Me can accomplish the NED task successfully. Although, the optimal pipeline for a given question can be identified experimentally by executing all possible pipelines, this approach is costly and even practically impossible, since covering all potential input questions is not feasible. Therefore, a heuristic approach to identify an optimal pipeline for a given input question is required.</p>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Problem Statement</h2>    </div>    </header>    <p>A full QA pipeline is composed of all the necessary tasks to transform a user-supplied Natural Language (NL) question into a query in a formal language (e.g.&#x00A0;SPARQL), whose evaluation retrieves the desired answer(s) from an underlying knowledge graph. Correctly answering a given input question <em>q</em> requires a QA pipeline that, ideally, uses those QA components that deliver the best precision and recall for answering <em>q</em>. Identifying the best performing QA pipeline for a given question <em>q</em> requires: (i) a prediction mechanism to predict the performance of a component given a question <em>q</em>, a required task, and a knowledge graph <em>&#x03BB;</em>; (ii) an approach for composing an optimised pipeline by integrating the most accurate components.</p>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Predicting Best Performing Components</h3>     </div>    </header>    <p>In this context, we formally define a set of necessary QA tasks as <span class="inline-equation"><span class="tex">$\mathcal {T}=\lbrace t_1,t_2,\dots ,t_n\rbrace$</span>     </span> such as NED, RL, and QB. Each task (<em>t<sub>i</sub>     </em>: <em>q</em>     <sup>*</sup> &#x2192; <em>q</em>     <sup>+</sup>) transforms a given representation <em>q</em>     <sup>*</sup> of a question <em>q</em> into another representation <em>q</em>     <sup>+</sup>. For example, NED and RL tasks transform the input representation <em>&#x201C;What is the capital of Canada?&#x201D;</em> into the representation <em>&#x201C;What is the <tt>dbo:capital</tt>of <tt>dbr:Canada</tt>?&#x201D;</em>. The entire set of QA components is denoted by <span class="inline-equation"><span class="tex">$\mathcal {C}=\lbrace C_1,C_2,\dots ,C_m\rbrace$</span>     </span>. Each component <em>C<sub>j</sub>     </em> solves one single QA task; <em>&#x03B8;</em>(<em>C<sub>j</sub>     </em>) corresponds to the QA task <em>t<sub>i</sub>     </em> in <span class="inline-equation"><span class="tex">$\mathcal {T}$</span>     </span> implemented by <em>C<sub>j</sub>     </em>. For example, ReMatch implements the relation linking QA task, i.e.&#x00A0;<em>&#x03B8;</em>(<em>ReMatch</em>) = <em>RL</em>. Let <em>&#x03C1;</em>(<em>C<sub>j</sub>     </em>) denote the performance of a QA component, then our first objective is to predict the likelihood of <em>&#x03C1;</em>(<em>C<sub>j</sub>     </em>) for a given representation <em>q</em>     <sup>*</sup> of <em>q</em>, a task <em>t<sub>i</sub>     </em>, and an underlying knowledge graph <em>&#x03BB;</em>. This is denoted as <em>Pr</em>(<em>&#x03C1;</em>(<em>C<sub>j</sub>     </em>)|<em>q</em>     <sup>*</sup>, <em>t<sub>i</sub>     </em>, <em>&#x03BB;</em>). In this work, we assume a single knowledge graph (i.e.&#x00A0;DBpedia); thus, <em>&#x03BB;</em> is considered a constant parameter that does not impact the likelihood leading to: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathit {Pr}(\rho (C_j)|q^*,t_i) = \mathit {Pr}(\rho (C_j)|q^*,t_i,\lambda) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div>    </p>    <p>Moreover, for each individual task <em>t<sub>i</sub>     </em> and question representation <em>q</em>     <sup>*</sup>, we predict the performance of all pertaining components. In other words, for a given task <em>t<sub>i</sub>     </em>, the set of components that can accomplish <em>t<sub>i</sub>     </em> is <span class="inline-equation"><span class="tex">$\mathcal {C}^{t_i} =\lbrace C_j,\dots ,C_k\rbrace$</span>     </span>. Thus, we factorise <em>t<sub>i</sub>     </em> as follows: <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \forall C_j \in \mathcal {C}^{t_i}, [\mathit {Pr}(\rho (C_j)|q^*) = \mathit {Pr}(\rho (C_j)|q^*,t_i)] \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> Further, we assume that the given representation <em>q</em>     <sup>*</sup> is equal to the initial input representation <em>q</em> for all the QA components, i.e.&#x00A0;<em>q</em>     <sup>*</sup> = <em>q</em>. Finally, the problem of finding the best performing component for accomplishing the task <em>t<sub>i</sub>     </em> for an input question <em>q</em>, denoted as <span class="inline-equation"><span class="tex">$\gamma ^{t_i}_{q}$</span>     </span>, is formulated as follows: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \gamma ^{t_i}_{q} = \arg \max _{C_j \in \mathcal {C}^{t_i}} \lbrace \mathit {Pr}(\rho (C_j)|q) \rbrace \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>    </p>    <p>     <em>Solution</em>. Suppose we are given a set of NL questions <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span> with the detailed results of performance for each component per task. We can then model the prediction goal <em>Pr</em>(<em>&#x03C1;</em>(<em>C<sub>j</sub>     </em>)|<em>q</em>, <em>t<sub>i</sub>     </em>) as a supervised learning problem on a training set, i.e.&#x00A0;a set of questions <span class="inline-equation"><span class="tex">$\mathcal {Q}$</span>     </span> and a set of labels <span class="inline-equation"><span class="tex">$\mathcal {L}$</span>     </span> representing the performance of <em>C<sub>j</sub>     </em> for a question <em>q</em> and a task <em>t<sub>i</sub>     </em>. In other words, for each individual task <em>t<sub>i</sub>     </em> and component <em>C<sub>j</sub>     </em>, the purpose is to train a supervised model that predicts the performance of the given component <em>C<sub>j</sub>     </em> for a given question <em>q</em> and task <em>t<sub>i</sub>     </em> leveraging the training set. If <span class="inline-equation"><span class="tex">$|\mathcal {T}|=n$</span>     </span> and each task is performed by <em>m</em> components, then <em>n</em> &#x00D7; <em>m</em> individual learning models have to be built up. Furthermore, since the input questions <span class="inline-equation"><span class="tex">$q \in \mathcal {Q}$</span>     </span> have a textual representation, it is necessary to automatically extract suitable features, i.e.&#x00A0;<span class="inline-equation"><span class="tex">$\mathcal {F}(q)=(f_1,\dots ,f_r)$</span>     </span>. The details of the feature extraction process are presented in Section&#x00A0;<a class="sec" href="#sec-24">5.2</a>.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Identifying Optimal QA Pipelines</h3>     </div>    </header>    <p>The second problem deals with finding a best performing pipeline of QA components <span class="inline-equation"><span class="tex">$\psi _q^\mathit {goal}$</span>     </span>, for a question <em>q</em> and a set of QA tasks called <em>goal</em>. Formally, we define this optimisation problem as follows: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \psi _q^\mathit {goal} = \arg \max _{ \eta \in \mathcal {E}(goal)} \lbrace \Omega (\eta ,q)\rbrace \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div>    </p>    <p>where <span class="inline-equation"><span class="tex">$\mathcal {E}(\mathit {goal})$</span>     </span> represents the set of pipelines of QA components that implement <em>goal</em> and <em>&#x03A9;</em>(<em>&#x03B7;</em>, <em>q</em>) corresponds to the estimated performance of the pipeline <em>&#x03B7;</em> on the question <em>q</em>. <em>Solution</em>. We propose a greedy algorithm that relies on the <em>optimisation principle</em> that states that an optimal pipeline for a goal and a question <em>q</em> is composed of the best performing components that implement the tasks of the goal for <em>q</em>. Suppose that &#x2295; denotes the composition of QA components, then an optimal pipeline <span class="inline-equation"><span class="tex">$\psi _q^{goal}$</span>     </span> is defined as follows: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \psi _q^\mathit {goal} := \oplus _{t_i\in \mathit {goal}} \lbrace \gamma _q^{t_i}\rbrace \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>    </p>    <p>The proposed greedy algorithm works in two steps: <em>QA Component Selection</em> and <em>QA Pipeline Generation</em>. During the first step of the algorithm, each task <em>t<sub>i</sub>     </em> in <em>goal</em> is considered in isolation to determine the best performing QA components that implement <em>t<sub>i</sub>     </em> for <em>q</em>, i.e.&#x00A0;<span class="inline-equation"><span class="tex">$\gamma _q^{t_i}$</span>     </span>. For each <em>t<sub>i</sub>     </em> an ordered set of QA components is created based on the performance predicted by the supervised models that learned to solve the problem described in Equation&#x00A0;<a class="eqn" href="#eq3">3</a>. Figure&#x00A0;<a class="fig" href="#fig2">2</a> illustrates the QA component selection steps for the question <em>q</em>=<em>&#x201C;What is the capital of Canada?&#x201D;</em> and <em>goal</em> = {<em>NED</em>, <em>RL</em>, <em>QB</em>}. The algorithm creates an ordered set <span class="inline-equation"><span class="tex">$\mathit {OS}_{t_{i}}$</span>     </span> of QA components for each task <em>t<sub>i</sub>     </em> in <em>goal</em>. Components are ordered in each <span class="inline-equation"><span class="tex">$\mathit {OS}_{t_{i}}$</span>     </span> according to the values of the performance function <em>&#x03C1;</em>(.) predicted by the supervised method trained for questions with the features <span class="inline-equation"><span class="tex">$\mathcal {F}(q)$</span>     </span> and task <em>t<sub>i</sub>     </em>; in our example, <span class="inline-equation"><span class="tex">$\mathcal {F}(q)$</span>     </span>={(QuestionType:What), (AnswerType:String), (#words:6), (#DT:1), (#IN:1), (#WP:1), (#VBZ:1), (#NNP:1), (#NN:1)} indicates that <em>q</em> is a <em>WHAT</em> question whose answer is a <em>String</em>; further, <em>q</em> has six words and POS tags such as determiner, noun etc. Based on this information, the algorithm creates three ordered sets: <em>OS<sub>NED</sub>     </em>, <em>OS<sub>RL</sub>     </em>, and <em>OS<sub>QB</sub>     </em>. The order in <em>OS<sub>NED</sub>     </em> indicates that Dandelion<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>, Tag Me, and DBpedia Spotlight are the top 3 best performing QA components for queries with the features <span class="inline-equation"><span class="tex">$\mathcal {F}(q)$</span>     </span> in the QA task NED; similarly, for <em>OS<sub>RL</sub>     </em> and <em>OS<sub>QB</sub>     </em>.</p>    <p>In the second step, the algorithm follows the optimisation principle in Equation&#x00A0;<a class="eqn" href="#eq5">5</a> and combines the top <em>k<sub>i</sub>     </em> best performing QA components of each ordered set. Values of <em>k<sub>i</sub>     </em> can be configured; however, we have empirically observed that for all studied types of questions and tasks, only the relation linking (RL) task requires considering the top 3 best performing QA components; for the rest of the tasks, the top 1 best performing QA component is sufficient to identify a best performing pipeline. Once the top <em>k<sub>i</sub>     </em> QA components have been selected for each ordered set, the algorithm constructs a QA pipeline and checks if the generated pipeline is able to produce a non-empty answer. If so, the generated QA pipeline is added to the algorithm output. In Equation&#x00A0;<a class="eqn" href="#eq5">5</a>, the algorithm finds that only the QA pipeline Dandelion, ReMatch, and SINA produces results; the other two pipelines fail because the QA components RNLIWOD<a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a> and Spot Property<a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a> are not able to perform the relation linking task of the question <em>q</em>=<em>&#x201C;What is the capital of Canada?&#x201D;</em>. The algorithm ends when the top <em>k<sub>i</sub>     </em> QA components have been combined and checked; the output is the union of the best performing QA pipelines that produce a non-empty answer. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186023/images/www2018-32-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">       <strong>QA Optimisation Pipeline Algorithm</strong>. The algorithm performs two steps: First, QA components are considered in isolation; supervised methods are used to predict the top <em>k</em> best performing QA components per task and question features. Second, the QA Pipelines are generated from the best performing QA component of the tasks NED and QB, and the top 3 QA components of RL. The QA pipeline formed of Dandelion, ReMatch, and SINA successfully answers <em>q</em>.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-19">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span>      <font style="font-variant: small-caps">Frankenstein</font> Framework</h2>    </div>    </header>    <p>    <font style="font-variant: small-caps">Frankenstein</font> is a framework that implements the QA optimisation pipeline algorithm and generates the best performing QA pipelines based on the input question features and QA goal. <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186023/images/www2018-32-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">       <font style="font-variant: small-caps">Frankenstein</font> architecture comprising separate modules for question feature extraction, pipeline generation and optimisation, as well as pipeline execution.</span>     </div>    </figure>    </p>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span>       <font style="font-variant: small-caps">Frankenstein</font> Architecture</h3>     </div>    </header>    <p>Figure&#x00A0;<a class="fig" href="#fig3">3</a> depicts the <font style="font-variant: small-caps">Frankenstein</font> architecture. <font style="font-variant: small-caps">Frankenstein</font> receives, as input, a natural language question as well as a goal consisting of the QA tasks to be executed in the QA pipeline. The features of an input question are extracted by the <em>Feature Extractor</em>; afterwards the <em>QA Component Classifiers</em> predict best performing components per task for the given question; these components are passed to the <em>Pipeline Generator</em>, which generates best performing pipelines to be executed, eventually, by the <em>Pipeline Executor</em>. The <font style="font-variant: small-caps">Frankenstein</font> architecture comprises the following modules: &#x00A0;</p>    <p>     <strong>Feature Extractor</strong>. This module extracts a set of features from a question. Features include question length, question and answer types, and POS tags. Features are discussed in Section&#x00A0;<a class="sec" href="#sec-24">5.2</a>. &#x00A0;</p>    <p>     <strong>QA Components</strong>. <font style="font-variant: small-caps">Frankenstein</font> currently integrates 29 QA components implementing five QA tasks, namely Named Entity Recognition (NER), Named Entity Disambiguation (NED), Relation Linking (RL), Class Linking (CL), and Query Building (QB). To the best of our knowledge, only two reusable CL and QB components, and five reusable RL components are available, therefore the component distribution among tasks is uneven. In most of the cases NED, RL and QB components are necessary to generate the SPARQL query for a NL question. However, to correctly generate a SPARQL query for certain NL questions, it is sometimes necessary to also disambiguate classes against the ontology. For example, in the question <em>&#x201C;Which comic characters are painted by Bill Finger&#x201D;</em>, <em>&#x201C;comic characters&#x201D;</em> needs to be mapped to <tt>dbo:ComicsCharacter</tt><a class="fn" href="#fn10" id="foot-fn10"><sup>10</sup></a>. Table&#x00A0;<a class="tbl" href="#tab2">2</a> provides a list of QA components integrated in <font style="font-variant: small-caps">Frankenstein</font>. The 11 NER components are used with AGDISTIS to disambiguate entities as AGDISTIS requires the question and spotted position of entities as input [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. Henceforth, any reference to NER tool, will refer to its combination with AGDISTIS, and we have excluded individual performance analysis of NER components. However, other 7 NED components recognise and disambiguate the entities directly from the input question. Hence, <font style="font-variant: small-caps">Frankenstein</font> has 18 NED, 5 RL, 2 CL, 2 QB components. &#x00A0;</p>    <p>     <strong>QA Component Classifiers</strong>. For each QA component, a separate Classifier is trained; it learns from a set of features of a question and predicts the performance of all pertaining components. &#x00A0;</p>    <p>     <strong>QA Pipeline optimiser</strong>. Pipeline optimisation is performed by two modules. The <strong>Component Selector</strong> selects the best performing components for accomplishing a given task based on the input features and the results of the QA Component Classifiers; the selected QA components are afterwards forwarded to the <strong>Pipeline Generator</strong> to dynamically generate the corresponding QA pipelines. &#x00A0;</p>    <p>     <strong>Pipeline Executor</strong>. This modules executes the generated pipelines for an input question in order to extract answers from the knowledge base (i.e.&#x00A0;DBpedia in our case).</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">       <strong>29 QA components integrated in</strong>       <font style="font-variant: small-caps">       <strong>Frankenstein</strong>       </font>: 8 QA components are not available as open source software, 25 provide a RESTful service API and 19 are accompanied by peer-reviewed publications.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>Component/</strong>       </th>       <th style="text-align:left;">        <strong>QA Task</strong>       </th>       <th style="text-align:left;">        <strong>Year</strong>       </th>       <th style="text-align:left;">        <strong>Open</strong>       </th>       <th style="text-align:left;">        <strong>RESTful</strong>       </th>       <th style="text-align:left;">        <strong>Publi-</strong>       </th>       </tr>       <tr>       <th style="text-align:left;">        <strong>Tool</strong>       </th>       <th style="text-align:left;"/>       <th style="text-align:left;"/>       <th style="text-align:left;">        <strong>Source</strong>       </th>       <th style="text-align:left;">        <strong>Service</strong>       </th>       <th style="text-align:left;">        <strong>cation</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <em>Entity Classifier</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0007">7</a>]</td>       <td style="text-align:left;">NER</td>       <td style="text-align:left;">2013</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Stanford NLP</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0010">10</a>]</td>       <td style="text-align:left;">NER</td>       <td style="text-align:left;">2005</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Ambiverse</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0011">11</a>]<sup>i</sup>       </td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">2014</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Babelfy</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0019">19</a>]<sup>ii</sup>       </td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">2014</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>AGDISTIS</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0032">32</a>]</td>       <td style="text-align:left;">NED</td>       <td style="text-align:left;">2014</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>MeaningCloud</em>        <sup>iii</sup>       </td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">2016</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>DBpedia Spotlight</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0018">18</a>]</td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">2011</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Tag Me API</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0008">8</a>]</td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">2012</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Aylien API</em>        <sup>iv</sup>       </td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">-</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>TextRazor</em>        <sup>v</sup>       </td>       <td style="text-align:left;">NER</td>       <td style="text-align:left;">-</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>OntoText</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0016">16</a>]<sup>vi</sup>       </td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">-</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Dandelion</em>        <sup>vii</sup>       </td>       <td style="text-align:left;">NER/NED</td>       <td style="text-align:left;">-</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>RelationMatcher</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0027">27</a>]</td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">2017</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>ReMatch</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0020">20</a>]</td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">2017</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>RelMatch</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0015">15</a>]</td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">2017</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>RNLIWOD</em>        <sup>viii</sup>       </td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">2016</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>Spot Property</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0015">15</a>]<sup>ix</sup>       </td>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">2017</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>OKBQA DM CLS</em>        <sup>ix</sup>       </td>       <td style="text-align:left;">CL</td>       <td style="text-align:left;">2017</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>NLIWOD CLS</em>        <sup>viii</sup>       </td>       <td style="text-align:left;">CL</td>       <td style="text-align:left;">2016</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>SINA</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0023">23</a>]</td>       <td style="text-align:left;">QB</td>       <td style="text-align:left;">2013</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2713;</td>       </tr>       <tr>       <td style="text-align:left;">        <em>NLIWOD QB</em>        <sup>         <em>viii</em>        </sup>       </td>       <td style="text-align:left;">QB</td>       <td style="text-align:left;">2016</td>       <td style="text-align:left;">&#x2713;</td>       <td style="text-align:left;">&#x2717;</td>       <td style="text-align:left;">&#x2717;</td>       </tr>      </tbody>      <tfoot>       <tr>       <td style="text-align:center;" colspan="6"><sup>i</sup><a class="link-inline force-break" href="https://developer.ambiverse.com/">https://developer.ambiverse.com/</a></td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>ii</sup><a class="link-inline force-break" href="https://github.com/dbpedia-spotlight/dbpedia-spotlight">https://github.com/dbpedia-spotlight/dbpedia-spotlight</a>       </td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>iii</sup><a class="link-inline force-break" href="https://www.meaningcloud.com/developer">https://www.meaningcloud.com/developer</a>       </td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>iv</sup><a class="link-inline force-break" href="http://docs.aylien.com/docs/introduction">http://docs.aylien.com/docs/introduction</a></td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>v</sup><a class="link-inline force-break" href="https://www.textrazor.com/docs/rest">https://www.textrazor.com/docs/rest</a></td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>vi</sup><a class="link-inline force-break" href="http://docs.s4.ontotext.com/display/S4docs/REST+APIs">http://docs.s4.ontotext.com/display/S4docs/REST+APIs</a></td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>vii</sup><a class="link-inline force-break" href="https://dandelion.eu/docs/api/datatxt/nex/getting-started/">https://dandelion.eu/docs/api/datatxt/nex/getting-started/</a></td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>viii</sup>Component is similar to Relation Linker of <a class="link-inline force-break" href="https://github.com/dice-group/NLIWOD">https://github.com/dice-group/NLIWOD</a>. </td>       </tr>       <tr>       <td style="text-align:center;" colspan="6"><sup>ix</sup>Component is similar to Class Linker of <a class="link-inline force-break" href="http://repository.okbqa.org/components/7">http://repository.okbqa.org/components/7</a>. </td>       </tr>      </tfoot>     </table>    </div>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Implementation Details</h3>     </div>    </header>    <p>The code for <font style="font-variant: small-caps">Frankenstein</font> including all 29 integrated components and empirical study results can be found in our open source GitHub repository<a class="fn" href="#fn11" id="foot-fn11"><sup>11</sup></a>. The integration of QA components within <font style="font-variant: small-caps">Frankenstein</font> as a loosely coupled architecture is based on the following guiding principles: (a) Reusability (the framework should be available as open source), (b) Interoperability between QA components, (c) Flexibility (easy integration of components at any step of the QA pipeline), and (d) Isolation (components are independent of each other and provide exchangeable interfaces). We studied the implementations of <em>OKBQA</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], <em>openQA</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>], <em>Qanary</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>] and <em>QALL-ME</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]; from these, to the best of our knowledge, only Qanary can fulfil the aforementioned guiding principles. Unlike a monolithic QA system the output of a component is not directly passed to the next component in the QA process and Qanary enhances the knowledge base after each step via the abstract level defined by the <tt>qa</tt> vocabulary. Therefore, components become independent of each other, and can easily be exchangeable just by configuration. The integration of the 29 new components with the <em>Qanary</em> methodology in <font style="font-variant: small-caps">Frankenstein</font> is implemented in Java 8. Remaining <font style="font-variant: small-caps">Frankenstein</font> modules are implemented in Python 3.4.</p>    </section>   </section>   <section id="sec-22">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Corpus Creation</h2>    </div>    </header>    <p>In this section, we describe the datasets used in our study and how we prepare the training dataset for our classification experiments. All experiments were executed on 10 virtual servers, each with 8 cores, 32&#x00A0;GB RAM and the Ubuntu 16.04.3 operating system. It took us 22 days to generate training data by executing questions of considered datasets for all 28 components, as some tools such as ReMatch[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] and RelationMatcher [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] took approximately 120 and 30 seconds, respectively, to process each question.</p>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Description of Datasets</h3>     </div>    </header>    <p>Throughout our experiment, we employed the Large-Scale Complex Question Answering Dataset<a class="fn" href="#fn12" id="foot-fn12"><sup>12</sup></a> (LC-QuAD)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>] as well as the 5<sup>th</sup> edition of Question Answering over Linked Data Challenge<a class="fn" href="#fn13" id="foot-fn13"><sup>13</sup></a> (QALD-5) dataset&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. &#x00A0;</p>    <p>     <strong>LC-QuAD</strong> has 5,000 questions expressed in natural language along with their formal representation (i.e.&#x00A0;SPARQL query), which is executable on DBpedia. W.r.t.&#x00A0;the state of the art, this is the largest available benchmark for the QA community over Linked Data. We ran the entire set of SPARQL queries (on 2017-10-02) over the DBpedia endpoint<a class="fn" href="#fn14" id="foot-fn14"><sup>14</sup></a>, and found that only 3,252 of them returned an answer. Therefore, we rely on these 3,252 questions throughout our experiment. &#x00A0;</p>    <p>     <strong>QALD-5</strong>. Out of the QALD challenge series, we chose the 5th version (QALD-5) because it provides the largest number of questions (350 questions). However, during the experimental phase the remote Web service of the ReMatch component went down and we were only able to obtain proper results for 204 of the 350 questions. Therefore, we took these 204 questions into account to provide a fair and comparable setting (although, we obtained the results for all 350 questions for all other components).</p>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Preparing Training Datasets</h3>     </div>    </header>    <p>Since we have to build an individual classifier for each component in order to predict the performance of that component, it is required to prepare a single training dataset per component. The whole sample set within the training dataset was formed by using the NL questions included from the datasets described previously (from both QALD and LC-QuAD). In order to obtain an abstract and concrete representation of NL questions, we extracted major features enumerated below.</p>    <ol class="list-no-style">     <li id="list4" label="(1)"><em>Question Length:</em>The length of a question w.r.t. the number of words has been introduced as a lexical feature by Blunsom et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0003">3</a>] in 2006. In our running example <em>&#x201C;What is the capital of Canada?&#x201D;</em>, this feature has the numeric value 6.<br/></li>     <li id="list5" label="(2)"><em>Question Word:</em>Huang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0014">14</a>] considered the question word (&#x201C;wh-head word&#x201D;) as a separate lexical feature for question classification. If a specific question word is present in the question, we assign the value 1, and 0 to the rest of the question words. We adapted 7 Wh-words: &#x201C;what&#x201D;, &#x201C;which&#x201D;, &#x201C;when&#x201D;, &#x201C;where&#x201D;, &#x201C;who&#x201D;, &#x201C;how&#x201D; and, &#x201C;why&#x201D;. In <em>&#x201C;What is the capital of Canada?&#x201D;</em>, <em>&#x201C;What&#x201D;</em> is assigned the value 1, and all the other words are assigned 0.<br/></li>     <li id="list6" label="(3)"><em>Answer Type:</em>This feature set has three dimensions, namely &#x201C;Boolean&#x201D;, &#x201C;List/Resource&#x201D;, and &#x201C;Number&#x201D;. These dimensions determine the category of the expected answer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0022">22</a>]. In our running example, we assign &#x201C;List/Resource&#x201D; for this dimension because the expected answer is the resource <tt>dbr:Ottawa</tt>.<br/></li>     <li id="list7" label="(4)"><em>POS Tags:</em>Part of Speech (POS) tags are considered an independent syntactical question feature that can affect the overall performance of a QA system&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0003">3</a>]. We used the Stanford Parser<a class="fn" href="#fn15" id="foot-fn15"><sup>15</sup></a> to identify the POS tags, where the number of occurrences is considered as a separate dimension in the question feature extraction.<br/></li>    </ol>    <p>We prepared two separate datasets from LC-QuAD and QALD. We adopted the methodology presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] for the benchmark creation of the subsequent steps of the QA pipelines. Furthermore, the accuracy metrics are <em>micro&#x00A0;F-Score&#x00A0;(F-Score)</em> as a harmonic mean of micro precision and micro recall. Thus, the label set of the training datasets for a given component was set up by measuring the micro&#x00A0;F-Score&#x00A0;(F-Score) of every given question.</p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Evaluating Component Performance</h2>    </div>    </header>    <p>The aim of this experiment is to evaluate the performance of components on the micro and macro levels and then train a classifier to accurately predict the performance of each component.</p>    <p>    <em>Metrics</em>. <em>i</em>) <em>ii</em>) <tt>Answered Questions</tt>: The number of questions for which the QA pipeline returns an answer. <em>iii</em>) <tt>Micro Precision&#x00A0;(MP)</tt>: The ratio of correct answers vs. total number of answers retrieved by a component for a particular question. <em>iv</em>) <tt>Precision (P)</tt>: For a given component, the average of the Micro Precision over all questions. <em>v</em>) <tt>Micro&#x00A0;Recall&#x00A0;(MR)</tt>: For each question, the number of correct answers retrieved by a component vs. gold standard answers for the given question. <em>vi</em>) <tt>Recall&#x00A0;(R)</tt>: For a given component, the average of Micro Recall over all questions. <em>vii</em>) <tt>Micro&#x00A0;F-Score&#x00A0;(F-Score)</tt>: For each question, the harmonic mean of MP and MR. <em>viii</em>) <tt>Macro F-Score&#x00A0;(F)</tt>: For each component, harmonic mean of P and R.</p>    <section id="sec-26">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Macro-level Performance of Components</h3>     </div>    </header>    <p>In this experiment, we measured the performance of the reusable components from the QA community that are part of <font style="font-variant: small-caps">Frankenstein</font>. We executed each component for each individual query from both LC-QuAD and QALD datasets. Then, for each dataset we calculated the macro accuracy per component and selected those representing highest macro performance. The performance results of the best components are shown in Table&#x00A0;<a class="tbl" href="#tab3">3</a>. For brevity, detailed results for each component are placed in our GitHub repository<a class="fn" href="#fn16" id="foot-fn16"><sup>16</sup></a>.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">The macro accuracy of the best components for each task on the QALD and LC-QuAD corpora.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>QA Task</strong>       </th>       <th style="text-align:left;">        <strong>Dataset</strong>       </th>       <th style="text-align:left;">        <strong>Best Component</strong>       </th>       <th style="text-align:left;">        <strong>P</strong>       </th>       <th style="text-align:left;">        <strong>R</strong>       </th>       <th style="text-align:left;">        <strong>F</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">QB</td>       <td style="text-align:left;">LC-QuAD</td>       <td style="text-align:left;">NLIWOD QB</td>       <td style="text-align:left;">0.48</td>       <td style="text-align:left;">0.49</td>       <td style="text-align:left;">0.48</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">QALD-5</td>       <td style="text-align:left;">NLIWOD QB</td>       <td style="text-align:left;">0.49</td>       <td style="text-align:left;">0.50</td>       <td style="text-align:left;">0.49</td>       </tr>       <tr>       <td style="text-align:left;">CL</td>       <td style="text-align:left;">LC-QuAD</td>       <td style="text-align:left;">OKBQA DM CLS</td>       <td style="text-align:left;">0.47</td>       <td style="text-align:left;">0.59</td>       <td style="text-align:left;">0.52</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">QALD-5</td>       <td style="text-align:left;">OKBQA DM CLS</td>       <td style="text-align:left;">0.58</td>       <td style="text-align:left;">0.64</td>       <td style="text-align:left;">0.61</td>       </tr>       <tr>       <td style="text-align:left;">NED</td>       <td style="text-align:left;">LC-QuAD</td>       <td style="text-align:left;">Tag Me</td>       <td style="text-align:left;">0.69</td>       <td style="text-align:left;">0.66</td>       <td style="text-align:left;">0.67</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">QALD-5</td>       <td style="text-align:left;">DBpedia Spotlight</td>       <td style="text-align:left;">0.67</td>       <td style="text-align:left;">0.75</td>       <td style="text-align:left;">0.71</td>       </tr>       <tr>       <td style="text-align:left;">RL</td>       <td style="text-align:left;">LC-QuAD</td>       <td style="text-align:left;">RNLIWOD</td>       <td style="text-align:left;">0.25</td>       <td style="text-align:left;">0.22</td>       <td style="text-align:left;">0.23</td>       </tr>       <tr>       <td style="text-align:left;"/>       <td style="text-align:left;">QALD-5</td>       <td style="text-align:left;">ReMatch</td>       <td style="text-align:left;">0.54</td>       <td style="text-align:left;">0.74</td>       <td style="text-align:left;">0.62</td>       </tr>      </tbody>     </table>    </div>    <p>     <strong>Key Observation: Dependency on Quality of Input Question</strong>. From Table&#x00A0;<a class="tbl" href="#tab3">3</a>, it is clear that the performance considerably varies per dataset. This is because the quality of questions differs across datasets. Quality has various dimensions, such as complexity or expressiveness. For example, only 728 (22&#x00A0;%) questions of LC-QuAD are simple (i.e.&#x00A0;with single relation, single entity), compared to 108 questions (53&#x00A0;%) of QALD. The average length of a question in LC-QuAD is 10.63, compared to 7.41 in QALD. Therefore, components that perform well for identifying an entity in a simple question may not perform equally well on LC-QuAD, which is also evident from Table&#x00A0;<a class="tbl" href="#tab3">3</a> considering the NED task. The same holds for RL components. ReMatch, which is the clear winner on QALD, is outperformed by RNLIWOD on LC-QuAD. Hence, there is no overall best performing QA component for these two tasks, and the definition of the best performing QA component differs across datasets. However, this does holds true neither for CL components nor for QB components (note, these two tasks only have two components each), even though the Macro F-Score values on both datasets have significant differences.</p>    </section>    <section id="sec-27">    <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Training the Classifiers</h3>     </div>    </header>    <p>The aim of this part is to build up classifiers which efficiently predict the performance of a given component for a given question w.r.t. a particular task. As observed in the micro F-Score values of the components, these values are not continuous but usually discrete, e.g., 0.0, 0.33, 0.5, 0.66 or 1. Hence, we adopted five classification algorithms (treating it as a classification problem) namely 1) Support Vector Machines (SVM), 2) Gaussian Naive Bayes, 3) Decision Tree, 4) Random Forest, and 5) Logistic Regression. During the training phase, each classifier was tuned with a range of regularisation parameters to optimise the performance of the classifier on the available datasets. We used the cross-validation approach with 10 folds on the LC-QuAD dataset. Figure&#x00A0;<a class="fig" href="#fig4">4</a> illustrates the details of our experiment for training classifiers. Predominantly, Logistic Regression and Support Vector Machines expose higher accuracy as illustrated in Figure&#x00A0;<a class="fig" href="#fig4">4</a>. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186023/images/www2018-32-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">       <strong>Comparison of Classifiers for all QA Components</strong>. Five Classifiers, namely Logistic Regression, Support Vector Machines, Decision Tree, Gaussian Naive Bayes, and Random Forest are compared with respect to accuracy.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-28">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Evaluating Pipeline Performance</h2>    </div>    </header>    <p>In this experiment, we pursue the evaluation question <em>&#x201C;Can an approach that dynamically combines different QA components taking the question type into account (such as</em>     <font style="font-variant: small-caps">Frankenstein</font>    <em>) take advantage of the multitude of components available for specific tasks?&#x201D;</em> To answer this question, we measure the <font style="font-variant: small-caps">Frankenstein</font> performance on the (i) task level and (ii) pipeline level. Throughout our experiment, we adopt a component selector strategy as follows:</p>    <ol class="list-no-style">    <li id="list8" label="(1)"><em>Baseline-LC-QuAD</em>: The best component for each task in terms of Macro F-Score on the LC-QuAD dataset (cf.&#x00A0;Section&#x00A0;<a class="sec" href="#sec-26">6.1</a>).<br/></li>    <li id="list9" label="(2)"><em>Baseline-QALD</em>: The best component for each task in terms of Macro F-Score on the QALD dataset (cf.&#x00A0;Section&#x00A0;<a class="sec" href="#sec-26">6.1</a>).<br/></li>    <li id="list10" label="(3)"><font style="font-variant: small-caps">Frankenstein</font>     <em>-Static</em>: The QA pipeline consisting of the best performing components for each task on the QALD dataset (cf.&#x00A0;Section&#x00A0;<a class="sec" href="#sec-26">6.1</a>).<br/></li>    <li id="list11" label="(4)"><font style="font-variant: small-caps">Frankenstein</font>     <em>-Dynamic</em>: The QA pipeline consisting of the top performing components from the learning approach.<br/></li>    <li id="list12" label="(5)"><font style="font-variant: small-caps">Frankenstein</font>     <em>-Improved</em>: Similar to the dynamic <font style="font-variant: small-caps">Frankenstein</font> pipeline with a different setting.<br/></li>    </ol>    <section id="sec-29">    <header>     <div class="title-info">      <h3>       <span class="section-number">7.1</span> Task-level Experiment</h3>     </div>    </header>    <p>Our major goal is to examine whether or not we can identify the N-best components for each QA task. Accordingly, we utilised the following metrics for evaluation <em>i</em>) <em>Total Questions</em>: the average number of questions in the underlying test dataset. <em>ii</em>) <em>Answerable</em>: the average number of questions for which at least one of the components has an F-Score greater than 0.5. <em>iii</em>) <em>Top N</em>: the average number of questions for which at least one of the Top N components selected by the Classifier has an F-Score greater than 0.5. Furthermore, we rely on a top-<em>N</em> approach for choosing the best performing component during our judgement. &#x00A0;</p>    <p>     <strong>Experiments on LC-QuAD</strong>. This experiment was run on the questions from the LC-QuAD dataset by applying a cross-validation approach. We compare the component selector approach in (i) learning-based manner &#x2013; called <font style="font-variant: small-caps">Frankenstein</font>, and (ii) Baseline-LC-QuAD manner &#x2013; called Baseline. Table&#x00A0;<a class="tbl" href="#tab4">4</a> shows the results of our experiment. <font style="font-variant: small-caps">Frankenstein</font>&#x2019;s learning-based approach selects the top-<em>N</em> components with the highest predicted performance values for a given input question. Obviously, this approach outperforms the Baseline approach for the NED, RL, and QB tasks and equals the Baseline for CL task. When we select the top-2 or top-3 best performing components, <font style="font-variant: small-caps">Frankenstein</font>&#x2019;s performance improves further.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">10-fold Validation on LC-QuAD.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>QA</strong>       </th>       <th style="text-align:left;">        <strong>Total</strong>       </th>       <th style="text-align:left;">        <strong>Answer-</strong>       </th>       <th style="text-align:center;" colspan="3">        <font style="font-variant: small-caps">         <strong>Frankenstein</strong>        </font>        <hr/>       </th>       <th style="text-align:left;">        <strong>Baseline</strong>       </th>       </tr>       <tr>       <th style="text-align:left;">        <strong>Task</strong>       </th>       <th style="text-align:left;">        <strong>Questions</strong>       </th>       <th style="text-align:left;">        <strong>able</strong>       </th>       <th style="text-align:left;">        <strong>Top1</strong>       </th>       <th style="text-align:left;">        <strong>Top2</strong>       </th>       <th style="text-align:left;">        <strong>Top3</strong>       </th>       <th style="text-align:left;"/>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">QB</td>       <td style="text-align:center;">324.3</td>       <td style="text-align:center;">175.4</td>       <td style="text-align:center;">162.7</td>       <td style="text-align:center;">175.4</td>       <td style="text-align:center;">&#x2013;</td>       <td style="text-align:center;">159.6</td>       </tr>       <tr>       <td style="text-align:left;">CL</td>       <td style="text-align:center;">324.3</td>       <td style="text-align:center;">76</td>       <td style="text-align:center;">68.1</td>       <td style="text-align:center;">76</td>       <td style="text-align:center;">&#x2013;</td>       <td style="text-align:center;">68.2</td>       </tr>       <tr>       <td style="text-align:left;">NED</td>       <td style="text-align:center;">324.3</td>       <td style="text-align:center;">294.2</td>       <td style="text-align:center;">245.2</td>       <td style="text-align:center;">270.9</td>       <td style="text-align:center;">284.3</td>       <td style="text-align:center;">236.3</td>       </tr>       <tr>       <td style="text-align:left;">RL</td>       <td style="text-align:center;">324.3</td>       <td style="text-align:center;">153.1</td>       <td style="text-align:center;">90.3</td>       <td style="text-align:center;">118.9</td>       <td style="text-align:center;">134.4</td>       <td style="text-align:center;">84.2</td>       </tr>      </tbody>     </table>    </div>    <p>&#x00A0;</p>    <p>     <strong>Cross Training Experiment</strong>. The purpose of this experiment is similar to the previous experiment but in order to verify the credibility of our approach, we extended our dataset by including questions from QALD. In fact, questions from QALD are utilised as the test dataset. The results of this experiment are shown in Table&#x00A0;<a class="tbl" href="#tab5">5</a>. We observe that <font style="font-variant: small-caps">Frankenstein</font> significantly outperforms the LC-QuAD Baseline components for the NED (i.e.&#x00A0;Tag Me) and RL (i.e.&#x00A0;RNLIWOD) tasks while it achieves comparable results for the CL task.</p>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Performance comparison on task level using LC-QuAD as training and QALD as test dataset (204 questions).</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <strong>QA</strong>       </th>       <th style="text-align:right;">        <strong>Answer-</strong>       </th>       <th style="text-align:center;" colspan="3">        <font style="font-variant: small-caps">         <strong>Frankenstein</strong>        </font>        <hr/>       </th>       <th style="text-align:right;">        <strong>Baseline</strong>       </th>       <th style="text-align:right;">        <strong>Baseline</strong>       </th>       </tr>       <tr>       <th style="text-align:left;">        <strong>Task</strong>       </th>       <th style="text-align:right;">        <strong>able</strong>       </th>       <th style="text-align:right;">        <strong>Top1</strong>       </th>       <th style="text-align:right;">        <strong>Top2</strong>       </th>       <th style="text-align:right;">        <strong>Top3</strong>       </th>       <th style="text-align:right;">        <strong>QALD</strong>       </th>       <th style="text-align:right;">        <strong>LC-QuAD</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">QB</td>       <td style="text-align:right;">119</td>       <td style="text-align:right;">91</td>       <td style="text-align:right;">119</td>       <td style="text-align:right;">&#x2013;</td>       <td style="text-align:right;">102</td>       <td style="text-align:right;">102</td>       </tr>       <tr>       <td style="text-align:left;">CL</td>       <td style="text-align:right;">55</td>       <td style="text-align:right;">52</td>       <td style="text-align:right;">55</td>       <td style="text-align:right;">&#x2013;</td>       <td style="text-align:right;">52</td>       <td style="text-align:right;">52</td>       </tr>       <tr>       <td style="text-align:left;">NED</td>       <td style="text-align:right;">168</td>       <td style="text-align:right;">132</td>       <td style="text-align:right;">153</td>       <td style="text-align:right;">163</td>       <td style="text-align:right;">144</td>       <td style="text-align:right;">109</td>       </tr>       <tr>       <td style="text-align:left;">RL</td>       <td style="text-align:right;">138</td>       <td style="text-align:right;">83</td>       <td style="text-align:right;">107</td>       <td style="text-align:right;">121</td>       <td style="text-align:right;">105</td>       <td style="text-align:right;">46</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-30">    <header>     <div class="title-info">      <h3>       <span class="section-number">7.2</span> Pipeline-level Experiment</h3>     </div>    </header>    <p>In this experiment, we greedily arranged a pipeline by choosing the best performing components per task from three strategies. We use the same settings as cross training experiments by utilising QALD questions as test dataset. The first one is the <font style="font-variant: small-caps">Frankenstein</font>-Static pipeline composed of Baseline components driven by QALD (i.e.&#x00A0;DBpedia Spotlight for NED, ReMatch for RL, OKBQA DM for CL, and NLIWOD QB for QB). The other two strategies are the <font style="font-variant: small-caps">Frankenstein</font>-Dynamic and <font style="font-variant: small-caps">Frankenstein</font>-Improved pipelines composed by the learning-based component selector with top-1 setting (top-3 for RL of the improved strategy). The results of the comparison are demonstrated in Table&#x00A0;<a class="tbl" href="#tab6">6</a>. We conclude that the accuracy metrics for the dynamic pipeline are lower than for the static pipeline. (Note: As performance metrics for state-of-the-art QA systems on the same set of questions in QALD were not available, we excluded this comparison in the table.)</p>    <div class="table-responsive" id="tab6">     <div class="table-caption">      <span class="table-number">Table 6:</span>      <span class="table-title">       <strong>Comparison with the Baseline Pipeline.</strong>      </span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">        <font style="font-variant: small-caps">         <strong>Frankenstein</strong>        </font>        <strong>-</strong>       </th>       <th style="text-align:left;">        <strong>Answered</strong>       </th>       <th style="text-align:left;">        <strong>P</strong>       </th>       <th style="text-align:left;">        <strong>R</strong>       </th>       <th style="text-align:left;">        <strong>Macro F-</strong>       </th>       </tr>       <tr>       <th style="text-align:left;">        <strong>Pipeline</strong>       </th>       <th style="text-align:left;">        <strong>Questions</strong>       </th>       <th style="text-align:left;"/>       <th style="text-align:left;"/>       <th style="text-align:left;">        <strong>Score</strong>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Static</td>       <td style="text-align:left;">37</td>       <td style="text-align:left;">0.17</td>       <td style="text-align:left;">0.19</td>       <td style="text-align:left;">0.18</td>       </tr>       <tr>       <td style="text-align:left;">Dynamic</td>       <td style="text-align:left;">29</td>       <td style="text-align:left;">0.14</td>       <td style="text-align:left;">0.14</td>       <td style="text-align:left;">0.14</td>       </tr>       <tr>       <td style="text-align:left;">Improved</td>       <td style="text-align:left;">41</td>       <td style="text-align:left;">0.20</td>       <td style="text-align:left;">0.21</td>       <td style="text-align:left;">0.20</td>       </tr>      </tbody>     </table>    </div>    <p>     <em>i</em>) We noticed that the failure of the RL component significantly affects the total performance of both static or dynamic pipelines. Thus, we selected the top-3 components for the RL task to compensate for this deficiency. This setting yields the <font style="font-variant: small-caps">Frankenstein</font>-Improved pipeline, where we ran three dynamically composed pipelines out of the 360 possible ones per question. Although this strategy is expected to affect the total accuracy negatively, this did not happen in practice; we even observed an increase in the overall precision, recall, and F-Score. Typically, the available RL and QB components have a full accomplishment or full failure for the input question. For example, considering the question <em>&#x201C;What is the capital of Canada?&#x201D;</em>, two of the top-3 selected RL components do not process the question. Hence, eventually, only one of the three pipelines returns a final answer. Thus, this simple modification in the setting significantly improves overall pipeline performance and the number of answered questions. With the static pipeline, the number of answered questions is fixed, however, with dynamic pipelines, the number of answered questions can be increased.</p>    </section>   </section>   <section id="sec-31">    <header>    <div class="title-info">     <h2>      <span class="section-number">8</span> Discussion</h2>    </div>    </header>    <p>Despite the significant overall performance achieved by <font style="font-variant: small-caps">Frankenstein</font>, we investigated erroneous cases in performance specifically w.r.t. classifiers. For instance, in our exemplary question <em>&#x201C;What is the capital of Canada?&#x201D;</em>, the top-1 component predicted by the Classifier for the RL task fails to map <em>&#x201C;capital of&#x201D;</em> to <tt>dbo:capital</tt>. Similarly, for the question (<em>&#x201C;Who is the mayor of Berlin?&#x201D;</em>), the predicted Dandelion NED component can not recognise and disambiguate <em>&#x201C;Berlin&#x201D;</em>. One of the reasons is related to the primary features extracted from questions. We plan to extend the feature set especially using the recent embedding models and also using different features per task as we have currently used the same features for all tasks. This can be done by associating features with component performance and calculating Cram&#x00E9;r&#x0027;s V-coefficient for each feature and a component&#x0027;s ability to answer the given question&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. One more extension is about switching to more sophisticated learning approaches like HMM, or deep learning approaches which require significantly larger datasets. Another observation is that the existing RL and QB components generally result in poor performance. The QB components need improvement in cases where previous tasks yield a low F-Score for a given question (i.e. returning more than one DBpedia URL as an answer). Hence, QB components should intuitively learn based on available URLs of entities and relations, and then form the right query. The current QB components fail to do so, which severely affected the overall performance of the complete QA pipelines (cf. Table&#x00A0;<a class="tbl" href="#tab6">6</a>). Further, RL and QB components need significant improvements in runtime efficiency and performance on complex questions. Thus, the QA community has to pay more attention to improve the components accomplishing these two tasks. To the best of our knowledge, currently very few independent components are available for these tasks (also for Class Linking) and the QA community can contribute building more independent components for these tasks. The <font style="font-variant: small-caps">Frankenstein</font> architecture is not rigid and not restricted to the tasks considered in this paper. With the availability of more components performing new QA tasks (e.g., answer type prediction, syntactic parsing, query re-ranking, etc.), just by extending concepts of the <tt>qa</tt> vocabulary, new components can be added to the platform. Furthermore, in real world settings, a greedy approach may negatively affect the runtime of the pipeline. Hence, to provide a more efficient framework for creating QA pipelines, we plan to replace the greedy approach with concepts similar to web service composition&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>], where we assign cost metrics (e.g., precision, runtime, or memory consumption) to select components using a pipeline optimiser in an automatic way.</p>   </section>   <section id="sec-32">    <header>    <div class="title-info">     <h2>      <span class="section-number">9</span> Related Work</h2>    </div>    </header>    <p>Since 2011, 38 QA systems have participated in the eight editions of the Question Answering of Linked Data (QALD) challenge&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. However, most participants started building their QA system from scratch and did not reuse the plethora of already available components for the QA subtasks that had to be addressed. Nevertheless, some early approaches have already tried to solve this problem; examples include openQA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>], a modular open-source framework for implementing QA systems from components having rigid but standardised Java interfaces. Also, QALL-ME&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] provides a SOA-based architecture skeleton for building components for multilingual QA systems over structured data. Moreover, OAQA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] follows several architectural commitments to components to enable interchangeability. QANUS&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] also provides capabilities for the rapid development of information retrieval based QA systems. In 2014, Both et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] presented a first semantic approach for integrating components, where modules that carry semantic self-descriptions in RDF are collected for a particular query until the search intent can be served. A more recent approach in this direction is Qanary&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], which generates QA systems from semantically described components wired together manually. Using the QA ontology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] provided by Qanary, modules can be exchanged, e.g.&#x00A0;various versions of NER tools, to benchmark various pipelines and choose the best performing one. In 2017, QA4ML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] described a similar approach to <font style="font-variant: small-caps">Frankenstein</font>, where a QA system is selected out of 6 QA systems based on the question type. To the best of our knowledge there is no question answering framework that enables the automatic and dynamic composition of QA components to generate optimised QA pipelines based on question features.</p>   </section>   <section id="sec-33">    <header>    <div class="title-info">     <h2>      <span class="section-number">10</span> Conclusions and Future Work</h2>    </div>    </header>    <p>    <font style="font-variant: small-caps">Frankenstein</font> is the first framework of its kind for integrating all state-of-the-art QA components to build more powerful QA systems with collaborative efforts. It simplifies the integration of emerging components and is sensitive to the input question. The rationale was not to build a QA system from scratch but instead to reuse the currently existing 29 major components being available today to the QA community. Furthermore, our effort demonstrates the ability to integrate the components released by the research community in a single platform. <font style="font-variant: small-caps">Frankenstein</font>&#x2019;s loosely coupled architecture enables easy integration of newer components in the framework and implements a model that learns from the features extracted from the input questions to direct the user&#x0027;s question to the best performing component per QA task. Also, <font style="font-variant: small-caps">Frankenstein</font>&#x2019;s design is component agnostic; therefore, <font style="font-variant: small-caps">Frankenstein</font> can easily be applied to new knowledge bases and domains. Thus, it is a framework for automatically producing intelligent QA pipelines. Our experimental study provides a holistic overview on the performance of state-of-the-art QA components for various QA tasks and pipelines. In addition, it measures and compares the performance of <font style="font-variant: small-caps">Frankenstein</font> from several perspectives in multiple settings and demonstrates the beneficial characteristics of the approach.</p>    <p>We plan to extend our work in the following directions: (i) improving quality as well as quantity of extracted features, (ii) improving the learning algorithm, (iii) extending our training datasets, and (iv) including more emerging QA components. In conclusion, our component-oriented approach enables the research community to further improve the performance of state-of-the-art QA systems by adding new components to the ecosystem, or to extend the available data (i.e.&#x00A0;gold standard) to adapt the training process.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-34">    <header>    <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>    </div>    </header>    <p>This work has received funding from the EU H2020 R&#x0026;I programme for the Marie Sk&#x0142;odowska-Curie action <em>WDAqua</em> (GA No 642795), Eurostars project <em>QAMEL</em> (E!9725), and EU H2020 R&#x0026;I <em>HOBBIT</em> (GA 688227). We thank Yakun Li, Osmar Zaiane, and Anant Gupta for their useful suggestions.</p>   </section>   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">S&#x00F6;ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary&#x00A0;G. Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In <em>      <em>The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007.</em>     </em></li>    <li id="BibPLXBIB0002" label="[2]">Rainer Berbner, Michael Spahn, Nicolas Repp, Oliver Heckmann, and Ralf Steinmetz. 2006. Heuristics for QoS-aware Web Service Composition. In <em>      <em>2006 IEEE International Conference on Web Services (ICWS 2006), 18-22 September 2006, Chicago, Illinois, USA</em>     </em>. 72&#x2013;82. <a class="link-inline force-break" href="https://doi.org/10.1109/ICWS.2006.69"      target="_blank">https://doi.org/10.1109/ICWS.2006.69</a></li>    <li id="BibPLXBIB0003" label="[3]">Phil Blunsom, Krystle Kocik, and James&#x00A0;R. Curran. 2006. Question classification with log-linear models. In <em>      <em>SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006</em>     </em>. ACM, 615&#x2013;616.</li>    <li id="BibPLXBIB0004" label="[4]">Andreas Both, Dennis Diefenbach, Kuldeep Singh, Saeedeh Shekarpour, Didier Cherix, and Christoph Lange. 2016. Qanary - A Methodology for Vocabulary-Driven Open Question Answering Systems. In <em>      <em>The Semantic Web. Latest Advances and New Domains - 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 - June 2, 2016, Proceedings</em>     </em>. Springer, 625&#x2013;641.</li>    <li id="BibPLXBIB0005" label="[5]">Andreas Both, Axel-Cyrille&#x00A0;Ngonga Ngomo, Ricardo Usbeck, Denis Lukovnikov, Christiane Lemke, and Maximilian Speicher. 2014. A service-oriented search framework for full text, geospatial and semantic search. In <em>      <em>Proceedings of the 10th International Conference on Semantic Systems, SEMANTICS 2014, Leipzig, Germany, September 4-5, 2014</em>     </em>. ACM, 65&#x2013;72.</li>    <li id="BibPLXBIB0006" label="[6]">Dennis Diefenbach, Kuldeep Singh, Andreas Both, Didier Cherix, Christoph Lange, and S&#x00F6;ren Auer. 2017. The Qanary Ecosystem: Getting New Insights by Composing Question Answering Pipelines. In <em>      <em>Web Engineering - 17th International Conference, ICWE 2017, Rome, Italy, June 5-8, 2017, Proceedings</em>     </em>. Springer, 171&#x2013;189.</li>    <li id="BibPLXBIB0007" label="[7]">Milan Dojchinovski and Tomas Kliegr. 2013. Entityclassifier.eu: Real-time Classification of Entities in Text with Wikipedia. In <em>      <em>Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</em>     </em>(ECMLPKDD&#x2019;13). Springer-Verlag, 654&#x2013;658.</li>    <li id="BibPLXBIB0008" label="[8]">Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia entities). In <em>      <em>Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010</em>     </em>. 1625&#x2013;1628. <a class="link-inline force-break" href="https://doi.org/10.1145/1871437.1871689"      target="_blank">https://doi.org/10.1145/1871437.1871689</a></li>    <li id="BibPLXBIB0009" label="[9]">&#x00D3;scar Ferr&#x00E1;ndez, Christian Spurk, Milen Kouylekov, Iustin Dornescu, Sergio Ferr&#x00E1;ndez, Matteo Negri, Rub&#x00E9;n Izquierdo, David Tom&#x00E1;s, Constantin Orasan, Guenter Neumann, Bernardo Magnini, and Jos&#x00E9; Luis&#x00A0;Vicedo Gonz&#x00E1;lez. 2011. The QALL-ME Framework: A specifiable-domain multilingual Question Answering architecture. <em>      <em>J. Web Sem.</em>     </em>9, 2 (2011), 137&#x2013;145.</li>    <li id="BibPLXBIB0010" label="[10]">Jenny&#x00A0;Rose Finkel, Trond Grenager, and Christopher&#x00A0;D. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In <em>      <em>ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA</em>     </em>. The Association for Computer Linguistics, 363&#x2013;370.</li>    <li id="BibPLXBIB0011" label="[11]">Johannes Hoffart, Mohamed&#x00A0;Amir Yosef, Ilaria Bordino, Hagen F&#x00FC;rstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust Disambiguation of Named Entities in Text. In <em>      <em>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL</em>     </em>. 782&#x2013;792. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/D11-1072"      target="_blank">http://www.aclweb.org/anthology/D11-1072</a></li>    <li id="BibPLXBIB0012" label="[12]">Konrad H&#x00F6;ffner, Sebastian Walter, Edgard Marx, Ricardo Usbeck, Jens Lehmann, and Axel-Cyrille&#x00A0;Ngonga Ngomo. 2017. Survey on challenges of Question Answering in the Semantic Web. <em>      <em>Semantic Web</em>     </em> (2017).</li>    <li id="BibPLXBIB0013" label="[13]">Zhiheng Huang, Marcus Thint, and Asli &#x00C7;elikyilmaz. 2009. Investigation of Question Classifier in Question Answering. In <em>      <em>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP 2009, 6-7 August 2009, Singapore, A meeting of SIGDAT, a Special Interest Group of the ACL</em>     </em>.</li>    <li id="BibPLXBIB0014" label="[14]">Zhiheng Huang, Marcus Thint, and Zengchang Qin. 2008. Question Classification using Head Words and their Hypernyms. In <em>      <em>2008 Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008, Honolulu, Hawaii, USA, A meeting of SIGDAT, a Special Interest Group of the ACL</em>     </em>. ACL, 927&#x2013;936.</li>    <li id="BibPLXBIB0015" label="[15]">Jin-Dong Kim, Christina Unger, Axel-Cyrille&#x00A0;Ngonga Ngomo, Andr&#x00E9; Freitas, Young-gyun Hahm, Jiseong Kim, Sangha Nam, Gyu-Hyun Choi, Jeong-uk Kim, Ricardo Usbeck, and others. 2017. OKBQA Framework for collaboration on developing natural language question answering systems. (2017). <a class="link-inline force-break" href="http://sigir2017.okbqa.org/papers/OKBQA2017_paper_9.pdf"      target="_blank">http://sigir2017.okbqa.org/papers/OKBQA2017_paper_9.pdf</a></li>    <li id="BibPLXBIB0016" label="[16]">Atanas Kiryakov, Borislav Popov, Ivan Terziev, Dimitar Manov, and Damyan Ognyanoff. 2004. Semantic annotation, indexing, and retrieval. <em>      <em>J. Web Sem.</em>     </em> (2004), 49&#x2013;79. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.websem.2004.07.005"      target="_blank">https://doi.org/10.1016/j.websem.2004.07.005</a></li>    <li id="BibPLXBIB0017" label="[17]">Edgard Marx, Ricardo Usbeck, Axel-Cyrille&#x00A0;Ngonga Ngomo, Konrad H&#x00F6;ffner, Jens Lehmann, and S&#x00F6;ren Auer. 2014. Towards an open question answering architecture. In <em>      <em>Proceedings of the 10th International Conference on Semantic Systems, SEMANTICS 2014, Leipzig, Germany, September 4-5, 2014</em>     </em>. ACM, 57&#x2013;60. <a class="link-inline force-break" href="https://doi.org/10.1145/2660517.2660519"      target="_blank">https://doi.org/10.1145/2660517.2660519</a></li>    <li id="BibPLXBIB0018" label="[18]">Pablo&#x00A0;N. Mendes, Max Jakob, Andr&#x00E9;s Garc&#x00ED;a-Silva, and Christian Bizer. 2011. DBpedia spotlight: shedding light on the web of documents. In <em>      <em>Proceedings the 7th International Conference on Semantic Systems, I-SEMANTICS 2011, Graz, Austria, September 7-9, 2011</em>     </em>. ACM, 1&#x2013;8. <a class="link-inline force-break" href="https://doi.org/10.1145/2063518.2063519"      target="_blank">https://doi.org/10.1145/2063518.2063519</a></li>    <li id="BibPLXBIB0019" label="[19]">Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambiguation: a Unified Approach. <em>      <em>TACL</em>     </em>2(2014), 231&#x2013;244. <a class="link-inline force-break" href="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/291"      target="_blank">https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/291</a></li>    <li id="BibPLXBIB0020" label="[20]">Isaiah&#x00A0;Onando Mulang, Kuldeep Singh, and Fabrizio Orlandi. 2017. Matching Natural Language Relations to Knowledge Graph Properties for Question Answering. In <em>      <em>Semantics 2017</em>     </em>.</li>    <li id="BibPLXBIB0021" label="[21]">Jun-Ping Ng and Min-Yen Kan. 2015. QANUS: An Open-source Question-Answering Platform. <em>      <em>CoRR</em>     </em>abs/1501.00311(2015). <a class="link-inline force-break" href="http://arxiv.org/abs/1501.00311"      target="_blank">http://arxiv.org/abs/1501.00311</a></li>    <li id="BibPLXBIB0022" label="[22]">Muhammad Saleem, Samaneh&#x00A0;Nazari Dastjerdi, Ricardo Usbeck, and Axel-Cyrille&#x00A0;Ngonga Ngomo. 2017. Question Answering Over Linked Data: What is Difficult to Answer? What Affects the F scores?. In <em>      <em>Joint Proceedings of BLINK2017: 2nd International Workshop on Benchmarking Linked Data and NLIWoD3: Natural Language Interfaces for the Web of Data co-located with 16th International Semantic Web Conference (ISWC 2017), Vienna, Austria, October 21st - to - 22nd, 2017.</em>     </em>CEUR-WS.org. <a class="link-inline force-break" href="http://ceur-ws.org/Vol-1932/paper-02.pdf"      target="_blank">http://ceur-ws.org/Vol-1932/paper-02.pdf</a></li>    <li id="BibPLXBIB0023" label="[23]">Saeedeh Shekarpour, Edgard Marx, Axel-Cyrille&#x00A0;Ngonga Ngomo, and S&#x00F6;ren Auer. 2015. SINA: Semantic interpretation of user queries for question answering on interlinked data. <em>      <em>J. Web Sem.</em>     </em>30(2015), 39&#x2013;51. <a class="link-inline force-break"      href="https://doi.org/10.1016/j.websem.2014.06.002"      target="_blank">https://doi.org/10.1016/j.websem.2014.06.002</a></li>    <li id="BibPLXBIB0024" label="[24]">Kuldeep Singh, Andreas Both, Dennis Diefenbach, and Saeedeh Shekarpour. 2016. Towards a Message-Driven Vocabulary for Promoting the Interoperability of Question Answering Systems. In <em>      <em>Tenth IEEE International Conference on Semantic Computing, ICSC 2016, Laguna Hills, CA, USA, February 4-6, 2016</em>     </em>. IEEE Computer Society, 386&#x2013;389. <a class="link-inline force-break" href="https://doi.org/10.1109/ICSC.2016.59"      target="_blank">https://doi.org/10.1109/ICSC.2016.59</a></li>    <li id="BibPLXBIB0025" label="[25]">Kuldeep Singh, Andreas Both, Dennis Diefenbach, Saeedeh Shekarpour, Didier Cherix, and Christoph Lange. 2016. Qanary - The Fast Track to Creating a Question Answering System with Linked Data Technology. In <em>      <em>The Semantic Web - ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 - June 2, 2016, Revised Selected Papers</em>     </em>. 183&#x2013;188. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-47602-5_36"      target="_blank">https://doi.org/10.1007/978-3-319-47602-5_36</a></li>    <li id="BibPLXBIB0026" label="[26]">Kuldeep Singh, Ioanna Lytra, Maria-Esther Vidal, Dharmen Punjani, Harsh Thakkar, Christoph Lange, and S&#x00F6;ren Auer. 2017. QAestro - Semantic-Based Composition of Question Answering Pipelines. In <em>      <em>Database and Expert Systems Applications - 28th International Conference, DEXA 2017, Lyon, France, August 28-31, 2017, Proceedings, Part I</em>     </em>. Springer, 19&#x2013;34. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-64468-4_2"      target="_blank">https://doi.org/10.1007/978-3-319-64468-4_2</a></li>    <li id="BibPLXBIB0027" label="[27]">Kuldeep Singh, Isaiah&#x00A0;Onando Mulang, Ioanna Lytra, Mohamad&#x00A0;Yaser Jaradeh, Ahmad Sakor, Maria-Esther Vidal, Christoph Lange, and S&#x00F6;ren Auer. 2017. Capturing Knowledge in Semantically-typed Relational Patterns to Enhance Relation Linking. In <em>      <em>Proceedings of the Knowledge Capture Conference, K-CAP 2017, Austin, TX, USA, December 4-6, 2017</em>     </em>. 31:1&#x2013;31:8. <a class="link-inline force-break" href="https://doi.org/10.1145/3148011.3148031"      target="_blank">https://doi.org/10.1145/3148011.3148031</a></li>    <li id="BibPLXBIB0028" label="[28]">Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017. LC-QuAD: A Corpus for Complex Question Answering over Knowledge Graphs. In <em>      <em>The Semantic Web - ISWC 2017 - 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part II</em>     </em>. Springer, 210&#x2013;218. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-68204-4_22"      target="_blank">https://doi.org/10.1007/978-3-319-68204-4_22</a></li>    <li id="BibPLXBIB0029" label="[29]">Christina Unger, Lorenz B&#x00FC;hmann, Jens Lehmann, Axel-Cyrille&#x00A0;Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. In <em>      <em>Proceedings of the 21st World Wide Web Conference 2012, WWW 2012, Lyon, France, April 16-20, 2012</em>     </em>. ACM, 639&#x2013;648. <a class="link-inline force-break" href="https://doi.org/10.1145/2187836.2187923"      target="_blank">https://doi.org/10.1145/2187836.2187923</a></li>    <li id="BibPLXBIB0030" label="[30]">Christina Unger, Corina Forascu, Vanessa L&#x00F3;pez, Axel-Cyrille&#x00A0;Ngonga Ngomo, Elena Cabrio, Philipp Cimiano, and Sebastian Walter. 2015. Question Answering over Linked Data (QALD-5). In <em>      <em>Working Notes of CLEF 2015 - Conference and Labs of the Evaluation forum, Toulouse, France, September 8-11, 2015.</em>     </em>CEUR-WS.org. <a class="link-inline force-break" href="http://ceur-ws.org/Vol-1391/173-CR.pdf"      target="_blank">http://ceur-ws.org/Vol-1391/173-CR.pdf</a></li>    <li id="BibPLXBIB0031" label="[31]">Ricardo Usbeck, Michael Hoffmann, Michael R&#x00F6;der, Jens Lehmann, and Axel-Cyrille&#x00A0;Ngonga Ngomo. 2017. Using Multi-Label Classification for Improved Question Answering. <em>      <em>CoRR (submitted)</em>     </em> (2017). <a class="link-inline force-break" href="https://arxiv.org/abs/1710.08634"      target="_blank">https://arxiv.org/abs/1710.08634</a></li>    <li id="BibPLXBIB0032" label="[32]">Ricardo Usbeck, Axel-Cyrille&#x00A0;Ngonga Ngomo, Michael R&#x00F6;der, Daniel Gerber, Sandro&#x00A0;Athaide Coelho, S&#x00F6;ren Auer, and Andreas Both. 2014. AGDISTIS - Graph-Based Disambiguation of Named Entities Using Linked Data. In <em>      <em>The Semantic Web - ISWC 2014</em>     </em>. Springer, 457&#x2013;471. <a class="link-inline force-break"      href="https://doi.org/10.1007/978-3-319-11964-9_29"      target="_blank">https://doi.org/10.1007/978-3-319-11964-9_29</a></li>    <li id="BibPLXBIB0033" label="[33]">Ricardo Usbeck, Michael R&#x00F6;der, Michael Hoffmann, Felix Conrads, Jonathan Huthmann, Axel-Cyrille&#x00A0;Ngonga Ngomo, Christian Demmler, and Christina Unger. Benchmarking Question Answering Systems. <em>      <em>Semantic Web Journal (to be published)</em>     </em>(????). <a class="link-inline force-break" href="http://www.semantic-web-journal.net/content/benchmarking-question-answering-systems"      target="_blank">http://www.semantic-web-journal.net/content/benchmarking-question-answering-systems</a></li>    <li id="BibPLXBIB0034" label="[34]">Zi Yang, Elmer Gardu&#x00F1;o, Yan Fang, Avner Maiberg, Collin McCormack, and Eric Nyberg. 2013. Building optimal information systems automatically: configuration space exploration for biomedical information systems. In <em>      <em>22nd ACM International Conference on Information and Knowledge Management, CIKM&#x2019;13, San Francisco, USA</em>     </em>. ACM, 1421&#x2013;1430.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break"    href="https://qald.sebastianwalter.org/index.php?x=home&#x0026;q=5">https://qald.sebastianwalter.org/index.php?x=home&#x0026;q=5</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://lc-quad.sda.tech/">http://lc-quad.sda.tech/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="http://docs.aylien.com/docs/introduction">http://docs.aylien.com/docs/introduction</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Component is based on <a class="link-inline force-break" href="https://github.com/dice-group/NLIWOD">https://github.com/dice-group/NLIWOD</a> and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0029">29</a>].</p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>The prefix <tt>dbr</tt> is bound to <tt><a class="link-inline force-break" href="http://dbpedia.org/resource/"      target="_blank">http://dbpedia.org/resource/</a></tt>.</p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>The prefix <tt>dbo</tt> is bound to <tt><a class="link-inline force-break" href="http://dbpedia.org/ontology/"      target="_blank">http://dbpedia.org/ontology/</a></tt>.</p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break"    href="https://dandelion.eu/docs/api/datatxt/nex/getting-started/">https://dandelion.eu/docs/api/datatxt/nex/getting-started/</a>   </p>   <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>Component based on <a class="link-inline force-break" href="https://github.com/dice-group/NLIWOD">https://github.com/dice-group/NLIWOD</a>. </p>   <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a>This component is the combination of the NLIWOD and RL components of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0015">15</a>].</p>   <p id="fn10"><a href="#foot-fn10"><sup>10</sup></a><a class="link-inline force-break"    href="http://dbpedia.org/ontology/ComicsCharacter">http://dbpedia.org/ontology/ComicsCharacter</a>   </p>   <p id="fn11"><a href="#foot-fn11"><sup>11</sup></a><a class="link-inline force-break" href="https://github.com/WDAqua/Frankenstein">https://github.com/WDAqua/Frankenstein</a>   </p>   <p id="fn12"><a href="#foot-fn12"><sup>12</sup></a><a class="link-inline force-break" href="http://lc-quad.sda.tech/">http://lc-quad.sda.tech/</a>   </p>   <p id="fn13"><a href="#foot-fn13"><sup>13</sup></a><a class="link-inline force-break"    href="https://qald.sebastianwalter.org/index.php?x=home&#x0026;q=5">https://qald.sebastianwalter.org/index.php?x=home&#x0026;q=5</a>   </p>   <p id="fn14"><a href="#foot-fn14"><sup>14</sup></a><a class="link-inline force-break" href="https://dbpedia.org/sparql">https://dbpedia.org/sparql</a>   </p>   <p id="fn15"><a href="#foot-fn15"><sup>15</sup></a><a class="link-inline force-break" href="http://nlp.stanford.edu:8080/parser/">http://nlp.stanford.edu:8080/parser/</a>   </p>   <p id="fn16"><a href="#foot-fn16"><sup>16</sup></a><a class="link-inline force-break" href="https://github.com/WDAqua/Frankenstein">https://github.com/WDAqua/Frankenstein</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186023">https://doi.org/10.1145/3178876.3186023</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
