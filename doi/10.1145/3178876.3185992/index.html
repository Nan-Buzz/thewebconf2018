<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Detecting Absurd Conversations from Intelligent Assistant
  Logs by Exploiting User Feedback Utterances</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3185992'>https://doi.org/10.1145/3178876.3185992</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3185992'>https://w3id.org/oa/10.1145/3178876.3185992</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Detecting Absurd Conversations
          from Intelligent Assistant Logs by Exploiting User
          Feedback Utterances</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Chikara</span> <span class=
          "surName">Hashimoto</span>, Yahoo Japan Corporation,
          Tokyo, Japan, <a href=
          "mailto:chashimo@yahoo-corp.jp">chashimo@yahoo-corp.jp</a>
        </div>
        <div class="author">
          <span class="givenName">Manabu</span> <span class=
          "surName">Sassano</span>, Yahoo Japan Corporation, Tokyo,
          Japan, <a href=
          "mailto:msassano@yahoo-corp.jp">msassano@yahoo-corp.jp</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3185992"
        target=
        "_blank">https://doi.org/10.1145/3178876.3185992</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Intelligent assistants, such as Siri, are
        expected to converse comprehensibly with users. To
        facilitate improvement of their conversational ability, we
        have developed a method that detects absurd conversations
        recorded in intelligent assistant logs by identifying user
        feedback utterances that indicate users’ favorable and
        unfavorable evaluations of intelligent assistant responses;
        e.g., <em>“great!”</em> is favorable, whereas <em>“what are
        you talking about?”</em> is unfavorable. Assuming that
        absurd/comprehensible conversations tend to be followed by
        unfavorable/favorable utterances, our method extracts some
        absurd/comprehensible conversations from the log to train a
        conversation classifier that sorts all the conversations
        recorded in the log as either absurd or not. The challenge
        is that user feedback utterances are often ambiguous; e.g.,
        a user may give an unfavorable utterance (e.g., <em>“don't
        be silly!”</em>) to a comprehensible conversation in which
        the intelligent assistant was attempting to make a joke. An
        utterance classifier is thus used to score the feedback
        utterances in accordance with how unambiguously they
        indicate absurdity. Experiments showed that our method
        significantly outperformed methods that lacked a
        conversation and/or utterance classifier, indicating the
        effectiveness of the two classifiers. Our method only
        requires user feedback utterances, which would be
        independent of domains. Experiments focused on <font style=
        "font-variant: small-caps">chitchat</font>, <font style=
        "font-variant: small-caps">web search</font>, and
        <font style="font-variant: small-caps">weather</font>
        domains indicated that our method is likely
        domain-independent.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Computing methodologies</strong>
        → <strong>Information extraction;</strong>
        <strong>Discourse, dialogue and pragmatics;</strong> •
        <strong>Applied computing</strong> → Document
        analysis;</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Chikara Hashimoto and Manabu Sassano. 2018. Detecting
          Absurd Conversations from Intelligent Assistant Logs by
          Exploiting User Feedback Utterances. In <em>WWW 2018: The
          2018 Web Conference,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 10 Pages.
          <a href="https://doi.org/10.1145/3178876.3185992" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3185992</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Intelligent assistants (also known as virtual assistants),
      such as Apple's Siri, have become popular, partly because
      their natural-language user interfaces simplify access to an
      enormous amount of information on the web. The key to
      satisfying users is ensuring that such intelligent assistants
      converse comprehensibly. To continuously improve intelligent
      assistant conversational ability, it is important to examine
      a vast amount of data in a log database in which
      conversations between users and intelligent assistants have
      been recorded in order to detect absurd conversations that
      need to be fixed. Automation of this laborious work would
      facilitate improvement of intelligent assistant
      conversational ability.</p>
      <p>In this paper, we propose a method for detecting absurd
      conversations in intelligent assistant logs. The basic idea
      is simple: absurd conversations tend to be followed by an
      unfavorable utterance by the user (e.g., <em>“What are you
      talking about?”</em>) and tend not to be followed by a
      favorable utterance (e.g., <em>“Great!”</em>). Table
      <a class="tbl" href="#tab1">1</a> shows two conversations
      between a user and an intelligent assistant. The first
      conversation was followed by the favorable utterance <em>“I
      see, thanks,”</em> indicating that it was likely
      comprehensible. The second one was followed by the
      unfavorable utterance <em>“What do you mean?”</em>,
      indicating that it was likely absurd.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Example conversations between user and
          intelligent assistant (IA).First conversation was
          followed by favorable utterance, indicating that it was
          likely comprehensible. Second conversation was followed
          by unfavorable utterance, indicating that it was likely
          absurd.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:left;">
              <strong>User</strong></td>
              <td style="text-align:left;"><em>From Osaka to
              Tokyo.</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>IA</strong></td>
              <td style="text-align:left;"><em>Bullet train
              departing Osaka at 9:00 is available.</em></td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <strong>User</strong></td>
              <td style="text-align:left;"><em>I see, thanks.
              &nbsp;&nbsp;&nbsp;</em>
              <strong><em>(Favorable)</em></strong></td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <strong>User</strong></td>
              <td style="text-align:left;"><em>I think I caught a
              cold.</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>IA</strong></td>
              <td style="text-align:left;"><em>Ha-ha.</em></td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <strong>User</strong></td>
              <td style="text-align:left;"><em>What do you mean?
              &nbsp;&nbsp;&nbsp;</em>
              <strong><em>(Unfavorable)</em></strong></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p><em>Favorable</em> and <em>unfavorable</em> in this study
      are used in a broad sense, as shown in Table <a class="tbl"
      href="#tab2">2</a>. Favorable utterances include those
      indicating, for example, admiration, acceptance,
      comprehension, appreciation, and amusement. Unfavorable ones
      typically indicate disappointment, miscommunication,
      incomprehensibility, contempt, and boredom. We collectively
      call them <em>feedback utterances</em>.</p>
      <div class="table-responsive" id="tab2">
        <div class="table-caption">
          <span class="table-number">Table 2:</span> <span class=
          "table-title">Examples of favorable and unfavorable
          utterances.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;"><em>Favorable</em></th>
              <th style="text-align:center;">Unfavorable</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;"><em>You are
              smart.</em></td>
              <td style="text-align:left;"><em>This is
              useless.</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>Thanks, I got
              it.</em></td>
              <td style="text-align:left;"><em>It doesn't make
              sense.</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>It was a lot of
              fun.</em></td>
              <td style="text-align:left;"><em>I'm gonna uninstall
              you, bye.</em></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>A simple method of detecting absurd conversations would be
      to match unfavorable feedback utterances against the log
      entries and extract the immediately preceding conversations
      (e.g., retrieving the second conversation in Table <a class=
      "tbl" href="#tab1">1</a>). However, this simple method would
      face the following two difficulties.</p>
      <ul class="list-no-style">
        <li id="uid3" label="Ambiguity:">As is often the case with
        natural languages, feedback utterances are ambiguous as to
        which aspect of the intelligent assistant's response the
        user gave a favorable or unfavorable evaluation. That is,
        the user may give feedback to not only the overall
        conversation but also, for example, to the way the
        intelligent assistant responded (politely or rudely), its
        speech recognition ability, and the contents (sophisticated
        or commonplace). For example, a user may give unfavorable
        feedback (e.g., <em>“Don't be silly!”</em>) to a
        comprehensible response from an intelligent assistant that
        attempted to make a joke. Furthermore, some users curse at
        an intelligent assistant for no particular reason on a
        whim. In contrast, a user may say <em>“thank you”</em>
        after an absurd response that happens to be comforting,
        such as <em>“you are wonderful just the way you are”</em>,
        even though the response makes no sense in the
        conversation. In other words, feedback utterances are not
        always indicative of the absurdity of a
        conversation.<br /></li>
        <li id="uid4" label="Infrequency:">Users give feedback
        utterances only occasionally; hence, they are infrequent in
        the log (about 12% of all user utterances in the log we
        used in our experiments). Since a significant portion of
        the absurd conversations may not be followed by a feedback
        utterance, and simply relying on feedback utterances in the
        log could result in low recall.<br /></li>
      </ul>
      <p>Regarding the ambiguity problem, we observed that some
      feedback utterances indicate absurdity or comprehensibility
      more clearly and unambiguously (e.g., <em>“Your response
      doesn't make sense.”</em> and <em>“I see, thanks.”</em>) than
      others. Therefore, an <em>utterance classifier</em> is thus
      used to score the feedback utterances in accordance with how
      unambiguously they indicate absurdity. To train the utterance
      classifier, we prepare labeled data by collecting
      conversations that were followed by a feedback utterance
      (e.g., the conversations in Table <a class="tbl" href=
      "#tab1">1</a>) and labeling the conversations as absurd or
      comprehensible. From these data, we can learn the strength of
      association between feedback utterances and absurd
      conversations, enabling identification of those utterances
      that tend to unambiguously indicate absurdity.</p>
      <p>Regarding the infrequency problem, absurd and
      comprehensible conversation samples that are followed by a
      favorable or unfavorable feedback utterances are retrieved.
      These data are used to train a <em>conversation
      classifier</em>, which sorts conversations, regardless of
      whether they are followed by a feedback utterance, into
      absurd or comprehensible. In other words, in our method,
      absurd and comprehensible conversation samples that are
      followed by a feedback utterance are not the final detection
      results but rather are used as labeled data to train the
      conversation classifier, which can determine the absurdity of
      <em>any</em> conversation even if it is not followed by a
      feedback utterance. Even if feedback utterances appear
      infrequently in the log, a large volume of conversation
      samples can be automatically acquired if the log is large
      enough. Note that training the conversation classifier
      requires no manual labor once the feedback utterances are
      obtained and the log is prepared. Thus, application of this
      method to a new domain requires only that a log be prepared
      for the target domain since the feedback utterances are
      domain-independent in most cases.</p>
      <p>We evaluated our basic method and two simplified versions,
      one without the utterance classifier and the other without
      the conversation classifier, using five-years’ worth of log
      data for an actual intelligent assistant. The two simplified
      versions performed much worse than the basic version,
      indicating that the two classifiers greatly contribute to the
      performance of our method.</p>
      <p>We also compared our method with two baseline methods. One
      is based on majority voting: whether a conversation is absurd
      is determined by the number of favorable and unfavorable
      feedback utterances that follow the conversation in the log.
      The other is a simple supervised method: the absurdity of a
      conversation is determined by a supervised classifier trained
      using manually created conversation data. Obviously, the
      supervised method requires manual labor for preparing the
      labeled data for each new domain. Our method substantially
      outperformed the majority voting method, indicating that
      simply matching feedback utterances in a log in order to
      detect absurd conversations works poorly. Our method also
      outperformed the simple supervised method due to the large
      volume of automatically acquired training samples.</p>
      <p>We evaluated the domain-independence of our method for the
      three most frequently used domains: <font style=
      "font-variant: small-caps">chitchat</font>, <font style=
      "font-variant: small-caps">web search</font>, and
      <font style="font-variant: small-caps">weather</font>. We
      first prepared out-of-domain models that were trained without
      using training samples for the target domain and were unable
      to use the feedback utterances that appeared only in the
      target domain. Then we compared the performance of the
      out-of-domain and in-domain models using two test sets. There
      were no statistically significant difference between the
      models in five of the six settings (three domains × two test
      sets), indicating that our method is likely
      domain-independent.</p>
      <p>The contribution of this paper is two-fold: (1)&nbsp;We
      present a novel method for detecting absurd conversations
      that exploits user feedback utterances. (2)&nbsp;We
      empirically demonstrated the effectiveness of our method
      using a large volume of actual log data.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <p>The research most related to ours is on how to evaluate an
      intelligent assistant. Quality metrics can be divided into
      component metrics and end-to-end metrics&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>]. Our method can be seen
      as an online end-to-end metric, which is further categorized
      as one based on reference responses&nbsp; [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0003">3</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0014">14</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>] or one using external
      knowledge&nbsp; [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>]. Reference-based methods typically
      use the BLEU method&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>], which was originally used for the
      automatic evaluation of machine-translation quality and
      basically measures word overlap between an intelligent
      assistant's conversations and reference conversations&nbsp;
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0012">12</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0013">13</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>]. Another
      reference-based method learns to compare reference
      conversations to an intelligent assistant's
      conversations&nbsp; [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>]. Reference conversations tend to
      depend on the target domain since the content of reference
      conversations includes phrases and wording that are domain
      specific. This means that reference conversations have to be
      prepared for each domain. On the other hand, user feedback
      utterances are generally domain-independent.</p>
      <p>With methods using external knowledge, it is assumed that
      some signals outside the conversation can help identify the
      absurdity of conversations. Such signals include comments
      describing the cause of conversational breakdown that are
      collected from human annotators&nbsp; [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>], manually extracted
      cues&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0011">11</a>]
      and features [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>], and positive-reward signals given
      to correct answers&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>]. However, such signals are usually
      expensive to obtain. User feedback utterances, on the other
      hand, are likely to occur naturally in conversations with
      intelligent assistants and are easier to obtain.</p>
      <p>Other research directions involving the evaluation of
      intelligent assistants include predicting user satisfaction
      with an intelligent assistant. Jiang et al.&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>] proposed a
      method for predicting user satisfaction session-wise. The
      method proposed by Sano et al.&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0019">19</a>] predicts user
      satisfaction user-wise, i.e., by examining multiple sessions
      of individual users. Our method evaluates intelligent
      assistants conversation-wise, which is more useful for
      detecting absurd conversations, since it would be difficult
      to pinpoint absurd conversations with session- and user-wise
      methods. Schmitt et al.&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>] proposed a method for predicting
      interaction quality, an objective measure of user
      satisfaction, at arbitrary points in a conversation. Since
      user satisfaction and interaction quality are affected by not
      only the absurdity of the conversation but also various
      factors like usability and automatic speech recognition
      performance&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>], we think that methods tailored to
      user satisfaction prediction are less suitable for absurd
      conversation detection than our method.</p>
      <p>Some studies have used confirmations from users like
      <em>“yes”</em> and <em>“nope”</em>, which are a kind of user
      feedback utterances&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>]. Our method uses a wider variety of
      user feedback utterances like <em>“I'm gonna uninstall you,
      bye.”</em> More importantly, we are the first to address the
      ambiguity and infrequency problems inherent to user feedback
      utterances, as far as we are aware of.</p>
      <p>Reformulation is another kind of feedback utterance that
      users typically use to correct intelligent assistant speech
      recognition errors&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>], which is outside the scope of this
      paper.</p>
      <p>The use of feedback utterances to automatically acquire
      the labeled data for the conversation classifier in our
      method was inspired by distant supervision&nbsp; [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>]. In
      distant supervision, a classifier is trained using
      automatically collected, weakly labeled samples. To the best
      of our knowledge, we are the first to apply distant
      supervision to detecting absurd conversations recorded in
      intelligent assistant logs.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Proposed
          Method</h2>
        </div>
      </header>
      <p>Figure&nbsp;<a class="fig" href="#fig1">1</a> gives an
      overview of our method. In Phase&nbsp;1, a
      <em>scored-feedback-utterance database (DB)</em> is
      constructed. It contains favorable and unfavorable feedback
      utterances that are scored by the utterance classifier in
      accordance with how unambiguously they indicate absurdity.
      Feedback utterances to be scored and training data for the
      utterance classifier are acquired from the log. In
      Phase&nbsp;2, absurd conversations recorded in the log are
      detected using the conversation classifier, which is trained
      using absurd and comprehensible conversation samples
      retrieved from the log by exploiting the
      scored-feedback-utterance DB constructed in the first
      phase.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3185992/images/www2018-1-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Overview of our method.</span>
        </div>
      </figure>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Intelligent
            Assistant Log</h3>
          </div>
        </header>
        <p>The intelligent assistant log used in this study was
        that of a Japanese commercial intelligent assistant, Yahoo!
        Voice Assist<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a> from April 2012 to May 2017.
        Yahoo! Voice Assist covers a wide variety of domains,
        including web search, weather forecasts, chitchat,
        fortune-telling, sports, and cooking as well as controlling
        alarms, schedulers, and other applications on smartphones
        and tablets. Table&nbsp;<a class="tbl" href="#tab3">3</a>
        shows example conversations (web search, weather forecast,
        and chitchat) between Yahoo! Voice Assist and a user.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Examples of conversations between Yahoo!
            Voice Assist (IA) and its users.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">
                <strong>User</strong></td>
                <td style="text-align:left;"><em>Search the web for
                Tokyo Skytree.</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>IA</strong></td>
                <td style="text-align:left;"><em>Here is the result
                of web search for Tokyo Skytree.</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>User</strong></td>
                <td style="text-align:left;"><em>Will it rain
                tomorrow?</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>IA</strong></td>
                <td style="text-align:left;"><em>The chance of rain
                tomorrow is 10%.</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>User</strong></td>
                <td style="text-align:left;"><em>I don't feel like
                going to work today.</em></td>
              </tr>
              <tr>
                <td style="text-align:left;">
                <strong>IA</strong></td>
                <td style="text-align:left;"><em>I know how you
                feel.</em></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>We organized the log into sessions, each containing
        conversations with a single user and not containing two
        adjacent utterances with an interval exceeding 30 minutes,
        in accordance with [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>].</p>
        <p>Although we used a log of Japanese conversations to
        develop our method, our method is applicable to other
        languages as well since feedback utterances such as
        <em>“your response doesn't make sense.”</em> are common in
        many languages. The examples were translated into English
        throughout the paper for illustrative purposes.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Collecting
            Feedback Utterances</h3>
          </div>
        </header>
        <p>We first collect feedback utterances, which can be done
        either manually or automatically. We crowdsourced the
        labeling, i.e., <em>favorable</em>, <em>unfavorable</em>,
        and <em>neutral</em>,<a class="fn" href="#fn2" id=
        "foot-fn2"><sup>2</sup></a> of 39,435 utterances in the log
        by using Yahoo!&nbsp;Crowdsourcing.<a class="fn" href=
        "#fn3" id="foot-fn3"><sup>3</sup></a> About 20,000 of the
        utterances were randomly sampled from the log, while the
        remainder were feedback utterance candidates collected in
        our preliminary experiments. The utterance length was
        restricted to less than 30 characters.</p>
        <p>Each utterance was labeled by three crowd workers, with
        the final decision by majority vote. Each utterance was
        presented to workers without context. The number of workers
        was 780.<a class="fn" href="#fn4" id=
        "foot-fn4"><sup>4</sup></a> The same label was given by the
        trio of workers to 69.8% of the utterances while only 1.5%
        were given three different labels, indicating that this
        labeling task tends to be stable across workers. We
        discarded those utterances that were given three different
        labels.</p>
        <p>From the crowdsourced labeling results for the 20,000
        randomly sampled utterances, we estimated that about 5% and
        7% of all the utterances in the log were favorable and
        unfavorable, respectively.</p>
        <p>We then augmented the crowdsourced labeling result with
        additional feedback utterances (4,066 favorable and 2,434
        unfavorable) that we labeled ourselves. Finally, one of the
        authors checked the augmented labeling result and obtained
        20,304 feedback utterances (8,988 favorable and 11,316
        unfavorable). We did not include neutral utterances in the
        feedback-utterance DB.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Training
            Utterance Classifier</h3>
          </div>
        </header>
        <p>The utterance classifier takes a feedback utterance as
        input and outputs a confidence score that the preceding
        conversation is absurd. We expect that feedback utterances
        that tend to unambiguously indicate
        absurdity/comprehensibility are given large/small values
        and ambiguous ones are given middle values.</p>
        <section id="sec-9">
          <p><em>3.3.1 Labeled Data.</em> To train the classifier,
          we prepared labeled data as follows. First, we sampled
          from the log 38,183 ⟨<em>u</em>, <em>r</em>⟩ pairs
          consisting of a user's utterance (<em>u</em>) and the
          intelligent assistant's response (<em>r</em>). The
          <em>u</em> and <em>r</em> were consecutive in a session,
          and the number of characters was restricted to less than
          30 and 150, respectively. We then crowdsourced the
          annotation of ⟨<em>u</em>, <em>r</em>⟩s with labels
          <em>absurd</em> and <em>comprehensible</em> to
          Yahoo!&nbsp;Crowdsourcing. The number of crowd workers
          was 1,071.<a class="fn" href="#fn5" id=
          "foot-fn5"><sup>5</sup></a> Since the absurdity judgment
          of some conversations requires broader contexts, we
          allowed the workers to label such ⟨<em>u</em>,
          <em>r</em>⟩s with <em>uncertain</em>. Each ⟨<em>u</em>,
          <em>r</em>⟩ was labeled by three crowd workers. 55.7% of
          the conversations were given the same label by the trio
          of workers, while only 6.4% were given three different
          labels, which indicates that this labeling task is
          reasonably stable across workers. The label was
          determined to be <em>absurd</em> if more than one worker
          labeled it with <em>absurd</em> and
          <em>comprehensible</em> if all three workers labeled it
          with <em>comprehensible</em>. We discarded the other
          cases. We thereby obtained 22,394 labeled ⟨<em>u</em>,
          <em>r</em>⟩s among which 8,484 (38%) were <em>absurd</em>
          and 13,910 (62%) were <em>comprehensible</em>. We split
          the labeled pairs ⟨<em>u</em>, <em>r</em>, <em>l</em>⟩s
          (<em>l</em> is the label for ⟨<em>u</em>, <em>r</em>⟩)
          into three parts, as shown in Table&nbsp;<a class="tbl"
          href="#tab4">4</a>.</p>
          <div class="table-responsive" id="tab4">
            <div class="table-caption">
              <span class="table-number">Table 4:</span>
              <span class="table-title">Datasets of ⟨<em>u</em>,
              <em>r</em>, <em>l</em>⟩s.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"></th>
                  <th style="text-align:center;">Set 1</th>
                  <th style="text-align:center;">Set 2</th>
                  <th style="text-align:center;">Set 3</th>
                  <th style="text-align:center;">Total</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;"><em>Absurd</em></td>
                  <td style="text-align:right;">4,787</td>
                  <td style="text-align:right;">1,870</td>
                  <td style="text-align:right;">1,827</td>
                  <td style="text-align:right;">8,484</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <em>Comprehensible</em></td>
                  <td style="text-align:right;">7,746</td>
                  <td style="text-align:right;">3,056</td>
                  <td style="text-align:right;">3,108</td>
                  <td style="text-align:right;">13,910</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Total</td>
                  <td style="text-align:right;">12,533</td>
                  <td style="text-align:right;">4,926</td>
                  <td style="text-align:right;">4,935</td>
                  <td style="text-align:right;">22,394</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>From the sessions in the log, we then retrieved
          feedback utterances for each ⟨<em>u</em>, <em>r</em>,
          <em>l</em>⟩ to prepare tuples ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s consisting of a user's
          utterance&nbsp;(<em>u</em>), the intelligent assistant's
          response&nbsp;(<em>r</em>), the label&nbsp;(<em>l</em>)
          for the ⟨<em>u</em>, <em>r</em>⟩, and the user's
          feedback&nbsp;(<em>f</em>) that followed the <em>r</em>.
          These feedback utterances were those described in Section
          <a class="sec" href="#sec-7">3.2</a>. See
          Table&nbsp;<a class="tbl" href="#tab5">5</a> for examples
          of ⟨<em>u</em>, <em>r</em>, <em>l</em>, <em>f</em>⟩s. As
          a result, we obtained 299,163 ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s; the breakdown is shown in
          Table&nbsp;<a class="tbl" href="#tab6">6</a>. Note that
          the numbers are larger than those in Table&nbsp;<a class=
          "tbl" href="#tab4">4</a> since some ⟨<em>u</em>,
          <em>r</em>, <em>l</em>⟩s were followed by more than one
          feedback utterance (<em>f</em>) in the log.</p>
          <div class="table-responsive" id="tab5">
            <div class="table-caption">
              <span class="table-number">Table 5:</span>
              <span class="table-title">Examples of ⟨<em>u</em>,
              <em>r</em>, <em>l</em>, <em>f</em>⟩s.</span>
            </div>
            <table class="table">
              <tbody>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>u</em></strong></td>
                  <td style="text-align:left;"><em>From Osaka to
                  Tokyo.</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>r</em></strong></td>
                  <td style="text-align:left;"><em>Bullet train
                  departing Osaka at 9:00 is available.</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>l</em></strong></td>
                  <td style="text-align:left;">
                  <em>comprehensible</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>f</em></strong></td>
                  <td style="text-align:left;"><em>I see, thanks.
                  &nbsp;&nbsp;&nbsp;</em>
                  <strong><em>(Favorable)</em></strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>u</em></strong></td>
                  <td style="text-align:left;"><em>I think I caught
                  a cold.</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>r</em></strong></td>
                  <td style="text-align:left;"><em>Ha-ha.</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>l</em></strong></td>
                  <td style="text-align:left;"><em>absurd</em></td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <strong><em>f</em></strong></td>
                  <td style="text-align:left;"><em>What do you
                  mean? &nbsp;&nbsp;&nbsp;</em>
                  <strong><em>(Unfavorable)</em></strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab6">
            <div class="table-caption">
              <span class="table-number">Table 6:</span>
              <span class="table-title">Datasets of ⟨<em>u</em>,
              <em>r</em>, <em>l</em>, <em>f</em>⟩s. Note that the
              numbers are larger than those in Table <a class="tbl"
              href="#tab4">4</a>, since some ⟨<em>u</em>,
              <em>r</em>, <em>l</em>⟩s were followed by more than
              one feedback utterance (<em>f</em>) in the
              log.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"></th>
                  <th style="text-align:center;">Set 1</th>
                  <th style="text-align:center;">Set 2</th>
                  <th style="text-align:center;">Set 3</th>
                  <th style="text-align:center;">Total</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;"><em>Absurd</em></td>
                  <td style="text-align:right;">77,366</td>
                  <td style="text-align:right;">7,907</td>
                  <td style="text-align:right;">6,677</td>
                  <td style="text-align:right;">91,950</td>
                </tr>
                <tr>
                  <td style="text-align:left;">
                  <em>Comprehensible</em></td>
                  <td style="text-align:right;">167,826</td>
                  <td style="text-align:right;">19,505</td>
                  <td style="text-align:right;">19,882</td>
                  <td style="text-align:right;">207,213</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Total</td>
                  <td style="text-align:right;">245,192</td>
                  <td style="text-align:right;">27,412</td>
                  <td style="text-align:right;">26,559</td>
                  <td style="text-align:right;">299,163</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-10">
          <p><em>3.3.2 Model.</em> We use fastText<a class="fn"
          href="#fn6" id="foot-fn6"><sup>6</sup></a> with
          pre-trained word vectors for the utterance classifier due
          to its speed and accuracy&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0001">1</a>,
          <a class="bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0009">9</a>]. All
          hyper-parameters are set to fastText's default values.
          The word vectors were trained on Japanese Wikipedia
          articles (as of June 2017) using fastText's skip-gram
          with all hyper-parameters set to their default values
          (e.g., the dimension was set to 100). The input to the
          utterance classifier is an utterance tokenized using the
          MeCab morphological analyzer.<a class="fn" href="#fn7"
          id="foot-fn7"><sup>7</sup></a> The output is a label,
          <em>absurd</em> or <em>comprehensible</em>, and a
          confidence score associated with the label. The
          confidence scores <em>s</em> associated with the
          <em>comprehensible</em> label are converted to −
          <em>s</em> so that the scores all indicate absurdity; the
          larger the score, the more likely an utterance indicates
          absurdity. The output is sorted in descending order of
          the score to draw a precision-recall curve.</p>
          <p>Although our method is independent of particular
          machine learning algorithms and they are not the focus of
          the current paper, we compare the utterance classifier
          based on fastText with one based on Support Vector
          Machine (SVM)&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0002">2</a>] and one based on Convolutional
          Neural Network (CNN)&nbsp;[<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0010">10</a>] for the purpose of
          reference.</p>
          <p>The SVM-based classifier uses as features a variety of
          N-grams (surface- and base-form words, word
          pronunciations and parts-of-speech) with N ranging from 1
          to 5. <a class="fn" href="#fn8" id=
          "foot-fn8"><sup>8</sup></a> The kernel type and
          hyper-parameter C, which controls trade-off between
          training error and margin, were determined so that the
          area under the precision-recall curve (AUC) was maximized
          on half of Set&nbsp;2 of ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s. They were set to polynomial
          <em>d</em> = 3 and <em>C</em> = 1.0. <a class="fn" href=
          "#fn9" id="foot-fn9"><sup>9</sup></a> The output was
          sorted in descending order of distance from the SVM
          hyperplane.</p>
          <p>The CNN-based classifier was implemented based on
          Kim's CNN [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0010">10</a>].<a class="fn" href="#fn10" id=
          "foot-fn10"><sup>10</sup></a> The hyper-parameter
          settings were the same as those used by
          Kim&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0010">10</a>]; filter windows of 3, 4, and 5
          with 100 feature maps each, a dropout rate of 0.5, a
          mini-batch size of 50, and a dimension of 300 for word
          vectors (trained on Japanese Wikipedia articles using
          fastText's skip-gram). The network was trained for 50
          epochs, and the network weights obtained for the first
          epoch were the best with regard to the binary
          crossentropy loss on the half of Set&nbsp;2 used to tune
          the SVM-based classifier. The results were sorted in
          descending order of the output of the network.</p>
          <p>We evaluated the three utterance classifiers using the
          other half of Set&nbsp;2 of ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s that was not used for tuning the
          SVM- and CNN-based classifiers. Figure <a class="fig"
          href="#fig2">2</a> shows the precision-recall curves and
          the AUC values. The curves indicate that they have
          similar performances though the one based on fastText was
          not tuned, unlike the others.</p>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3185992/images/www2018-1-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">Precision-recall curves
              for utterance classifiers.</span>
            </div>
          </figure>
        </section>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Scoring
            Feedback Utterances</h3>
          </div>
        </header>
        <p>Using the utterance classifier based on fastText, we
        scored the feedback utterances described in
        Section&nbsp;<a class="sec" href="#sec-7">3.2</a> to
        construct a scored-feedback-utterance DB.
        Table&nbsp;<a class="tbl" href="#tab7">7</a> shows examples
        of scored feedback utterances with their scores. The scores
        and the degree to which feedback utterances indicate
        absurdity were generally correlated.</p>
        <div class="table-responsive" id="tab7">
          <div class="table-caption">
            <span class="table-number">Table 7:</span> <span class=
            "table-title">Examples of scored feedback utterances.
            Feedback utterances with high/low scores tend to
            unambiguously indicate
            absurdity/comprehensibility.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:right;">Score</th>
                <th style="text-align:center;">Feedback
                utterance</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:right;">0.789062</td>
                <td style="text-align:left;"><em>It does not make
                sense.</em></td>
              </tr>
              <tr>
                <td style="text-align:right;">0.673828</td>
                <td style="text-align:left;"><em>I have no idea
                what you are talking about.</em></td>
              </tr>
              <tr>
                <td style="text-align:right;">-0.697266</td>
                <td style="text-align:left;"><em>Goody,
                thanks.</em></td>
              </tr>
              <tr>
                <td style="text-align:right;">-0.966797</td>
                <td style="text-align:left;"><em>Well
                done!</em></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span> Retrieving
            Absurd and Comprehensible Conversation Samples</h3>
          </div>
        </header>
        <p>Using the scored feedback utterances, we automatically
        retrieved absurd and comprehensible conversations from the
        log. Basically, we retrieved ⟨<em>u</em>, <em>r</em>⟩s that
        were followed by feedback utterances with high scores as
        absurd and those that were followed by feedback utterances
        with low scores as comprehensible.</p>
        <p>More specifically, we first retrieved all the
        ⟨<em>u</em>, <em>r</em>⟩s followed by any of the scored
        feedback utterances from the log. The retrieved
        ⟨<em>u</em>, <em>r</em>⟩s were given the scores of the
        corresponding feedback utterances. For example, if
        ⟨<em>u</em>, <em>r</em>⟩, (<em>“I think I caught a
        cold.”</em>, <em>“Ha-ha.”</em>), was retrieved and the
        feedback utterance was <em>“It does not make sense.”</em>
        with a score of 0.789062, the ⟨<em>u</em>, <em>r</em>⟩ was
        given a score of 0.789062. Some ⟨<em>u</em>, <em>r</em>⟩s
        were followed by more than one feedback utterance with
        various scores. They were given the maximum score for the
        feedback utterances. The retrieved ⟨<em>u</em>,
        <em>r</em>⟩s were then sorted in descending score order.
        Finally, we took the <em>K</em> ⟨<em>u</em>, <em>r</em>⟩s
        with the highest scores as <em>absurd conversation
        samples</em> and the <em>K</em> ⟨<em>u</em>, <em>r</em>⟩s
        with the lowest scores as <em>comprehensible conversation
        samples</em>. Since the task was to detect absurd
        conversations, the absurd conversation samples were
        regarded as positive.</p>
        <p>The hyper-parameter <em>K</em> was determined so as to
        maximize the area under the precision-recall curve for
        Set&nbsp;2 of ⟨<em>u</em>, <em>r</em>, <em>l</em>⟩s. As a
        result, <em>K</em> = 50, 000 was chosen among 10,000,
        50,000, 100,000, 150,000, 200,000, 250,000, 300,000,
        350,000, 400,000, 450,000, and 500,000.</p>
        <p>Table&nbsp;<a class="tbl" href="#tab8">8</a> shows
        examples of absurd and comprehensible conversation samples,
        feedback utterances that followed the conversations, and
        scores given to the feedback utterances.</p>
        <div class="table-responsive" id="tab8">
          <div class="table-caption">
            <span class="table-number">Table 8:</span> <span class=
            "table-title">Examples of absurd and comprehensible
            conversation samples, feedback utterances that followed
            the conversations, and scores given to the feedback
            utterances.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th colspan="2" style="text-align:center;">Absurd
                conversation samples</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>What is on your
                mind?</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>My name is Voice
                Assist.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>f</em></td>
                <td style="text-align:left;"><em>It does not make
                sense.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;"><em>0.789062</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>Is it hot this
                summer?</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>I sometimes get
                hot, too... Sorry.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>f</em></td>
                <td style="text-align:left;"><em>I have no idea
                what you are talking about.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;"><em>0.673828</em></td>
              </tr>
            </tbody>
            <thead>
              <tr>
                <th colspan="2" style="text-align:center;">
                Comprehensible conversation samples</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>What is 365÷30 ×
                12 × 2÷3?</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>The answer is
                97.3333.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>f</em></td>
                <td style="text-align:left;"><em>You're smart,
                aren't you?</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;">
                <em>-0.798828</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>From here to
                Hiraizumi.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>The bus departing
                at 17:55 is available.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>f</em></td>
                <td style="text-align:left;"><em>Great.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;">
                <em>-0.917969</em></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.6</span> Training
            Conversation Classifier</h3>
          </div>
        </header>
        <p>The conversation classifier is trained using the absurd
        and comprehensible conversation samples. The fastText with
        pre-trained word vectors is used for the conversation
        classifier with all hyper-parameters set to their default
        values. The word vectors are the ones used for the
        fastText-based utterance classifier. The input to the
        conversation classifier is a concatenation of <em>u</em>
        and <em>r</em> of a ⟨<em>u</em>, <em>r</em>⟩ segmented by
        the morphological analyzer. The output of fastText is
        converted the same way as the utterance classifier as
        described in Section&nbsp;<a class="sec" href=
        "#sec-10">3.3.2</a>; the confidence score <em>s</em> for
        <em>comprehensible</em> is switched to − <em>s</em> so that
        confidence scores indicate absurdity.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.7</span> Detecting
            Absurd Conversations</h3>
          </div>
        </header>
        <p>As the final step, the conversation classifier takes the
        ⟨<em>u</em>, <em>r</em>⟩s in the log as inputs and outputs
        scores reflecting their absurdity. We applied the
        conversation classifier to Set 3 of the ⟨<em>u</em>,
        <em>r</em>, <em>l</em>⟩s.</p>
        <p>Table&nbsp;<a class="tbl" href="#tab9">9</a> shows
        examples of the classifier's <em>erroneous</em> outputs
        consisting of a ⟨<em>u</em>, <em>r</em>⟩, the score given
        by the conversation classifier, and the label given by the
        crowd workers. The first example was given a high score,
        which indicates absurdity, though it is actually
        comprehensible. This is due to the nature of conversations
        about sayings; users tend to inquire about the meaning of a
        saying given by an intelligent assistant by making
        utterances like <em>“What does it mean?”</em>. These
        utterances tend to resemble unfavorable feedback utterances
        indicating absurdity. To resolve this problem, we need to
        distinguish real feedback utterances from utterances that
        look the same as feedback ones but are actually part of a
        comprehensible conversation. This will be addressed in
        future work.</p>
        <div class="table-responsive" id="tab9">
          <div class="table-caption">
            <span class="table-number">Table 9:</span> <span class=
            "table-title">Examples of the conversation classifier's
            <em>erroneous</em> outputs that consist of ⟨<em>u</em>,
            <em>r</em>⟩, the score given by the classifier, and the
            label given by the crowd workers.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>Tell me a
                saying.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>The dog that trots
                about finds a bone.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;"><em>0.964844</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Label</td>
                <td style="text-align:left;">
                <em>comprehensible</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>Is it cold
                tomorrow?</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>Be careful not to
                catch a cold.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;"><em>0.544922</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Label</td>
                <td style="text-align:left;"><em>absurd</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>u</em></td>
                <td style="text-align:left;"><em>The weather in Rio
                de Janeiro.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>r</em></td>
                <td style="text-align:left;"><em>The weather in
                Sendai City is snowing and then clear.</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Score</td>
                <td style="text-align:left;">
                <em>-0.945313</em></td>
              </tr>
              <tr>
                <td style="text-align:center;">Label</td>
                <td style="text-align:left;"><em>absurd</em></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The second example shows a conversation that is not
        perfectly comprehensible since the intelligent assistant
        did not answer the question directly. However, the response
        is at least acceptable as the user can infer or find the
        answer thanks to the response. This vague and unclear
        nature is indicated by the mid-range score.</p>
        <p>The third example shows a conversation with a low score,
        which indicates comprehensibility. However, it is in fact
        absurd since the intelligent assistant gave weather
        information for the wrong location. Resolving this problem
        requires commonsense knowledge about locations. Apart from
        the wrong location, the conversation sounds natural, which
        would explain the low score.</p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Evaluation</h2>
        </div>
      </header>
      <p>We evaluated the performance of our method by comparing it
      with those of baseline methods and variations of our method
      as described in Section&nbsp;<a class="sec" href=
      "#sec-16">4.1</a>. We also examined its domain-independence,
      as described in Section&nbsp;<a class="sec" href=
      "#sec-19">4.2</a>. First we summarize the results:</p>
      <ol class="list-no-style">
        <li id="list1" label="(1)">Both the utterance and
        conversation classifiers contribute to the performance of
        our method.<br /></li>
        <li id="list2" label="(2)">A method that simply matches
        feedback utterances against entries in a log to extract
        absurd conversations has both ambiguity and infrequency
        problems.<br /></li>
        <li id="list3" label="(3)">Our method is likely
        domain-independent.<br /></li>
      </ol>
      <p>Preliminary results for our feedback-utterance acquisition
      method are presented in Section&nbsp;<a class="sec" href=
      "#sec-22">4.3</a>. This method is well suited for eliminating
      the manual labor involved in collecting feedback
      utterances.</p>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Performance
            of Proposed Method</h3>
          </div>
        </header>
        <section id="sec-17">
          <p><em>4.1.1 Methods Compared.</em> The performances of
          six methods were compared.</p>
          <ul class="list-no-style">
            <li id="uid38" label="Proposed:">Our proposed
            method.<br /></li>
            <li id="uid39" label="Proposed-UC:"><font style=
            "font-variant: small-caps">Proposed</font> without the
            utterance classifier for determining the effectiveness
            of the utterance classifier. <font style=
            "font-variant: small-caps">Proposed-UC</font> does not
            use the scores given to feedback utterances when
            retrieving absurd and comprehensible conversation
            samples to train the conversation classifier. Instead,
            it uses majority vote; it retrieves as absurd
            conversation samples those that are followed by more
            unfavorable feedback utterances than favorable ones,
            whereas conversations followed by more favorable
            feedback utterances than unfavorable ones are retrieved
            as comprehensible conversation samples. To be precise,
            first, each conversation is given a score calculated
            using |<em>FB<sub>unf</sub></em> | −
            |<em>FB<sub>fav</sub></em> |, where
            |<em>FB<sub>unf</sub></em> | and
            |<em>FB<sub>fav</sub></em> | are the numbers of
            unfavorable and favorable feedback utterances that
            follow the conversation (i.e., large scores indicate
            absurdity). Then, <em>K</em> conversations with the
            highest scores and <em>K</em> conversations with the
            lowest scores are retrieved as absurd and
            comprehensible conversation samples, respectively.
            Finally, <font style=
            "font-variant: small-caps">Proposed-UC</font> trains
            the conversation classifier using the retrieved
            conversation samples and scores each conversation the
            same way as <font style=
            "font-variant: small-caps">Proposed</font>. The
            hyper-parameter <em>K</em> was set to 50,000 on the
            basis of Set&nbsp;2 of ⟨<em>u</em>, <em>r</em>,
            <em>l</em>⟩s.<br /></li>
            <li id="uid40" label="Proposed-CC:">
              <font style=
              "font-variant: small-caps">Proposed</font> without
              the conversation classifier for determining the
              effectiveness of the conversation classifier.
              <font style=
              "font-variant: small-caps">Proposed-CC</font> gives
              each conversation a score calculated using
              |<em>FB<sub>large</sub></em> (<em>s</em>)| −
              |<em>FB<sub>small</sub></em> (− <em>s</em>)|, where
              |<em>FB<sub>large</sub></em> (<em>s</em>)| and
              |<em>FB<sub>small</sub></em> (− <em>s</em>)| are the
              numbers of feedback utterances that follow the
              conversation with the utterance classifier scores
              larger than <em>s</em> and smaller than − <em>s</em>,
              respectively (i.e., large scores indicate absurdity).
              For conversations without any following feedback
              utterances, <font style=
              "font-variant: small-caps">Proposed-CC</font> gives a
              very low score (-9,999) since comprehensible
              conversations are the majority in the log. Since
              <font style=
              "font-variant: small-caps">Proposed-CC</font> lacks
              the conversation classifier, we set threshold
              <em>t</em> for classification between <em>absurd</em>
              and <em>comprehensible</em>. The hyper-parameters
              <em>s</em> and <em>t</em> were set to 0.5 and -190 on
              the basis of Set&nbsp;2 of ⟨<em>u</em>, <em>r</em>,
              <em>l</em>⟩s.<a class="fn" href="#fn11" id=
              "foot-fn11"><sup>11</sup></a><br />
            </li>
            <li id="uid42" label="MajorityVote:">A baseline that
            lacks both the utterance and conversation classifiers.
            <font style=
            "font-variant: small-caps">MajorityVote</font> simply
            scores each conversation using
            |<em>FB<sub>unf</sub></em> | −
            |<em>FB<sub>fav</sub></em> |, i.e., the same score used
            in <font style=
            "font-variant: small-caps">Proposed-UC</font>, and
            ranks conversations on the basis of this score.
            Conversations without any following feedback utterances
            are given a very low score (-9,999). Since <font style=
            "font-variant: small-caps">MajorityVote</font> lacks
            the conversation classifier, we set classification
            threshold <em>t</em> the same way as <font style=
            "font-variant: small-caps">Proposed-CC</font>
            (<em>t</em> = −80). <font style=
            "font-variant: small-caps">MajorityVote</font> shows
            that simply relying on feedback utterances without
            considering the ambiguity and infrequency problems does
            not work well.<br /></li>
            <li id="uid43" label="Supervised:">A baseline
            supervised method. <font style=
            "font-variant: small-caps">Supervised</font> does not
            use feedback utterances and the utterance classifier.
            It simply trains the conversation classifier using
            Set&nbsp;1 of ⟨<em>u</em>, <em>r</em>, <em>l</em>⟩s.
            The classifier is based on fastText with the
            pre-trained word vectors, similar to <font style=
            "font-variant: small-caps">Proposed</font>. Note that
            <font style=
            "font-variant: small-caps">Supervised</font> (and
            <font style=
            "font-variant: small-caps">Proposed+Sup</font> below)
            can be used at the cost of domain-independence since
            training data for the conversation classifier must be
            manually created each time this method is applied to a
            new domain.<br /></li>
            <li id="uid44" label="Proposed+Sup:">A variant of
            <font style="font-variant: small-caps">Proposed</font>
            for determining performance improvement gained by
            augmenting <font style=
            "font-variant: small-caps">Proposed</font> with
            manually created labeled data. The training data for
            the conversation classifier are the absurd and
            comprehensible conversation samples
            (Section&nbsp;<a class="sec" href="#sec-12">3.5</a>)
            plus Set&nbsp;1 of ⟨<em>u</em>, <em>r</em>,
            <em>l</em>⟩s, which is used for <font style=
            "font-variant: small-caps">Supervised</font>.<br />
            </li>
          </ul>
        </section>
        <section id="sec-18">
          <p><em>4.1.2 Results.</em> Figure&nbsp;<a class="fig"
          href="#fig3">3</a> shows the precision-recall curves and
          AUCs for the methods compared. Table&nbsp;<a class="tbl"
          href="#tab10">10</a> summarizes their accuracy,
          precision, recall, and F1 score. Comparing the
          performances of <font style=
          "font-variant: small-caps">Proposed</font>, <font style=
          "font-variant: small-caps">Proposed-UC</font>, and
          <font style=
          "font-variant: small-caps">Proposed-CC</font>, we see
          that the utterance and conversation classifiers both
          contributed to the performance of <font style=
          "font-variant: small-caps">Proposed</font>. The poor
          performance of <font style=
          "font-variant: small-caps">Proposed-CC</font> was mainly
          due to its inability to deal with conversations that are
          not followed by feedback utterances in the log; it was
          thus greatly affected by the infrequency problem.
          <font style=
          "font-variant: small-caps">Proposed+Sup</font>’s
          performance shows that adding Set&nbsp;1, which was
          manually created, to the automatically acquired training
          data improves the performance of <font style=
          "font-variant: small-caps">Proposed</font>, as
          expected.</p>
          <figure id="fig3">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3185992/images/www2018-1-fig3.jpg"
            class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span>
              <span class="figure-title">Precision-recall curves
              and AUCs: Those in top graph are for our method and
              its variants; those in bottom graph are for our
              method and baseline methods.</span>
            </div>
          </figure>
          <div class="table-responsive" id="tab10">
            <div class="table-caption">
              <span class="table-number">Table 10:</span>
              <span class="table-title">Performance of all methods.
              Differences from <font style=
              "font-variant: small-caps">Proposed</font> are all
              statistically significant (McNemar's test: <em>p</em>
              &lt; 0.01).</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"></th>
                  <th style="text-align:center;">Accuracy</th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F1 score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Proposed</font></td>
                  <td style="text-align:right;">0.784</td>
                  <td style="text-align:right;">0.7158</td>
                  <td style="text-align:right;">
                  <strong>0.6907</strong></td>
                  <td style="text-align:right;">0.7031</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Proposed+Sup</font></td>
                  <td style="text-align:right;">
                  <strong>0.7998</strong></td>
                  <td style="text-align:right;">
                  <strong>0.751</strong></td>
                  <td style="text-align:right;">0.6869</td>
                  <td style="text-align:right;">
                  <strong>0.7176</strong></td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Proposed-UC</font></td>
                  <td style="text-align:right;">0.7106</td>
                  <td style="text-align:right;">0.6041</td>
                  <td style="text-align:right;">0.6338</td>
                  <td style="text-align:right;">0.6186</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Proposed-CC</font></td>
                  <td style="text-align:right;">0.5277</td>
                  <td style="text-align:right;">0.3683</td>
                  <td style="text-align:right;">0.3859</td>
                  <td style="text-align:right;">0.3769</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Supervised</font></td>
                  <td style="text-align:right;">0.7435</td>
                  <td style="text-align:right;">0.6735</td>
                  <td style="text-align:right;">0.5961</td>
                  <td style="text-align:right;">0.6324</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">MajorityVote</font></td>
                  <td style="text-align:right;">0.5293</td>
                  <td style="text-align:right;">0.3685</td>
                  <td style="text-align:right;">0.3804</td>
                  <td style="text-align:right;">0.3744</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p><font style="font-variant: small-caps">Proposed</font>
          outperformed <font style=
          "font-variant: small-caps">Supervised</font>, which we
          attribute to the automatically acquired training data
          (Section&nbsp;<a class="sec" href="#sec-12">3.5</a>),
          which is more extensive than that of <font style=
          "font-variant: small-caps">Supervised</font>.
          <font style="font-variant: small-caps">MajorityVote</font>
          performed poorly, mainly due to the ambiguity and
          infrequency problems.</p>
        </section>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Evaluation
            of Domain-Independence</h3>
          </div>
        </header>
        <section id="sec-20">
          <p><em>4.2.1 Experimental Conditions.</em> To evaluate
          domain independence, we compared out-of-domain and
          in-domain models of our method. The out-of-domain model
          was prepared by first training the utterance classifier
          without training samples, i.e., ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s, for the target domain and then
          constructing a scored-feedback-utterance DB without
          feedback utterances for the target domain. The domain of
          ⟨<em>u</em>, <em>r</em>, <em>l</em>, <em>f</em>⟩s was
          determined by our target intelligent assistant, Yahoo!
          Voice Assist; it classifies each user utterance
          (<em>u</em>) into a domain (i.e., <font style=
          "font-variant: small-caps">web search</font>,
          <font style="font-variant: small-caps">weather</font>,
          <font style="font-variant: small-caps">chitchat</font>,
          <font style=
          "font-variant: small-caps">fortune-telling</font>,
          <font style="font-variant: small-caps">sports</font>,
          <font style="font-variant: small-caps">cooking</font>,
          and so on), which is assigned to the corresponding
          ⟨<em>u</em>, <em>r</em>, <em>l</em>, <em>f</em>⟩. The
          domain of each feedback utterance (<em>f</em>) in the
          feedback-utterance DB is similarly determined; each
          <em>f</em> is assigned the domain of the user utterance
          in the conversation that <em>f</em> follows in the
          log.<a class="fn" href="#fn12" id=
          "foot-fn12"><sup>12</sup></a> The conversation classifier
          is then learned in the same way as described in
          Sections&nbsp;<a class="sec" href="#sec-12">3.5</a> and
          <a class="sec" href="#sec-13">3.6</a>. In other words, we
          assume that we know nothing about the target domain in
          Phase&nbsp;1 (Figure&nbsp;<a class="fig" href=
          "#fig1">1</a>) and that conversations for the target
          domain are recorded in the log in Phase&nbsp;2. <a class=
          "fn" href="#fn13" id="foot-fn13"><sup>13</sup></a> Note
          that if we can skip Phase&nbsp;1, which is the part
          requiring manual labor in our method, our method can be
          seen as domain-independent to adapt to a new domain since
          we can mostly automate domain adaptation. Hyper-parameter
          <em>K</em> for the out-of-domain model was set to 50,000,
          i.e., the same value as described in
          Section&nbsp;<a class="sec" href="#sec-12">3.5</a>.</p>
          <p>The in-domain model was prepared as described in
          Section&nbsp;<a class="sec" href="#sec-5">3</a>, except
          that the number of ⟨<em>u</em>, <em>r</em>, <em>l</em>,
          <em>f</em>⟩s for training the utterance classifier and
          that of feedback utterances in the scored-feedback
          utterance DB were reduced to the same numbers as for the
          out-of-domain model by randomly sampling ⟨<em>u</em>,
          <em>r</em>, <em>l</em>, <em>f</em>⟩s and the feedback
          utterances, so that the experimental conditions would be
          the same for the in-domain and out-of-domain models.</p>
          <p>We used two kinds of test sets for absurd conversation
          detection: one was ⟨<em>u</em>, <em>r</em>, <em>l</em>⟩s
          of Set&nbsp;3, which covers all domains,<a class="fn"
          href="#fn14" id="foot-fn14"><sup>14</sup></a> and the
          other covered only the target domain. As target domains,
          we chose <font style=
          "font-variant: small-caps">chitchat</font>, <font style=
          "font-variant: small-caps">web search</font>, and
          <font style="font-variant: small-caps">weather</font>
          since they were the most frequent in Set&nbsp;3.
          Consequently, we used six settings (three domains × two
          kinds of test sets).</p>
          <div class="table-responsive" id="tab11">
            <div class="table-caption">
              <span class="table-number">Table 11:</span>
              <span class="table-title">Numbers of ⟨<em>u</em>,
              <em>r</em>, <em>l</em>, <em>f</em>⟩s (training
              samples for the utterance classifier), feedback
              utterances, and samples for target domain's (Test
              (Target)) and all domains’ (Test (All)) test sets
              used in domain-independence experiments. Ratios (%)
              of <em>absurd</em> are given in parentheses. Note
              that the ⟨<em>u</em>, <em>r</em>, <em>l</em>,
              <em>f</em>⟩s and the feedback utterances are for the
              out-of-domain models; thus, the numbers are for those
              ⟨<em>u</em>, <em>r</em>, <em>l</em>, <em>f</em>⟩s and
              feedback utterances that do NOT belong to the target
              domain.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"></th>
                  <th style="text-align:center;"><font style=
                  "font-variant: small-caps">Chitchat</font></th>
                  <th style="text-align:center;"><font style=
                  "font-variant: small-caps">Web search</font></th>
                  <th style="text-align:center;"><font style=
                  "font-variant: small-caps">Weather</font></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">⟨<em>u</em>,
                  <em>r</em>, <em>l</em>, <em>f</em>⟩s</td>
                  <td style="text-align:right;">55,573 (9.2%)</td>
                  <td style="text-align:right;">223,457
                  (33.5%)</td>
                  <td style="text-align:right;">243,313
                  (31.7%)</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Feedback</td>
                  <td style="text-align:right;">10,664</td>
                  <td style="text-align:right;">19,025</td>
                  <td style="text-align:right;">19,998</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Test (Target)</td>
                  <td style="text-align:right;">2,274 (66.9%)</td>
                  <td style="text-align:right;">1,084 (12.0%)</td>
                  <td style="text-align:right;">827 (9.7%)</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Test (All)</td>
                  <td style="text-align:right;">4,935 (37.0%)</td>
                  <td style="text-align:right;">4,935 (37.0%)</td>
                  <td style="text-align:right;">4,935 (37.0%)</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Table&nbsp;<a class="tbl" href="#tab11">11</a> shows
          the numbers of training samples for the utterance
          classifier (i.e., ⟨<em>u</em>, <em>r</em>, <em>l</em>,
          <em>f</em>⟩s), feedback utterances, and samples of the
          two kinds of test sets used for absurd conversation
          detection. Ratios of <em>absurd</em> are given in
          parentheses. The number of ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s for <font style=
          "font-variant: small-caps">chitchat</font> was smaller
          than for the other two since <font style=
          "font-variant: small-caps">chitchat</font> was the most
          frequent domain, so removing the ⟨<em>u</em>, <em>r</em>,
          <em>l</em>, <em>f</em>⟩s for <font style=
          "font-variant: small-caps">chitchat</font> would lead to
          a smaller number of ⟨<em>u</em>, <em>r</em>, <em>l</em>,
          <em>f</em>⟩s. The ratio of <em>absurd</em> for
          <font style="font-variant: small-caps">chitchat</font>
          ⟨<em>u</em>, <em>r</em>, <em>l</em>, <em>f</em>⟩s was
          also smaller since many conversations in <font style=
          "font-variant: small-caps">chitchat</font> were
          absurd,<a class="fn" href="#fn15" id=
          "foot-fn15"><sup>15</sup></a> so removing the
          ⟨<em>u</em>, <em>r</em>, <em>l</em>, <em>f</em>⟩s for
          <font style="font-variant: small-caps">chitchat</font>
          would lead to a smaller ratio of <em>absurd</em>. The
          differences in the number of feedback utterances and in
          the number of test samples between target domains was due
          to differences in the frequency of domains. The ratio of
          <em>absurd</em> for a target domain reflects the level of
          difficulty of having a comprehensible conversation in
          that domain; it was the most difficult for the
          <font style="font-variant: small-caps">chitchat</font>
          domain, so the test set for the <font style=
          "font-variant: small-caps">chitchat</font> domain had the
          largest ratio of <em>absurd</em>. In contrast, it was the
          easiest for the <font style=
          "font-variant: small-caps">weather</font> domain since
          the patterns of weather-related utterances are relatively
          easy to enumerate. In short, absurd conversation
          detection should be the easiest for the <font style=
          "font-variant: small-caps">chitchat</font> domain since
          more than half of the test samples were
          <em>absurd</em>.</p>
          <p>We used Set&nbsp;3 as the all-domains’ test set (Test
          (All), Table&nbsp;<a class="tbl" href=
          "#tab11">11</a>).</p>
        </section>
        <section id="sec-21">
          <p><em>4.2.2 Results.</em> Figure&nbsp;<a class="fig"
          href="#fig4">4</a> and Table&nbsp;<a class="tbl" href=
          "#tab12">12</a> show the results. The difference in
          performance between the in- and out-of-domain models
          across the three domains was small; the difference was
          statistically significant only for <font style=
          "font-variant: small-caps">weather</font> for the
          all-domains’ test set (McNemar's test: <em>p</em> &lt;
          0.01) though the difference in precision-recall curves
          between the two models for the test set was rather
          small.</p>
          <figure id="fig4">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3185992/images/www2018-1-fig4.jpg"
            class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span>
              <span class="figure-title">Precision-recall curves:
              chitchat (left), web search (center), and weather
              (right). Upper graphs show results for target domain
              test set; lower graphs show results for all domains
              test set. In upper graphs, differences in ratios of
              <em>absurd</em> (positive samples) reflect difficulty
              of having comprehensible conversations for target
              domain; it was the most difficult for <font style=
              "font-variant: small-caps">chitchat</font> domain, so
              the ratio was high (upper left graph). Therefore,
              absurd conversation detection for <font style=
              "font-variant: small-caps">chitchat</font> was the
              easiest since the majority were <em>absurd</em>. In
              contrast, it is easier to converse comprehensibly in
              <font style="font-variant: small-caps">web
              search</font> and <font style=
              "font-variant: small-caps">weather</font> domains,
              since they are less open-ended than <font style=
              "font-variant: small-caps">chitchat</font>, which
              explains the low ratio of <em>absurd</em> (upper
              right and upper middle graphs).</span>
            </div>
          </figure>
          <div class="table-responsive" id="tab12">
            <div class="table-caption">
              <span class="table-number">Table 12:</span>
              <span class="table-title">Performance of in- and
              out-of-domain models. Difference between in- and
              out-of-domain models were statistically significant
              only for <font style=
              "font-variant: small-caps">weather</font> for
              all-domains’ test set (McNemar's test: <em>p</em>
              &lt; 0.01).</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th colspan="2" style="text-align:left;">
                    (Target domain)
                    <hr />
                  </th>
                  <th style="text-align:center;">Accuracy</th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F1 score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Chitchat</font></td>
                  <td style="text-align:left;">In</td>
                  <td style="text-align:right;">0.6988</td>
                  <td style="text-align:right;">0.7887</td>
                  <td style="text-align:right;">0.7508</td>
                  <td style="text-align:right;">0.7693</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">Out</td>
                  <td style="text-align:right;">0.6812</td>
                  <td style="text-align:right;">0.7671</td>
                  <td style="text-align:right;">0.7515</td>
                  <td style="text-align:right;">0.7592</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Web search</font></td>
                  <td style="text-align:left;">In</td>
                  <td style="text-align:right;">0.7934</td>
                  <td style="text-align:right;">0.2169</td>
                  <td style="text-align:right;">0.2769</td>
                  <td style="text-align:right;">0.2432</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">Out</td>
                  <td style="text-align:right;">0.7887</td>
                  <td style="text-align:right;">0.2171</td>
                  <td style="text-align:right;">0.2923</td>
                  <td style="text-align:right;">0.2492</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Weather</font></td>
                  <td style="text-align:left;">In</td>
                  <td style="text-align:right;">0.9069</td>
                  <td style="text-align:right;">1.0</td>
                  <td style="text-align:right;">0.0375</td>
                  <td style="text-align:right;">0.0723</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">Out</td>
                  <td style="text-align:right;">0.9069</td>
                  <td style="text-align:right;">1.0</td>
                  <td style="text-align:right;">0.0375</td>
                  <td style="text-align:right;">0.0723</td>
                </tr>
              </tbody>
              <thead>
                <tr>
                  <th colspan="2" style="text-align:left;">
                    (All domains)
                    <hr />
                  </th>
                  <th style="text-align:center;">Accuracy</th>
                  <th style="text-align:center;">Precision</th>
                  <th style="text-align:center;">Recall</th>
                  <th style="text-align:center;">F1 score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Chitchat</font></td>
                  <td style="text-align:left;">In</td>
                  <td style="text-align:right;">0.7627</td>
                  <td style="text-align:right;">0.682</td>
                  <td style="text-align:right;">0.6727</td>
                  <td style="text-align:right;">0.6773</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">Out</td>
                  <td style="text-align:right;">0.7611</td>
                  <td style="text-align:right;">0.6884</td>
                  <td style="text-align:right;">0.6481</td>
                  <td style="text-align:right;">0.6676</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Web search</font></td>
                  <td style="text-align:left;">In</td>
                  <td style="text-align:right;">0.7803</td>
                  <td style="text-align:right;">0.7027</td>
                  <td style="text-align:right;">0.705</td>
                  <td style="text-align:right;">0.7038</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">Out</td>
                  <td style="text-align:right;">0.7868</td>
                  <td style="text-align:right;">0.7246</td>
                  <td style="text-align:right;">0.6842</td>
                  <td style="text-align:right;">0.7038</td>
                </tr>
                <tr>
                  <td style="text-align:left;"><font style=
                  "font-variant: small-caps">Weather</font></td>
                  <td style="text-align:left;">In</td>
                  <td style="text-align:right;">0.7919</td>
                  <td style="text-align:right;">0.7307</td>
                  <td style="text-align:right;">0.6935</td>
                  <td style="text-align:right;">0.7116</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">Out</td>
                  <td style="text-align:right;">0.7801</td>
                  <td style="text-align:right;">0.7032</td>
                  <td style="text-align:right;">0.7028</td>
                  <td style="text-align:right;">0.703</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>These results indicate that our method is likely
          domain-independent; further investigation using more
          domains is needed to draw a more definitive
          conclusion.</p>
        </section>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span>
            Feedback-Utterance Acquisition</h3>
          </div>
        </header>
        <p>To explore the possibility of automating the collection
        of feedback utterances rather than using crowdsourcing as
        described in Section&nbsp;<a class="sec" href=
        "#sec-7">3.2</a>, we created an utterance-acquisition
        classifier that sorts utterances into favorable,
        unfavorable, and neutral.</p>
        <p>The training set for the classifier consisted of 9,634
        favorable, 10,324 unfavorable, and 9,854 neutral
        utterances. The test set consisted of 383 favorable, 609
        unfavorable, and 8,690 neutral utterances. The test set was
        sampled from the log mostly randomly, and the training set
        contained utterances collected in preliminary experiments,
        which explains why the distributions of favorable,
        unfavorable, and neutral differed between the two sets.</p>
        <p>We used fastText with pre-trained word vectors as the
        classifier with all the hyper-parameters set to their
        default values.</p>
        <p>We evaluated the classifier for <em>favorable</em> and
        <em>unfavorable</em> utterances separately as follows. The
        <em>favorable</em> results were sorted in descending order
        of the output values for <em>favorable</em>. The true
        labels of the test data were then matched with the
        classifier's predictions to measure performance. The
        classifier's <em>unfavorable</em> performance was similarly
        evaluated.</p>
        <p>The accuracy and F1 score for <em>favorable</em> were
        0.9089 and 0.3404, while those for <em>unfavorable</em>
        were 0.9274 and 0.5102. Figure&nbsp;<a class="fig" href=
        "#fig5">5</a> shows the precision-recall curves and AUCs.
        Given the skewed distribution of the test set, we believe
        that these results are reasonable and that they can at
        least serve as a starting point for further study. The
        worse performance for <em>favorable</em> was partly due to
        utterances containing favorable wording but actually
        expressing an unfavorable evaluation; e.g., <em>“Siri is
        smarter.”</em></p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185992/images/www2018-1-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Precision-recall curves for
            utterance acquisition.</span>
          </div>
        </figure>
      </section>
    </section>
    <section id="sec-23">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion</h2>
        </div>
      </header>
      <p>Our proposed method for detecting absurd conversations
      with an intelligent assistant exploits user feedback
      utterances in the log. Experiments showed that our method
      overcomes the common problems of ambiguity and infrequency by
      using utterance and conversation classifiers and that it is
      likely domain-independent.</p>
      <p>Since feedback utterances would be common in most
      languages, we will investigate the language independence of
      our method. We also plan to release our feedback utterances
      and the labeled data for utterance and conversation
      classifiers.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Piotr Bojanowski,
        Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016.
        Enriching Word Vectors with Subword Information.
        <em><em>arXiv preprint arXiv:1607.04606</em></em>
        (2016).</li>
        <li id="BibPLXBIB0002" label="[2]">Corinna Cortes and
        Vladimir Vapnik. 1995. Support-Vector Networks.
        <em><em>Machine Learning</em></em> 20, 3 (1995),
        273–297.</li>
        <li id="BibPLXBIB0003" label="[3]">Michel Galley, Chris
        Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli,
        Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill
        Dolan. 2015. deltaBLEU: A Discriminative Metric for
        Generation Tasks with Intrinsically Diverse Targets. In
        <em><em>Proceedings of the 53rd Annual Meeting of the
        Association for Computational Linguistics and the 7th
        International Joint Conference on Natural Language
        Processing (Volume 2: Short Papers).</em></em>
        445–450.</li>
        <li id="BibPLXBIB0004" label="[4]">Ahmed
        Hassan&nbsp;Awadallah, Ranjitha Gurunath&nbsp;Kulkarni,
        Umut Ozertem, and Rosie Jones. 2015. Characterizing and
        Predicting Voice Query Reformulation. In
        <em><em>Proceedings of the 24th ACM International on
        Conference on Information and Knowledge Management
        (CIKM).</em></em> 543–552.</li>
        <li id="BibPLXBIB0005" label="[5]">Ryuichiro Higashinaka,
        Masahiro Mizukami, Kotaro Funakoshi, Masahiro Araki,
        Hiroshi Tsukahara, and Yuka Kobayashi. 2015. Fatal or not?
        Finding errors that lead to dialogue breakdowns in
        chat-oriented dialogue systems. In <em><em>Proceedings of
        the 2015 Conference on Empirical Methods in Natural
        Language Processing.</em></em> 2243–2248.</li>
        <li id="BibPLXBIB0006" label="[6]">Guoliang Ji, Kang Liu,
        Shizhu He, and Jun Zhao. 2017. Distant Supervision for
        Relation Extraction with Sentence-Level Attention and
        Entity Descriptions. In <em><em>Proceedings of the
        Thirty-First AAAI Conference on Artificial
        Intelligence.</em></em> 3060–3066.</li>
        <li id="BibPLXBIB0007" label="[7]">Jiepu Jiang, Ahmed
        Hassan&nbsp;Awadallah, Rosie Jones, Umut Ozertem, Imed
        Zitouni, Ranjitha Gurunath&nbsp;Kulkarni, and Omar&nbsp;Zia
        Khan. 2015. Automatic Online Evaluation of Intelligent
        Assistants. In <em><em>Proceedings of the 24th
        International Conference on World Wide Web.</em></em>
        506–516.</li>
        <li id="BibPLXBIB0008" label="[8]">Jiepu Jiang, Wei Jeng,
        and Daqing He. 2013. How Do Users Respond to Voice Input
        Errors?: Lexical and Phonetic Query Reformulation in Voice
        Search. In <em><em>Proceedings of the 36th International
        ACM SIGIR Conference on Research and Development in
        Information Retrieval.</em></em> 143–152.</li>
        <li id="BibPLXBIB0009" label="[9]">Armand Joulin, Edouard
        Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of
        Tricks for Efficient Text Classification. <em><em>arXiv
        preprint arXiv:1607.01759</em></em> (2016).</li>
        <li id="BibPLXBIB0010" label="[10]">Yoon Kim. 2014.
        Convolutional Neural Networks for Sentence Classification.
        In <em><em>Proceedings of the 2014 Conference on Empirical
        Methods in Natural Language Processing (EMNLP).</em></em>
        1746–1751.</li>
        <li id="BibPLXBIB0011" label="[11]">Emiel Krahmer, Marc
        Swerts, Mariet Theune, and Mieke Weegels. 2001. Error
        detection in spoken human-machine interaction.
        <em><em>International Journal of Speech
        Technology</em></em> 4 (2001), 19–30.</li>
        <li id="BibPLXBIB0012" label="[12]">Jiwei Li, Michel
        Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016.
        A Diversity-Promoting Objective Function for Neural
        Conversation Models. In <em><em>Proceedings of the 2016
        Conference of the North American Chapter of the Association
        for Computational Linguistics: Human Language
        Technologies.</em></em> 110–119.</li>
        <li id="BibPLXBIB0013" label="[13]">Jiwei Li, Will Monroe,
        Alan Ritter, Daniel Jurafsky, Michel Galley, and Jianfeng
        Gao. 2016. Deep Reinforcement Learning for Dialogue
        Generation. In <em><em>Proceedings of the 2016 Conference
        on Empirical Methods in Natural Language
        Processing.</em></em> 1192–1202.</li>
        <li id="BibPLXBIB0014" label="[14]">Ryan Lowe, Michael
        Noseworthy, Iulian&nbsp;V. Serban, Nicolas
        Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. 2017.
        Towards an Automatic Turing Test: Learning to Evaluate
        Dialogue Responses. In <em><em>Proceedings of the 55th
        Annual Meeting of the Association for Computational
        Linguistics.</em></em> 1116–1126.</li>
        <li id="BibPLXBIB0015" label="[15]">Raveesh Meena, Jose
        Lopes, Gabriel Skantze, and Joakim Gustafson. 2015.
        Automatic Detection of Miscommunication in Spoken Dialogue
        Systems. In <em><em>Proceedings of the 16th Annual Meeting
        of the Special Interest Group on Discourse and Dialogue
        (SIGDIAL).</em></em> 354–363.</li>
        <li id="BibPLXBIB0016" label="[16]">Mike Mintz, Steven
        Bills, Rion Snow, and Dan Jurafsky. 2009. Distant
        supervision for relation extraction without labeled data.
        In <em><em>Proceedings of the 47th Annual Meeting of the
        ACL and the 4th IJCNLP of the AFNLP.</em></em>
        1003–1011.</li>
        <li id="BibPLXBIB0017" label="[17]">Kishore Papineni, Salim
        Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method
        for Automatic Evaluation of Machine Translation. In
        <em><em>Proceedings of the 40th Annual Meeting on
        Association for Computational Linguistics.</em></em>
        311–318.</li>
        <li id="BibPLXBIB0018" label="[18]">Alan Ritter, Colin
        Cherry, and William&nbsp;B. Dolan. 2011. Data-Driven
        Response Generation in Social Media. In <em><em>Proceedings
        of the 2011 Conference on Empirical Methods in Natural
        Language Processing.</em></em> 583–593.</li>
        <li id="BibPLXBIB0019" label="[19]">Shumpei Sano, Nobuhiro
        Kaji, and Manabu Sassano. 2016. Prediction of Prospective
        User Engagement with Intelligent Assistants. In
        <em><em>Proceedings of the 54th Annual Meeting of the
        Association for Computational Linguistics (Volume 1: Long
        Papers).</em></em> 1203–1212.</li>
        <li id="BibPLXBIB0020" label="[20]">Shumpei Sano, Nobuhiro
        Kaji, and Manabu Sassano. 2017. Predicting Causes of
        Reformulation in Intelligent Assistants. In
        <em><em>Proceedings of the 18th Annual Meeting of the
        Special Interest Group on Discourse and Dialogue
        (SIGDIAL).</em></em> 299–309.</li>
        <li id="BibPLXBIB0021" label="[21]">Ruhi Sarikaya. 2017.
        The Technology Behind Personal Digital Assistants: An
        overview of the system architecture and key components.
        <em><em>IEEE Signal Processing Magazine</em></em> 34, 1
        (2017), 67–81.</li>
        <li id="BibPLXBIB0022" label="[22]">Alexander Schmitt,
        Benjamin Schatz, and Wolfgang Minker. 2011. Modeling and
        Predicting Quality in Spoken Human-computer Interaction. In
        <em><em>Proceedings of the 12th Annual Meeting of the
        Special Interest Group on Discourse and Dialogue
        (SIGDIAL).</em></em> 173–184.</li>
        <li id="BibPLXBIB0023" label="[23]">Alessandro Sordoni,
        Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji,
        Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill
        Dolan. 2015. A Neural Network Approach to Context-Sensitive
        Generation of Conversational Responses.
        <em><em>CoRR</em></em> abs/1506.06714(2015).</li>
        <li id="BibPLXBIB0024" label="[24]">Marc Swerts, Diane
        Litman, and Julia Hirschberg. 2000. Corrections In Spoken
        Dialogue Systems. In <em><em>Proceedings of the Sixth
        International Conference on Spoken Language
        Processing.</em></em> 615–618.</li>
        <li id="BibPLXBIB0025" label="[25]">Jason Weston. 2016.
        Dialog-based Language Learning. In <em><em>Advances in
        Neural Information Processing Systems 29: Annual Conference
        on Neural Information Processing Systems 2016.</em></em>
        829–837.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>It has been
    developed and operated by Yahoo! JAPAN. <a class=
    "link-inline force-break" href=
    "https://v-assist.yahoo.co.jp">https://v-assist.yahoo.co.jp</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>In this study,
    any user utterance can be given one of these labels. Those that
    are not considered as feedback like <em>“what's today's
    weather?”</em> are labeled with <em>neutral</em>.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>It is a service
    by Yahoo! JAPAN. <a class="link-inline force-break" href=
    "https://crowdsourcing.yahoo.co.jp">https://crowdsourcing.yahoo.co.jp</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>The number of
    utterances each worker labeled ranged from 10 to 200. The 780
    workers were all considered reliable, since they correctly
    labeled a small number of utterances for which we knew
    appropriate labels beforehand. Other workers who failed to
    label the dummy utterances were counted out. All the workers
    lived in Japan.</p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>The number of
    conversations each worker labeled ranged from 5 to 150. All the
    1,071 workers correctly labeled a small number of ⟨<em>u</em>,
    <em>r</em>⟩s for which we knew appropriate labels beforehand.
    Other workers who failed to label the dummy ⟨<em>u</em>,
    <em>r</em>⟩s were counted out. All the workers lived in
    Japan.</p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>
    <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class=
    "link-inline force-break" href=
    "http://taku910.github.io/mecab/">http://taku910.github.io/mecab/</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>We used
    SVM-Light (<a class="link-inline force-break" href=
    "http://svmlight.joachims.org/">http://svmlight.joachims.org/</a>).
    For morphological analysis, we used MeCab (<a class=
    "link-inline force-break" href=
    "http://taku910.github.io/mecab/">http://taku910.github.io/mecab/</a>).</p>
    <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a>The kernel type
    was chosen from linear, polynomial <em>d</em> = 2, and
    <em>d</em> = 3. C was chosen from 0.01, 0.1, 1.0, 10.0, and
    100.0.</p>
    <p id="fn10"><a href="#foot-fn10"><sup>10</sup></a>We used
    Keras (<a class="link-inline force-break" href=
    "https://keras.io/">https://keras.io/</a>) for the
    implementation.</p>
    <p id="fn11"><a href="#foot-fn11"><sup>11</sup></a>The
    hyper-parameter <em>s</em> was chosen from 0.5, 0.6, 0.7, and
    0.8, and <em>t</em> was chosen from [ − 200, 200] with an
    increment of 10 (i.e., -200, -190, ..., 190, 200).</p>
    <p id="fn12"><a href="#foot-fn12"><sup>12</sup></a>A feedback
    utterance <em>f</em> can have multiple domains since <em>f</em>
    can appear multiple times in the log following different
    conversations. We removed from the scored-feedback-utterance DB
    those <em>f</em>s that followed only conversations of the
    target domain.</p>
    <p id="fn13"><a href="#foot-fn13"><sup>13</sup></a>If we also
    assume no information about the new domain in Phase&nbsp;2, we
    have no clue for learning about the domain and hence no chance
    of domain adaptation.</p>
    <p id="fn14"><a href="#foot-fn14"><sup>14</sup></a>The test set
    was not restricted to the three domains (<font style=
    "font-variant: small-caps">chitchat</font>, <font style=
    "font-variant: small-caps">web search</font>, and <font style=
    "font-variant: small-caps">weather</font>) but was for all the
    domains assumed in Yahoo! Voice Assist.</p>
    <p id="fn15"><a href="#foot-fn15"><sup>15</sup></a>
    <font style="font-variant: small-caps">Chitchat</font> is
    generally open-ended, so giving a proper response tends to be
    difficult.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3185992">https://doi.org/10.1145/3178876.3185992</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
