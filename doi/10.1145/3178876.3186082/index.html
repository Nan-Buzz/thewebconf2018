<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Finding Subcube Heavy Hitters in Analytics Data Streams</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Finding Subcube Heavy Hitters in Analytics Data Streams</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Branislav</span>      <span class="surName">Kveton</span><a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>,     Adobe Research, <a href="mailto:kveton@adobe.com">kveton@adobe.com</a>     </div>     <div class="author">     <span class="givenName">S.</span>      <span class="surName">Muthukrishnan</span>,     Rutgers University, <a href="mailto:muthu@cs.rutgers.edu">muthu@cs.rutgers.edu</a>     </div>     <div class="author">     <span class="givenName">Hoa T.</span>      <span class="surName">Vu</span><a class="fn" href="#fn2" id="foot-fn2"><sup>&#x2020;</sup></a>,     University of Massachusetts, <a href="mailto:hvu@cs.umass.edu">hvu@cs.umass.edu</a>     </div>     <div class="author">     <span class="givenName">Yikun</span>      <span class="surName">Xian</span>,     Rutgers University, <a href="mailto:siriusxyk@gmail.com">siriusxyk@gmail.com</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186082" target="_blank">https://doi.org/10.1145/3178876.3186082</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Modern data streams typically have high dimensionality. For example, digital analytics streams consist of user online activities (e.g., web browsing activity, commercial site activity, apps and social behavior, and response to ads). An important problem is to find frequent joint values (heavy hitters) of subsets of dimensions.</small>     </p>     <p>     <small>Formally, the data stream consists of <em>d</em>-dimensional items and a <em>       <em>k</em>-dimensional subcube T</em> is a subset of <em>k</em> distinct coordinates. Given a theshold <em>&#x03B3;</em>, a <em>subcube heavy hitter query</em> Query(<em>T</em>, <em>v</em>) outputs YES if <em>f<sub>T</sub>      </em>(<em>v</em>) &#x2265; <em>&#x03B3;</em> and NO if <em>f<sub>T</sub>      </em>(<em>v</em>) < <em>&#x03B3;</em>/4 where <em>f<sub>T</sub>      </em> is the ratio of the number of stream items whose coordinates <em>T</em> have joint values <em>v</em>. The <em>all subcube heavy hitters query</em> AllQuery(<em>T</em>) outputs all joint values <em>v</em> that return YES to Query(<em>T</em>, <em>v</em>). The problem is to answer these queries correctly for all <em>T</em> and <em>v</em>.</small>     </p>     <p>     <small>We present a simple one-pass sampling algorithm to solve the subcube heavy hitters problem in <span class="inline-equation"><span class="tex">$\tilde{O}(kd/\gamma)$</span>      </span> space. <span class="inline-equation"><span class="tex">$\tilde{O}(\cdot)$</span>      </span> suppresses polylogarithmic factors. This is optimal up to polylogarithmic factors based on the lower bound of Liberty et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0013">13</a>] In the worst case, this bound becomes <em>&#x0398;</em>(<em>d</em>      <sup>2</sup>/<em>&#x03B3;</em>) which is prohibitive for large <em>d</em>.</small>     </p>     <p>     <small>Our main contribution is to circumvent this quadratic bottleneck via a model-based approach. In particular, we assume that the dimensions are related to each other via the Naive Bayes model. We present a new two-pass, <span class="inline-equation"><span class="tex">$\tilde{O}(d/\gamma)$</span>      </span>-space algorithm for our problem, and a fast algorithm for answering AllQuery(<em>T</em>) in <span class="inline-equation"><span class="tex">$\tilde{O}((k/\gamma)^2)$</span>      </span> time.</small>     </p>     <p>     <small>We demonstrate the effectiveness of our approach on a synthetic dataset as well as real datasets from Adobe and Yandex. Our work shows the potential of model-based approach to data streams.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>algorithms</small>, </span>     <span class="keyword">      <small> data streams</small>, </span>     <span class="keyword">      <small> heavy hitters</small>, </span>     <span class="keyword">      <small> graphical models</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Branislav Kveton, S. Muthukrishnan, Hoa T. Vu, and Yikun Xian. 2018. Finding Subcube Heavy Hitters in Analytics Data Streams. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018 (WWW 2018),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186082" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186082</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>We study the problem of finding heavy hitters in high dimensional data streams. Most companies see transactions with items sold, time, store location, price, etc. that arrive over time. Modern online companies see streams of user web activities that typically have components of user information including ID (e.g. cookies), hardware (e.g., device), software (e.g., browser, OS), and contents such as web properties, apps. Activity streams also include events (e.g., impressions, views, clicks, purchases) and event attributes (e.g., product id, price, geolocation, time). Even classical IP traffic streams have many dimensions including source and destination IP addresses, port numbers and other features of an IP connection such as application type. Furthermore, in applications such as Natural Language Processing, streams of documents can be thought of as streams of a large number of bigrams or multi-grams over word combinations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. As these examples show, analytics data streams with 100&#x2019;s and 1000&#x2019;s of dimensions arise in many applications. Motivated by this, we study the problem of finding <em>heavy hitters</em> on data streams focusing on <em>d</em>, the number of dimensions, as a parameter. Given <em>d</em> one sees in practice, <em>d</em>     <sup>2</sup> in space usage is prohibitive, for solving the heavy hitters problem on such streams.</p>    <p>Formally, let us start with a one-dimensional stream of items <em>x</em>     <sub>1</sub>, &#x2026;<em>x<sub>m</sub>     </em> where each <em>x<sub>i</sub>     </em> &#x2208; [<em>n</em>] &#x2254; {1, 2, &#x2026;, <em>n</em>}. We can look at the <em>count c</em>(<em>v</em>) = |{<em>i</em>: <em>x<sub>i</sub>     </em> = <em>v</em>}| or the <em>frequency ratio f</em>(<em>v</em>) = <em>c</em>(<em>v</em>)/<em>m</em>. A <em>heavy hitter</em> value <em>v</em> is one with <em>c</em>(<em>v</em>) &#x2265; <em>&#x03B3;m</em> or equivalently <em>f</em>(<em>v</em>) &#x2265; <em>&#x03B3;</em>, for some constant <em>&#x03B3;</em>. The standard <em>data stream model</em> is that we maintain data structures of size <span class="inline-equation"><span class="tex">$\operatorname{polylog}(m,n)$</span>     </span> and determine if <em>v</em> is a heavy hitter with probability of success at least 3/4, that is, if <em>f</em>(<em>v</em>) &#x2265; <em>&#x03B3;</em> output YES and output NO if <em>f</em>(<em>v</em>) < <em>&#x03B3;</em>/4 for all <em>v</em>.<a class="fn" href="#fn3" id="foot-fn3"><sup>1</sup></a> We note that if <em>&#x03B3;</em>/4 &#x2264; <em>f</em>(<em>v</em>) < <em>&#x03B3;</em>, then either answer is acceptable.</p>    <p>Detecting heavy hitters on data streams is a fundamental problem that arises in guises such as finding elephant flows and network attacks in networking, finding hot trends in databases, finding frequent patterns in data mining, finding largest coefficients in signal analysis, and so on. Therefore, the heavy hitters problem has been studied extensively in theory, databases, networking and signal processing literature. See&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] for an early survey and &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] for a recent survey.</p>    <p>     <em>Subcube heavy hitter problems</em>. Our focus is on modern data streams such as in analytics cases, with <em>d</em> dimensions, for large <em>d</em>. The data stream consists of <em>d</em>-dimensional items <em>x</em>     <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>x<sub>m</sub>     </em>. In particular, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ x_i = (x_{i,1},\ldots ,x_{i,d}) \mbox{ and each } x_{i,j} \in [n] ~. \] </span>      <br/>     </div>     </div> A <em>     <em>k</em>-dimensional subcube T</em> is a subset of <em>k</em> distinct coordinates {<em>T</em>     <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>T<sub>k</sub>     </em>}&#x2286;[<em>d</em>]. We refer to the joint values of the coordinates <em>T</em> of <em>x<sub>i</sub>     </em> as <em>x</em>     <sub>     <em>i</em>, <em>T</em>     </sub>.</p>    <p>The number of items whose coordinates <em>T</em> have joint values <em>v</em> is denoted by <em>c<sub>T</sub>     </em>(<em>v</em>), i.e., <em>c<sub>T</sub>     </em>(<em>v</em>) = |{<em>i</em>: <em>x</em>     <sub>     <em>i</em>, <em>T</em>     </sub> = <em>v</em>}|. Finally, we use <em>X<sub>T</sub>     </em> to denote the random variable of the joint values of the coordinates <em>T</em> of a random item. We have the following relationship <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ f_T(v) := \Pr \left[ X_T = v \right] = \frac{c_T(v)}{m} ~. \] </span>      <br/>     </div>     </div> For a single coordinate <em>i</em>, we slightly abuse the notation by using <em>f<sub>i</sub>     </em> and <em>f</em>     <sub>{<em>i</em>}</sub> interchangeably. For example, <span class="inline-equation"><span class="tex">$f_{T_i}(v)$</span>     </span> is the same as <span class="inline-equation"><span class="tex">$f_{ \lbrace T_i \rbrace }(v)$</span>     </span>. Similarly, <em>X<sub>i</sub>     </em> is the same as <em>X</em>     <sub>{<em>i</em>}</sub>.</p>    <p>We are now ready to define our problems. They take <em>k</em>, <em>&#x03B3;</em> as parameters and the stream as the input and build data structures to answer:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;"><em>Subcube Heavy Hitter</em>: Query(<em>T</em>, <em>v</em>), where |<em>T</em>| = <em>k</em>, and <em>v</em> &#x2208; [<em>n</em>]<sup>      <em>k</em>     </sup>, returns an estimate if <em>f<sub>T</sub>     </em>(<em>v</em>) &#x2265; <em>&#x03B3;</em>. Specifically, output YES if <em>f<sub>T</sub>     </em>(<em>v</em>) &#x2265; <em>&#x03B3;</em> and NO if <em>f<sub>T</sub>     </em>(<em>v</em>) < <em>&#x03B3;</em>/4. If <em>&#x03B3;</em>/4 &#x2264; <em>f<sub>T</sub>     </em>(<em>v</em>) < <em>&#x03B3;</em>, then either output is acceptable. The required success probability <em>for all k</em>-dimensional subcubes <em>T</em> and <em>v</em> &#x2208; [<em>n</em>]<sup>      <em>k</em>     </sup> is at least 3/4.<br/></li>     <li id="list2" label="&#x2022;"><em>All Subcube Heavy Hitters</em>: AllQuery(<em>T</em>) outputs all joint values <em>v</em> that return YES to Query(<em>T</em>, <em>v</em>). This is conditioned on the algorithm used for Query(<em>T</em>, <em>v</em>).<br/></li>    </ul>    <p>It is important to emphasize that the stream is presented (in a single pass or constant passes) to the algorithm before the algorithm receives any query.</p>    <p>Subcube heavy hitters are relevant wherever one dimensional heavy hitters have found applications: combination of source and destination IP addresses forms the subcube heavy hitters that detect network attacks; combination of stores, sales quarters and nature of products forms the subcube heavy hitters that might be the pattern of interest in the data, etc. Given the omnipresence of multiple dimensions in digital analytics, arguably, subcube heavy hitters limn the significant data properties far more than the single dimensional view.</p>    <p>     <em>Related works</em>. The problem we address is directly related to frequent itemset mining studied in the data mining community. In frequent itemset mining, each dimension is binary (<em>n</em> = 2), and we consider Query(<em>T</em>, <em>v</em>) where <em>v</em> = (1, &#x2026;, 1) &#x2254; <strong>U</strong>     <sub>     <em>k</em>     </sub>. It is known that counting all maximal subcubes <em>T</em> that have a frequent itemset, i.e., <em>f<sub>T</sub>     </em>(<strong>U</strong>     <sub>     <em>k</em>     </sub>) &#x2265; <em>&#x03B3;</em>, is <span class="inline-equation"><span class="tex">$\#P$</span>     </span>-complete [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>]. Furthermore, finding even a single <em>T</em> of maximal size such that <em>f<sub>T</sub>     </em>(<strong>U</strong>     <sub>     <em>k</em>     </sub>) &#x2265; <em>&#x03B3;</em> is NP-hard [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. Recently, Liberty et al.&#x00A0;showed that any constant-pass streaming algorithm answering Query(<em>T</em>, <strong>U</strong>     <sub>     <em>k</em>     </sub>) requires <em>&#x03A9;</em>(<em>kd</em>/<em>&#x03B3;</em> &#x00B7; log&#x2009;(<em>d</em>/<em>k</em>)) bits of memory [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. In the worst case, this is <em>&#x03A9;</em>(<em>d</em>     <sup>2</sup>/<em>&#x03B3;</em>) for large <em>k</em>, ignoring the polylogarithmic factors. For this specific problem, sampling algorithms will nearly meet their lower bound for space. Our problem is more general, with arbitrary <em>n</em> and <em>v</em>.</p>    <p>     <em>Our contributions</em>. Clearly, the case <em>k</em> = 1 can be solved by building one of the many known single dimensional data structures for the heavy hitters problem on each of the <em>d</em> dimension; the <em>k</em> = <em>d</em> case can be thought of as a giant single dimensional problem by linearizing the space of all values in [<em>n</em>]<sup>     <em>k</em>     </sup>; for any other <em>k</em>, there are <span class="inline-equation"><span class="tex">${d \atopwithdelims ()k}$</span>     </span> distinct choices for subcube <em>T</em>, and these could be treated as separate one-dimensional problems by linearizing each of the subcubes. In general, this entails <span class="inline-equation"><span class="tex">${d \atopwithdelims ()k}$</span>     </span> and log&#x2009;(<em>n<sup>d</sup>     </em>) cost in space or time bounds over the one-dimensional case, which we seek to avoid. Also, our problem can be reduced to the binary case by unary encoding each dimension by <em>n</em> bits, and solving frequent itemset mining: the query then has <em>kn</em> dimensions. The resulting bound will have an additional <em>n</em> factor which is large.</p>    <p>First, we observe that the reservoir sampling approach &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] solves subcube heavy hitters problems more efficiently compared to the approaches mentioned above. Our analysis shows that the space we use is within polylogarithmic factors of the lower bound shown in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] for binary dimensions and query vector <strong>U</strong>     <sub>     <em>k</em>     </sub>, which is a special case of our problem. Therefore, the subcube heavy hitters problem can be solved using <span class="inline-equation"><span class="tex">$\tilde{O}(kd/\gamma)$</span>     </span> space. However, this is <em>&#x03A9;</em>(<em>d</em>     <sup>2</sup>) in worst case.</p>    <p>Our main contribution is to avoid this quadratic bottleneck for finding subcube heavy hitters. We adopt the notion that there is an underlying probabilistic model behind the data, and in the spirit of the Naive Bayes model, we assume that the dimensions are nearly (not exactly) mutually independent given an observable latent dimension. This could be considered as a low rank factorization of the dimensions. In particular, one could formalize this assumption by bounding the total variational distance between the data&#x0027;s joint distribution and that derived from the Naive Bayes formula. This assumption is common in statistical data analysis and highly prevalent in machine learning. Following this modeling, we make two main contributions:</p>    <ul class="list-no-style">     <li id="list3" label="&#x2022;">We present a two-pass, <span class="inline-equation"><span class="tex">$\tilde{O}(d/\gamma)$</span>     </span>-space streaming algorithm for answering Query(<em>T</em>, <em>v</em>). This improves upon the <em>kd</em> factor in the space complexity from sampling, without assumptions, to just <em>d</em> with the Naive Bayes assumption, which would make this algorithm practical for large <em>k</em>. Our algorithm uses sketching in each dimension in one pass to detect heavy hitters, and then needs a second pass to precisely estimate their frequencies.<br/></li>     <li id="list4" label="&#x2022;">We present a fast algorithm for answering AllQuery(<em>T</em>) in <span class="inline-equation"><span class="tex">$\tilde{O}((k/\gamma)^2)$</span>     </span> time. The naive procedure would take exponential time <em>&#x03A9;</em>((1/<em>&#x03B3;</em>)<sup>      <em>k</em>     </sup>) by considering the Cartesian product of the heavy hitters in each dimension. Our approach, on the other hand, uses the structure of the Naive Bayes assumption to iteratively construct the subcube heavy hitters one dimension at a time.<br/></li>    </ul>    <p>Our work develops the direction of model-based data stream analysis. Model-based data analysis has been effective in other areas. For example, in compressed sensing, realistic signal models that include dependencies between values and locations of the signal coefficients improve upon unconstrained cases [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. In statistics, using tree constrained models of multidimensional data sometimes improves point and density estimation. In high dimensional distribution testing, model based approach has also been studied to overcome the curse of dimensionality [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>].</p>    <p>In the data stream model, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] studied the problem of testing independence. McGregor and Vu [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] studied the problem of evaluating Bayesian Networks. In another work, Kveton et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] assumed a tree graphical model and designed a one-pass algorithm that estimates the joint frequency; their work however only solved the <em>k</em> = <em>d</em> case for the joint frequency estimation problem. Our model is a bit different and more importantly, we solve the subcube heavy hitters problem (addressing all the <span class="inline-equation"><span class="tex">${d \atopwithdelims ()k}$</span>     </span> subcubes) which prior work does not solve. In following such a direction, we have extended the fundamental heavy hitters problem to higher dimensional data. Given that many implementations already exist for the sketches we use for one-dimensional heavy hitters as a blackbox, our algorithms are therefore easily implementable.</p>    <p>     <em>Background on the Naive Bayes model and its use in our context.</em> The Naive Bayes Model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] is a Bayesian network over <em>d</em> features <em>X</em>     <sub>1</sub>, &#x2026;, <em>X<sub>d</sub>     </em> and a class variable <em>Y</em>. This model represents a joint probability distribution of the form <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} &#x0026; \Pr \left[ X_1 = x_1, \dots , X_d = x_d, Y = y \right] \\ = &#x0026; \Pr \left[ Y = y \right] \prod _{j = 1}^d \Pr \left[ X_j = x_j \mid Y = y \right]\,,\end{align*} </span>      <br/>     </div>     </div> which means that the values of the features are conditionally independent given the value of the class variable. The simplicity of the Naive Bayes model makes it a popular choice in text processing and information retrieval [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], with state-of-the-art performance in spam filtering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>], text classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>], and others.</p>    <p>     <em>Empirical study.</em> We perform detailed experimental study of subcube heavy hitters. We use a synthetic dataset where we generate data that confirms to the Naive Bayes model. We then experiment with real data sets from Yandex (Search) and Adobe (Marketing Cloud) which give multidimensional analytics streams. We experiment with the reservoir sampling based algorithm as a benchmark that works without modeling assumptions, and our two-pass subcube heavy hitters algorithm that improves upon it for data that satisfies the model. We also adopt our approach to give a simpler one-pass algorithm for which theoretical guarantees is weaker. Our experiments show substantial improvement of the model-based algorithms over the benchmark for synthetic as well as real data sets, and further show the benefits of the second pass.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> The Sampling Algorithm</h2>     </div>    </header>    <p>In this section, we show that sampling solves the problem efficiently compared to running one-dimensional heavy hitters algorithms for each of <span class="inline-equation"><span class="tex">${d \atopwithdelims ()k}$</span>     </span>     <em>k</em>-dimensional subcubes independently. It also matches the lower bound in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] up to polylogarithmic factors.</p>    <p>     <em>Algorithm details.</em> The algorithm samples <span class="inline-equation"><span class="tex">$m^{\prime } = \tilde{O}(\gamma ^{-1} k d)$</span>     </span> random items <span class="inline-equation"><span class="tex">$z_1,\ldots ,z_{m^{\prime }}$</span>     </span> from the stream using Reservoir sampling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. Let <span class="inline-equation"><span class="tex">$S = \lbrace z_1,\ldots ,z_{m^{\prime }}\rbrace$</span>     </span> be the sample set. Given Query(<em>T</em>, <em>v</em>), we output YES if and only if the sample frequency of <em>v</em>, denoted by <span class="inline-equation"><span class="tex">$\hat{f}_T(v)$</span>     </span>, is at least <em>&#x03B3;</em>/2. Specifically, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \hat{f}_T(v) := \frac{| \lbrace x_i : x_i \in S \mbox{ and }x_{i,T} = v \rbrace |}{m^{\prime }} ~. \] </span>      <br/>     </div>     </div>    </p>    <p>For all subcubes <em>T</em> and joint values <em>v</em> of <em>T</em>, the expected sample frequency <span class="inline-equation"><span class="tex">$\hat{f}_T(v)$</span>     </span> is <em>f<sub>T</sub>     </em>(<em>v</em>). Intuitively, if <em>v</em> is a frequent joint values, then its sample frequency <span class="inline-equation"><span class="tex">$\hat{f}_T(v) \approx f_T(v)$</span>     </span>; otherwise, <span class="inline-equation"><span class="tex">$\hat{f}_T(v)$</span>     </span> stays small.</p>    <p>Let us fix a <em>k</em>-dimensional subcube <em>T</em> and suppose that for all <em>v</em> &#x2208; [<em>n</em>]<sup>     <em>k</em>     </sup>, we have <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{align} \hat{f}_T (v) = f_T(v) \pm \frac{\max \lbrace \gamma ,f_T(v)\rbrace }{4} ~. \end{align} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div>    </p>    <p>It is then straightforward to see that if <em>f<sub>T</sub>     </em>(<em>v</em>) < <em>&#x03B3;</em>/4, then <span class="inline-equation"><span class="tex">$\hat{f}_T (v) {\lt} \gamma /4+ \gamma /4 = \gamma /2$</span>     </span>. Otherwise, if <em>f<sub>T</sub>     </em>(<em>v</em>) &#x2265; <em>&#x03B3;</em>, then <span class="inline-equation"><span class="tex">$\hat{f}_T (v) \ge 3f_T(v)/4 \ge 3\gamma /4 {\gt} \gamma /2$</span>     </span>. Hence, we output YES for all <em>v</em> where <span class="inline-equation"><span class="tex">$\hat{f}_T (v) \ge \gamma /2$</span>     </span>, and output NO otherwise.</p>    <p>     <div class="lemma" id="enc1">     <Label>Lemma 2.1.</Label>     <p>(Chernoff bound) Let <em>X</em>      <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>X<sub>n</sub>      </em> be independent or negatively correlated binary random variables. Let <span class="inline-equation"><span class="tex">$X = \sum _{i=1}^n X_i$</span>      </span> and <em>&#x03BC;</em> = E[<em>X</em>]. Then, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \Pr \left[ | X - \mu | \ge \epsilon \mu \right] \le \exp (-\min \lbrace \epsilon ^2 ,\epsilon \rbrace \mu /3) ~. \] </span>        <br/>       </div>      </div>     </p>     </div>    </p>    <p>Recall that <span class="inline-equation"><span class="tex">$S = \lbrace z_1,z_2,\ldots ,z_{m^{\prime }} \rbrace$</span>     </span> is the sample set returned by the algorithm. For a fixed <em>v</em> &#x2208; [<em>n</em>]<sup>     <em>k</em>     </sup>, we use <em>Z<sub>i</sub>     </em> as the indicator variable for the event <em>z</em>     <sub>     <em>i</em>, <em>T</em>     </sub> = <em>v</em>. Since we sample without replacement, the random variables <em>Z<sub>i</sub>     </em> are negatively correlated. The following lemma shows that Eq. <a class="eqn" href="#eq1">1</a> holds for all <em>v</em> and <em>k</em>-dimensional subcubes <em>T</em> via Chernoff bound.</p>    <p>     <div class="lemma" id="enc2">     <Label>Lemma 2.2.</Label>     <p> For all <em>k</em>-dimensional subcubes <em>T</em> and joint values <em>v</em> &#x2208; [<em>n</em>]<sup>       <em>k</em>      </sup>, with probability at least 0.9, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \hat{f}_T (v) = f_T(v) \pm \frac{ \max \lbrace \gamma ,f_T(v)\rbrace }{4} ~. \] </span>        <br/>       </div>      </div>     </p>     </div>    </p>    <div class="proof" id="proof1">     <Label>Proof.</Label>     <p> Let <em>m</em>&#x2032; = <em>c&#x03B3;</em>     <sup>&#x2212; 1</sup>log&#x2009;(<em>d<sup>k</sup>     </em> &#x00B7; <em>n<sup>k</sup>     </em>) for some sufficiently large constant <em>c</em>. We first consider a fixed <em>v</em> &#x2208; [<em>n</em>]<sup>      <em>k</em>     </sup> and define the random variables <em>Z<sub>i</sub>     </em> as above, i.e., <em>Z<sub>i</sub>     </em> = 1 if <em>z</em>     <sub>      <em>i</em>, <em>T</em>     </sub> = <em>v</em>. Suppose <em>f<sub>T</sub>     </em>(<em>v</em>) &#x2265; <em>&#x03B3;</em>. Appealing to Lemma <a class="enc" href="#enc1">2.1</a>, we have <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \Pr \left[ \left| \left(\sum _{i=1}^{m^{\prime }} \frac{Z_i}{m^{\prime }} \right) - f_T(v) \right| \ge \frac{f_T(v)}{4} \right] \\ = &#x0026; \Pr \left[ \left| \hat{f}_T(v) - f_T(v) \right| \ge \frac{f_T(v)}{4} \right] \\\le &#x0026; \exp \left(-\frac{f_T(v) m^{\prime }}{3 \times 16} \right) \le \frac{1}{10 d^k n^k } ~.\end{align*} </span>       <br/>      </div>     </div> On the other hand, if <em>f<sub>T</sub>     </em>(<em>v</em>) < <em>&#x03B3;</em>/4, then <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \Pr \left[ \left| \hat{f}_T(v) - f_T(v) \right| \ge \frac{\gamma }{4} \right] &#x0026; \le \exp \left(-\left(\frac{\gamma }{4 f_T(v)} \right) f_T(v) \frac{m^{\prime }}{3} \right) \\ &#x0026; \le \frac{1}{10 d^k n^k } ~.\end{align*} </span>       <br/>      </div>     </div> Therefore, by taking the union bound over all <span class="inline-equation"><span class="tex">${d \atopwithdelims ()k} \cdot n^k \le d^k \cdot n^k$</span>     </span> possible combinations of <em>k</em>-dimensional subcubes and the corresponding joint values <em>v</em> &#x2208; [<em>n</em>]<sup>      <em>k</em>     </sup>, we deduce the claim.</p>    </div>    <p>We therefore could answer all Query(<em>T</em>, <em>v</em>) correctly with probability at least 0.9 for all joint values <em>v</em> &#x2208; [<em>n</em>]<sup>     <em>k</em>     </sup> and <em>k</em>-dimensional subcubes <em>T</em>. Because storing each sample <em>z<sub>i</sub>     </em> requires <span class="inline-equation"><span class="tex">$\tilde{O}(d)$</span>     </span> bits of space, the algorithm uses <span class="inline-equation"><span class="tex">$\tilde{O}(dk \gamma ^{-1})$</span>     </span> space. We note that answering Query(<em>T</em>, <em>v</em>) requires computing <span class="inline-equation"><span class="tex">$\hat{f}_T(v)$</span>     </span> which takes <em>O</em>(|<em>S</em>|) time. We can answer AllQuery(<em>T</em>) by computing <span class="inline-equation"><span class="tex">$\hat{f}_T(v)$</span>     </span> for all joint values <em>v</em> of coordinates <em>T</em> that appear in the sample set which will take <em>O</em>(|<em>S</em>|<sup>2</sup>) time. We summarize the result as follows.</p>    <p>     <div class="theorem" id="enc3">     <Label>Theorem 2.3.</Label>     <p> There exists a 1-pass algorithm that uses <span class="inline-equation"><span class="tex">$\tilde{O}(d k \gamma ^{-1})$</span>      </span> space and solves <em>k</em>-dimensional subcube heavy hitters. Furthermore, Query(<em>T</em>, <em>v</em>) and AllQuery(<em>T</em>) take <span class="inline-equation"><span class="tex">$\tilde{O}(d k \gamma ^{-1})$</span>      </span> and <span class="inline-equation"><span class="tex">$\tilde{O}\left(\left(d k \gamma ^{-1} \right)^2 \right)$</span>      </span> time respectively.</p>     </div>    </p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> The Near-Independence Assumption</h2>     </div>    </header>    <p>     <em>The near-independence assumption.</em> .Suppose the random variables representing the dimensions <em>X</em>     <sub>1</sub>, <em>X</em>     <sub>2</sub>, &#x2026;, <em>X<sub>d</sub>     </em> are <em>near</em> independent. We show that there is a 2-pass algorithm that uses less space and has faster query time. At a high level, we make the assumption that the joint probability is approximately factorized <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ f_{\lbrace 1,\ldots ,d\rbrace }(v) \approx f_1 (v_1) f_2(v_2) \cdots f_d (v_d) ~. \] </span>      <br/>     </div>     </div>    </p>    <p>More formally, we assume that the total variation distance is bounded by a small quantity <em>&#x03B1;</em>. Furthermore, we assume that <em>&#x03B1;</em> is reasonable with respect to <em>&#x03B3;</em> that controls the heavy hitters. For example, <em>&#x03B1;</em> &#x2264; <em>&#x03B3;</em>/10 will suffice.</p>    <p>The formal <em>near-independence</em> assumption is as follows: There exists <em>&#x03B1;</em> &#x2264; <em>&#x03B3;</em>/10 such that for all subcubes <em>T</em>, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \max _{v \in [n]^{ \left| T \right|}} \left| f_{T}(v) - \prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \right| {\lt} \alpha ~. \] </span>      <br/>     </div>     </div>    </p>    <p>We observe that:</p>    <ul class="list-no-style">     <li id="list5" label="&#x2022;">If <em>f<sub>T</sub>     </em>(<em>v</em>) &#x2265; <em>&#x03B3;</em>, then <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \ge f_T(v)-\gamma /10 {\gt} \gamma /2~. \] </span>       <br/>      </div>     </div>     <br/></li>     <li id="list6" label="&#x2022;">If <em>f<sub>T</sub>     </em>(<em>v</em>) < <em>&#x03B3;</em>/4, then <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \le f_T(v) + \gamma /10 {\lt} \gamma /2~. \] </span>       <br/>      </div>     </div>     <br/></li>    </ul>    <p>Thus, it suffices to output YES to Query(<em>T</em>, <em>v</em>) if and only if the marginals product <span class="inline-equation"><span class="tex">$\prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \ge \gamma /2$</span>     </span>. For convenience, let <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \lambda := \gamma /2~. \] </span>      <br/>     </div>     </div>     <em>Algorithm details.</em> We note that simply computing <em>f<sub>i</sub>     </em>(<em>x</em>) for all coordinates <em>i</em> &#x2208; [<em>d</em>] and <em>x</em> &#x2208; [<em>n</em>] will need <em>&#x03A9;</em>(<em>dn</em>) space. To over come this, we make following simple but useful observation. We observe that if <em>v</em> is a heavy hitter in the subcube <em>T</em> and if <em>T</em>&#x2032; is a subcube of <em>T</em>, then <span class="inline-equation"><span class="tex">$v_{T^{\prime }}$</span>     </span> is a heavy hitter in the subcube <em>T</em>&#x2032;.</p>    <p>     <div class="lemma" id="enc4">     <Label>Lemma 3.1.</Label>     <p> For all subcubes <em>T</em>, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \ge \lambda \mbox{ implies }\prod _{i \in \mathcal {V}} f_{T_i} (v_i) \ge \lambda \] </span>        <br/>       </div>      </div> for all <span class="inline-equation"><span class="tex">$\mathcal {V} \subseteq [ \left| T \right|]$</span>      </span> (i.e., <span class="inline-equation"><span class="tex">$\lbrace T_i: i \in \mathcal {V} \rbrace$</span>      </span> is a subcube of <em>T</em>).</p>     </div>    </p>    <p>The proof is trivial since all <span class="inline-equation"><span class="tex">$f_{T_i} (v_i) \le 1$</span>     </span>. Therefore, we have the following corollary.</p>    <p>     <div class="corollary" id="enc5">     <Label>Corollary 3.2.</Label>     <p> For all subcubes <em>T</em>, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ \prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \ge \lambda \mbox{ implies }f_{T_i} (v_i) \ge \lambda \mbox{ for all }i \in [ \left| T \right|] ~. \] </span>        <br/>       </div>      </div>     </p>     </div>    </p>    <p>We therefore only need to compute <em>f<sub>i</sub>     </em>(<em>x</em>) if <em>x</em> is a heavy hitter in coordinate <em>i</em>. To this end, for each coordinate <em>i</em> &#x2208; [<em>d</em>], by using (for example) Misra-Gries algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] or Count-Min sketch [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], we can find a set <em>H<sub>i</sub>     </em> such that if <em>f<sub>i</sub>     </em>(<em>x</em>) &#x2265; <em>&#x03BB;</em>/2, then <em>x</em> &#x2208; <em>H<sub>i</sub>     </em> and if <em>f<sub>i</sub>     </em>(<em>x</em>) < <em>&#x03BB;</em>/4, then <em>x</em>&#x2209;<em>H<sub>i</sub>     </em>. In the second pass, for each <em>x</em> &#x2208; <em>H<sub>i</sub>     </em>, we compute <em>f<sub>i</sub>     </em>(<em>x</em>) exactly to obtain <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ S_i := \lbrace x \in [n]:f_{i}(x) \ge \lambda \rbrace ~. \] </span>      <br/>     </div>     </div>    </p>    <p>We output YES to Query(<em>T</em>, <em>v</em>) if and only if all <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span> and <span class="inline-equation"><span class="tex">$ \prod _{i=1}^{ \left| T \right|} f_{T_i} (v_i) \ge \lambda$</span>     </span>. Note that if <em>v</em> &#x2208; <em>S<sub>i</sub>     </em>, then <em>f<sub>i</sub>     </em>(<em>v</em>) is available to the algorithm since it is computed exactly in the second pass. The detailed algorithm is as follows.</p>    <ol class="list-no-style">     <li id="list7" label="(1)">First pass: For each coordinate <em>i</em> &#x2208; [<em>d</em>], use Misra-Gries algorithm to find <em>H<sub>i</sub>     </em>.<br/></li>     <li id="list8" label="(2)">Second pass: For each coordinate <em>i</em> &#x2208; [<em>d</em>], compute <em>f<sub>i</sub>     </em>(<em>x</em>) exactly for each <em>x</em> &#x2208; <em>H<sub>i</sub>     </em> to obtain <em>S<sub>i</sub>     </em>.<br/></li>     <li id="list9" label="(3)">Output YES to Query(<em>T</em>, <em>v</em>) if and only if <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span> for all <em>i</em> &#x2208; [|<em>T</em>|] and <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \prod _{i=1}^{{ \left| T \right|}} f_{T_i} (v_i) \ge \lambda ~. \] </span>       <br/>      </div>     </div>     <br/></li>    </ol>    <p>     <div class="theorem" id="enc6">     <Label>Theorem 3.3.</Label>     <p> There exists a 2-pass algorithm that uses <span class="inline-equation"><span class="tex">$\tilde{O}(d \gamma ^{-1})$</span>      </span> space and solves subcube heavy hitters under the near-independence assumption. The time to answer Query(<em>T</em>, <em>v</em>) and AllQuery(<em>T</em>) are <span class="inline-equation"><span class="tex">$\tilde{O}(k)$</span>      </span> and <span class="inline-equation"><span class="tex">$\tilde{O}(k \gamma ^{-1})$</span>      </span> respectively where <em>k</em> is the dimensionality of <em>T</em>.</p>     </div>    </p>    <div class="proof" id="proof2">     <Label>Proof.</Label>     <p> The first pass uses <span class="inline-equation"><span class="tex">$\tilde{O}(d \lambda ^{-1})$</span>     </span> space since Misra-Gries algorithm uses <span class="inline-equation"><span class="tex">$\tilde{O}(\lambda ^{-1})$</span>     </span> space for each coordinate <em>i</em> &#x2208; [<em>d</em>]. Since the size of each <em>H<sub>i</sub>     </em> is <em>O</em>(<em>&#x03BB;</em>     <sup>&#x2212; 1</sup>), the second pass also uses <span class="inline-equation"><span class="tex">$\tilde{O}(d \lambda ^{-1})$</span>     </span> space. Recall that <em>&#x03BB;</em> = <em>&#x03B3;</em>/2. We then conclude that the algorithm uses <span class="inline-equation"><span class="tex">$\tilde{O}(d \gamma ^{-1})$</span>     </span> space.</p>     <p>For an arbitrary Query(<em>T</em>, <em>v</em>), the algorithm&#x0027;s correctness follows immediately from Corollary <a class="enc" href="#enc5">3.2</a> and the observation that if <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span>, then <span class="inline-equation"><span class="tex">$f_{T_i} (v_i)$</span>     </span> is available since it was computed exactly in the second pass. Specifically, if <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{align} \prod _{i=1}^{{ \left| T \right|}} f_{T_i} (v_i) \ge \lambda ~, \end{align} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> then <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span> for all <em>i</em> &#x2208; [|<em>T</em>|] and we could verify the inequality and output YES. On the other hand, suppose Eq. <a class="eqn" href="#eq2">2</a> does not hold. Then, if <span class="inline-equation"><span class="tex">$v_i \notin S_{T_i}$</span>     </span> for some <em>i</em>, we correctly output NO. But if all <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span>, then we are able to verify that the inequality does not hold (and correctly output NO).</p>     <p>The parameter <em>k</em> only affects the query time. We now analyze the time to answer Query(<em>T</em>, <em>v</em>) and AllQuery(<em>T</em>) for a <em>k</em>-dimensional subcube <em>T</em>.</p>     <p>We can easily see that Query(<em>T</em>, <em>v</em>) takes <span class="inline-equation"><span class="tex">$\tilde{O}(k)$</span>     </span> time as we need to check if all <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span> (e.g., using binary searches) and compute <span class="inline-equation"><span class="tex">$\prod _{i=1}^k f_{T_i} (v_i)$</span>     </span>.</p>     <p>Next, we exhibit a fast algorithm to answer AllQuery(<em>T</em>). We note that naively checking all combinations (<em>v</em>     <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>v<sub>k</sub>     </em>) in <span class="inline-equation"><span class="tex">$S_{T_1} \times S_{T_2} \times \cdots \times S_{T_k}$</span>     </span> takes exponential <em>&#x03A9;</em>(<em>&#x03B3;</em>     <sup>&#x2212; <em>k</em>     </sup>) time in the worst case.</p>     <p>Our approach figures out the heavy hitters gradually and takes advantage of the near-independence assumption. In particular, define <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ W_j := \lbrace v \in [n]^j : f_{T_1}(v_1) \cdots f_{T_j}(v_j) \ge \lambda \rbrace ~. \] </span>       <br/>      </div>     </div>     </p>     <p>Recall that the goal is to find <em>W<sub>k</sub>     </em>. Note that <em>W</em>     <sub>1</sub> = <em>S</em>     <sub>1</sub> is obtained directly by the algorithm. We now show that it is possible to construct <em>W</em>     <sub>      <em>j</em> + 1</sub> from <em>W<sub>j</sub>     </em> in <span class="inline-equation"><span class="tex">$\tilde{O}(\lambda ^{-1})$</span>     </span> time which in turn means that we can find <em>W<sub>k</sub>     </em> in <span class="inline-equation"><span class="tex">$\tilde{O}(k \lambda ^{-1})$</span>     </span> time. We use the notation <em>T</em>     <sub>[<em>j</em>]</sub> &#x2254; {<em>T</em>     <sub>1</sub>, &#x2026;, <em>T<sub>j</sub>     </em>} and <em>v</em>     <sub>[<em>j</em>]</sub> &#x2254; (<em>v</em>     <sub>1</sub>, <em>v</em>     <sub>2</sub>, &#x2026;, <em>v<sub>j</sub>     </em>).</p>     <p>We note that |<em>W<sub>j</sub>     </em>| &#x2264; 5/(4<em>&#x03BB;</em>). This holds since if <em>y</em> &#x2208; <em>W<sub>j</sub>     </em>, then <span class="inline-equation"><span class="tex">$\prod _{i=1}^j f_{T_i} (y_i) \ge \lambda$</span>     </span>. Appealing to the near-independence assumption, we have <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ f_{{T_{[j]}}} (y) \ge \prod _{i=1}^j f_{T_i} (y_i) -\alpha \ge \lambda -\alpha \ge 4/5 \cdot \lambda ~. \] </span>       <br/>      </div>     </div> For each <em>y</em> &#x2208; <em>W<sub>j</sub>     </em>, we collect all <em>x</em> &#x2208; <em>S</em>     <sub>      <em>j</em> + 1</sub> such that <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \left({ \prod _{i=1}^{j} f_{T_i} (y_i)} \right) f_{T_{j+1}}(x) \ge {\lambda }\end{align*} </span>       <br/>      </div>     </div>     </p>     <p>and put (<em>y</em>     <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>y<sub>j</sub>     </em>, <em>x</em>) into <em>W</em>     <sub>      <em>j</em> + 1</sub>. Since |<em>W<sub>j</sub>     </em>| &#x2264; 5/4 &#x00B7; <em>&#x03BB;</em>     <sup>&#x2212; 1</sup> and |<em>S</em>     <sub>      <em>j</em> + 1</sub>| &#x2264; <em>&#x03BB;</em>     <sup>&#x2212; 1</sup>, this step obviously takes <em>O</em>(<em>&#x03BB;</em>     <sup>&#x2212; 2</sup>) time. However, by observing that there could be at most <span class="inline-equation"><span class="tex">$\lambda ^{-1} \prod _{i=1}^{j} f_{T_i} (y_i)$</span>     </span> such <em>x</em> for each <em>y</em> &#x2208; <em>W<sub>j</sub>     </em>, the upper bound for the number of combinations of <em>x</em> and <em>y</em> is <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \sum _{y \in W_j} \frac{1}{\lambda } \prod _{i=1}^{j} f_{T_i} (y_i) &#x0026; \le \sum _{y \in W_j} \frac{1}{\lambda } (f_{T_{[j]}} (y)+\alpha) \\ &#x0026; = \sum _{y \in W_j} \frac{ \alpha }{\lambda } + \sum _{y \in W_j} \frac{f_{T_{[j]}} (y)}{\lambda } \\ &#x0026; \le \left| W_j \right| + \frac{1}{\lambda } \le \frac{3}{ \lambda } ~.\end{align*} </span>       <br/>      </div>     </div> The last inequality follows from the assumption that <em>&#x03B1;</em> &#x2264; <em>&#x03BB;</em>/5 and <span class="inline-equation"><span class="tex">$\sum _{y \in W_j} f_{T_{[j]}}(y) \le 1$</span>     </span>. Thus, the algorithm can find <em>W</em>     <sub>      <em>j</em> + 1</sub> given <em>W<sub>j</sub>     </em> in <span class="inline-equation"><span class="tex">$\tilde{O}(\lambda ^{-1})$</span>     </span> time. Hence, we obtain <em>W<sub>k</sub>     </em> in <span class="inline-equation"><span class="tex">$\tilde{O}(k \lambda ^{-1}) = \tilde{O}(k \gamma ^{-1})$</span>     </span> time. The correctness of this procedure follows directly from Lemma <a class="enc" href="#enc4">3.1</a> and induction since <em>v</em> = (<em>v</em>     <sub>1</sub>, &#x2026;, <em>v</em>     <sub>      <em>j</em> + 1</sub>) &#x2208; <em>W</em>     <sub>      <em>j</em> + 1</sub> implies that <em>v</em>     <sub>[<em>j</em>]</sub> &#x2208; <em>W<sub>j</sub>     </em> and <em>v</em>     <sub>      <em>j</em> + 1</sub> &#x2208; <em>S</em>     <sub>      <em>j</em> + 1</sub>. Thus, by checking all combinations of <em>y</em> &#x2208; <em>W<sub>j</sub>     </em> and <em>x</em> &#x2208; <em>S</em>     <sub>      <em>j</em> + 1</sub>, we can construct <em>W</em>     <sub>      <em>j</em> + 1</sub> correctly.</p>    </div>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> The Naive Bayes Assumption</h2>     </div>    </header>    <p>     <em>The Naive Bayes assumption.</em> In this section, we focus on the data streams inspired by the Naive Bayes model which is strictly more general than the near-independence assumption. In particular, we assume that the coordinates are near-independent given an extra (<em>d</em> + 1)th observable <em>class coordinate</em> that has a value in {1, &#x2026;, &#x2113;}. The (<em>d</em> + 1)th coordinate is also often referred to as the <em>latent coordinate</em>.</p>    <p>As in typical in Naive Bayes analysis, we assume &#x2113; is a constant but perform the calculations in terms of &#x2113; so its role in the complexity of the problem is apparent.</p>    <p>Informally, this model asserts that the random variables representing coordinates <em>X</em>     <sub>1</sub>, &#x2026;, <em>X<sub>d</sub>     </em> are near independent conditioning on a the random variable <em>X</em>     <sub>     <em>d</em> + 1</sub> that represents the class coordinate.</p>    <p>We introduce the following notation <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{align*} f_{T \:\vert \:d+1} (v \:\vert \:z) &#x0026; := \frac{ \left| \lbrace x_i: x_{i,T} = v \wedge x_{i,\lbrace d+1\rbrace } = z \rbrace \right| }{ \left| \lbrace x_i: x_{i, \lbrace d+1\rbrace } = z \rbrace \right|} \\ &#x0026; = \Pr \left[ X_T = v \:\vert \:X_{d+1}=z \right]~.\end{align*} </span>      <br/>     </div>     </div>    </p>    <p>In other words, <em>f</em>     <sub>     <em>T</em>&#x2009;|&#x2009;<em>d</em> + 1</sub>(<em>v</em>&#x2009;|&#x2009;<em>z</em>) is the frequency of the joint values <em>v</em> in the <em>T</em> coordinates among the stream items where the class coordinate <em>d</em> + 1 has value <em>z</em>.</p>    <p>The formal <em>Naive Bayes</em> assumption is as follows: There exists <em>&#x03B1;</em> &#x2264; <em>&#x03B3;</em>/10 such that for all subcubes <em>T</em>, <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \max _{{{\scriptstyle {\begin{array}{*10c}v \in [n]^{ \left| T \right|}\end{array}}}}} \left| f_{T}(v) - \sum _{z\in [\ell ]}f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \right| {\lt} {\alpha } ~. \] </span>      <br/>     </div>     </div>     <em>Algorithm details.</em> As argued in the previous section, it suffices to output YES to Query(<em>T</em>, <em>v</em>) if and only if <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_{i} \:\vert \:d+1} (v_i \:\vert \:z) \ge \gamma /2 = \lambda ~. \] </span>      <br/>     </div>     </div> However, naively computing all <em>f</em>     <sub>     <em>i</em>&#x2009;|&#x2009;<em>d</em> + 1</sub>(<em>v</em>&#x2009;|&#x2009;<em>z</em>) uses <em>&#x03A9;</em>(&#x2113;<em>dn</em>) space. We circumvent this problem by generalizing Lemma <a class="enc" href="#enc4">3.1</a> as follows. If a joint values <em>v</em> is a heavy hitter in a subcube <em>T</em> in the Naive Bayes formula and <em>T</em>&#x2032; is a subcube of <em>T</em>, then <span class="inline-equation"><span class="tex">$v_{T^{\prime }}$</span>     </span> is a heavy hitter in the subcube <em>T</em>&#x2032;.</p>    <p>     <div class="lemma" id="enc7">     <Label>Lemma 4.1.</Label>     <p> For all subcubes <em>T</em>, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} &#x0026; q(v) := \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \ge \lambda \\ &#x0026; \mbox{ implies }\sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i \in \mathcal {V}} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \ge \lambda\end{align*} </span>        <br/>       </div>      </div> for all <span class="inline-equation"><span class="tex">$\mathcal {V} \subseteq [{ \left| T \right|}]$</span>      </span> (i.e., <span class="inline-equation"><span class="tex">$\lbrace T_i: i \in \mathcal {V} \rbrace$</span>      </span> is a subcube of <em>T</em>).</p>     </div>    </p>    <div class="proof" id="proof3">     <Label>Proof.</Label>     <p> For a fixed <em>z</em>, observe that <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \sum _{y_j \in [n]} f_{T_j \:\vert \:d+1}(y_j \:\vert \:z) = 1 ~.\end{align*} </span>       <br/>      </div>     </div> Suppose <em>q</em>(<em>v</em>) &#x2265; <em>&#x03BB;</em> and consider an arbitrary <span class="inline-equation"><span class="tex">$\mathcal {V} \subseteq [ \left| T \right|]$</span>     </span>. We have <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} &#x0026; \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i \in \mathcal {V}} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \\ = &#x0026; \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i \in \mathcal {V}} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \prod _{j \notin \mathcal {V}} \left(\sum _{y_j \in [n]} f_{T_j \:\vert \:d+1}(y_j \:\vert \:z) \right) \\\ge &#x0026; \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i \in \mathcal {V}} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \prod _{j \notin \mathcal {V}} f_{T_j \:\vert \:d+1}(v_j \:\vert \:z) \\ = &#x0026; \sum _{z \in [\ell ]} ~ f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) = q(v) \ge \lambda ~.\end{align*} </span>       <br/>      </div>     </div> An alternative proof is by noticing that <em>q</em>(<em>v</em>) is a valid probability density function of |<em>T</em>| variables. The claim follows by marginalizing over the the variables that are not in <span class="inline-equation"><span class="tex">$\mathcal {V}$</span>     </span>.</p>    </div>    <p>Setting <span class="inline-equation"><span class="tex">$\mathcal {V} = \lbrace i \rbrace$</span>     </span> for each <em>i</em> &#x2208; [|<em>T</em>|] and appealing to the fact that <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ \sum _{z \in [\ell ]} f_{d+1}(z) f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) = \sum _{z \in [\ell ]} f_{\lbrace T_i,d+1\rbrace }((v_i,z))= f_{T_i} (v_i)~, \] </span>      <br/>     </div>     </div> we deduce the following corollary.</p>    <p>     <div class="corollary" id="enc8">     <Label>Corollary 4.2.</Label>     <p> For all subcubes <em>T</em>, <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} &#x0026; \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \ge \lambda \mbox{ implies }f_{T_i}(v_i) \ge \lambda\end{align*} </span>        <br/>       </div>      </div> for all <em>i</em> &#x2208; [|<em>T</em>|].</p>     </div>    </p>    <p>Therefore, we only need to compute <em>f</em>     <sub>     <em>i</em>&#x2009;|&#x2009;<em>d</em> + 1</sub>(<em>x</em>&#x2009;|&#x2009;<em>z</em>) for all coordinates <em>i</em> &#x2208; [<em>d</em>], values <em>z</em> &#x2208; [&#x2113;] if <em>x</em> is a heavy hitter of coordinate <em>i</em>. Similar to the previous section, for each dimension <em>i</em> &#x2208; [<em>d</em>], we find <em>H<sub>i</sub>     </em> in the first pass and use <em>H<sub>i</sub>     </em> to find <em>S<sub>i</sub>     </em> in the second pass. Appealing to Corrollary <a class="enc" href="#enc8">4.2</a>, we deduce that if <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ q(v) := \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_{i} \:\vert \:d+1} (v_i \:\vert \:z) \ge \lambda \] </span>      <br/>     </div>     </div> then for all <em>i</em> = 1, 2, &#x2026;, |<em>T</em>|, we have <span class="inline-equation"><span class="tex">$ f_{T_i } (v_i) \ge \lambda$</span>     </span> which in turn implies that <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span>. Therefore, we output YES to Query(<em>T</em>, <em>v</em>) if and only if all <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span> and <em>q</em>(<em>v</em>) &#x2265; <em>&#x03BB;</em>.</p>    <p>To this end, we only need to compute <em>f</em>     <sub>     <em>i</em>&#x2009;|&#x2009;<em>d</em> + 1</sub>(<em>x</em>&#x2009;|&#x2009;<em>z</em>) and <em>f</em>     <sub>     <em>d</em> + 1</sub>(<em>z</em>) for all <em>x</em> &#x2208; <em>H<sub>i</sub>     </em>, <em>z</em> &#x2208; [&#x2113;], and <em>i</em> &#x2208; [<em>d</em>]. The detailed algorithm is as follows.</p>    <ol class="list-no-style">     <li id="list10" label="(1)">First pass:<br/>     <ol class="list-no-style">      <li id="list11" label="(a)">For each value <em>z</em> &#x2208; [&#x2113;], compute <em>f</em>       <sub>        <em>d</em> + 1</sub>(<em>z</em>) exactly.<br/></li>      <li id="list12" label="(b)">For each coordinate <em>i</em> &#x2208; [<em>d</em>], use Misra-Gries algorithm to find <em>H<sub>i</sub>       </em>.<br/></li>     </ol></li>     <li id="list13" label="(2)">Second pass:<br/>     <ol class="list-no-style">      <li id="list14" label="(a)">For each coordinate <em>i</em> &#x2208; [<em>d</em>] and each value <em>x</em> &#x2208; <em>H<sub>i</sub>       </em>, compute <em>f<sub>i</sub>       </em>(<em>x</em>) exactly to obtain <em>S<sub>i</sub>       </em>.<br/></li>      <li id="list15" label="(b)">For each value <em>z</em> &#x2208; [&#x2113;], coordinate <em>i</em> &#x2208; [<em>d</em>], and <em>x</em> &#x2208; <em>H<sub>i</sub>       </em>, compute <em>f</em>       <sub>        <em>i</em>&#x2009;|&#x2009;<em>d</em> + 1</sub>(<em>x</em>&#x2009;|&#x2009;<em>z</em>) exactly.<br/></li>     </ol></li>     <li id="list16" label="(3)">Output YES to Query(<em>T</em>, <em>v</em>) if and only if <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span> for all <em>i</em> &#x2208; [|<em>T</em>|] and <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \ge \lambda ~. \] </span>       <br/>      </div>     </div>     <br/></li>    </ol>    <p>     <div class="theorem" id="enc9">     <Label>Theorem 4.3.</Label>     <p> There exists a 2-pass algorithm that uses <span class="inline-equation"><span class="tex">$\tilde{O}(\ell d \gamma ^{-1})$</span>      </span> space and solves subcube heavy hitters under the Naive Bayes assumption. The time to answer Query(<em>T</em>, <em>v</em>) and AllQuery(<em>T</em>) are <span class="inline-equation"><span class="tex">$\tilde{O}(\ell k)$</span>      </span> and <em>O</em>(&#x2113;(<em>k</em>/<em>&#x03B3;</em>)<sup>2</sup>) respectively where <em>k</em> is the dimensionality of <em>T</em>.</p>     </div>    </p>    <div class="proof" id="proof4">     <Label>Proof.</Label>     <p> The space to obtain <em>H<sub>i</sub>     </em> and <em>S<sub>i</sub>     </em> over the two passes is <span class="inline-equation"><span class="tex">$\tilde{O}(d\lambda ^{-1})$</span>     </span>. Additionally, computing <em>f</em>     <sub>      <em>i</em>&#x2009;|&#x2009;<em>d</em> + 1</sub>(<em>x</em>&#x2009;|&#x2009;<em>z</em>) for all <em>i</em> &#x2208; [<em>d</em>], <em>z</em> &#x2208; [&#x2113;], and <em>x</em> &#x2208; <em>H<sub>i</sub>     </em> requires <span class="inline-equation"><span class="tex">$\tilde{O}(\ell d \lambda ^{-1})$</span>     </span> bits of space. The overall space we need is therefore <span class="inline-equation"><span class="tex">$\tilde{O}(\ell d \lambda ^{-1})=\tilde{O}(\ell d \gamma ^{-1})$</span>     </span>.</p>     <p>The correctness of answering an arbitrary Query(<em>T</em>, <em>v</em>) follows directly from Corollary <a class="enc" href="#enc8">4.2</a>. Specifically, if <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{align} \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^{ \left| T \right|} f_{T_i \:\vert \:d+1} (v_i \:\vert \:z) \ge \lambda ~, \end{align} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> then, <span class="inline-equation"><span class="tex">$v_i \in S_{T_i} \subseteq H_{T_i}$</span>     </span> for all <em>i</em> &#x2208; [|<em>T</em>|] as argued. Hence, <span class="inline-equation"><span class="tex">$f_{T_i \:\vert \:d+1} (v_i \:\vert \:z)$</span>     </span> is computed exactly in the second pass for all <em>z</em> &#x2208; [&#x2113;]. As a result, we could verify the inequality and output YES. On the other hand, if Eq. <a class="eqn" href="#eq3">3</a> does not hold. Then, if some <span class="inline-equation"><span class="tex">$v_i \notin S_{T_i}$</span>     </span>, we will correctly output NO. Otherwise if all <span class="inline-equation"><span class="tex">$v_i \in S_{T_i}$</span>     </span>, then we can compute the left hand side and verify that Eq. <a class="eqn" href="#eq3">3</a> does not hold (and correctly output NO).</p>     <p>Obviously, Query(<em>T</em>, <em>v</em>) takes <span class="inline-equation"><span class="tex">$\tilde{O}(\ell k)$</span>     </span> time for a <em>k</em>-dimensional subcube <em>T</em>. We now exhibit a fast algorithm to answer AllQuery(<em>T</em>) for a <em>k</em>-dimensional subcube <em>T</em>. Define <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ W_j := \lbrace v \in [n]^j : \sum _{z \in [\ell ]} f_{d+1}(z) \prod _{i=1}^j f_{T_i \:\vert \:d+1}(v_i \:\vert \:z) \ge \lambda \rbrace ~. \] </span>       <br/>      </div>     </div> Recall that the goal is to find <em>W<sub>k</sub>     </em>. We note that <em>W</em>     <sub>1</sub> = <em>S</em>     <sub>1</sub> is obtained directly by the algorithm. Next, we show how to obtain <em>W</em>     <sub>      <em>j</em> + 1</sub> in <span class="inline-equation"><span class="tex">$\tilde{O}(\lambda ^{-2})$</span>     </span> time from <em>W<sub>j</sub>     </em>. Note that |<em>W<sub>j</sub>     </em>| &#x2264; 5/(4<em>&#x03BB;</em>) because if <em>y</em> &#x2208; <em>W<sub>j</sub>     </em>, then <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \sum _{z \in [\ell ]} f_{T_1 \:\vert \:d+1}(y_1 \:\vert \:z) \cdots f_{T_j \:\vert \:d+1}(y_j \:\vert \:z) f_{d+1}(z) \ge \lambda \\\end{align*} </span>       <br/>      </div>     </div> and hence <span class="inline-equation"><span class="tex">$f_{{T_{[j]}}} (y) \ge \lambda -\alpha = 4/5 \cdot \lambda ^{-1}$</span>     </span> according to the Naive Bayes assumption. This implies that |<em>W<sub>j</sub>     </em>| &#x2264; 5/(4<em>&#x03BB;</em>).</p>     <p>For each (<em>v</em>     <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>v<sub>j</sub>     </em>) in <em>W<sub>j</sub>     </em>, we collect all <em>v</em>     <sub>      <em>j</em> + 1</sub> &#x2208; <em>S</em>     <sub>      <em>j</em> + 1</sub> such that <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \sum _{z \in [\ell ]} f_{T_1 \:\vert \:d+1}(v_1 \:\vert \:z) \cdots f_{T_{j+1} \:\vert \:d+1}(v_{j+1} \:\vert \:z) f_{d+1}(z) \ge \lambda \] </span>       <br/>      </div>     </div> and put (<em>v</em>     <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>v</em>     <sub>      <em>j</em> + 1</sub>) to <em>W</em>     <sub>      <em>j</em> + 1</sub>. Since |<em>W<sub>j</sub>     </em>| &#x2264; 5/(4<em>&#x03BB;</em>) and |<em>S</em>     <sub>      <em>j</em> + 1</sub>| &#x2264; 1/<em>&#x03BB;</em>, this step obviously takes <span class="inline-equation"><span class="tex">$\tilde{O}(\ell k \lambda ^{-2})$</span>     </span> time. Since we need to do this for <em>j</em> = 2, 3, &#x2026;, <em>k</em>, we attain <em>W<sub>k</sub>     </em> in <span class="inline-equation"><span class="tex">$\tilde{O}(\ell (k/\gamma)^{2})$</span>     </span> time. The correctness of this procedure follows directly from Lemma <a class="enc" href="#enc7">4.1</a> and induction since (<em>v</em>     <sub>1</sub>, &#x2026;, <em>v</em>     <sub>      <em>j</em> + 1</sub>) &#x2208; <em>W</em>     <sub>      <em>j</em> + 1</sub> implies that (<em>v</em>     <sub>1</sub>, &#x2026;, <em>v<sub>j</sub>     </em>) is in <em>W<sub>j</sub>     </em> and <em>v</em>     <sub>      <em>j</em> + 1</sub> is in <em>S</em>     <sub>      <em>j</em> + 1</sub>. Since we check all possible combinations of (<em>v</em>     <sub>1</sub>, &#x2026;, <em>v<sub>j</sub>     </em>) &#x2208; <em>W<sub>j</sub>     </em> and <em>v</em>     <sub>      <em>j</em> + 1</sub> &#x2208; <em>S</em>     <sub>      <em>j</em> + 1</sub>, we guarantee to construct <em>W</em>     <sub>      <em>j</em> + 1</sub> correctly.</p>    </div>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Experimental study</h2>     </div>    </header>    <p>     <em>Overview.</em> We experiment with our algorithms on a synthetic dataset generated from a Naive Bayes model, and two real-world datasets from Adobe Marketing Cloud<a class="fn" href="#fn4" id="foot-fn4"><sup>2</sup></a> and Yandex. We thoroughly compare the following approaches:</p>    <ul class="list-no-style">     <li id="list17" label="&#x2022;">The sampling method (<span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span>) in Section <a class="sec" href="#sec-6">2</a>.<br/></li>     <li id="list18" label="&#x2022;">The 2-pass algorithms (<span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span>) described in Section <a class="sec" href="#sec-7">3</a> and <a class="sec" href="#sec-8">4</a> depending on the context of the experiment.<br/></li>     <li id="list19" label="&#x2022;">The Count-Min sketch heuristic (<span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span>): this heuristic uses Count-Min sketch&#x0027;s point query estimation (see [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]) to estimate the frequencies given by the near-independence formula (instead of making a second pass through the stream to compute their exact values). We note that this approach has no theoretical guarantee.<br/></li>    </ul>    <p>We highlight the main differences between the theoretical algorithms in Sections <a class="sec" href="#sec-7">3</a> and <a class="sec" href="#sec-8">4</a> and the actual implementation:</p>    <ul class="list-no-style">     <li id="list20" label="&#x2022;">Instead of running our algorithms with the theoretical memory bounds, we run and compare them for different memory limits. This approach is more practical and natural from the implementation perspective.<br/></li>     <li id="list21" label="&#x2022;">In theory, <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> use a fixed threshold <em>&#x03B3;</em>     <sup>*</sup> = <em>&#x03B3;</em>/2 to decide between outputting YES or NO. We however experiment with different values of <em>&#x03B3;</em>     <sup>*</sup> which is helpful when the memory is more limited or when the assumptions are not perfect in real data.<br/></li>    </ul>    <p>The heavy hitters threshold <em>&#x03B3;</em> is carefully chosen so that the proportion of the number of heavy hitters to the total number joint values to be reasonably small, i.e., approximately at most <span class="inline-equation"><span class="tex">$ 1\%$</span>     </span> in this paper. Therefore, we use different values of <em>&#x03B3;</em> for each dataset (see Table <a class="tbl" href="#tab1">1</a> for the actual parameter values).</p>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Synthetic dataset</h3>     </div>     </header>     <p>The synthetic dataset is sampled from a pre-trained Naive Bayes model that is used to estimate the probability of a page view. The model was provided by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] and built on the same Clickstream dataset that we used in Section <a class="sec" href="#sec-11">5.2</a>. The coordinates consist of one class variable <em>Z</em> and five feature variables (<em>X</em>     <sub>1</sub>, &#x2026;, <em>X</em>     <sub>5</sub>) with high cardinalities. The dataset strongly follows the property that <em>X</em>     <sub>1</sub>, &#x2026;, <em>X</em>     <sub>5</sub> are conditionally independent given <em>Z</em>. Specifically, the variables and their corresponding approximate cardinalities are: country (7), city (10,500), page name (8,500), starting page name (6,400), campaign (3,500), browser (300) where country is the class variable.</p>     <p>     <em>Warm up experiment</em>. We first evaluate <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span> on this synthetic dataset. As mentioned earlier, we compare the performance of the two approaches for each fixed memory size.<a class="fn" href="#fn5" id="foot-fn5"><sup>3</sup></a> We take a subset of approximately 135,000 records conditioned on a fixed and most frequent value of <em>Z</em> so that <em>X</em>     <sub>1</sub>, &#x2026;, <em>X</em>     <sub>5</sub> are independent in this subset. We then run experiments on three different subcubes: {<em>X</em>     <sub>1</sub>, <em>X</em>     <sub>2</sub>, <em>X</em>     <sub>3</sub>}, {<em>X</em>     <sub>2</sub>, <em>X</em>     <sub>3</sub>, <em>X</em>     <sub>4</sub>}, and {<em>X</em>     <sub>3</sub>, <em>X</em>     <sub>4</sub>, <em>X</em>     <sub>5</sub>}. In this warm up experiment, the main goal is not to find the heavy hitters but to compare the accuracy of the heavy hitters frequency estimations given by <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span>. We measure the performance via the mean square error (MSE), the mean absolute error (MAE), and the mean absolute percentage error (MAPE). To do this, the true frequencies were pre-computed. We use the frequencies of the top 10 heavy hitters in each of the above subcubes. The results (see Figure <a class="fig" href="#fig1">1</a>) indicate that <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span> outperforms <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> when restricted to small memory. This warm up experiment gives evidence that knowing the underlying distribution structure helps improving small space heuristic&#x0027;s performance in estimating the heavy hitters frequencies. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186082/images/www2018-91-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">Warm up experiment on synthetic data. Memory size ranges from 0.1% to 10% of data size. We report the error as a function of memory size.</span>      </div>     </figure>     </p>     <p>     <em>Experiment with the near-independence assumption.</em> We compare performance of the three aforementioned methods on finding heavy hitters under the near-independence assumption. In this experiment, we use the same subset of data and subcubes as in the previous experiment. We fix the memory to be 2% of data size.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Parameter values for each experiment. (The columns correspond to memory size relative to the dataset, number of the experimented subcubes, average number of heavy hitters, average percentage of heavy hitters.)</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">Dataset</th>        <th style="text-align:center;">Mem.</th>        <th style="text-align:center;">#Subcubes</th>        <th style="text-align:center;">        <em>&#x03B3;</em>        </th>        <th style="text-align:center;">#HH</th>        <th style="text-align:center;">HH ratio</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:center;">Synthetic (fixed <em>Z</em>)</td>        <td style="text-align:center;">2%</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">0.002</td>        <td style="text-align:center;">29.7</td>        <td style="text-align:center;">0.079%</td>       </tr>       <tr>        <td style="text-align:center;">Synthetic (whole)</td>        <td style="text-align:center;">2%</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">0.002</td>        <td style="text-align:center;">28.7</td>        <td style="text-align:center;">0.054%</td>       </tr>       <tr>        <td style="text-align:center;">Clickstream</td>        <td style="text-align:center;">10%</td>        <td style="text-align:center;">4</td>        <td style="text-align:center;">0.002</td>        <td style="text-align:center;">42.0</td>        <td style="text-align:center;">0.165%</td>       </tr>       <tr>        <td style="text-align:center;">Yandex</td>        <td style="text-align:center;">0.2%</td>        <td style="text-align:center;">8</td>        <td style="text-align:center;">0.1</td>        <td style="text-align:center;">2.2</td>        <td style="text-align:center;">1.51 %</td>       </tr>      </tbody>     </table>     </div>     <p>We measure the performance, for different values of <em>&#x03B3;</em>     <sup>*</sup>, based on the number of true positives and false positives. As shown in Figure <a class="fig" href="#fig2">2</a>, for small memory, both <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> manage to find more heavy hitters than <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span>. In terms of false positives, <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> beats both <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> for smaller space. One possible explanation is that when <em>&#x03B3;</em>     <sup>*</sup> is small (close to <em>&#x03B3;</em>), <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span>, with the advantage of the second pass, accurately estimates frequencies of potential heavy hitters whereas the other two methods, especially <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span>, overestimate the frequencies and therefore report more false positives. For larger <em>&#x03B3;</em>     <sup>*</sup>, false positives become less likely and all three approaches achieve similar performances. In general, <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> obtains the best performance as seen in the ROC curve. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186082/images/www2018-91-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Near-independence experiment on synthetic dataset. We measure the performance based on the number of true and false positives (as a function of <em>&#x03B3;</em>        <sup>*</sup>), and the ROC curve.</span>      </div>     </figure>     </p>     <p>     <em>Experiment with the Naive Bayes assumption</em>. We use the whole dataset of approximately 168,000 records without fixing <em>Z</em> and keep other settings unchanged. We only compared the performance of <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> because the conditional probabilities cannot be directly derived from <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span>. In Figure <a class="fig" href="#fig3">3</a>, we observe that when restricted to small memory, <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> attains a better performance by reporting more true heavy hitters and fewer false heavy hitters. As we allow more space, the performance of <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> improves as predicted by our theoretical analysis. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186082/images/www2018-91-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Naive Bayes experiment on synthetic dataset.</span>      </div>     </figure>     </p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Clickstream dataset</h3>     </div>     </header>     <p>To evaluate <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> on real data, we use an advertising dataset called <em>Clickstream Data Feeds</em> from Adobe Marketing Cloud. The approximate dataset size is 168,000 and all values have been anonymized in advance.</p>     <p>There are 19 high cardinality variables grouped by categories as follows: geography info (city, region, country, domain, carrier), page info (page name, start page name, first-hit page name), search info (visit number, referrer, campaign, keywords, search engine), external info (browser, browser width/height, plugins, language, OS).</p>     <p>We avoid obvious correlated features in the query subcubes, e.g., &#x201C;search engine&#x201D; and &#x201C;keywords&#x201D; are highly correlated. For example, some highly correlated variables and their correlations are: start page name &#x0026; first-hit page name (0.67), browser &#x0026; OS (0.40), region &#x0026; country (0.32), search engine &#x0026; country (0.27).</p>     <p>We carefully select a subset of coordinates that may follow the near independence assumption to query on. For instance, we show our experiment results for the following subcubes, along with the number of heavy hitters recorded: {region, page name , language}, {region, campaign, plugins}, {carrier, first-hit page name, plugins}, {carrier, keywords, OS }.</p>     <p>Since strong independence property is not guaranteed in this real dataset, we increase memory size to 10% of the data size in order to obtain better estimation for all methods. Recall that the memory used by <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> is partially determined by the number of dimensions and therefore it is reasonable to use a relatively larger memory size.</p>     <p>In this experiment, all three algorithms are able to find most true heavy hitters (see Figure <a class="fig" href="#fig4">4</a>), but <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> returns far fewer false positives than the other two methods when <em>&#x03B3;</em>     <sup>*</sup> is small. In addition, <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> reaches zero false positive for reasonably large <em>&#x03B3;</em>     <sup>*</sup>. We can see in the ROC curve that <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> performs slightly better than <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>     </span> and much better than <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> for small space. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186082/images/www2018-91-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Near-independence experiment with Clickstream dataset.</span>      </div>     </figure>     <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186082/images/www2018-91-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Naive Bayes experiment with Yandex dataset.</span>      </div>     </figure>     </p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Yandex dataset</h3>     </div>     </header>     <p>Finally, we consider the <em>Yandex dataset</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] which is a web search dataset (with more than 167 millions data points). Each record in the dataset contains a query ID, the corresponding date, the list of 10 displayed items, and the corresponding click indicators of each displayed item. In the pre-processing step, we extracted 10 subsets from the whole dataset according to top 10 frequent user queries. The sizes of subsets range from 61,000 to 454,000. In each subset, we treat the first 10 search results as variables <em>X</em>     <sub>1</sub>, .., <em>X</em>     <sub>10</sub> and &#x201C;day of query&#x201D; as the latent variable <em>Z</em>. We conjecture that the search results <em>X</em>     <sub>1</sub>, .., <em>X</em>     <sub>10</sub> are approximately independent conditioned on a given day <em>Z</em>. We observe that web patterns typically experience heavy weekly seasonality and these search results largely depend on user query time for some fixed query. We proceed to evaluate <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> under the Naive Bayes assumption on this dataset.</p>     <p>We consider 8 subcubes in the form {<em>X<sub>i</sub>     </em>, <em>X</em>     <sub>      <em>i</em> + 1</sub>, <em>X</em>     <sub>      <em>i</em> + 2</sub>} and deliberately set a smaller memory size for this experiment because the cardinality of this dataset is relatively low. We note that different subsets of data may have different number of heavy hitters, so we take the average over 10 subsets as the final result.</p>     <p>We report the results in Figure <a class="fig" href="#fig5">5</a>. We observe that both <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> are able to find most true heavy hitters. However, <span class="inline-equation"><span class="tex">$\mathsf { TwoPassAlg}$</span>     </span> performs significantly better in terms of false positives.</p>    </section>   </section>   <section id="sec-13">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Concluding Remarks</h2>     </div>    </header>    <p>Our work demonstrates the power of model-based approach for analyzing high dimensional data that abounds in digital analytics applications. We exhibit algorithms, with fast query time, that overcome worst case space lower bound under the classical Naive Bayes assumption. Our approach to subspace heavy hitters opens several directions for further study. For example,</p>    <ul class="list-no-style">     <li id="list22" label="&#x2022;">Can heavy hitters be detected efficiently under more general models?<br/></li>     <li id="list23" label="&#x2022;">Can these models be learned or fitted over data streams with polylogarithmic space? We believe this is an algorithmic problem of great interest and will have applications in machine learning beyond the context here.<br/></li>     <li id="list24" label="&#x2022;">We assumed that we observe the latent dimension. Can this be learned from the data stream?<br/></li>     <li id="list25" label="&#x2022;">Can the model-based approach be extended to other problems besides heavy hitters, including clustering, anomaly detection, geometric problems and others which have been studied in the streaming literature.</li>    </ul>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Ion Androutsopoulos, Georgios Paliouras, Vangelis Karkaletsis, Georgios Sakkis, Constantine&#x00A0;D. Spyropoulos, and Panagiotis Stamatopoulos. 2000. Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a Memory-Based Approach. <em>      <em>CoRR</em>     </em>cs.CL/0009009(2000).</li>     <li id="BibPLXBIB0002" label="[2]">Vladimir Braverman, Kai-Min Chung, Zhenming Liu, Michael Mitzenmacher, and Rafail Ostrovsky. 2010. AMS Without 4-Wise Independence on Product Domains. In <em>      <em>STACS</em>     </em>(LIPIcs), Vol.&#x00A0;5. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 119&#x2013;130.</li>     <li id="BibPLXBIB0003" label="[3]">Vladimir Braverman and Rafail Ostrovsky. 2010. Measuring independence of datasets. In <em>      <em>STOC</em>     </em>. ACM, 271&#x2013;280.</li>     <li id="BibPLXBIB0004" label="[4]">Graham Cormode. 2008. Finding Frequent Items in Data Streams. <a href="http://dmac.rutgers.edu/Workshops/WGUnifyingTheory/Slides/cormode.pdf" target="_blank">http://dmac.rutgers.edu/Workshops/WGUnifyingTheory/Slides/cormode.pdf</a>. (2008). DIMACS Workshop.</li>     <li id="BibPLXBIB0005" label="[5]">Graham Cormode and S. Muthukrishnan. 2004. An Improved Data Stream Summary: The Count-Min Sketch and Its Applications. In <em>      <em>LATIN</em>     </em>(Lecture Notes in Computer Science), Vol.&#x00A0;2976. Springer, 29&#x2013;38.</li>     <li id="BibPLXBIB0006" label="[6]">Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. 2016. Testing Ising Models. <em>      <em>CoRR</em>     </em>abs/1612.03147(2016). arxiv:<a href="http://arxiv.org/abs/1612.03147" target="_blank">1612.03147</a> <a class="link-inline force-break" href="http://arxiv.org/abs/1612.03147"      target="_blank">http://arxiv.org/abs/1612.03147</a></li>     <li id="BibPLXBIB0007" label="[7]">Marco&#x00A0;F. Duarte, Volkan Cevher, and Richard&#x00A0;G. Baraniuk. 2009. Model-based compressive sensing for signal ensembles. In <em>      <em>Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on</em>     </em>. IEEE, 244&#x2013;250.</li>     <li id="BibPLXBIB0008" label="[8]">Amit Goyal, Hal&#x00A0;Daum&#x00E9; III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language Modeling. In <em>      <em>Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, May 31 - June 5, 2009, Boulder, Colorado, USA</em>     </em>. The Association for Computational Linguistics, 512&#x2013;520. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/N09-1058"      target="_blank">http://www.aclweb.org/anthology/N09-1058</a></li>     <li id="BibPLXBIB0009" label="[9]">Matthew Hamilton, Rhonda Chaytor, and Todd Wareham. 2006. The Parameterized Complexity of Enumerating Frequent Itemsets. In <em>      <em>IWPEC</em>     </em>(Lecture Notes in Computer Science), Vol.&#x00A0;4169. Springer, 227&#x2013;238.</li>     <li id="BibPLXBIB0010" label="[10]">Piotr Indyk and Andrew McGregor. 2008. Declaring independence via the sketching of sketches. In <em>      <em>SODA</em>     </em>. SIAM, 737&#x2013;745.</li>     <li id="BibPLXBIB0011" label="[11]">Branislav Kveton, Hung&#x00A0;Hai Bui, Mohammad Ghavamzadeh, Georgios Theocharous, S. Muthukrishnan, and Siqi Sun. 2016. Graphical Model Sketch. In <em>      <em>ECML/PKDD (1)</em>     </em>(Lecture Notes in Computer Science), Vol.&#x00A0;9851. Springer, 81&#x2013;97.</li>     <li id="BibPLXBIB0012" label="[12]">David&#x00A0;D. Lewis, Yiming Yang, Tony&#x00A0;G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. <em>      <em>Journal of Machine Learning Research</em>     </em>5 (2004), 361&#x2013;397.</li>     <li id="BibPLXBIB0013" label="[13]">Edo Liberty, Michael Mitzenmacher, Justin Thaler, and Jonathan Ullman. 2016. Space Lower Bounds for Itemset Frequency Sketches. In <em>      <em>PODS</em>     </em>. ACM, 441&#x2013;454.</li>     <li id="BibPLXBIB0014" label="[14]">Christopher&#x00A0;D. Manning, Prabhakar Raghavan, and Hinrich Sch&#x00FC;tze. 2008. <em>      <em>Introduction to Information Retrieval</em>     </em>. Cambridge University Press.</li>     <li id="BibPLXBIB0015" label="[15]">Andrew McGregor and Hoa&#x00A0;T. Vu. 2015. Evaluating Bayesian Networks via Data Streams. In <em>      <em>COCOON</em>     </em>(Lecture Notes in Computer Science), Vol.&#x00A0;9198. Springer, 731&#x2013;743.</li>     <li id="BibPLXBIB0016" label="[16]">Jayadev Misra and David Gries. 1982. Finding Repeated Elements. <em>      <em>Sci. Comput. Program.</em>     </em>2, 2 (1982), 143&#x2013;152.</li>     <li id="BibPLXBIB0017" label="[17]">Stuart&#x00A0;J. Russell and Peter Norvig. 2010. <em>      <em>Artificial Intelligence - A Modern Approach</em>     </em>. Pearson Education.</li>     <li id="BibPLXBIB0018" label="[18]">Jeffrey&#x00A0;Scott Vitter. 1985. Random Sampling with a Reservoir. <em>      <em>ACM Trans. Math. Softw.</em>     </em>11, 1 (1985), 37&#x2013;57.</li>     <li id="BibPLXBIB0019" label="[19]">David&#x00A0;P. Woodruff. 2016. New Algorithms for Heavy Hitters in Data Streams (Invited Talk). In <em>      <em>ICDT</em>     </em>(LIPIcs), Vol.&#x00A0;48. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 4:1&#x2013;4:12.</li>     <li id="BibPLXBIB0020" label="[20]">Yandex2013. Yandex Personalized Web Search Challenge. <a href="https://www.kaggle.com/c/yandex-personalized-web-search-challenge" target="_blank">https://www.kaggle.com/c/yandex-personalized-web-search-challenge</a>. (2013).</li>     <li id="BibPLXBIB0021" label="[21]">Guizhen Yang. 2004. The complexity of mining maximal frequent itemsets and maximal frequent patterns. In <em>      <em>KDD</em>     </em>. ACM, 344&#x2013;353.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>The authors are listed alphabetically.</p>   <p id="fn2"><a href="#foot-fn2"><sup>&#x2020;</sup></a>Part of this work was done at Adobe Research.</p>   <p id="fn3"><a href="#foot-fn3"><sup>1</sup></a>The gap constant 4 can be narrowed arbitrarily and the success probability can be amplified to 1 &#x2212; <em>&#x03B4;</em> as needed, and we omit these factors in the discussions.</p>   <p id="fn4"><a href="#foot-fn4"><sup>2</sup></a><a class="link-inline force-break"     href="http://www.adobe.com/marketing-cloud.html">http://www.adobe.com/marketing-cloud.html</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>3</sup></a>We compute the memory use by the <span class="inline-equation"><span class="tex">$\mathsf { Sampling}$</span>    </span> as the product of dimension and the sample size. The memory used by <span class="inline-equation"><span class="tex">$\mathsf { Heuristic}$</span>    </span> is computed as the product of dimension and the Count-Min sketch&#x0027;s size.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186082">https://doi.org/10.1145/3178876.3186082</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
