<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Iterative Knowledge Extraction from Social Networks</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
<link rel="cite-as" href="https://doi.org/10.1145/3184558.3191578"/></head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191578'>https://doi.org/10.1145/3184558.3191578</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191578'>https://w3id.org/oa/10.1145/3184558.3191578</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Iterative Knowledge Extraction from Social Networks</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Marco</span> <span class="surName">Brambilla</span>, Politecnico di Milano. Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5Milan, Italy 20133
        </div>
        <div class="author">
          <span class="givenName">Stefano</span> <span class="surName">Ceri</span>, Politecnico di Milano. Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5Milan, Italy 20133
        </div>
        <div class="author">
          <span class="givenName">Florian</span> <span class="surName">Daniel</span>, Politecnico di Milano. Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5Milan, Italy 20133
        </div>
        <div class="author">
          <span class="givenName">Marco Di</span> <span class="surName">Giovanni</span>, Politecnico di Milano. Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5Milan, Italy 20133
        </div>
        <div class="author">
          <span class="givenName">Andrea</span> <span class="surName">Mauri</span>, Politecnico di Milano. Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5Milan, Italy 20133
        </div>
        <div class="author">
          <a href="https://orcid.org/0000-0002-8753-2434" ref="author"><span class="givenName">Giorgia</span> <span class="surName">Ramponi</span></a>, Politecnico di Milano. Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5Milan, Italy 20133, <a href="mailto:[firstname].[lastname]@polimi.it">[firstname].[lastname]@polimi.it</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191578" target="_blank">https://doi.org/10.1145/3184558.3191578</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Knowledge in the world continuously evolves, and ontologies are largely incomplete, especially regarding data belonging to the so-called long tail. We propose a method for discovering emerging knowledge by extracting it from social content. Once initialized by domain experts, the method is capable of finding emerging entities by means of a mixed syntactic-semantic method. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors built by using terms occurring in their social content and ranks the candidates by using their distance from the centroid of seeds, returning the top candidates. Our method can run continuously or with periodic iterations, using the results as new seeds. In this paper we address the following research questions: (1) How does reconstructed domain knowledge evolve if the candidates of one extraction are recursively used as seeds? (2) How does the reconstructed domain knowledge spread geographically? (3) Can the method be used to inspect the past, present, and future of knowledge? (4) Can the method be used to find emerging knowledge?</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Web searching and information discovery;</strong> <strong>Web mining;</strong> <strong>Document representation;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Social media analysis</small>,</span> <span class="keyword"><small>knowledge extraction</small>,</span> <span class="keyword"><small>domain model</small>,</span> <span class="keyword"><small>Twitter</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Marco Brambilla, Stefano Ceri, Florian Daniel, Marco Di Giovanni, Andrea Mauri, and Giorgia Ramponi. 2018. Iterative Knowledge Extraction from Social Networks. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 6 Pages. <a href="https://doi.org/10.1145/3184558.3191578" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3191578</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-2">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Massive technologies have been used recently to produce very large ontologies: DBpedia, YAGO, the Knowledge Graphs in Google and Facebook derive from structured or semi-structured curated data [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]. However, knowledge in the world continuously evolves: new entities continuously emerge, and existing ones change their properties or become obsolete. In many cases, these new entities belong to the so-called <em>long tail</em>, i.e. the portion of the entity's distribution having fewer occurrences [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], and as such they are not included within knowledge graphs.</p>
      <p>For discovering knowledge and its evolution, we can take advantage of an extremely powerful and massive source: the content produced on social media. One can conjecture that somewhere, within such a massive content, any entity (and its evolution) has left some traces. The problem is that such traces are unclassified, dispersed, disorganized, uncertain, partial, possibly incorrect. Therefore, deriving information about entities from social content is extremely difficult.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191578/images/www18companion-317-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Iterative knowledge extraction process.</span>
        </div>
      </figure>
      <p></p>
      <p>In this paper we propose an iterative knowledge extraction method for discovering knowledge by extracting it from social content. The method is the evolution and extension of our research presented at WWW 2017 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] and is defined in the context of a broader vision on knowledge discovery, whose general framework is illustrated in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>]. We use <strong>Twitter</strong> as social content source; Twitter can be accessed via its public APIs, which extract tweets related to a given hashtag or Twitter account. We refer to <strong>DBpedia</strong> as generic source of ontological knowledge; DBpedia is publicly available through its open API. DBpedia <em>types</em> are used to partition the existing ontological knowledge, organized within a type hierarchy.</p>
      <p>The domain of interest is described by a selection of DBpedia types. This selection is performed by domain experts and typically includes few (from 5 to 10) types. We find entities within such domain, by extracting them from the social content. The expert must also provide <strong>seeds</strong>, i.e. prototypes of the interesting entities, simply described by providing their twitter accounts. A small set of seeds is sufficient: we normally use 10 to 20 seeds.</p>
      <p>Once initialized by domain experts, the method is capable of finding entities by means of a mix of syntactic and semantic techniques. Our method collects information from the seed's tweets and generates <strong>candidates</strong>, i.e. other twitter accounts which are mentioned within the extracted tweets; then, it associates each candidate to a <strong>feature vector</strong>, built by using terms occurring in their social content, giving more relevance to terms which match the types selected by the expert; then it associates each candidate to a <strong>score</strong>, equivalent to the distance of each candidate from the centroid of the seeds; finally, it returns the top candidates, listed in decreasing score order. Once the candidates are generated, they can be forwarded to a <strong>crowd of evaluators</strong> that can assess the correctness of the extraction. Furthermore, the user can select a subset of candidates and reuse them as new seeds in a new execution of the knowledge extraction process.</p>
      <p>In this paper we study how the method captures <em>evolving knowledge</em>; this is a crucial aspect, as the method can be repeatedly applied in an iterative manner over the social content to capture new trends or to track knowledge spreading and evolution. We answer the following questions:</p>
      <ul class="list-no-style">
        <li id="uid2" label="RQ1:"><em>How does reconstructed domain knowledge evolve if the candidates of one extraction are iteratively used as seeds for the next extraction?</em><br /></li>
        <li id="uid3" label="RQ2:"><em>How does the reconstructed domain knowledge spread geographically?</em><br /></li>
        <li id="uid4" label="RQ3:"><em>Can the method be used to inspect the past, present, and future of knowledge?</em><br /></li>
        <li id="uid5" label="RQ4:"><em>Can the method be used to find emerging knowledge?</em><br /></li>
      </ul>
      <p>The paper is organized as follows: Section 2 describes our iterative knowledge extraction process; Section 3 presents the seven domains of interest over which we experiment the approach; Section 4 describes the four usage scenarios that respond to the above research questions and show the method at work on eacho of them; Section 5 describes our implementation; Section 6 discusses the related work; and Section 7 concludes.</p>
    </section>
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Iterative Extraction Process</h2>
        </div>
      </header>
      <p>The <strong>knowledge extraction process</strong> we propose is reported in Fig. <a class="fig" href="#fig1">1</a>:</p>
      <ol class="list-no-style">
        <li id="list1" label="(1)"><em>The user submits a set of seeds</em> in input as samples of concepts to search for. These seeds consist of Twitter handles (usernames);<br /></li>
        <li id="list2" label="(2)"><em>The user submits a set of expert types</em> as descriptors of the domain of interest.<br /></li>
        <li id="list3" label="(3)">
          <em>The extraction of new candidates</em> is then launched and proceeds as follows:<br />
          <ol class="list-no-style">
            <li id="list4" label="(1)">Elimination from the seeds of outliers according to principal component analysis and computation of the centroid of the filtered seeds;<br /></li>
            <li id="list5" label="(2)">Collection of all the posts of each seed;<br /></li>
            <li id="list6" label="(3)">Definition of the set of candidate new entities as all the user handles that are mentioned by the seeds (which may lead to several thousand candidates);<br /></li>
            <li id="list7" label="(4)">Filter of candidates based on <em>tf-df</em> similarity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], which allows one to reduce the space of analysis of the candidates to a limited set of relevant ones;<br />
            </li>
            <li id="list8" label="(5)">Collection of all the posts of each such candidate;<br /></li>
            <li id="list9" label="(6)">Computation of the feature vector representing each candidate;<br /></li>
            <li id="list10" label="(7)">Rank of the candidates based on the vectorial distance from the seed centroid and production of the result based upon the ranking.<br /></li>
          </ol>
        </li>
        <li id="list11" label="(4)">Once the candidates are retrieved and ranked, the user can:<br />
          <ol class="list-no-style">
            <li id="list12" label="(1)"><em>Export</em> them (in CSV format for human consumption or data analysis purpose or in RDF format for further integration in existing semantic knowledge bases);<br /></li>
            <li id="list13" label="(2)">Forward them to <em>domain experts or a generic crowd for result evaluation</em>purposes (validation);<br /></li>
            <li id="list14" label="(3)">Use them (or a subset of them) as new seeds and <em>iterate the whole pipeline</em>.<br /></li>
          </ol>
        </li>
      </ol>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class="table-title">Precision @10 and @20 of iterative knowledge extraction experiments using candidates produced in one run as seeds of a consecutive run; #results are the overall identified candidates.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"><strong>RUN #1</strong></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"><strong>RUN #2</strong></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"></th>
              <th style="text-align:left;"><strong>RUN #3</strong></th>
              <th></th>
            </tr>
            <tr>
              <th style="text-align:left;"><strong>SCENARIO</strong></th>
              <th style="text-align:left;">#seeds</th>
              <th style="text-align:left;">#results</th>
              <th style="text-align:left;">Pre@10</th>
              <th style="text-align:left;">Pre@20</th>
              <th style="text-align:left;">#seeds</th>
              <th style="text-align:left;">#results</th>
              <th style="text-align:left;">Pre@10</th>
              <th style="text-align:left;">Pre@20</th>
              <th style="text-align:left;">#seeds</th>
              <th style="text-align:left;">#results</th>
              <th style="text-align:left;">Pre@10</th>
              <th>Pre@20</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;"><strong>Fashion Designers</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">407</td>
              <td style="text-align:left;">0.3</td>
              <td style="text-align:left;">0.3</td>
              <td style="text-align:left;">6</td>
              <td style="text-align:left;">282</td>
              <td style="text-align:left;">0.1</td>
              <td style="text-align:left;">0.25</td>
              <td style="text-align:left;">5</td>
              <td style="text-align:left;">295</td>
              <td style="text-align:left;">0.2</td>
              <td>0.15</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Fiction Writers</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">426</td>
              <td style="text-align:left;">0.7</td>
              <td style="text-align:left;">0.55</td>
              <td style="text-align:left;">11</td>
              <td style="text-align:left;">435</td>
              <td style="text-align:left;">0.4</td>
              <td style="text-align:left;">0.5</td>
              <td style="text-align:left;">10</td>
              <td style="text-align:left;">439</td>
              <td style="text-align:left;">0.3</td>
              <td>0.55</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Chess Players</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">418</td>
              <td style="text-align:left;">0.7</td>
              <td style="text-align:left;">0.5</td>
              <td style="text-align:left;">10</td>
              <td style="text-align:left;">389</td>
              <td style="text-align:left;">0.7</td>
              <td style="text-align:left;">0.6</td>
              <td style="text-align:left;">12</td>
              <td style="text-align:left;">424</td>
              <td style="text-align:left;">0.6</td>
              <td>0.6</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Finance</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">777</td>
              <td style="text-align:left;">0.5</td>
              <td style="text-align:left;">0.3</td>
              <td style="text-align:left;">6</td>
              <td style="text-align:left;">432</td>
              <td style="text-align:left;">0.3</td>
              <td style="text-align:left;">0.45</td>
              <td style="text-align:left;">9</td>
              <td style="text-align:left;">514</td>
              <td style="text-align:left;">0.4</td>
              <td>0.45</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Craft Breweries</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">972</td>
              <td style="text-align:left;">0.1</td>
              <td style="text-align:left;">0.25</td>
              <td style="text-align:left;">5</td>
              <td style="text-align:left;">240</td>
              <td style="text-align:left;">0.1</td>
              <td style="text-align:left;">0.1</td>
              <td style="text-align:left;">2</td>
              <td style="text-align:left;">128</td>
              <td style="text-align:left;">0.4</td>
              <td>0.3</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Jazz Players</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">428</td>
              <td style="text-align:left;">0.8</td>
              <td style="text-align:left;">0.8</td>
              <td style="text-align:left;">15</td>
              <td style="text-align:left;">431</td>
              <td style="text-align:left;">0.8</td>
              <td style="text-align:left;">0.8</td>
              <td style="text-align:left;">16</td>
              <td style="text-align:left;">426</td>
              <td style="text-align:left;">0.9</td>
              <td>0.85</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Fashion Models</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">413</td>
              <td style="text-align:left;">0.1</td>
              <td style="text-align:left;">0.2</td>
              <td style="text-align:left;">4</td>
              <td style="text-align:left;">138</td>
              <td style="text-align:left;">0.1</td>
              <td style="text-align:left;">0.2</td>
              <td style="text-align:left;">4</td>
              <td style="text-align:left;">211</td>
              <td style="text-align:left;">0.4</td>
              <td>0.35</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>Talk Shows</strong></td>
              <td style="text-align:left;">20</td>
              <td style="text-align:left;">423</td>
              <td style="text-align:left;">0.5</td>
              <td style="text-align:left;">0.45</td>
              <td style="text-align:left;">9</td>
              <td style="text-align:left;">440</td>
              <td style="text-align:left;">0.3</td>
              <td style="text-align:left;">0.45</td>
              <td style="text-align:left;">9</td>
              <td style="text-align:left;">437</td>
              <td style="text-align:left;">0.4</td>
              <td>0.35</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Example Domains of Interest</h2>
        </div>
      </header>
      <p>We applied our method on different domains and usage scenarios, so as to demonstrate its generality:</p>
      <ul class="list-no-style">
        <li id="list15" label="•">
          <strong>Fashion designers</strong>: the research team of the Fashion In Process Lab<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> (especially Paola Bertola, Chiara Colombi and Federica Vacca) was among the inspirators of this work, as it brought to us the problem of identifying emerging fashion designers. In the original experiment, the domain experts started with 200 emerging Italian brands as seeds.<br />
        </li>
        <li id="list16" label="•"><strong>Finance influencers</strong>: a team of economics and statistics researchers at University of Pavia executed experiments on the extraction of influencers in finance. In this case the team selected as seeds 120 bloggers and journalists in the finance sector.<br /></li>
        <li id="list17" label="•">
          <strong>Fiction writers</strong>: We considered some fiction authors engaged in the Melbourne Emerging Writers Festival<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>by picking 20 seeds from the participants to the event.<br />
        </li>
        <li id="list18" label="•"><strong>Craft breweries</strong>: We considered as seeds a set of 20 well-known US craft breweries, all present in DBpedia. was to understand if it is possible to identify new craft breweries before they are widely acknowledged by consumers.<br /></li>
        <li id="list19" label="•">
          <strong>Chess players</strong>: We used a list of 20 top chess players and their accounts.<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a><br />
        </li>
        <li id="list20" label="•">
          <strong>Jazz players</strong>: We used a list of 10 top jazz players and their accounts.<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a><br />
        </li>
        <li id="list21" label="•"><strong>Fashion models</strong>: We used a list of 20 fashion top models known in fashion, by extracting the top 20.<br /></li>
        <li id="list22" label="•"><strong>Talk shows</strong>: We used a list of 20 official Twitter accounts of popular TV talk shows.<br /></li>
      </ul>
      <p>These scenarios cover different information needs and domains. For instance, fashion design is characterized by a very high concentration of the domain in few brands only, most of which well known; on the opposite, fiction writers is an open domain where authors can be considered widespread; finance is a well established domain with renowned influencers; and craft beer is experiencing a tremendous growth with new craft breweries emerging almost daily.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Extraction Scenarios</h2>
        </div>
      </header>
      <p>At the purpose of responding to the four research question presented in Section 1, we describe four possible usage scenarios for our method, and we report the findings obtained by experimenting with the scenarios on the seven domains discussed above.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Iterative Knowledge Extraction</h3>
          </div>
        </header>
        <p>The first usage scenario we want propose is <em>iterative</em> knowledge extraction, where successful candidates of one extraction are used as seeds for a subsequent extraction. We identified 20 seeds per domain, and ran 3 iterations of the method for each of them.</p>
        <p>Table <a class="tbl" href="#tab1">1</a> shows the precision@10 and precision@20 obtained for each extraction for the eight domains; the #seeds in the second and third run correspond to the candidates of the respectively first and second run that were considered correct among the top 20 candidates (#results is the number of all candidates identified in a given run). Correctness was assessed against a manually tagged ground truth built through crowdsourcing; each run was executed twice. Every run after the first takes the good candidates of the previous run as seeds.</p>
        <p>Within a given domain, consecutive runs tend to produce similar precision, independently of the number of seeds and results. It seems that certain domains are most suited to the method, such as chess or jazz players, most likely because the twitter accounts of these entities are focused on (if not limited to) their respective domains, whereas the method is less effective in other domains, such as breweries or fashion models. This latter result may be due to tweets that are less focused and contain generic topics, making similarity search less effective, but also to the presence of entities with high similarity but different ontological types (e.g., beer lovers/distributors or fashion bloggers). If initial entities are chosen from a specific subdomain (e.g., writers in Melbourne), iterations progressively extract entities from a wider semantic and geographic domain (e.g., from outside Australia).</p>
        <p>If we consider all runs as independent (considering neither the domain nor the order of execution), we find a correlation of 0.65 between the number of seeds and that of results (at the edge of significance) and one of 0.91 between precision at 10 and precision at 20. If we analyze the domains individually, pair-wise t-tests among the three runs neither identify any significant difference (<em>α</em> = 0.05) between precision at 10 nor between precision at 20. The method thus works well even after several iterations, as precision remains rather stable (it decreases in certain domains, but it also increases in others); hence <em>a recursive application of knowledge extraction methods finds an increasing number of domain entities</em> <strong>(RQ1)</strong>. Especially when precision is high, one can find a good number of correct emerging entities from within the list of top-20 candidates.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Geographical Spreading of Knowledge</h3>
          </div>
        </header>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191578/images/www18companion-317-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Geographical dispersion of the knowledge graph in response to iterative knowledge extractions (US chess players).</span>
          </div>
        </figure>
        <p>In order to study the geographical spreading of knowledge, we applied a similar iterative knowledge extraction approach as in Section <a class="sec" href="#sec-6">4.1</a> to one selected domain: chess players from the US. We decided to focus on this sub-domain (of all chess players) to study if our method can find entities from other geographical regions and, if yes, how fast the knowledge graph expands.</p>
        <p>The experiment lasted three runs. For the first run of the experiment we took 7 seeds and a set of expert types. The next two re-runs were performed selecting the correct candidates from the top 20 results of the respective previous run. The actual localization of candidates was performed manually, either using the declared Twitter user location or, if that was missing, by searching other social resources and matching entities. After careful study of the location field as used by different Twitter accounts, we set the granularity of the locations to the level of individual countries.</p>
        <p>The result is an instance-based graph of <em>mentions</em> from a seed to a candidate and <em>co-occurences</em> of two candidates, i.e., tweets by one of the seeds that mentioned the two candidates together, mapped to physical locations. The result is illustrated in Figure <a class="fig" href="#fig2">2</a>. The first knowledge extraction produced 12 good candidates from different countries and continents, reaching Europe and the Middle-East. The first iteration found 15 good candidates, adding new data points also to South America and Asia. The second iteration produced 10 good candidates, while some seeds did not find any valid candidate.</p>
        <p>We conclude that <em>discovered knowledge which is iteratively found spans large geographical areas very fast</em> <strong>(RQ2)</strong>. The finding is somewhat surprising, but can likely be explained with the open nature of Twitter, e.g., compared to Facebook (where we would expect a slower spreading).</p>
        <p>With this experiment we proved that even starting from a small set of seeds from a single country, our approach is able to find good candidates from different states, countries and continents.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Capturing of Knowledge Evolution</h3>
          </div>
        </header>
        <p>The third usage scenario we propose is the study of how knowledge evolves over time. While the previous two scenarios are instances of an iterative knowledge extraction process, with selected candidates being used as seeds, here we propose a <em>periodic</em> extraction process, with knowledge extracted at periodic time intervals. For convenience, we fix the interval to three months, starting from September 2017 and looking back until January 2016. At each period, we consider all tweets since the beginning of 2016 up to the last month of the period being studied, constructing smaller reference data sets as we go back in time. It is important to note that to go back in time all cut-offs are computed from one cumulative download of tweets performed in the end of September 2017 using one set of seeds.</p>
        <p>Table <a class="tbl" href="#tab2">2</a> reports the numbers of candidates extracted for the four domains studied so far. The four domains have a different evolution over time, with <em>Finance</em> being the youngest domain (our seeds started tweeting only in 2017). For the other three domains, one can observe that the <em>Fashion</em> domain growing slower than both <em>Chess</em> and <em>Australian Writers</em>. Looking at the table, it is also important to note that the rate at which knowledge increases is fast, that is, the knowledge we extract today is significantly bigger then the one we would have extracted only 3 months ago. Projected into the future, this solicits a continuous knowledge extraction instead of a periodic or random extraction. In conclusion, <em>knowledge can be extracted from social data at arbitrary points of time in the past and it is possible to trace how knowledge will evolve in the future</em>, thanks to the possibility to extract knowledge continuously <strong>(RQ3)</strong>.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Looking back in time in the four domains: periodic knowledge extractions over a period of 21 months.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"><strong>Time interval</strong></th>
                <th style="text-align:center;"><strong>Chess</strong></th>
                <th style="text-align:center;"><strong>Finance</strong></th>
                <th style="text-align:center;"><strong>Writers</strong></th>
                <th><strong>Fashion</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2017/09</strong></td>
                <td style="text-align:center;">545</td>
                <td style="text-align:center;">151</td>
                <td style="text-align:center;">780</td>
                <td>153</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2017/06</strong></td>
                <td style="text-align:center;">310</td>
                <td style="text-align:center;">52</td>
                <td style="text-align:center;">329</td>
                <td>123</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2017/03</strong></td>
                <td style="text-align:center;">210</td>
                <td style="text-align:center;">45</td>
                <td style="text-align:center;">237</td>
                <td>103</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2016/12</strong></td>
                <td style="text-align:center;">146</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">177</td>
                <td>95</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2016/09</strong></td>
                <td style="text-align:center;">78</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">94</td>
                <td>79</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2016/06</strong></td>
                <td style="text-align:center;">43</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">45</td>
                <td>61</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>2016/01 - 2016/03</strong></td>
                <td style="text-align:center;">10</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">25</td>
                <td>27</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Identification of Emerging Knowledge</h3>
          </div>
        </header>
        <p>For the analysis of how much knowledge reconstructed from social content can be considered as <em>emerging</em> (low-frequency entities not yet included in generic ontologies with high-frequency knowledge), we refer to <em>Wikipedia</em> as generic source of knowledge. We performed the analysis over four domains (<em>Fashion designers</em>, <em>Finance</em>, <em>Chess Players</em> and <em>Australian Writers</em>) we took the candidates produced with <em>one</em> iteration of the method and calculated the percentage of correctly identified candidates. To assess this aspect, we proceeded by counting how many candidates have a Wikipedia page, i.e., had already been captured formally.</p>
        <p>Table <a class="tbl" href="#tab3">3</a> plots the obtained results. These are very domain dependent, likely due to the different social context behind the domains. For instance, in the <em>Fashion Designers</em> domain, the method produced an unexpected 100% of emerging designers. Instead, the domain that produced the lowest number of emerging entities is Australian writers (36%). Despite these fluctuations across domains and the fact that the reported results may not grant statistical representativeness, it is however important to note that in all cases <em>knowledge extracted from social content using the described method includes some relevant emerging knowledge that can be added to ontologies</em> <strong>(RQ4)</strong>.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">Emerging knowledge compared to Wikipedia among the correctly identified candidates.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"><strong>Domain</strong></th>
                <th><strong>Emerging entities</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Fashion Designers</td>
                <td>100%</td>
              </tr>
              <tr>
                <td style="text-align:left;">Finance</td>
                <td>77%</td>
              </tr>
              <tr>
                <td style="text-align:left;">Chess Player</td>
                <td>42%</td>
              </tr>
              <tr>
                <td style="text-align:left;">Australian Writer</td>
                <td>36%</td>
              </tr>
            </tbody>
          </table>
        </div>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191578/images/www18companion-317-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Architecture of the tool implementing our approach.</span>
          </div>
        </figure>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Implementation</h2>
        </div>
      </header>
      <p>The proposed approach has been implemented as a Python application. With respect to our original research [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], which extensively investigated more than 900 alternative extraction strategies, in this work we propose a light-weight tool, which only applies one strategy (the best one in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>]). While the quality of the results slightly decreases, the tool performance is quite good, in terms of tweets download, DBpedia matching, and score computation; the performance is adequate for exploring many domains on daily basis. The tool can run continuously or with periodic iterations, using the results as new seeds.</p>
      <p>Fig. <a class="fig" href="#fig3">3</a> represents the high-level view of the <strong>system architecture</strong>. The <em>Web Interface</em> allows users to interact with the system: it supports the phases of experiment definition and results visualization by the expert, as well as the validation of the results by the crowd. The <em>Pipeline Orchestrator</em> manages the execution of the process and is responsible of coordinating the components that perform each step of the analysis. The involved components are the following:</p>
      <ul class="list-no-style">
        <li id="list23" label="•">
          <em>Social Crawler</em>: this component receives in input a list of Twitter handles (i.e., user identifiers) and uses the Twitter API&nbsp;<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>to crawl their tweets. It is used for retrieving the posts of both the seeds and the candidates.<br />
        </li>
        <li id="list24" label="•">
          <em>Entity Extractor</em>: this component receive in input the text of the tweet and uses the Dandelion API<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a>to find entities mentioned in the text. Dandelion is a commercial software which matches a text to either instances or types of DBpedia.<br />
        </li>
        <li id="list25" label="•"><em>Candidate Finder</em>: this component is responsible of ranking the candidates using the information retrieved by the other components. In particular it creates the feature vectors and computes the similarity score of each candidate.<br /></li>
      </ul>
      <p>The data involved in the process is persisted in a MongoDB<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a> database, which stores, for every user, the track of all his experiments, in terms of seeds, candidates, and evaluations.</p>
      <p>The tool is available on GitHub under the Apache 2.0 open source license.<a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a></p>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Related Work</h2>
        </div>
      </header>
      <p>This paper presents a method and tool to harvest the collective intelligence of the Social Web in developing a collective knowledge system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. P. Mika pioneered this area in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>], by identifying broader and narrower terms using social network analysis methods such as centrality and other measures like the clustering coefficient. Our interest is on the <em>circle of knowledge life</em> proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>], where emerging knowledge is extracted from the Social Web using known facts captured in a knowledge graph. Our approach is grounded in <em>homophily</em>, a key aspect of social networks: entities are related when they have similar characteristics [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>]. Homophily can be used to explain the scale-free nature<a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a> of social networks; in our approach, the seeds guide the process that identifies homophily patterns and thus constructs the domain graph. As pointed out in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] and [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>], the grand challenge in automating the discovery of emerging knowledge is to find entities, relationships and attributes not mainstream, belonging to niches in the long tail [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>].</p>
      <p>We found two works that also proposed to use Twitter for ontology enrichment. P. Monachesi and T. Markus in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] proposed an ontology enrichment pipeline that can automatically enrich a domain ontology using data extracted by a crawler from social media applications. C. Wagner and M. Strohmaier [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>] investigated a network-theoretic model called <em>tweetonomy</em> to study emerging semantics. Complementary to our work, they investigated how the selection of tweets (so-called Social Awareness Streams) can lead to different results. Incorporating their work is part of our future work.</p>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Conclusions</h2>
        </div>
      </header>
      <p>In this paper, we explored a method for discovering knowledge from social media. The method consists of an iterative approach, in which the entities produced by one application of the method are used as seeds for the next application; we considered eight different domains. We specifically described the geographic and temporal spreading of entities extracted by the method. Finally, we measured the number of emerging entities found by the method; we regard as emerging those entities which are not present in Wikipedia.</p>
      <p>We show that the method succeeds in achieving a high precision after several iterations in many domains, and particularly in the domains of chess and jazz players; we observe that in such domains the terms used in social communications are the most domain-specific. Future work includes the semi-automatic building of a richer domain model, by studying other twitter features (such as verbs and the bag of words which appear in tweet texts). This work is part of a general effort for building automatic knowledge discovery systems on top of socially provided content.</p>
      <p><strong>Acknowledgement.</strong> This work was partially supported by the ERC Advanced Grant 693174, Data-Diven Genomic Computing.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">A.-L. Barabási and R.&nbsp;Albert. Emergence of scaling in random networks. <em>science</em>, 286(5439):509–512, 1999.</li>
        <li id="BibPLXBIB0002" label="[2]">M.&nbsp;Brambilla, S.&nbsp;Ceri, F.&nbsp;Daniel, and E.&nbsp;Della Valle. On the quest for changing knowledge. In <em>DDI@WebSci</em>, pages 3:1–3:5. ACM, 2016.</li>
        <li id="BibPLXBIB0003" label="[3]">M.&nbsp;Brambilla, S.&nbsp;Ceri, E.&nbsp;Della&nbsp;Valle, R.&nbsp;Volonterio, and F.&nbsp;X. Acero&nbsp;Salazar. Extracting emerging knowledge from social media. In <em>WWW 2017</em>, pages 795–804, 2017.</li>
        <li id="BibPLXBIB0004" label="[4]">A.&nbsp;Chris. <em>The long tail: Why the future of business is selling less of more</em>. New York: Hyperion, 2006.</li>
        <li id="BibPLXBIB0005" label="[5]">O.&nbsp;Etzioni, A.&nbsp;Fader, J.&nbsp;Christensen, S.&nbsp;Soderland, and Mausam. Open information extraction: The second generation. In <em>IJCAI</em>, pages 3–10. IJCAI/AAAI, 2011.</li>
        <li id="BibPLXBIB0006" label="[6]">T.&nbsp;Gruber. Collective knowledge systems: Where the social web meets the semantic web. <em>Web semantics: science, services and agents on the World Wide Web</em>, 6(1):4–13, 2008.</li>
        <li id="BibPLXBIB0007" label="[7]">J.&nbsp;Lehmann, R.&nbsp;Isele, M.&nbsp;Jakob, A.&nbsp;Jentzsch, D.&nbsp;Kontokostas, P.&nbsp;Mendes, S.&nbsp;Hellmann, M.&nbsp;Morsey, P.&nbsp;van Kleef, S.&nbsp;Auer, and C.&nbsp;Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. <em>Semantic Web Journal</em>, 2014.</li>
        <li id="BibPLXBIB0008" label="[8]">A.&nbsp;Maedche. <em>Ontology learning for the semantic web</em>, volume 665. Springer Science &amp; Business Media, 2012.</li>
        <li id="BibPLXBIB0009" label="[9]">P.&nbsp;Mika. Ontologies are us: A unified model of social networks and semantics. In <em>International semantic web conference</em>, pages 522–536. Springer, 2005.</li>
        <li id="BibPLXBIB0010" label="[10]">P.&nbsp;Monachesi and T.&nbsp;Markus. Using social media for ontology enrichment. In <em>Extended Semantic Web Conference</em>, pages 166–180. Springer, 2010.</li>
        <li id="BibPLXBIB0011" label="[11]">T.&nbsp;Rebele, F.&nbsp;M. Suchanek, J.&nbsp;Hoffart, J.&nbsp;Biega, E.&nbsp;Kuzey, and G.&nbsp;Weikum. YAGO: A multilingual knowledge base from wikipedia, wordnet, and geonames. In <em>ISWC 2016</em>, pages 177–185, 2016.</li>
        <li id="BibPLXBIB0012" label="[12]">A.&nbsp;Sheth, C.&nbsp;Thomas, and P.&nbsp;Mehra. Continuous semantics to analyze real-time data. <em>IEEE Internet Computing</em>, 14(6):84, 2010.</li>
        <li id="BibPLXBIB0013" label="[13]">A.&nbsp;Singhal. Introducing the knowledge graph: things, not strings. Available online at <a class="link-inline force-break" href="http://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html,%202012">http://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html</a>, 2012.
        </li>
        <li id="BibPLXBIB0014" label="[14]">E.&nbsp;Sun and V.&nbsp;Iyer. Under the hood: The entities graph. Available online at <a class="link-inline force-break" href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-the-entities-graph/10151490531588920/">https://www.facebook.com/notes/facebook-engineering/under-the-hood-the-entities-graph/10151490531588920/</a>, 2013.
        </li>
        <li id="BibPLXBIB0015" label="[15]">C.&nbsp;Wagner and M.&nbsp;Strohmaier. The wisdom in tweetonomies: Acquiring latent conceptual structures from social awareness streams. In <em>Proceedings of the 3rd International Semantic Search Workshop</em>, page&nbsp;6. ACM, 2010.</li>
        <li id="BibPLXBIB0016" label="[16]">G.&nbsp;Weikum and M.&nbsp;Theobald. From information to knowledge: harvesting entities and relationships from web sources. In <em>PODS</em>, pages 65–76. ACM, 2010.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://www.fashioninprocess.com/">http://www.fashioninprocess.com/</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://www.emergingwritersfestival.org.au">http://www.emergingwritersfestival.org.au</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://www.reddit.com/r/chess/comments/32t5ov/list_of_top_chess_player_journalist_twitter/">https://www.reddit.com/r/chess/comments/32t5ov/list_of_top_chess_player_journalist_twitter/</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break" href="http://oneworkingmusician.com/10-jazz-musicians-you-should-follow-on-twitter/">http://oneworkingmusician.com/10-jazz-musicians-you-should-follow-on-twitter/</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break" href="https://dev.twitter.com/rest/public">https://dev.twitter.com/rest/public</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a><a class="link-inline force-break" href="https://dandelion.eu/">https://dandelion.eu/</a></p>
    <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break" href="https://www.mongodb.com">https://www.mongodb.com</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a><a class="link-inline force-break" href="https://github.com/DataSciencePolimi/social-knowledge-extractor">https://github.com/DataSciencePolimi/social-knowledge-extractor</a></p>
    <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a>I.e., the vertex connectivity follows a power-law distribution.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191578">https://doi.org/10.1145/3184558.3191578</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
