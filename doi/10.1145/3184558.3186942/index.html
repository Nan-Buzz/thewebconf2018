<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Matching Resumes to Jobs via Deep Siamese Network</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Matching Resumes to Jobs via Deep Siamese Network</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Saket</span>      <span class="surName">Maheshwary</span>     IIIT Hyderabad, India, <a href="mailto:saket.maheshwary@research.iiit.ac.in">saket.maheshwary@research.iiit.ac.in</a>     </div>     <div class="author">     <span class="givenName">Hemant</span>      <span class="surName">Misra</span>     IBM Research &#x0026; Global Decision Management, Citi Group, <a href="mailto:hemant.misra@citi.com">hemant.misra@citi.com</a>     </div>            </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186942" target="_blank">https://doi.org/10.1145/3184558.3186942</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>In this paper we investigate the important and challenging task of recommending appropriate jobs for job seeking candidates by matching semi structured resumes of candidates to job descriptions. To perform this task, we propose to use a siamese adaptation of convolutional neural network. The proposed approach effectively captures the underlying semantics thus enabling to project similar resumes and job descriptions closer to each other, and make dissimilar resumes and job descriptions distant from each other in the semantic space. Our experimental results on a set of 1314 resumes and a set of 3809 job descriptions (5,005,026 resume-job description pairs) demonstrate that our approach is better than the current state-of-the-art approaches.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Saket Maheshwary and Hemant Misra. 2018. Matching Resumes to Jobs via Deep Siamese Network. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 3 Pages. <a href="https://doi.org/10.1145/3184558.3186942" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186942</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Job search via online matching engines is very important and extremely beneficial for both job seekers and employers. We are interested in finding job descriptions (JDs) that are an appropriate match to a candidate&#x0027;s resume, where an appropriate match means that a prospective candidate would be interested in reading and applying for the retrieved job descriptions. Given a collection of semi-structured resumes <em>R</em>, a collection of semi-structured job descriptions <em>J</em>, and some known matched and unmatched resume/job description pairs < <em>r</em>, <em>j</em> > , the task is to retrieve matching job descriptions for any existing or new resume <em>r</em>.</p>    <p>For a trained human, reading a resume or a job description and understanding its relevance is an easy task. In contrast, a computer system that parses such documents needs to be continuously trained and adapted to deal with the endless expressivity of human language. The existing traditional engines fail to understand the underlying semantic meanings of different resumes and have not kept pace with the recent progress in machine learning and natural language processing (NLP) techniques. These solutions are generally driven by manually engineered features and set of rules with predefined weights to keywords which lead to an inefficient and ineffective search experience for job seeking candidates. Besides, these solutions are not scalable as well.</p>    <p>Text based tasks rely heavily on representations that could be effectively learned. Such representations must capture the underlying semantics among textual elements, where textual elements can be a sequences of sentences, words or characters. Such effective learning model should not be affected by the variation of words in text used for expressing the same idea. This is a hard and complex problem due to the lack of availability of annotated data, complex structure and variable length of text. Though state-of-the-art models like bag-of-words (BOW) and TF-IDF are effective in many NLP tasks, in the context of understanding the underlying semantics they prove to be ineffective because of their inherent term specifity&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. Recently proposed models such as Word2vec and GloVe are less effective at document level&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>].</p>    <p>In this paper, we used an intuitive approach to successfully handle the problem of matching appropriate jobs for resumes by learning a similarity metric between the JDs and the resumes. Siamese networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] possess the capability of learning a similarity metric from the available training records. Our proposed network contains a siamese adaptation of convolutional neural networks (CNN) with a contrastive loss energy function joining the twin network at the top. These twin networks share the weights with each other (parameter sharing). The main contributions of our paper are as follows: (1) We investigate the application of deep siamese network for the task of matching resumes with appropriate JDs. (2) Our approach is universal and robust as it effectively handles JDs from variety of domains. (3) Our model beats state-of-the-art methods by a large margin.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Proposed Approach</h2>     </div>    </header>    <p>Our deep siamese network consists of a pair of identical CNN that contains repeating convolution, max pooling and leaky rectified linear unit layers with a fully connected layer on the top. As expected, any part of text in resumes and job descriptions can influence the semantics of a word. In order to effectively capture such an influence, we want our network to see the entire input at once &#x2013; here CNN comes to our rescue. Our use of CNN can be contrasted with the possible use of Long Short-Term Memory (LSTM) networks for the same task. LSTM networks read their input from left to right (or right to left) but could be ineffective when the context at the end of a sentence influence the thoughts about its beginning. Though we can overcome this drawback by running two LSTMs, one left to right and the other right to left, and concatenating their outputs. However this approach is inefficient as it doubles our computational load. CNN on the other hand, grows a larger receptive field as we stack more and more layers. It does give us the desired effect in a controllable fashion and with a low computational cost. Besides, CNN helps to give a non linear projection to the resumes and JD in the semantic space. The semantic vectors yielded from CNN are connected to a layer measuring similarity between resumes and JD. The contrastive loss function combines the measured distance and the label. The gradient of the loss function with respect to the weights and biases shared by the sub-networks is computed using back-propagation.</p>    <p>Parameter sharing between twin networks is a fundamental property of siamese network. Training the network with a shared set of parameters not only reduces the number of parameters (thus, saving a lot of computations) but also ensures consistency of the representation of resumes and job descriptions in semantic space. The shared parameters of the network are learnt with the aim to minimize the semantic distance between the resumes and the relevant JDs and maximize the semantic distance between the resumes and irrelevant JDs. The document embedding of resumes and JDs (each of 200 dimension), generated using Doc2Vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] are given as inputs to our network.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experiments</h2>     </div>    </header>    <p>     <strong>     <span style="text-decoration: underline;">Baseline Methods:</span>     </strong> As baselines, we experiment with six commonly used representations. (1) Word n-grams and (2) TF-IDF: Baseline methods to compare with due to their effectiveness in variety of NLP tasks. (3) BOW: The word frequency from the training set (resumes and job descriptions) is selected and the count of each word is used as features. (4) Bag-of-Means: The average word2vec embedding of the training data is used as a feature set. (5) Doc2Vec: An unsupervised algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] that learns fixed-length feature representations from variable-length pieces of texts. (6) CNN: Recently used method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>], showing significant performance improvement over that of other traditional methods for resume job matching task. We used cosine similarity as final step with all these baseline models in order to find the similarity of < <em>r</em>, <em>j</em> > pair. The value of threshold for cosine similarity is determined by grid search on values {0.1, 0.15, 0.2, ..., 0.80}.<strong>     <span style="text-decoration: underline;">Dataset and Experimental Settings:</span>     </strong> The dataset consists of 1314 resumes which came in as a part of summer research intern application at IBM Research Labs and a set of 3809 JDs from various domains were crawled from a popular India based job portal, generating a corpus of 5,005,026 distinct < <em>r</em>, <em>j</em> > pairs with a label as either 0 (Match) or 1 (Not Match). We annotated the corpus in semi supervised fashion. At first, few resume-JD pairs were manually annotated by interns. An exercise was carried out where 40 interns who were part of the research internship program were asked to annotate 15 distinct JDs each. Then we (the authors) manually annotated 100 JDs, 50 out of these were a match (class label 1) and the other 50 were a non-match (class label 0) to all these 40 interns. Thus in total we now had 115 JDs annotated for each intern. These 40 interns were from engineering background pursuing either bachelors, masters or doctorate degrees. Besides, their expertise in diverse technology domains makes our model robust and effective. Rest of the pairs were labelled via label propagation. We split 40 interns into two groups of 30 and 10 respectively. For all the experiments the 30 interns set was taken as training set. Testing was carried out on the group of 10 interns (test set A) and also on the remaining corpus that was annotated using label propagation (test set B). Note that the 30 interns used for training were never present in the test set. Each resume and JD was pre-processed by lower-casing, stemming and special character removal. We performed training in batches of size 128. The length of semantic vector is 256, depth of CNN is 4, kernel width of max pooling is 100 and that of convolution is 10. The learning rate was set to 0.01. <em>Adam</em> optimization method is used to update the parameters of the sub-networks.</p>    <p>     <strong>     <span style="text-decoration: underline;">Results and Analysis:</span>     </strong> Table 1-3 shows the results of various methods on the task of matching resumes to jobs. We compare our approach with 6 baseline methods. It is evident from tables 1-3 that the performance of our proposed siamese adaptation of CNN (parameters are sharing is done) is significantly better then that of the simple CNN model (no parameter sharing) is used. Among baselines, TF-IDF is better than BOW and Word n-gram.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Accuracy comparison with baseline models</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">        <strong>Method</strong>       </td>       <td style="text-align:center;">        <strong>Accuracy on test set A</strong>       </td>       <td style="text-align:center;">        <strong>Accuracy on test set B</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">BOW</td>       <td style="text-align:center;">56.92</td>       <td style="text-align:center;">57.06</td>      </tr>      <tr>       <td style="text-align:left;">Word n-gram</td>       <td style="text-align:center;">58.24</td>       <td style="text-align:center;">59.01</td>      </tr>      <tr>       <td style="text-align:left;">TF-IDF</td>       <td style="text-align:center;">60.46</td>       <td style="text-align:center;">61.23</td>      </tr>      <tr>       <td style="text-align:left;">Bag of Means</td>       <td style="text-align:center;">62.34</td>       <td style="text-align:center;">63.92</td>      </tr>      <tr>       <td style="text-align:left;">Doc2Vec</td>       <td style="text-align:center;">65.24</td>       <td style="text-align:center;">67.24</td>      </tr>      <tr>       <td style="text-align:left;">CNN</td>       <td style="text-align:center;">73.31</td>       <td style="text-align:center;">74.68</td>      </tr>      <tr>       <td style="text-align:left;">        <strong>Our Approach</strong>       </td>       <td style="text-align:center;">        <strong>81.34</strong>       </td>       <td style="text-align:center;">        <strong>83.19</strong>       </td>      </tr>     </tbody>     <tfoot>      <tr>       <td>&#x00A0;</td>       <td/>       <td/>      </tr>     </tfoot>     </table>    </div>    <div class="table-responsive" id="tab2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Comparisons with baseline models on test set A</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:center;">        <strong>Method</strong>       </td>       <td style="text-align:center;">        <strong>Class Label</strong>       </td>       <td style="text-align:center;">        <strong>Precision</strong>       </td>       <td style="text-align:center;">        <strong>Recall</strong>       </td>       <td style="text-align:center;">        <strong>f1-score</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">BOW</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.63</td>       <td style="text-align:center;">0.62</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.54</td>      </tr>      <tr>       <td style="text-align:center;">Word n-gram</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.65</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.56</td>       <td style="text-align:center;">0.56</td>      </tr>      <tr>       <td style="text-align:center;">TF-IDF</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.70</td>       <td style="text-align:center;">0.74</td>       <td style="text-align:center;">0.72</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.58</td>       <td style="text-align:center;">0.56</td>      </tr>      <tr>       <td style="text-align:center;">Bag of Means</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.70</td>       <td style="text-align:center;">0.74</td>       <td style="text-align:center;">0.73</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.60</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.62</td>      </tr>      <tr>       <td style="text-align:center;">Doc2Vec</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.78</td>       <td style="text-align:center;">0.78</td>       <td style="text-align:center;">0.78</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.60</td>       <td style="text-align:center;">0.64</td>       <td style="text-align:center;">0.64</td>      </tr>      <tr>       <td style="text-align:center;">CNN</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.80</td>       <td style="text-align:center;">0.82</td>       <td style="text-align:center;">0.82</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.70</td>       <td style="text-align:center;">0.72</td>       <td style="text-align:center;">0.70</td>      </tr>      <tr>       <td style="text-align:center;">        <strong>Our Approach</strong>       </td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.92</td>       <td style="text-align:center;">0.92</td>       <td style="text-align:center;">0.92</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.75</td>       <td style="text-align:center;">0.78</td>       <td style="text-align:center;">0.78</td>      </tr>     </tbody>     <tfoot>      <tr>       <td>&#x00A0;</td>       <td/>       <td/>       <td/>       <td/>      </tr>     </tfoot>     </table>    </div>    <div class="table-responsive" id="tab3">     <div class="table-caption">     <span class="table-number">Table 3:</span>     <span class="table-title">Comparisons with baseline models on test set B</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:center;">        <strong>Method</strong>       </td>       <td style="text-align:center;">        <strong>Class Label</strong>       </td>       <td style="text-align:center;">        <strong>Precision</strong>       </td>       <td style="text-align:center;">        <strong>Recall</strong>       </td>       <td style="text-align:center;">        <strong>f1-score</strong>       </td>      </tr>      <tr>       <td style="text-align:center;">BOW</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.63</td>       <td style="text-align:center;">0.62</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.55</td>       <td style="text-align:center;">0.56</td>       <td style="text-align:center;">0.56</td>      </tr>      <tr>       <td style="text-align:center;">Word n-gram</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.65</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.65</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.54</td>       <td style="text-align:center;">0.56</td>       <td style="text-align:center;">0.56</td>      </tr>      <tr>       <td style="text-align:center;">TF-IDF</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.70</td>       <td style="text-align:center;">0.74</td>       <td style="text-align:center;">0.72</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.59</td>       <td style="text-align:center;">0.60</td>       <td style="text-align:center;">0.60</td>      </tr>      <tr>       <td style="text-align:center;">Bag of Means</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.70</td>       <td style="text-align:center;">0.74</td>       <td style="text-align:center;">0.73</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.64</td>       <td style="text-align:center;">0.63</td>      </tr>      <tr>       <td style="text-align:center;">Doc2Vec</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.78</td>       <td style="text-align:center;">0.78</td>       <td style="text-align:center;">0.78</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.62</td>       <td style="text-align:center;">0.66</td>       <td style="text-align:center;">0.64</td>      </tr>      <tr>       <td style="text-align:center;">CNN</td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.82</td>       <td style="text-align:center;">0.82</td>       <td style="text-align:center;">0.82</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.70</td>       <td style="text-align:center;">0.72</td>       <td style="text-align:center;">0.72</td>      </tr>      <tr>       <td style="text-align:center;">        <strong>Our Approach</strong>       </td>       <td style="text-align:center;">0</td>       <td style="text-align:center;">0.92</td>       <td style="text-align:center;">0.94</td>       <td style="text-align:center;">0.92</td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">1</td>       <td style="text-align:center;">0.79</td>       <td style="text-align:center;">0.80</td>       <td style="text-align:center;">0.80</td>      </tr>     </tbody>     <tfoot>      <tr>       <td>&#x00A0;</td>       <td/>       <td/>       <td/>       <td/>      </tr>     </tfoot>     </table>    </div>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Conclusion</h2>     </div>    </header>    <p>In this paper, we propose a siamese adaptation of CNN to tackle the task of recommending appropriate jobs to job seeking candidates. To evaluate the approach, we curated a dataset of 5,005,026 resume-job description pairs and annotated it in a semi-supervised manner. The proposed approach effectively captures the underlying semantics and significantly outperforms the existing methods.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Jane Bromley, James&#x00A0;W. Bentz, L&#x00E9;on Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard S&#x00E4;ckinger, and Roopak Shah. 1993. Signature Verification Using A &#x201D;Siamese&#x201D; Time Delay Neural Network. <em>      <em>IJPRAI</em>     </em>7(1993), 669&#x2013;688.</li>     <li id="BibPLXBIB0002" label="[2]">Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric discriminatively, with application to face verification. <em>      <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#x2019;05)</em>     </em>1(2005), 539&#x2013;546 vol. 1.</li>     <li id="BibPLXBIB0003" label="[3]">Quoc&#x00A0;V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In <em>      <em>ICML</em>     </em>.</li>     <li id="BibPLXBIB0004" label="[4]">Yiou Lin, Hang Lei, Prince&#x00A0;Clement Addo, and Xiaoyu Li. 2016. Machine Learned Resume-Job Matching Solution. <em>      <em>CoRR</em>     </em>abs/1607.07657(2016).</li>     <li id="BibPLXBIB0005" label="[5]">Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In <em>      <em>AAAI</em>     </em>.</li>     <li id="BibPLXBIB0006" label="[6]">Luiza Sayfullina, Eric Malmi, Yiping Liao, and Alex Jung. 2017. Domain Adaptation for Resume Classification Using Convolutional Neural Networks. <em>      <em>CoRR</em>     </em>abs/1707.05576(2017).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186942">https://doi.org/10.1145/3184558.3186942</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
