<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Verification of the Expected Answer Type for Biomedical Question Answering</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Verification of the Expected Answer Type for Biomedical Question Answering</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Sanjay</span>     <span class="surName">Kamath</span>     LIMSI, LRI, Univ. Paris-Sud, CNRS, Universit&#x00E9; Paris-Saclay, rue John von NeumannOrsay, FranceF-91405, <a href="mailto:sanjay@lri.fr">sanjay@lri.fr</a>    </div>    <div class="author">     <span class="givenName">Brigitte</span>     <span class="surName">Grau</span>     LIMSI, CNRS, ENSIIE, Universit&#x00E9; Paris-Saclay, rue John von NeumannOrsay, FranceF-91405, <a href="mailto:brigitte.grau@limsi.fr">brigitte.grau@limsi.fr</a>    </div>    <div class="author">     <span class="givenName">Yue</span>     <span class="surName">Ma</span>     LRI, Univ. Paris-Sud, CNRS, Universit&#x00E9; Paris-SaclayOrsay, France, <a href="mailto:yue.ma@lri.fr">yue.ma@lri.fr</a>    </div>                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3191542" target="_blank">https://doi.org/10.1145/3184558.3191542</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Extractive Question Answering (QA) focuses on extracting precise answers from a given paragraph to questions posed in natural language. Deep learning models are widely used to address this problem and can fetch good results, provided there exists enough data for learning. Such large datasets have been released in open domain, but not in specific domains, such as the medical domain. However, the medical domain has a great amount of resources such as UMLS thesaurus, ontologies such as SNOMED CT, and tools such as Metamap etc that could be useful. In this paper, we apply transfer learning for getting a DNN baseline system on biomedical questions and we study if structured resources can help in selecting the answers based on the recognition of the Expected Answer Type (EAT), which has been proved useful in open domain QA systems. This study relies on different representations for LAT and we study if gold standard answers and answers of our model have some positive impact from the LAT.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Question-Answering</small>, </span>     <span class="keyword">      <small> Neural Network Model</small>, </span>     <span class="keyword">      <small> Expected Answer Type</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Sanjay Kamath, Brigitte Grau, and Yue Ma. 2018. Verification of the Expected Answer Type for Biomedical Question Answering. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href="https://doi.org/10.1145/3184558.3191542" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3191542</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Question Answering (QA) focuses on giving a user precise answers to the questions posed in natural language. In previous evaluation conferences TREC and CLEF, the QA task was, given questions and a large corpus, to provide precise answers extracted from texts with their supporting passage. It involved complex systems, with a pipeline of modules. Since the availability of large training datasets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>], the task has been reformulated as a Machine Reading task: given a question and a paragraph, systems must extract the precise answer in the paragraph. This task is also called Extractive QA. Such a task can be helpful either in open domain, with questions about entities or events, or in specific domains. In this paper, we are interested about medical domain. Even though the lexicon used in these domains are quite different, the types of questions are rather similar and finding information about a gene, a disease or medical events, etc. and information about named entities in open domain have similar structures, and similar approaches can be applied.</p>    <p>One of the large scale datasets for extractive QA in open domain, is SQUAD<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> which was released by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia passages. For medical domain, the BIOASQ challenge (Task B) -&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] provide questions with a set of snippets extracted from medical scientific articles. Biomedical questions with their exact answers, relevant text snippets, concepts, articles, summaries were constructed or selected by biomedical experts from around Europe -[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. An example question with one snippet from BIOASQ data is shown below:</p>    <p> [l]Question: What is the mode of inheritance of Wilson&#x0027;s disease? Answer:&#x00A0;&#x00A0;&#x00A0; autosomal recessive Snippets:&#x00A0; The overall sex ratio of patients was nearly 1:1,and genetic analysis of 20 families confirmed an autosomal recessive mode of inheritance. </p>    <p>Deep learning models are used widely to address the QA task in open domain and have been proven to be effective. Results of several Deep Neural Network (DNN) models using the SQUAD Dataset can be found on the leaderboard note1. As it is impossible to create a large scale dataset for biomedical QA without extensive efforts of domain experts, transfer learning can be seen as an alternative approach to use DNN models for small scale biomedical QA as used by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. The authors train a deep neural network model based on FASTQA - [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] on open domain data using SQUAD dataset, and then use it to retrain the model on the BioAsq dataset. We use a similar approach for transfer learning with the DRQA model by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] as it obtains comparable results on open domain QA and its implementation is available <a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>.</p>    <p>In this paper we are interested to study if adding knowledge belonging to structured resources can help in re-ranking or selecting answers provided by the DNN model. In former QA systems (non deep learning approaches) on text, one of the main criteria for selecting an answer is based on recognizing the Expected Answer Type (EAT) or Lexical Answer Type (LAT) in order to do a matching with candidate answers. It relies on named entity recognition and additional resources, as text corpus and knowledge bases, have been used for improving this verification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. Biomedical domain has great amount of resources such as UMLS thesaurus, ontologies such as SNOMED CT, and tools such as Metamap for annotating texts. Thus, we study if this feature may have a role on getting better answers by studying the relevance of different EAT representations on the corpus provided by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] by using semantic groups of UMLS and word embeddings of the EAT.</p>   </section>   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related work</h2>    </div>    </header>    <section id="sec-6">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> QA systems</h3>     </div>    </header>    <p>Former QA systems on text are made of several pipeline modules: question analysis, passage selection, answer selection. Question analysis allows to extract features that are used for selecting passages and extracting the answer. Apart from the content words, these features can be different from a system to another, but they all make use of the Expected Answer Type (EAT) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>]. The EAT is either a named entity type organized in an answer type taxonomy, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] for open domain or UMLS terminology for bio-medical domain [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>], or a word found in the question or a general category as NP (noun phrase) when no information is given about it. Best methods for verifying if a candidate answer matches the EAT, other than NP, involved feature based supervised learning based on the use of different resources, as co-occurrences and presence in structured resources [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]. In medical domain, this verification was made using UMLS [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>].</p>    <p>Recent QA approaches are based on deep neural network architectures, mainly in the open domain (see results of such models on SQUAD Dataset on the leaderboard note1). On medical domain Wiese et&#x00A0;al. apply domain adaptation for their participation to BIOASQ 2017 and in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] introduce as supplementary feature the embedding for EAT, defined as the question word or the word close to the question word. However they did not report results that allows to evaluate the impact of EAT.</p>    </section>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Resources</h3>     </div>    </header>    <p>     <em>UMLS</em>. (Unified Medical Language System) Metathesaurus, created in 1986, has become an important and a large resource for biomedical science. It provides over 3,100,000 biomedical concepts imported from nearly 200 vocabularies. Each concept is assigned a Concept Unique Identifier (CUI) that uniquely identifies a single meaning. To consistently categorize these huge number of concepts, 133 Semantic Types are defined in UMLS Metathesaurus. In order to further reduce the complexity of the Metathesaurus, these semantic types are divided into 14 groups, called Semantic Groups, as presented in <a class="link-inline force-break"      href="https://semanticnetwork.nlm.nih.gov/download/SemGroups.txt">https://semanticnetwork.nlm.nih.gov/download/SemGroups.txt</a>. </p>    <p>Semantic types and semantic groups have been used in various biomedical information system, including categorizing clinical research eligibility criteria&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], learning biomedical ontology&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>], and representing clinical questions for medical QA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>].</p>    </section>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> QA system overview</h2>    </div>    </header>    <p>We present here the adaptation of an existing model named DRQA reader by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] to the biomedical domain.</p>    <p>DRQA reader has three components: 1) Input layer: where the input question words and input passage words are encoded using a pretrained word embedding space; 2) Neural layer: RNN or LSTM networks; 3) Output layer or decoding layer: where the outputs are start and end tokens representing a span of an extracted answer.</p>    <p>In the input layer, word embeddings are used to encode the words of paragraphs and questions into vectors, along with textual features such as Part of Speech tags, Named-Entity tokens, Term frequencies of the words in the paragraph. Authors use <em>Aligned question embeddings</em> where an attention score captures the similarity between paragraph words and questions words. Neural layer, where the core DNN model is defined uses different NN architectures to capture semantic similarities between the QA pairs. In the output layer, two independent classifiers use a bilinear term to capture the similarity between paragraph words and question words and compute the probabilities of each token being start and end of the answer span. An argmax value over the unnormalized exponential is calculated on the spans, to get a final prediction. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191542/images/www18companion-281-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Transfer learning from open domain to biomedical domain</span>     </div>    </figure>    </p>    <p>We apply transfer learning as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] where the authors train a neural network model based on FASTQA [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] on open domain data using SQUAD dataset, and then use it to retrain the model on the BIOASQ dataset as shown in the Figure <a class="fig" href="#fig1">1</a>. Following this model, we first train the DRQA model on SQUAD dataset with its default hyperparameters, and then retrain the model on BIOASQ questions. Several embedding spaces were tested as input vectors [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] and the best performing ones which were the Glove embeddings, were chosen as input to the system.</p>    <p>The BioAsq task is a little different from the SQUAD task. BioAsq provides several snippets that have been considered as relevant by medical experts. Thus, for a same question, the system takes as input each pair with the question and a snippet. In that way, several answers will be predicted for a question from several snippets. We can also notice that some of the snippets does not contain the answer, or the answer is not justified regarding the question, i.e. the snippet contains relevant but non-answerable text extract. Our model predicts one scored answer by snippet, and the final result is made of the ordered list of answers for a same question. We will keep the 5 top answers to study them regarding the EAT representation.</p>    <p>The DNN model does not make use any LAT information or any medical domain related resources. Thus the goal is to study if it can be interesting to add information regarding the LAT, or if the embeddings and the attention model of the model already capture such information.</p>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Verification of the Expected Answer Type</h2>    </div>    </header>    <p>Expected Answer Type or Lexical Answer Type (LAT) helps to identify the type of answers which are to be returned for a question. EAT can be a named entity, number, address, year etc. in open domain, and medical entities such as disease names, genes, drugs, symptoms etc. in biomedical domain. Here is an example below.</p>    <p> [l]Question: What <em>disease</em> in Loxapine prominently used for? Answer:&#x00A0;&#x00A0;&#x00A0; schizophrenia Expected/Lexical Answer Type: disease Semantic Group: DISO Disease or Syndrome </p>    <p>In this regard, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] released a corpus of LAT annotations for BioAsq questions<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> which were manually annotated with LAT words into them and their semantic types from UMLS.</p>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Material</h3>     </div>    </header>    <p>In our experiments, we consider different representations for the LAT:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">the semantic group the LAT refers to, which can be inferred from semantic types from the UMLS semantic network<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> (SGLAT);<br/></li>     <li id="list2" label="&#x2022;">a word embedding LAT (WELAT).<br/></li>    </ul>    <p>For computing WELAT when the LAT is made of several words, we compute the average of each word embedding of the LAT. When a word has no embedding, we set its vector to 0. We use Word2Vec skipgram model with 300 dimensions from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>] for computing word embeddings on the biomedical texts of BioAsq 5A task data which consist of 12.8 Million PUBMED articles.</p>    <p>To determine if the recognition of the LAT can be useful for selecting an answer, we study if we can match the LAT given in the annotated corpus (the gold standard LAT for questions (GoldLAT)) with the expected answers (the answers in the gold standard (GoldAns)) and with the answers given by our QA system (PredAns).</p>    <p>First we compute the semantic groups for the GoldLATs by implying the relations between semantic types and semantic groups in the UMLS semantic network<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>. Then we annotate the answers by using MetaMap to obtain the semantic groups of each answers if they exist. We get the correct answers to the questions of the LAT corpus from the Gold Standard data given by BioAsq organizers. We add to these lists the different forms of the answers found in the snippets (short forms, abbreviations etc.). The objective is to perform a realistic automatic evaluation<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a>.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Experiments and Results</h3>     </div>    </header>    <p>For the experiments, we consider only the factoid questions from BiomedLat corpus. We split the dataset into train and test set (80% train and 20% test). The statistics reported in the Figure <a class="fig" href="#fig2">2</a> are for the factoid question test set.</p>    <p>We compute cosine similarities between LAT word embeddings in questions and three different answer word embeddings which are detailed below:</p>    <ul class="list-no-style">     <li id="list3" label="&#x2022;">GoldStandard-maxCosine (crossed points): Answer words are Gold standard data annotated with all answer representations that have the maximal cosine similarity with WELAT.<br/></li>     <li id="list4" label="&#x2022;">DRQA-cosine-top1 (triangular points): Answer words are top 1 answers from DRQA output. The similarities of correct (resp. false) answers are plotted above (resp. below) the X-axis.<br/></li>     <li id="list5" label="&#x2022;">DRQA-maxCosine (round points): Answer words are from top 5 answers of DRQA output that have the maximal cosine similarity with WELAT. The similarities of correct (resp. false) answers are plotted above (resp. below) the X-axis.<br/></li>    </ul>    <p>From Figure <a class="fig" href="#fig2">2</a>, we can see that gold standard answers (GoldStandard-maxCosine) show a significant correlation with LAT in terms of word embeddings, although there are 6 questions whose LAT have 0 similarity with WELAT caused by missing word embeddings for the medical domain vocabulary. Another clear observation is that many of top-1 wrong answers from DRQA system have low similarities (less than 0.25), which indicates that we could remove some wrong answers according to this criterion.</p>    <p>Moreover, Figure <a class="fig" href="#fig2">2</a> shows that there are around 50% top-1 answers having zero similarity with question LAT. This could be caused by the out-of-vocabulary problem of word embeddings such as short answers with specific words that have never appeared in the training corpus.</p>    <p>For the round points below the X-axis, they also present an important similarity (around 0.5) correlation with WELAT, which means that by simply selecting the answer with highest similarity as the best answer is not an effective strategy. Indeed, when we used this re-ranking strategy to select one answer from DRQA candidate answers, the strict accuracy with respect to the annotated gold standard decreased from 38% to 33%. Again, the missing word embedding for correct answers has a strong impact on this results.</p>    <p>The observations above show that a fine-grained study of word embeddings is important for medical QA systems.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">SGLAT associated to answers</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:center;">        <strong>Dataset</strong>       </td>       <td>        <strong>Answer count</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">Gold standard data</td>       <td>40/59</td>       </tr>       <tr>       <td style="text-align:center;">DRQA correct top-1 output</td>       <td>18/23</td>       </tr>       <tr>       <td style="text-align:center;">DRQA wrong top-1 output</td>       <td>16/36</td>       </tr>      </tbody>     </table>    </div>    <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191542/images/www18companion-281-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">The distribution of answers in three different answering settings for 59 questions: the red crossed points are for gold-standard answers that have the maximal similarity with the question LAT word embedding; So all the crossed points are correct answers. The blue round points above the X-axis are correct answers returned by DRQA system with maximal cosine similarity with WELAT; The round points under the X-axis are false answers found by DRQA system with maximal cosine similarity. The absolute value is the similarity. The green triangles stand for the top-1 results of DRQA system, where the upper parts are correct answers and the low parts are wrong answers.</span>     </div>    </figure>    <p>To determine the importance of SGLAT in answer words, we studied if the semantic group of the question LAT words are present in the answers. We report this on three datasets, one being the Gold standard questions in BIOMEDLAT corpus and other two being the correct and wrong outputs of DRQA system (top-1).</p>    <p>Table <a class="tbl" href="#tab1">1</a> shows the count of matches of SGLAT and answer words. It is clear that many correct answers (gold standard - 40/59) have a matching SGLAT. For DRQA outputs, we compute how many correct and wrong top-1 answers has a matching SGLAT. From the reported findings, there are more correctly answered DRQA outputs (18/23) with matching SGLAT than the wrong ones.</p>    <p>Using the semantic group annotations from UMLS should have positive impact on the performance of QA systems.</p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusion</h2>    </div>    </header>    <p>The expected type of answer of questions has proved to be very useful in former feature-based QA systems as it allows to select correct answers in texts according to their matching type. Nowadays, end-to-end neural network models have been successfully developed for answering questions, in particular in open domain where large datasets were released. These models avoid complex feature engineering. However their adaptation to a specific domain, as medical (bio-medical) domain, by transfer learning shows lower results. Thus we wanted to study if adding some information about LAT could help for improving their results. In this paper, we studied different representations of the LAT, based on structured taxonomy or word embeddings, and showed a correlation with the correct answers. When comparing with the answers provided by our model, we can show that wrong answers might be withdrawn when adding such a criterion and that the computation of word embedding for biomedical terms has to be improved for a neural network based QA system. In the future, we will study on how we can model this information in our system.</p>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>Acknowledgements</h2>    </div>    </header>    <p>This work is funded by the ANR project GoAsQ (ANR-15-CE23-0022).</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">2011. Dynamic categorization of clinical research eligibility criteria by hierarchical clustering. <em>      <em>Journal of Biomedical Informatics</em>     </em>44, 6 (2011), 927 &#x2013; 935.</li>    <li id="BibPLXBIB0002" label="[2]">Asma&#x00A0;Ben Abacha and Pierre Zweigenbaum. 2015. MEANS: A medical question-answering system combining NLP techniques and semantic Web technologies. <em>      <em>Information processing &#x0026; management</em>     </em>51, 5 (2015), 570&#x2013;594.</li>    <li id="BibPLXBIB0003" label="[3]">Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. In <em>      <em>Proceedings of ACL 2017</em>     </em>. 1870&#x2013;1879.</li>    <li id="BibPLXBIB0004" label="[4]">Jennifer Chu-Carroll, James Fan, BK Boguraev, David Carmel, Dafna Sheinwald, and Chris Welty. 2012. Finding needles in the haystack: Search and candidate generation. <em>      <em>IBM Journal of Research and Development</em>     </em>56, 3.4 (2012), 6&#x2013;1.</li>    <li id="BibPLXBIB0005" label="[5]">Arnaud Grappy and Brigitte Grau. 2010. Answer type validation in question answering systems. In <em>      <em>RIAO 2010, 9th International Conference, Paris, France, April 28-30, 2010, Proceedings</em>     </em>. 9&#x2013;15. <a class="link-inline force-break"      href="http://portal.acm.org/citation.cfm?id=1937058CFID=17354760CFTOKEN=88565769"      target="_blank">http://portal.acm.org/citation.cfm?id=1937058CFID=17354760CFTOKEN=88565769</a></li>    <li id="BibPLXBIB0006" label="[6]">Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco, Anne-Laure Ligozat, Isabelle Robba, and Anne Vilnat. 2011. Selecting answers to questions from Web documents by a robust validation process. In <em>      <em>WI</em>     </em>.</li>    <li id="BibPLXBIB0007" label="[7]">Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot. 2016. Wikireading: A novel large-scale language understanding task over wikipedia. <em>      <em>arXiv preprint arXiv:1608.03542</em>     </em>(2016).</li>    <li id="BibPLXBIB0008" label="[8]">Sanjay Kamath, Brigitte Grau, and Yue Ma. 2017. A Study of Word Embeddings for Biomedical Question Answering. In <em>      <em>SIIM&#x2019;17</em>     </em>.</li>    <li id="BibPLXBIB0009" label="[9]">Tetsuya Kobayashi and Chi-Ren Shyu. 2006. Representing clinical questions by semantic type for better classification. In <em>      <em>AMIA Annual Symposium Proceedings</em>     </em>. 987987.</li>    <li id="BibPLXBIB0010" label="[10]">Oleksandr Kolomiyets and Marie-Francine Moens. 2011. A survey on question answering technology from an information retrieval perspective. <em>      <em>Information Sciences</em>     </em>181, 24 (2011), 5412&#x2013;5434.</li>    <li id="BibPLXBIB0011" label="[11]">Xin Li and Dan Roth. 2006. Learning question classifiers: the role of semantic information. <em>      <em>Natural Language Engineering</em>     </em>12, 3 (2006), 229&#x2013;249.</li>    <li id="BibPLXBIB0012" label="[12]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&#x00A0;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em>      <em>Advances in neural information processing systems</em>     </em>. 3111&#x2013;3119.</li>    <li id="BibPLXBIB0013" label="[13]">Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, Georgios Paliouras, and Ioannis Kakadiaris. 2017. Results of the fifth edition of the BioASQ Challenge. In <em>      <em>BioNLP 2017</em>     </em>. 48&#x2013;57. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/W17-2306"      target="_blank">http://www.aclweb.org/anthology/W17-2306</a></li>    <li id="BibPLXBIB0014" label="[14]">Mariana Neves and Milena Kraus. 2016. BioMedLAT corpus: Annotation of the lexical answer type for biomedical questions. In <em>      <em>Proceedings of the Open Knowledge Base and Question Answering Workshop (OKBQA 2016)</em>     </em>. 49&#x2013;58.</li>    <li id="BibPLXBIB0015" label="[15]">Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. <em>      <em>arXiv preprint arXiv:1611.09268</em>     </em>(2016).</li>    <li id="BibPLXBIB0016" label="[16]">Alina Petrova, Yue Ma, George Tsatsaronis, Maria Kissa, Felix Distel, Franz Baader, and Michael Schroeder. 2015. Formalizing biomedical concepts from textual definitions. <em>      <em>J. Biomedical Semantics</em>     </em>6 (2015), 22.</li>    <li id="BibPLXBIB0017" label="[17]">Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. <em>      <em>arXiv preprint arXiv:1606.05250</em>     </em>(2016).</li>    <li id="BibPLXBIB0018" label="[18]">George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, <em>et al.</em> 2015. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. <em>      <em>BMC Bioinformatics</em>     </em>16, 1 (30 Apr 2015), 138. <a class="link-inline force-break"      href="https://doi.org/10.1186/s12859-015-0564-6"      target="_blank">https://doi.org/10.1186/s12859-015-0564-6</a></li>    <li id="BibPLXBIB0019" label="[19]">Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017. Making neural qa as simple as possible but not simpler. In <em>      <em>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</em>     </em>. 271&#x2013;280.</li>    <li id="BibPLXBIB0020" label="[20]">Georg Wiese, Dirk Weissenborn, and Mariana Neves. 2017. Neural Domain Adaptation for Biomedical Question Answering. In <em>      <em>Proceedings of CoNLL 2017</em>     </em>. 281&#x2013;289. <a class="link-inline force-break" href="https://doi.org/10.18653/v1/K17-1029"      target="_blank">https://doi.org/10.18653/v1/K17-1029</a></li>    <li id="BibPLXBIB0021" label="[21]">Georg Wiese, Dirk Weissenborn, and Mariana Neves. 2017. Neural question answering at bioasq 5b. <em>      <em>arXiv preprint arXiv:1706.08568</em>     </em>(2017).</li>    <li id="BibPLXBIB0022" label="[22]">Zi Yang, Yue Zhou, and Eric Nyberg. 2016. Learning to answer biomedical questions: Oaqa at bioasq 4b. In <em>      <em>Proceedings of the Fourth BioASQ workshop</em>     </em>. 23&#x2013;37.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>https://rajpurkar.github.io/SQuAD-explorer/</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>https://github.com/facebookresearch/DrQA</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>https://github.com/mariananeves/BioMedLAT</p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class="link-inline force-break"    href="https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml">https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml</a>   </p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break"    href="https://metamap.nlm.nih.gov/Docs/SemGroups_2013.txt">https://metamap.nlm.nih.gov/Docs/SemGroups_2013.txt</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>Note that we did the same annotation of all the training data so that the models built on such datasets should learn from all kinds of forms of answers</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191542">https://doi.org/10.1145/3184558.3191542</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
