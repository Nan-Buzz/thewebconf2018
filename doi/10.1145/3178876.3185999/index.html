<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Dynamic Embeddings for Language Evolution</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Dynamic Embeddings for Language
          Evolution</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Maja</span> <span class=
          "surName">Rudolph</span>, Columbia University, <a href=
          "mailto:maja@cs.columbia.edu">maja@cs.columbia.edu</a>
        </div>
        <div class="author">
          <span class="givenName">David</span> <span class=
          "surName">Blei</span>, Columbia University, <a href=
          "mailto:david.blei@columbia.edu">david.blei@columbia.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3185999"
        target=
        "_blank">https://doi.org/10.1145/3178876.3185999</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Word embeddings are a powerful approach for
        unsupervised analysis of language. Recently, Rudolph et al.
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0035">35</a>]
        developed exponential family embeddings, which cast word
        embeddings in a probabilistic framework. Here, we develop
        dynamic embeddings, building on exponential family
        embeddings to capture how the meanings of words change over
        time. We use dynamic embeddings to analyze three large
        collections of historical texts: the U.S. Senate speeches
        from 1858 to 2009, the history of computer science ACM
        abstracts from 1951 to 2014, and machine learning papers on
        the ArXiv from 2007 to 2015. We find dynamic embeddings
        provide better fits than classical embeddings and capture
        interesting patterns about how language
        changes.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>word
          embeddings</small>,</span> <span class=
          "keyword"><small>exponential family
          embeddings</small>,</span> <span class=
          "keyword"><small>probabilistic modeling</small>,</span>
          <span class="keyword"><small>dynamic
          modeling</small>,</span> <span class=
          "keyword"><small>semantic change</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Maja Rudolph and David Blei. 2018. Dynamic Embeddings for
          Language Evolution. In <em>WWW 2018: The 2018 Web
          Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 9 Pages. <a href=
          "https://doi.org/10.1145/3178876.3185999" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3185999</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Word embeddings are a collection of unsupervised learning
      methods for capturing latent semantic structure in language.
      Embedding methods analyze text data to learn distributed
      representations of the vocabulary. The learned
      representations are then useful for reasoning about word
      usage and meaning [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>]. With large data sets and approaches
      from neural networks, word embeddings have become an
      important tool for analyzing language [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0003">3</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0006">6</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0025">25</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0033">33</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0042">42</a>].</p>
      <p>Recently, Rudolph et al.&nbsp;<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a> developed <em>exponential family
      embeddings</em>. Exponential family embeddings distill the
      key assumptions of an embedding problem, generalize them to
      many types of data, and cast the distributed representations
      as latent variables in a probabilistic model. They encompass
      many existing methods for embeddings and open the door to
      bringing expressive probabilistic modeling&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>] to the
      task of learning distributed representations.</p>
      <p>Here we use exponential family embeddings to develop
      <em>dynamic word embeddings</em>, a method for learning
      distributed representations that change over time. Dynamic
      embeddings analyze long-running texts, e.g., documents that
      span many years, where the way words are used changes over
      the course of the collection. The goal of dynamic embeddings
      is to characterize those changes.</p>
      <p>Figure&nbsp;<a class="fig" href="#fig1">1</a> illustrates
      the approach. It shows the changing representation of
      <font style="font-variant: small-caps">intelligence</font> in
      two corpora, the collection of computer science abstracts
      from the ACM 1951–2014 and the U.S. Senate speeches
      1858–2009. On the y-axis is “meaning,” a proxy for the
      dynamic representation of the word; in both corpora, its
      representation changes dramatically over the years. To
      understand where it is located, the plots also show similar
      words (according to their changing representations) at
      various points. Loosely, in the ACM corpus <font style=
      "font-variant: small-caps">intelligence</font> changes from
      government intelligence to cognitive intelligence to
      artificial intelligence; in the Congressional record
      <font style="font-variant: small-caps">intelligence</font>
      changes from psychological intelligence to government
      intelligence. Section&nbsp;<a class="sec" href="#sec-5">3</a>
      gives other examples from these corpora, such as for the
      terms <font style="font-variant: small-caps">iraq</font>,
      <font style="font-variant: small-caps">data</font>, and
      <font style="font-variant: small-caps">computer</font>.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3185999/images/www2018-8-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The dynamic embedding of <font style=
          "font-variant: small-caps">intelligence</font> reveals
          how the term's usage changes over the years in a historic
          corpus of ACM abstracts (a) and U.S. Senate speeches (b).
          The <em>y</em>-axis is “meaning,” a one dimensional
          projection of the embedding vectors. For selected years,
          we list words with similar dynamic embeddings.</span>
        </div>
      </figure>
      <p></p>
      <p>In more detail, a word embedding uses representation
      vectors to parameterize the conditional probabilities of
      words in the context of other words. Dynamic embeddings
      divide the documents into time slices, e.g., one per year,
      and cast the embedding vector as a latent variable that
      drifts via a Gaussian random walk. When fit to data, the
      dynamic embeddings capture how the representation of each
      word drifts from slice to slice.</p>
      <p>Section&nbsp;<a class="sec" href="#sec-4">2</a> describes
      dynamic embeddings and how to fit them.
      Section&nbsp;<a class="sec" href="#sec-5">3</a> studies this
      approach on three datasets: 9 years of ArXiv machine learning
      papers (2007–2015), 64 years of computer science abstracts
      (1951–2014), and 151 years of U.S. Senate speeches
      (1858–2009). Dynamic embeddings give better predictive
      performance than existing approaches and provide an
      interesting exploratory window into how language changes.</p>
      <p><strong>Related work.</strong>&nbsp; Language is known to
      evolve [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0001">1</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>] and there
      have been several lines of research around capturing semantic
      shifts. Mihalcea and Nastase&nbsp;<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a> and Tang et
      al.&nbsp;<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0038">38</a>
      detect semantic changes of words using features such as
      part-of-speech tags and entropy. Sagi et al.&nbsp;<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0037">37</a> and Basile
      et al.&nbsp;<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>
      employ latent semantic analysis and temporal semantic
      indexing for quantifying changes in meaning.</p>
      <p>Most closely related to our work are methods for dynamic
      embeddings [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0015">15</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>]. These
      methods train a separate embedding for each time slice of the
      data. While interesting, this requires enough data in each
      time slice such that a high quality embedding can be trained
      for each. Further, because each time slice is trained
      independently, the dimensions of the embeddings are not
      comparable across time; they must use initialization
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>] or ad-hoc
      alignment techniques [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0048">48</a>] to stitch them together.</p>
      <p>In contrast, the representations of our model for dynamic
      embeddings are sequential latent variables. This naturally
      accommodates time slices with sparse data and assures that
      the dimensions of the embeddings are connected across time.
      In Section&nbsp;<a class="sec" href="#sec-5">3</a>, we show
      that our method provides quantitative improvements over
      methods that fit each slice independently.</p>
      <p>We note that two models similar to ours have been
      developed independently [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0046">46</a>]. Bamler and Mandt&nbsp;<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a> model both
      the embeddings and the context vectors using an
      Uhlenbeck-Ornstein process [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0041">41</a>]. Yao et al.&nbsp;<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0046">46</a> factorize the pmi matrix
      at different time slices. Their regularization also resembles
      an Uhlenbeck-Ornstein process. Both employ the matrix
      factorization perspective of embeddings&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>], while our work builds on
      exponential family embeddings [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>], which generalize embeddings using
      exponential families. A related perspective is given by
      Cotterell et al.&nbsp;<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a> who show that exponential family PCA
      can generalize embeddings to higher order tensors.</p>
      <p>Another area of related work is dynamic topic models,
      which are also used to analyze text data over time
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0008">8</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0012">12</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0013">13</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0027">27</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0043">43</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0044">44</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0045">45</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0047">47</a>]. This
      class of models describes documents in terms of topics, which
      are distributions over the vocabulary, and then allows the
      topics to change. As in dynamic embeddings, some dynamic
      topic models use a Gaussian random walk to capture drift in
      the underlying language model; for example, see Blei and
      Lafferty&nbsp;<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>, Wang et al.&nbsp;<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0043">43</a>, Gerrish and
      Blei&nbsp;<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0013">13</a>
      and Frermann and Lapata&nbsp;<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>.</p>
      <p>Though topic models and word embeddings are related, they
      are ultimately different approaches to language analysis.
      Topic models capture co-occurrence of words at the document
      level and focus on heterogeneity, i.e., that a document can
      exhibit multiple topics [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>]. Word embeddings capture
      co-occurrence in terms of proximity in the text, usually
      focusing on small neighborhoods around each word [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>]. Combining
      dynamic topic models and dynamic word embeddings is an area
      for future study.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Dynamic
          Embeddings</h2>
        </div>
      </header>
      <p>We develop demb, a type of efe [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0035">35</a>] that captures sequential
      changes in the representation of the data. We focus on text
      data and the Bernoulli embedding model. In this section, we
      review Bernoulli embeddings for text and show how to include
      dynamics into the model. We then derive the objective
      function for dynamic embeddings and develop stochastic
      gradients to optimize it on large collections of text.</p>
      <p><strong>Bernoulli embeddings for text.</strong>&nbsp; An
      efe is a conditional model [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>]. It has three ingredients: The
      <em>context</em>, the <em>conditional distribution</em> of
      each data point, and the <em>parameter sharing
      structure</em>.</p>
      <p>In an efe for text, the data is a corpus of text, a
      sequence of words (<em>x</em> <sub>1</sub>, …,
      <em>x<sub>N</sub></em> ) from a vocabulary of size
      <em>V</em>. Each word <em>x<sub>i</sub></em> ∈ {0, 1}
      <sup><em>V</em></sup> is an indicator vector (also called a
      “one-hot” vector). It has one nonzero entry at <em>v</em>,
      where <em>v</em> is the vocabulary term at position
      <em>i</em>.</p>
      <p>In an efe model, each data point has a <em>context</em>.
      In text, the context of each word is its neighborhood; Each
      word is modelled conditionally on the words that come before
      and after. Typical context sizes range between 2 and 10 words
      and are set in advance.</p>
      <p>Here, we will build on Bernoulli embeddings, which provide
      a conditional model for the individual entries of the
      indicator vectors <em>x<sub>iv</sub></em> ∈ {0, 1}. Let
      <em>c<sub>i</sub></em> be the set of positions in the
      neighborhood of position <em>i</em> and let <span class=
      "inline-equation"><span class="tex">$\mathbf
      {x}_{c_i}$</span></span> denote the collection of data points
      indexed by those positions. The conditional distribution of
      <em>x<sub>iv</sub></em> is</p>
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} x_{iv} | \mathbf
          {x}_{c_{i}} \sim \text{Bern}(p_{iv}),
          \end{align}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>where <em>p<sub>iv</sub></em> ∈ (0, 1) is the Bernoulli
      probability.<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a>
      <p></p>
      <p>Bernoulli embeddings specify the natural parameter of this
      distribution, the log odds <span class=
      "inline-equation"><span class="tex">$\eta _{iv} = \log
      \frac{p_{iv}}{1-p_{iv}}$</span></span> , as a function of the
      representation of term <em>v</em> and the terms in the
      context of position <em>i</em>. Specifically, each index
      (<em>i</em>, <em>v</em>) in the data is associated with two
      parameter vectors, the <em>embedding vector</em> <span class=
      "inline-equation"><span class="tex">$\rho _v \in \mathbb
      {R}^K$</span></span> and the <em>context vector</em>
      <span class="inline-equation"><span class="tex">$\alpha _v
      \in \mathbb {R}^K$</span></span> . Together, the embedding
      vectors and context vectors form the natural parameter of the
      Bernoulli. It is</p>
      <div class="table-responsive" id="eq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \eta _{iv} = \rho
          _v^\top \left(\textstyle \sum _{j \in c_{i}} \sum
          _{v^{\prime }} \alpha _{v^{\prime }} x_{jv^{\prime
          }}\right). \end{align}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>This is the inner product between the embedding
      <em>ρ<sub>v</sub></em> and the context vectors of the words
      that surround position <em>i</em>. (Because
      <em>x<sub>j</sub></em> is an indicator vector, the sum over
      the vocabulary selects the appropriate context vector
      <em>α</em> at position <em>j</em>.) The goal is to learn the
      embeddings and context vectors.
      <p></p>
      <p>The index on the parameters does not depend on position
      <em>i</em>, but only on term <em>v</em>; the embeddings are
      shared across all positions in the text. This is what Rudolph
      et al.&nbsp;<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0035">35</a>
      call the <em>parameter sharing structure</em>. It ensures,
      for example, that the embedding vector for <font style=
      "font-variant: small-caps">intelligence</font> is the same
      wherever it appears. (Dynamic embeddings partially relax this
      restriction.)</p>
      <p>Finally, Rudolph et al.&nbsp;<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a> regularize the Bernoulli embedding by
      placing priors on the embedding and context vectors. They use
      Gaussian priors with diagonal covariance, i.e., ℓ<sub>2</sub>
      regularization. Without the regularization, fitting a
      Bernoulli embedding closely relates to other embedding
      techniques such as CBOW [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>] and negative sampling [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0025">25</a>]. But the
      probabilistic perspective of efe —and in particular the
      priors and the parameter sharing—allows us to extend this
      setting to capture dynamics.</p>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3185999/images/www2018-8-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">Graphical representation of a demb for
          text data in <em>T</em> time slices, <em>X</em>
          <sup>(1)</sup>, ⋅⋅⋅, <em>X</em> <sup>(<em>T</em>)</sup>.
          The embedding vectors <em>ρ<sub>v</sub></em> of each term
          evolve over time. The context vectors are shared across
          all time slices.</span>
        </div>
      </figure>
      <p></p>
      <p><strong>Dynamic Bernoulli embeddings</strong>&nbsp; (demb)
      extend Bernoulli embeddings to text data over time. Each
      observation <em>x<sub>iv</sub></em> is associated with a time
      slice <em>t<sub>i</sub></em> , such as the year of the
      observation. Context vectors are shared across all positions
      in the text but the embedding vectors are only shared within
      a time slice. Thus dynamic embeddings posit a sequence of
      embeddings for each term <span class=
      "inline-equation"><span class="tex">$\rho ^{(t)}_v\in \mathbb
      {R}^{K}$</span></span> while the static context vectors help
      ensure that consecutive embeddings are grounded in the same
      semantic space.</p>
      <p>The natural parameter of the conditional likelihood is
      similar to Equation&nbsp;(<a class="eqn" href="#eq2">2</a>)
      but with the embedding vector <em>ρ<sub>v</sub></em> replaced
      by the per-time-slice embedding vector <span class=
      "inline-equation"><span class="tex">$\rho
      _v^{(t_i)}$</span></span> ,</p>
      <div class="table-responsive" id="eq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \eta _{iv} = \rho
          _v^{(t_i)\top } \left(\textstyle \sum _{j \in c_{j}} \sum
          _{v^{\prime }} \alpha _{v^{\prime }} x_{jv^{\prime
          }}\right). \end{align}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>
      <p></p>
      <p>Finally, dynamic embeddings use a Gaussian random walk as
      a prior on the embedding vectors,</p>
      <div class="table-responsive" id="eq4">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \alpha _v, \rho
          _v^{(0)} &amp;\sim \mathcal {N} (0, \lambda _{0}^{-1} I)
          \end{align}</span><br />
          <span class="equation-number">(4)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq5">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \rho _v^{(t)}
          &amp;\sim \mathcal {N} (\rho _v^{(t-1)}, \lambda ^{-1}
          I). \end{align}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>Given data, this leads to smoothly changing estimates
      of each term's embedding.<a class="fn" href="#fn2" id=
      "foot-fn2"><sup>2</sup></a>
      <p></p>
      <p>Figure&nbsp;<a class="fig" href="#fig2">2</a> gives the
      graphical model for dynamic embeddings. Dynamic embeddings
      are a conditionally specified model, which in general are not
      guaranteed to imply a consistent joint distribution. But
      dynamic Bernoulli embeddings model binary data, and thus a
      joint exists [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>].</p>
      <p><strong>Fitting dynamic embeddings.</strong>&nbsp;
      Calculating the joint is computationally intractable. Rather,
      we fit dynamic embeddings with the <em>pseudo log
      likelihood</em>, the sum of the log conditionals, a commonly
      used objective for conditional models [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>].</p>
      <p>In detail, we regularize the pseudo log likelihood with
      the log priors and then maximize to obtain a pseudo MAP
      estimate. For dynamic Bernoulli embeddings, this objective is
      the sum of the log priors and the conditional log likelihoods
      of the data <em>x<sub>iv</sub></em> .</p>
      <p>We divide the data likelihood into two parts, the
      contribution of nonzero data entries <span class=
      "inline-equation"><span class="tex">$\mathcal
      {L}_{\text{pos}}$</span></span> and of zero data entries
      <span class="inline-equation"><span class="tex">$\mathcal
      {L}_{\text{neg}}$</span></span> ,</p>
      <div class="table-responsive" id="eq6">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathcal {L}({\rho
          }, {\alpha }) &amp;= \mathcal {L}_{\text{pos}} + \mathcal
          {L}_{\text{neg}} + \mathcal {L}_{\text{prior}}.
          \end{align}</span><br />
          <span class="equation-number">(6)</span>
        </div>
      </div>The likelihoods are
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\begin{align*} \mathcal
          {L}_{\text{pos}} &amp;= \sum _{i=1}^N \sum
          _{v=1}^Vx_{iv}\log \sigma (\eta _{iv}) \nonumber
          \\\mathcal {L}_{\text{neg}} &amp;= \sum _{i=1}^N \sum
          _{v=1}^V(1 - x_{iv}) \log (1 - \sigma (\eta _{iv})),
          \nonumber \\\end{align*}</span><br />
        </div>
      </div>where <em>σ</em>(·) is the sigmoid, which maps natural
      parameters to probabilities. The prior is
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\begin{align*} \mathcal
          {L}_{\text{prior}} &amp;= \log p({\alpha }) + \log
          p({\rho }),\end{align*}</span><br />
        </div>
      </div>where
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\begin{align*} \log p({\alpha })
          =&amp;-\frac{\lambda _{0}}{2} \sum _v || \alpha _v||^2
          \\\log p({\rho }) =&amp; -\frac{\lambda _{0}}{2} \sum _v
          ||\rho _v^{(0)}||^2 \\ &amp; -\frac{\lambda }{2} \sum
          _{v,t} ||\rho _v^{(t)}-\rho
          _v^{(t-1)}||^2.\end{align*}</span><br />
        </div>
      </div>
      <p></p>
      <p>The parameters ρ and α appear in the natural parameters
      <em>η<sub>iv</sub></em> of Equations&nbsp;(<a class="eqn"
      href="#eq2">2</a>) and&nbsp;(<a class="eqn" href=
      "#eq3">3</a>) and in the log prior. The random walk prior
      penalizes consecutive word vectors <span class=
      "inline-equation"><span class="tex">$\rho
      _v^{(t-1)}$</span></span> and <span class=
      "inline-equation"><span class="tex">$\rho
      _v^{(t)}$</span></span> for drifting too far apart. It
      prioritizes parameter settings for which the norm of their
      difference is small.</p>
      <p>The most expensive term in the objective is <span class=
      "inline-equation"><span class="tex">$\mathcal
      {L}_{\text{neg}}$</span></span> , the contribution of the
      zeroes to the conditional log likelihood. The objective is
      cheaper if we subsample the zeros. Rather than summing over
      all words which are not at position <em>i</em>, we sum over a
      subset of <em>n</em> negative samples <span class=
      "inline-equation"><span class="tex">$\mathcal
      {S}_{i}$</span></span> drawn at random. Mikolov et
      al.&nbsp;<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0025">25</a>
      call this negative sampling and recommend sampling from
      <span class="inline-equation"><span class=
      "tex">$\hat{p}$</span></span> , the unigram distribution
      raised to the power of 0.75.</p>
      <p>With negative sampling, we redefine <span class=
      "inline-equation"><span class="tex">$\mathcal
      {L}_{\text{neg}}$</span></span> as</p>
      <div class="table-responsive" id="eq7">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathcal
          {L}_{\text{neg}} = \sum _{i=1}^N \sum _{v\in \mathcal
          {S}_i}\log (1 - \sigma (\eta _{iv})).
          \end{align}</span><br />
          <span class="equation-number">(7)</span>
        </div>
      </div>This sum has fewer terms and reduces the contribution
      of the zeros to the objective. In a sense, this incurs a
      bias—the expectation with respect to the negative samples is
      not equal to the original objective—but “downweighting the
      zeros” can improve prediction accuracy [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0017">17</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0022">22</a>] and leads to significant
      computational gains.
      <p></p>
      <p><img src=
      "../../../data/deliveryimages.acm.org/10.1145/3190000/3185999/images/www2018-8-img1.svg"
      class="img-responsive" alt="" longdesc="" /></p>
      <p>We fit the objective (Equation&nbsp;(<a class="eqn" href=
      "#eq6">6</a>) with Equation&nbsp;(<a class="eqn" href=
      "#eq7">7</a>)) using stochastic gradients [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0034">34</a>] and with adaptive
      learning rates [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>]. The negative samples are resampled
      at each gradient step. Pseudo code is in Algorithm 1 . To
      avoid deriving the gradients of Equation&nbsp;(<a class="eqn"
      href="#eq6">6</a>), we implemented the algorithm in Edward
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0040">40</a>]. Edward is
      based on tensorflow [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0039">39</a>] and employs automatic
      differentiation.<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>3</sup></a></p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Time range and size of the three corpora
          analyzed in Section&nbsp;<a class="sec" href=
          "#sec-5">3</a>.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;"><strong>ArXiv
              ML</strong></td>
              <td style="text-align:center;">
              <strong>ACM</strong></td>
              <td style="text-align:center;"><strong>Senate
              speeches</strong></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;">2007 − 2015</td>
              <td style="text-align:center;">1951 − 2014</td>
              <td style="text-align:center;">1858 − 2009</td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <strong>slices</strong></td>
              <td style="text-align:center;">9</td>
              <td style="text-align:center;">64</td>
              <td style="text-align:center;">76</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>slice
              size</strong></td>
              <td style="text-align:center;">1 year</td>
              <td style="text-align:center;">1 year</td>
              <td style="text-align:center;">2 years</td>
            </tr>
            <tr>
              <td style="text-align:left;"><strong>vocab
              size</strong></td>
              <td style="text-align:center;">50k</td>
              <td style="text-align:center;">25k</td>
              <td style="text-align:center;">25k</td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <strong>words</strong></td>
              <td style="text-align:center;">6.5M</td>
              <td style="text-align:center;">21.6M</td>
              <td style="text-align:center;">13.7M</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Empirical
          Study</h2>
        </div>
      </header>
      <p>This empirical study has two parts. In a quantitative
      evaluation we benchmark dynamic embeddings against static
      embeddings [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0024">24</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0025">25</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0035">35</a>]. We found
      that dynamic embeddings improve over static embeddings in
      terms of the conditional likelihood of held-out predictions.
      Further, dynamic embeddings perform better than embeddings
      trained on the individual time slices [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>]. In a qualitative
      evaluation we use fitted dynamic embeddings to extract which
      word vectors change most and we visualize their dynamics.
      Dynamic embeddings provide a new window into how language
      changes.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Data</h3>
          </div>
        </header>
        <p>We study three datasets. Their details are summarized in
        Table&nbsp;<a class="tbl" href="#tab1">1</a>.</p>
        <section id="sec-7">
          <p><em>Machine Learning Papers (2007 - 2015).</em> This
          dataset contains the full text from all machine learning
          papers (tagged “stat.ML”) published on the ArXiv between
          April 2007 and June 2015. It spans 9 years and we treat
          each year as a time slice. The number of ArXiv papers
          about machine learning has increased over the years.
          There were 101 papers in 2007, while there were 1,573
          papers in 2014.</p>
        </section>
        <section id="sec-8">
          <p><em>Computer Science Abstracts (1951 - 2014).</em>
          This dataset contains abstracts of computer science
          papers published by the Association of Computing
          Machinery (ACM) from 1951 to 2014. We treat each year as
          a time slice and here too, the amount of data increases
          over the years. For 1953, there are only around 10
          abstracts and their combined length is only 471 words;
          the total length of the abstracts from 2009 is over
          2M.</p>
        </section>
        <section id="sec-9">
          <p><em>Senate Speeches (1858 - 2009).</em> This dataset
          contains all U.S. Senate speeches from 1858 to mid 2009.
          Here we treat every 2 years as a time slice. Unlike the
          other datasets, this is a transcript of spoken language.
          It contains many infrequent words that occur only in a
          few of the time slices.</p>
        </section>
        <section id="sec-10">
          <p><em>Preprocessing.</em> We convert the text to
          lowercase and strip it of all punctuation. Frequent
          n-grams such as <font style=
          "font-variant: small-caps">united states</font> are
          treated as a single term. The vocabulary consists of the
          25,000 most frequent terms and all words which are not in
          the vocabulary are removed. As in [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0025">25</a>], we
          additionally remove each word with probability
          <span class="inline-equation"><span class="tex">$p = 1 -
          \sqrt (\frac{10^{-5}}{f_i})$</span></span> where
          <em>f<sub>i</sub></em> is the frequency of the word. This
          effectively downsamples especially the frequent words and
          speeds up training.</p>
        </section>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span>
            Quantitative evaluation</h3>
          </div>
        </header>
        <p>We compare dembdemb to temb [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0015">15</a>] and semb [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0035">35</a>]. There
        are many embedding techniques, without dynamics, that enjoy
        comparable performance. For the semb, we study Bernoulli
        embeddings [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0035">35</a>], which are similar to cbow with
        negative sampling [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>]. For time-binned embeddings,
        Hamilton et al.&nbsp;<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a> train a separate embedding on each
        time slice.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185999/images/www2018-8-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">The dynamic embedding
            captures how the usage of the word <font style=
            "font-variant: small-caps">iraq</font> changes over the
            years (1858-2009). The <em>x</em>-axis is time and the
            <em>y</em>-axis is a one-dimensional projection of the
            embeddings using PCA. We include the embedding
            neighborhoods for <font style=
            "font-variant: small-caps">Iraq</font> in the years
            1858, 1954, 1980 and 2008.</span>
          </div>
        </figure>
        <p></p>
        <p><strong>Evaluation metric.</strong>&nbsp; From each time
        slice <span class="inline-equation"><span class="tex">$80
        \%$</span></span> of the words are used for training. A
        random subsample of <span class=
        "inline-equation"><span class="tex">$10 \%$</span></span>
        of the words is held out for validation and another
        <span class="inline-equation"><span class="tex">$10
        \%$</span></span> for testing. We evaluate models by
        held-out Bernoulli probability. Given a model, each
        held-out position (validation or testing) is associated
        with a Bernoulli probability for each vocabulary term. At
        that position, a better model assigns higher probability to
        the observed word and lower probability to the others. This
        metric is straightforward because the competing methods all
        produce Bernoulli conditional likelihoods
        (Equation&nbsp;(<a class="eqn" href="#eq1">1</a>)). Since
        we hold out chunks of consecutive words usually both a word
        and its context are held out. All methods require the words
        in the context to compute the conditional likelihoods.</p>
        <p>We report <span class="inline-equation"><span class=
        "tex">$\mathcal {L}_\text{eval} = \mathcal {L}_\text{pos} +
        \frac{1}{n}\mathcal {L}_\text{neg}$</span></span> , where
        <em>n</em> is the number of negative samples. Renormalizing
        with <em>n</em> assures that the metric is balanced. It
        equally weights the positive and negative examples. To make
        results comparable, all methods are trained with the same
        number of negative samples.</p>
        <p><strong>Model training and
        hyperparameters.</strong>&nbsp; Each method takes a maximum
        of 10 passes over the data. (The corresponding number of
        stochastic gradient steps depends on the size of the
        minibatches.) The parameters of semb are initialized
        randomly. We initialize both demb and temb from a fit of
        semb which has been trained from one pass, and then train
        for 9 additional passes.</p>
        <p>We set the dimension of the embeddings to 100 and the
        number of negative samples to 20. We study two context
        sizes, 2 and 8.</p>
        <p>Other parameters are set by validation error. All
        methods use validation error to set the initial learning
        rate <em>η</em> and minibatch sizes <em>m</em>. The model
        selects <em>η</em> ∈ [0.01, 0.1, 1, 10] and</p>
        <p><em>m</em> ∈ [0.001<em>N</em>, 0.0001<em>N</em>,
        0.00001<em>N</em>], where <em>N</em> is the size of
        training data. The only parameter specific to demb is the
        precision of the random drift. To have one less hyper
        parameter to tune, we fix the precision on the context
        vectors and the initial dynamic embeddings to <em>λ</em>
        <sub>0</sub> = <em>λ</em>/1000, a constant multiple of the
        precision on the dynamic embeddings. We choose <em>λ</em> ∈
        [1, 10] by validation error.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">semb temb demb demb consistently achieve
            highest held-out <span class=
            "inline-equation"><span class="tex">$\mathcal
            {L}_{\text{eval}}$</span></span> . We compare to semb
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0035">35</a>], and temb [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0015">15</a>]. The
            largest standard error on the held-out predictions is
            0.002 which means all reported results are
            significant.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;"></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td colspan="3" style="text-align:center;">
                  <strong>ArXiv ML</strong>
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:center;">context size 2</td>
                <td style="text-align:center;">context size 8</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  semb [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0035">35</a>]
                </td>
                <td style="text-align:center;">− 2.77</td>
                <td style="text-align:center;">− 2.54</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  temb [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0015">15</a>]
                </td>
                <td style="text-align:center;">− 2.97</td>
                <td style="text-align:center;">− 2.81</td>
              </tr>
              <tr>
                <td style="text-align:left;">demb [this paper]</td>
                <td style="text-align:center;">− 2.58</td>
                <td style="text-align:center;">− 2.44</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td colspan="3" style="text-align:center;">
                  <strong>Senate speeches</strong>
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:center;">context size 2</td>
                <td style="text-align:center;">context size 8</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  semb [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0035">35</a>]
                </td>
                <td style="text-align:center;">− 2.41</td>
                <td style="text-align:center;">− 2.29</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  temb [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0015">15</a>]
                </td>
                <td style="text-align:center;">− 2.44</td>
                <td style="text-align:center;">− 2.46</td>
              </tr>
              <tr>
                <td style="text-align:left;">demb [this paper]</td>
                <td style="text-align:center;">− 2.33</td>
                <td style="text-align:center;">− 2.28</td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td colspan="3" style="text-align:center;">
                  <strong>ACM</strong>
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:left;"></td>
                <td style="text-align:center;">context size 2</td>
                <td style="text-align:center;">context size 8</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  semb [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0035">35</a>]
                </td>
                <td style="text-align:center;">− 2.48</td>
                <td style="text-align:center;">− 2.30</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                  temb [<a class="bib" data-trigger="hover"
                  data-toggle="popover" data-placement="top" href=
                  "#BibPLXBIB0015">15</a>]
                </td>
                <td style="text-align:center;">− 2.55</td>
                <td style="text-align:center;">− 2.42</td>
              </tr>
              <tr>
                <td style="text-align:left;">demb [this paper]</td>
                <td style="text-align:center;">− 2.45</td>
                <td style="text-align:center;">− 2.27</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p><strong>Results.</strong>&nbsp; We train each model on
        each training set and use each validation set for selecting
        parameters like the minibatch size and the learning rate.
        Table&nbsp;<a class="tbl" href="#tab2">2</a> reports the
        results on the test set. Dynamic embeddings consistently
        have higher held-out likelihood.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Qualitative
            exploration</h3>
          </div>
        </header>
        <p>There are different reasons for a word's usage to change
        over the course of a collection. Words can become obsolete
        or obtain a new meaning. As society makes progress and
        words are used to describe that progress, that progress
        also gradually changes the meaning of words. A word might
        also have multiple alternative meanings. Over time, one
        meaning might become more relevant than the other. We now
        show how to use dynamic embeddings to explore text data and
        to discover such changes in the usage of words.</p>
        <p>A word's <em>embedding neighborhood</em> helps visualize
        its usage and how it changes over time. It is simply a list
        of other words with similar usage. For a given query word
        (e.g., <font style=
        "font-variant: small-caps">computer</font>) we take its
        index <em>v</em> and select the top ten words according
        to</p>
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            \text{neighborhood}(v, t) =
            \text{argsort}_{w}\left(\frac{\text{sign}(\rho
            _v^{(t)})^{\top }\rho ^{(t)}_w}{|| \rho ^{(t)}_v||\cdot
            ||\rho ^{(t)}_w||}\right). \end{align}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>
        <p></p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Embedding neighborhoods
            (Equation&nbsp;(<a class="eqn" href="#eq8">8</a>))
            reveal how the usage of a word changes over time. The
            embedding neighborhoods of <font style=
            "font-variant: small-caps">computer</font> and
            <font style="font-variant: small-caps">bush</font> were
            computed from a dynamic embedding fitted to Congress
            speeches (1858-2009). <font style=
            "font-variant: small-caps">computer</font> used to be a
            profession but today it is used to refer to the
            electronic device. The word <font style=
            "font-variant: small-caps">bush</font> is a plant but
            eventually in congress <font style=
            "font-variant: small-caps">Bush</font> is used to refer
            to the political figures. The embedding neighborhood of
            <font style="font-variant: small-caps">data</font>
            comes from a dynamic embedding fitted to ACM abstracts
            (1951-2014).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>computer</strong></font>
                  <strong>(Senate)</strong>
                  <hr />
                </th>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>bush</strong></font>
                  <strong>(Senate)</strong>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>1986</strong></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>1990</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">draftsman</td>
                <td style="text-align:center;">software</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">barberry</td>
                <td style="text-align:center;">cheney</td>
              </tr>
              <tr>
                <td style="text-align:center;">draftsmen</td>
                <td style="text-align:center;">computers</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">rust</td>
                <td style="text-align:center;">nonsense</td>
              </tr>
              <tr>
                <td style="text-align:center;">copyist</td>
                <td style="text-align:center;">copyright</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">bushes</td>
                <td style="text-align:center;">nixon</td>
              </tr>
              <tr>
                <td style="text-align:center;">photographer</td>
                <td style="text-align:center;">technological</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">borer</td>
                <td style="text-align:center;">reagan</td>
              </tr>
              <tr>
                <td style="text-align:center;">computers</td>
                <td style="text-align:center;">innovation</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">eradication</td>
                <td style="text-align:center;">george</td>
              </tr>
              <tr>
                <td style="text-align:center;">copyists</td>
                <td style="text-align:center;">mechanical</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">grasshoppers</td>
                <td style="text-align:center;">headed</td>
              </tr>
              <tr>
                <td style="text-align:center;">janitor</td>
                <td style="text-align:center;">hardware</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">cancer</td>
                <td style="text-align:center;">criticized</td>
              </tr>
              <tr>
                <td style="text-align:center;">accountant</td>
                <td style="text-align:center;">technologies</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">tick</td>
                <td style="text-align:center;">clinton</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td></td>
                <td></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td colspan="5" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>data</strong></font>
                  <strong>(ACM)</strong>
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <strong>1961</strong></td>
                <td style="text-align:center;">
                <strong>1969</strong></td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">
                <strong>1991</strong></td>
                <td style="text-align:center;">
                <strong>2014</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">directories</td>
                <td style="text-align:center;">repositories</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">voluminous</td>
                <td style="text-align:center;">data streams</td>
              </tr>
              <tr>
                <td style="text-align:center;">files</td>
                <td style="text-align:center;">voluminous</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">raw data</td>
                <td style="text-align:center;">voluminous</td>
              </tr>
              <tr>
                <td style="text-align:center;">bibliographic</td>
                <td style="text-align:center;">lineage</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">repositories</td>
                <td style="text-align:center;">raw data</td>
              </tr>
              <tr>
                <td style="text-align:center;">formatted</td>
                <td style="text-align:center;">metadata</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">data streams</td>
                <td style="text-align:center;">warehouses</td>
              </tr>
              <tr>
                <td style="text-align:center;">retrieval</td>
                <td style="text-align:center;">snapshots</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">data sources</td>
                <td style="text-align:center;">dws</td>
              </tr>
              <tr>
                <td style="text-align:center;">publishing</td>
                <td style="text-align:center;">data streams</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">volumes</td>
                <td style="text-align:center;">repositories</td>
              </tr>
              <tr>
                <td style="text-align:center;">archival</td>
                <td style="text-align:center;">raw data</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">dws</td>
                <td style="text-align:center;">data sources</td>
              </tr>
              <tr>
                <td style="text-align:center;">archives</td>
                <td style="text-align:center;">cleansing</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">dsms</td>
                <td style="text-align:center;">data mining</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>As an example, we fit a dynamic embedding fit to the
        Senate speeches. Table&nbsp;<a class="tbl" href=
        "#tab3">3</a> gives the embedding neighborhoods of
        <font style="font-variant: small-caps">computer</font> for
        the years 1858 and 1986. Its usage changed dramatically
        over the years. In 1858, a <font style=
        "font-variant: small-caps">computer</font> was a
        profession, a person who was hired to compute things. Now
        the profession is obsolete; <font style=
        "font-variant: small-caps">computer</font> refers to the
        electronic device.</p>
        <p>Table&nbsp;<a class="tbl" href="#tab3">3</a> provides
        another example, <font style=
        "font-variant: small-caps">bush</font>. In 1858 this word
        always referred to the plant. A <font style=
        "font-variant: small-caps">bush</font> still is a plant,
        but in the 1990’s, it usually refers to a politician.
        Unlike <font style=
        "font-variant: small-caps">computer</font>, where the
        embedding neighborhoods reveal two mutually exclusive
        meanings, the embedding neighborhoods of <font style=
        "font-variant: small-caps">bush</font> reflect which
        meaning is more prevalent in a given period.</p>
        <p>A final example in Table&nbsp;<a class="tbl" href=
        "#tab3">3</a> is the word <font style=
        "font-variant: small-caps">data</font>, from a demb of the
        ACM abstracts. The evolution of the embedding neighborhoods
        of <font style="font-variant: small-caps">data</font>
        reflects it changes meaning in the computer science
        literature.</p>
        <p><strong>Finding changing words with absolute
        drift.</strong>&nbsp; We have highlighted example words
        whose usage changes. However, not all words have changing
        usage. We now define a metric to discover which words
        change most.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">A list of the top 16 words whose dynamic
            embedding on Senate speeches changes most. The number
            represents the absolute drift (Equation&nbsp;(<a class=
            "eqn" href="#eq9">9</a>)). The dynamics of the
            capitalized words are in Table&nbsp;<a class="tbl"
            href="#tab5">5</a> and discussed in the text.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th colspan="4" style="text-align:center;">
                  <strong>words with largest drift
                  (Senate)</strong>
                  <hr />
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;"><font style=
                "font-variant: small-caps">iraq</font></td>
                <td style="text-align:left;">3.09</td>
                <td style="text-align:left;">coin</td>
                <td style="text-align:left;">2.39</td>
              </tr>
              <tr>
                <td style="text-align:left;">tax cuts</td>
                <td style="text-align:left;">2.84</td>
                <td style="text-align:left;">social security</td>
                <td style="text-align:left;">2.38</td>
              </tr>
              <tr>
                <td style="text-align:left;">health care</td>
                <td style="text-align:left;">2.62</td>
                <td style="text-align:left;"><font style=
                "font-variant: small-caps">fine</font></td>
                <td style="text-align:left;">2.38</td>
              </tr>
              <tr>
                <td style="text-align:left;">energy</td>
                <td style="text-align:left;">2.55</td>
                <td style="text-align:left;">signal</td>
                <td style="text-align:left;">2.38</td>
              </tr>
              <tr>
                <td style="text-align:left;">medicare</td>
                <td style="text-align:left;">2.55</td>
                <td style="text-align:left;">program</td>
                <td style="text-align:left;">2.36</td>
              </tr>
              <tr>
                <td style="text-align:left;"><font style=
                "font-variant: small-caps">discipline</font></td>
                <td style="text-align:left;">2.44</td>
                <td style="text-align:left;">moves</td>
                <td style="text-align:left;">2.35</td>
              </tr>
              <tr>
                <td style="text-align:left;">text</td>
                <td style="text-align:left;">2.41</td>
                <td style="text-align:left;">credit</td>
                <td style="text-align:left;">2.34</td>
              </tr>
              <tr>
                <td style="text-align:left;"><font style=
                "font-variant: small-caps">values</font></td>
                <td style="text-align:left;">2.40</td>
                <td style="text-align:left;"><font style=
                "font-variant: small-caps">unemployment</font></td>
                <td style="text-align:left;">2.34</td>
              </tr>
            </tbody>
          </table>
        </div>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185999/images/www2018-8-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">According to demb fitted to
            the Senate Speeches, most words change most in the
            1947-1947 time slice.</span>
          </div>
        </figure>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">Embedding neighborhoods extracted from a
            dynamic embedding fitted to Senate speeches (1858 -
            2009). <font style=
            "font-variant: small-caps">discipline</font>,
            <font style="font-variant: small-caps">values</font>,
            <font style="font-variant: small-caps">fine</font>, and
            <font style=
            "font-variant: small-caps">unemployment</font> are
            within the 16 words whose dynamic embedding has largest
            absolute drift. (Table&nbsp;<a class="tbl" href=
            "#tab4">4</a>).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>discipline</strong></font>
                  <hr />
                </th>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>values</strong></font>
                  <hr />
                </th>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>fine</strong></font>
                  <hr />
                </th>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>unemployment</strong></font>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>2004</strong></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>2000</strong></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>2004</strong></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>2000</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">hazing</td>
                <td style="text-align:center;">balanced</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">fluctuations</td>
                <td style="text-align:center;">sacred</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">luxurious</td>
                <td style="text-align:center;">punished</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">unemployed</td>
                <td style="text-align:center;">jobless</td>
              </tr>
              <tr>
                <td style="text-align:center;">westpoint</td>
                <td style="text-align:center;">balancing</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">value</td>
                <td style="text-align:center;">inalienable</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">finest</td>
                <td style="text-align:center;">penitentiaries</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">depression</td>
                <td style="text-align:center;">rate</td>
              </tr>
              <tr>
                <td style="text-align:center;">assaulting</td>
                <td style="text-align:center;">fiscal</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">currencies</td>
                <td style="text-align:center;">unique</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">coarse</td>
                <td style="text-align:center;">imprisonment</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">acute</td>
                <td style="text-align:center;">depression</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class=
            "table-title">Using dynamic embeddings we can study a
            social phenomenon of interest. We pick a target word of
            interest, such as <font style=
            "font-variant: small-caps">jobs</font> or <font style=
            "font-variant: small-caps">prostitution</font> and
            create their embedding neighborhoods
            (Equation&nbsp;(<a class="eqn" href=
            "#eq8">8</a>)).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th colspan="3" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>jobs</strong></font>
                  <hr />
                </th>
                <th style="text-align:center;"></th>
                <th colspan="6" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>prostitution</strong></font>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>1938</strong></th>
                <th style="text-align:center;">
                <strong>2008</strong></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">
                <strong>1858</strong></th>
                <th style="text-align:center;">
                <strong>1930</strong></th>
                <th style="text-align:center;">
                <strong>1945</strong></th>
                <th style="text-align:center;">
                <strong>1962</strong></th>
                <th style="text-align:center;">
                <strong>1988</strong></th>
                <th style="text-align:center;">
                <strong>1990</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">employment</td>
                <td style="text-align:center;">unemployed</td>
                <td style="text-align:center;">job</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">punishing</td>
                <td style="text-align:center;">punishing</td>
                <td style="text-align:center;">indecent</td>
                <td style="text-align:center;">indecent</td>
                <td style="text-align:center;">intimidation</td>
                <td style="text-align:center;">servitude</td>
              </tr>
              <tr>
                <td style="text-align:center;">unemployed</td>
                <td style="text-align:center;">employment</td>
                <td style="text-align:center;">create</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">immoral</td>
                <td style="text-align:center;">immoral</td>
                <td style="text-align:center;">vile</td>
                <td style="text-align:center;">harassment</td>
                <td style="text-align:center;">prostitution</td>
                <td style="text-align:center;">harassment</td>
              </tr>
              <tr>
                <td style="text-align:center;">overtime</td>
                <td style="text-align:center;">job</td>
                <td style="text-align:center;">creating</td>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">illegitimate</td>
                <td style="text-align:center;">bootlegging</td>
                <td style="text-align:center;">immoral</td>
                <td style="text-align:center;">intimidation</td>
                <td style="text-align:center;">counterfeit</td>
                <td style="text-align:center;">intimidation</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>One way to find words that change is to use <em>absolute
        drift</em>. For word <em>v</em>, it is</p>
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \text{drift}(v) =
            ||\rho _{v}^{(T)} - \rho _{v}^{(0)}||.
            \end{align}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>This is the Euclidean distance between the word's
        embedding at the last and at the first time slice.
        <p></p>
        <p>In the Senate speeches, Table&nbsp;<a class="tbl" href=
        "#tab4">4</a> shows the 16 words that have largest absolute
        drift. The word <font style=
        "font-variant: small-caps">iraq</font> has largest drift.
        Figure&nbsp;<a class="fig" href="#fig3">3</a> highlights
        <font style="font-variant: small-caps">iraq</font>’s
        embedding neighborhood in four time slices: 1858, 1950,
        1980, and 2008. At first the neighborhood contains other
        countries and regions. Later, Arab countries move to the
        top of the neighborhood, suggesting that the speeches start
        to use rhetoric more specific to Arab countries. In 1980,
        Iraq invades Iran and the Iran-Iraq war begins. In these
        years, words such as <font style=
        "font-variant: small-caps">troops</font>, and <font style=
        "font-variant: small-caps">invasion</font> appear in the
        embedding neighborhood. Eventually, by 2008, the
        neighborhood contains <font style=
        "font-variant: small-caps">terror</font>, <font style=
        "font-variant: small-caps">terrorism</font>, and
        <font style="font-variant: small-caps">saddam</font>.</p>
        <p>Four other words with large drift are <font style=
        "font-variant: small-caps">discipline</font>, <font style=
        "font-variant: small-caps">values</font>, <font style=
        "font-variant: small-caps">fine</font> and <font style=
        "font-variant: small-caps">unemployment</font>
        (Table&nbsp;<a class="tbl" href="#tab4">4</a>).
        Table&nbsp;<a class="tbl" href="#tab5">5</a> shows their
        embedding neighborhoods. Of these words, <font style=
        "font-variant: small-caps">discipline</font>, <font style=
        "font-variant: small-caps">values</font> and, <font style=
        "font-variant: small-caps">fine</font> have multiple
        meanings. Their neighborhoods reflect how the dominant
        meaning changes over time. For example, <font style=
        "font-variant: small-caps">values</font> can be either a
        numerical quantity or can be used to refer to moral values
        and principles. In contrast, <font style=
        "font-variant: small-caps">iraq</font> and <font style=
        "font-variant: small-caps">unemployment</font> are both
        words which have always had the same definition. Yet, the
        evolution of their neighborhood captures changes in the way
        they are used.</p>
        <p><strong>Changepoint analysis.</strong>&nbsp; We use the
        fitted dynamic embeddings to find instances in time where a
        word's usage changes drastically. We make no assumption
        that a word's meaning makes only a single phase transition
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0020">20</a>]. Since
        in our formulation of demb the context vectors are shared
        between all time slices, the embeddings are grounded in one
        semantic space and no postprocessing is needed to align the
        embeddings. We can directly compute large jumps in word
        usage on the learned embedding vectors.</p>
        <p>For each word, we compute a list of time slices where
        the word's usage changed most.</p>
        <div class="table-responsive" id="eq10">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \text{max
            change}(v) = \text{argsort}_{t}\left(\frac{||\rho
            _v^{(t)} - \rho ^{(t-1)}_v||}{\sum _w ||\rho _w^{(t)} -
            \rho ^{(t-1)}_w||}\right). \end{align}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>The changes in time slice <em>t</em> are normalized
        by how much all other words changed within the same time
        slice. The normalization, makes the <em>max change</em>
        ranking sensitive to time slices in which a word's
        embedding drifted farthest, compared to how far other words
        drifted within the time slice.
        <p></p>
        <p>For example, for the word <font style=
        "font-variant: small-caps">iraq</font> the largest change
        is in the years 1990-1992. Indeed, that year the Gulf war
        started. Note that this is consistent with
        Figure&nbsp;<a class="fig" href="#fig3">3</a> where we see
        in the one dimensional projection of the trajectory of the
        embedding a large jump around the year 1990. The trajectory
        in the Figure captures only the variation in the first
        principal component, while Equation&nbsp;<a class="eqn"
        href="#eq10">10</a> measures difference of embedding
        vectors in all the dimensions combined.</p>
        <p>Next, we examine in which years many words changed most
        in terms of their usage. In Figure <a class="fig" href=
        "#fig4">4</a> is a histogram of the years in which each
        word changed the most. For example, <font style=
        "font-variant: small-caps">iraq</font> falls into the
        1990-1992 bin, together with almost 300 other words which
        also had their largest relative change in 1990 - 1992. We
        can see that the bin with the most words (marked in red) is
        1946-1947 which marks the end of the Second World War.
        Almost 1000 words had their largest relative change in that
        time slice.</p>
        <p>In Table <a class="tbl" href="#tab7">7</a> is a list of
        the 10 words with the largest change in the years
        1946-1947. On top of the list is <font style=
        "font-variant: small-caps">marshall</font>, the middle name
        of John Marshall Harlan, and John Marshall Harlan II,
        father and son who both served as U.S. Supreme Court
        Justices. It is also the last name of George Marshall who
        became the U.S. Secretary of State in 1947. He conceived
        and carried out the Marshall plan, an economic relief
        program to aid post-war Europe.</p>
        <div class="table-responsive" id="tab7">
          <div class="table-caption">
            <span class="table-number">Table 7:</span> <span class=
            "table-title">Dynamic embeddings identify <font style=
            "font-variant: small-caps">marshall</font>, as the word
            changing most in 1946-1947. On the left is a list of
            the top words with largest change in the 1946-1947 time
            bin (marked red in Figure&nbsp;<a class="fig" href=
            "#fig4">4</a>). On the right, are the embedding
            neighborhoods of <font style=
            "font-variant: small-caps">marshall</font> before and
            after the jump.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <th style="text-align:left;"><strong>top change in
                1946</strong></th>
                <th style="text-align:left;"></th>
                <th colspan="2" style="text-align:center;">
                  <font style=
                  "font-variant: small-caps"><strong>marshall</strong></font>
                  <strong>(Senate)</strong>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;">1. marshall</th>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">
                <strong>1944</strong></th>
                <th style="text-align:left;">
                <strong>1948</strong></th>
              </tr>
            </tbody>
            <tbody>
              <tr>
                <td style="text-align:left;">2. atlantic</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">wheaton</td>
                <td style="text-align:left;">plan</td>
              </tr>
              <tr>
                <td style="text-align:left;">3. korea</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">taney</td>
                <td style="text-align:left;">satellites</td>
              </tr>
              <tr>
                <td style="text-align:left;">4. douglas</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">harlan</td>
                <td style="text-align:left;">britain</td>
              </tr>
              <tr>
                <td style="text-align:left;">5. holland</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">vs</td>
                <td style="text-align:left;">great britain</td>
              </tr>
              <tr>
                <td style="text-align:left;">6. steam</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">gibbons</td>
                <td style="text-align:left;">acheson</td>
              </tr>
              <tr>
                <td style="text-align:left;">7. security</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">mcreynolds</td>
                <td style="text-align:left;">democracies</td>
              </tr>
              <tr>
                <td style="text-align:left;">8. truman</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">waite</td>
                <td style="text-align:left;">france</td>
              </tr>
              <tr>
                <td style="text-align:left;">9. plan</td>
                <td style="text-align:left;"></td>
                <td style="text-align:left;">webster</td>
                <td style="text-align:left;">western europe</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In Table <a class="tbl" href="#tab7">7</a> are the
        embedding neighborhoods for <font style=
        "font-variant: small-caps">marshall</font> before and after
        the 1946-1947 time bin. In 1944-1945, <font style=
        "font-variant: small-caps">marshall</font> is similar to
        other names with importance to the U.S. judicial system but
        by 1948-1950 the most similar word is <font style=
        "font-variant: small-caps">plan</font> as the Marshall plan
        is now frequently discussed in the U.S. Senate
        Speeches.</p>
        <p><strong>Dynamic embeddings as a tool to study a
        text.</strong>&nbsp; Our hope is that dynamic embeddings
        provide a suggestive tool for understanding change in
        language. For example, researchers interested in
        <font style="font-variant: small-caps">unemployment</font>
        can complement their investigation by looking at the
        embedding neighborhood of related words such as
        <font style="font-variant: small-caps">employment</font>,
        <font style="font-variant: small-caps">jobs</font> or
        <font style="font-variant: small-caps">labor</font>. In
        Table&nbsp;<a class="tbl" href="#tab6">6</a> we list the
        neighborhoods of <font style=
        "font-variant: small-caps">jobs</font> for the years 1858,
        1938, and 2008. In 2008 the embedding neighborhood contains
        words like <font style=
        "font-variant: small-caps">create</font> and <font style=
        "font-variant: small-caps">creating</font>, suggesting a
        different outlook on <font style=
        "font-variant: small-caps">jobs</font> than in earlier
        years.</p>
        <p>Another interesting example is <font style=
        "font-variant: small-caps">prostitution</font>. It used to
        be <font style="font-variant: small-caps">immoral</font>
        and <font style="font-variant: small-caps">vile</font>,
        went to <font style=
        "font-variant: small-caps">indecent</font>, and in modern
        days it is considered <font style=
        "font-variant: small-caps">harassment</font>. We note the
        word <font style=
        "font-variant: small-caps">prostitution</font> is not a
        frequent word. On average, it is used once per time slice
        and, in two thirds of the time slices, it is not mentioned
        at all. Yet, the model is able to learn about <font style=
        "font-variant: small-caps">prostitution</font> and the
        temporal evolution of the embedding neighborhood reveals
        how over the years a judgemental stance turns into concern
        over a social issue.</p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Summary</h2>
        </div>
      </header>
      <p>We described dynamic embeddings, distributed
      representations of words that drift over the course of the
      collection. Building on Rudolph et al.&nbsp;<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0035">35</a>, we formulate word
      embeddings with conditional probabilistic models and then
      incorporate dynamics with a Gaussian random walk prior. We
      fit dynamic embeddings to language data using stochastic
      optimization.</p>
      <p>We used dynamic embeddings to analyze 3 datasets: 8 years
      of machine learning papers, 63 years of computer science
      abstracts, and 151 years of U.S. Senate speeches. Dynamic
      embeddings provide a better fit than static embeddings and
      other methods that account for time. In addition, dynamic
      embeddings can help identify interesting ways in which
      language changes. A word's meaning can change (e.g.,
      <font style="font-variant: small-caps">computer</font>); its
      dominant meaning can change (e.g., <font style=
      "font-variant: small-caps">values</font>); or its related
      subject matter can change (e.g., <font style=
      "font-variant: small-caps">iraq</font>).</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2>Acknowledgements</h2>
        </div>
      </header>
      <p>We thank Francisco Ruiz and Liping Liu for discussion and
      helpful suggestions, Elliot Ash and Suresh Naidu for access
      to the Congress speeches, and Aaron Plasek and Matthew Jones
      for access to the ACM abstracts. This work is supported by
      ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, the
      Alfred P. Sloan Foundation, and the John Simon Guggenheim
      Foundation.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Jean Aitchison. 2001.
        <em><em>Language change: progress or decay?</em></em>
        Cambridge University Press.</li>
        <li id="BibPLXBIB0002" label="[2]">Barry&nbsp;C Arnold,
        Enrique Castillo, Jose&nbsp;Maria Sarabia, <em>et al.</em>
        2001. Conditionally specified distributions: an
        introduction (with comments and a rejoinder by the
        authors). <em><em>Statist. Sci.</em></em> 16, 3 (2001),
        249–274.</li>
        <li id="BibPLXBIB0003" label="[3]">Sanjeev Arora, Yuanzhi
        Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2015.
        RAND-WALK: A latent variable model approach to word
        embeddings. <em><em>arXiv preprint
        arXiv:1502.03520</em></em> (2015).</li>
        <li id="BibPLXBIB0004" label="[4]">Robert Bamler and
        Stephan Mandt. 2017. Dynamic Word Embeddings via Skip-gram
        Filtering. <em><em>arXiv preprint
        arXiv:1702.08359</em></em> (2017).</li>
        <li id="BibPLXBIB0005" label="[5]">Pierpaolo Basile,
        Annalina Caputo, and Giovanni Semeraro. 2014. Analysing
        word meaning over time by exploiting temporal random
        indexing. In <em><em>First Italian Conference on
        Computational Linguistics CLiC-it</em></em> .</li>
        <li id="BibPLXBIB0006" label="[6]">Yoshua Bengio, Réjean
        Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A
        neural probabilistic language model. <em><em>Journal of
        machine learning research</em></em> 3, Feb (2003),
        1137–1155.</li>
        <li id="BibPLXBIB0007" label="[7]">Christopher&nbsp;M
        Bishop. 2006. Machine learning and pattern recognition.
        <em><em>Information Science and Statistics. Springer,
        Heidelberg</em></em> (2006).</li>
        <li id="BibPLXBIB0008" label="[8]">David&nbsp;M Blei and
        John&nbsp;D Lafferty. 2006. Dynamic topic models. In
        <em><em>Proceedings of the 23rd international conference on
        Machine learning</em></em> . ACM, 113–120.</li>
        <li id="BibPLXBIB0009" label="[9]">David&nbsp;M Blei,
        Andrew&nbsp;Y Ng, and Michael&nbsp;I Jordan. 2003. Latent
        dirichlet allocation. <em><em>Journal of machine Learning
        research</em></em> 3, Jan (2003), 993–1022.</li>
        <li id="BibPLXBIB0010" label="[10]">Ryan Cotterell, Adam
        Poliak, Benjamin Van&nbsp;Durme, and Jason Eisner. 2017.
        Explaining and Generalizing Skip-Gram through Exponential
        Family Principal Component Analysis. <em><em>EACL
        2017</em></em> (2017), 175.</li>
        <li id="BibPLXBIB0011" label="[11]">John Duchi, Elad Hazan,
        and Yoram Singer. 2011. Adaptive subgradient methods for
        online learning and stochastic optimization.
        <em><em>Journal of Machine Learning Research</em></em> 12,
        Jul (2011), 2121–2159.</li>
        <li id="BibPLXBIB0012" label="[12]">Lea Frermann and
        Mirella Lapata. 2016. A Bayesian Model of Diachronic
        Meaning Change. <em><em>Transactions of the Association for
        Computational Linguistics</em></em> 4 (2016), 31–45.</li>
        <li id="BibPLXBIB0013" label="[13]">S. Gerrish and D. Blei.
        2010. A Language-based Approach to Measuring Scholarly
        Impact. In <em><em>International Conference on Machine
        Learning</em></em> .</li>
        <li id="BibPLXBIB0014" label="[14]">Michael Gutmann and
        Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new
        estimation principle for unnormalized statistical models.
        In <em><em>AISTATS</em></em> .</li>
        <li id="BibPLXBIB0015" label="[15]">William&nbsp;L
        Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic
        Word Embeddings Reveal Statistical Laws of Semantic Change.
        <em><em>arXiv preprint arXiv:1605.09096</em></em>
        (2016).</li>
        <li id="BibPLXBIB0016" label="[16]">Zellig&nbsp;S Harris.
        1954. Distributional structure. <em><em>Word</em></em> 10,
        2-3 (1954), 146–162.</li>
        <li id="BibPLXBIB0017" label="[17]">Yifan Hu, Yehuda Koren,
        and Chris Volinsky. 2008. Collaborative filtering for
        implicit feedback datasets. In <em><em>Data Mining, 2008.
        ICDM’08. Eighth IEEE International Conference on</em></em>
        . Ieee, 263–272.</li>
        <li id="BibPLXBIB0018" label="[18]">Yoon Kim, Yi-I Chiu,
        Kentaro Hanaki, Darshan Hegde, and Slav Petrov. 2014.
        Temporal analysis of language through neural language
        models. <em><em>arXiv preprint arXiv:1405.3515</em></em>
        (2014).</li>
        <li id="BibPLXBIB0019" label="[19]">Simon Kirby, Mike
        Dowman, and Thomas&nbsp;L Griffiths. 2007. Innateness and
        culture in the evolution of language. <em><em>Proceedings
        of the National Academy of Sciences</em></em> 104,
        12(2007), 5241–5245.</li>
        <li id="BibPLXBIB0020" label="[20]">Vivek Kulkarni, Rami
        Al-Rfou, Bryan Perozzi, and Steven Skiena. 2015.
        Statistically significant detection of linguistic change.
        In <em><em>Proceedings of the 24th International Conference
        on World Wide Web</em></em> . ACM, 625–635.</li>
        <li id="BibPLXBIB0021" label="[21]">Omer Levy and Yoav
        Goldberg. 2014. Neural word embedding as implicit matrix
        factorization. In <em><em>Neural Information Processing
        Systems</em></em> . 2177–2185.</li>
        <li id="BibPLXBIB0022" label="[22]">Dawen Liang, Laurent
        Charlin, James McInerney, and David&nbsp;M Blei. 2016.
        Modeling user exposure in recommendation. In
        <em><em>Proceedings of the 25th International Conference on
        World Wide Web</em></em> . International World Wide Web
        Conferences Steering Committee, 951–961.</li>
        <li id="BibPLXBIB0023" label="[23]">Rada Mihalcea and Vivi
        Nastase. 2012. Word epoch disambiguation: Finding how words
        change over time. In <em><em>Proceedings of the 50th Annual
        Meeting of the Association for Computational Linguistics:
        Short Papers-Volume 2</em></em> . Association for
        Computational Linguistics, 259–263.</li>
        <li id="BibPLXBIB0024" label="[24]">Tomas Mikolov, Kai
        Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
        estimation of word representations in vector space.
        <em><em>ICLR Workshop Proceedings.
        arXiv:1301.3781</em></em> (2013).</li>
        <li id="BibPLXBIB0025" label="[25]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Greg&nbsp;S Corrado, and Jeff Dean.
        2013. Distributed representations of words and phrases and
        their compositionality. In <em><em>Neural Information
        Processing Systems</em></em> . 3111–3119.</li>
        <li id="BibPLXBIB0026" label="[26]">Tomas Mikolov,
        Wen-T&nbsp;au Yih, and Geoffrey Zweig. 2013. Linguistic
        Regularities in Continuous Space Word Representations.. In
        <em><em>HLT-NAACL</em></em> . 746–751.</li>
        <li id="BibPLXBIB0027" label="[27]">Sunny Mitra, Ritwik
        Mitra, Suman&nbsp;Kalyan Maity, Martin Riedl, Chris
        Biemann, Pawan Goyal, and Animesh Mukherjee. 2015. An
        automatic approach to identify word sense changes in text
        media across timescales. <em><em>Natural Language
        Engineering</em></em> 21, 05 (2015), 773–798.</li>
        <li id="BibPLXBIB0028" label="[28]">Sunny Mitra, Ritwik
        Mitra, Martin Riedl, Chris Biemann, Animesh Mukherjee, and
        Pawan Goyal. 2014. That's sick dude!: Automatic
        identification of word sense change across different
        timescales. <em><em>arXiv preprint
        arXiv:1405.4392</em></em> (2014).</li>
        <li id="BibPLXBIB0029" label="[29]">Andriy Mnih and
        Geoffrey&nbsp;E Hinton. 2009. A scalable hierarchical
        distributed language model. In <em><em>Advances in neural
        information processing systems</em></em> . 1081–1088.</li>
        <li id="BibPLXBIB0030" label="[30]">Andriy Mnih and Koray
        Kavukcuoglu. 2013. Learning word embeddings efficiently
        with noise-contrastive estimation. In <em><em>Neural
        Information Processing Systems</em></em> . 2265–2273.</li>
        <li id="BibPLXBIB0031" label="[31]">Frederic Morin and
        Yoshua Bengio. 2005. Hierarchical Probabilistic Neural
        Network Language Model.. In <em><em>Aistats</em></em> ,
        Vol.&nbsp;5. Citeseer, 246–252.</li>
        <li id="BibPLXBIB0032" label="[32]">Kevin&nbsp;P Murphy.
        2012. <em><em>Machine learning: a probabilistic
        perspective</em></em> . MIT press.</li>
        <li id="BibPLXBIB0033" label="[33]">Jeffrey Pennington,
        Richard Socher, and Christopher&nbsp;D Manning. 2014.
        Glove: Global Vectors for Word Representation.. In
        <em><em>Conference on Empirical Methods on Natural Language
        Processing</em></em> , Vol.&nbsp;14. 1532–1543.</li>
        <li id="BibPLXBIB0034" label="[34]">Herbert Robbins and
        Sutton Monro. 1951. A stochastic approximation method.
        <em><em>The annals of mathematical statistics</em></em>
        (1951), 400–407.</li>
        <li id="BibPLXBIB0035" label="[35]">Maja Rudolph, Francisco
        Ruiz, Stephan Mandt, and David Blei. 2016. Exponential
        Family Embeddings. In <em><em>Advances in Neural
        Information Processing Systems</em></em> . 478–486.</li>
        <li id="BibPLXBIB0036" label="[36]">David&nbsp;E Rumelhart,
        Geoffrey&nbsp;E Hinton, and Ronald&nbsp;J Williams. 1986.
        Learning representations by back-propagating errors.
        <em><em>Nature</em></em> 323(1986), 9.</li>
        <li id="BibPLXBIB0037" label="[37]">Eyal Sagi, Stefan
        Kaufmann, and Brady Clark. 2011. Tracing semantic change
        with latent semantic analysis. <em><em>Current methods in
        historical semantics</em></em> (2011), 161–183.</li>
        <li id="BibPLXBIB0038" label="[38]">Xuri Tang, Weiguang Qu,
        and Xiaohe Chen. 2016. Semantic change computation: A
        successive approach. <em><em>World Wide Web</em></em> 19, 3
        (2016), 375–415.</li>
        <li id="BibPLXBIB0039" label="[39]">Tensorflow Team. 2015.
        TensorFlow: Large-Scale Machine Learning on Heterogeneous
        Systems. (2015). http://tensorflow.org/Software available
        from tensorflow.org.</li>
        <li id="BibPLXBIB0040" label="[40]">Dustin Tran, Alp
        Kucukelbir, Adji&nbsp;B. Dieng, Maja Rudolph, Dawen Liang,
        and David&nbsp;M. Blei. 2016. Edward: A library for
        probabilistic modeling, inference, and criticism.
        <em><em>arXiv preprint arXiv:1610.09787</em></em>
        (2016).</li>
        <li id="BibPLXBIB0041" label="[41]">George&nbsp;E Uhlenbeck
        and Leonard&nbsp;S Ornstein. 1930. On the theory of the
        Brownian motion. <em><em>Physical review</em></em> 36, 5
        (1930), 823.</li>
        <li id="BibPLXBIB0042" label="[42]">Luke Vilnis and Andrew
        McCallum. 2015. Word representations via Gaussian
        embedding. In <em><em>International Conference on Learning
        Representations</em></em> .</li>
        <li id="BibPLXBIB0043" label="[43]">C. Wang, D. Blei, and
        D. Heckerman. 2008. Continuous Time Dynamic Topic Models.
        In <em><em>Uncertainty in Artificial Intelligence
        (UAI)</em></em> .</li>
        <li id="BibPLXBIB0044" label="[44]">Xuerui Wang and Andrew
        McCallum. 2006. Topics over time: a non-Markov
        continuous-time model of topical trends. In
        <em><em>Proceedings of the 12th ACM SIGKDD international
        conference on Knowledge discovery and data mining</em></em>
        . ACM, 424–433.</li>
        <li id="BibPLXBIB0045" label="[45]">Derry&nbsp;Tanti Wijaya
        and Reyyan Yeniterzi. 2011. Understanding semantic change
        of words over centuries. In <em><em>Proceedings of the 2011
        international workshop on DETecting and Exploiting Cultural
        diversiTy on the social web</em></em> . ACM, 35–40.</li>
        <li id="BibPLXBIB0046" label="[46]">Zijun Yao, Yifan Sun,
        Weicong Ding, Nikhil Rao, and Hui Xiong. 2017. Discovery of
        Evolving Semantics through Dynamic Word Embedding Learning.
        <em><em>arXiv preprint arXiv:1703.00607</em></em>
        (2017).</li>
        <li id="BibPLXBIB0047" label="[47]">D. Yogatama, C. Wang,
        B. Routledge, N.&nbsp;A Smith, and E. Xing. 2014. Dynamic
        Language Models for Streaming Text. <em><em>Transactions of
        the Association for Computational Linguistics</em></em> 2
        (2014), 181–192.</li>
        <li id="BibPLXBIB0048" label="[48]">Yating Zhang, Adam
        Jatowt, Sourav&nbsp;S Bhowmick, and Katsumi Tanaka. 2016.
        The Past is Not a Foreign Country: Detecting Semantically
        Similar Terms across Time. <em><em>IEEE Transactions on
        Knowledge and Data Engineering</em></em> 28, 10(2016),
        2793–2807.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>Multinomial
    embeddings [<a class="bib" data-trigger="hover" data-toggle=
    "popover" data-placement="top" href="#BibPLXBIB0035">35</a>]
    model each indicator vector <em>x<sub>i</sub></em> with a
    categorical conditional distribution, but this requires
    expensive normalization in form of a softmax function. For
    computational efficiency, one can replace the softmax with the
    hierarchical softmax [<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0031">31</a>] or employ approaches related to noise
    contrastive estimation [<a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover"
    data-toggle="popover" data-placement="top" href=
    "#BibPLXBIB0030">30</a>]. Bernoulli embeddings relax the
    one-hot constraint of <em>x<sub>i</sub></em> , and work well in
    practice; they relate to the negative sampling [<a class="bib"
    data-trigger="hover" data-toggle="popover" data-placement="top"
    href="#BibPLXBIB0025">25</a>].</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Because α and ρ
    appear only as inner products in &nbsp;Equation&nbsp;(<a class=
    "eqn" href="#eq2">2</a>), we can capture that their
    interactions change over time even by placing temporal dynamics
    on the embeddings ρ only. Exploring dynamics in α is a subject
    for future study.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>Code is
    available at <a class="link-inline force-break" href=
    "http://github.com/mariru/dynamic_bernoulli_embeddings">http://github.com/mariru/dynamic_bernoulli_embeddings</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3185999">https://doi.org/10.1145/3178876.3185999</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
