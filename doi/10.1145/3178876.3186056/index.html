<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Arrays of (locality-sensitive) Count Estimators (ACE): Anomaly Detection on the Edge</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Arrays of (locality-sensitive) Count Estimators (ACE): Anomaly Detection on the Edge</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Chen</span>      <span class="surName">Luo</span>,     Rice University, Houston, Texas, <a href="mailto:cl67@rice.edu">cl67@rice.edu</a>     </div>     <div class="author">     <span class="givenName">Anshumali</span>      <span class="surName">Shrivastava</span>,     Rice University, Houston, Texas, <a href="mailto:anshumali@rice.edu">anshumali@rice.edu</a>     </div>            </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186056" target="_blank">https://doi.org/10.1145/3178876.3186056</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Anomaly detection is one of the frequent and important subroutines deployed in large-scale data processing applications. Even being a well-studied topic, existing techniques for unsupervised anomaly detection require storing significant amounts of data, which is prohibitive from memory, latency and privacy perspectives, especially for small mobile devices which has ultra-low memory budget and limited computational power. In this paper, we propose ACE (Arrays of (locality-sensitive) Count Estimators) algorithm that can be 60x faster than most state-of-the-art unsupervised anomaly detection algorithms. In addition, ACE has appealing privacy properties. Our experiments show that ACE algorithm has significantly smaller memory footprints (< 4<em>MB</em> in our experiments) which can exploit Level 3 cache of any modern processor. At the core of the ACE algorithm, there is a novel statistical estimator which is derived from the sampling view of Locality Sensitive Hashing (LSH). This view is significantly different and efficient than the widely popular view of LSH for near-neighbor search. We show the superiority of ACE algorithm over 11 popular baselines on 3 benchmark datasets, including the KDD-Cup99 data which is the largest available public benchmark comprising of more than half a million entries with ground truth anomaly labels.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Chen Luo and Anshumali Shrivastava. 2018. Arrays of (locality-sensitive) Count Estimators (ACE): Anomaly Detection on the Edge. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018 (WWW 2018),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186056" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186056</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>The problem of anomaly (or outlier) detections is the task of identifying instances (or patterns) in data that do not conform to the expected behavior&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. These non-conforming examples are popularly referred to as anomalies, or outliers, sometimes interchangeably.</p>    <p>Anomaly detection can be either supervised&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] or unsupervised&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. Supervised anomaly detection leverages machine learning algorithms, such as classification, over datasets labeled as anomalous or non-anomalous. However, there are three major issues with supervised anomaly detection algorithms: 1) In most applications, label information about anomalies is not available; 2) Anomalies are rare, and hence there is a huge class imbalance, and 3) Supervised algorithms need to be re-trained for drifting data distributions with new label information. Drifting data distribution is quite common in big-data systems, where supervised re-training is prohibitive. Therefore, we are interested in unsupervised anomaly detection which does not require any label information, and which can automatically deal with changes in data distributions over time. We briefly describe some of the modern challenges for unsupervised anomaly detection that we will address in this work.</p>    <p>     <strong>Challenge 1: High-Speed Drifting Data:</strong> Many applications demand fast-response and real-time inference from dynamic and drifting high volumes of sensor data over time. Most anomaly detection applications, for example over the web-network servers, require dealing with unprecedented amounts of data in a fraction of seconds. The data distribution is constantly changing, and it is often bursty&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. Detecting anomaly events in real-time, such as DDoS (Distributed Denial of Service) attacks, network failures, <em>etc.</em>, is highly beneficial in monitoring network performance.</p>    <p>     <strong>Challenge 2: Ultra-Low Memory Budget:</strong> In many high-speed streaming applications, such as High Energy Physics (HEP) and network servers, any algorithm requiring to store and process a significant fraction of data is prohibitive. Another critical pushing need for ultra-low memory algorithm is anomaly detection on mobile phones or smart sensors. Algorithms which require significant resources are prohibitive for low-resource platforms.</p>    <p>     <strong>Challenge 3: Anomaly Detection on the Edge (Mobile Devices):</strong> Anomaly detection on portable devices or mobile devices often requires dealing with high-speed drifting data, low-memory, and in addition ultra-low power. A modern smartphone usually only have a relatively small memory capacity (1,2 Gigabytes) and limited computational power. Battery life is a significant concern, and transmitting data to cloud for analysis has privacy risks as well as are not sustainable due to their energy demands. With the accelerated adoption of 4G (or 5G) technologies including WiMAX and LTE, cellular devices will become the primary means of broadband Internet access for many users. According to the report from Cisco, Global mobile data traffic reached 7.2 exabytes per month at the end of 2016 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>]. Thus, the traffic data that is monitored, or generated, by the mobile devices are extremely high-speed and enormous, and it is hopeless to rely on anomaly detection methods which require consulting a significant fraction of the data. Unfortunately, most unsupervised anomaly detection techniques are near-neighbor based and require querying, and hence the prohibitive storage of the historical data.</p>    <p>     <strong>Challenge 4: Privacy:</strong> As the IoT becomes more widespread, consumers must demand better security and privacy protections that do not leave them vulnerable to corporate surveillance and data breaches. Thus, storing a significant fraction of data for finding anomalous behavior is prohibitive. Privacy-preserving anomaly detection is a challenging problem in itself&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>].</p>    <p>The significance and the impact of the above challenges have put high-speed data mining among the top-10 big-data challenges&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>], which will also be the focus of the present work.</p>    <p>     <strong>Popular Approaches for Unsupervised Anomaly Detection:</strong> There are numerous methods for unsupervised anomaly detection in literature. We review and compare with 11 of these popular methods in our experiments. Unsupervised anomaly detection can be broadly categorized into two categories: 1) Near-Neighbor (NN) Based and 2) Aggregate Statistics (or score) based. NN based approaches typically define the outlier score of a point <em>q</em> based on the difference between <em>q</em>&#x2019;s own behavior and the behavior of <em>q</em>&#x2019;s near-neighbors. The first category is the most common category.</p>    <p>Aggregate statistics based methods, on the other hand, define the outlier score of a point <em>q</em> based on the expected behavior of a global function <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> of the data <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span>, relative to <em>q</em>. A notable method among them is ABOD (Angle-based outlier detection)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. ABOD computes the variance of the angle formed by different pairs of points, in the dataset, incident on the point of interest <em>q</em>. It is expected that the outliers will have a small variance&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>].</p>    <p>There are several implementations of existing anomaly detection algorithms. A notable among them is the <em>ELKI</em> package&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] which is currently one of the most efficient and popular packages for outlier detection because of highly optimized implementations.</p>    <p>Both categories of anomaly detection algorithms require storing the complete dataset to either compute near-neighbor or the desired statistics from the data. The bottleneck computational cost is at least one pass over the data to either calculate the near-neighbor or the statistics. Thus, these methods have poor computational and memory requirements. Furthermore, change in the distribution of data requires storing and processing a larger set of observations.</p>    <p>     <strong>Sampling and Fast Near-Neighbors:</strong> To work around the computational requirements it is natural to resort to fast alternatives&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. There are plenty of techniques which exploits efficient near-neighbor capabilities to speed up NN. However, they still require storing the data in the memory. Even with the computational speedups, the methodologies are still slow for ultra-high speed data mining, as an accurate near-neighbor search over large data is costly.</p>    <p>Relying on random sampling and projections of the data to estimate the aggregate statistics efficiently is not new&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. For example, recently, &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] showed that using smart random sampling and hashing algorithms, we can speed up the anomaly detection and also reduce the memory requirement. Instead of storing all the data points, we only need few random samples and their quantized projections. They proposed FastVOA which uses a modified ABOD statistics that can be estimated in near-constat time and is as good as ABOD for anomaly detection.</p>    <p>However, these approximation methods still require storing a significant number of data samples, which makes the algorithm slow and prohibitive from privacy perspective. FastVOA involves various computation of medians and other costly statistics. Our experiments show that the sampling based FastVOA approach is significantly slower than fast NN based alternatives.</p>    <p>There is a third category of anomaly detection algorithms over a sliding window in data streams&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>]. The notions of anomalies in these algorithms are confined to a given fixed-size window over time. Not surprisingly, if the size of sliding window is increased to take into account large amounts of data, we again observe the same memory and latency issues. The focus of this paper is on unsupervised outlier detection, where the notion of anomaly is with respect to the compete data and not constrained to a limited sliding window.</p>    <p>     <strong>Our Contributions:</strong> We propose a family of statistics which provides a &#x201C;sweet&#x201D; spot between the ability to discriminate anomalies and the resource efficiency. These special statistics, due to their form, can be efficiently computed in ultra-low memory, and they do not require storing even a single data sample. Furthermore, any updates to the data can be incorporated on the fly making our proposal ideal for high-speed data applications. The proposed algorithm, in addition, has strong privacy properties making it ideal for IoT (Internet of Things) setting.</p>    <p>Our proposed family of statistics are derived from the collision probability of locality sensitive hashing (LSH) functions. We show that these classes of statistics have strong discriminative property for identifying outliers and most importantly, it can be accurately estimated using Arrays of Count Estimators (ACE), a novel and tiny LSH based data structure. Designing these estimators requires using the sampling view of LSH rather than the widely popular near-neighbor search view. To the best of our knowledge, this is the first use of LSH counts as unbiased estimators of outlierness.</p>    <p>We demonstrate, empirically and theoretically, that the proposed LSH based count estimators are significantly more accurate than random sampling approaches. Our ACE algorithm only requires computing few locality sensitive hashes of the data and a small set of count array lookups to estimate the proposed statistics sharply. Our approach does not require even a single distance computation. The theory and the class of estimators presented in the paper, could of independent interest in itself.</p>    <p>We demonstrate rigorous experimental evidence on three public outlier detection benchmarks including the largest publicly available benchmark dataset KDD-cup99 HTTP dataset having more than half a million labeled instances. Empirically, our algorithm only requires around 4<em>MB</em> of memory and near-constant amount of computations, for all the three benchmark datasets. Thus, we can exploit fast L3 caches (Level 3 caches), which can be significantly faster than dealing with main memory.</p>    <p>We provide a comparison of our algorithm with 11 different methodologies, which include some of the fastest and most popular anomaly detection algorithms. Our experiment shows that we are around at least 60x faster than of the best performing competitor on the largest benchmark KDD-cup99 HTTP dataset. This disruptive speedup is not surprising given the computational simplicity of our algorithm and ultra-low memory print.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Background: Locality Sensitive Hashing</h2>     </div>    </header>    <p>Locality-Sensitive Hashing (LSH) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] is a popular technique for efficient approximate nearest-neighbor search. LSH is a family of functions, such that a function uniformly sampled from this hash family has the property that, under the hash mapping, similar points have a high probability of having the same hash value. More precisely, consider <span class="inline-equation"><span class="tex">$\mathcal {H}$</span>     </span> a family of hash functions mapping <span class="inline-equation"><span class="tex">$\mathbb {R}^D$</span>     </span> to a discrete set [0, <em>R</em> &#x2212; 1].</p>    <p>     <div class="definition" id="enc1">     <Label>Definition 2.1.</Label>     <p>      <strong>Locality Sensitive Hashing (LSH) Family</strong> A family <span class="inline-equation"><span class="tex">$\mathcal {H}$</span>      </span> is called (<em>S</em><sub>0</sub>, <em>cS</em><sub>0</sub>, <em>u</em><sub>1</sub>, <em>u</em><sub>2</sub>)-sensitive if for any two points <span class="inline-equation"><span class="tex">$x,y \in \mathbb {R}^d$</span>      </span> and <em>h</em> chosen uniformly from <span class="inline-equation"><span class="tex">$\mathcal {H}$</span>      </span> satisfies the following:</p>     <p>      <ul class="list-no-style">       <li id="list1" label="&#x2022;">if <em>Sim</em>(<em>x</em>, <em>y</em>) &#x2265; <em>S</em><sub>0</sub> then <span class="inline-equation"><span class="tex">${Pr}_\mathcal {H}(h(x) = h(y)) \ge u_1$</span>        </span>        <br/></li>       <li id="list2" label="&#x2022;">if <em>Sim</em>(<em>x</em>, <em>y</em>) &#x2264; <em>cS</em><sub>0</sub> then <span class="inline-equation"><span class="tex">${Pr}_\mathcal {H}(h(x) = h(y)) \le u_2$</span>        </span>        <br/></li>      </ul>     </p>     </div>    </p>    <p>A collision occurs when the hash values for two data vectors are equal, meaning that <em>h</em>(<em>x</em>) = <em>h</em>(<em>y</em>).</p>    <p>LSH is a very well studied topic in computer science theory and database literature. There are a number of well-known LSH families in the literature. Please refer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] for details. The most popular one is Signed Random Projections&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>].</p>    <p>Signed Random Projections(SRP) is an LSH for the cosine similarity measure, which originates from the concept of <strong>randomized rounding&#x00A0;(SRP)</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>]. Given a vector <em>x</em>, SRP utilizes a random <em>w</em> vector with each component generated from i.i.d. normal, <em>i.e.</em>, <em>w<sub>i</sub>     </em> &#x223C; <em>N</em>(0, 1), and only stores the sign of the projection. Formally SRP family is given by <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} h_w(x) = sign(w^Tx). \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>     </div> It was shown in the seminal work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] that collision under SRP satisfies the following equation: <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} Pr_w(h_w(x) = h_w(y)) = 1 - \frac{\theta}{\pi} \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>     </div> where $\theta = cos^{-1}\left( \frac{x^Ty}{||x||_2 ||y||_2}\right)$. $\frac{x^Ty}{||x||_2 ||y||_2}$, is the cosine similarity. If we generate <em>K</em> independent SRP bits, by sampling <em>w</em> independently <em>k</em> times, and use the generated <em>K</em>-bit number as the new hash function <em>H</em>, then the new collision probability is <div class="table-responsive" id="eq2">     <div class="display-equation">      <span class="tex mytex">\begin{align} Pr(H(x) = H(y))= (1 - \frac{\theta }{\pi })^K \end{align} </span>      <br/>      <span class="equation-number">(3)</span>     </div>     </div> by the simple multiplicative law of probability. We will be using this observation heavily in our work.</p>    <p>Over the last decade, there has been a significant advancement in reducing the amortized computational and memory requirements for computing several LSH signatures of the data vector. For random projections based LSH, of which signed random projection is a special case, we can calculate <em>m</em> LSH hashes of the data vector, with dimensions <em>d</em>, in time <em>O</em>(<em>d</em> log&#x2009;<em>d</em> + <em>m</em>), a significant improvement over <em>O</em>(<em>dm</em>). This speedup is possible due to the theory of Fast-Johnson-Lindenstrauss transformation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. On the orthogonal side, even better speedup of <em>O</em>(<em>d</em> + <em>m</em>) has been obtained with permutation-based LSH, such as minwise hashing, using ideas of densification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. These drastic reductions in hashing time have been instrumental in making LSH based algorithms more appealing and practical.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Our Proposal</h2>     </div>    </header>    <p>Denote the dataset with <span class="inline-equation"><span class="tex">$\mathcal {D}=\lbrace x_{i} | i \in [1,n]\rbrace$</span>     </span>, where <em>n</em> is the number of data points in <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span>. By definition, outliers are significantly separated from an average data point. Therefore, any reasonable statistics of <em>x<sub>i</sub>     </em> with respect to all other <span class="inline-equation"><span class="tex">$x_j \in \mathcal {D}$</span>     </span> will deviate significantly for outliers compared to a normal data point. Even an average distance of <em>x<sub>i</sub>     </em> with all other elements of <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span> is a reasonably good statistics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. However, as noted before, computing these statistics requires storing the complete data <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span>. In general, calculating every single <span class="inline-equation"><span class="tex">$S(x_i,\mathcal {D})$</span>     </span> requires one complete pass over the dataset <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span>. Besides, our experiments show that alternative estimations based on random sampling and random projections still lead to significant computational overheads.</p>    <p>We instead focus on classes of scoring functions <em>S</em>(., .) over the dataset that can be estimated efficiently using a tiny (memory efficient) data structure that can easily fit fast processor cache. Furthermore, we also want to update the data structure on the fly. In particular, any change in data from <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span> to <span class="inline-equation"><span class="tex">$\mathcal {D}^{\prime }$</span>     </span> requires no change, and the estimates get dynamically adjusted.</p>    <p>We show that a class of scoring functions of the following form have the aforementioned property: <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} S(q,\mathcal {D}) = \sum _{x_i \in \mathcal {D}} p(q,x_i)^K, \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>     </div> where <em>p</em> is the collision probability of any LSH family and <em>K</em> &#x2265; 1 is an integer.</p>    <p>The analysis of this paper extends naturally to any LSH scheme. For this work, we will focus on the popular signed random projections (SRP) as the LSH because of its simplicity. Furthermore, advances in fast SRP have lead to some very lightweight hashing variants. With SRP, the collision probability <em>p</em>(<em>q</em>, <em>x<sub>i</sub>     </em>) is given by: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ p(q,x_i) = 1 - \frac{1}{\pi }\cos ^{-1} (\frac{q^T x_i}{\left\Vert q \right\Vert \left\Vert x_i \right\Vert }) \] </span>      <br/>     </div>     </div> which will also be the value of <em>p</em>(<em>q</em>, <em>x<sub>i</sub>     </em>) for the rest of the paper.</p>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Can it Discriminate Outliers?</h3>     </div>     </header>     <p>To demonstrate the discriminative power of the scoring function in Equation&#x00A0;<a class="eqn" href="#eq3">4</a>, we do a simulation experiment similar to the one performed in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>]. We first generate a simple dataset with an outlier point. Figure <a class="fig" href="#fig1">1</a> (left) shows the snapshot of the data. There are two sets of data points. The outlier and the general data points. For the general data points, in addition, we make a distinction between the border points and inner points as illustrated in the figure.</p>     <p>In Figure <a class="fig" href="#fig1">1</a> (right), we plot the value of our proposed statistics <span class="inline-equation"><span class="tex">$\frac{1}{n}S(q,\mathcal {D})$</span>     </span>, given by Equation&#x00A0;<a class="eqn" href="#eq3">4</a>, for different sets of data points as a function of <em>K</em>. We can see from the figure the value of <span class="inline-equation"><span class="tex">$\frac{1}{n}S(q,\mathcal {D})$</span>     </span> for an outlier point is near zero. In particular, it is significantly lower compared to the values of the same statistics for inner points and even border points. This behavior is expected. Note that our statistics is a sum of collision probabilities of the LSH mapping over all the data points <span class="inline-equation"><span class="tex">$x_i \in \mathcal {D}$</span>     </span>. From the theory of LSH, the collision probability <em>p</em>(<em>q</em>, <em>x<sub>i</sub>     </em>) indicates the level of similarity between <em>q</em> and <em>x<sub>i</sub>     </em>. If <em>q</em> is an outlier, <em>p</em>(<em>q</em>, <em>x<sub>i</sub>     </em>) is expected to be significantly low. We will further demonstrate the usefulness of these statistics in the experiments section. <figure id="fig1">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186056/images/www2018-65-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 1:</span>       <span class="figure-title">        <strong>Discriminative power of </strong>        <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>        </span>        <strong>:</strong> We can see from the figure that the value of <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>        </span> for an Outlier is significantly lower (different) compared to that of non-outliers.</span>      </div>     </figure>     </p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> ACE (Arrays of (locality-sensitive) Counts Estimator) Algorithm</h3>     </div>     </header>     <p>For the ease of explanation, we first describe the procedure of our proposed ACE algorithm. We later show that this procedure is an efficient statistical estimator of our proposed outlier score <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> defined by Equation&#x00A0;<a class="eqn" href="#eq3">4</a>.</p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186056/images/www2018-65-img1.svg" class="img-responsive" alt="" longdesc=""/>     </p>     <p>The overall process of ACE is summarized in Algorithm&#x00A0;1 . Our ACE algorithm, uses <em>K</em> &#x00D7; <em>L</em> independent SRP hash functions <em>h<sub>i</sub>     </em>, each given by Equation&#x00A0;<a class="eqn" href="#eq1">1</a>. <em>K</em> and <em>L</em> are hyperparameters that are pre-specified. Note, this is analogous to the traditional (<em>K</em>, <em>L</em>) parameterized LSH algorithm for near-neighbor search. However, we do not perform any retrieval which requires heavy hash tables with buckets of candidates for each hash index. For near-neighbor, we further need to compute the distances of these candidates to identify the best.</p>     <p>On the contrary, our algorithm does not require a single distance computation. Our method only needs to check the value of a simple counter at each index. We only need arrays of counters. The process is significantly efficient, both in memory and speed, compared to a single LSH near-neighbor query.</p>     <p>We use Signed Random Projections(SRP) <em>h<sup>sim</sup>     </em> (Equations&#x00A0;<a class="eqn" href="#eq1">1</a>) which gives one-bit output. Using these 1-bit outputs, we then generate <em>L</em> different meta-hash functions given by</p>     <p>     <em>H<sub>j</sub>     </em>(<em>x</em>) = [<em>h</em>     <sub>      <em>j</em>1</sub>(<em>x</em>); <em>h</em>     <sub>      <em>j</em>2</sub>(<em>x</em>); ...; <em>h<sub>jK</sub>     </em>(<em>x</em>)] of <em>K</em> bits each. The <em>K</em> bits are generated by concatenating the individual bits. Here <em>h<sub>ij</sub>     </em>, <em>i</em> &#x2208; {1, 2, ..., <em>K</em>} and <em>j</em> &#x2208; {1, 2, ..., <em>K</em>}, are <em>K</em> &#x00D7; <em>L</em> independent evaluations of the SRP.</p>     <p>The overall algorithm works in the following two phases:</p>     <p>1) <strong>Counting Phase:</strong> We construct <em>L</em> short arrays, <em>A<sub>j</sub>     </em>, <em>j</em> = {1,&#x2009;2, ..,&#x2009;<em>L</em>}, of size 2<sup>      <em>K</em>     </sup> each initialized with zeros. Given any observed element <span class="inline-equation"><span class="tex">$x \in \mathcal {D}$</span>     </span>, we increment the count of the corresponding counter <em>H<sub>j</sub>     </em>(<em>x</em>) in array <em>A<sub>j</sub>     </em>, for all <em>j</em>s. Thus, every counter keeps the total count of the number of hits to that particular index (See Figure&#x00A0;<a class="fig" href="#fig2">2</a>). The total cost of updating the data structure for any given <em>x</em> is <em>KL</em> SRP computations followed by <em>L</em> increments.</p>     <p>     <strong>Mean Update on Fly:</strong> For each <span class="inline-equation"><span class="tex">$x_i \in \mathcal {D}$</span>     </span> our estimated score is <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \widehat{S(x_i,\mathcal {D})} = \frac{1}{L}\sum _{j=1}^L A_j[H_j(x_i)]. \] </span>       <br/>      </div>     </div> We compute the mean behavior <em>&#x03BC;</em> of the scores over all the element in dataset <span class="inline-equation"><span class="tex">$x \in \mathcal {D}$</span>     </span>. <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \mu = \frac{1}{n}\sum _{i=1}^{n}\widehat{S(x_i,\mathcal {D})}. \] </span>       <br/>      </div>     </div> Deviation from this mean will indicate outlierness. It turns out that we can dynamically update the mean <em>&#x03BC;</em> on fly, as we read (or observe) the new data as shown in the algorithm. See Section&#x00A0;<a class="sec" href="#sec-10">3.4.1</a> for details.</p>     <p>2) <strong>Real-time (query) Phase:</strong> Given a query <em>q</em>, for which we want to compute the score, we report the average of all the counters <em>A<sub>j</sub>     </em>[<em>H<sub>j</sub>     </em>(<em>q</em>)] &#x2200;<em>j</em> &#x2208; {1, 2, ..., <em>L</em>}, <em>i.e.</em>, <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})} = \frac{1}{L}\sum _{j=1}^L A_j[H_j(q)]$</span>     </span>. We report <em>q</em> as an anomaly if the estimated score <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})}$</span>     </span> is less than <em>&#x03BC;</em> &#x2212; <em>&#x03B1;</em>, where <em>&#x03B1;</em> is some preselected hyperparameter. The overall cost for querying is <em>KL</em> SRP computations and <em>L</em> lookups followed by a simple average calculation. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186056/images/www2018-65-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">We use the LSH hash of the data points to increment corresponding counters into different (independent) hash arrays. We do not save anything, we only increase the value by 1 for each bucket and then forget the data.</span>      </div>     </figure>     </p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Theory: Analysis and Superiority over Random Sampling</h3>     </div>     </header>     <p>We first define few notations needed for analysis. Given a query point <em>q</em>. For convenience, we will denote <em>p</em>(<em>q</em>, <em>x<sub>i</sub>     </em>), the collision probability of the SRP of <em>q</em> with that of <span class="inline-equation"><span class="tex">$x_i \in \mathcal {D}$</span>     </span>, by <em>p<sub>i</sub>     </em>. Due to space limitations the proofs are omitted.</p>     <p>     <strong>Intuition: LSH as Samplers</strong> LSH is widely accepted as a black box algorithm for near neighbor search. We take an alternative adaptive sampling view of LSH which has emerged very recently&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>]. As argued in Section&#x00A0;<a class="sec" href="#sec-4">2</a>, for a given query <em>q</em> and <em>K</em>-bit SRP hash function <em>H<sub>j</sub>     </em>, the probability that any element <em>x<sub>i</sub>     </em> increments the count of location <em>H<sub>j</sub>     </em>(<em>q</em>) (the location of query) in array <em>A<sub>j</sub>     </em> is precisely <em>p</em>(<em>q</em>, <em>x<sub>i</sub>     </em>)<sup>      <em>K</em>     </sup>. Using this observation, we will show that the count of the number of elements, from <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span>, hitting the bucket of query <em>H<sub>j</sub>     </em>(<em>q</em>) is an unbiased estimator of the <span class="inline-equation"><span class="tex">$S(q,\mathcal {C})= \sum _{i=1}^n p(q,x_i)^K$</span>     </span>. This novel use of LSH as efficient data structure for statistical estimation could be of independent interest in itself.</p>     <p>We define indicator variable <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_i \in B_q}$</span>     </span> as <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb {I}_{x_i \in B_q} = {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}1, &#x0026; \mbox{if $x_i$ is in the bucket of $q$} \\ 0, &#x0026; \mbox{otherwise}. \end{array}\right.} \end{align} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> Here, <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_i \in B_q}$</span>     </span> is an indicator for the event that data element <em>x<sub>i</sub>     </em> and the query <em>q</em> are in the same bucket. It should be noted that <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{align} Pr(\mathbb {I}_{x_i \in B_q} =1) &#x0026;= p(q,x_i)^K = p_i^K \end{align} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div>     </p>     <p>Note that, <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_i \in B_q}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_j \in B_q}$</span>     </span> are correlated. If <em>x<sub>i</sub>     </em> and <em>x<sub>j</sub>     </em> are &#x201C;similar&#x201D; then <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_i \in B_q}=1$</span>     </span> is likely to imply <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_j \in B_q}=1$</span>     </span>. In other words, high similarity indicates positive correlation. Due to correlations, we may have both the cases: <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb {E}[\mathbb {I}_{x_i \in B_q}\mathbb {I}_{x_j \in B_q}] &#x0026;{\left\lbrace \begin{array}{@{}l@{\quad }l@{}}\ge p_i^Kp_j^K, &#x0026; \mbox{(positive correlations)} \\\le p_i^Kp_j^K, &#x0026; \mbox{(negative correlation)}. \end{array}\right.} \end{align} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> Here, <span class="inline-equation"><span class="tex">$\mathbb {E}$</span>     </span> is the expectation.</p>     <p>Using the above notations we can show that, for a given query <em>q</em>, <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})}$</span>     </span>, computed in Algorithm&#x00A0;1, is an unbiased estimator of <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> with variance given by:</p>     <div class="theorem" id="enc2">     <Label>Theorem 3.1.</Label>     <p>      <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} \nonumber \mathbb {E}[\widehat{S(q,\mathcal {D}}] &#x0026;= \sum _{x_i \in \mathcal {D}} p_i^K = {S(q,\mathcal {D})} \\\nonumber Var(\widehat{S(q,\mathcal {D})}) &#x0026;= \frac{1}{L}\bigg (\sum _{i=1}^n p_i^K(1 - p_i^K) \\\nonumber &#x0026;+ \sum _{i \ne j} \big [\mathbb {E}[\mathbb {I}_{x_i \in B_q}\mathbb {I}_{x_j \in B_q}] - p_i^K p_j^K\big ]\bigg)\end{align*} </span>        <br/>       </div>      </div>     </p>     </div>     <p>The variance of <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})}$</span>     </span> is dependent on the data distribution. There are two terms in the variance <span class="inline-equation"><span class="tex">$\frac{1}{L}\bigg (\sum _{i=1}^n p_i^K(1 - p_i^K)\bigg)$</span>     </span> and <span class="inline-equation"><span class="tex">$\frac{1}{L}\sum _{i \ne j}\big [\mathbb {E}[\mathbb {I}_{x_i \in B_q}\mathbb {I}_{x_j \in B_q}] - p_i^K p_j^K\big ]$</span>     </span>. The terms inside summation is precisely the covariance between <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_i \in B_q}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {I}_{x_j \in B_q}$</span>     </span>      <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb {E}[\mathbb {I}_{x_i \in B_q}\mathbb {I}_{x_j \in B_q}] - p_i^K p_j^K &#x0026;= \mathbb {E}[\mathbb {I}_{x_i \in B_q}\mathbb {I}_{x_j \in B_q}]\end{align} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>     <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026;- \mathbb {E}[\mathbb {I}_{x_i \in B_q}]\mathbb {E}[\mathbb {I}_{x_j \in B_q}] \end{align} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>     <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{align} &#x0026;= Cov(\mathbb {I}_{x_i \in B_q},\mathbb {I}_{x_j \in B_q}) \end{align} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div>     </p>     <p>There are <em>n</em>(<em>n</em> &#x2212; 1) covariance terms in the second term of variance, <span class="inline-equation"><span class="tex">$\frac{1}{L}\sum _{i \ne j}\big [\mathbb {E}[\mathbb {I}_{x_i \in B_q}\mathbb {I}_{x_j \in B_q}] - p_i^K p_j^K\big ]$</span>     </span>. To see why almost all of them will be negative, let <em>m</em> be the number of elements in the buckets of the query. So only pairs <em>x<sub>i</sub>     </em> and <em>x<sub>j</sub>     </em> in the bucket (<em>O</em>(<em>m</em>     <sup>2</sup>) pairs) of query <em>H<sub>j</sub>     </em>(<em>q</em>) will contribute <span class="inline-equation"><span class="tex">$1 - p_i^K p_j^K \ge 0$</span>     </span> to the summation (product of indicators is 1 &#x21D4; both are 1). Rest all pairs (<em>O</em>((<em>n</em> &#x2212; <em>m</em>)<sup>2</sup>)) will contribute negative terms <span class="inline-equation"><span class="tex">$-p_i^Kp_j^K$</span>     </span>. Thus, if we choose <em>K</em> large enough then the expected number of elements in the bucket <em>m</em> is quite small. Hence, we can expect the variance to be significantly smaller than <span class="inline-equation"><span class="tex">$\bigg (\sum _{i=1}^n p_i^K\frac{(1 - p_i^K)}{L}\bigg)$</span>     </span>. We observe in our experiments that <em>K</em> = 15 is a good recommended constant value.</p>     <p>As noted the variance is dependent on the data distribution. If we have all exact duplicates, then all the covariances are positive. However, for real datasets, for any randomly chosen pair <em>x<sub>i</sub>     </em>,&#x2009;<em>x<sub>j</sub>     </em>, the covariance <span class="inline-equation"><span class="tex">$Cov(\mathbb {I}_{x_i \in B_q},\mathbb {I}_{x_j \in B_q})$</span>     </span> will be almost always be negative.</p>     <p>An alternative way of estimating <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> is to use the random sampling. The idea is to uniformly sample a subset <span class="inline-equation"><span class="tex">$\mathcal {S} \subseteq \mathcal {D}$</span>     </span> of size <em>L</em> and report the random sampling estimator <span class="inline-equation"><span class="tex">$RSE(q,\mathcal {D})$</span>     </span> as: <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{align} RSE(q,\mathcal {D}) = \frac{n}{L} [\sum _{x_i \in \mathcal {S}}p_i^K] \end{align} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div> From the theory of random sampling this estimator is also unbiased and has the following variance:</p>     <div class="theorem" id="enc3">     <Label>Theorem 3.2.</Label>     <p>      <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\begin{align*} \nonumber \mathbb {E}[RSE(q,\mathcal {D})] &#x0026;= \sum _{x_i \in \mathcal {D}} p_i^K = {S(q,\mathcal {D})} \\\nonumber Var(RSE(q,\mathcal {D})) &#x0026;= \sum _{i=1}^n p_i^{K}\bigg (\bigg [\frac{n}{L}-1\bigg ]p_i^K\bigg)\end{align*} </span>        <br/>       </div>      </div>     </p>     </div>     <p>Both <span class="inline-equation"><span class="tex">$RSE(q,\mathcal {D})$</span>     </span> and <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})}$</span>     </span> are unbiased. For the same number of samples, the estimator with smaller variance is superior.</p>     <p>We can get some insights from the leading terms <span class="inline-equation"><span class="tex">$\sum _{i=1}^n p_i^{K}\bigg (\bigg [\frac{n}{L}-1\bigg ]p_i^K\bigg)$</span>     </span> and <span class="inline-equation"><span class="tex">$\frac{1}{L}\bigg (\sum _{i=1}^n p_i^K(1 - p_i^K)\bigg)$</span>     </span>. Generally, for any <em>i</em> and large enough <em>n</em>, we always have <span class="inline-equation"><span class="tex">$\bigg [\frac{n}{L}-1\bigg ] \ge \frac{1}{L}(\frac{1-p_i^K}{p_i^K})$</span>     </span>. Thus, for large enough <em>n</em>, <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ Var(RSE(q,\mathcal {D})) {\gt} \frac{1}{L}\bigg (\sum _{i=1}^n p_i^K(1 - p_i^K)\bigg) \] </span>       <br/>      </div>     </div> As argued before for real data we can expect <span class="inline-equation"><span class="tex">$\frac{1}{L}\bigg (\sum _{i=1}^n p_i^K(1 - p_i^K)\bigg) {\gt} Var(\widehat{S(q,\mathcal {D})})$</span>     </span>. Precise mathematical comparison between the variances of these two estimators is fairly challenging due to data-dependent correlation. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186056/images/www2018-65-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Comparison of ACE estimator with random sampling estimator on three datasets. The x-axis denotes the number to arrays and size of samples for ACE estimator and random sampling estimator respectively. ACE estimator is not only more accurate but also cheaper compared to random sampling estimators from the computational perspective.</span>      </div>     </figure>     </p>     <p>     <strong>Empirical Comparison:</strong> As argued, we expect that for real datasets the ACE estimator to be more accurate (less variance) compared to the random sampling estimator. To validate our arguments empirically, we compare these estimators on the three benchmark anomaly detection datasets. These are the same datasets used in the experiment sections (see section&#x00A0;<a class="sec" href="#sec-13">5.1</a> for details). For all the three datasets, we randomly chose 50 queries and estimate their <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> using the two competing estimators. We use <em>K</em> = 15 which is the fixed value used in all our experiments.</p>     <p>We plot the mean square error of the estimates, computed using the actual and the estimated values on three real anomaly detection datasets, in Figure&#x00A0;<a class="fig" href="#fig3">3</a>. We vary the number of samples for random sampling estimator <span class="inline-equation"><span class="tex">$RSE(q,\mathcal {D})$</span>     </span> and the number of arrays for <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})}$</span>     </span>. From the plots, it is clear that on all the three real datasets, as expected from our analysis, our ACE estimator <span class="inline-equation"><span class="tex">$\widehat{S(q,\mathcal {D})}$</span>     </span> consistently outperforms the random sampling estimator <span class="inline-equation"><span class="tex">$RSE(q,\mathcal {D})$</span>     </span> at the same level of <em>L</em>. Note, these estimators are unbiased and hence mean square error value is also the theoretical variance. These experiments indicate that the variance of our ACE estimator is superior for estimating <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> over random sampling.</p>     <p>In addition to providing sharper estimates, in the next section, we show that our ACE algorithm only needs <em>O</em>(<em>d</em> log&#x2009;<em>d</em> + <em>KL</em>) computations to calculate the score. Here, <em>d</em> is the dimensions of the dataset. For the same number of samples <em>L</em>, random sampling estimator requires <em>O</em>(<em>Ld</em>) computations. Given that <em>K</em> = 15 is a fixed constant. For high dimensional datasets, we will have <em>d</em> > <em>K</em>. Thus, our estimator is not only more accurate but also cheaper compared to random sampling estimators from the computational perspective.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Implementation Details, Running Time, Cache Utilization and Memory</h3>     </div>     </header>     <p>     <strong>Running Time:</strong> From Algorithm&#x00A0;1, it is not difficult to see that for a query <em>q</em>, we need to compute <em>KL</em> hashes of the data followed by a simple addition of size <em>L</em>. The costliest step is the computations of <em>KL</em> hashes, which for <em>d</em> dimensional data can be accomplished in <em>O</em>(<em>d</em> log&#x2009;<em>d</em> + <em>KL</em>) computations using advances in fast random projections (Section&#x00A0;<a class="sec" href="#sec-4">2</a>). If instead, we are using minwise hashing as the LSH then it can be done in mere <em>O</em>(<em>d</em> + <em>KL</em>) using fast minwise hashes. However, minwise hashing is limited to binary datasets only.</p>     <p>Note that computing the original score <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> via naive calculation requires <em>O</em>(<em>nd</em>) computations. It further require to store all the data for outlier detection, which for large and high dimensional datasets can be prohibitive.</p>     <p>In all our experiments, we use <em>K</em> = 15 and <em>L</em> = 50 for all the three datasets (see Section&#x00A0;<a class="sec" href="#sec-16">6</a>). Thus, with these small constant values, our scoring time negligible compared to the other algorithms which requires one pass over the full dataset <span class="inline-equation"><span class="tex">$\mathcal {D}$</span>     </span>. In the experiments, we see that even with such minuscule computation, our method provides competitive accuracy while being orders of magnitude faster than 11 state-of-the-art methods.</p>     <p>     <strong>Memory:</strong> Since we have 2<sup><em>K</em>     </sup> counters, it is unlikely that the counters will get too many hits. To save memory by a factor of two, we can use short integers (16 bits) instead of integer counters. The total amount of memory required by <em>L</em> counter arrays is 2<sup><em>K</em>     </sup> bytes each if we use short counters. The total space needed for the arrays is <em>L</em> &#x00D7; 2<sup><em>K</em>     </sup> &#x00D7; 2 bytes. For <em>K</em> = 15 and <em>L</em> = 50, the total space required by the ACE algorithm is around 3.2<em>MB</em>. In addition, we need to compute <em>KL</em> = 750 hashes, which requires storing 750 random seeds (integers) from which we can generate hashes on the fly. 750 integers require negligible space compared to 3.2<em>MB</em>. In the worst case, even if we decide to store the full random projections, we only need 750 &#x00D7; <em>d</em> &#x00D7; 8 bytes (approx 6<em>d</em> kilobytes).</p>     <p>     <strong>L3 Cache Utilizations:</strong> For all of our experiments, the total memory requirement of the ACE algorithm is &#x2264; 4<em>MB</em> for all the datasets. Our query data structure, the arrays, can easily fit into L3 cache of any modern processor, where the memory access can be anywhere from 2-10x faster than the main memory (DRAM) access. Detecting anomaly requires scoring which only needs reading count from the arrays. Due to all these unique favorable properties, our algorithm is orders of magnitude faster than the fastest available packages for unsupervised anomaly detection.</p>     <section id="sec-10">     <p><em>3.4.1 Dynamic Updates.</em> One of the appealing features of the ACE algorithm is that data can be dynamically updated. It is straightforward to increment the counters if we decide to add any data <em>x</em>. However, we will lose all the data information. We only store a set of count arrays, so it is not clear how we update the global mean <em>&#x03BC;</em> of counts. Updating <em>&#x03BC;</em> is an important part of Algorithm&#x00A0;1 . Note that the updated mean, <em>&#x03BC;</em>&#x2032;, should be the average of all the estimated score of all the data in <span class="inline-equation"><span class="tex">$\mathcal {D}^{\prime }= \mathcal {D} + x$</span>      </span>.</p>     <p>It turns out that we can exactly compute the new value of <em>&#x03BC;</em>&#x2032; from the existing count arrays. To simplify, let us convert old mean <em>&#x03BC;</em> to sum by multiplying it by the size of dataset <span class="inline-equation"><span class="tex">$n = |\mathcal {D}|$</span>      </span>. It is easy to keep track of the sum <div class="table-responsive">       <div class="display-equation">        <span class="tex mytex">\[ n\mu = \sum _{x_i \in \mathcal {D}} \frac{1}{L}\sum _{j=1}^{L} A_j[H_j(x_i)]. \] </span>        <br/>       </div>      </div> Observe that, if the new <em>x</em> goes to location <em>H<sub>j</sub>      </em>(<em>x</em>) in array <em>A<sub>j</sub>      </em> for any <em>j</em>. The count of location <em>H<sub>j</sub>      </em> will be increment by 1. This will also lead to an increment in the scores of all the elements which maps to <em>H<sub>j</sub>      </em>(<em>x</em>) in <em>j<sup>th</sup>      </em> array by exactly <span class="inline-equation"><span class="tex">$\frac{1}{L}$</span>      </span>. Since we already know the count value of <em>A<sub>j</sub>      </em>[<em>H<sub>j</sub>      </em>(<em>x<sub>i</sub>      </em>)], the total increment to the sum would be <span class="inline-equation"><span class="tex">$\frac{A_j[H_j(x_i)]}{L}$</span>      </span>. In addition, the new data <em>x</em> will add an extra <span class="inline-equation"><span class="tex">$\frac{A_j[H_j(x_i)]+1}{L}$</span>      </span> for its own count. Thus, we can precisely compute the increment in the sum. The new mean <em>&#x03BC;</em>&#x2032;, for an addition of data <em>x</em>, can be computed as <div class="table-responsive" id="eq11">       <div class="display-equation">        <span class="tex mytex">\begin{align} \mu ^{\prime } = \frac{1}{n+1}\bigg (n\mu + \sum _{j=1}^L \frac{2A_j[H_j(x)] + 1}{L}\bigg). \end{align} </span>        <br/>        <span class="equation-number">(12)</span>       </div>      </div>     </p>     </section>    </section>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Discussions: Privacy Preserving Anomaly Detection</h2>     </div>    </header>    <p>Privacy is becoming one of the sought after directions in data mining and machine learning. Privacy preserving anomaly detection is of broad interest in the big-data and IoT (Internet of Things) community&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>]. In many setting, we do want to detect anomalies in the data. However, it also desirable that the attribute information remains private and secure. It turns out that our proposed ACE algorithm has ideal properties for privacy preserving anomaly detection.</p>    <p>ACE does not require storing any data attributes, and the complete algorithm works only over aggregated counts generated from hashed data. If the hashes are not invertible, then the algorithm is safe. We can exploit advances in the secure computation to design protocols which hide the hashing mechanism&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>].</p>    <p>Obtaining differential privacy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] with ACE is quite appealing and neat. Since ACE algorithm relies on random projections to compute hashes, instead of original data, we can make ACE algorithm differentially private by adding only Gaussian noise instead of heavy-tailed Laplacian noise. &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] shows a way to release user information in a privacy-preserving way for near-neighbor search. The paper showed that adding Gaussian noise <em>N</em>(0, <em>&#x03C3;</em> <sup>2</sup>) after the random projection preserves differential privacy. Any function of differentially private object it also differentially private. Thus, to compute a private variant of SRP (Signed random projection), we used the sign of the differentially private random projections (generated by adding Gaussian noise to usual projection) as suggested in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>].</p>    <p>The final algorithm is very simple. The data is never revealed to anyone. At the source itself, the sign of differentially private random projections of data is used instead of usual SRP. All other process remains the same. Now since, we are only perturbing our algorithm with Gaussian noise, instead of Laplacian, we can expect a minimal loss in utility (or change in output).</p>    <p>Note, that privacy is significantly harder with other state-of-the-art anomaly detection algorithms that store the actual data or even samples. Making such algorithms private requires perturbing the system with heavy-tailed Laplacian noise, which can significantly hurt the outcome of the algorithm.</p>   </section>   <section id="sec-12">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Experimental Evaluations</h2>     </div>    </header>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Datasets</h3>     </div>     </header>     <p>We choose three real-world benchmark datasets for anomaly detection: 1) <strong>Statlog Shuttle</strong>, 2) <strong>Object Images (ALOI)</strong>, and 3) <strong>KDD-Cup99 HTTP</strong>. These datasets are labeled and hence can be used for quantifying the effectiveness of anomaly detection measure. These three datasets also cover a broad spectrum of applications of unsupervised anomaly detection.</p>     <p>The first dataset we use is the shuttle dataset <a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. This dataset describes radiator positions in a NASA space shuttle with 9 attributes. It was designed for supervised anomaly detection. In the original datasets, about 20% of the data regarded as anomaly. The entire dataset contains 34,987 instances with 879 anomalies.</p>     <p>The second dataset is Object Images (ALOI) datasets<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>. The aloi dataset is derived from the &#x201C;Amsterdam Library of Object Images&#x201D; collection [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. It contains about 110 images of 1000 small objects taken under different light conditions and viewing angles. From the original images, a 27 dimensional feature vector was extracted using HSB color histograms [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>]. Some objects were chosen as anomalies, and the data was down-sampled such that the resulting dataset contains 50,000 instances including 1508 anomalies.</p>     <p>The third dataset is KDD-Cup99 HTTP. <a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. KDD-Cup99 HTTP dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>] is the largest benchmark for unsupervised anomaly detection evaluation. It contains simulated normal and attack traffic on an IP level in a computer network environment in order to test intrusion detection systems. There are total of 36 dimensions. The dataset contains 596,853 instances with 1055 labeled anomalies.</p>     <p>The statistics of these datasets are shown in Table. <a class="tbl" href="#tab1">1</a>.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">The statistics of the three datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>Dataset</strong>        </th>        <th style="text-align:center;">        <strong>Instances</strong>        </th>        <th style="text-align:center;">        <strong>Outliers</strong>        </th>        <th>        <strong>Dimension</strong>        </th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:center;">Statlog Shuttle</td>        <td style="text-align:center;">34,987</td>        <td style="text-align:center;">879</td>        <td>9</td>       </tr>       <tr>        <td style="text-align:center;">Object Images (ALOI)</td>        <td style="text-align:center;">50,000</td>        <td style="text-align:center;">1508</td>        <td>27</td>       </tr>       <tr>        <td style="text-align:center;">KDD-Cup99</td>        <td style="text-align:center;">596,853</td>        <td style="text-align:center;">1055</td>        <td>36</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Baselines</h3>     </div>     </header>     <p>We use 11 different state-of-the-art methodologies to compare with ACE. These methodologies cover the whole spectrum of unsupervised anomaly detection techniques with all sorts of variations developed over the years. Our baselines cover very recent scoring mechanisms based on simple to sophisticated strategies which include near-neighbor, kernel density estimation, graph connectedness, <em>etc</em>. The competing methodologies are: <strong>ACE</strong> (Proposed), <strong>LOF</strong>&#x00A0;(Local Outlier Factor)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], <strong>FastVOA</strong>&#x00A0;(Fast Variance of Angles)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], <strong>kNN</strong>&#x00A0;(KNNOutlier)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], <strong>KNNW</strong>&#x00A0;(KNNWeightOutlier) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>], <strong>LoOP</strong>&#x00A0;(Local Outlier probability) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>], <strong>LDOF</strong>&#x00A0;(Local Distance based Outlier Factor) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0047">47</a>], <strong>ODIN</strong>&#x00A0;(Outlier Detection using Indegree Number) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], <strong>LDF</strong>&#x00A0;(Local density factor) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>], <strong>KDEOS</strong>&#x00A0;(Kernel Density Estimation Outlier Score)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>], <strong>COF</strong>&#x00A0;(Connectivity-based Outlier Factor) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0043">43</a>] and <strong>INFLO</strong>&#x00A0;(Influenced Outlierness) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>].</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Comparison Algorithms and Their Parameter Values Recommended for These Benchmark Datasets.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">        <strong>Method</strong>        </th>        <th style="text-align:center;">        <strong>Shuttle</strong>        </th>        <th style="text-align:center;">        <strong>Image Object</strong>        </th>        <th>        <strong>KDD-CUP 99</strong>        </th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:center;">ACE</td>        <td style="text-align:center;">        <em>K</em> = 15, <em>L</em> = 50</td>        <td style="text-align:center;">        <em>K</em> = 15, <em>L</em> = 50</td>        <td>        <em>K</em> = 15, <em>L</em> = 50</td>       </tr>       <tr>        <td style="text-align:center;">LOF</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">kNN</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">kNNW</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">LoOP</td>        <td style="text-align:center;">        <em>k<sub>r</sub>        </em> = <em>k<sub>c</sub>        </em> = 5</td>        <td style="text-align:center;">        <em>k<sub>r</sub>        </em> = <em>k<sub>c</sub>        </em> = 5</td>        <td>        <em>k<sub>r</sub>        </em> = <em>k<sub>c</sub>        </em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">&#x00A0;</td>        <td style="text-align:center;">        <em>&#x03BB;</em> = 0.2</td>        <td style="text-align:center;">        <em>&#x03BB;</em> = 0.2</td>        <td>        <em>&#x03BB;</em> = 0.2</td>       </tr>       <tr>        <td style="text-align:center;">LDOF</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">ODIN</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">KDEOS</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">&#x00A0;</td>        <td style="text-align:center;">        <em>B</em> = 5, <em>s</em> = 0.2</td>        <td style="text-align:center;">        <em>B</em> = 5, <em>s</em> = 0.2</td>        <td>        <em>B</em> = 5, <em>s</em> = 0.2</td>       </tr>       <tr>        <td style="text-align:center;">&#x00A0;</td>        <td style="text-align:center;">Gaussian Kernel</td>        <td style="text-align:center;">Gaussian Kernel</td>        <td>Gaussian Kernel</td>       </tr>       <tr>        <td style="text-align:center;">COF</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td style="text-align:center;">        <em>k</em> = 5</td>        <td>        <em>k</em> = 10</td>       </tr>       <tr>        <td style="text-align:center;">LDF</td>        <td style="text-align:center;">        <em>h</em> = 1, <em>c</em> = 0.1</td>        <td style="text-align:center;">        <em>h</em> = 1, <em>c</em> = 0.1</td>        <td>        <em>h</em> = 1, <em>c</em> = 0.1</td>       </tr>       <tr>        <td style="text-align:center;">&#x00A0;</td>        <td style="text-align:center;">Gaussian Kernel</td>        <td style="text-align:center;">Gaussian Kernel</td>        <td>Gaussian Kernel</td>       </tr>       <tr>        <td style="text-align:center;">INFLO</td>        <td style="text-align:center;">        <em>k</em> = 5, <em>m</em> = 0.5</td>        <td style="text-align:center;">        <em>k</em> = 5, <em>m</em> = 0.5</td>        <td>        <em>k</em> = 10, <em>m</em> = 0.5</td>       </tr>       <tr>        <td style="text-align:center;">FastVOA</td>        <td style="text-align:center;">        <em>k</em> = 5, |<em>S</em>        <sub>2</sub>| = 2, |<em>S</em>        <sub>1</sub>| = 320</td>        <td style="text-align:center;">        <em>k</em> = 5, |<em>S</em>        <sub>2</sub>| = 2, |<em>S</em>        <sub>1</sub>| = 320</td>        <td>        <em>k</em> = 10, |<em>S</em>        <sub>2</sub>| = 2, |<em>S</em>        <sub>1</sub>| = 320</td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Comparison Results.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:center;">Dateset</th>        <th style="text-align:center;">Method</th>        <th style="text-align:center;">Reported</th>        <th style="text-align:center;">Correct</th>        <th style="text-align:center;">Missed</th>        <th style="text-align:center;">F1-score</th>        <th style="text-align:center;">F1-Rank</th>        <th style="text-align:center;">Time (s)</th>        <th style="text-align:center;">Speed-up with ACE</th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:center;">Statlog Shuttle</td>        <td style="text-align:center;">ACE</td>        <td style="text-align:center;">6763</td>        <td style="text-align:center;">273</td>        <td style="text-align:center;">606</td>        <td style="text-align:center;">0.071</td>        <td style="text-align:center;">5</td>        <td style="text-align:center;">        <strong>0.81s</strong>        </td>        <td style="text-align:center;">1x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LOF</td>        <td style="text-align:center;">4356</td>        <td style="text-align:center;">381</td>        <td style="text-align:center;">498</td>        <td style="text-align:center;">0.145</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">14.12s</td>        <td style="text-align:center;">17.4x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">kNN</td>        <td style="text-align:center;">4897</td>        <td style="text-align:center;">493</td>        <td style="text-align:center;">386</td>        <td style="text-align:center;">0.170</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">12.35s</td>        <td style="text-align:center;">15.2x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">kNNW</td>        <td style="text-align:center;">5264</td>        <td style="text-align:center;">610</td>        <td style="text-align:center;">269</td>        <td style="text-align:center;">0.199</td>        <td style="text-align:center;">1</td>        <td style="text-align:center;">13.54s</td>        <td style="text-align:center;">16.7x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LoOP</td>        <td style="text-align:center;">6145</td>        <td style="text-align:center;">201</td>        <td style="text-align:center;">678</td>        <td style="text-align:center;">0.057</td>        <td style="text-align:center;">8</td>        <td style="text-align:center;">14.51s</td>        <td style="text-align:center;">17.9x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LDOF</td>        <td style="text-align:center;">6433</td>        <td style="text-align:center;">330</td>        <td style="text-align:center;">549</td>        <td style="text-align:center;">0.090</td>        <td style="text-align:center;">4</td>        <td style="text-align:center;">16.42s</td>        <td style="text-align:center;">20.3x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">ODIN</td>        <td style="text-align:center;">9775</td>        <td style="text-align:center;">375</td>        <td style="text-align:center;">504</td>        <td style="text-align:center;">0.071</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">12.21s</td>        <td style="text-align:center;">15.1x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">KDEOS</td>        <td style="text-align:center;">12630</td>        <td style="text-align:center;">314</td>        <td style="text-align:center;">565</td>        <td style="text-align:center;">0.046</td>        <td style="text-align:center;">12</td>        <td style="text-align:center;">11.73s</td>        <td style="text-align:center;">14.5x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">COF</td>        <td style="text-align:center;">9133</td>        <td style="text-align:center;">280</td>        <td style="text-align:center;">599</td>        <td style="text-align:center;">0.056</td>        <td style="text-align:center;">11</td>        <td style="text-align:center;">13.45s</td>        <td style="text-align:center;">16.6x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LDF</td>        <td style="text-align:center;">9809</td>        <td style="text-align:center;">375</td>        <td style="text-align:center;">504</td>        <td style="text-align:center;">0.070</td>        <td style="text-align:center;">7</td>        <td style="text-align:center;">19.93s</td>        <td style="text-align:center;">24.6x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">INFLO</td>        <td style="text-align:center;">4488</td>        <td style="text-align:center;">183</td>        <td style="text-align:center;">696</td>        <td style="text-align:center;">0.068</td>        <td style="text-align:center;">8</td>        <td style="text-align:center;">14.03</td>        <td style="text-align:center;">17.3x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">FastVOA</td>        <td style="text-align:center;">8532</td>        <td style="text-align:center;">271</td>        <td style="text-align:center;">608</td>        <td style="text-align:center;">0.057</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">235.10s</td>        <td style="text-align:center;">290.2x</td>       </tr>       <tr>        <td style="text-align:center;">Image Object</td>        <td style="text-align:center;">ACE</td>        <td style="text-align:center;">7216</td>        <td style="text-align:center;">340</td>        <td style="text-align:center;">1168</td>        <td style="text-align:center;">0.078</td>        <td style="text-align:center;">5</td>        <td style="text-align:center;">        <strong>1.26s</strong>        </td>        <td style="text-align:center;">1x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LOF</td>        <td style="text-align:center;">4476</td>        <td style="text-align:center;">519</td>        <td style="text-align:center;">989</td>        <td style="text-align:center;">0.1735</td>        <td style="text-align:center;">1</td>        <td style="text-align:center;">72.31s</td>        <td style="text-align:center;">57.4x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">kNN</td>        <td style="text-align:center;">5428</td>        <td style="text-align:center;">447</td>        <td style="text-align:center;">1061</td>        <td style="text-align:center;">0.1289</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">63.27s</td>        <td style="text-align:center;">50.2x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">kNNW</td>        <td style="text-align:center;">5558</td>        <td style="text-align:center;">329</td>        <td style="text-align:center;">1508</td>        <td style="text-align:center;">0.089</td>        <td style="text-align:center;">4</td>        <td style="text-align:center;">89.96s</td>        <td style="text-align:center;">71.4x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LoOP</td>        <td style="text-align:center;">5121</td>        <td style="text-align:center;">253</td>        <td style="text-align:center;">1179</td>        <td style="text-align:center;">0.077</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">59.97s</td>        <td style="text-align:center;">47.6x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LDOF</td>        <td style="text-align:center;">7501</td>        <td style="text-align:center;">470</td>        <td style="text-align:center;">1038</td>        <td style="text-align:center;">0.1043</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">60.39s</td>        <td style="text-align:center;">47.9x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">ODIN</td>        <td style="text-align:center;">10110</td>        <td style="text-align:center;">162</td>        <td style="text-align:center;">1346</td>        <td style="text-align:center;">0.028</td>        <td style="text-align:center;">12</td>        <td style="text-align:center;">72.69s</td>        <td style="text-align:center;">57.6x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">KDEOS</td>        <td style="text-align:center;">9515</td>        <td style="text-align:center;">404</td>        <td style="text-align:center;">1104</td>        <td style="text-align:center;">0.073</td>        <td style="text-align:center;">7</td>        <td style="text-align:center;">55.89s</td>        <td style="text-align:center;">44.36x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">COF</td>        <td style="text-align:center;">8746</td>        <td style="text-align:center;">284</td>        <td style="text-align:center;">1224</td>        <td style="text-align:center;">0.055</td>        <td style="text-align:center;">11</td>        <td style="text-align:center;">81.74s</td>        <td style="text-align:center;">64.9x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LDF</td>        <td style="text-align:center;">9133</td>        <td style="text-align:center;">301</td>        <td style="text-align:center;">1207</td>        <td style="text-align:center;">0.056</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">60.51s</td>        <td style="text-align:center;">48.0x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">INFLO</td>        <td style="text-align:center;">10328</td>        <td style="text-align:center;">420</td>        <td style="text-align:center;">1088</td>        <td style="text-align:center;">0.071</td>        <td style="text-align:center;">8</td>        <td style="text-align:center;">72.13s</td>        <td style="text-align:center;">57.2x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">FastVOA</td>        <td style="text-align:center;">8931</td>        <td style="text-align:center;">319</td>        <td style="text-align:center;">1189</td>        <td style="text-align:center;">0.061</td>        <td style="text-align:center;">9</td>        <td style="text-align:center;">291.10s</td>        <td style="text-align:center;">231.0x</td>       </tr>       <tr>        <td style="text-align:center;">KDD-CUP 99</td>        <td style="text-align:center;">ACE</td>        <td style="text-align:center;">15160</td>        <td style="text-align:center;">406</td>        <td style="text-align:center;">649</td>        <td style="text-align:center;">0.051</td>        <td style="text-align:center;">5</td>        <td style="text-align:center;">        <strong>23.33s</strong>        </td>        <td style="text-align:center;">1x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LOF</td>        <td style="text-align:center;">13260</td>        <td style="text-align:center;">523</td>        <td style="text-align:center;">532</td>        <td style="text-align:center;">0.073</td>        <td style="text-align:center;">1</td>        <td style="text-align:center;">1813.63s</td>        <td style="text-align:center;">77.7x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">kNN</td>        <td style="text-align:center;">15432</td>        <td style="text-align:center;">365</td>        <td style="text-align:center;">690</td>        <td style="text-align:center;">0.044</td>        <td style="text-align:center;">7</td>        <td style="text-align:center;">1483.54s</td>        <td style="text-align:center;">63.5x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">kNNW</td>        <td style="text-align:center;">14328</td>        <td style="text-align:center;">460</td>        <td style="text-align:center;">595</td>        <td style="text-align:center;">0.059</td>        <td style="text-align:center;">2</td>        <td style="text-align:center;">2125.43s</td>        <td style="text-align:center;">91.1x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LoOP</td>        <td style="text-align:center;">16578</td>        <td style="text-align:center;">396</td>        <td style="text-align:center;">659</td>        <td style="text-align:center;">0.045</td>        <td style="text-align:center;">6</td>        <td style="text-align:center;">1594.54s</td>        <td style="text-align:center;">68.3x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LDOF</td>        <td style="text-align:center;">16579</td>        <td style="text-align:center;">496</td>        <td style="text-align:center;">559</td>        <td style="text-align:center;">0.056</td>        <td style="text-align:center;">3</td>        <td style="text-align:center;">1674.43s</td>        <td style="text-align:center;">71.7x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">ODIN</td>        <td style="text-align:center;">18054</td>        <td style="text-align:center;">365</td>        <td style="text-align:center;">690</td>        <td style="text-align:center;">0.038</td>        <td style="text-align:center;">10</td>        <td style="text-align:center;">1918.34s</td>        <td style="text-align:center;">82.2x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">KDEOS</td>        <td style="text-align:center;">21095</td>        <td style="text-align:center;">469</td>        <td style="text-align:center;">586</td>        <td style="text-align:center;">0.042</td>        <td style="text-align:center;">8</td>        <td style="text-align:center;">1428.32s</td>        <td style="text-align:center;">61.2x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">COF</td>        <td style="text-align:center;">20658</td>        <td style="text-align:center;">584</td>        <td style="text-align:center;">471</td>        <td style="text-align:center;">0.054</td>        <td style="text-align:center;">4</td>        <td style="text-align:center;">2043.43s</td>        <td style="text-align:center;">87.5x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">LDF</td>        <td style="text-align:center;">19574</td>        <td style="text-align:center;">368</td>        <td style="text-align:center;">687</td>        <td style="text-align:center;">0.036</td>        <td style="text-align:center;">11</td>        <td style="text-align:center;">1485.85s</td>        <td style="text-align:center;">63.7x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">INFLO</td>        <td style="text-align:center;">25704</td>        <td style="text-align:center;">565</td>        <td style="text-align:center;">490</td>        <td style="text-align:center;">0.042</td>        <td style="text-align:center;">9</td>        <td style="text-align:center;">1684.47s</td>        <td style="text-align:center;">72.2x</td>       </tr>       <tr>        <td style="text-align:center;"/>        <td style="text-align:center;">FastVOA</td>        <td style="text-align:center;">29316</td>        <td style="text-align:center;">354</td>        <td style="text-align:center;">701</td>        <td style="text-align:center;">0.023</td>        <td style="text-align:center;">12</td>        <td style="text-align:center;">3510.26s</td>        <td style="text-align:center;">150.4x</td>       </tr>       <tr>        <td style="text-align:center;"></td>        <td/>        <td/>        <td/>        <td/>        <td/>        <td/>        <td/>        <td/>       </tr>      </tbody>     </table>     </div>     <p>We use the highly optimized recent <em>ELKI</em> (Environment for Developing KDD-Applications Supported by Index-Structures) package&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] which is the most advanced set of anomaly detection algorithms noted for its efficient implementations. 10 of our baselines methodologies are implemented in this package. For FastVOA, a state-of-the-art randomized algorithm for variance of angle computation, we use the C++ package provided by the authors.</p>     <p>It should be noted that ACE and FastVOA are implemented in C++, while <em>ELKI</em> is a java package. A direct wall clock comparison is not fair. However, given the simplicity of our algorithm (Algorithm&#x00A0;1) which only requires simple hashing, use of primitive arrays, and simple summations. We do need any complex object other than arrays of short integers (primitives only). All other operations are primitive multiplications and summations. Thus, we expect that the difference between Java and C++ implementation would not be any significant for ACE. Furthermore, we show a significant speedup which cannot be explained by the difference in platforms.</p>     <p>     <strong>Parameter Settings:</strong> Almost all of our baseline algorithms need hyper-parameters. We use most of the default settings of the parameters as implemented. For the baseline algorithms, the <em>ELKI</em> package has the recommended settings of parameters for these benchmark datasets. To avoid complications, we directly use those recommended settings. For the sake of reproducibility, we provide the precise recommended settings of the parameters for different methods and datasets used in the paper in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. It should be noted that for ACE we use the fixed value of <em>K</em> = 15 and <em>L</em> = 50 for all the datasets. ACE does not need the near neighbor parameter <em>k</em> (small). Variations in parameter <em>K</em> and <em>L</em> are discussed in Section&#x00A0;<a class="sec" href="#sec-16">6</a>.</p>     <p>     <strong>System and Platform Details:</strong> Experiments were conducted on a 3.50 GHz core Xeon Windows platform with 16GB of RAM. We use g++ (version 5.4.0) as the C++ compiler for ACE and fastVOA. For <em>ELKI</em> package, we use OpenJDK 64bits version 1.8.0.</p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Methodology and Results</h3>     </div>     </header>     <p>All of these 12 algorithms associate a score with every element in the data. After association, a significantly lower score from the mean indicates an anomaly. In order to convert these scores into an anomaly detections algorithm, there are many reasonable strategies. We can rank each candidate, based on scores, and report bottom-<em>k</em> as the anomalies, but such rankings are not realistic. In real-time applications, ranking all the seen records is artificial. A more practical approach is to use a threshold strategy to report anomalies. We compute the mean <em>&#x03BC;</em> and the standard deviations <em>&#x03C3;</em> of the scores on the dataset of interest and report any element with the associated score less than <em>&#x03BC;</em> &#x2212; <em>&#x03C3;</em> as an anomaly.</p>     <p>With the above introduced anomaly detection strategies, we run all the 12 algorithms on the three datasets. We report seven different numbers separately for each of the three datasets: (1) Number of outliers reported; (2) Number of outliers correctly reported; (3) Number of Outliers missed; (4) F1-score; (5) Ranking of F1-score; (6) the CPU execution time for the different methods, and (7) Relative speed with ACE. The CPU executing time is the end to end time of the complete run of the algorithm, which includes data reading, preprocessing (if any), scoring every data instance, and reporting outliers. Relative speedup reports the ratio of the time required by a given algorithm to the time required by ACE algorithm. The results of each datasets are shown in Table <a class="tbl" href="#tab3">3</a>.</p>     <p>     <strong>Accuracy Comparison.</strong> We report the F1-scores[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] of each method. F1-score is a widely used method for evaluating the performance of anomaly detection methods, for the detailed definition of F1-score please refer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. Based on the F1-scores of each method, we rank the different methods. From the results, we can see LOF seems to be consistently more accurate than others. ACE is ranked consistently among the top-5 ranked methods on all the datasets. The number of anomalies reported correctly (true positives) with ACE is similar to other algorithms. ACE, however, seems to report slightly more anomalies (high false positives) than other algorithms. This is not a major concern though. Few extra false positives are easy to deal with because we can always further filter them using a more sophisticated algorithm, so long as they are small. Overall, our proposed new scoring scheme <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span> and the corresponding estimator performs very competitively, in terms of accuracy, in comparison with many successful algorithms.</p>     <p>     <strong>Running Time Comparison.</strong> The most exciting part is the computational savings with ACE. From the result, we observe that ACE is significantly faster than any other alternatives irrespective of the choice of dataset. ACE algorithm is at least around 15x, 45x and 60x faster than the best competitor on Statlog Shuttle, Object Images (ALOI), and KDD-Cup99 HTTP datasets respectively. Most of the algorithms, based on near-neighbors except FastVOA, have similar speeds. This could be because almost all of them requires computation of the order of the data. FastVOA is consistently very slow, which we suspect is because the estimators used in FastVOA is computationally very expensive. FastVOA estimators require multiple sorting and frequently computing costly medians. See&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>] for details. ACE is around 150-300x faster than FastVOA.</p>     <p>     <strong>Memory Analysis.</strong> The results are even more exciting if we start considering the memory requirements. With <em>K</em> = 15 and <em>L</em> = 50, our methodology requires less than 4<em>MB</em> of operating memory for the complete run of the algorithm. Since we use the same <em>K</em> and <em>L</em> across all datasets, this 4<em>MB</em> requirement is unaltered. We never keep any data in the memory. On the other hand, all other methods except FastVOA require storing complete data in the memory. In our case, the KDD-Cup99 HTTP dataset itself is around 165MB to store. Although KDD-Cup99 HTTP dataset is the largest labeled benchmark, it is still tiny from big-data perspective. The disruptive performance of ACE is not surprising given the simplicity of the process. However, as argued, the process is a statistically sound procedure for estimating the proposed score <span class="inline-equation"><span class="tex">$S(q,\mathcal {D})$</span>     </span>.</p>    </section>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Discussion: Effects of K and L</h2>     </div>    </header>    <p>Parameters <em>K</em> and <em>L</em> determine the memory and also the running time of the ACE algorithm. Note <em>L</em> is also the number of independent samples used for averages. Therefore, a reasonably large <em>L</em> is good enough, after which increasing <em>L</em> does not give significant accuracy but hurts the performance. <em>K</em> cannot be too small because locations in arrays should distinguish anomalies with everything else. However, too large <em>K</em> is not needed either. Ideally, if <em>K</em> is log&#x2009;<em>n</em>, then under random assignments all data will go to the single bucket. Beyond this <em>K</em>, the performance is lost for no gain in accuracy.</p>    <p>To stress test, we ran ACE for with different values of <em>K</em> = {2,&#x2009;5,&#x2009;...,&#x2009;20} and <em>L</em> = {10,&#x2009;20,&#x2009;...,&#x2009;100}. For the Image dataset, the minimum reasonable result appears at K=8, L=30. For the shuttle dataset, K=11, L=10 is fine, and for the KDDCUP dataset, the minimum fair result appears at K=9, L=30. These parameters give similar results as shown with fixed <em>K</em> = 15 and <em>L</em> = 50. With these parameters, the ACE took mere 0.5, 0.2 and 11.5 seconds on the Image, shuttle and KDDCUP dataset for a negligible loss in accuracy compared to what is shown in Table&#x00A0;<a class="tbl" href="#tab3">3</a>. The results degrade if we decrease <em>K</em> and <em>L</em> beyond these numbers. Increasing <em>K</em> and <em>L</em> values, significantly beyond <em>K</em> = 15 and <em>L</em> = 50, does not increase the accuracies significantly but, as expected, hurts the performance.</p>   </section>   <section id="sec-17">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion</h2>     </div>    </header>    <p>Statistical measures for popular learning and data mining problems, such as anomaly detection, were designed without taking into account the computational complexity of the estimation process. When faced with current big-data challenges, most of these estimation process fail to address tight resources constraints. In this paper, we showed that for the problem of unsupervised anomaly detection, we could leverage advances in probabilistic indexing and redesign a significantly efficient statistical measure.</p>    <p>We proposed ACE algorithm, for unsupervised anomaly detection, which is 60-300x faster than existing approaches with competing accuracy. Our algorithm requires mere 4<em>MB</em> of memory which can utilize L3 caches of modern processors leading to fast-lookups. We believe ACE will replace existing unsupervised anomaly detection algorithms deployed in resource-frugal environments.</p>   </section>   <section id="sec-18">    <header>     <div class="title-info">     <h2>Acknowledgments</h2>     </div>    </header>    <p>This work was supported by National Science Foundation IIS-1652131, RI-1718478, and a GPU grant from NVIDIA</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Elke Achtert, Thomas Bernecker, Hans-Peter Kriegel, Erich Schubert, and Arthur Zimek. 2009. ELKI in Time: ELKI 0.2 for the Performance Evaluation of Distance Measures for Time Series. In <em>      <em>Advances in Spatial and Temporal Databases, 11th International Symposium, SSTD 2009, Aalborg, Denmark, July 8-10, 2009, Proceedings.</em>     </em> 436&#x2013;440.</li>     <li id="BibPLXBIB0002" label="[2]">Charu&#x00A0;C Aggarwal and Philip&#x00A0;S Yu. 2001. Outlier detection for high dimensional data. In <em>      <em>ACM Sigmod Record</em>     </em>, Vol.&#x00A0;30. ACM, 37&#x2013;46.</li>     <li id="BibPLXBIB0003" label="[3]">Nir Ailon and Bernard Chazelle. 2006. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In <em>      <em>Proceedings of the thirty-eighth annual ACM symposium on Theory of computing.</em>     </em> ACM, 557&#x2013;563.</li>     <li id="BibPLXBIB0004" label="[4]">Fabrizio Angiulli and Clara Pizzuti. 2005. Outlier mining in large high-dimensional data sets. <em>      <em>IEEE transactions on Knowledge and Data engineering</em>     </em>17, 2(2005), 203&#x2013;215.</li>     <li id="BibPLXBIB0005" label="[5]">Kanishka Bhaduri, Mark&#x00A0;D Stefanski, and Ashok&#x00A0;N Srivastava. 2011. Privacy-preserving outlier detection through random nonlinear data distortion. <em>      <em>IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</em>     </em>41, 1 (2011), 260&#x2013;272.</li>     <li id="BibPLXBIB0006" label="[6]">Markus&#x00A0;M Breunig, Hans-Peter Kriegel, Raymond&#x00A0;T Ng, and J&#x00F6;rg Sander. 2000. LOF: identifying density-based local outliers. In <em>      <em>ACM sigmod record</em>     </em>, Vol.&#x00A0;29. ACM, 93&#x2013;104.</li>     <li id="BibPLXBIB0007" label="[7]">Martin Burkhart, Mario Strasser, Dilip Many, and Xenofontas Dimitropoulos. 2010. SEPIA: Privacy-preserving aggregation of multi-domain network events and statistics. <em>      <em>Network</em>     </em>1, 101101 (2010).</li>     <li id="BibPLXBIB0008" label="[8]">Aniket Chakrabarti, Venu Satuluri, Atreya Srivathsan, and Srinivasan Parthasarathy. 2015. A bayesian perspective on locality sensitive hashing with extensions for kernel methods. <em>      <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>     </em>10, 2(2015), 19.</li>     <li id="BibPLXBIB0009" label="[9]">Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. <em>      <em>ACM computing surveys (CSUR)</em>     </em>41, 3 (2009), 15.</li>     <li id="BibPLXBIB0010" label="[10]">Moses Charikar and Paris Siminelakis. [n. d.]. Hashing-based-estimators for kernel density in high dimensions.</li>     <li id="BibPLXBIB0011" label="[11]">Moses&#x00A0;S Charikar. 2002. Similarity estimation techniques from rounding algorithms. In <em>      <em>Proceedings of the thiry-fourth annual ACM symposium on Theory of computing.</em>     </em> ACM, 380&#x2013;388.</li>     <li id="BibPLXBIB0012" label="[12]">Beidi Chen, Anshumali Shrivastava, and Rebecca&#x00A0;C Steorts. 2017. Unique Entity Estimation with Application to the Syrian Conflict. <em>      <em>arXiv preprint arXiv:1710.02690</em>     </em>(2017).</li>     <li id="BibPLXBIB0013" label="[13]">Beidi Chen, Yingchen Xu, and Anshumali Shrivastava. 2018. LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION. (2018). <a class="link-inline force-break" href="https://openreview.net/forum?id=SyVOjfbRb">https://openreview.net/forum?id=SyVOjfbRb</a></li>     <li id="BibPLXBIB0014" label="[14]">Cisco Visual Networking&#x00A0;Index Cisco. 2014. Global mobile data traffic forecast update, 2013&#x2013;2018. <em>      <em>white paper</em>     </em> (2014).</li>     <li id="BibPLXBIB0015" label="[15]">Anirban Dasgupta, Ravi Kumar, and Tam&#x00E1;s Sarl&#x00F3;s. 2011. Fast locality-sensitive hashing. In <em>      <em>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining.</em>     </em> ACM, 1073&#x2013;1081.</li>     <li id="BibPLXBIB0016" label="[16]">Jan-Mark Geusebroek, Gertjan&#x00A0;J Burghouts, and Arnold&#x00A0;WM Smeulders. 2005. The Amsterdam library of object images. <em>      <em>International Journal of Computer Vision</em>     </em>61, 1 (2005), 103&#x2013;112.</li>     <li id="BibPLXBIB0017" label="[17]">Aristides Gionis, Piotr Indyk, Rajeev Motwani, <em>et al.</em> 1999. Similarity search in high dimensions via hashing. In <em>      <em>VLDB</em>     </em>, Vol.&#x00A0;99. 518&#x2013;529.</li>     <li id="BibPLXBIB0018" label="[18]">Michel&#x00A0;X Goemans and David&#x00A0;P Williamson. 1994. .879-approximation algorithms for max cut and max 2sat. In <em>      <em>Proceedings of the twenty-sixth annual ACM symposium on Theory of computing.</em>     </em> ACM, 422&#x2013;431.</li>     <li id="BibPLXBIB0019" label="[19]">Oded Goldreich. 1998. Secure multi-party computation. <em>      <em>Manuscript. Preliminary version</em>     </em>(1998), 86&#x2013;97.</li>     <li id="BibPLXBIB0020" label="[20]">Nico G&#x00F6;rnitz, Marius&#x00A0;Micha Kloft, Konrad Rieck, and Ulf Brefeld. 2013. Toward supervised anomaly detection. <em>      <em>Journal of Artificial Intelligence Research</em>     </em> (2013).</li>     <li id="BibPLXBIB0021" label="[21]">Ville Hautamaki, Ismo Karkkainen, and Pasi Franti. 2004. Outlier detection using k-nearest neighbour graph. In <em>      <em>Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on</em>     </em>, Vol.&#x00A0;3. IEEE, 430&#x2013;433.</li>     <li id="BibPLXBIB0022" label="[22]">Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In <em>      <em>Proceedings of the thirtieth annual ACM symposium on Theory of computing.</em>     </em> ACM, 604&#x2013;613.</li>     <li id="BibPLXBIB0023" label="[23]">Wen Jin, Anthony&#x00A0;KH Tung, Jiawei Han, and Wei Wang. 2006. Ranking outliers using symmetric neighborhood relationship. In <em>      <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining.</em>     </em> Springer, 577&#x2013;593.</li>     <li id="BibPLXBIB0024" label="[24]">Krishnaram Kenthapadi, Aleksandra Korolova, Ilya Mironov, and Nina Mishra. 2012. Privacy via the johnson-lindenstrauss transform. <em>      <em>arXiv preprint arXiv:1204.2606</em>     </em>(2012).</li>     <li id="BibPLXBIB0025" label="[25]">Jon Kleinberg. 2002. Bursty and hierarchical structure in streams. In <em>      <em>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining.</em>     </em> ACM, 91&#x2013;101.</li>     <li id="BibPLXBIB0026" label="[26]">Hans-Peter Kriegel, Peer Kr&#x00F6;ger, Erich Schubert, and Arthur Zimek. 2009. LoOP: local outlier probabilities. In <em>      <em>Proceedings of the 18th ACM conference on Information and knowledge management.</em>     </em> ACM, 1649&#x2013;1652.</li>     <li id="BibPLXBIB0027" label="[27]">Longin&#x00A0;Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. 2007. Outlier detection with kernel density functions. In <em>      <em>International Workshop on Machine Learning and Data Mining in Pattern Recognition.</em>     </em> Springer, 61&#x2013;75.</li>     <li id="BibPLXBIB0028" label="[28]">Kingsly Leung and Christopher Leckie. 2005. Unsupervised anomaly detection in network intrusion detection using clusters. In <em>      <em>Proceedings of the Twenty-eighth Australasian conference on Computer Science-Volume 38.</em>     </em> Australian Computer Society, Inc., 333&#x2013;342.</li>     <li id="BibPLXBIB0029" label="[29]">Chen Luo, Jian-Guang Lou, Qingwei Lin, Qiang Fu, Rui Ding, Dongmei Zhang, and Zhe Wang. 2014. Correlating events with time series for incident diagnosis. In <em>      <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining.</em>     </em> ACM, 1583&#x2013;1592.</li>     <li id="BibPLXBIB0030" label="[30]">Chen Luo and Anshumali Shrivastava. 2017. SSH (Sketch, Shingle, &#x0026; Hash) for Indexing Massive-Scale Time Series. In <em>      <em>NIPS 2016 Time Series Workshop.</em>     </em> 38&#x2013;58.</li>     <li id="BibPLXBIB0031" label="[31]">Ashwin Machanavajjhala, Daniel Kifer, John Abowd, Johannes Gehrke, and Lars Vilhuber. 2008. Privacy: Theory meets practice on the map. In <em>      <em>Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on.</em>     </em> IEEE, 277&#x2013;286.</li>     <li id="BibPLXBIB0032" label="[32]">Ninh Pham and Rasmus Pagh. 2012. A near-linear time approximation algorithm for angle-based outlier detection in high-dimensional data. In <em>      <em>Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining.</em>     </em> ACM, 877&#x2013;885.</li>     <li id="BibPLXBIB0033" label="[33]">Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. 2000. Efficient algorithms for mining outliers from large data sets. In <em>      <em>ACM Sigmod Record</em>     </em>, Vol.&#x00A0;29. ACM, 427&#x2013;438.</li>     <li id="BibPLXBIB0034" label="[34]">Erich Schubert, Alexander Koos, Tobias Emrich, Andreas Z&#x00FC;fle, Klaus&#x00A0;Arthur Schmid, and Arthur Zimek. 2015. A framework for clustering uncertain data. <em>      <em>Proceedings of the VLDB Endowment</em>     </em>8, 12 (2015), 1976&#x2013;1979.</li>     <li id="BibPLXBIB0035" label="[35]">Erich Schubert, Remigius Wojdanowski, Arthur Zimek, and Hans-Peter Kriegel. 2012. On evaluation of outlier rankings and outlier scores. In <em>      <em>Proceedings of the 2012 SIAM International Conference on Data Mining.</em>     </em> SIAM, 1047&#x2013;1058.</li>     <li id="BibPLXBIB0036" label="[36]">Erich Schubert, Arthur Zimek, and Hans-Peter Kriegel. 2014. Generalized outlier detection with flexible kernel density estimates. In <em>      <em>Proceedings of the 2014 SIAM International Conference on Data Mining.</em>     </em> SIAM, 542&#x2013;550.</li>     <li id="BibPLXBIB0037" label="[37]">Anshumali Shrivastava. 2016. Simple and efficient weighted minwise hashing. In <em>      <em>Advances in Neural Information Processing Systems.</em>     </em> 1498&#x2013;1506.</li>     <li id="BibPLXBIB0038" label="[38]">Anshumali Shrivastava. 2017. Optimal Densification for Fast and Accurate Minwise Hashing.. In <em>      <em>ICML.</em>     </em></li>     <li id="BibPLXBIB0039" label="[39]">Anshumali Shrivastava and Ping Li. 2014. Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search.. In <em>      <em>ICML.</em>     </em> 557&#x2013;565.</li>     <li id="BibPLXBIB0040" label="[40]">Anshumali Shrivastava and Ping Li. 2014. Improved Densification of One Permutation Hashing.. In <em>      <em>UAI.</em>     </em></li>     <li id="BibPLXBIB0041" label="[41]">Ryan Spring and Anshumali Shrivastava. 2017. A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators for Partition Function Computation in Log-Linear Models. <em>      <em>ArXiv e-prints</em>     </em> (2017). arXiv:1703.05160</li>     <li id="BibPLXBIB0042" label="[42]">Ryan Spring and Anshumali Shrivastava. 2017. Scalable and sustainable deep learning via randomized hashing. In <em>      <em>KDD.</em>     </em></li>     <li id="BibPLXBIB0043" label="[43]">Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David&#x00A0;W Cheung. 2002. Enhancing effectiveness of outlier detections for low density patterns. In <em>      <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining.</em>     </em> Springer, 535&#x2013;548.</li>     <li id="BibPLXBIB0044" label="[44]">Luan Tran, Liyue Fan, and Cyrus Shahabi. 2016. Distance-based outlier detection in data streams. <em>      <em>Proceedings of the VLDB Endowment</em>     </em>9, 12 (2016), 1089&#x2013;1100.</li>     <li id="BibPLXBIB0045" label="[45]">Jaideep Vaidya and Chris Clifton. 2004. Privacy-preserving outlier detection. In <em>      <em>Data Mining, 2004. ICDM&#x2019;04. Fourth IEEE International Conference on.</em>     </em> IEEE, 233&#x2013;240.</li>     <li id="BibPLXBIB0046" label="[46]">Xindong Wu, Vipin Kumar, J&#x00A0;Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi Motoda, Geoffrey&#x00A0;J McLachlan, Angus Ng, Bing Liu, S&#x00A0;Yu Philip, <em>et al.</em> 2008. Top 10 algorithms in data mining. <em>      <em>Knowledge and information systems</em>     </em>14, 1 (2008), 1&#x2013;37.</li>     <li id="BibPLXBIB0047" label="[47]">Ke Zhang, Marcus Hutter, and Huidong Jin. 2009. A new local distance-based outlier detection approach for scattered real-world data. <em>      <em>Advances in knowledge discovery and data mining</em>     </em> (2009), 813&#x2013;822.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://archive.ics.uci.edu/ml/datasets/Statlog">https://archive.ics.uci.edu/ml/datasets/Statlog</a>+(Shuttle)</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="http://aloi.science.uva.nl/">http://aloi.science.uva.nl/</a></p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break"     href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html">http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/> ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186056">https://doi.org/10.1145/3178876.3186056</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
