<!DOCTYPE html> htmlhtml<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>On Exploring Semantic Meanings of Links for Embedding Social Networks</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">On Exploring Semantic Meanings of Links for Embedding Social Networks</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Linchuan</span>      <span class="surName">Xu</span>,     The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, <a href="mailto:cslcxu@comp.polyu.edu.hk">cslcxu@comp.polyu.edu.hk</a>     </div>     <div class="author">     <span class="givenName">Xiaokai</span>      <span class="surName">Wei</span>,     Facebook Inc., 1 Hacker Way, Menlo Park, CA, USA<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a>, <a href="mailto:weixiaokai@gmail.com">weixiaokai@gmail.com</a>     </div>     <div class="author">     <span class="givenName">Jiannong</span>      <span class="surName">Cao</span>,     The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, <a href="mailto:csjcao@comp.polyu.edu.hk">csjcao@comp.polyu.edu.hk</a>     </div>     <div class="author">     <span class="givenName">Philip S.</span>      <span class="surName">Yu</span>,     University of Illinois at Chicago &#x0026; Institute for Data Science, Tsinghua University, Chicago, Illinois, USA, <a href="mailto:psyu@uic.edu">psyu@uic.edu</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186114" target="_blank">https://doi.org/10.1145/3178876.3186114</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>There are increasing interests in learning low-dimensional and dense node representations from the network structure which is usually high-dimensional and sparse. However, most existing methods fail to consider semantic meanings of links. Different links may have different semantic meanings because the similarities between two nodes can be different, e.g., two nodes share common neighbors and two nodes share similar interests which are demonstrated in node-generated content. In this paper, the former type of links are referred to as structure-close links while the latter type are referred to as content-close links. These two types of links naturally indicate there are two types of characteristics that nodes expose in a social network. Hence, we propose to learn two representations for each node, and render each representation responsible for encoding the corresponding type of node characteristics, which is achieved by jointly embedding the network structure and inferring the type of each link. In the experiments, the proposed method is demonstrated to be more effective than five recent methods on four social networks through applications including visualization, link prediction and multi-label classification.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Data mining;</strong> <strong>Social networks;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Network embedding; social networks; data mining</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip S. Yu. 2018. On Exploring Semantic Meanings of Links for Embedding Social Networks. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186114" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186114</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Social networks can meet and even create social needs for human beings as it is very convenient for people to reach out and get connected to others. We can analyze data regarding social network involvement to help improve networks and user experiences. This may explain why there are increasing interests from both industry and academia in personalizing services to users&#x2019; interests, behaviors and attributes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>], detecting communities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>], recommending products [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>] or friends [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>]. However, the network representation, e.g., adjacency matrix, is usually high-dimensional and spare. High-dimensionality makes it inefficient to train data mining models while sparsity makes it not easy to generalize trained models for future usage.</p>    <p>Recently, network embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>] has been utilized to learn low-dimensional and dense node representations basically by embedding the network structure in a certain Euclidean space. Although being demonstrated effective to preserve the network structure, almost all existing methods fail to consider semantic meanings of links in a social network where different links may have different semantic meanings. Semantically different links result from different reasons for which links are established and the reasons can be of various types in practice, e.g., two persons share a common university friend and two persons are fans of Star Wars. Hence, semantically different links may indicate nodes expose different types of information in a social network, e.g., the former example mentioned above may indicate the two persons expose information about their education while the latter example may indicate they expose information about their interests. As a result, it is more reasonable to embed the network structure with the semantic meanings of each link considered.</p>    <p>In this paper, we thus take the first step towards exploring semantic meanings of social links in the context of network embedding. Specifically, we propose to categorize social links according to their semantic meanings and learn node representations with semantic concepts. Inspired by homophily principle [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] which suggests links exist between nodes with similarities, we categorize links according to the type of similarities two nodes share. As the first work, this paper only considers two types of coarse-grained similarities, i.e., common neighbors and similar interests demonstrated in node-generated content, which represent two most frequently used types of similarity measurement, i.e., topology-based similarities and node content-based similarities, for two nodes to have a link [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. If two nodes share 	  common neighbors, the link can be referred to as a 	  <em>structure-close</em> link while if they share 	  interests, the link can be referred to as a 	  <em>content-close</em> link as illustrated in 	  Fig. <a class="fig" href="#fig1">1</a>. It is worthy of noting that a particular link can be both structure-close and content-close since the two nodes can have common neighbors and share similar content at the same time. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Structure-close link between node 1 and node 4 while content-close link between node 4 and node 5. Capital letters in cloud-like box denote node-generated content.</span>     </div>     </figure>    </p>    <p>We further generalize the concept of structure-close links between two nodes that are close in the network structure, e.g., two nodes can be close when they are connected by third-order links or even fourth-order links (second-order links exist between two nodes sharing common neighbors). But we do not set a threshold for the order of links to define the structure-close links because the threshold is not that important in this paper.</p>    <p>The existence of two types of semantic meanings of links suggests that it may not be proper to learn a single representation for each node because they may indicate the existence of two types of node characteristics, e.g., education and interests mentioned above. Education represents the type of characteristics that are intrinsic to nodes while interests represent the type nodes acquire from the particular social environment. The problem with a single representation is determined by the nature of network embedding, i.e., nodes connected by links should be close in the embedding space of interest. Accordingly, two nodes with different types of characteristics may be wrongly presented to be close.</p>    <p>For example, in Fig. <a class="fig" 	 href="#fig1">1</a>, since node 5 is connected to node 4 	  and node 4 is connected to node 1, the representation of node 5 may also be close to node 1 in the embedding space, which may suggest node 5 and node 1 have a potential link. However, if the structure-close link between node 1 and node 4 is established because they share many university alumni while node 5 is not one of the alumni, node 1 and node 5 are not much likely to have a link because they also do not share interests. In this example, we make several assumptions about the links and node-generated content. One may question the assumption that node 1 and node 4 are connected by a structure-close link but do not share content. To validate this assumption, we present density of cosine similarity between tweets of two users connected by a structure-close link whose nodes share neighbors in a Twitter network described in Section 6 and corresponding cumulative density in Fig. <a class="fig" href="#fig2">2</a>. We can see there is a large portion of links whose nodes share little or even no common content. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Similarity density plot and cumulative density plot</span>     </div>     </figure>    </p>    <p>In this paper, we thus propose to learn two representations for each node and render each representation responsible for encoding a particular type of node characteristics. Since content-close links mainly result from sharing similar interests, we name node characteristics exposed in these links as interests, and the corresponding representation as interest representation. For structure-close links resulting from sharing neighbors, we may think each node has an idea of whom the other node is by referring to its neighbors. Hence, we name the other type of node characteristics as identity and the corresponding representation as identity representation. The proposed method is thus referred to as <span style="text-decoration: underline;">i</span>nterest and <span style="text-decoration: underline;">i</span>dentity <span style="text-decoration: underline;">r</span>epresentation <span style="text-decoration: underline;">l</span>earning (IIRL).</p>    <p>With the two representations, the problem faced by learning a single representation mentioned above can be solved as follows: node 1 and 4 are presented to be close in the embedding space corresponding to identity representations while node 4 and 5 are close in the space for interest representations. As a result, node 1 may not be close to node 5 because they are far away from each other in both of the spaces. Moreover, with these two salient semantic meanings, the two representations can make natural data mining tasks, e.g., link prediction and classification, more interpretable than single representations without semantic meanings [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>].</p>    <p>The idea of learning two representations imposes two major challenges. Firstly, unlike learning a single representation that preserves both interest-related and identity-related characteristics, IIRL has to make sure that these two types of characteristics are preserved where they should be. But each link can be both structure-close and content-close. Hence, the challenge is how to infer the type of node characteristics exposed in each link so that the network structure and semantic meanings of links are well preserved in node representations. Secondly, besides the network structure, we also need to encode node-generated content into interest representations.</p>    <p>To address the first challenge, we infer the responsibility weight for each type of node characteristics given a particular link. The responsibility weights are estimated because characteristics in a particular link can be an arbitrary mixture of interest-related characteristics and identity-related ones. For the second challenge, we use node content as guidance while learning the interest representation as the content has the ground truth about node interests.</p>    <p>The contributions of the paper are summarized as follows:</p>    <ul class="list-no-style">     <li id="uid4" label="1.">To our best knowledge, this is the first work to explore semantic meanings of social links in network embedding.<br/></li>     <li id="uid5" label="2.">We propose IIRL to learn two representations for each node by embedding the network structure and node-generated content where the network structure is gracefully embedded via joint link type inference.<br/></li>     <li id="uid6" label="3.">We shed light on understanding social behaviors of individual users and an entire social network as a by-product of embedding social networks.<br/></li>     <li id="uid7" label="4.">Via comprehensive evaluation on four social networks, we show data mining models built on the two representations are more accurate as well as more interpretable.<br/></li>    </ul>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related work</h2>     </div>    </header>    <p>The development of recent network embedding starts with DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>], which employs Skip-gram [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], a language model, to present pairs of nodes reached in truncated random walks to be close in the embedding space. With the success of DeepWalk, there emerge a series of Skip-gram based methods, such as TADW [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>] to embed both network structure and node attributes, and node2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] to explore diverse neighborhoods of each node.</p>    <p>There are also many other kinds of methods. LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] is proposed to embed large-scale networks by directly presenting pairs of nodes with first-order or second order connections to be close. GraRep [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] models first-order up to a pre-defined k-order proximities into transition matrices, and then factorizes each transition matrix using SVD. A recent method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] concludes that modeling high-order proximities can improve the quality of node representations, and then makes improvement to aforementioned models. Besides simply preserving the network structure, some methods also preserve network properties, such as HOPE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] preserving asymmetric transitivities and M-NMF [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>] preserving communities. Some methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>] even embed heterogeneous networks including more than one types of nodes and edges. Deep learning models have also been applied for network embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>]. Most of the methods above are unsupervised learning methods. Semi-supervised methods [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] have also been studied since representations with label information can perform better in subsequent classification tasks.</p>    <p>However, none of the methods mentioned above consider semantic meanings of links in a social network.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Preliminaries</h2>     </div>    </header>    <p>     <div class="definition" id="enc1">     <Label>DEFINITION 1.</Label>     <p>      <em>A</em>      <strong>social network with node-generated content</strong>      <em>is denoted as <em>G</em>(<em>N</em>, <em>E</em>, <em>A</em>), where <em>N</em> is a set of nodes, <em>E</em> is a set of weighted or unweighted, directed or undirected edges. The type of node characteristics, i.e., interest-related characteristics and identity-related characteristics, in each edge is unknown. </em>      <span class="inline-equation"><span class="tex">$A \in \mathbb {R}^{M\times L}$</span>      </span>      <em> is a term frequency matrix of attributes extracted from the content where <em>M</em> is the number of nodes and <em>L</em> is the number of attributes. It is assumed that each node content is non-empty.</em>     </p>     </div>    </p>    <p>As a network embedding model, IIRL learns node representations through embedding first-order linkage information and non-linkage information of <em>G</em>(<em>N</em>, <em>E</em>, <em>A</em>) in an Euclidean space. More specifically, each pair of nodes connected by an edge is presented to be close while each pair not connected is presented apart. Since there are two representations for each node, i.e., identity representation and interest representation, there are two corresponding spaces. The closeness of two nodes in the identity space is quantified as follows: <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">An illustration of embedding a toy network by IIRL and the expected embedding results. It is assumed that links among node 1, 2, 3, and 4 are structure-close links while links among node 4, 5 and 6 are content-close links. Moreover, the link between node 1 and 3 is also a content-close link. The middle box illustrates the application of IIRL on edge (5,6) and node 5 with its content. Circles in blue denote identity representations while circles in green denote interest representations. <em>a</em>       <sub>5</sub> is node attribute vector of node 5, and <span class="inline-equation"><span class="tex">$||\cdot ||^2_{2}$</span>       </span> is &#x2113;-2 norm.</span>     </div>     </figure>    </p>    <p>     <div class="definition" id="enc2">     <Label>DEFINITION 2.</Label>     <p>      <em>The</em>      <strong>closeness</strong>      <em>of two nodes in the identity Euclidean space is quantified as the probability of a structure-close edge between them, where the probability is defined as follows:</em>      <div class="table-responsive" id="Xeq1">       <div class="display-equation">        <span class="tex mytex">\begin{equation} p(e_{ij_{u}})=\frac{1}{1+\text{exp}\lbrace -u_{i}^\top u_{j}\rbrace }, \end{equation} </span>        <br/>        <span class="equation-number">(1)</span>       </div>      </div>      <em>where </em>      <span class="inline-equation"><span class="tex">$e_{ij_{u}}$</span>      </span>      <em> is a structure-close edge, </em>      <span class="inline-equation"><span class="tex">$u_{i} \in \mathbb {R}^{D_{u}}$</span>      </span>      <em> and </em>      <span class="inline-equation"><span class="tex">$u_{j}\in \mathbb {R}^{D_{u}}$</span>      </span>      <em> are column vectors of identity representations for nodes <em>i</em> and <em>j</em>, respectively, and <em>D<sub>u</sub>       </em> is the dimension of the identity Euclidean space. In the rest of the paper, the closeness is referred to as identity closeness.</em>     </p>     </div>    </p>    <p>Eq. (<a class="eqn" href="#Xeq1">1</a>) is symmetric in terms of node representations. For directed edges, the asymmetric property is not considered as LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] and EOE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] do. We may consider it in future work. Interest closeness is similarly defined. The closenesses can be quantified in this way because larger probabilities indicate larger inner products of two vectors, and the inner product is a measurement of closeness of two points in Euclidean space. Moreover, the closeness quantified in this way is to facilitate the formulation of an optimization objective presented in the following section.</p>    <p>Although the definition of closeness is similar to that of LINE(1st) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] and EOE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>], IIRL further assigns a responsibility weight to the type of node characteristics in each edge, which is reflected on the overall closeness. The overall closeness of two nodes is summarized over weighted identity closeness and weighted interest closeness as follows: <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} p(e_{ij})=\pi _{ij_{u}}p(e_{ij_{u}})+\pi _{ij_{v}}p(e_{ij_{v}}), \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$p(e_{ij_{v}})$</span>     </span> quantifies interest closeness, <span class="inline-equation"><span class="tex">$e_{ij_{v}}$</span>     </span> is a content-close edge, and <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span> are defined as follows:</p>    <p>     <div class="definition" id="enc3">     <Label>DEFINITION 3.</Label>     <p>      <strong>Identity proportion</strong>      <em>of a linkage relationship denoted as </em>      <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>      </span>      <em> quantifies the responsibility weight of identity-related characteristics in the link between node <em>i</em> and node <em>j</em>. Similarly,</em>      <strong>interest proportion</strong>      <em>of a linkage relationship denoted as </em>      <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>      </span>      <em> quantifies the responsibility weight of interest-related characteristics. As there are only two types of node characteristics as indicated in the introduction, </em>      <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}+\pi _{ij_{v}}=1$</span>      </span>      <em>.</em>     </p>     </div>    </p>    <p>According to Eq. (<a class="eqn" href="#Xeq2">2</a>), to correctly estimate the responsibility weights of the linkage relationship between two nodes, interest proportion is expected to be larger than identity proportion when the probability estimated by interest representations is larger than that estimated by identity representations, and vice versa. Otherwise, the weighted sum of the probability is less optimal. This expectation is consistent with our intuition that the stronger one out of two characteristics is more likely to dominate the relationship of two persons, e.g., the same attitude to Golden State Warriors or Cleveland Cavaliers may be more likely than the same born state, even the same university of two US citizens to dominate their relationships. And this intuition sheds light on how IIRL infers the link type.</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> IIRL Model</h2>     </div>    </header>    <p>To obtain optimal node representations and link types for embedding the network structure, the optimization objective can be formulated according to maximum likelihood estimation since closeness is quantified by a concept of probability. Hence, the network structure embedding is formally formulated as the following problem: <div class="table-responsive" id="Xeq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026;\underset{U,V, \pi }{\text{max}} \prod _{e_{ij}\in E} p(e_{ij}) \prod _{e_{hk}\notin E}(1-p(e_{hk}))\\ &#x0026;s.t.\, \pi _{ij_{u}}+\pi _{ij_{v}}=1\,\&#x0026;\,\pi _{hk_{u}}+\pi _{hk_{v}}=1, \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$U\in \mathbb {R}^{M\times D_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$V\in \mathbb {R}^{M\times D_{v}}$</span>     </span> is the matrix of identity representations and interest representations, respectively, and <em>&#x03C0;</em> is the collection of <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span>, <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span>, <span class="inline-equation"><span class="tex">$\pi _{hk_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\pi _{hk_{v}}$</span>     </span>. <span class="inline-equation"><span class="tex">$\pi _{hk_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\pi _{hk_{v}}$</span>     </span> are similar to <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span>, <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span> but node <em>h</em> and <em>k</em> are not connected.</p>    <p>By taking negative natural logarithm of Eq. 	  (<a class="eqn" href="#Xeq3">3</a>), the problem is equivalent to the following minimization problem: <div class="table-responsive" id="Xeq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} &#x0026;\underset{U,V, \pi }{\text{min}}- \sum _{e_{ij}\in E}^{} w_{ij}\log p(e_{ij})- \sum _{e_{hk}\notin E}^{} \log (1- p(e_{hk}))\\ &#x0026;s.t.\, \pi _{ij_{u}}+\pi _{ij_{v}}=1\,\&#x0026;\,\pi _{hk_{u}}+\pi _{hk_{v}}=1, \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$w_{ij}\in \mathbb {R}$</span>     </span> is the weight of edge <em>e<sub>ij</sub>     </em> to reflect to link strength, and does not 	  violate the original objective as indicated in Eq. 	  (<a class="eqn" href="#Xeq3">3</a>). The number of <em>e<sub>hk</sub>     </em> can be largely reduced by negative sampling to reduce model complexity as LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] does, and the model performs well with negative sampling as shown in the experiments.</p>    <p>We also illustrate the proposed network structure 	  embedding by a four-layer feedforward neural network 	  model as presented in the middle dashed box of Fig. 	  <a class="fig" href="#fig3">3</a>. The input are node 	  representations. By manipulating the connections 	  between the input layer and the first hidden layer, 	  the first hidden layer realizes dimension-wise 	  multiplication of identity representations of two nodes and multiplication of interest representations. The second hidden layer produces identity closeness and interest closeness by employing sigmoid function as the activation function. The output layer produces the overall closeness by summarizing weighted closenesses shown in Eq. (<a class="eqn" href="#Xeq2">2</a>).</p>    <p>Since user-generated content explicitly indicates user&#x0027;s interests, the interest representation of each node should comply with its content. To achieve this purpose, we regularize interest representations to node content through the following minimization: <div class="table-responsive" id="Xeq5">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \underset{V,P}{\text{min}} ||VP-A||^2_{F}, \end{equation} </span>      <br/>      <span class="equation-number">(5)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$P\in \mathbb {R}^{D_{v}\times L}$</span>     </span> is a projection matrix, and <span class="inline-equation"><span class="tex">$||\cdot ||^2_{F}$</span>     </span> is Frobenius norm. The intuition behind the equation is that the user interests should be well represented by the interest representation <em>V</em> through the projection matrix <em>P</em>. As the first work to explore semantic meanings of social links, the paper assumes the content is non-trivial. Hence, attribute selection is not performed.</p>    <p>By directly combining Eq. (<a class="eqn" 	 href="#Xeq4">4</a>) for structure embedding and Eq. 	  (<a class="eqn" href="#Xeq5">5</a>) for content embedding, the overall loss can be drawn as follows: <div class="table-responsive" id="Xeq6">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} \mathcal {L}(U, V, P, \pi)=&#x0026;- \sum _{e_{ij}\in E}^{} w_{ij}\log p(e_{ij})- \sum _{e_{hk}\notin E}^{} \log (1- p(e_{hk}))\\ &#x0026;+||VP-A||^2_{F}+\lambda ||U||^2_{F}+\lambda ||V||^2_{F}+\lambda ||P||^2_{F}\\ &#x0026;s.t.\, \pi _{ij_{u}}+\pi _{ij_{v}}=1\,\&#x0026;\,\pi _{hk_{u}}+\pi _{hk_{v}}=1, \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(6)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$||U||^2_{F}$</span>     </span>, <span class="inline-equation"><span class="tex">$||V||^2_{F}$</span>     </span>, and <span class="inline-equation"><span class="tex">$||P||^2_{F}$</span>     </span> are regularization terms, and <span class="inline-equation"><span class="tex">$\lambda \in \mathbb {R}$</span>     </span> is a regularization coefficient.</p>    <p>Since larger probabilities of edges indicate larger 	  inner products of node representations suggested by 	  Eq. (<a class="eqn" href="#Xeq1">1</a>), nodes connected by edges are expected to be close in the embedding space. More specifically, nodes connected by structure-close edges should be close in the identity space such as node 1, 2, 3, and 4 in Fig. <a class="fig" href="#fig3">3</a> while nodes connected by content-close edges should be close in the interest space such as node 4 and 5, node 5 and 6. Moreover, node 1 and 3 are close in both identity space and interest space. In this sense, the network structure is well preserved in node representations.</p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> The Optimization Algorithm</h2>     </div>    </header>    <p>     <span class="inline-equation"><span class="tex">$\mathcal {L}(U, V, P, \pi)$</span>     </span> is not jointly convex over the four input variables. Hence, we replace it with a sequence of easier optimizations by an alternating optimization algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. More specifically, the minimization problem can be alternatingly solved with respect to one of the four variables at a time with other variables fixed. We then solve each variable according to the corresponding problem.</p>    <p>     <strong>Problem (1):</strong> The optimization problem for <em>U</em> or <em>V</em> can be solved by gradient-based algorithms, e.g., gradient descent and L-BFGS. The derivative w.r.t <em>U</em> can be obtained as follows: <div class="table-responsive" id="Xeq7">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} \frac{\partial \mathcal {L}(U,V,P, \pi)}{\partial u_{i}}=&#x0026;-\sum _{e_{ij}\in E}^{}\frac{w_{ij}\pi _{ij_{u}}\text{exp}\lbrace -u^T_{i}u_{j}\rbrace u_{j}}{p(e_{ij})(1+\text{exp}\lbrace -u^T_{i}u_{j}\rbrace)^2}+2\lambda u_{i}\\ &#x0026;-\sum _{e_{ik}\notin E}^{}\frac{\pi _{ik_{u}}\text{exp}\lbrace -u^T_{i}u_{k}\rbrace u_{k}}{(p(e_{ik})-1)(1+\text{exp}\lbrace -u^T_{i}u_{k}\rbrace)^2}, \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(7)</span>     </div>     </div> where <em>e<sub>ik</sub>     </em> is the same concept as <em>e<sub>hk</sub>     </em>.</p>    <p>     <strong>Problem (2):</strong> Similarly, the derivative w.r.t <em>V</em> is as follows: <div class="table-responsive" id="Xeq8">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \begin{aligned} \frac{\partial \mathcal {L}(U,V,P, \pi)}{\partial v_{i}}=&#x0026;-\sum _{e_{ij}\in E}^{}\frac{w_{ij}\pi _{ij_{v}}\text{exp}\lbrace -v^T_{i}v_{j}\rbrace v_{j}}{p(e_{ij})(1+\text{exp}\lbrace -v^T_{i}v_{j}\rbrace)^2}\\ &#x0026;-\sum _{e_{ik}\notin E}^{}\frac{\pi _{ik_{v}}\text{exp}\lbrace -v^T_{i}v_{k}\rbrace v_{k}}{(p(e_{ik})-1)(1+\text{exp}\lbrace -v^T_{i}v_{k}\rbrace)^2} \\ &#x0026;+2(v^T_{i}P-a^T_{i})P^T+2\lambda v_{i} \end{aligned} \end{equation} </span>      <br/>      <span class="equation-number">(8)</span>     </div>     </div>    </p>    <p>     <strong>Problem (3):</strong> With respect to <em>P</em>, the optimization objective turns into solving the following problem: <div class="table-responsive" id="Xeq9">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \underset{P}{\text{min}} ||VP-A||^2_{F}+\lambda ||P||^2_{F}, \end{equation} </span>      <br/>      <span class="equation-number">(9)</span>     </div>     </div> Hence, the optimal <em>P</em> can be easily 	  obtained by setting the derivative of Eq. 	  (<a class="eqn" href="#Xeq9">9</a>) with respect to <em>P</em> to zero, where the derivative is quantified as the following formulation: <div class="table-responsive" id="Xeq10">     <div class="display-equation">      <span class="tex mytex">\begin{equation} 2V^TV P-2V^T A+2\lambda P. \end{equation} </span>      <br/>      <span class="equation-number">(10)</span>     </div>     </div> By setting Eq. (<a class="eqn" 	  href="#Xeq10">10</a>) to zero, the optimal <em>P</em> is quantified by the following equation: <div class="table-responsive" id="Xeq11">     <div class="display-equation">      <span class="tex mytex">\begin{equation} P=(V^TV+\lambda I)^{-1}V^T A, \end{equation} </span>      <br/>      <span class="equation-number">(11)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$I \in \mathbb {R}^{L\times L}$</span>     </span> is an identity matrix.</p>    <p>     <strong>Problem (4):</strong> W.r.t. <em>&#x03C0;</em>, rather than solving it directly, it is easier to optimize unconstrained &#x201D;softmax weights&#x201D; defined as follows: <div class="table-responsive" id="Xeq12">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \pi _{ij_{u}}=\frac{\text{exp}\lbrace \xi _{ij_{u}}\rbrace }{\text{exp}\lbrace \xi _{ij_{u}}\rbrace +\text{exp}\lbrace \xi _{ij_{v}}\rbrace }, \end{equation} </span>      <br/>      <span class="equation-number">(12)</span>     </div>     </div> where <span class="inline-equation"><span class="tex">$\xi _{ij_{u}}\in \mathbb {R}$</span>     </span> and <span class="inline-equation"><span class="tex">$\xi _{ij_{v}}\in \mathbb {R}$</span>     </span>. As a result, the problem turns into solving <span class="inline-equation"><span class="tex">$\xi _{ij_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\xi _{ij_{v}}$</span>     </span>, which can be solved by gradient descent.</p>    <p>Hence, the derivative w.r.t <span class="inline-equation"><span class="tex">$\xi _{ij_{u}}$</span>     </span> is expressed as follows: <div class="table-responsive" id="Xeq13">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \frac{\partial \mathcal {L}(U,V,P,\pi)}{\partial \xi _{ij_{u}}}=\frac{w_{ij}\left[p(v_{i},v_{j})-p(u_{i},u_{j})\right]}{p(e_{ij})\left[\text{exp}\lbrace \xi _{ij_{u}}-\xi _{ij_{v}}\rbrace +2+\text{exp}\lbrace \xi _{ij_{v}}-\xi _{ij_{u}}\rbrace \right]}, \end{equation} </span>      <br/>      <span class="equation-number">(13)</span>     </div>     </div> and <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {L}(U,V,P,\pi)}{\partial \xi _{ij_{v}}}$</span>     </span> is similar to <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {L}(U,V,P,\pi)}{\partial \xi _{ij_{u}}}$</span>     </span> except that the nominator is <em>w<sub>ij</sub>     </em>[<em>p</em>(<em>u<sub>i</sub>     </em>, <em>u<sub>j</sub>     </em>) &#x2212; <em>p</em>(<em>v<sub>i</sub>     </em>, <em>v<sub>j</sub>     </em>)].</p>    <p>Similarly, the derivatives w.r.t <span class="inline-equation"><span class="tex">$\xi _{hk_{u}}$</span>     </span> and w.r.t <span class="inline-equation"><span class="tex">$\xi _{hk_{v}}$</span>     </span> are expressed as follows: <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {L}(U,V,P,\pi)}{\partial \xi _{hk_{u}}}=$</span>     </span>     <div class="table-responsive" id="Xeq14">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \frac{p(v_{h},v_{k})-p(u_{h},u_{k})}{(p(e_{hk})-1)\left[\text{exp}\lbrace \xi _{hk_{u}}-\xi _{hk_{v}}\rbrace +2+\text{exp}\lbrace \xi _{hk_{v}}-\xi _{hk_{u}}\rbrace \right]}, \end{equation} </span>      <br/>      <span class="equation-number">(14)</span>     </div>     </div> and <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {L}(U,V,P,\pi)}{\partial \xi _{hk_{v}}}$</span>     </span> is similar to <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {L}(U,V,P,\pi)}{\partial \xi _{hk_{u}}}$</span>     </span> except that the nominator is <em>p</em>(<em>u<sub>h</sub>     </em>, <em>u<sub>k</sub>     </em>) &#x2212; <em>p</em>(<em>v<sub>h</sub>     </em>, <em>v<sub>k</sub>     </em>).</p>    <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>    <p>The workflow of jointly solving these four variables 	  is presented in Algorithm 1. Algorithm 1 starts with pre-training <em>U</em> and <em>V</em>, which is performed to obtain good initialization values for <em>U</em> and <em>V</em> as introduced below, and the parameter <em>k</em> is used in the pre-training. The parameter negative ratio is the ratio of the number of <em>e<sub>hk</sub>     </em> to that of <em>e<sub>ij</sub>     </em> as used in negative sampling by LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. It is observed that Algorithm 1 works well with this parameter in the experiments. For all gradient descents, the learning rate in each iteration is obtained by backtracking line search [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>].</p>    <p>     <strong>Pre-training</strong> is important as it can initialize a model to a point in parameter space that renders the learning process more effective [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. In our case, to make the learning process more effective, <em>U</em> should be pre-trained to take values reflecting identity-related information as expected, and <em>V</em> should take values reflecting interest-related information. As identity-related information may only be recovered from the network structure given <em>G</em>(<em>N</em>, <em>E</em>, <em>A</em>), we thus pre-train <em>U</em> by embedding the network structure without considering the link type. Based on the proposed network structure embedding, the loss function for pre-training <em>U</em> can be quantified as follows: <em>L</em>(<em>U</em>) = <div class="table-responsive" id="Xeq15">     <div class="display-equation">      <span class="tex mytex">\begin{equation} -\sum _{e_{ij}\in E}^{}w_{ij} \log (p(u_{i},u_{j}))-\sum _{e_{hk}\notin E}^{}\log (1-p(u_{h},u_{k}))+\lambda ||U||^2_{F}, \end{equation} </span>      <br/>      <span class="equation-number">(15)</span>     </div>     </div> where <em>p</em>(<em>u<sub>i</sub>     </em>, <em>u<sub>j</sub>     </em>) and <em>p</em>(<em>u<sub>h</sub>     </em>, <em>u<sub>k</sub>     </em>) are quantified in the same way as Eq. 	  (<a class="eqn" href="#Xeq1">1</a>). We can solve <em>L</em>(<em>U</em>) by gradient descent, and the derivative is obtained as follows: <span class="inline-equation"><span class="tex">$\frac{\partial L(U)}{\partial u_{i}}=$</span>     </span>     <div class="table-responsive" id="Xeq16">     <div class="display-equation">      <span class="tex mytex">\begin{equation} -\sum _{e_{ij}\in E}^{}\frac{w_{ij}\text{exp}\lbrace -u_{i}^\top u_{j}\rbrace }{1+\text{exp}\lbrace -u_{i}^\top u_{j}\rbrace }\times u_{j} +\sum _{e_{hk}\notin E}^{}\frac{u_{k}}{1+\text{exp}\lbrace -u_{i}^\top u_{k}\rbrace } +2\lambda (u_{i}), \end{equation} </span>      <br/>      <span class="equation-number">(16)</span>     </div>     </div>    </p>    <p>As for <em>V</em>, because we may not know which links are content-close, we thus pre-train <em>V</em> by embedding node content. To make this problem simple, we construct a k-nearest neighbor graph of the nodes so that the method for pre-training <em>U</em> can be directly applied to pre-train <em>V</em>. The links in the kNN graph are established from each node to its first-k neighbors with a weight of 1. The similarity of two nodes is quantified by cosine similarity of their content.</p>    <p>     <strong>Complexity</strong>: Referring to the derivatives, the complexity of Algorithm 1 is dominated by learning node representations from the network structure and the term frequency matrix constructed from node content. Taking the identity representations as an example, the complexity of learning from the network structure is <em>O</em>(<em>D<sub>u</sub>     </em>|<em>E</em>|) due to negative sampling and the complexity of learning from the term frequency matrix is <em>O</em>(<em>D<sub>u</sub>     </em>|<em>N</em>|<em>L</em>) where <em>L</em> is the number of terms. Since social networks are usually sparse, the scalability to large-scale networks can be guaranteed.</p>    <p>     <strong>Convergence</strong>: Algorithm 1 is essentially a block-wise coordinate descent algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] with <em>U</em>, <em>V</em>, <em>&#x03C0;</em> and <em>P</em> as block variables. So convergence can be guaranteed based on the general proof of convergence for block-wise coordinate descent. Moreover, in the experiments, we observe Algorithm 1 converges fast in terms of the outer iterations.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Empirical Evaluation</h2>     </div>    </header>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Datasets</h3>     </div>     </header>     <p>Four social networks studied in the experiments are as follows:</p>     <ul class="list-no-style">     <li id="list1" label="&#x2022;">DBLP [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0026">26</a>]: We sample a co-authorship network of authors who published as least 2 papers during the period from 2000 to 2009 from selected conferences, which are KDD, ICDM, SDM, and PAKDD in DM field, AAAI, ICML, NIPS, IJCAI, CVPR, and ECML in ML field, SIGMOD, VLDB, ICDE, PODS, and EDBT in DB field, WWW, SIGIR, CIKM, WSDM, and ECIR in IR field. Words of paper abstract are assigned as the attributes of each author. Stop words and words with frequency less than 6 are omitted.<br/></li>     <li id="list2" label="&#x2022;">Twitter [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>]: The dataset has directed links among users, and words from tweets demonstrating user&#x0027;s interests. We sample users who have attributes no less than 20 and have followed no less than 6 users, and sample the words with a minimal frequency of 8.<br/></li>     <li id="list3" label="&#x2022;">BlogCatalog [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0032">32</a>]: The links among users are undirected friendships. We aggregate tags on blogs as users&#x2019; attributes. We sample users who published as least one blog belonging to one of 15 popular categories, which are Art, Computers, Music, Photography, Travel, Sports, Technology, Internet, Humor, Business, Writing, Culture, Health, Finance, and Film. The threshold of attributes for each user is set as 6, and the threshold of the number of friends is 3. Attributes with frequency less than 3 are omitted.<br/></li>     <li id="list4" label="&#x2022;">Flickr[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0031">31</a>]: The links between users are friendships, and the attributes for each user are aggregated tags on their photos. We sample users from five popular groups which are 7, 36, 77, 119, and 148. We sample users with minimal 30 attributes and with minimal 10 friends, and set the threshold of frequency for each feature as 25.<br/></li>     </ul>     <p>Network statistics are summarized in Table 		 <a class="tab" href="#tab1">1</a>.</p>     <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Network statistics.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:left;">Network</th>        <th style="text-align:left;">DBLP</th>        <th style="text-align:left;">Twitter</th>        <th style="text-align:left;">BlogCatalog</th>        <th style="text-align:left;">Flickr</th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:left;">#Nodes</td>        <td style="text-align:left;">6482</td>        <td style="text-align:left;">9244</td>        <td style="text-align:left;">7857</td>        <td style="text-align:left;">6318</td>       </tr>       <tr>        <td style="text-align:left;">#Edges</td>        <td style="text-align:left;">19265</td>        <td style="text-align:left;">323922</td>        <td style="text-align:left;">137649</td>        <td style="text-align:left;">404085</td>       </tr>       <tr>        <td style="text-align:left;">#Attributes</td>        <td style="text-align:left;">8298</td>        <td style="text-align:left;">10360</td>        <td style="text-align:left;">5351</td>        <td style="text-align:left;">8523</td>       </tr>       <tr>        <td style="text-align:left;">#Groups</td>        <td style="text-align:left;">4</td>        <td style="text-align:left;">N/A</td>        <td style="text-align:left;">15</td>        <td style="text-align:left;">5</td>       </tr>       <tr>        <td style="text-align:left;">Type</td>        <td style="text-align:left;">weighted undirected</td>        <td style="text-align:left;">unweighted directed</td>        <td style="text-align:left;">unweighted undirected</td>        <td style="text-align:left;">unweighted undirected</td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Experiment Settings</h3>     </div>     </header>     <p>Five recent network embedding models are employed as baselines, which are DeepWalk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], LINE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>], and node2vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] for embedding only the network structure, TADW [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] and EOE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>] for embedding both the network structure and node content.</p>     <p>In the experiments, the dimension of both identity representation and interest representation is set as 128 as used in all the baselines. Because there are two representations for each node in both IIRL and TADW, the two representations are concatenated as one representation in the evaluation except for the representations of IIRL used in link prediction. Though the dimension of node representation of both IIRL and TADW is two times of that of other baselines, a larger dimension actually does not bring advantages as suggested in the parameter sensitivity of this paper, of LINE, of TADW, and of EOE. Other settings are that <span class="inline-equation"><span class="tex">$\xi _{ij_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\xi _{ij_{v}}$</span>     </span> are initialized as 0.5, <em>k</em> for pre-training is set as top 1% of all the nodes, negative ratio is set as 5 as used in LINE, the regularization coefficient is set as 1, commonly used settings are used in backtracking line search, and the relative loss that determines the convergence of Algorithm 1 is set as 0.001.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Representation Visualization</h3>     </div>     </header>     <p>This section presents a visual evaluation of the 		 effectiveness of the identity representations and 		 interest embeddings by visualizing them in a two-dimension space. Firstly, we present locations of 11 nodes from the Twitter network in identity space and interest space in Fig. <a class="fig" href="#fig4">4</a>, which is obtained by t-SNE [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>]. The true user IDs 		 in Twitter for these 11 nodes are listed in the 		 following Table <a class="tab" href="#tab4">4</a>. The relationships between them 		 are that 1&#x0026;2, 3&#x0026;4, and 5&#x0026;6 		 have many common friends, but have shown little or 		 no similar interests. This may suggest that links 		 between them may be purely structure-close. 		 6&#x0026;7, 8&#x0026;9, and 10&#x0026;11 show 		 similar interests but no or one common neighbor, which may suggest the interest proportion of these links is larger than identity proportion. Moreover, node 5, 6 and 7 is a real example of the motivation scenario in Fig. <a class="fig" href="#fig1">1</a> in the introduction. As suggested by the model illustration in Fig. <a class="fig" href="#fig3">3</a>, 1&#x0026;2, 3&#x0026;4 and 5&#x0026;6 should be close in the identity space and far away in the interest space while 6&#x0026;7, 8&#x0026;9, and 10&#x0026;11 should have the exactly opposite pattern. Moreover, node 7 should be far away from node 5 in both spaces even though they share node 6 as a common neighbor. By examining Fig. <a class="fig" href="#fig4">4</a>, we find the representations work as expected. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Locations of Twitter nodes in different spaces.</span>      </div>     </figure>     <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Visualization of representations learned from the DBLP co-authorship network. Points of different colors denote authors from different research fields, where red color represents Data Mining, green color represents Database, light blue represents Information Retrieval, and dark blue represents Machine Learning. The filed of each author is determined as the one where he/she published the most papers.</span>      </div>     </figure>     </p>     <p>Secondly, we visualize representations in network 		 scale. The DBLP co-authorship network is studied as 		 it is a well-structured professional network. The 		 results are presented in Fig.<a class="fig" 		 href="#fig5">5</a>.</p>     <p>Co-authorships tend to take place within each 		 research field, but the selected four fields, i.e., 		 DB, DM, ML, and IR, are closely related. In other 		 words, many co-authorships may be cross-filed, 		 which implies that the co-authorships alone may not 		 be sufficient to distinguish researchers from 		 different fields. Hence, representations learned 		 from the network structure may not be able to 		 visually distinguish authors from one field to 		 another. And this is indeed demonstrated by all the 		 models that can only embed the network structure. 		 But TADW, EOE, and the proposed IIRL work much 		 better by combining information of the network 		 structure and of the node content as illustrated in 		 Fig. <a class="fig" href="#fig5">5</a>(5), Fig. 		 <a class="fig" href="#fig5">5</a>(6), and Fig. 		 <a class="fig" href="#fig5">5</a>(7), respectively. 		 This is because node content demonstrates research 		 interests, which are different among these four 		 fields as illustrated in Fig. <a class="fig" 		 href="#fig5">5</a>(8) and Fig. <a class="fig" 		 href="#fig5">5</a>(9).</p>     <p>The proposed IIRL and TADW perform a slightly 		 better than EOE, which may because research 		 interests are directly combined with 		 representations learned from the network structure 		 in IIRL and TADW while EOE can only incorporate 		 interests in an implicit way. Though it is not easy 		 to visually tell which one of IIRL and TADW 		 performs better, the way of IIRL to combine the 		 network structure and node content is more 		 reasonable. TADW performs the combination by 		 approximating the entire network structure through node content. Analogously, IIRL performs the approximation selectively, which is motivated by the fact that different edges have different proportions of interest characteristics and identity characteristics. IIRL performs the selection through inferring responsibility weights so that node characteristics can be embedded in appropriate representations other than are blindly embedded in identity representations or interest ones. The effectiveness of this selective embedding is demonstrated by the observation that final interest representations in Fig. <a class="fig" href="#fig5">5</a>(9) perform better than pre-trained ones in Fig. <a class="fig" href="#fig5">5</a>(8). The performance of how correctly IIRL infers responsibility weights is evaluated via link type inference below.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.4</span> Link Type Inference</h3>     </div>     </header>     <p>As there is no ground truth about the identity 		 proportion and interest proportion of each edge 		 given <em>G</em>(<em>N</em>, <em>E</em>, 		 <em>A</em>), we define four types of links in terms 		 of common neighbors and the relationship in 		 interests in Table <a class="tab" 		 href="#tab2">2</a>. The neighbor relationship is 		 defined in the pre-training section. Type 		 <em>I</em> corresponds to purely structure-close 		 social links, Type <em>II</em> corresponds to purely content-close social links, and Type <em>III</em> corresponds to both structure-close and content-close links while Type <em>IV</em> remains uncertain as there is no clear clue about the type of node characteristics. Bottom of Table <a class="tab" href="#tab1">1</a> lists the statistics of four types of links of the Twitter network.</p>     <p>It is worthy of noting that the distribution of Twitter links over the four types depends on the choice of the number <em>k</em> in the k-nearest neighbor network. The number <em>k</em> determines the number of links composed of nodes with neighbor relationship in interests. Here, the <em>k</em> is set as 1% of nodes, a commonly used number for constructing kNN networks in network embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>]. As a result, the 		 number of neighbor links is 109320, taking up 33.7% 		 of all the links. The number is reasonable as there 		 more than 60% of links whose endpoint vertices 		 share few interests as indicated by small 		 similarities (e.g., &#x2264; 0.2) in the cumulative density plot of Fig. <a class="fig" href="#fig2">2</a>.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Four types of social links in terms of the common neighbors and the relationship in interests.</span>     </div>     <table class="table"> 			<thead>       <tr>        <th style="text-align:right;"/>        <th style="text-align:center;">Type I</th>        <th style="text-align:center;">Type II</th>        <th style="text-align:center;">Type III</th>        <th style="text-align:center;">Type IV</th>       </tr> 			 </thead>      <tbody>       <tr>        <td style="text-align:left;">Sharing neighbors</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"> &#x00D7;</td>       </tr>       <tr>        <td style="text-align:left;">Not sharing neighbors</td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>       </tr>       <tr>        <td style="text-align:left;">Neighbor in interests</td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;"> &#x00D7;</td>       </tr>       <tr>        <td style="text-align:left;">Non-neighbor in interests</td>        <td style="text-align:center; border-bottom: solid 2px">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center; border-bottom: solid 2px"> &#x00D7;</td>        <td style="text-align:center; border-bottom: solid 2px"> &#x00D7;</td>        <td style="text-align:center; border-bottom: solid 2px">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>       </tr>       <tr>        <td style="text-align:center;">Twitter</td>        <td style="text-align:center;">208598</td>        <td style="text-align:center;">845</td>        <td style="text-align:center;">108475</td>        <td style="text-align:center;">6014</td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Confusion matrix for link type inference.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:right;"/>        <th style="text-align:center;">Predicted Type <em>I</em>        </th>        <th style="text-align:center;">Predicted Type <em>II</em>        </th>        <th style="text-align:center;">Recall 			  Rate</th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:center;">True Type <em>I</em>        </td>        <td style="text-align:center;">181751</td>        <td style="text-align:center;">26847</td>        <td style="text-align:center;">87.13%</td>       </tr>       <tr>        <td style="text-align:center;"> True Type <em>II</em>        </td>        <td style="text-align:center;">640</td>        <td style="text-align:center;">205</td>        <td style="text-align:center;">75.73%</td>       </tr>       <tr>        <td style="text-align:left;">Precision</td>        <td style="text-align:center;">99.65%</td>        <td style="text-align:center;">99.24%</td>        <td style="text-align:center;"/>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Representatives of Type <em>I</em> and Type <em>II</em> links, where R. N. is acronym of Reference Number used in Section 6.3, #C. F. is acronym of #Common Friends, and N. I. is acronym of Neighbor in Interests.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:left;">Types</th>        <th style="text-align:center;">User ID (R. 			  N.)</th>        <th style="text-align:center;">#C. F.</th>        <th style="text-align	:center;">N. I.</th>        <th style="text-align:center;">Common 			  Attributes</th>        <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>        </span>        </th>        <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>        </span>        </th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">4119741 (1), 32423136 (2)</td>        <td style="text-align:center;">22</td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">@TheAtlantic</td>        <td style="text-align:center;">0.82</td>        <td style="text-align:center;">0.18</td>       </tr>       <tr>        <td style="text-align:left;">Type <em>I</em>        </td>        <td style="text-align:left;">155969606 (3), 65913144 (4)</td>        <td style="text-align:center;">23</td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">N/A</td>        <td style="text-align:center;">0.89</td>        <td style="text-align:center;">0.11</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">618593 (5), 157218534 (6)</td>        <td style="text-align:center;">9</td>        <td style="text-align:center;"> &#x00D7;</td>        <td style="text-align:center;">N/A</td>        <td style="text-align:center;">0.87</td>        <td style="text-align:center;">0.13</td>       </tr>       <tr>        <td style="text-align:left;">Type <em>II</em>        </td>        <td style="text-align:center;">157218534 (6), 387210978 (7)</td>        <td style="text-align:center;">0</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">#StarWars, @darthvader, @DepressedDarth</td>        <td style="text-align:center;">0.15</td>        <td style="text-align:center;">0.85</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">85815410 (8), 23508439 (9)</td>        <td style="text-align:center;">1</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:center;">#Cowboys, #Patriots, #NFLDraft</td>        <td style="text-align:center;">0.12</td>        <td style="text-align:center;">0.88</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:left;">11348282 (10), 77351627 (11)</td>        <td style="text-align:center;">1</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\checkmark$</span>        </span>        </td>        <td style="text-align:left;">#gamedev, @jonjones, @polycount, @GearboxSoftware</td>        <td style="text-align:center;">0.28</td>        <td style="text-align:center;">0.72</td>       </tr>      </tbody>     </table>     </div>     <p>We can quantitatively evaluate how IIRL correctly infers Type <em>I</em> and Type <em>II</em> links in a way that Type <em>I</em> links should have larger values of <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span> while Type <em>II</em> links should have larger values of <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span>. For Type <em>III</em> links, we may not be able to do a quantitative evaluation but we can do a qualitative evaluation based on the difference between <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span>, which should not be too large because there may not be a dominant factor for Type <em>III</em> links.</p>     <p>The confusion matrix on inferring Type <em>I</em> 		 and Type <em>II</em> social links is presented in 		 Table <a class="tab" href="#tab3">3</a>. It shows 		 both Type <em>I</em> and Type <em>II</em> links can 		 be accurately inferred, and the performance on 		 inferring Type <em>I</em> links is better. To 		 verify this performance, three representatives of 		 accurately predicted Type <em>I</em> links and 		 three of Type <em>II</em> links are presented in 		 Table <a class="tab" href="#tab4">4</a>. We can see 		 from Table <a class="tab" href="#tab4">4</a> that all estimated <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span> are consistent with link types. Also, for Type <em>I</em> links, besides there are many common friends shared by the two users, the friendship is bi-directional, which largely supports the claim that two users connected by structure-close links may have an idea of whom each other is in the real world.</p>     <p>To evaluate the performance on inferring Type <em>III</em> links, we present the distribution of absolute differences between <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span> and <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span> and the cumulative density plot in Fig. 		 <a class="fig" href="#fig6">6</a>. The PDF plot shows the majority of differences are small, and the CDF plot shows around 70% of differences are within 0.2. Because it is expected that Type <em>III</em> links may not have a dominant type of characteristics, the identity proportions should take values similar to that of the interest proportions. To this point, it is concluded that the proposed IIRL model can accurately infer link types, and responsibility weights. <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">PDF plot and CDF 			 plot.</span>      </div>     </figure>     </p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.5</span> Social Profiling</h3>     </div>     </header>     <p>After inferring link types, we can build <em>social profiles</em> so as to understand users&#x2019; social behaviors and the social network. The social profile of a user refers to information about his/her friendships with other users. Social profiling is a by-product of learning representations because IIRL is able to infer the type of social links of which all the baselines, in contrast, are not capable.</p>     <p>According to the link types, we categorize users 		 into five groups as illustrated in Table 		 <a class="tab" href="#tab5">5</a>, where <em>only identity-exposing</em> users are those who only have structure-close links, and other terms are defined similarly. It is worthy of noting that structure-close links here are inferred by IIRL during the training process, and they are associated with relatively larger identity proportion <span class="inline-equation"><span class="tex">$\pi _{ij_{u}}$</span>     </span> compared with interest proportion <span class="inline-equation"><span class="tex">$\pi _{ij_{v}}$</span>     </span>. Social profiles in network scale by 		 summarizing over users are also presented in Table 		 <a class="tab" href="#tab5">5</a>. Two interesting observations are drawn as follows:</p>     <ul class="list-no-style">     <li id="list5" label="&#x2022;">From the perspective of links, all networks show that more social links are established between users who may know each other in the real world because structure-close links out-number content-close links. This suggests that the major reason for people to involve in social networks may be to maintain relationships with real-world friends. Also, all the networks except the Twitter network agree with the conclusion from the perspective of users. The reason behind the Twitter network is explored below.<br/></li>     <li id="list6" label="&#x2022;">Uni-directional relationships would encourage connections between people who do not know each other. By referring to the Twitter network, there are more interest-exposing users, and the ratio of content-close links to structure-close links is not that small compared with that in Flickr and BlogCatalog. This is excepted because friendships are easier to be acknowledged by one side than by both sides.<br/></li>     </ul>     <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Social profiling for networks.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;"/>        <th style="text-align:center;">Twitter</th>        <th style="text-align:center;">Flickr</th>        <th style="text-align:center;">Blog</th>        <th style="text-align:center;">DBLP</th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">Only identity-exposing</td>        <td style="text-align:center;">240</td>        <td style="text-align:center;">1359</td>        <td style="text-align:center;">403</td>        <td style="text-align:center;">1378</td>       </tr>       <tr>        <td style="text-align:left;">#Users</td>        <td style="text-align:center;">Only interest-exposing</td>        <td style="text-align:center;">1837</td>        <td style="text-align:center;">1170</td>        <td style="text-align:center;">79</td>        <td style="text-align:center;">1937</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">More Identity-exposing</td>        <td style="text-align:center;">3389</td>        <td style="text-align:center;">3876</td>        <td style="text-align:center;">5120</td>        <td style="text-align:center;">1486</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">More Interest-exposing</td>        <td style="text-align:center;">3595</td>        <td style="text-align:center;">1227</td>        <td style="text-align:center;">590</td>        <td style="text-align:center;">1642</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">Equally exposing</td>        <td style="text-align:center;">116</td>        <td style="text-align:center;">259</td>        <td style="text-align:center;">126</td>        <td style="text-align:center;">39</td>       </tr>       <tr>        <td style="text-align:left;">#Links</td>        <td style="text-align:center;">structure-close</td>        <td style="text-align:center;">218158</td>        <td style="text-align:center;">125220</td>        <td style="text-align:center;">352531</td>        <td style="text-align:center;">21568</td>       </tr>       <tr>        <td style="text-align:left;"/>        <td style="text-align:center;">content-close</td>        <td style="text-align:center;">105764</td>        <td style="text-align:center;">12429</td>        <td style="text-align:center;">53157</td>        <td style="text-align:center;">16962</td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab6">     <div class="table-caption">      <span class="table-number">Table 6:</span>      <span class="table-title">AUC scores of link prediction tasks, where percentage numbers are percentages of links used as training links.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:center;">Models</th>        <th colspan="4" style="text-align:center;">BlogCatalog<hr/>        </th>        <th colspan="4" style="text-align:center;">Flickr<hr/>        </th>        <th colspan="4" style="text-align:center;">Twitter<hr/>        </th>        <th>DBLP</th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">20%</th>        <th style="text-align:center;">40%</th>        <th style="text-align:center;">60%</th>        <th style="text-align:center;">80%</th>        <th style="text-align:center;">20%</th>        <th style="text-align:center;">40%</th>        <th style="text-align:center;">60%</th>        <th style="text-align:center;">80%</th>        <th style="text-align:center;">20%</th>        <th style="text-align:center;">40%</th>        <th style="text-align:center;">60%</th>        <th style="text-align:center;">80%</th>        <th style="text-align:center;">100%</th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:center;">DeepWalk</td>        <td style="text-align:center;">69.92</td>        <td style="text-align:center;">76.44</td>        <td style="text-align:center;">79.32</td>        <td style="text-align:center;">81.32</td>        <td style="text-align:center;">71.79</td>        <td style="text-align:center;">77.04</td>        <td style="text-align:center;">78.30</td>        <td style="text-align:center;">78.69</td>        <td style="text-align:center;">96.25</td>        <td style="text-align:center;">97.62</td>        <td style="text-align:center;">97.68</td>        <td style="text-align:center;">97.80</td>        <td>73.26</td>       </tr>       <tr>        <td style="text-align:center;">TADW</td>        <td style="text-align:center;">67.12</td>        <td style="text-align:center;">70.67</td>        <td style="text-align:center;">73.02</td>        <td style="text-align:center;">74.97</td>        <td style="text-align:center;">62.59</td>        <td style="text-align:center;">65.04</td>        <td style="text-align:center;">68.10</td>        <td style="text-align:center;">70.16</td>        <td style="text-align:center;">94.25</td>        <td style="text-align:center;">95.24</td>        <td style="text-align:center;">95.82</td>        <td style="text-align:center;">96.90</td>        <td>66.26</td>       </tr>       <tr>        <td style="text-align:center;">LINE(1st)</td>        <td style="text-align:center;">53.61</td>        <td style="text-align:center;">73.70</td>        <td style="text-align:center;">65.36</td>        <td style="text-align:center;">79.60</td>        <td style="text-align:center;">61.31</td>        <td style="text-align:center;">66.31</td>        <td style="text-align:center;">69.10</td>        <td style="text-align:center;">70.83</td>        <td style="text-align:center;">92.56</td>        <td style="text-align:center;">95.90</td>        <td style="text-align:center;">97.02</td>        <td style="text-align:center;">97.70</td>        <td>62.45</td>       </tr>       <tr>        <td style="text-align:center;">LINE(2nd)</td>        <td style="text-align:center;">65.77</td>        <td style="text-align:center;">68.25</td>        <td style="text-align:center;">70.47</td>        <td style="text-align:center;">72.05</td>        <td style="text-align:center;">66.25</td>        <td style="text-align:center;">68.02</td>        <td style="text-align:center;">68.66</td>        <td style="text-align:center;">68.79</td>        <td style="text-align:center;">91.04</td>        <td style="text-align:center;">94.84</td>        <td style="text-align:center;">96.12</td>        <td style="text-align:center;">96.63</td>        <td>76.04</td>       </tr>       <tr>        <td style="text-align:center;">node2vec</td>        <td style="text-align:center;">73.69</td>        <td style="text-align:center;">76.99</td>        <td style="text-align:center;">77.75</td>        <td style="text-align:center;">78.54</td>        <td style="text-align:center;">71.20</td>        <td style="text-align:center;">70.80</td>        <td style="text-align:center;">70.71</td>        <td style="text-align:center;">71.75</td>        <td style="text-align:center;">96.12</td>        <td style="text-align:center;">97.64</td>        <td style="text-align:center;">97.68</td>        <td style="text-align:center;">97.68</td>        <td>71.59</td>       </tr>       <tr>        <td style="text-align:center;">EOE</td>        <td style="text-align:center;">78.28</td>        <td style="text-align:center;">83.36</td>        <td style="text-align:center;">86.81</td>        <td style="text-align:center;">88.94</td>        <td style="text-align:center;">74.10</td>        <td style="text-align:center;">77.36</td>        <td style="text-align:center;">78.69</td>        <td style="text-align:center;">79.39</td>        <td style="text-align:center;">96.93</td>        <td style="text-align:center;">97.79</td>        <td style="text-align:center;">98.01</td>        <td style="text-align:center;">98.09</td>        <td>76.19</td>       </tr>       <tr>        <td style="text-align:center;">        <strong>IIRL</strong>        </td>        <td style="text-align:center;">        <strong>82.30</strong>        </td>        <td style="text-align:center;">        <strong>86.54</strong>        </td>        <td style="text-align:center;">        <strong>90.26</strong>        </td>        <td style="text-align:center;">        <strong>91.92</strong>        </td>        <td style="text-align:center;">        <strong>78.62</strong>        </td>        <td style="text-align:center;">        <strong>79.41</strong>        </td>        <td style="text-align:center;">        <strong>82.03</strong>        </td>        <td style="text-align:center;">        <strong>81.29</strong>        </td>        <td style="text-align:center;">        <strong>97.60</strong>        </td>        <td style="text-align:center;">        <strong>98.69</strong>        </td>        <td style="text-align:center;">        <strong>98.37</strong>        </td>        <td style="text-align:center;">        <strong>98.45</strong>        </td>        <td>        <strong>77.21</strong>        </td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab7">     <div class="table-caption">      <span class="table-number">Table 7:</span>      <span class="table-title">Micro-F1 and Macro-F1 			of multi-label classification.</span>     </div>     <table class="table"> 		  <thead>       <tr>        <th style="text-align:left;">Models</th>        <th colspan="3" style="text-align:center;">Micro-F1<hr/>        </th>        <th colspan="3" style="text-align:center;">Macro-F1<hr/>        </th>       </tr>       <tr>        <th style="text-align:left;"/>        <th style="text-align:center;">Blog</th>        <th style="text-align:center;">Flickr</th>        <th style="text-align:center;">DBLP</th>        <th style="text-align:center;">Blog</th>        <th style="text-align:center;">Flickr</th>        <th style="text-align:center;">DBLP</th>       </tr> 			</thead>      <tbody>       <tr>        <td style="text-align:left;">DeepWalk</td>        <td style="text-align:center;">58.42</td>        <td style="text-align:center;">66.52</td>        <td style="text-align:center;">78.48</td>        <td style="text-align:center;">57.86</td>        <td style="text-align:center;">55.71</td>        <td style="text-align:center;">77.46</td>       </tr>       <tr>        <td style="text-align:left;">TADW</td>        <td style="text-align:center;">63.62</td>        <td style="text-align:center;">67.96</td>        <td style="text-align:center;">84.68</td>        <td style="text-align:center;">61.55</td>        <td style="text-align:center;">58.58</td>        <td style="text-align:center;">83.96</td>       </tr>       <tr>        <td style="text-align:left;">LINE(1st)</td>        <td style="text-align:center;">59.55</td>        <td style="text-align:center;">63.95</td>        <td style="text-align:center;">77.62</td>        <td style="text-align:center;">58.72</td>        <td style="text-align:center;">56.92</td>        <td style="text-align:center;">76.51</td>       </tr>       <tr>        <td style="text-align:left;">LINE(2nd)</td>        <td style="text-align:center;">57.60</td>        <td style="text-align:center;">66.07</td>        <td style="text-align:center;">75.93</td>        <td style="text-align:center;">56.95</td>        <td style="text-align:center;">56.55</td>        <td style="text-align:center;">75.45</td>       </tr>       <tr>        <td style="text-align:left;">node2vec</td>        <td style="text-align:center;">55.85</td>        <td style="text-align:center;">57.76</td>        <td style="text-align:center;">76.96</td>        <td style="text-align:center;">55.73</td>        <td style="text-align:center;">48.72</td>        <td style="text-align:center;">75.66</td>       </tr>       <tr>        <td style="text-align:left;">EOE</td>        <td style="text-align:center;">62.95</td>        <td style="text-align:center;">67.86</td>        <td style="text-align:center;">84.56</td>        <td style="text-align:center;">62.03</td>        <td style="text-align:center;">58.62</td>        <td style="text-align:center;">83.32</td>       </tr>       <tr>        <td style="text-align:left;">        <strong>IIRL</strong>        </td>        <td style="text-align:center;">        <strong>66.08</strong>        </td>        <td style="text-align:center;">        <strong>68.79</strong>        </td>        <td style="text-align:center;">        <strong>85.30</strong>        </td>        <td style="text-align:center;">        <strong>63.53</strong>        </td>        <td style="text-align:center;">        <strong>59.36</strong>        </td>        <td style="text-align:center;">        <strong>84.77</strong>        </td>       </tr>      </tbody>     </table>     </div>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.6</span> Link Prediction</h3>     </div>     </header>     <p>Link prediction is usually performed by measuring similarities of pairs of nodes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>]. Inner product of 		 two node representations normalized by sigmoid 		 function is employed as the similarity measurement 		 for all the baselines. For IIRL, the similarity is 		 the summary of identity similarity and interest 		 similarity weighted by identity proportion and interest proportion. On Twitter, BlogCatalog, and Flickr networks, we conduct 4 runs of experiments in which different ratios of links are used as training links and the remaining ones as test links. On the DBLP co-authorship network, new co-authorships occur from 2010 to 2012 are used as test links. For all networks, the same number of negative links are randomly sampled for the evaluation purpose. For IIRL, if a particular pair of nodes does not appear during the training process, the identity proportion and interest proportion are both set as initialized 0.5. The performance measured on AUC score is presented in Table <a class="tab" href="#tab6">6</a>.</p>     <p>Table <a class="tab" href="#tab6">6</a> shows the proposed IIRL model consistently outperforms all baselines on all datasets. The reason behind the superior performance of IIRL over DeepWalk, LINE and node2vec is that a considerable number of links should be content-close as demonstrated previously and IIRL can capture users&#x2019; interests exhibited in user-generated content. The superior performance over TADW is because not all links are content-close. Moreover, TADW even under-performs DeepWalk except on the Twitter network which has more content-close links than the other three networks. Hence, it not appropriate to approximate the entire network structure by node content like TADW but to approximate the content-close links by node content like the proposed IIRL. The superior performance over EOE is because interests are more explicitly employed in inferring new interactions. Moreover, the unexpected consequence of learning a single representation for two types of node characteristics mentioned in the introduction may also explain the inferior performance of all the baselines.</p>     <p>     <strong>Interpretability on link prediction</strong>: Besides being more accurate, IIRL can provide explanations for each piece of link prediction, e.g., it is established because two users know each other in the real world or because two users share similar interests. This can be easily obtained by referring to identity similarity and interest similarity because the larger similarity is expected to be associated with the larger proportion as discussed in Definition 4.</p>    </section>    <section id="sec-17">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.7</span> Multi-label Classification</h3>     </div>     </header>     <p>The groups are used as labels for the evaluation. 		 We employ the binary-relevance SVM with polynomial 		 kernel as the classifier, and the performance in 		 terms of Micro-F1 and Macro-F1 obtained by 5-fold 		 cross validation is presented in Table 		 <a class="tab" href="#tab7">7</a>. 		 Similarly, IIRL outperforms all the baselines. An intuitive explanation for the superior performance can be obtained from the visualization of the DBLP network in Fig. <a class="fig" href="#fig5">5</a>. Specifically, as nodes of the same group are distributed closer to each other in IIRL than in baselines, the decision boundary estimated in SVM can be more effective. <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186114/images/www2018-123-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Sensitivity 			 analysis and convergence analysis.</span>      </div>     </figure>     </p>    </section>    <section id="sec-18">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.8</span> Parameter Sensitivity and Convergence</h3>     </div>     </header>     <p>In this section, we study the performance w.r.t to 		 the dimension of representations and the number 		 <em>k</em> used in kNN networks. Fig. 		 <a class="fig" href="#fig7">7</a>(a) presents the 		 performance on the link prediction task for the 		 DBLP dataset w.r.t different dimensions of 		 representations. It shows that the proposed IIRL is 		 not much sensitive to the dimension. Fig. 		 <a class="fig" href="#fig7">7</a>(b) presents the performance on the link prediction task for the DBLP dataset w.r.t <em>k</em>. It shows that the number <em>k</em> should not be too small (e.g., 0.1%) as well as too large (e.g., 10%).</p>     <p>We also study the convergence of Algorithm 1. 		 Specifically, we study the performance of the 		 algorithm on link prediction for Flickr when 80% of 		 links used as training data and for DBLP w.r.t. the 		 number of outer iterations. The results are 		 presented in Fig. <a class="fig" href="#fig7">7</a> 		 (c) and Fig. <a class="fig" href="#fig7">7</a> (d). It shows that the algorithm converges very fast and may converge to stable performance after about 5 outer iterations.</p>    </section>   </section>   <section id="sec-19">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusion and Future Work</h2>     </div>    </header>    <p>In this paper, we explore semantic meanings of social links in network embedding for the first time. We categorize social links according to whether they are structure-close or content-close, and propose IIRL to learn identity representations and interest representations. In the future, we plan to explore more fine-grained semantic meanings of social links instead of two coarse-grained meanings.</p>   </section>   <section id="sec-20">    <header>     <div class="title-info">     <h2>Acknowledgement</h2>     </div>    </header>    <p>The work described in this paper was partially supported by the funding for Project of Strategic Importance provided by The Hong Kong Polytechnic University (Project Code: 1-ZE26), the University&#x0027;s Support for Application of Major Research Funding provided by The Hong Kong Polytechnic University (Project Code: 1-BBA1), RGC General Research Fund under Grant PolyU 152199/17E, NSFC Key Grant with Project No. 61332004, NSF through grants IIS-1526499, and CNS-1626432, and NSFC 61672313.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Mohammad Al&#x00A0;Hasan, Vineet Chaoji, Saeed Salem, and Mohammed Zaki. 2006. Link prediction using supervised learning. In <em>      <em>SDM06: workshop on link analysis, counter-terrorism and security</em></em>.</li>     <li id="BibPLXBIB0002" label="[2]">Larry Armijo. 1966. Minimization of functions having Lipschitz continuous first partial derivatives. <em>      <em>Pacific Journal of mathematics</em>     </em>16, 1 (1966), 1&#x2013;3.</li>     <li id="BibPLXBIB0003" label="[3]">Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, <em>et al.</em> 2007. Greedy layer-wise training of deep networks. <em>      <em>Advances in neural information processing systems</em>     </em>19 (2007), 153.</li>     <li id="BibPLXBIB0004" label="[4]">James&#x00A0;C Bezdek and Richard&#x00A0;J Hathaway. 2002. Some notes on alternating optimization. In <em>      <em>AFSS International Conference on Fuzzy Systems</em></em>. Springer, 288&#x2013;300.</li>     <li id="BibPLXBIB0005" label="[5]">Smriti Bhagat, Graham Cormode, and S Muthukrishnan. 2011. Node classification in social networks. In <em>      <em>Social network data analytics</em></em>. Springer, 115&#x2013;148.</li>     <li id="BibPLXBIB0006" label="[6]">Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In <em>      <em>Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</em></em>. ACM, 891&#x2013;900.</li>     <li id="BibPLXBIB0007" label="[7]">Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu&#x00A0;C Aggarwal, and Thomas&#x00A0;S Huang. 2015. Heterogeneous network embedding via deep architectures. In <em>      <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em>. ACM, 119&#x2013;128.</li>     <li id="BibPLXBIB0008" label="[8]">Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In <em>      <em>Advances in Neural Information Processing Systems</em></em>. 2172&#x2013;2180.</li>     <li id="BibPLXBIB0009" label="[9]">Yuxiao Dong, Nitesh&#x00A0;V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable representation learning for heterogeneous networks. In <em>      <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em>. ACM, 135&#x2013;144.</li>     <li id="BibPLXBIB0010" label="[10]">Santo Fortunato. 2010. Community detection in graphs. <em>      <em>Physics reports</em>     </em>486, 3 (2010), 75&#x2013;174.</li>     <li id="BibPLXBIB0011" label="[11]">Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In <em>      <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em>.</li>     <li id="BibPLXBIB0012" label="[12]">William&#x00A0;H Hsu, Andrew&#x00A0;L King, Martin&#x00A0;SR Paradesi, Tejaswi Pydimarri, and Tim Weninger. 2006. Collaborative and Structural Recommendation of Friends using Weblog-based Social Network Analysis.. In <em>      <em>AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs</em></em>, Vol.&#x00A0;6. 55&#x2013;60.</li>     <li id="BibPLXBIB0013" label="[13]">Juzheng Li, Jun Zhu, and Bo Zhang. 2016. Discriminative Deep Random Walk for Network Classification.. In <em>      <em>ACL (1)</em></em>.</li>     <li id="BibPLXBIB0014" label="[14]">Rui Li, Shengjie Wang, Hongbo Deng, Rui Wang, and Kevin Chen-Chuan Chang. 2012. Towards social user profiling: unified and discriminative influence model for inferring home locations. In <em>      <em>Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. ACM, 1023&#x2013;1031.</li>     <li id="BibPLXBIB0015" label="[15]">David Liben-Nowell and Jon Kleinberg. 2007. The link-prediction problem for social networks. <em>      <em>Journal of the American society for information science and technology</em>     </em>58, 7 (2007), 1019&#x2013;1031.</li>     <li id="BibPLXBIB0016" label="[16]">Laurens van&#x00A0;der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. <em>      <em>Journal of Machine Learning Research</em>     </em>9, Nov (2008), 2579&#x2013;2605.</li>     <li id="BibPLXBIB0017" label="[17]">Julian&#x00A0;J McAuley and Jure Leskovec. 2012. Learning to Discover Social Circles in Ego Networks.. In <em>      <em>NIPS</em></em>, Vol.&#x00A0;2012. 548&#x2013;56.</li>     <li id="BibPLXBIB0018" label="[18]">Miller McPherson, Lynn Smith-Lovin, and James&#x00A0;M Cook. 2001. Birds of a feather: Homophily in social networks. <em>      <em>Annual review of sociology</em>     </em>(2001), 415&#x2013;444.</li>     <li id="BibPLXBIB0019" label="[19]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. <em>      <em>arXiv preprint arXiv:1301.3781</em>     </em>(2013).</li>     <li id="BibPLXBIB0020" label="[20]">Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric Transitivity Preserving Graph Embedding.. In <em>      <em>KDD</em></em>. 1105&#x2013;1114.</li>     <li id="BibPLXBIB0021" label="[21]">Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In <em>      <em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. ACM, 701&#x2013;710.</li>     <li id="BibPLXBIB0022" label="[22]">Sam&#x00A0;T Roweis and Lawrence&#x00A0;K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. <em>      <em>Science</em>     </em>290, 5500 (2000), 2323&#x2013;2326.</li>     <li id="BibPLXBIB0023" label="[23]">Chuan Shi, Binbin Hu, Wayne&#x00A0;Xin Zhao, and Philip&#x00A0;S Yu. 2017. Heterogeneous Information Network Embedding for Recommendation. <em>      <em>arXiv preprint arXiv:1711.10730</em>     </em>(2017).</li>     <li id="BibPLXBIB0024" label="[24]">Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Predictive text embedding through large-scale heterogeneous text networks. In <em>      <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em>. ACM, 1165&#x2013;1174.</li>     <li id="BibPLXBIB0025" label="[25]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In <em>      <em>Proceedings of the 24th International Conference on World Wide Web</em></em>. ACM, 1067&#x2013;1077.</li>     <li id="BibPLXBIB0026" label="[26]">Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In <em>      <em>Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. ACM, 990&#x2013;998.</li>     <li id="BibPLXBIB0027" label="[27]">Paul Tseng. 2001. Convergence of a block coordinate descent method for nondifferentiable minimization. <em>      <em>Journal of optimization theory and applications</em>     </em>109, 3(2001), 475&#x2013;494.</li>     <li id="BibPLXBIB0028" label="[28]">Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. 2016. Max-Margin DeepWalk: Discriminative Learning of Network Representation.. In <em>      <em>IJCAI</em></em>. 3889&#x2013;3895.</li>     <li id="BibPLXBIB0029" label="[29]">Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In <em>      <em>Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</em></em>. ACM, 1225&#x2013;1234.</li>     <li id="BibPLXBIB0030" label="[30]">Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. 2017. Community Preserving Network Embedding.. In <em>      <em>AAAI</em></em>. 203&#x2013;209.</li>     <li id="BibPLXBIB0031" label="[31]">Xufei Wang, Lei Tang, Huiji Gao, and Huan Liu. 2010. Discovering Overlapping Groups in Social Media. In <em>      <em>the 10th IEEE International Conference on Data Mining series (ICDM2010)</em></em>. Sydney, Australia.</li>     <li id="BibPLXBIB0032" label="[32]">Xufei Wang, Lei Tang, Huan Liu, and Lei Wang. 2012. Learning with Multi-Resolution Overlapping Communities. <em>      <em>Knowledge and Information Systems (KAIS)</em>     </em>(2012). <a class="link-inline force-break"      href="https://doi.org/10.1007/s10115-012-0555-0"      target="_blank">https://doi.org/10.1007/s10115-012-0555-0</a></li>     <li id="BibPLXBIB0033" label="[33]">Xiaokai Wei, Bokai Cao, and S&#x00A0;Yu Philip. 2016. Unsupervised Feature Selection on Networks: A Generative View.. In <em>      <em>AAAI</em></em>. 2215&#x2013;2221.</li>     <li id="BibPLXBIB0034" label="[34]">Xiaokai Wei, Sihong Xie, and Philip&#x00A0;S Yu. 2015. Efficient partial order preserving unsupervised feature selection on networks. In <em>      <em>Proceedings of the 2015 SIAM International Conference on Data Mining</em></em>. SIAM, 82&#x2013;90.</li>     <li id="BibPLXBIB0035" label="[35]">Xiaokai Wei, Linchuan Xu, Bokai Cao, and Philip&#x00A0;S Yu. 2017. Cross view link prediction by learning noise-resilient representation consensus. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web</em></em>. International World Wide Web Conferences Steering Committee, 1611&#x2013;1619.</li>     <li id="BibPLXBIB0036" label="[36]">Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip&#x00A0;S Yu. 2017. Disentangled Link Prediction for Signed Social Networks via Disentangled Representation Learning. In <em>      <em>Proceedings of the Fourth IEEE International Conference on Data Science and Advanced Analytics</em></em>. IEEE.</li>     <li id="BibPLXBIB0037" label="[37]">Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip&#x00A0;S Yu. 2017. Embedding of Embedding (EOE): Joint Embedding for Coupled Heterogeneous Networks. In <em>      <em>Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</em></em>. ACM, 741&#x2013;749.</li>     <li id="BibPLXBIB0038" label="[38]">Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip&#x00A0;S Yu. 2017. Multi-task Network Embedding. In <em>      <em>Proceedings of the Fourth IEEE International Conference on Data Science and Advanced Analytics</em></em>. IEEE.</li>     <li id="BibPLXBIB0039" label="[39]">Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip&#x00A0;S Yu. 2017. Multiple Social Role Embedding. In <em>      <em>Proceedings of the Fourth IEEE International Conference on Data Science and Advanced Analytics</em></em>. IEEE.</li>     <li id="BibPLXBIB0040" label="[40]">Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward&#x00A0;Y Chang. 2015. Network representation learning with rich text information. In <em>      <em>Proceedings of the 24th International Joint Conference on Artificial Intelligence, Buenos Aires, Argentina</em></em>. 2111&#x2013;2117.</li>     <li id="BibPLXBIB0041" label="[41]">Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. 2017. Fast network embedding enhancement via high order proximity approximation. In <em>      <em>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI</em></em>. 19&#x2013;25.</li>     <li id="BibPLXBIB0042" label="[42]">Zhilin Yang, William&#x00A0;W Cohen, and Ruslan Salakhutdinov. 2016. Revisiting semi-supervised learning with graph embeddings. <em>      <em>arXiv preprint arXiv:1603.08861</em>     </em>(2016).</li>     <li id="BibPLXBIB0043" label="[43]">Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation: A heterogeneous information network approach. In <em>      <em>Proceedings of the 7th ACM international conference on Web search and data mining</em></em>. ACM, 283&#x2013;292.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>The work was done when the author was a Ph.D. student at University of Illinois at Chicago.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186114">https://doi.org/10.1145/3178876.3186114</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
