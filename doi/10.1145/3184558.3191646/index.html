<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Querying Wikimedia Images using Wikidata Facts</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Querying Wikimedia Images using Wikidata Facts</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Sebasti&#x00E1;n</span>      <span class="surName">Ferrada</span>,     Institute for the Foundations of Data DCC, Universidad de Chile, Santiago, Chile, <a href="mailto:sferrada@dcc.uchile.cl">sferrada@dcc.uchile.cl</a>     </div>     <div class="author">     <span class="givenName">Nicol&#x00E1;s</span>      <span class="surName">Bravo</span>,     DCC, Universidad de Chile, Santiago, Chile, <a href="mailto:nbravo@dcc.uchile.cl">nbravo@dcc.uchile.cl</a>     </div>     <div class="author">     <span class="givenName">Benjamin</span>      <span class="surName">Bustos</span>,     Institute for the Foundations of Data DCC, Universidad de Chile, Santiago, Chile, <a href="mailto:bebustos@dcc.uchile.cl">bebustos@dcc.uchile.cl</a>     </div>     <div class="author">     <span class="givenName">Aidan</span>      <span class="surName">Hogan</span>,     Institute for the Foundations of Data DCC, Universidad de Chile, Santiago, Chile, <a href="mailto:ahogan@dcc.uchile.cl">ahogan@dcc.uchile.cl</a>     </div>                     </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191646" target="_blank">https://doi.org/10.1145/3184558.3191646</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Despite its importance to the Web, multimedia content is often neglected when building and designing knowledge-bases: though descriptive metadata and links are often provided for images, video, etc., the multimedia content itself is often treated as opaque and is rarely analysed. <font style="font-variant: small-caps">IMGpedia</font> is an effort to bring together the images of <font style="font-variant: small-caps">Wikimedia Commons</font> (including visual information), and relevant knowledge-bases such as <font style="font-variant: small-caps">Wikidata</font> and <font style="font-variant: small-caps">DBpedia</font>. The result is a knowledge-base that incorporates similarity relations between the images based on visual descriptors, as well as links to the resources of <font style="font-variant: small-caps">Wikidata</font> and <font style="font-variant: small-caps">DBpedia</font> that relate to the image. Using the <font style="font-variant: small-caps">IMGpedia</font> SPARQL endpoint, it is then possible to perform visuo-semantic queries, combining the semantic facts extracted from the external resources and the similarity relations of the images. This paper presents a new web interface to browse and explore the dataset of <font style="font-variant: small-caps">IMGpedia</font> in a more friendly manner, as well as new visuo-semantic queries that can be answered using 6 million recently added links from <font style="font-variant: small-caps">IMGpedia</font> to <font style="font-variant: small-caps">Wikidata</font>. We also discuss future directions we foresee for the <font style="font-variant: small-caps">IMGpedia</font> project.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Multimedia databases;</strong> Wikis;</small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Multimedia</small>, </span>     <span class="keyword">      <small> Linked Data</small>, </span>     <span class="keyword">      <small> Wikimedia</small>, </span>     <span class="keyword">      <small> Wikidata</small>, </span>     <span class="keyword">      <small> IMGpedia</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Sebasti&#x00E1;n Ferrada, Nicol&#x00E1;s Bravo, Benjamin Bustos, and Aidan Hogan. 2018. Querying Wikimedia Images using Wikidata Facts. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 7 Pages. <a href="https://doi.org/10.1145/3184558.3191646" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191646</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Multimedia Retrieval and the Web of Data have largely remained separate areas of research with little work in their intersection. However, the benefits of working in the intersection of these two areas are clear: the goal of Multimedia Retrieval is to find videos, images, etc., based on analyses of their context and content, while the Web of Data aims to structure the content of the Web to better automate various complex tasks (including retrieval). Further observing that multimedia content is forming an ever-more visible component of the Web, we argue that work in this intersection could prove very fruitful, both to help bring Web of Data techniques more into the mainstream enabling more tangible applications; as well as allowing more advanced Multimedia Retrieval using the knowledge-bases, query languages and reasoning techniques that the Web of Data already provides. Such a combination would allow, for example, to ask semantic queries on the visual content and real-world context of the Web&#x0027;s images, linking media to related sources of information, events, news articles, or indeed other media.</p>    <p>Along these lines, in previous work we initially proposed the <font style="font-variant: small-caps">IMGpedia</font>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] knowledge-base: a linked dataset that provides information about 15 million images from the <font style="font-variant: small-caps">Wikimedia Commons</font> collection. The dataset of <font style="font-variant: small-caps">IMGpedia</font> includes different visual descriptors that capture various features from the images, such as colour distribution, shape patterns and greyscale intensities. It also provides static similarity relations among the images and links to related entities on <font style="font-variant: small-caps">DBpedia</font>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] and (now) to <font style="font-variant: small-caps">Wikidata</font>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>]. With <font style="font-variant: small-caps">IMGpedia</font> it is then possible to answer <em>visuo-semantic</em> queries through a public SPARQL endpoint. These queries combine the similarity relations with the semantic facts that can be extracted from the linked sources; an example of such a query might be &#x201C;<em>retrieve images of museums that look similar to European cathedrals</em>.&#x201D;</p>    <p>In our previous work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] we described the creation of the dataset of <font style="font-variant: small-caps">IMGpedia</font> and illustrated some preliminary queries that it can respond to using the links provided to <font style="font-variant: small-caps">DBpedia</font>. In this paper we report on some new visuo-semantic queries that are enabled by newly added links to <font style="font-variant: small-caps">Wikidata</font>, which provides a more flexible way to request for external entities. We also present a new user interface for <font style="font-variant: small-caps">IMGpedia</font> that allows people to browse the results of the SPARQL queries featuring the images involved, and to explore details about the images, such as their related web resources and their similar images. Finally we present some future directions in which we foresee the <font style="font-variant: small-caps">IMGpedia</font> project developing and possible ways in which it might contribute to existing <font style="font-variant: small-caps">Wikimedia</font> projects.</p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>There have been a number of works in the intersection of the Semantic Web and Multimedia areas, and like <font style="font-variant: small-caps">IMGpedia</font>, there are some knowledge-bases based on Semantic Web standards that incorporate &#x2013; or even focus on &#x2013; multimedia content. <font style="font-variant: small-caps">DBpedia Commons</font> automatically extracts metadata from the media of <font style="font-variant: small-caps">Wikimedia Commons</font> providing triples for licensing, file extension and annotations&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]. Bechhofer et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] provide a linked dataset of live music archives, containing feature analysis and links to existing musical and geographical resources. The <font style="font-variant: small-caps">MIDI</font> dataset&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] represents multiple music collections in the form of Linked Data using MIDI versions of songs curated by the authors or provided by the community. There are also manually curated datasets such as <font style="font-variant: small-caps">LinkedMDB</font> providing facts about movies, and <font style="font-variant: small-caps">BBC Music</font>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] describing bands, records and songs. <font style="font-variant: small-caps">IMGpedia</font>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] is a related effort along these lines but with a current focus on describing images; however, unlike (e.g.) <font style="font-variant: small-caps">DBpedia Commons</font>, which focuses purely on contextual meta-data, <font style="font-variant: small-caps">IMGpedia</font> aims to leverage the visual content of images to create new semantic links in the knowledge-base that go beyond surface meta-data.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> IMGpedia Overview</h2>     </div>    </header>    <p>Before we present novel aspects of <font style="font-variant: small-caps">IMGpedia</font> &#x2013; the user interface and the new queries enabled by links to <font style="font-variant: small-caps">Wikidata</font> &#x2013; we first provide an overview of <font style="font-variant: small-caps">IMGpedia</font>, the images from which it is built, the visual descriptors used, the types of relations considered, and so forth. Here our goal is to provide an overview of the knowledge-base; for more details we refer the reader to our previous paper&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>].</p>    <p>     <font style="font-variant: small-caps">IMGpedia</font> contains information about 14.7 million images taken from the <font style="font-variant: small-caps">Wikimedia Commons</font> multimedia dataset. We only consider the images with extensions <tt>JPG</tt> and <tt>PNG</tt> (92% of the dataset) in order to perform a standardised analysis of them. We store the images locally and characterise them by extracting their visual descriptors, which are high-dimensional vectors that capture different features of the images. The descriptors used where the Histogram of Oriented Gradients (HOG), the Gray Histogram Descriptor (GHD), and the Color Layout Descriptor (CLD). These descriptors extract features related to the borders of the image, to the brightness of the greyscale image, and to the distribution of the colour in the image, respectively. Using these sets of vectors we computed static similarity relations among the images: for each image and descriptor, we find its 10 nearest neighbours and we store these relations along with the computed distances and the type of descriptor being used (to scale to the full image-set, we use an approximate nearest neighbour algorithm). Finally, we take all this information and represent it as an RDF Graph, upload it to a Virtuoso Server and provide a public SPARQL endpoint for clients to issue queries. In Table&#x00A0;<a class="tbl" href="#tab1">1</a> we present some statistics to give an idea of the size of the dataset and to show the main entities and properties provided by <font style="font-variant: small-caps">IMGpedia</font>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">High-level statistics for <font style="font-variant: small-caps">IMGpedia</font>.     </span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:left;">1cName</th>       <th style="text-align:center;">Count</th>       <th style="text-align:center;">Description</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">        <tt>imo:Image</tt>       </td>       <td style="text-align:right;">14,765,300</td>       <td style="text-align:left;">Resources describing individual images</td>      </tr>      <tr>       <td style="text-align:left;">        <tt>imo:Descriptor</tt>       </td>       <td style="text-align:right;">44,295,900</td>       <td style="text-align:left;">Visual descriptors of the images</td>      </tr>      <tr>       <td style="text-align:left;">        <tt>imo:similar</tt>       </td>       <td style="text-align:right;">473,680,270</td>       <td style="text-align:left;">Similarity relations</td>      </tr>      <tr>       <td style="text-align:left;">        <tt>imo:associatedWith</tt>       </td>       <td style="text-align:right;">19,297,667</td>       <td style="text-align:left;">Links to <font style="font-variant: small-caps">DBpedia</font> and <font style="font-variant: small-caps">Wikidata</font>       </td>      </tr>      <tr>       <td style="text-align:left;">Total Triples</td>       <td style="text-align:right;">3,138,266,288</td>       <td style="text-align:left;">RDF triples present in the <font style="font-variant: small-caps">IMGpedia</font> graph</td>      </tr>     </tbody>     </table>    </div>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Browsing the Data</h2>     </div>    </header>    <p>The initial release of <font style="font-variant: small-caps">IMGpedia</font> used the default interfaces from Virtuoso Server<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> for writing queries, browsing the results and exploring the resources. Though these interfaces provided the necessary mechanisms for agents to access the resources &#x2013; a SPARQL endpoint with Linked Data dereferencing &#x2013; the interfaces for human users were mainly plain HTML tables for displaying SPARQL query results and almost illegible HTML documents for displaying the details of individual resources. We foresaw that the lack of more human-friendly interfaces would prevent people from using and querying the dataset; conversely, being a knowledge-base centred around images, we foresaw much potential for creating a visually-appealing querying and browsing interface over <font style="font-variant: small-caps">IMGpedia</font>.</p>    <p>Along these lines, to initially address the usability of <font style="font-variant: small-caps">IMGpedia</font>, we set up a front-end application that makes the process of querying and browsing our data a more friendly experience. The application was developed with AngularJS 5 framework and uses the Virtuoso Server as a back-end.<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> The application consists of three main components: a SPARQL query editor, a browser for the results of SPARQL queries that shows the images in-place, and an interface for exploring the details of individual visual entities.</p>    <p>The SPARQL query editor<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> provides a text area for writing the queries and a button for executing them. The interface then displays the results of the SPARQL queries below the query editor; if the result contains any visual entities, rather than simply display the URL of the image, it automatically displays the corresponding image by making a request to <font style="font-variant: small-caps">Wikimedia Commons</font>. In Figure&#x00A0;<a class="fig" href="#fig1">1</a> we present a screenshot of the interface showing a simple query for three pairs of similar images within a particular visual similarity distance, with the results displayed below.</p>    <p>A user may click on an image displayed in such a result, which will lead them to a detailed focus view of the information available for that visual entity. This view shows the focus image that is being described, along with its name in <font style="font-variant: small-caps">Wikimedia</font>, and links to related <font style="font-variant: small-caps">Wikipedia</font>, <font style="font-variant: small-caps">Wikidata</font> and <font style="font-variant: small-caps">DBpedia</font> resources (based on links in those knowledge-bases, appearances in the Wikipedia article associated to those entities, etc.). We also display images similar to the focus image based on precomputed similarity relations present in the <font style="font-variant: small-caps">IMGpedia</font> knowledge-base; similar images are grouped by visual descriptor and are sorted by distance; the user can hover over each such image to see the distance or can click to go to its focus page. In Figure&#x00A0;<a class="fig" href="#fig2">2</a> a screenshot of the interface can be found showing a drawing of Queen Mary I of England by Lucas Horenbout; below this the interface displays similar images found through the grey intensity descriptor in ascending order of distance. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">IMGpedia SPARQL query editor and results.</span>     </div>     </figure>     <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Detail page for visual entities; URL displayed: <a class="link-inline force-break"        href="http://imgpedia.dcc.uchile.cl/detail/Mary_Tudor_by_Horenbout.jpg">http://imgpedia.dcc.uchile.cl/detail/Mary_Tudor_by_Horenbout.jpg</a>.      </span>     </div>     </figure>    </p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Querying the Data</h2>     </div>    </header>    <p>In the first version of <font style="font-variant: small-caps">IMGpedia</font>, the supported visuo-semantic queries relied heavily on the existence of categories for the resources of <font style="font-variant: small-caps">DBpedia</font>; for the example query in Section&#x00A0;<a class="sec" href="#sec-5">1</a>, we use the <font style="font-variant: small-caps">DBpedia</font> category <tt>dbc:Roman_Catholic_cathedrals_in_Europe</tt> for obtaining European Cathedrals (see&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, Figure&#x00A0;<a class="fig" href="#fig3">3</a>]). To address this issue and in order to support more diverse queries, we have added links to other complementary context sources; in particular, <font style="font-variant: small-caps">IMGpedia</font> now provides 6,614,244 links to <font style="font-variant: small-caps">Wikidata</font>, where a visual entity in <font style="font-variant: small-caps">IMGpedia</font> is linked to an entity from <font style="font-variant: small-caps">Wikidata</font> if the image appears on the English <font style="font-variant: small-caps">Wikipedia</font> article corresponding to that <font style="font-variant: small-caps">Wikidata</font> entity. The match between <font style="font-variant: small-caps">Wikipedia</font> articles and <font style="font-variant: small-caps">Wikidata</font> entities was made querying a dump of <font style="font-variant: small-caps">Wikidata</font>.<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>    </p>    <p>Using these new links, we can ask novel queries that were not possible before. For example, now we can request images of governmental palaces in Latin America. This was not possible before since there is no category referring to such palaces in <font style="font-variant: small-caps">DBpedia</font>; if we now leverage these new links to <font style="font-variant: small-caps">Wikidata</font>, we can combine different predicates in order to achieve our goal. In Listing&#x00A0;1 we show the SPARQL query that satisfies our requirements; it first requests from <font style="font-variant: small-caps">Wikidata</font> all entities referring to governmental palaces in Latin America through SPARQL federation, further retrieving the URLs of related images, along with the label of the palace and the name of the country. Figure&#x00A0;<a class="fig" href="#fig3">3</a> then shows a sample of the results returned.<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>     <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Government palaces in Latin America.</span>     </div>     </figure>    </p>    <p>The previous query is what we would refer to as a purely &#x201C;semantic query&#x201D;, meaning that it does not rely on any of the visual information present in <font style="font-variant: small-caps">IMGpedia</font> computed from the content of the images. On the other hand, with links to <font style="font-variant: small-caps">Wikidata</font>, we can perform new <em>visuo-semantic queries</em> combining <font style="font-variant: small-caps">Wikidata</font> facts and <font style="font-variant: small-caps">IMGpedia</font> visual similarity; for example, we can ask for educational institutions that are similar to the images of government palaces obtained before. In Listing&#x00A0;2 we show the SPARQL query required to do so. First we use the same service clause requesting for the palaces as in Listing&#x00A0;1 so we omit its body here. Later we obtain the similar images and the entities they are associated with and finally we keep those images that are related with educational institutions using SPARQL property paths to consider any subclasses. In Figure&#x00A0;<a class="fig" href="#fig4">4</a>, we show the result of the query. <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Educational institutions similar to Latin American government palaces.</span>     </div>     </figure>    </p>    <p>In Listing&#x00A0;3 we show another example of a visuo-semantic query where we look for people with images similar to those associated with paintings in the Louvre. In Figure&#x00A0;<a class="fig" href="#fig5">5</a> we show the result of the query, where each painting has two similar people. It is worth noting that the first painting is not on display at the Louvre; however the image is related to the Louvre since it is a self-portrait of Jean Auguste Ingres, the painter of <em>Napoleon I on His Imperial Throne</em> which is on display at the Louvre, where the portrait of Ingres then appears in the <font style="font-variant: small-caps">Wikipedia</font> article of the painting.</p>    <p>The SPARQL query of Listing&#x00A0;3 can be modified to obtain other paintings similar to those exhibited at the Louvre by changing the type requested on the second <tt>SERVICE</tt> clause from <tt>wd:Q5</tt> (human) to, for example, <tt>wd:Q3305213</tt> (painting). An interesting result of this query variation can be seen in Figure&#x00A0;<a class="fig" href="#fig6">6</a>, where the two images depict the same painting. In such cases we can say that the images are <tt>near_copy</tt> of each other. Such relations among images could allow reasoning over the entities related to the images; in this case one image corresponds to the painting entity and the other is associated to the painter, and hence it is probable that the painter is the author of the artwork. Section&#x00A0;<a class="sec" href="#sec-10">6</a> discusses this topic further. <figure id="fig5">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">People similar to paintings in the Louvre.</span>     </div>     </figure>     <figure id="fig6">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 6:</span>      <span class="figure-title">Paintings similar to other paintings at the Louvre; the image on the right appears in the Olympia artwork article while the (near-copy) image on the left appears in the article of its painter, &#x00C9;douard Manet.</span>     </div>     </figure>    </p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Future Directions</h2>     </div>    </header>    <p>We presented new features of <font style="font-variant: small-caps">IMGpedia</font>: a more friendly user interface that helps people explore the knowledge-base, and links to <font style="font-variant: small-caps">Wikidata</font> resources that enable novel types of visuo-semantic queries. However, our future goal is to use <font style="font-variant: small-caps">IMGpedia</font> as a starting point to explore and potentially prove a more general concept: that the areas of Multimedia Retrieval and the Web of Data can potentially complement each other in many relevant aspects. Thus there are many features that can be added, many use-cases that can be conceived, and many research questions that may arise from the current version of <font style="font-variant: small-caps">IMGpedia</font> in the intersection of both areas.</p>    <p>The following are some of the directions we envisage:</p>    <ul class="list-no-style">     <li id="uid13" label="Novel visual descriptors:">Currently <font style="font-variant: small-caps">IMGpedia</font> visual relations are based on three core descriptors relating to colour, intensity and edges. In some cases these descriptors can give good results, while in others the results leave room for improvement. An interesting direction to improve the similarity relations in <font style="font-variant: small-caps">IMGpedia</font> is to incorporate and test novel visual descriptors and potentially combinations thereof. However two major challenges here are scale and evaluation. For scale, computing similarity joins over 15 million images requires specialised (and approximated) methods to avoid a quadratic behaviour; furthermore, for some descriptors even computing the descriptors is computationally challenging (for example, we performed initial experiments with a neural-network-based descriptor DeCAF&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>] but estimated it would take years to compute over all images on our hardware). Another challenge is evaluation, which would require human judgement but where creating a gold-standard <em>a priori</em> is unfeasible; while human users could help to estimate precision, estimating recall seems a particular challenge.<br/></li>     <li id="uid14" label="Specialising resource links:">As it was discussed previously with the example of Jean Auguste Ingres, our links from images to related knowledge-base entities is rather coarse being based on the existing image relations in external knowledge-bases, and the appearance of an image in the associated Wikipedia article of that image. It would thus be interesting to enhance <font style="font-variant: small-caps">IMGpedia</font> by offering more specialised types of links between images and resources, such as to deduce (with high confidence) that a given entity is really depicted by an image. More ambitiously, it would be interesting to use Media Fragments&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] to try to identify which parts of the image depict a given entity.<br/></li>     <li id="uid15" label="Image-based neural networks:">There exist a variety of neural networks trained over millions of images to identify different types of entities in images (e.g.,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>]). Given that our images are linked to external knowledge-base entities, we could use these links to create a very large labelled dataset of images where labels can take varying levels of granularity (e.g., <tt>latin american government palaces</tt>, <tt>government palaces</tt>, <tt>government buildings</tt>, <tt>buildings</tt>, etc.). Such a dataset could be used to train a novel neural network and/or to evaluate current machine vision techniques over labelled images of varying levels of granularity. Furthermore, neural networks could enable us to describe what is happening in the image by extracting triples about the scene&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>], to provide deep-learning based feature vectors&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]; to verify that an image is indeed a depiction of a linked entity; and so forth.<br/></li>     <li id="uid16" label="Image-based reasoning:">We are planning to add new relations between images, such as <tt>near_copy</tt> or <tt>same_subject</tt>; potentially such relations may allow to perform novel types of reasoning over the data; for example if two entities in an external knowledge-base are related to a pair of images that are related by <tt>near_copy</tt> or <tt>same_subject</tt>, we could deduce new relations about the entities: if both entities are people, we can say that they have met (with some likelihood); or if one entity is a person and the other a place, there is a high chance that the person has visited the place.<br/></li>     <li id="uid17" label="SPARQL similarity queries:">Extending SPARQL with functions to dynamically compute similarities between images after a semantic filter is applied, would obviate the need to rely on a bounded number of statically computed relations. Doing so efficiently may require hybrid cost models that understand the selectivity of not only the data-centred results, but also the similarity-based results. We are thus starting to research different ways of indexing the data to efficiently perform similarity joins.<br/></li>     <li id="uid18" label="Video and other multimedia:">Though <font style="font-variant: small-caps">Wikimedia</font> resources are predominantly images, there are also resources relating to video, audio, etc. Although audio would require a very different type of processing, it may be possible in future to try to match stills of video to images in <font style="font-variant: small-caps">IMGpedia</font> to, for example, identify possible topics of the video, or in general to bootstrap a similar form of semantic retrieval for videos. Likewise other sources of multimedia (e.g., YouTube, flickr, etc.) could be linked to <font style="font-variant: small-caps">IMGpedia</font> in the future.<br/></li>     <li id="uid19" label="Updating IMGpedia:">Currently the initial version of <font style="font-variant: small-caps">IMGpedia</font> is built statically. An interesting technical challenge then is to maintain the knowledge-base up-to-date with <font style="font-variant: small-caps">Wikimedia</font> and the external knowledge-bases, including, for example, the ability to incrementally update similarity relations.<br/></li>    </ul>    <p>While the prior topics relate to extending or enhancing <font style="font-variant: small-caps">IMGpedia</font>, an orthogonal but crucial aspect is the development of user applications on top of the knowledge-base. While the interface described here is a step in that direction &#x2013; allowing users to explore the images, their associated resources, and their similarity relations &#x2013; there is potentially much more left to be done. As a first step, we would like users to be able to specify keyword searches over the images. More ambitiously, for example, we would like users to be able to pose more complex visuo-semantic queries over the data (currently this still requires knowledge of RDF and SPARQL). Such development of applications on top of <font style="font-variant: small-caps">IMGpedia</font> would also require appropriate usability testing to validate. In general, we believe that through its focus on multimedia and use of existing knowledge-bases, <font style="font-variant: small-caps">IMGpedia</font> has the potential to become a tangible demonstration of the value of Semantic Web technologies.</p>    <p>Another important question that we wish to address is: <em>how can</em>     <font style="font-variant: small-caps">IMGpedia</font>     <em>compliment the existing projects of the Wikimedia Foundation?</em> Could the similarity relations computed by <font style="font-variant: small-caps">IMGpedia</font> be added, for example, to the associated descriptions in <font style="font-variant: small-caps">Wikidata</font>? Could the detection of near-duplicate images help in some way the editors of <font style="font-variant: small-caps">Wikipedia</font>? Could some further visual information be added back to the <font style="font-variant: small-caps">Wikimedia</font> pages for images? What other possible use-cases might there be for a knowledge-base such as <font style="font-variant: small-caps">IMGpedia</font> in the context of the varied <font style="font-variant: small-caps">Wikimedia</font> projects? We are eager to discuss and explore such use-cases.</p>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>Acknowledgements</h2>     </div>    </header>    <p>This work was supported by the Millennium Institute for the Foundations of Data, by CONICYT-PFCHA 2017-21170616 and by Fondecyt Grant No. 1181896. We would like to thank Larry Gonz&#x00E1;lez and Camila Fa&#x00FA;ndez for their assistance.</p>   </section>  </section>  <section class="back-matter">   <Appendix>    <section id="sec-12">     <header>     <div class="title-info">      <h2>       <span class="section-number">A</span> SPARQL Queries</h2>     </div>     </header>     <p>Here we provide the SPARQL queries used to generate the examples.</p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-img1.jpg" class="img-responsive" alt=""       longdesc=""/>     </p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-img2.jpg" class="img-responsive" alt=""       longdesc=""/>     </p>     <p>     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191646/images/www18companion-385-img3.jpg" class="img-responsive" alt=""       longdesc=""/>     </p>    </section>   </Appendix>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Stephan Baier, Yunpu Ma, and Volker Tresp. 2017. Improving visual relationship detection using semantic modeling of scene descriptions. In <em>      <em>International Semantic Web Conference</em>     </em>. Springer, 53&#x2013;68.</li>     <li id="BibPLXBIB0002" label="[2]">Sean Bechhofer, Kevin Page, David&#x00A0;M Weigl, Gy&#x00F6;rgy Fazekas, and Thomas Wilmering. 2017. Linked Data Publication of Live Music Archives and Analyses. In <em>      <em>International Semantic Web Conference</em>     </em>. Springer, 29&#x2013;37.</li>     <li id="BibPLXBIB0003" label="[3]">Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2014. Decaf: A deep convolutional activation feature for generic visual recognition. In <em>      <em>International conference on machine learning</em>     </em>. 647&#x2013;655.</li>     <li id="BibPLXBIB0004" label="[4]">Sebasti&#x00E1;n Ferrada, Benjamin Bustos, and Aidan Hogan. 2017. IMGpedia: a linked dataset with content-based analysis of Wikimedia images. In <em>      <em>International Semantic Web Conference</em>     </em>. Springer, 84&#x2013;93.</li>     <li id="BibPLXBIB0005" label="[5]">Alex Krizhevsky, Ilya Sutskever, and Geoffrey&#x00A0;E Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In <em>      <em>Advances in Neural Information Processing Systems 25</em>     </em>. Curran Associates, Inc., 1097&#x2013;1105.</li>     <li id="BibPLXBIB0006" label="[6]">Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo&#x00A0;N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van&#x00A0;Kleef, S&#x00F6;ren Auer, <em>et al.</em> 2015. DBpedia&#x2013;a large-scale, multilingual knowledge base extracted from Wikipedia. <em>      <em>Semantic Web</em>     </em>6, 2 (2015), 167&#x2013;195.</li>     <li id="BibPLXBIB0007" label="[7]">Albert Mero&#x00F1;o-Pe&#x00F1;uela, Rinke Hoekstra, Aldo Gangemi, Peter Bloem, Reinier de Valk, Bas Stringer, Berit Janssen, Victor de Boer, Alo Allik, Stefan Schlobach, <em>et al.</em> 2017. The MIDI Linked Data Cloud. In <em>      <em>International Semantic Web Conference</em>     </em>. Springer, 156&#x2013;164.</li>     <li id="BibPLXBIB0008" label="[8]">Yves Raimond, Christopher Sutton, and Mark&#x00A0;B. Sandler. 2009. Interlinking Music-Related Data on the Web. <em>      <em>IEEE MultiMedia</em>     </em>16, 2 (2009), 52&#x2013;63. <a class="link-inline force-break" href="https://doi.org/10.1109/MMUL.2009.29"      target="_blank">https://doi.org/10.1109/MMUL.2009.29</a></li>     <li id="BibPLXBIB0009" label="[9]">Rapha&#x00EB;l Troncy, Erik Mannens, Silvia Pfeiffer, and Davy&#x00A0;Van Deursen. 2012. Media Fragments URI 1.0. W3C Recommendation. (2012).</li>     <li id="BibPLXBIB0010" label="[10]">Gaurav Vaidya, Dimitris Kontokostas, Magnus Knuth, Jens Lehmann, and Sebastian Hellmann. 2015. DBpedia Commons: Structured multimedia metadata from the Wikimedia Commons. In <em>      <em>International Semantic Web Conference</em>     </em>. Springer, 281&#x2013;289.</li>     <li id="BibPLXBIB0011" label="[11]">Denny Vrande&#x010D;i&#x0107; and Markus Kr&#x00F6;tzsch. 2014. Wikidata: a free collaborative knowledgebase. <em>      <em>Commun. ACM</em>     </em>57, 10 (2014), 78&#x2013;85.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>    <a class="link-inline force-break" target="_blank" href="http://imgpedia.dcc.uchile.cl/sparql">http://imgpedia.dcc.uchile.cl/sparql</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>The source code of the interface application can be found at <a class="link-inline force-break" target="_blank" href="https://github.com/NicolasBravo94/imgpediavis">https://github.com/NicolasBravo94/imgpediavis</a>.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>    <a class="link-inline force-break" target="_blank" href="http://imgpedia.dcc.uchile.cl/query">http://imgpedia.dcc.uchile.cl/query</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>The <font style="font-variant: small-caps">Wikidata</font> dump used was that from 2017-07-25</p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>In the figure we can see that there is an image misplaced: the image in the upper right depicts Kedleston Hall in Derbyshire, however in the <font style="font-variant: small-caps">Wikipedia</font> dump that was used (2015-12-01) it was stated that said image was used in the English version of the article about the Casa Rosada, and hence it is presented and mislabelled by <font style="font-variant: small-caps">IMGpedia</font>.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191646">https://doi.org/10.1145/3184558.3191646</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
