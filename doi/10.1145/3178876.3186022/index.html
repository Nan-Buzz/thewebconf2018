<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Matching Natural Language Sentences with Hierarchical
  Sentence Factorization</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186022'>https://doi.org/10.1145/3178876.3186022</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186022'>https://w3id.org/oa/10.1145/3178876.3186022</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Matching Natural Language
          Sentences with Hierarchical Sentence
          Factorization</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Bang</span> <span class=
          "surName">Liu</span>, University of Alberta, Edmonton,
          AB, Canada
        </div>
        <div class="author">
          <span class="givenName">Ting</span> <span class=
          "surName">Zhang</span>, University of Alberta, Edmonton,
          AB, Canada
        </div>
        <div class="author">
          <span class="givenName">Fred X.</span> <span class=
          "surName">Han</span>, University of Alberta, Edmonton,
          AB, Canada
        </div>
        <div class="author">
          <span class="givenName">Di</span> <span class=
          "surName">Niu</span>, University of Alberta, Edmonton,
          AB, Canada
        </div>
        <div class="author">
          <span class="givenName">Kunfeng</span> <span class=
          "surName">Lai</span>, Mobile Internet Group, Tencent,
          Shenzhen, China
        </div>
        <div class="author">
          <span class="givenName">Yu</span> <span class=
          "surName">Xu</span>, Mobile Internet Group, Tencent,
          Shenzhen, China
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186022"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186022</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Semantic matching of natural language sentences
        or identifying the relationship between two sentences is a
        core research problem underlying many natural language
        tasks. Depending on whether training data is available,
        prior research has proposed both unsupervised
        distance-based schemes and supervised deep learning schemes
        for sentence matching. However, previous approaches either
        omit or fail to fully utilize the ordered, hierarchical,
        and flexible structures of language objects, as well as the
        interactions between them. In this paper, we propose
        <em>Hierarchical Sentence Factorization</em>—a technique to
        factorize a sentence into a hierarchical representation,
        with the components at each different scale reordered into
        a “predicate-argument” form. The proposed sentence
        factorization technique leads to the invention of: 1) a new
        unsupervised distance metric which calculates the semantic
        distance between a pair of text snippets by solving a
        penalized optimal transport problem while preserving the
        logical relationship of words in the reordered sentences,
        and 2) new multi-scale deep learning models for supervised
        semantic training, based on factorized sentence
        hierarchies. We apply our techniques to text-pair
        similarity estimation and text-pair relationship
        classification tasks, based on multiple datasets such as
        STSbenchmark, the Microsoft Research paraphrase
        identification (MSRP) dataset, the SICK dataset, etc.
        Extensive experiments show that the proposed hierarchical
        sentence factorization can be used to significantly improve
        the performance of existing unsupervised distance-based
        metrics as well as multiple supervised deep learning models
        based on the convolutional neural network (CNN) and long
        short-term memory (LSTM).</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Bang Liu, Ting Zhang, Fred X. Han, Di Niu, Kunfeng Lai,
          and Yu Xu. 2018. Matching Natural Language Sentences with
          Hierarchical Sentence Factorization. In <em>WWW 2018: The
          2018 Web Conference,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 10 Pages.
          <a href="https://doi.org/10.1145/3178876.3186022" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186022</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Semantic matching, which aims to model the underlying
      semantic similarity or dissimilarity among different textual
      elements such as sentences and documents, has been playing a
      central role in many Natural Language Processing (NLP)
      applications, including information extraction [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0014">14</a>],
      top-<em>k</em> re-ranking in machine translation [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>],
      question-answering [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0044">44</a>], automatic text summarization
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>]. However,
      semantic matching based on either supervised or unsupervised
      learning remains a hard problem. Natural language
      demonstrates complicated hierarchical structures, where
      different words can be organized in different orders to
      express the same idea. As a result, appropriate semantic
      representation of text plays a critical role in matching
      natural language sentences.</p>
      <p>Traditional approaches represent text objects as
      bag-of-words (BoW), term frequency inverse document frequency
      (TF-IDF) [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0043">43</a>]
      vectors, or their enhanced variants [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0032">32</a>]. However, such
      representations can not accurately capture the similarity
      between individual words, and do not take the semantic
      structure of language into consideration. Alternatively, word
      embedding models, such as <em>word2vec</em> [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>] and <em>Glove</em>
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>], learn a
      distributional semantic representation of each word and have
      been widely used.</p>
      <p>Based on the word-vector representation, a number of
      unsupervised and supervised matching schemes have been
      recently proposed. As an unsupervised learning approach, the
      Word Mover's Distance (WMD) metric [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>] measures the
      dissimilarity between two sentences (or documents) as the
      minimum distance to transport the embedded words of one
      sentence to those of another sentence. However, the
      sequential and structural nature of sentences is omitted in
      WMD. For example, two sentences containing exactly the same
      words in different orders can express totally different
      meanings. On the other hand, many supervised learning schemes
      based on deep neural networks have also been proposed for
      sentence matching [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0041">41</a>]. A common characteristic of many of
      these neural network models is that they adopt a Siamese
      architecture, taking the word embedding sequences of a pair
      of sentences (or documents) as the input, transforming them
      into intermediate contextual representations via either
      convolutional or recurrent neural networks, and performing
      scoring over the contextual representations to yield final
      matching results. However, these methods rely purely on
      neural networks to learn the complicated relationships among
      sentences, and many obvious compositional and hierarchical
      features are often overlooked or not explicitly utilized.</p>
      <p>In this paper, however, we argue that a successful
      semantic matching algorithm needs to best characterize the
      sequential, hierarchical and flexible structure of natural
      language sentences, as well as the rich interaction patterns
      among semantic units. We present a technique named
      <em>Hierarchical Sentence Factorization</em> (or <em>Sentence
      Factorization</em> in short), which is able to represent a
      sentence in a hierarchical semantic tree, with each node
      (semantic unit) at different depths of the tree reorganized
      into a normalized “predicate-argument” form. Such normalized
      sentence representation enables us to propose new methods to
      both improve unsupervised semantic matching by taking the
      structural and sequential differences between two text
      entities into account, and enhance a range of supervised
      semantic matching schemes, by overcoming the limitation of
      the representation capability of convolutional or recurrent
      neural networks, especially when labelled training data is
      limited. Specifically, we make the following
      contributions:</p>
      <p><em>First</em>, the proposed <em>Sentence
      Factorization</em> scheme factorizes a sentence recursively
      into a hierarchical tree of semantic units, where each unit
      is a subset of words from the original sentence. Words are
      then reordered into a “predicate-argument” structure. Such
      form of sentence representation offers two benefits: i) the
      flexible syntax structures of the same sentence, for example,
      active and passive sentences, can be normalized into a
      unified representation; ii) the semantic units in a pair of
      sentences can be aligned according to their depth and order
      in the factorization tree.</p>
      <p><em>Second</em>, for unsupervised text matching, we
      combine the factorized and reordered representation of
      sentences and the Order-preserving Wasserstein Distance
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0037">37</a>] (which was
      originally proposed to match hand-written characters in
      computer vision) to propose a new semantic distance metric
      between text objects, which we call <em>Ordered Word Mover's
      Distance</em>. Compared with the recently proposed Word
      Mover's Distance [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>], our new metric achieves significant
      improvement by taking the sequential structures of sentences
      into account. For example, without considering the order of
      words, the Word Mover's Distance between the sentences “Tom
      is chasing Jerry” and “Jerry is chasing Tom” is zero. In
      contrast, our new metric is able to penalize such order
      mismatch between words, and identify the difference between
      the two sentences.</p>
      <p><em>Third</em>, for supervised semantic matching, we
      extend the existing Siamese network architectures (both for
      CNN and LSTM) to multi-scaled models, where each scale adopts
      an individual Siamese network, taking as input the vector
      representations of the two sentences at the corresponding
      depth in the factorization trees, ranging from the
      coarse-grained scale to fine-grained scales. When increasing
      the number of layers in the corresponding neural network can
      hardly improve performance, hierarchical sentence
      factorization provides a novel means to extend the original
      deep networks to a “richer” model that matches a pair of
      sentences through a multi-scaled semantic unit matching
      process. Our proposed multi-scaled deep neural networks can
      effectively improve existing deep models by measuring the
      similarity between a pair of sentences at different semantic
      granularities. For instance, Siamese networks based on CNN
      and BiLSTM [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0024">24</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0036">36</a>] that
      originally only take the word sequences as the inputs.</p>
      <p>We extensively evaluate the performance of our proposed
      approaches on the task of semantic textual similarity
      estimation and paraphrase identification, based on multiple
      datasets, including the STSbenchmark dataset, the Microsoft
      Research Paraphrase identification (MSRP) dataset, the SICK
      dataset and the MSRvid dataset. Experimental results have
      shown that our proposed algorithms and models can achieve
      significant improvement compared with multiple existing
      unsupervised text distance metrics, such as the Word Mover's
      Distance [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0021">21</a>],
      as well as supervised deep neural network models, including
      Siamese Neural Network models based on CNN and BiLSTM
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0024">24</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0036">36</a>].</p>
      <p>The remainder of this paper is organized as follows.
      Sec.&nbsp;<a class="sec" href="#sec-8">2</a> presents our
      hierarchical sentence factorization algorithm.
      Sec.&nbsp;<a class="sec" href="#sec-10">3</a> presents our
      Ordered Word Mover's Distance metric based on sentence
      structural reordering. In Sec.&nbsp;<a class="sec" href=
      "#sec-11">4</a>, we propose our multi-scaled deep neural
      network architectures based on hierarchical sentence
      representation. In Sec.&nbsp;<a class="sec" href=
      "#sec-12">5</a>, we conduct extensive evaluations of the
      proposed methods based on multiple datasets on multiple
      tasks. Sec.&nbsp;<a class="sec" href="#sec-16">6</a> reviews
      the related literature. The paper is concluded in
      Sec.&nbsp;<a class="sec" href="#sec-17">7</a>.</p>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Hierarchical
          Sentence Factorization and Reordering</h2>
        </div>
      </header>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186022/images/www2018-31-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">An example of the sentence factorization
          process. Here we show: A. The original sentence pair; B.
          The procedures of creating sentence factorization trees;
          C. The predicate-argument form of original sentence pair;
          D. The alignment of semantic units with the reordered
          form.</span>
        </div>
      </figure>
      <p>In this section, we present our <em>Hierarchical Sentence
      Factorization</em> techniques to transform a sentence into a
      hierarchical tree structure, which also naturally produces a
      reordering of the sentence at the root node. This
      multi-scaled representation form proves to be effective at
      improving both unsupervised and supervised semantic matching,
      which will be discussed in Sec.&nbsp;<a class="sec" href=
      "#sec-10">3</a> and Sec.&nbsp;<a class="sec" href=
      "#sec-11">4</a>, respectively.</p>
      <p>We first describe our desired factorization tree structure
      before presenting the steps to obtain it. Given a natural
      language sentence <em>S</em>, our objective is to transform
      it into a semantic factorization tree denoted by <span class=
      "inline-equation"><span class="tex">$T^f_S$</span></span> .
      Each node in <span class="inline-equation"><span class=
      "tex">$T^f_S$</span></span> is called a <em>semantic
      unit</em>, which contains one or a few tokens (tokenized
      words) from the original sentence <em>S</em>, as illustrated
      in Fig.&nbsp;<a class="fig" href="#fig1">1</a> (<em>a</em>4),
      (<em>b</em>4). The tokens in every semantic unit in
      <span class="inline-equation"><span class=
      "tex">$T^f_S$</span></span> is re-organized into a
      “predicate-argument” form. For example, a semantic unit for
      “Tom catches Jerry” in the “predicate-argument” form will be
      “catch Tom Jerry”.</p>
      <p>Our proposed factorization tree recursively factorizes a
      sentence into a hierarchy of semantic units at different
      granularities to represent the semantic structure of that
      sentence. The root node in a factorization tree contains the
      entire sentence reordered in the predicate-argument form,
      thus providing a “normalized” representation for sentences
      expressed in different ways (e.g., passive vs. active
      tenses). Moreover, each semantic unit at depth <em>d</em>
      will be further split into several child nodes at depth
      <em>d</em> + 1, which are smaller semantic sub-units. Each
      sub-unit also follows the predicate-argument form.</p>
      <p>For example, in Fig.&nbsp;<a class="fig" href=
      "#fig1">1</a>, we convert sentence <em>A</em> into a
      hierarchical factorization tree (<em>a</em>4) using a series
      of operations. The root node of the tree contains the
      semantic unit “chase Tom Jerry little yard big”, which is the
      reordered representation of the original sentence “The little
      Jerry is being chased by Tom in the big yard” in a
      semantically normalized form. Moreover, the semantic unit at
      depth 0 is factorized into four sub-units at depth 1:
      “chase”, “Tom”, “Jerry little” and “yard big”, each in the
      “predicate-argument” form. And at depth 2, the semantic
      sub-unit “Jerry little” is further factorized into two
      sub-units “Jerry” and “little”. Finally, a semantic unit that
      contains only one token (e.g., “chase” and “Tom” at depth 1)
      can not be further decomposed. Therefore, it only has one
      child node at the next depth through self-duplication.</p>
      <p>We can observe that each depth of the tree contains all
      the tokens (except meaningless ones) in the original
      sentence, but re-organizes these tokens into semantic units
      of different granularities.</p>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Hierarchical Sentence Factorization</h3>
          </div>
        </header>
        <p>We now describe our detailed procedure to transform a
        natural language sentence to the desired factorization tree
        mentioned above. Our Hierarchical Sentence Factorization
        algorithm mainly consists of five steps: 1) AMR parsing and
        alignment, 2) AMR purification, 3) index mapping, 4) node
        completion, and 5) node traversal. The latter four steps
        are illustrated in the example in Fig.&nbsp;<a class="fig"
        href="#fig1">1</a> from left to right.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186022/images/www2018-31-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">An example of a sentence and
            its Abstract Meaning Representation (AMR), as well as
            the alignment between the words in the sentence and the
            nodes in AMR.</span>
          </div>
        </figure>
        <p></p>
        <p><strong>AMR parsing and alignment.</strong> Given an
        input sentence, the first step of our hierarchical sentence
        factorization algorithm is to acquire its Abstract Meaning
        Representation (AMR), as well as perform AMR-Sentence
        alignment to align the concepts in AMR with the tokens in
        the original sentence.</p>
        <p>Semantic parsing [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>] can be performed to generate the
        formal semantic representation of a sentence. Abstract
        Meaning Representation (AMR) [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>] is a semantic parsing language that
        represents a sentence by a directed acyclic graph (DAG).
        Each AMR graph can be converted into an AMR tree by
        duplicating the nodes that have more than one parent.</p>
        <p>Fig.&nbsp;<a class="fig" href="#fig2">2</a> shows the
        AMR of the sentence “I observed that the army moved
        quickly.” In an AMR graph, leaves are labeled with
        concepts, which represent either English words (e.g.,
        “army”), PropBank framesets (e.g., “observe-01”) [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0018">18</a>], or
        special keywords (e.g., dates, quantities, world regions,
        etc.). For example, “(a / army)” refers to an instance of
        the concept army, where “a” is the variable name of army
        (each entity in AMR has a variable name). “ARG0”, “ARG1”,
        “:manner” are different kinds of relations defined in AMR.
        Relations are used to link entities. For example, “:manner”
        links “m / move-01” and “q / quick”, which means “move in a
        quick manner”. Similarly, “:ARG0” links “m / move-01” and
        “a / army”, which means that “army” is the first argument
        of “move”.</p>
        <p>Each leaf in AMR is a concept rather than the original
        token in a sentence. The alignment between a sentence and
        its AMR graph is not given in the AMR annotation.
        Therefore, AMR alignment [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>] needs to be performed to link the
        leaf nodes in the AMR to tokens in the original sentence.
        Fig.&nbsp;<a class="fig" href="#fig2">2</a> shows the
        alignment between sentence tokens and AMR concepts by the
        alignment indexes. The alignment index 0 is for the root
        node, 0.0 for the first child of the root node, 0.1 for the
        second child of the root node, and so forth. For example,
        in Fig.&nbsp;<a class="fig" href="#fig2">2</a>, the word
        “army” in sentence is linked with index “0.1.0”, which
        represents the concept node “a / army” in its AMR. We refer
        interested readers to [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>] for more detailed description about
        AMR.</p>
        <p>Various parsers have been proposed for AMR parsing and
        alignment [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0039">39</a>]. We choose the JAMR parser
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0013">13</a>] in our
        algorithm implementation.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186022/images/www2018-31-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">An example to show the
            operation of AMR purification.</span>
          </div>
        </figure>
        <p></p>
        <p><strong>AMR purification.</strong> Unfortunately, AMR
        itself cannot be used to form the desired factorization
        tree. First, it is likely that multiple concepts in AMR may
        link to the same token in the sentence. For example,
        Fig.&nbsp;<a class="fig" href="#fig3">3</a> shows AMR and
        its alignment for the sentence “Three Asian kids are
        dancing.”. The token “Asian” is linked to four concepts in
        the AMR graph: “ continent (0.0.0)”, “name (0.0.0.0)”,
        “Asia (0.0.0.0.0)” and “wiki Asia (0.0.0.1)”. This is
        because AMR will match a named entity with predefined
        concepts which it belongs to, such as “c / continent” for
        “Asia”, and form a compound representation of the entity.
        For example, in Fig. <a class="fig" href="#fig3">3</a>, the
        token “Asian” is represented as a continent whose name is
        Asia, and its Wikipedia entity name is also Asia.</p>
        <p>In this case, we select the link index with the smallest
        tree depth as the token's position in the tree. Suppose
        <span class="inline-equation"><span class="tex">$\mathcal
        {P}_w = \lbrace p_1, p_2, \cdots , p_{|\mathcal
        {P}|}\rbrace$</span></span> denotes the set of alignment
        indexes of token <em>w</em>. We can get the desired
        alignment index of <em>w</em> by calculating the longest
        common prefix of all the index strings in <span class=
        "inline-equation"><span class="tex">$\mathcal
        {P}_w$</span></span> . After getting the alignment index
        for each token, we then replace the concepts in AMR with
        the tokens in sentence by the alignment indexes, and remove
        relation names (such as “:ARG0”) in AMR, resulting into a
        compact tree representation of the original sentence, as
        shown in the right part of Fig.&nbsp;<a class="fig" href=
        "#fig3">3</a>.</p>
        <p><strong>Index mapping.</strong> A purified AMR tree for
        a sentence obtained in the previous step is still not in
        our desired form. To transform it into a hierarchical
        sentence factorization tree, we perform index mapping and
        calculate a new position (or index) for each token in the
        desired factorization tree given its position (or index) in
        the purified AMR tree. Fig.&nbsp;<a class="fig" href=
        "#fig1">1</a> illustrates the process of index mapping.
        After this step, for example, the purified AMR trees in
        Fig.&nbsp;<a class="fig" href="#fig1">1</a> (<em>a</em>1)
        and (<em>b</em>1) will be transformed into (<em>a</em>2)
        and (<em>b</em>2).</p>
        <p>Specifically, let <span class=
        "inline-equation"><span class="tex">$T^p_S$</span></span>
        denote a purified AMR tree of sentence <em>S</em>, and
        <span class="inline-equation"><span class=
        "tex">$T^f_S$</span></span> our desired sentence
        factorization tree of <em>S</em>. Let <span class=
        "inline-equation"><span class="tex">$I^p_N =
        \overline{i_0.i_1.i_2.\cdots .i_d}$</span></span> denote
        the index of node <em>N</em> in <span class=
        "inline-equation"><span class="tex">$T^p_S$</span></span> ,
        where <em>d</em> is the depth of <em>N</em> in <span class=
        "inline-equation"><span class="tex">$T^p_S$</span></span>
        (where depth 0 represents the root of a tree). Then, the
        index <span class="inline-equation"><span class=
        "tex">$I^f_N$</span></span> of node <em>N</em> in our
        desired factorization tree <span class=
        "inline-equation"><span class="tex">$T^f_S$</span></span>
        will be calculated as follows:</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} I^f_N :=
            {{\lt}CASE-START{\gt}\left\lbrace
            \begin{array}{@{}l@{\quad }l@{}}\overline{0.0} &amp;
            \quad \text{if } d=0, \\
            \overline{i_0.(i_1+1).(i_2+1).\cdots .(i_d + 1)} &amp;
            \quad \text{otherwise}.
            \end{array}\right.{\lt}CASE-END{\gt}}
            \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>After index mapping, we add an empty root node with
        index 0 in the new factorization tree, and link all nodes
        at depth 1 to it as its child nodes. Note that the
        <em>i</em> <sub>0</sub> in every node index will always be
        0.
        <p></p>
        <p><strong>Node completion.</strong> We then perform node
        completion to make sure each branch of the factorization
        tree have the same maximum depth and to fill in the missing
        nodes caused by index mapping, illustrated by
        Fig.&nbsp;<a class="fig" href="#fig1">1</a> (<em>a</em>3)
        and (<em>b</em>3).</p>
        <p>First, given a pre-defined maximum depth <em>D</em>, for
        each leaf node <em>N<sup>l</sup></em> with depth <em>d</em>
        &lt; <em>D</em> in the current <span class=
        "inline-equation"><span class="tex">$T^f_S$</span></span>
        after index mapping, we duplicate it for <em>D</em> −
        <em>d</em> times and append all of them sequentially to
        <em>N<sup>l</sup></em> , as shown in Fig.&nbsp;<a class=
        "fig" href="#fig1">1</a> (<em>a</em>3), (<em>b</em>3), such
        that the depths of the ending nodes will always be
        <em>D</em>. For example, in Fig.&nbsp;<a class="fig" href=
        "#fig1">1</a> with <em>D</em> = 2, the node “chase (0.0)”
        and “Tom (0.1)” will be extended to reach depth 2 via
        self-duplication.</p>
        <p>Second, after index mapping, the children of all the
        non-leaf nodes, except the root node, will be indexed
        starting from 1 rather than 0. For example, in
        Fig.&nbsp;<a class="fig" href="#fig1">1</a> (<em>a</em>2),
        the first child node of “Jerry (0.2)” is “little (0.2.1)”.
        In this case, we duplicate “Jerry (0.2)” itself to “Jerry
        (0.2.0)” to fill in the missing first child of “Jerry
        (0.2)”. Similar filling operations are done for other
        non-leaf nodes after index mapping as well.</p>
        <p><strong>Node traversal to complete semantic
        units</strong>. Finally, we complete each semantic unit in
        the formed factorization tree via node traversal, as shown
        in Fig.&nbsp;<a class="fig" href="#fig1">1</a>
        (<em>a</em>4), (<em>b</em>4). For each non-leaf node
        <em>N</em>, we traverse its sub-tree by Depth First Search
        (DFS). The original semantic unit in <em>N</em> will then
        be replaced by the concatenation of the semantic units of
        all the nodes in the sub-tree rooted at <em>N</em>,
        following the order of traversal.</p>
        <p>For example, for sentence <em>A</em> in
        Fig.&nbsp;<a class="fig" href="#fig1">1</a>, after node
        traversal, the root node of the factorization tree becomes
        “chase Tom Jerry little yard big” with index “0”. We can
        see that the original sentence has been reordered into a
        predicate-argument structure. A similar structure is
        generated for the other nodes at different depths. Until
        now, each depth of the factorization tree <span class=
        "inline-equation"><span class="tex">$T^f_S$</span></span>
        can express the full sentence <em>S</em> in terms of
        semantic units at different granularity.</p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Ordered Word
          Mover's Distance</h2>
        </div>
      </header>
      <figure id="fig4">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186022/images/www2018-31-fig4.jpg"
        class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class=
          "figure-title">Compare the sentence matching results
          given by Word Mover's Distance and Ordered Word Mover's
          Distance.</span>
        </div>
      </figure>
      <p>The proposed hierarchical sentence factorization technique
      naturally reorders an input sentence into a unified format at
      the root node. In this section, we introduce the <em>Ordered
      Word Mover's Distance</em> metric which measures the semantic
      distance between two input sentences based on the unified
      representation of reordered sentences.</p>
      <p>Assume <span class="inline-equation"><span class=
      "tex">$X\in \mathbb {R}^{d\times n}$</span></span> is a
      <em>word2vec</em> embedding matrix for a vocabulary of
      <em>n</em> words, and the <em>i</em>-th column <span class=
      "inline-equation"><span class="tex">$\mathbf {x}_i \in
      \mathbb {R}^d$</span></span> represents the
      <em>d</em>-dimensional embedding vector of <em>i</em>-th word
      in vocabulary. Denote a sentence <span class=
      "inline-equation"><span class="tex">$S =
      \overline{a_1a_2\cdots a_K}$</span></span> where
      <em>a<sub>i</sub></em> represents the <em>i</em>-th word (or
      the word embedding vector). The Word Mover's Distance
      considers a sentence <em>S</em> as its normalized
      bag-of-words (nBOW) vectors where the weights of the words in
      <em>S</em> is <strong><em>α</em></strong> = {<em>α</em>
      <sub>1</sub>, <em>α</em> <sub>2</sub>, ⋅⋅⋅,
      <em>α<sub>K</sub></em> }. Specifically, if word
      <em>a<sub>i</sub></em> appears <em>c<sub>i</sub></em> times
      in <em>S</em>, then <span class=
      "inline-equation"><span class="tex">$\alpha _i =
      \frac{c_i}{\sum _{j=1}^K c_j}$</span></span> .</p>
      <p>The Word Mover's Distance metric combines the normalized
      bag-of-words representation of sentences with Wasserstein
      distance (also known as Earth Mover's Distance [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0033">33</a>]) to
      measure the semantic distance between two sentences. Given a
      pair of sentences <span class="inline-equation"><span class=
      "tex">$S_1 = \overline{a_1a_2\cdots a_M}$</span></span> and
      <span class="inline-equation"><span class="tex">$S_2 =
      \overline{b_1b_2\cdots b_N}$</span></span> , where
      <span class="inline-equation"><span class="tex">$b_j \in
      \mathbb {R}^d$</span></span> is the embedding vector of the
      <em>j</em>-th word in <em>S</em> <sub>2</sub>. Let
      <strong><em>α</em></strong> = {<em>α</em> <sub>1</sub>, ⋅⋅⋅,
      <em>α<sub>M</sub></em> } and <strong><em>β</em></strong> =
      {<em>β</em> <sub>1</sub>, ⋅⋅⋅, <em>β<sub>N</sub></em> }
      represents the normalized bag-of-words vectors of <em>S</em>
      <sub>1</sub> and <em>S</em> <sub>2</sub>. We can calculate a
      distance matrix <span class="inline-equation"><span class=
      "tex">$D \in \mathbb {R}^{M\times N}$</span></span> where
      each element <em>D<sub>ij</sub></em> =
      ‖<em>a<sub>i</sub></em> − <em>b<sub>j</sub></em>
      ‖<sub>2</sub> measures the distance between word
      <em>a<sub>i</sub></em> and <em>b<sub>j</sub></em> (we use the
      same notation to denote the word itself or its word vector
      representation). Let <span class=
      "inline-equation"><span class="tex">$T \in \mathbb
      {R}^{M\times N}$</span></span> be a non-negative sparse
      transport matrix where <em>T<sub>ij</sub></em> denotes the
      portion of word <em>a<sub>i</sub></em> ∈ <em>S</em>
      <sub>1</sub> that transports to word <em>b<sub>j</sub></em> ∈
      <em>S</em> <sub>2</sub>. The Word Mover's Distance between
      sentences <em>S</em> <sub>1</sub> and <em>S</em> <sub>2</sub>
      is given by ∑ <sub><em>i</em>, <em>j</em></sub>
      <em>T<sub>ij</sub>D<sub>ij</sub></em> . The transport matrix
      <em>T</em> is computed solving the following constrained
      optimization problem:</p>
      <div class="table-responsive" id="eq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          \underset{T \in \mathbb {R}_{+}^{M\times
          N}}{\mbox{minimize}}\quad &amp; \sum _{i,j} T_{ij} D_{ij}
          \\ \mbox{subject to}\quad &amp; \sum \limits _{i = 1}^{M}
          T_{ij} = \beta _j \quad 1\le j \le N,\\ &amp; \sum
          \limits _{j = 1}^{N} T_{ij} = \alpha _i \quad 1\le i \le
          M. \end{split} \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>Where the minimum “word travel cost” between two bags
      of words for a pair of sentences is calculated to measure the
      their semantic distance.
      <p></p>
      <p>However, the Word Mover's Distance fails to consider a few
      aspects of natural language. First, it omits the sequential
      structure. For example, in Fig.&nbsp;<a class="fig" href=
      "#fig4">4</a>, the pair of sentences “Morty is laughing at
      Rick” and “Rick is laughing at Morty” only differ in the
      order of words. The Word Mover's Distance metric will then
      find an exact match between the two sentences and estimate
      the semantic distance as zero, which is obviously false.
      Second, the normalized bag-of-words representation of a
      sentence can not distinguish duplicated words shown in
      multiple positions of a sentence.</p>
      <p>To overcome the above challenges, we propose a new kind of
      semantic distance metric named Ordered Word Mover's Distance
      (OWMD). The Ordered Word Mover's Distance combines our
      sentence factorization technique with Order-preserving
      Wasserstein Distance proposed in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>]. It casts the calculation
      of semantic distance between texts as an optimal transport
      problem while preserving the sequential structure of words in
      sentences. The Ordered Word Mover's Distance differs from the
      Word Mover's Distance in multiple aspects.</p>
      <p>First, rather than using normalized bag-of-words vector to
      represent a sentence, we decompose and re-organize a sentence
      using the sentence factorization algorithm described in
      Sec.&nbsp;<a class="sec" href="#sec-8">2</a>. Given a
      sentence <em>S</em>, we represent it by the reordered word
      sequence <em>S</em>′ in the root node of its sentence
      factorization tree. Such representation normalizes a sentence
      into “predicate-argument” structure to better handle
      syntactic variations. For example, after performing sentence
      factorization, sentences “Tom is chasing Jerry” and “Jerry is
      being chased by Tom” will both be normalized as “chase Tom
      Jerry”.</p>
      <p>Second, we calculate a new transport matrix <em>T</em> by
      solving the following optimization problem</p>
      <div class="table-responsive" id="eq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          \underset{T \in \mathbb {R}_{+}^{M\times
          N}}{\mbox{minimize}}\quad &amp; \sum _{i,j} T_{ij} D_{ij}
          - \lambda _1 I(T) + \lambda _2 KL(T||P)\\ \mbox{subject
          to}\quad &amp; \sum \limits _{i = 1}^{M} T_{ij} = \beta
          _j^{\prime } \quad 1\le j \le N^{\prime },\\ &amp; \sum
          \limits _{j = 1}^{N} T_{ij} = \alpha _i^{\prime } \quad
          1\le i \le M^{\prime }, \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>where <em>λ</em> <sub>1</sub> &gt; 0 and <em>λ</em>
      <sub>2</sub> &gt; 0 are two hyper parameters. <em>M</em>′ and
      <em>N</em>′ denotes the number of words in <span class=
      "inline-equation"><span class="tex">$S_1^{\prime
      }$</span></span> and <span class=
      "inline-equation"><span class="tex">$S_2^{\prime
      }$</span></span> . <span class="inline-equation"><span class=
      "tex">$\alpha _i^{\prime }$</span></span> denotes the weight
      of the <em>i</em>-th word in normalized sentence <span class=
      "inline-equation"><span class="tex">$S_1^{\prime
      }$</span></span> and <span class=
      "inline-equation"><span class="tex">$\beta _j^{\prime
      }$</span></span> denotes the weight of the <em>j</em>-th word
      in normalized sentence <span class=
      "inline-equation"><span class="tex">$S_2^{\prime
      }$</span></span> . Usually we can set <span class=
      "inline-equation"><span class="tex">$\mathbf {\alpha ^{\prime
      }} = (\frac{1}{M^{\prime }}, \cdots , \frac{1}{M^{\prime
      }})$</span></span> and <span class=
      "inline-equation"><span class="tex">$\mathbf {\beta ^{\prime
      }} = (\frac{1}{N^{\prime }}, \cdots , \frac{1}{N^{\prime
      }})$</span></span> without any prior knowledge of word
      differences.
      <p></p>
      <p>The first penalty term <em>I</em>(<em>T</em>) is the
      inverse difference moment [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>] of the transport matrix <em>T</em>
      that measures local homogeneity of <em>T</em>. It is defined
      as:</p>
      <div class="table-responsive" id="eq4">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          I(T) = \sum \limits _{i=1}^{M^{\prime }} \sum \limits
          _{j=1}^{N^{\prime }} \frac{T_{ij}}{(\frac{i}{M^{\prime }}
          - \frac{j}{N^{\prime }})^2 + 1}. \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(4)</span>
        </div>
      </div><em>I</em>(<em>T</em>) will have a relatively large
      value if the large values of <em>T</em> mainly appear near
      its diagonal.
      <p></p>
      <p>Another penalty term <em>KL</em>(<em>T</em>||<em>P</em>)
      denotes the KL-divergence between <em>T</em> and <em>P</em>.
      <em>P</em> is a two-dimensional distribution used as the
      prior distribution for values in <em>T</em>. It is defined
      as</p>
      <div class="table-responsive" id="eq5">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          P_{ij} = \frac{1}{\sigma \sqrt {2\pi }}e^{-
          \frac{l^2(i,j)}{2\sigma ^2}} \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>where <em>l</em>(<em>i</em>, <em>j</em>) is the
      distance from position (<em>i</em>, <em>j</em>) to the
      diagonal line, which is calculated as
      <div class="table-responsive" id="eq6">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          l(i, j) = \frac{|i/M^{\prime } - j/N^{\prime }|}{\sqrt
          {1/M^{\prime 2} + 1/N^{\prime 2}}}. \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(6)</span>
        </div>
      </div>As we can see, the farther a word in one sentence is
      from the other word in another sentence in terms of word
      orders, the less likely it will be transported to that word.
      Therefore, by introducing the two penalty terms
      <em>I</em>(<em>T</em>) and
      <em>KL</em>(<em>T</em>||<em>P</em>) into
      problem&nbsp;(<a class="eqn" href="#eq3">3</a>), we encourage
      words at similar positions in two sentences to be matched.
      Words at distant positions are less likely to be matched by
      <em>T</em>.
      <p></p>
      <p>The problem (<a class="eqn" href="#eq3">3</a>) has a
      unique optimal solution <span class=
      "inline-equation"><span class="tex">$T^{\lambda _1, \lambda
      _2}$</span></span> since both the objective and the feasible
      set are convex. It has been proved in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>] that the optimal
      <span class="inline-equation"><span class="tex">$T^{\lambda
      _1, \lambda _2}$</span></span> has the same form with
      <em>diag</em>(<strong>k</strong> <sub>1</sub>) ·
      <strong>K</strong> · <em>diag</em>(<strong>k</strong>
      <sub>2</sub>), where <span class=
      "inline-equation"><span class="tex">$diag(\mathbf {k}_1) \in
      \mathbb {R}^{M^{\prime }}$</span></span> and <span class=
      "inline-equation"><span class="tex">$diag(\mathbf {k}_2) \in
      \mathbb {R}^{N^{\prime }}$</span></span> are two diagonal
      matrices with strictly positive diagonal elements.
      <span class="inline-equation"><span class="tex">$\mathbf {K}
      \in \mathbb {R}^{M^{\prime }\times N^{\prime
      }}$</span></span> is a matrix defined as</p>
      <div class="table-responsive" id="eq7">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          K_{ij} = P_{ij} e^{\frac{1}{\lambda _2}(S_{ij}^{\lambda
          _1} - D_{ij})}, \end{split} \end{equation}</span><br />
          <span class="equation-number">(7)</span>
        </div>
      </div>where
      <div class="table-responsive" id="eq8">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          S_{ij} = \frac{\lambda _1}{(\frac{i}{M^{\prime }} -
          \frac{j}{N^{\prime }})^2 + 1}. \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(8)</span>
        </div>
      </div>The two matrices <strong>k</strong> <sub>1</sub> and
      <strong>k</strong> <sub>2</sub> can be efficiently obtained
      by the Sinkhorn-Knopp iterative matrix scaling algorithm
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>]:
      <div class="table-responsive" id="eq9">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split}
          \mathbf {k}_1 &amp;\leftarrow \mathbf {\alpha ^{\prime }}
          ./ K \mathbf {k}_2, \\ \mathbf {k}_2 &amp;\leftarrow
          \mathbf {\beta ^{\prime }} ./ K^T \mathbf {k}_1.
          \end{split} \end{equation}</span><br />
          <span class="equation-number">(9)</span>
        </div>
      </div>where ./ is the element-wise division operation.
      Compared with Word Mover's Distance, the Ordered Word Mover's
      Distance considers the positions of words in a sentence, and
      is able to distinguish duplicated words at different
      locations. For example, in Fig.&nbsp;<a class="fig" href=
      "#fig4">4</a>, while the WMD finds an exact match and get a
      semantic distance of zero for the sentence pair “Morty is
      laughing at Rick” and “Rick is laughing at Morty”, the OWMD
      metric is able to find a better match relying on the penalty
      terms, and gives a semantic distance greater than zero.
      <p></p>
      <p>The computational complexity of OWMD is also effectively
      reduced compared to WMD. With the additional constraints, the
      time complexity is <em>O</em>(<em>dM</em>′<em>N</em>′) where
      <em>d</em> is the dimension of word vectors [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>], while it is
      <em>O</em>(<em>dp</em> <sup>3</sup>log <em>p</em>) for WMD,
      where <em>p</em> denotes the number of uniques words in
      sentences or documents [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>].</p>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Multi-scale
          Sentence Matching</h2>
        </div>
      </header>
      <figure id="fig5">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186022/images/www2018-31-fig5.jpg"
        class="img-responsive" alt="Figure 5" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 5:</span> <span class=
          "figure-title">Extend the Siamese network architecture
          for sentence matching by feeding into the multi-scale
          representations of sentence pairs.</span>
        </div>
      </figure>
      <p>Our sentence factorization algorithm parses a sentence
      <em>S</em> into a hierarchical factorization tree
      <span class="inline-equation"><span class=
      "tex">$T^f_S$</span></span> , where each depth of
      <span class="inline-equation"><span class=
      "tex">$T^f_S$</span></span> contains the semantic units of
      the sentence at a different granularity. In this section, we
      exploit this multi-scaled representation of <em>S</em>
      present in <span class="inline-equation"><span class=
      "tex">$T^f_S$</span></span> to propose a multi-scaled Siamese
      network architecture that can extend any existing CNN or
      RNN-based Siamese architectures to leverage the hierarchical
      representation of sentence semantics.</p>
      <p>Fig.&nbsp;<a class="fig" href="#fig5">5</a> (a) shows the
      network architecture of the popular Siamese
      “matching-aggregation” framework [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0025">25</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0035">35</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0040">40</a>] for sentence matching
      tasks. The matching process is usually performed as follows:
      First, the sequence of word embeddings in two sentences will
      be encoded by a context representation layer, which usually
      contains one or multiple layers of LSTM, bi-directional LSTM
      (BiLSTM), or CNN with max pooling layers. The goal is to
      capture the contextual information of each sentence into a
      context vector. In a Siamese network, every sentence is
      encoded by the same context representation layer. Second, the
      context vectors of two sentences will be concatenated in the
      aggregation layer. They may be further transformed by more
      layers of neural network to get a fixed length matching
      vector. Finally, a prediction layer will take in the matching
      vector and outputs a similarity score for the two sentences
      or the probability distribution over different sentence-pair
      relationships.</p>
      <p>Compared with the typical Siamese network shown in
      Fig.&nbsp;<a class="fig" href="#fig5">5</a> (a), our proposed
      architecture shown in Fig.&nbsp;<a class="fig" href=
      "#fig5">5</a> (b) differs in two aspects. First, our network
      contains three Siamese sub-modules that are similar to (a).
      They correspond to the factorized representations from depth
      0 (the root layer) to depth 2. We only select the semantic
      units from the top 3 depths of the factorization tree as our
      input, because usually most semantic units at depth 2 are
      already single words and can not be further factorized.
      Second, for each Siamese sub-module in our network
      architecture, the input is not the embedding vectors of words
      from the original sentences. Instead, we use semantic units
      at different depths of sentence factorization tree for
      matching. We sum up the embedding vectors of the words
      contained in a semantic unit to represent that unit. Assuming
      each semantic unit at depth <em>d</em> can be factorized into
      <em>k</em> semantic sub-units at depth <em>d</em> + 1. If a
      semantic unit has less than <em>k</em> sub-units, we add
      empty units as its child node to make each non-leaf node in a
      factorization tree has exactly <em>k</em> child nodes. The
      empty units are embedded with a vector of zeros. After this
      procedure, the number of semantic units at depth <em>d</em>
      of a sentence factorization tree is <em>k<sup>d</sup></em>
      .</p>
      <p>Taking Fig.&nbsp;<a class="fig" href="#fig1">1</a> as an
      example. We set <em>k</em> = 4 in Fig.&nbsp;<a class="fig"
      href="#fig1">1</a>. For sentence A “The little Jerry is being
      chased by Tom in the big yard”, the input at depth 0 is the
      sum of word embedding {chase, Tom, Jerry, little, yard, big}.
      The input at depth 1 are the embedding vectors of four
      semantic units: {chase, Tome, Jerry little, yard big}.
      Finally, at depth 2, the semantic units are {chase, -, -, -,
      Tom, -, -, -, Jerry, little, -, -, yard, big, -, -}, where “
      − ” denotes an empty unit.</p>
      <p>As we can see, based on this factorized sentence
      representation, our network architecture explicitly matches a
      pair of sentences at several semantic granularities. In
      addition, we align the semantic units in two sentences by
      mapping their positions in the tree to the corresponding
      indices in the input layer of the neural network. For
      example, as shown in Fig.&nbsp;<a class="fig" href=
      "#fig1">1</a>, the semantic units at depth 2 are aligned
      according to their unit indices: “chase” matches with
      “catch”, “Tom” matches with “cat blue”, “Jerry little”
      matches with “mouse brown”, and “yard big” matches with
      “forecourt”.</p>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Evaluation</h2>
        </div>
      </header>
      <p>In this section, we evaluate the performance of our
      unsupervised Ordered Word Mover's Distance metric and
      supervised Multi-scale Sentence Matching model with
      factorized sentences as input. We apply our algorithms to
      semantic textual similarity estimation tasks and sentence
      pair paraphrase identification tasks, based on four datasets:
      STSbenchmark, SICK, MSRP and MSRvid.</p>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span>
            Experimental Setup</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Description of evaluation
            datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Dataset</th>
                <th style="text-align:left;">Task</th>
                <th style="text-align:left;">Train</th>
                <th style="text-align:left;">Dev</th>
                <th style="text-align:left;">Test</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">STSbenchmark</td>
                <td style="text-align:left;">Similarity
                scoring</td>
                <td style="text-align:left;">5748</td>
                <td style="text-align:left;">1500</td>
                <td style="text-align:left;">1378</td>
              </tr>
              <tr>
                <td style="text-align:left;">SICK</td>
                <td style="text-align:left;">Similarity
                scoring</td>
                <td style="text-align:left;">4500</td>
                <td style="text-align:left;">500</td>
                <td style="text-align:left;">4927</td>
              </tr>
              <tr>
                <td style="text-align:left;">MSRP</td>
                <td style="text-align:left;">Paraphrase
                identification</td>
                <td style="text-align:left;">4076</td>
                <td style="text-align:left;">-</td>
                <td style="text-align:left;">1725</td>
              </tr>
              <tr>
                <td style="text-align:left;">MSRvid</td>
                <td style="text-align:left;">Similarity
                scoring</td>
                <td style="text-align:left;">750</td>
                <td style="text-align:left;">-</td>
                <td style="text-align:left;">750</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>We will start with a brief description for each
        dataset:</p>
        <ul class="list-no-style">
          <li id="list1" label="•">
            <strong>STSbenchmark</strong>[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0009">9</a>]: it
            is a dataset for semantic textual similarity (STS)
            estimation. The task is to assign a similarity score to
            each sentence pair on a scale of 0.0 to 5.0, with 5.0
            being the most similar.<br />
          </li>
          <li id="list2" label="•">
            <strong>SICK</strong>[<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0022">22</a>]: it is another STS
            dataset from the SemEval 2014 task 1. It has the same
            scoring mechanism as STSbenchmark, where 0.0 represents
            the least amount of relatedness and 5.0 represents the
            most.<br />
          </li>
          <li id="list3" label="•"><strong>MSRvid</strong>: the
          Microsoft Research Video Description Corpus contains 1500
          sentences that are concise summaries on the content of a
          short video. Each pair of sentences is also assigned a
          semantic similarity score between 0.0 and 5.0.<br /></li>
          <li id="list4" label="•">
            <strong>MSRP</strong>[<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0031">31</a>]: the Microsoft Research
            Paraphrase Corpus is a set of 5800 sentence pairs
            collected from news articles on the Internet. Each
            sentence pair is labeled 0 or 1, with 1 indicating that
            the two sentences are paraphrases of each other.<br />
          </li>
        </ul>
        <p>Table <a class="tbl" href="#tab1">1</a> shows a detailed
        breakdown of the datasets used in evaluation. For
        STSbenchmark dataset we use the provided train/dev/test
        split. The SICK dataset does not provide development set
        out of the box, so we extracted 500 instances from the
        training set as the development set. For MSRP and MSRvid,
        since their sizes are relatively small to begin with, we
        did not create any development set for them.</p>
        <p>One metric we used to evaluate the performance of our
        proposed models on the task of semantic textual similarity
        estimation is the Pearson Correlation coefficient, commonly
        denoted by <em>r</em>. Pearson Correlation is defined
        as:</p>
        <div class="table-responsive" id="eq10">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} r = cov(X,Y)
            /(\sigma _X \sigma _Y), \end{equation}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>where <em>cov</em>(<em>X</em>, <em>Y</em>) is the
        co-variance between distributions X and Y, and
        <em>σ<sub>X</sub></em> , <em>σ<sub>Y</sub></em> are the
        standard deviations of X and Y. The Pearson Correlation
        coefficient can be thought as a measure of how well two
        distributions fit on a straight line. Its value has range
        [-1, 1], where a value of 1 indicates that data points from
        two distribution lie on the same line with a positive
        slope.
        <p></p>
        <p>Another metric we utilized is the Spearman's Rank
        Correlation coefficient. Commonly denoted by
        <em>r<sub>s</sub></em> , the Spearman's Rank Correlation
        coefficient shares a similar mathematical expression with
        the Pearson Correlation coefficient, but it is applied to
        ranked variables. Formally it is defined as [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0042">42</a>]:</p>
        <div class="table-responsive" id="eq11">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \rho =
            cov(rg_X, rg_Y) / (\sigma _{rg_X} \sigma _{rg_Y}),
            \end{equation}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>where <em>rg<sub>X</sub></em> ,
        <em>rg<sub>Y</sub></em> denotes the ranked variables
        derived from <em>X</em> and <em>Y</em>.
        <em>cov</em>(<em>rg<sub>X</sub></em> ,
        <em>rg<sub>Y</sub></em> ), <span class=
        "inline-equation"><span class="tex">$\sigma
        _{rg_X}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\sigma
        _{rg_Y}$</span></span> corresponds to the co-variance and
        standard deviations of the rank variables. The term ranked
        simply means that each instance in X is ranked higher or
        lower against every other instances in X and the same for
        Y. We then compare the rank values of X and Y with
        <a class="eqn" href="#eq11">11</a>. Like the Pearson
        Correlation coefficient, the Spearman's Rank Correlation
        coefficient has an output range of [-1, 1], and it measures
        the monotonic relationship between X and Y. A Spearman's
        Rank Correlation value of 1 implies that as X increases, Y
        is guaranteed to increase as well. The Spearman's Rank
        Correlation is also less sensitive to noise created by
        outliers compared to the Pearson Correlation.
        <p></p>
        <p>For the task of paraphrase identification, the
        classification accuracy of label 1 and the F1 score are
        used as metrics.</p>
        <p>In the supervised learning portion, we conduct the
        experiments on the aforementioned four datasets. We use
        training sets to train the models, development set to tune
        the hyper-parameters and each test set is only used once in
        the final evaluation. For datasets without any development
        set, we will use cross-validation in the training process
        to prevent overfitting, that is, use 10% of the training
        data for validation and the rest is used in training. For
        each model, we carry out training for 10 epochs. We then
        choose the model with the best validation performance to be
        evaluated on the test set.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span>
            Unsupervised Matching with OWMD</h3>
          </div>
        </header>
        <p>To evaluate the effectiveness of our Ordered Word
        Mover's Distance metric, we first take an unsupervised
        approach towards the similarity estimation task on the
        STSbenchmark, SICK and MSRvid datasets. Using the distance
        metrics listed in Table <a class="tbl" href="#tab2">2</a>
        and <a class="tbl" href="#tab3">3</a>, we first computed
        the distance between two sentences, then calculated the
        Pearson Correlation coefficients and the Spearman's Rank
        Correlation coefficients between all pair's distances and
        their labeled scores. We did not use the MSRP dataset since
        it is a binary classification problem.</p>
        <p>In our proposed Ordered Word Mover's Distance metric,
        distance between two sentences is calculated using the
        order preserving Word Mover's Distance algorithm. For all
        three datasets, we performed hyper-parameter tuning using
        the training set and calculated the Pearson Correlation
        coefficients on the test and development set. We found that
        for the STSbenchmark dataset, setting <em>λ</em>
        <sub>1</sub> = 10, <em>λ</em> <sub>2</sub> = 0.03 produces
        the most optimal result. For the SICK dataset, a
        combination of <em>λ</em> <sub>1</sub> = 3.5, <em>λ</em>
        <sub>2</sub> = 0.015 works best. And for the MSRvid
        dataset, the highest Pearson Correlation is attained when
        <em>λ</em> <sub>1</sub> = 0.01, <em>λ</em> <sub>2</sub> =
        0.02. We maintain a max iteration of 20 since in our
        experiments we found that it is sufficient for the
        correlation result to converge. During hyper-parameter
        tuning we discovered that using the Euclidean metric along
        with <em>σ</em> = 10 produces better results, so all OWMD
        results summarized in Table <a class="tbl" href=
        "#tab2">2</a> and <a class="tbl" href="#tab3">3</a> are
        acquired under these parameter settings. Finally, it is
        worth mentioning that our OWMD metric calculates the
        distances using factorized versions of sentences, while all
        other metrics use the original sentences. Sentence
        factorization is a necessary preprocessing step for the
        OWMD metric.</p>
        <p>We compared the performance of Ordered Word Mover's
        Distance metric with the following methods:</p>
        <ul class="list-no-style">
          <li id="list5" label="•"><strong>Bag-of-Words
          (BoW)</strong>: in the Bag-of-Words metric, distance
          between two sentences is computed as the cosine
          similarity between the word counts of the
          sentences.<br /></li>
          <li id="list6" label="•">
            <strong>LexVec</strong>&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0034">34</a>]:
            calculate the cosine similarity between the averaged
            300-dimensional LexVec word embedding of the two
            sentences.<br />
          </li>
          <li id="list7" label="•">
            <strong>GloVe</strong>&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0028">28</a>]:
            calculate the cosine similarity between the averaged
            300-dimensional GloVe 6B word embedding of the two
            sentences.<br />
          </li>
          <li id="list8" label="•">
            <strong>Fastext</strong>&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0017">17</a>]:
            calculate the cosine similarity between the averaged
            300-dimensional Fastext word embedding of the two
            sentences.<br />
          </li>
          <li id="list9" label="•">
            <strong>Word2vec</strong>&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0023">23</a>]:
            calculate the cosine similarity between the averaged
            300-dimensional Word2vec word embedding of the two
            sentences.<br />
          </li>
          <li id="list10" label="•">
            <strong>Word Mover's Distance
            (WMD)</strong>&nbsp;[<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0021">21</a>]: estimating the semantic
            distance between two sentences by WMD introduced in
            Sec.&nbsp;<a class="sec" href="#sec-10">3</a>.<br />
          </li>
        </ul>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Pearson Correlation results on different
            distance metrics.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Algorithm</th>
                <th style="text-align:center;" colspan="2">
                  STSbenchmark
                  <hr />
                </th>
                <th style="text-align:center;" colspan="2">
                  SICK
                  <hr />
                </th>
                <th>MSRvid</th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">Test</th>
                <th style="text-align:center;">Dev</th>
                <th style="text-align:center;">Test</th>
                <th style="text-align:center;">Dev</th>
                <th>Test</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">BoW</td>
                <td style="text-align:center;">0.5705</td>
                <td style="text-align:center;">0.6561</td>
                <td style="text-align:center;">0.6114</td>
                <td style="text-align:center;">0.6087</td>
                <td>0.5044</td>
              </tr>
              <tr>
                <td style="text-align:center;">LexVec</td>
                <td style="text-align:center;">0.5759</td>
                <td style="text-align:center;">0.6852</td>
                <td style="text-align:center;">0.6948</td>
                <td style="text-align:center;">
                <strong>0.6811</strong></td>
                <td>0.7318</td>
              </tr>
              <tr>
                <td style="text-align:center;">GloVe</td>
                <td style="text-align:center;">0.4064</td>
                <td style="text-align:center;">0.5207</td>
                <td style="text-align:center;">0.6297</td>
                <td style="text-align:center;">0.5892</td>
                <td>0.5481</td>
              </tr>
              <tr>
                <td style="text-align:center;">Fastext</td>
                <td style="text-align:center;">0.5079</td>
                <td style="text-align:center;">0.6247</td>
                <td style="text-align:center;">0.6517</td>
                <td style="text-align:center;">0.6421</td>
                <td>0.5517</td>
              </tr>
              <tr>
                <td style="text-align:center;">Word2vec</td>
                <td style="text-align:center;">0.5550</td>
                <td style="text-align:center;">0.6911</td>
                <td style="text-align:center;">
                <strong>0.7021</strong></td>
                <td style="text-align:center;">0.6730</td>
                <td>0.7209</td>
              </tr>
              <tr>
                <td style="text-align:center;">WMD</td>
                <td style="text-align:center;">0.4241</td>
                <td style="text-align:center;">0.5679</td>
                <td style="text-align:center;">0.5962</td>
                <td style="text-align:center;">0.5953</td>
                <td>0.3430</td>
              </tr>
              <tr>
                <td style="text-align:center;">OWMD</td>
                <td style="text-align:center;">
                <strong>0.6144</strong></td>
                <td style="text-align:center;">
                <strong>0.7240</strong></td>
                <td style="text-align:center;">0.6797</td>
                <td style="text-align:center;">0.6772</td>
                <td><strong>0.7519</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Spearman's Rank Correlation results on
            different distance metrics.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Algorithm</th>
                <th style="text-align:center;" colspan="2">
                  STSbenchmark
                  <hr />
                </th>
                <th style="text-align:center;" colspan="2">
                  SICK
                  <hr />
                </th>
                <th>MSRvid</th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">Test</th>
                <th style="text-align:center;">Dev</th>
                <th style="text-align:center;">Test</th>
                <th style="text-align:center;">Dev</th>
                <th>Test</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">BoW</td>
                <td style="text-align:center;">0.5592</td>
                <td style="text-align:center;">0.6572</td>
                <td style="text-align:center;">0.5727</td>
                <td style="text-align:center;">0.5894</td>
                <td>0.5233</td>
              </tr>
              <tr>
                <td style="text-align:center;">LexVec</td>
                <td style="text-align:center;">0.5472</td>
                <td style="text-align:center;">0.7032</td>
                <td style="text-align:center;">0.5872</td>
                <td style="text-align:center;">0.5879</td>
                <td>0.7311</td>
              </tr>
              <tr>
                <td style="text-align:center;">GloVe</td>
                <td style="text-align:center;">0.4268</td>
                <td style="text-align:center;">0.5862</td>
                <td style="text-align:center;">0.5505</td>
                <td style="text-align:center;">0.5490</td>
                <td>0.5828</td>
              </tr>
              <tr>
                <td style="text-align:center;">Fastext</td>
                <td style="text-align:center;">0.4874</td>
                <td style="text-align:center;">0.6424</td>
                <td style="text-align:center;">0.5739</td>
                <td style="text-align:center;">0.5941</td>
                <td>0.5634</td>
              </tr>
              <tr>
                <td style="text-align:center;">Word2vec</td>
                <td style="text-align:center;">0.5184</td>
                <td style="text-align:center;">0.7021</td>
                <td style="text-align:center;">0.6082</td>
                <td style="text-align:center;">0.6056</td>
                <td>0.7175</td>
              </tr>
              <tr>
                <td style="text-align:center;">WMD</td>
                <td style="text-align:center;">0.4270</td>
                <td style="text-align:center;">0.5781</td>
                <td style="text-align:center;">0.5488</td>
                <td style="text-align:center;">0.5612</td>
                <td>0.3699</td>
              </tr>
              <tr>
                <td style="text-align:center;">OWMD</td>
                <td style="text-align:center;">
                <strong>0.5855</strong></td>
                <td style="text-align:center;">
                <strong>0.7253</strong></td>
                <td style="text-align:center;">
                <strong>0.6133</strong></td>
                <td style="text-align:center;">
                <strong>0.6188</strong></td>
                <td><strong>0.7543</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Table <a class="tbl" href="#tab2">2</a> and Table
        <a class="tbl" href="#tab3">3</a> compare the performance
        of different metrics in terms of the Pearson Correlation
        coefficients and the Spearman's Rank Correlation
        coefficients. We can see that the result of our OWMD metric
        achieves the best performance on all the datasets in terms
        of the Spearman's Rank Correlation coefficients. It also
        produced the best Pearson Correlation results on the
        STSbenchmark and the MSRvid dataset, while the performance
        on SICK datasets are close to the best. This can be
        attributed to the two characteristics of OWMD. First, the
        input sentence is re-organized into a predicate-argument
        structure using the sentence factorization tree. Therefore,
        corresponding semantic units in the two sentences will be
        aligned roughly in order. Second, our OWMD metric takes
        word positions into consideration and penalizes disordered
        matches. Therefore, it will produce less mismatches
        compared with the WMD metric.</p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Supervised
            Multi-scale Semantic Matching</h3>
          </div>
        </header>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">A comparison among different supervised
            learning models in terms of accuracy, F1 score,
            Pearson's <em>r</em> and Spearman's <em>ρ</em> on
            various test sets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Model</th>
                <th style="text-align:center;" colspan="2">
                  MSRP
                  <hr />
                </th>
                <th style="text-align:center;" colspan="2">
                  SICK
                  <hr />
                </th>
                <th style="text-align:center;" colspan="2">
                  MSRvid
                  <hr />
                </th>
                <th style="text-align:center;" colspan="2">
                  STSbenchmark
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">Acc.(%)</th>
                <th style="text-align:center;">F1(%)</th>
                <th style="text-align:center;"><em>r</em></th>
                <th style="text-align:center;"><em>ρ</em></th>
                <th style="text-align:center;"><em>r</em></th>
                <th style="text-align:center;"><em>ρ</em></th>
                <th style="text-align:center;"><em>r</em></th>
                <th><em>ρ</em></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">MaLSTM</td>
                <td style="text-align:center;">66.95</td>
                <td style="text-align:center;">73.95</td>
                <td style="text-align:center;">0.7824</td>
                <td style="text-align:center;">0.71843</td>
                <td style="text-align:center;">0.7325</td>
                <td style="text-align:center;">0.7193</td>
                <td style="text-align:center;">0.5739</td>
                <td>0.5558</td>
              </tr>
              <tr>
                <td style="text-align:center;">Multi-scale
                MaLSTM</td>
                <td style="text-align:center;">
                <strong>74.09</strong></td>
                <td style="text-align:center;">
                <strong>82.18</strong></td>
                <td style="text-align:center;">
                <strong>0.8168</strong></td>
                <td style="text-align:center;">
                <strong>0.74226</strong></td>
                <td style="text-align:center;">
                <strong>0.8236</strong></td>
                <td style="text-align:center;">
                <strong>0.8188</strong></td>
                <td style="text-align:center;">
                <strong>0.6839</strong></td>
                <td><strong>0.6575</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">HCTI</td>
                <td style="text-align:center;">73.80</td>
                <td style="text-align:center;">80.85</td>
                <td style="text-align:center;">0.8408</td>
                <td style="text-align:center;">0.7698</td>
                <td style="text-align:center;">
                <strong>0.8848</strong></td>
                <td style="text-align:center;">
                <strong>0.8763</strong></td>
                <td style="text-align:center;">
                <strong>0.7697</strong></td>
                <td><strong>0.7549</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">Multi-scale
                HCTI</td>
                <td style="text-align:center;">
                <strong>74.03</strong></td>
                <td style="text-align:center;">
                <strong>81.76</strong></td>
                <td style="text-align:center;">
                <strong>0.8437</strong></td>
                <td style="text-align:center;">
                <strong>0.7729</strong></td>
                <td style="text-align:center;">0.8763</td>
                <td style="text-align:center;">0.8686</td>
                <td style="text-align:center;">0.7269</td>
                <td>0.7033</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The use of sentence factorization can improve both
        existing unsupervised metrics and existing supervised
        models. To evaluate how the performance of existing Siamese
        neural networks can be improved by our sentence
        factorization technique and the multi-scale Siamese
        architecture, we implemented two types of Siamese sentence
        matching models, HCTI [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>] and MaLSTM [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0036">36</a>]. HCTI is a
        Convolutional Neural Network (CNN) based Siamese model,
        which achieves the best Pearson Correlation coefficient on
        STSbenchmark dataset in SemEval2017 competition (compared
        with all the other neural network models). MaLSTM is a
        Siamese adaptation of the Long Short-Term Memory (LSTM)
        network for learning sentence similarity. As the source
        code of HCTI is not released in public, we implemented it
        according to [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>] by Keras [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0010">10</a>]. With the same
        parameter settings listed in paper [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0036">36</a>] and tried our best to
        optimize the model, we got a Pearson correlation of 0.7697
        (0.7833 in paper [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>]) in STSbencmark test dataset.</p>
        <p>We extended HCTI and MaLSTM to our proposed Siamese
        architecture in Fig. <a class="fig" href="#fig5">5</a>,
        namely the Multi-scale MaLSTM and the Multi-scale HCTI. To
        evaluate the performance of our models, the experiment is
        conducted on two tasks: 1) semantic textual similarity
        estimation based on the STSbenchmark, MSRvid, and SICK2014
        datasets; 2) paraphrase identification based on the MSRP
        dataset.</p>
        <p>Table <a class="tbl" href="#tab4">4</a> shows the
        results of HCTI, MaLSTM and our multi-scale models on
        different datasets. Compared with the original models, our
        models with multi-scale semantic units of the input
        sentences as network inputs significantly improved the
        performance on most datasets. Furthermore, the improvements
        on different tasks and datasets also proved the general
        applicability of our proposed architecture.</p>
        <p>Compared with MaLSTM, our multi-scaled Siamese models
        with factorized sentences as input perform much better on
        each dataset. For MSRvid and STSbenmark dataset, both
        Pearson's <em>r</em> and Spearman's <em>ρ</em> increase
        about 10% with Multi-scale MaLSTM. Moreover, the
        Multi-scale MaLSTM achieves the highest accuracy and F1
        score on the MSRP dataset compared with other models listed
        in Table <a class="tbl" href="#tab4">4</a>.</p>
        <p>There are two reasons why our Multi-scale MaLSTM
        significantly outperforms MaLSTM model. First, for an input
        sentence pair, we explicitly model their semantic units
        with the factorization algorithm. Second, our multi-scaled
        network architecture is specifically designed for
        multi-scaled sentences representations. Therefore, it is
        able to explicitly match a pair of sentences at different
        granularities.</p>
        <p>We also report the results of HCTI and Multi-scale HCTI
        in Table <a class="tbl" href="#tab4">4</a>. For the
        paraphrase identification task, our model shows better
        accuracy and F1 score on MSRP dataset. For the semantic
        textual similarity estimation task, the performance varies
        across datasets. On the SICK dataset, the performance of
        Multi-scale HCTI is close to HCTI with slightly better
        Pearson’ <em>r</em> and Spearman's <em>ρ</em>. However, the
        Multi-scale HCTI is not able to outperform HCTI on MSRvid
        and STSbenchmark. HCTI is still the best neural network
        model on the STSbenchmark dataset, and the MSRvid dataset
        is a subset of STSbenchmark. Although HCTI has strong
        performance on these two datasets, it performs worse than
        our model on other datasets. Overall, the experimental
        results demonstrated the general applicability of our
        proposed model architecture, which performs well on various
        semantic matching tasks.</p>
      </section>
    </section>
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Related
          Work</h2>
        </div>
      </header>
      <p>The task of natural language sentence matching has been
      extensively studied for a long time. Here we review related
      unsupervised and supervised models for sentence matching.</p>
      <p>Traditional unsupervised metrics for document
      representation, including bag of words (BOW), term frequency
      inverse document frequency (TF-IDF) [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0043">43</a>], Okapi BM25 score
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>]. However,
      these representations can not capture the semantic distance
      between individual words. Topic modeling approaches such as
      Latent Semantic Indexing (LSI) [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>] and Latent Dirichlet Allocation
      (LDA) [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0007">7</a>]
      attempt to circumvent the problem through learning a latent
      representation of documents. But when applied to
      semantic-distance based tasks such as text-pair semantic
      similarity estimation, these algorithms usually cannot
      achieve good performance.</p>
      <p>Learning distributional representation for words,
      sentences or documents based on deep learning models have
      been popular recently. <em>word2vec</em> [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>] and <em>Glove</em>
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>] are two
      high quality word embeddings that have been extensively used
      in many NLP tasks. Based on word vector representation, the
      Word Mover's Distance (WMD) [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>] algorithm measures the dissimilarity
      between two sentences (or documents) as the minimum distance
      that the embedded words of one sentence need to “travel” to
      reach the embedded words of another sentence. However, when
      applying these approaches to sentence pair matching tasks,
      the interactions between sentence pairs are omitted, also the
      ordered and hierarchical structure of natural languages is
      not considered.</p>
      <p>Different neural network architectures have been proposed
      for sentence pair matching tasks. Models based on Siamese
      architectures [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>] usually transform the word embedding
      sequences of text pairs into context representation vectors
      through a multi-layer Long Short-Term Memory (LSTM)
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0038">38</a>] network or
      Convolutional Neural Networks (CNN) [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>], followed by a fully
      connected network or score function which gives the
      similarity score or classification label based on the context
      representation vectors. However, Siamese models defer the
      interaction between two sentences until the hidden
      representation layer, therefore may lose details of sentence
      pairs for matching tasks [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>].</p>
      <p>Aside from Siamese architectures, [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0041">41</a>] introduced a matching
      layer into Siamese network to compare the contextual
      embedding of one sentence with another. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0016">16</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0027">27</a>] proposed convolutional
      matching models that consider all pair-wise interactions
      between words in sentence pairs. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>] propose to explicitly
      model pairwise word interactions with a pairwise word
      interaction similarity cube and a similarity focus layer to
      identify important word interactions.</p>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we propose a technique named
      <em>Hierarchical Sentence Factorization</em> that is able to
      transform a sentence into a hierarchical factorization tree.
      Each node in the tree is a semantic unit consists of one or
      several words in the sentence and reorganized into the form
      of “predicate-argument” structure. Each depth in the tree
      factorizes the sentence into semantic units of different
      scales. Based on the hierarchical tree-structured
      representation of sentences, we propose both an unsupervised
      metric and two supervised deep models for sentence matching
      tasks. On one hand, we design a new unsupervised distance
      metric, named <em>Ordered Word Mover's Distance</em> (OWMD),
      to measure the semantic difference between a pair of text
      snippets. OWMD takes the sequential structure of sentences
      into account, and is able to handle the flexible syntactical
      structure of natural language sentences. On the other hand,
      we propose the multi-scale Siamese neural network
      architecture which takes the multi-scale representation of a
      pair of sentences as network input and matches the two
      sentences at different granularities.</p>
      <p>We apply our techniques to the task of text-pair
      similarity estimation and the task of text-pair paraphrase
      identification, based on multiple datasets. Our extensive
      experiments show that both the unsupervised distance metric
      and the supervised multi-scale Siamese network architecture
      can achieve significant improvement on multiple datasets
      using the technique of sentence factorization.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Fritz Albregtsen <em>et
        al.</em> 2008. Statistical texture measures computed from
        gray level coocurrence matrices. <em><em>Image processing
        laboratory, department of informatics, university of
        oslo</em></em> 5(2008).</li>
        <li id="BibPLXBIB0002" label="[2]">Collin&nbsp;F Baker,
        Charles&nbsp;J Fillmore, and John&nbsp;B Lowe. 1998. The
        berkeley framenet project. In <em><em>Proceedings of the
        36th Annual Meeting of the Association for Computational
        Linguistics and 17th International Conference on
        Computational Linguistics-Volume 1</em></em> . Association
        for Computational Linguistics, 86–90.</li>
        <li id="BibPLXBIB0003" label="[3]">Laura Banarescu, Claire
        Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf
        Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and
        Nathan Schneider. 2012. Abstract meaning representation
        (AMR) 1.0 specification. In <em><em>Parsing on Freebase
        from Question-Answer Pairs. In Proceedings of the 2013
        Conference on Empirical Methods in Natural Language
        Processing. Seattle: ACL</em></em> . 1533–1544.</li>
        <li id="BibPLXBIB0004" label="[4]">Laura Banarescu, Claire
        Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf
        Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and
        Nathan Schneider. 2013. Abstract meaning representation for
        sembanking. In <em><em>Proceedings of the 7th Linguistic
        Annotation Workshop and Interoperability with
        Discourse</em></em> . 178–186.</li>
        <li id="BibPLXBIB0005" label="[5]">Petr Baudiš, Jan Pichl,
        Tomáš Vyskočil, and Jan Šedivỳ. 2016. Sentence pair
        scoring: Towards unified framework for text comprehension.
        <em><em>arXiv preprint arXiv:1603.06127</em></em>
        (2016).</li>
        <li id="BibPLXBIB0006" label="[6]">Jonathan Berant and
        Percy Liang. 2014. Semantic Parsing via Paraphrasing.. In
        <em><em>ACL (1)</em></em> . 1415–1425.</li>
        <li id="BibPLXBIB0007" label="[7]">David&nbsp;M Blei,
        Andrew&nbsp;Y Ng, and Michael&nbsp;I Jordan. 2003. Latent
        dirichlet allocation. <em><em>Journal of machine Learning
        research</em></em> 3, Jan (2003), 993–1022.</li>
        <li id="BibPLXBIB0008" label="[8]">Peter&nbsp;F Brown,
        Vincent J&nbsp;Della Pietra, Stephen A&nbsp;Della Pietra,
        and Robert&nbsp;L Mercer. 1993. The mathematics of
        statistical machine translation: Parameter estimation.
        <em><em>Computational linguistics</em></em> 19, 2 (1993),
        263–311.</li>
        <li id="BibPLXBIB0009" label="[9]">Daniel Cer, Mona Diab,
        Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017.
        SemEval-2017 Task 1: Semantic Textual
        Similarity-Multilingual and Cross-lingual Focused
        Evaluation. <em><em>arXiv preprint
        arXiv:1708.00055</em></em> (2017).</li>
        <li id="BibPLXBIB0010" label="[10]">François Chollet <em>et
        al.</em> 2015. Keras. <a href=
        "https://github.com/fchollet/keras." target=
        "_blank">https://github.com/fchollet/keras</a>. (2015).
        </li>
        <li id="BibPLXBIB0011" label="[11]">Marco Damonte,
        Shay&nbsp;B Cohen, and Giorgio Satta. 2016. An incremental
        parser for abstract meaning representation. <em><em>arXiv
        preprint arXiv:1608.06111</em></em> (2016).</li>
        <li id="BibPLXBIB0012" label="[12]">Scott Deerwester,
        Susan&nbsp;T Dumais, George&nbsp;W Furnas, Thomas&nbsp;K
        Landauer, and Richard Harshman. 1990. Indexing by latent
        semantic analysis. <em><em>Journal of the American society
        for information science</em></em> 41, 6(1990), 391.</li>
        <li id="BibPLXBIB0013" label="[13]">Jeffrey Flanigan, Sam
        Thomson, Jaime&nbsp;G Carbonell, Chris Dyer, and
        Noah&nbsp;A Smith. 2014. A discriminative graph-based
        parser for the abstract meaning representation.
        (2014).</li>
        <li id="BibPLXBIB0014" label="[14]">Ralph Grishman. 1997.
        Information extraction: Techniques and challenges. In
        <em><em>Information extraction a multidisciplinary approach
        to an emerging information technology</em></em> . Springer,
        10–27.</li>
        <li id="BibPLXBIB0015" label="[15]">Hua He and Jimmy&nbsp;J
        Lin. 2016. Pairwise Word Interaction Modeling with Deep
        Neural Networks for Semantic Similarity Measurement.. In
        <em><em>HLT-NAACL</em></em> . 937–948.</li>
        <li id="BibPLXBIB0016" label="[16]">Baotian Hu, Zhengdong
        Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural
        network architectures for matching natural language
        sentences. In <em><em>Advances in neural information
        processing systems</em></em> . 2042–2050.</li>
        <li id="BibPLXBIB0017" label="[17]">Armand Joulin, Edouard
        Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of
        tricks for efficient text classification. <em><em>arXiv
        preprint arXiv:1607.01759</em></em> (2016).</li>
        <li id="BibPLXBIB0018" label="[18]">Paul Kingsbury and
        Martha Palmer. 2002. From TreeBank to PropBank.. In
        <em><em>LREC</em></em> . 1989–1993.</li>
        <li id="BibPLXBIB0019" label="[19]">Philip&nbsp;A Knight.
        2008. The Sinkhorn–Knopp algorithm: convergence and
        applications. <em><em>SIAM J. Matrix Anal. Appl.</em></em>
        30, 1 (2008), 261–275.</li>
        <li id="BibPLXBIB0020" label="[20]">Alex Krizhevsky, Ilya
        Sutskever, and Geoffrey&nbsp;E Hinton. 2012. Imagenet
        classification with deep convolutional neural networks. In
        <em><em>Advances in neural information processing
        systems</em></em> . 1097–1105.</li>
        <li id="BibPLXBIB0021" label="[21]">Matt Kusner, Yu Sun,
        Nicholas Kolkin, and Kilian Weinberger. 2015. From word
        embeddings to document distances. In <em><em>International
        Conference on Machine Learning</em></em> . 957–966.</li>
        <li id="BibPLXBIB0022" label="[22]">Marco Marelli, Stefano
        Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi,
        and Roberto Zamparelli. 2014. A SICK cure for the
        evaluation of compositional distributional semantic
        models.. In <em><em>LREC</em></em> . 216–223.</li>
        <li id="BibPLXBIB0023" label="[23]">Tomas Mikolov, Kai
        Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
        estimation of word representations in vector space.
        <em><em>arXiv preprint arXiv:1301.3781</em></em>
        (2013).</li>
        <li id="BibPLXBIB0024" label="[24]">Jonas Mueller and
        Aditya Thyagarajan. 2016. Siamese Recurrent Architectures
        for Learning Sentence Similarity.. In
        <em><em>AAAI</em></em> . 2786–2792.</li>
        <li id="BibPLXBIB0025" label="[25]">Paul Neculoiu, Maarten
        Versteegh, Mihai Rotaru, and Textkernel&nbsp;BV Amsterdam.
        2016. Learning Text Similarity with Siamese Recurrent
        Networks. <em><em>ACL 2016</em></em> (2016), 148.</li>
        <li id="BibPLXBIB0026" label="[26]">Georgios Paltoglou and
        Mike Thelwall. 2010. A study of information retrieval
        weighting schemes for sentiment analysis. In
        <em><em>Proceedings of the 48th annual meeting of the
        association for computational linguistics</em></em> .
        Association for Computational Linguistics, 1386–1395.</li>
        <li id="BibPLXBIB0027" label="[27]">Liang Pang, Yanyan Lan,
        Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016.
        Text Matching as Image Recognition.. In
        <em><em>AAAI</em></em> . 2793–2799.</li>
        <li id="BibPLXBIB0028" label="[28]">Jeffrey Pennington,
        Richard Socher, and Christopher Manning. 2014. Glove:
        Global vectors for word representation. In
        <em><em>Proceedings of the 2014 conference on empirical
        methods in natural language processing (EMNLP)</em></em> .
        1532–1543.</li>
        <li id="BibPLXBIB0029" label="[29]">Luca Ponzanelli, Andrea
        Mocci, and Michele Lanza. 2015. Summarizing complex
        development artifacts by mining heterogeneous data. In
        <em><em>Proceedings of the 12th Working Conference on
        Mining Software Repositories</em></em> . IEEE Press,
        401–405.</li>
        <li id="BibPLXBIB0030" label="[30]">Nima Pourdamghani, Yang
        Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning
        English Strings with Abstract Meaning Representation
        Graphs.. In <em><em>EMNLP</em></em> . 425–429.</li>
        <li id="BibPLXBIB0031" label="[31]">Chris Quirk, Chris
        Brockett, and William Dolan. 2004. Monolingual machine
        translation for paraphrase generation. In
        <em><em>Proceedings of the 2004 conference on empirical
        methods in natural language processing</em></em> .</li>
        <li id="BibPLXBIB0032" label="[32]">Stephen&nbsp;E
        Robertson and Steve Walker. 1994. Some simple effective
        approximations to the 2-poisson model for probabilistic
        weighted retrieval. In <em><em>Proceedings of the 17th
        annual international ACM SIGIR conference on Research and
        development in information retrieval</em></em> .
        Springer-Verlag New York, Inc., 232–241.</li>
        <li id="BibPLXBIB0033" label="[33]">Yossi Rubner, Carlo
        Tomasi, and Leonidas&nbsp;J Guibas. 2000. The earth mover's
        distance as a metric for image retrieval.
        <em><em>International journal of computer vision</em></em>
        40, 2 (2000), 99–121.</li>
        <li id="BibPLXBIB0034" label="[34]">Alexandre Salle, Marco
        Idiart, and Aline Villavicencio. 2016. Enhancing the LexVec
        Distributed Word Representation Model Using Positional
        Contexts and External Memory. <em><em>arXiv preprint
        arXiv:1606.01283</em></em> (2016).</li>
        <li id="BibPLXBIB0035" label="[35]">Aliaksei Severyn and
        Alessandro Moschitti. 2015. Learning to rank short text
        pairs with convolutional deep neural networks. In
        <em><em>Proceedings of the 38th International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval</em></em> . ACM, 373–382.</li>
        <li id="BibPLXBIB0036" label="[36]">Yang Shao. 2017. HCTI
        at SemEval-2017 Task 1: Use convolutional neural network to
        evaluate semantic textual similarity. In
        <em><em>Proceedings of the 11th International Workshop on
        Semantic Evaluation (SemEval-2017)</em></em> .
        130–133.</li>
        <li id="BibPLXBIB0037" label="[37]">Bing Su and Gang Hua.
        2017. Order-preserving wasserstein distance for sequence
        matching. In <em><em>Proc. IEEE Conf. Comput. Vis. Pattern
        Recognit.</em></em> 1049–1057.</li>
        <li id="BibPLXBIB0038" label="[38]">Martin Sundermeyer,
        Ralf Schlüter, and Hermann Ney. 2012. LSTM neural networks
        for language modeling. In <em><em>Thirteenth Annual
        Conference of the International Speech Communication
        Association</em></em> .</li>
        <li id="BibPLXBIB0039" label="[39]">Chuan Wang, Nianwen
        Xue, and Sameer Pradhan. 2015. Boosting Transition-based
        AMR Parsing with Refined Actions and Auxiliary Analyzers..
        In <em><em>ACL (2)</em></em> . 857–862.</li>
        <li id="BibPLXBIB0040" label="[40]">Shuohang Wang and Jing
        Jiang. 2016. A Compare-Aggregate Model for Matching Text
        Sequences. <em><em>arXiv preprint
        arXiv:1611.01747</em></em> (2016).</li>
        <li id="BibPLXBIB0041" label="[41]">Zhiguo Wang, Wael
        Hamza, and Radu Florian. 2017. Bilateral multi-perspective
        matching for natural language sentences. <em><em>arXiv
        preprint arXiv:1702.03814</em></em> (2017).</li>
        <li id="BibPLXBIB0042" label="[42]">Wikipedia. 2017.
        Spearman's rank correlation coefficient — Wikipedia, The
        Free Encyclopedia. (2017). <a href=
        "https://en.wikipedia.org/w/index.php?title=Spearman%27s_rank_correlation_coefficientoldid=801404677"
          target=
          "_blank">https://en.wikipedia.org/w/index.php?title=Spearman%27s_rank_correlation_coefficientoldid=801404677</a>[Online;
          accessed 31-October-2017].
        </li>
        <li id="BibPLXBIB0043" label="[43]">Ho&nbsp;Chung Wu,
        Robert Wing&nbsp;Pong Luk, Kam&nbsp;Fai Wong, and
        Kui&nbsp;Lam Kwok. 2008. Interpreting tf-idf term weights
        as making relevance decisions. <em><em>ACM Transactions on
        Information Systems (TOIS)</em></em> 26, 3 (2008), 13.</li>
        <li id="BibPLXBIB0044" label="[44]">Lei Yu,
        Karl&nbsp;Moritz Hermann, Phil Blunsom, and Stephen Pulman.
        2014. Deep learning for answer sentence selection.
        <em><em>arXiv preprint arXiv:1412.1632</em></em>
        (2014).</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186022">https://doi.org/10.1145/3178876.3186022</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
