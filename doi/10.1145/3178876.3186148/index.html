<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Robust Factorization Machines for User Response Prediction</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186148"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186148'>https://doi.org/10.1145/3178876.3186148</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186148'>https://w3id.org/oa/10.1145/3178876.3186148</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Robust Factorization Machines for User Response Prediction</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Surabhi</span> <span class="surName">Punjabi</span><a class="fn" href="#fn1" id="foot-fn1"><sup>⁎</sup></a>, @WalmartLabs, Bangalore, India, <a href="mailto:surabhi.punjabi@gmail.com">surabhi.punjabi@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Priyanka</span> <span class="surName">Bhatt</span>, @WalmartLabs, Bangalore, India, <a href="mailto:priyankabhatt91@gmail.com">priyankabhatt91@gmail.com</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186148" target="_blank">https://doi.org/10.1145/3178876.3186148</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Factorization machines (FMs) are a state-of-the-art model class for user response prediction in the computational advertising domain. Rapid growth of internet and mobile device usage has given rise to multiple customer touchpoints. This coupled with factors like high cookie churn rate results in a fragmented view of user activity at the advertiser's end. Current literature assumes procured user signals as the absolute truth, which is contested by the absence of deterministic identity linkage across a user's multiple avatars. In this work, we characterize the data uncertainty using Robust Optimization (RO) paradigm to design approaches that are immune against perturbations. We propose two novel algorithms: robust factorization machine (RFM) and its field-aware variant (RFFM), under interval uncertainty. These formulations are generic and can find applicability in any classification setting under noise. We provide a distributed and scalable Spark implementation using parallel stochastic gradient descent. In the experiments conducted on three real-world datasets, the robust counterparts outperform the baselines significantly under perturbed settings. Our experimental findings reveal interesting connections between choice of uncertainty set and the noise-proofness of resulting models.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Factorization Machines; Field-aware Factorization Machines; Robust Optimization; Computational Advertising; Response Prediction; Interval Uncertainty</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Surabhi Punjabi and Priyanka Bhatt. 2018. Robust Factorization Machines for User Response Prediction. In <em>WWW 2018: The 2018 Web Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186148" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3178876.3186148</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>User response prediction is a central problem in the computational advertising domain. The primary stakeholders in this ecosystem are: publishers who possess ad inventory, advertisers who bid for these ad slots and users who are exposed to the ad experience. Publishers and advertisers leverage signals like online user footprint, demographics and associated context for modeling user intent. Clicks and conversions being the key objectives, response prediction problem is generally formulated as estimating the probability of click or conversion given an ad impression. This probability subsequently translates to user-level bids or manifests itself in creating discrete user segments according to the propensity of user intent. This area has garnered interest from both the industry and academia, with logistic regression (LR) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] being the conventional choice. The recently proposed factorization machines (FMs) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>] and field-aware factorization machines (FFMs) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] which model feature interactions in latent space have outperformed LR on several experimental and production datasets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>].</p>
      <p>User interaction signals, which serve as primary input to the predictive modeling, are procured from a wide variety of online sources like social media, search engines, e-commerce platforms, and news portals. A user's activity might be spread across multiple devices like desktop, mobile, and tablet. Interestingly, users exhibit a manifold of browsing patterns and device specific avatars: a user seeming to be a keen shopper on desktop might just be a casual browser on mobile. Without cross device linkages in place, the user interaction dataset comprises of multiple incomplete views for the same user. For US, Criteo [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] estimates that a whopping 31% of online transactions involve two or more devices and that both the conversion rates and buyer-journeys increase by about 40% in user-centric view of activity across multiple devices as compared to a partial device-specific view.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">(A) User visits an advertiser's site several times on different devices and/or browsers: the advertiser observes multiple fragmented views of user's real activity. (B) Additional noise induced during data collection. (C, D) User visiting a publisher's site results in an online auction and a bid request is sent to different advertisers. (E) Advertiser responds with a bid computed using data from only one of the user views.</span>
        </div>
      </figure>
      <p></p>
      <p>Even for the same device, factors like operating system, network connectivity and browser type have their own associated data leakages. These heterogeneities in the underlying generative mechanism lead to noise in the collected data. However, this problem has been heavily overlooked with only 5% marketers having seamlessly integrated customer touchpoints [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. With around 65% cookies being deleted monthly [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>], cookie churn acts as an additional contributor to noise and makes the user-advertiser association short lived. See Figure&nbsp;<a class="fig" href="#fig1">1</a>.</p>
      <p>These cumulative inefficiencies during data collection end up camouflaging the user's holistic identity and raise a compelling question on the quality of the same data hose that had generated these signals in the first place. Some recent works have attempted to probabilistically stitch the user identity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>], but a complete consolidation of user profiles remains an open problem.</p>
      <p>Objective functions in the existing response prediction literature assume user profiles to be precisely known and remain agnostic to the inherent noise present in the input signals [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>]. Consequently, the learnt classifiers possess a fuzzy understanding of the underlying causal relationships and thus exhibit remarkable sensitivity towards small data perturbations. Since model predictions guide bid price determination, a monetary loss or an opportunity cost is incurred for every misclassification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>].</p>
      <p>This work aims at characterizing the environment-induced uncertainty in the user signals and reformulating the FM and FFM objective functions to be immune against data fluctuations. For this we utilize the robust optimization (RO) framework [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] which assumes a deterministic, set based uncertainty model and seeks solutions that are computationally tractable and remain near-optimal in the worst case realization of uncertainty. To the best of our knowledge, this is the first work advocating the application of RO in the user modeling domain. The main contributions of this paper are summarized below:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We employ robust optimization principles to model the noise arising in online advertising signals as bounded box-type interval uncertainty sets.<br /></li>
        <li id="list2" label="•">We propose two novel algorithms: robust factorization machine (RFM) and robust field-aware factorization machine (RFFM), as robust minimax formulations for FM and FFM respectively.<br /></li>
        <li id="list3" label="•">We provide a distributed and scalable Spark based implementation for solving the optimization problem using parallel stochastic gradient decent.<br /></li>
        <li id="list4" label="•">We present a comprehensive evaluation of our formulations on three publicly available response prediction datasets from Criteo and Avazu. The price of robustness is a classifier which takes a slight performance hit in the standard setting (-0.24<span class="inline-equation"><span class="tex">$\%$</span></span> to -1.1<span class="inline-equation"><span class="tex">$\%$</span></span> ) but significantly outperforms the non-robust counterparts (4.45<span class="inline-equation"><span class="tex">$\%$</span></span> to 38.65<span class="inline-equation"><span class="tex">$\%$</span></span> ) when subjected to noise.<br /></li>
        <li id="list5" label="•">We systematically assess the tradeoff between robustness under noise and optimality of performance in standard setting and provide guidelines for selection of uncertainty sets.<br /></li>
        <li id="list6" label="•">We extensively study model calibration and the effects of hyperparameters, initialization strategies, and parallelism on model performance and convergence.<br /></li>
        <li id="list7" label="•">The final formulations obtained are generic and can aid in any noise sensitive classification domain. To demonstrate this broad applicability, we present results on a credit card fraud detection dataset.<br /></li>
      </ul>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Preliminaries</h2>
        </div>
      </header>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Response Prediction</h3>
          </div>
        </header>
        <p>We begin with an overview of the state-of-the-art approaches for predicting user propensity to click or convert given ad exposure. This is a supervised learning setting wherein the learner is presented with a set of <em>m</em> training instances <span class="inline-equation"><span class="tex">$\lbrace (\mathbf {x}^{({i})}, y^{({i})}) | \mathbf {x}^{({i})}\in \ \mathbb {R}^{d}, y^{({i})} \in \lbrace 1,-1 \rbrace \text{ } \forall i \in \lbrace 1,\ldots ,m\rbrace \rbrace$</span></span> , where <strong>x</strong> <sup>(<em>i</em>)</sup> represents activity and context signals for user <em>i</em> and <em>y</em> <sup>(<em>i</em>)</sup> is the binary response variable which captures whether or not the user subsequently clicked or converted.</p>
        <p>Logistic regression has long been the preferred classifier for user response modeling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] since it offers the advantage of well calibrated probability outputs, is highly scalable and also yields interpretable models. It learns weight vector <span class="inline-equation"><span class="tex">$\mathbf {w} \in \mathbb {R}^{d}$</span></span> by maximizing log likelihood against a regularization penalty. The corresponding loss minimization equivalent takes the following form:</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \min _{\mathbf {w}} \frac{\lambda }{2} \Vert \mathbf {w}\Vert _{2}^2 + \frac{1}{m} \sum _{i=1}^{m}{\log {(1+\exp ({-y^{({i})} \phi (\mathbf {x}^{({i})}, \mathbf {w})}))}}\\ \text{where, } \phi (\mathbf {x}, \mathbf {w}) = w_{0} + \sum _{j=1}^{d}{w_{j}x_{j}}.\end{align}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <p></p>
        <p>Linear models suffer with the limitation of not being able to model the effect of feature interactions on the dependent variable. Factorization machines (FMs) proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>] have recently gained popularity as an effective learning paradigm for capturing impact of feature conjunctions especially for sparse datasets. They aim at learning a projection <strong>v</strong> <sub><em>j</em></sub> for every feature <em>j</em> in a latent factor space <span class="inline-equation"><span class="tex">$\mathbb {R}^{p}$</span></span> . The strength of feature interactions is quantified by the inner product of the corresponding factors. The optimization problem remains similar to (<a class="eqn" href="#eq1">1</a>), with <em>ϕ</em> evolving to <em>ϕ<sub>FM</sub></em> to incorporate these interaction terms:</p>
        <div class="table-responsive" id="Xeq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \phi _{FM}(\mathbf {x},\mathbf {w},\mathbf {V}) = w_{0} + \sum _{j=1}^{d}{w_{j}x_{j}} + \sum _{j = 1}^{d}{\sum _{k = j + 1} ^ {d}{ \langle \mathbf {v}_{j}, \mathbf {v}_{k}\rangle x_{j}x_{k}}}\end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <p>where <span class="inline-equation"><span class="tex">$\langle . \rangle$</span></span> represents inner product and <span class="inline-equation"><span class="tex">$\mathbf{V} \in \mathbb{R}^{d \times p}$</span></span> is a factor matrix composed of the latent vectors <span class="inline-equation"><span class="tex">$\mathbf{v}_{j} $</span></span>. Response prediction datasets largely comprise of categorical features a.k.a. fields. Typical examples of fields are publisher, device, brand, etc. which may take values from sets <span class="inline-equation"><span class="tex">$\{ \text{CNN, Vogue}\}$</span></span>, <span class="inline-equation"><span class="tex">$\{\text{desktop, mobile, tablet}\}$</span></span>, and <span class="inline-equation"><span class="tex">$\{\text{Nike, Adidas}\}$</span></span> respectively. LR and FM use the expanded feature space generated by one-hot encoding of these categorical variables and the semantics of the `field' itself are lost.</p>
        <p>Field-aware factorization machines (FFMs) are a model class that leverage the field information associated with every feature and extend the concept of FM to learn a dedicated latent vector corresponding to every (feature, field) combination [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. So instead of learning a latent vector per feature, i.e. v<sub>Vogue</sub>, v<sub>Nike</sub>, etc. the model learns separate latent vectors for capturing fieldwise interactions like v<sub>(Vogue, device)</sub>, v<sub>(Vogue, brand)</sub>, etc. The function <em>ϕ</em> thus further evolves to:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \phi _{FFM}(\mathbf {x},\mathbf {w},\mathbf {V}) = w_{0} + \sum _{j=1}^{d}{w_{j}x_{j}} + \sum _{j = 1}^{d}{\sum _{k = j + 1} ^ {d}{ \langle \mathbf {v}_{j, f_{k}}, \mathbf {v}_{k, f_{j}}\rangle x_{j}x_{k}}}\end{equation}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <p>where <span class="inline-equation"><span class="tex">$ \mathbf{v}_{j, f_{k}} \in \mathbb{R}^{p }$</span></span> is a latent vector capturing interactions between feature <span class="inline-equation"><span class="tex">$j$</span></span> and field of feature <span class="inline-equation"><span class="tex">$k$</span></span> and <span class="inline-equation"><span class="tex">$\mathbf{V} \in \mathbb{R}^{d \times q \times p}$</span></span> is a tensor composed of all these <span class="inline-equation"><span class="tex">$ \mathbf{v}_{j, f_{k}}$</span></span> vectors. Here $q$ denotes the number of fields in the dataset.</p>
        <p>FMs and FFMs have demonstrated superior generalization ability against other classifiers by winning two Kaggle competitions<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> <sup>,</sup><a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a> in the past. Other techniques like model ensembles [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] and deep learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>] have also been explored for the user response prediction task. For this work, we restrict our focus on formulating robust counterparts for FM and FFM models.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Robust Optimization</h3>
          </div>
        </header>
        <p>Traditional stochastic optimization provides probabilistic characterization of noise in measurements. In contrast, paradigm of Robust Optimization (RO) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] models uncertainty as bounded set based variability in the input observations. The uncertainty set is defined as <em>U</em> = {<em>μ</em> <sup>(<em>i</em>)</sup>|<strong>x</strong> <sup>(<em>i</em>)</sup> − <em>η</em> <sup>(<em>i</em>)</sup> ≤ <strong>x</strong> <sup>(<em>i</em>)</sup> + <em>μ</em> <sup>(<em>i</em>)</sup> ≤ <strong>x</strong> <sup>(<em>i</em>)</sup> + <em>η</em> <sup>(<em>i</em>)</sup>, ∀<em>i</em> ∈ {1, …, <em>m</em>}}. Here <span class="inline-equation"><span class="tex">$\text{$\eta $}^{(i)} \in \mathbb {R}_{\ge 0}^{d}$</span></span> represents the uncertainty bound for input <strong>x</strong> <sup>(<em>i</em>)</sup>. Incorporating this notion of deterministic uncertainty allows for multiple manifestations of the input data points anywhere within the specified bounds. RO seeks to learn a function that remains feasible and near optimal for all possible realizations of uncertainty. For classification setting, this translates to minimizing the worst case loss suffered against all possible data perturbations. Assuming a general loss function <span class="inline-equation"><span class="tex">$\mathcal {L}(\mathbf {w}, \mathbf {X})$</span></span> , the robust counterpart takes the following minimax form:</p>
        <div class="table-responsive" id="Xeq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \min _{\mathbf {w}} \max _{U} \frac{1}{m} \sum _{i = 1} ^{m} \mathcal {L}(\mathbf {w}, \mathbf {x}^{(i)} + \text{$\mu $}^{(i)}).\end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>
        <p>Computational tractability of robust formulations is governed by the choice of uncertainty sets. Box-type, ellipsoidal, conic, and polyhedral are commonly employed classes of uncertainty sets in the RO literature [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]. In this work, we design robust formulations assuming box-type (or interval) uncertainty under which, for each observation <span class="inline-equation"><span class="tex">$\mathbf {x} \in \mathbb {R}^{d}$</span></span> , there is a corresponding uncertainty vector <span class="inline-equation"><span class="tex">$\text{$\mu $} \in \mathbb {R}^{d}$</span></span> such that each dimension of the vector is bounded independently, i.e. |<em>μ<sub>j</sub></em> | ≤ <em>η<sub>j</sub></em> , ∀<em>j</em> ∈ {1, …, <em>d</em>}. The choice of interval uncertainty facilitates noise independence amongst individual features. Geometrically, this can be visualized as data points residing in a bounded hyperrectangular manifold.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">(a) Classifier trained on original data. (b) When box-type uncertainty is associated with data points, the learnt classifier boundary shifts in order to accommodate the effect of perturbations.</span>
          </div>
        </figure>
        <p></p>
        <p>Figure <a class="fig" href="#fig2">2</a> illustrates how the training instances appear to a learner in a standard setting and after introducing box-type uncertainty. Note the shift in decision boundary of the learnt classifier. The RO framework presents a systematic tradeoff between choosing optimal classifier weights for given observations and robustifying against perturbations. Robust formulations for LR and support vector machines (SVMs) have been proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>]. Our work is the first attempt to systematically introduce robustness in the factorization machines.</p>
      </section>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Proposed Approach</h2>
        </div>
      </header>
      <p>FM and its extensions have witnessed a rapid adoption recently, not just within the purview of Kaggle competitions but also for the real-world bidding systems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. To incorporate noise-proofness against data perturbations in these models, we design robust counterparts for FM and FFM using RO principles under interval uncertainty. The resulting minimax formulation is then reduced to a pure minimization problem by obtaining upper bounds on terms involving uncertainty. We propose a stochastic gradient descent based parallelized training algorithm which can be deployed on a distributed environment like Spark for learning the final weight matrices.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Robust FM</h3>
          </div>
        </header>
        <p>Factorization machines consider both linear and pairwise feature interactions. This presents us with a choice of either sharing the same uncertainty vectors across the two types of interactions or decoupling the uncertainty parameters. We go with the second alternative and for each data point <strong>x</strong>, we associate uncertainty vector <span class="inline-equation"><span class="tex">$\text{$\mu $} \in \mathbb {R}^{d}$</span></span> s.t.  |<em>μ<sub>j</sub></em> | ≤ <em>η<sub>j</sub></em> , ∀<em>j</em> ∈ {1, …, <em>d</em>} for characterizing noise in linear interactions and matrix <span class="inline-equation"><span class="tex">$\mathbf {\Sigma } \in \mathbb {R}^{d \times d}$</span></span> s.t. <em>Σ</em> <sub><em>j</em>, <em>k</em></sub> = <em>σ<sub>j</sub>σ<sub>k</sub></em> , |<em>σ<sub>j</sub></em> | ≤ <em>ρ<sub>j</sub></em> , ∀<em>j</em> ∈ {1, …, <em>d</em>} for capturing noise induced by pairwise interaction terms. This choice is motivated by two reasons. The presence of <strong><em>Σ</em></strong> offers another degree of freedom while tuning the model. Also, order-2 interactions are being learnt in a latent space, which might not have similar semantics as the original feature space. This definition of <em>μ</em> and <em>Σ</em> confines the hyperparameter space to be linear in the number of features for a given training example. We now introduce these uncertainty terms and formulate <em>ϕ</em> for robust factorization machines (RFMs) as <em>ϕ<sub>RFM</sub></em> . For mathematical convenience, we add self-interaction terms to the robust variant.</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} &amp;\phi _{RFM}(\mathbf {x},\mathbf {w},\mathbf {V}, \text{$\mu $}, \mathbf {\Sigma }) \\ = &amp; w_{0} + \sum _{j=1}^{d}{w_{j}(x_{j} + \mu _{ j})} + \sum _{j = 1}^{d}{\sum _{k = j} ^ {d}{\langle \mathbf {v}_{j}, \mathbf {v}_{k}\rangle (x_{j}x_{k} + \Sigma _{j, k})}} \\ = &amp; w_{0} + \sum _{j=1}^{d}{w_{j}(x_{j} + \mu _{ j})} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j = 1}^{d}{\sum _{k = 1} ^ {d}{\langle \mathbf {v}_{j}, \mathbf {v}_{k}\rangle (x_{j}x_{k} + \Sigma _{j, k})}} \\ &amp; + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j=1}^{d}{ \langle \mathbf {v}_{j}, \mathbf {v}_{j}\rangle (x_{j}^{2} + \Sigma _{j, j})} \quad\text{ (Rearranging terms)}\\ = &amp; w_{0} + \sum _{j=1}^{d}{w_{j}x_{j}} + \sum _{j=1}^{d}{w_{j}\mu _{ j}} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j = 1}^{d}{\sum _{k = 1} ^ {d}{\langle \mathbf {v}_{j}, \mathbf {v}_{k}\rangle x_{j}x_{k}}} +\\ &amp; \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j = 1}^{d}{ \sum _{k = 1} ^ {d}{ \langle \mathbf {v}_{j}, \mathbf {v}_{k}\rangle \Sigma _{j, k}}} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j=1}^{d}{\langle \mathbf {v}_{j}, \mathbf {v}_{j}\rangle x_{j}^{2} } + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j=1}^{d}{\langle \mathbf {v}_{j}, \mathbf {v}_{j}\rangle \Sigma _{j,j}}\\ = &amp; w_{0} + \sum _{j=1}^{d}{w_{j}x_{j}} + \sum _{j=1}^{d}{w_{j}\mu _{ j}} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j = 1}^{d}{\sum _{k = 1} ^ {d}{ \sum _{f=1}^{p}{v_{j,f}v_{k,f}x_{j}x_{k}}}} +\\ &amp; \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j = 1}^{d}{ \sum _{k = 1} ^ {d}{ \sum _{f=1}^{p}{v_{j,f}v_{k,f} \Sigma _{j, k}}}} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j=1}^{d}{\sum _{f=1}^{p}{v_{j, f}^{2} x_{j}^{2}} } + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{j=1}^{d}{\sum _{f=1}^{p}{v_{j, f}^{2} \Sigma _{j,j}}}\\ &amp; \quad\text{ (Expanding terms along the factor space)}\\ = &amp; w_{0} + \sum _{j=1}^{d}{w_{j}x_{j}} + \sum _{j=1}^{d}{w_{j}\mu _{ j}} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{ v_{j, f}x_{j} }})^2 +\\ &amp; \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{ v_{j, f} \sigma _{j}})^{2}} + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ \sum _{j=1}^{d}{v_{j,f}^{2} x_{j}^{2}} } + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ \sum _{j=1}^{d}{v_{j,f}^{2} \sigma _{j}^{2}}}\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>With the redefined <em>ϕ</em> for RFM, the loss minimization view under uncertainties results in the following minimax formulation:</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} &amp; \min _{\mathbf {w}, \mathbf {V}} \max _{{{\scriptstyle {\begin{array}{*10c}\text{$\mu $}^{(j)}, \text{$\Sigma $}^{(j)}\\\forall 1\le j \le m\end{array}}}}} \frac{{{\scriptstyle {\begin{array}{*10c}\lambda \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \Vert \mathbf {w}\Vert _{2}^2 + \frac{{{\scriptstyle {\begin{array}{*10c}\lambda \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \Vert \mathbf {V}\Vert _{2}^2 + \frac{1}{m} \sum _{i=1}^{m}{\log {(1+\exp (\Omega _{RFM}^{i}))}} \\ \nonumber &amp; \text{where, } \Omega _{RFM}^{i} \text{ is shorthand for }\Omega _{RFM}(\mathbf {x}^{(i)}, y^{(i)}, \mathbf {w}, \mathbf {V}, \text{$\mu $}^{(i)}, \mathbf {\Sigma }^{(i)}) \\\nonumber &amp; \text{and, } \Omega _{RFM}(\mathbf {x}, y, \mathbf {w}, \mathbf {V}, \text{$\mu $}, \mathbf {\Sigma }) = -y \text{ }\phi _{RFM}(\mathbf {x}, \mathbf {w}, \mathbf {V}, \text{$\mu $}, \mathbf {\Sigma }).\end{align}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p></p>
        <p>The inner maximization signifies the worst case loss incurred because of the uncertainty parameters <em>μ</em> <sup>(<em>j</em>)</sup> and <em>Σ</em> <sup>(<em>j</em>)</sup>, ∀ 1 ≤ <em>j</em> ≤ <em>m</em>. Due to monotonicity of the terms in summation, maximizing the objective function in (<a class="eqn" href="#eq3">5</a>) reduces to maximizing <em>Ω<sub>RFM</sub></em> . We hereby refer to the optimal solution of the reduced subproblem as <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}$</span></span> .</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \begin{split} \Omega _{RFM}^{wc}{(\mathbf {x}, y, \mathbf {w}, \mathbf {V})} = \max _{\text{$\mu $}, \text{$\Sigma $}} \Omega _{RFM} {(\mathbf {x}, y, \mathbf {w}, \mathbf {V}, \text{$\mu $}, \mathbf {\Sigma })} \end{split} \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <p></p>
        <p>Further, we derive the value of <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}$</span></span> by obtaining upper bounds on the terms with uncertainties. Since the linear and pairwise uncertainty parameters have been considered independent, we can examine the relevant terms in isolation. We first group and analyze the terms associated with pairwise uncertainty <em>σ</em>.</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \begin{split} &amp; -y \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} (\sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{ v_{j,f} \sigma _{j}})^{2}} + \sum _{f=1}^{p} \sum _{j=1}^{d}{v_{j,f}^{2} \sigma _{j}^{2}}) \\ &amp; \le \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} (\sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{ |v_{j,f}| |\sigma _{j}|})^{2}} + \sum _{f=1}^{p} \sum _{j=1}^{d}{v_{j,f}^{2} \sigma _{j}^{2}}) \\ &amp; \le \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} (\sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{ |v_{j,f}| \rho _{j}})^{2}} + \sum _{f=1}^{p} \sum _{j=1}^{d}{v_{j,f}^{2} \rho _{j}^{2}}) \end{split} \end{equation}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>The last inequality follows from the definition of interval uncertainty, where all covariates are bounded independently. Similarly for the linear uncertainty terms we have,
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \begin{split} -y \sum _{j=1}^{d}{w_{j}\mu _{ j}} \le \sum _{j=1}^{d}{|w_{j}| |\mu _{j}|} \le \sum _{j=1}^{d}{|w_{j}| \eta _{j}}. \end{split} \end{equation}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>
        <p></p>
        <p>Using the upper bounds obtained from (<a class="eqn" href="#eq5">7</a>) and (<a class="eqn" href="#eq6">8</a>) on uncertainty terms, we derive the value for <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}$</span></span> as:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \Omega _{RFM}^{wc}(&amp;\mathbf {x},y,\mathbf {w},\mathbf {V}) = -yw_{0} + \sum _{j=1}^{d}{(-yw_{j}x_{j} + |w_{j}| \eta _{j})} - \frac{{{\scriptstyle {\begin{array}{*10c}y\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{v_{j, f}x_{j} }})^2 \\ &amp; +\frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ (\sum _{j = 1}^{d}{ |v_{j, f}| \rho _{j}})^{2}} - \frac{{{\scriptstyle {\begin{array}{*10c}y\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ \sum _{j=1}^{d}{v_{j,f}^{2} x_{j}^{2}} } + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \sum _{f=1}^{p}{ \sum _{j=1}^{d}{v_{j,f}^{2} \rho _{j}^{2}}}.\end{align*}</span><br />
          </div>
        </div>
        <p></p>
        <p>For notational convenience, we hereby refer to <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}(\mathbf {x},y,\mathbf {w},\mathbf {V})$</span></span> as <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}(\mathbf {x},y)$</span></span> . Using the derived value for <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}$</span></span> the optimization problem in (<a class="eqn" href="#eq3">5</a>) simplifies to:</p>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \min _{\mathbf {w}, \text{$V$}} \frac{{{\scriptstyle {\begin{array}{*10c}\lambda \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \Vert \mathbf {w}\Vert _{2}^2 + \frac{{{\scriptstyle {\begin{array}{*10c}\lambda \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}2\end{array}}}}} \Vert \mathbf {V}\Vert _{2}^2 + \frac{1}{m} \sum _{i=1}^{m}{\log {(1+\exp (\Omega _{RFM}^{wc}(\mathbf {x}^{(i)}, y^{(i)})))}}. \end{align}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Table of Notations.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Notation</th>
                <th style="text-align:left;">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><strong>x</strong> <sup>(<em>i</em>)</sup> (or <strong>x</strong>)</td>
                <td style="text-align:left;">Feature vector <span class="inline-equation"><span class="tex">$\in \mathbb {R}^d$</span></span> for sample <em>i</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>y</em> <sup>(<em>i</em>)</sup> (or <em>y</em>)</td>
                <td style="text-align:left;">Label ∈ { − 1, 1} for sample <em>i</em></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>x<sub>j</sub></em></td>
                <td style="text-align:left;">Feature value <span class="inline-equation"><span class="tex">$\in \mathbb {R}$</span></span> for <em>j<sup>th</sup></em> dimension in a sample</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>t</em></td>
                <td style="text-align:left;">Number of epochs</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>m</em></td>
                <td style="text-align:left;">Number of training samples</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>q</em></td>
                <td style="text-align:left;">Number of fields in the feature data</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>d</em></td>
                <td style="text-align:left;">Dimensionality of original feature space</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>p</em></td>
                <td style="text-align:left;">Dimensionality of latent factors learnt per feature</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>α</em></td>
                <td style="text-align:left;">Learning rate for stochastic gradient descent</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>λ</em></td>
                <td style="text-align:left;">Regularization parameter</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>w</strong></td>
                <td style="text-align:left;">Weight vector <span class="inline-equation"><span class="tex">$\in \mathbb {R}^{d}$</span></span> for linear interactions</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>V</strong></td>
                <td style="text-align:left;">Factor matrix <span class="inline-equation"><span class="tex">$\in \mathbb {R}^{d \times p}$</span></span> for FM</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left;">and tensor <span class="inline-equation"><span class="tex">$\in \mathbb {R}^{d \times q \times p}$</span></span> for FFM</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>μ</em> <sup>(<em>i</em>)</sup> (or <em>μ</em>)</td>
                <td style="text-align:left;">Linear uncertainty vector <span class="inline-equation"><span class="tex">$\in \mathbb {R}^{d}$</span></span> for a sample</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>Σ</em> <sup>(<em>i</em>)</sup> (or <em>Σ</em>)</td>
                <td style="text-align:left;">Pairwise uncertainty matrix <span class="inline-equation"><span class="tex">$\in \mathbb {R}^{d \times d}$</span></span> for a sample</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>η</em> <sup>(<em>i</em>)</sup> (or <em>η</em>)</td>
                <td style="text-align:left;">Linear uncertainty bound <span class="inline-equation"><span class="tex">$\in \mathbb {R}_{\ge 0}^{d}$</span></span> for a sample</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ρ</em> <sup>(<em>i</em>)</sup> (or <em>ρ</em>)</td>
                <td style="text-align:left;">Simplified pairwise uncertainty bound <span class="inline-equation"><span class="tex">$\in \mathbb {R}_{\ge 0}^{d}$</span></span></td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:left;">for a sample</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Note that by minimizing the worst case loss, we are encoding pessimism in the classifier, the magnitude of which varies with the size of hyperrectangles in which the data is bounded. Table <a class="tbl" href="#tab1">1</a> summarizes the notations used in this paper.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Parameter Learning: Robust FM</h3>
          </div>
        </header>
        <p>We use minibatch stochastic gradient descent (SGD) to solve the optimization problem (<a class="eqn" href="#eq7">9</a>) for robust FM. The corresponding loss gradient is given by:</p>
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \frac{{{\scriptstyle {\begin{array}{*10c}\delta \mathcal {L}\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}\delta \theta \end{array}}}}} = \lambda \theta + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}m\end{array}}}}}\sum _{i=1}^{m}{ \frac{{{\scriptstyle {\begin{array}{*10c}{\exp {(\Omega _{RFM}^{wc}(\mathbf {x}^{(i)}, y^{(i)}))}}\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}1 + \exp {(\Omega _{RFM}^{wc}(\mathbf {x}^{(i)}, y^{(i)}))}\end{array}}}}} * \frac{{{\scriptstyle {\begin{array}{*10c}\delta \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}\delta \theta \end{array}}}}} (\Omega _{RFM}^{wc}(\mathbf {x}^{(i)}, y^{(i)})) } \end{equation}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>where,
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation*} \begin{split} &amp; \frac{{{\scriptstyle {\begin{array}{*10c}\delta \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}\delta \theta \end{array}}}}} (\Omega _{RFM}^{wc}(\mathbf {x}, y)) = \left\lbrace \begin{array}{@{}l@{\quad }l@{}}-y &amp; \text{if } \theta = w_{0} \\ -yx_{j} + \eta _{j} \text{sgn}(w_{j}) &amp; \text{if } \theta = w_{j} \\ -yx_{j} \sum _{k=1}^{d}{v_{k,f} x_{k}} -yv_{j,f}x_{j}^{2} + \\\rho _{j} \text{sgn}(v_{j,f})\sum _{k=1}^{d}{|v_{k,f}|\rho _{k}} + v_{j,f}\rho _{j}^{2} &amp; \text{if } \theta = v_{j,f}. \end{array}\right. \end{split}\end{equation*}</span><br />
          </div>
        </div>Here sgn(.) represents the sign function. Note that the update rule is composed of deterministic and uncertainty terms, the latter being independent of <em>y</em> since we have arrived at the formulation by maximizing <em>Ω</em>. The terms <span class="inline-equation"><span class="tex">$\sum _{k=1}^{d}{v_{k,f} x_{k}}$</span></span> and <span class="inline-equation"><span class="tex">$\sum _{k=1}^{d}{|v_{k,f}|\rho _{k}}$</span></span> being independent of <em>j</em> can be computed in advance. The details of our approach are outlined in Algorithm 1 .
        <p></p>
        <p><img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-img1.png" class="img-responsive" alt="" longdesc="" /></p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Robust FFM</h3>
          </div>
        </header>
        <p>We now derive the robust counterpart for field-aware factorization machine (FFM), the more rigorous and expressive variant of FMs. Incorporating linear and pairwise uncertainty parameters in the original function <em>ϕ<sub>FFM</sub></em> in equation (<a class="eqn" href="#eq2">3</a>) yields <em>ϕ<sub>RFFM</sub></em> .</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation*} \begin{split} \phi _{RFFM} (\mathbf {x},\mathbf {w},\mathbf {V}, \text{$\mu $}, \mathbf {\Sigma }) &amp;= w_{0} + \sum _{j=1}^{d}{w_{j}(x_{j} + \mu _{j})} \\ &amp; + \sum _{j = 1}^{d}{\sum _{k = j} ^ {d}{ \langle \mathbf {v}_{j, f_{k}}, \mathbf {v}_{k, f_{j}}\rangle (x_{j}x_{k} + \sigma _{j}\sigma _{k})}} \end{split}\end{equation*}</span><br />
          </div>
        </div>Note that <strong>V</strong> is a tensor comprising of latent vectors learnt per (feature, field) combination. Following steps as in Section <a class="sec" href="#sec-8">3.1</a> we derive an upper bound on − <em>yϕ<sub>RFFM</sub></em> (or <em>Ω<sub>RFFM</sub></em> ), given by <span class="inline-equation"><span class="tex">$\Omega _{RFFM}^{wc}$</span></span> .
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation*} \begin{split} \Omega _{RFFM}^{wc}(&amp; \mathbf {x}, y, \mathbf {w},\mathbf {V}) = -yw_{0} + \sum _{j=1}^{d}{(-yw_{j}x_{j} + |w_{j}|\eta _{j})} \\ &amp; - y\sum _{j = 1}^{d}{\sum _{k = j} ^ {d}{ \langle \mathbf {v}_{j, f_{k}}, \mathbf {v}_{k, f_{j}}\rangle x_{j}x_{k} }} + \sum _{j = 1}^{d}{\sum _{k = j} ^ {d} { \langle |\mathbf {v}_{j, f_{k}}|, |\mathbf {v}_{k, f_{j}}|}\rangle \rho _{j}\rho _{k}} \end{split}\end{equation*}</span><br />
          </div>
        </div>Replacing <span class="inline-equation"><span class="tex">$\Omega _{RFM}^{wc}$</span></span> with <span class="inline-equation"><span class="tex">$\Omega _{RFFM}^{wc}$</span></span> in (<a class="eqn" href="#eq7">9</a>) gives the loss minimization problem for RFFM.
        <p></p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Parameter Learning: Robust FFM</h3>
          </div>
        </header>
        <p>We employ stochastic gradient descent (SGD) for parameter estimation. The gradient of loss function is given by:</p>
        <div class="table-responsive" id="Xeq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \frac{{{\scriptstyle {\begin{array}{*10c}\delta \mathcal {L}\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}\delta \theta \end{array}}}}} = \lambda \theta + \frac{{{\scriptstyle {\begin{array}{*10c}1\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}m\end{array}}}}} \sum _{i=1}^{m} \frac{{{\scriptstyle {\begin{array}{*10c}{\exp {(\Omega _{RFFM}^{wc}(\mathbf {x}^{(i)}, y^{(i)}))}}\end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}1 + \exp {(\Omega _{RFFM}^{wc}(\mathbf {x}^{(i)}, y^{(i)}))}\end{array}}}}} * \frac{{{\scriptstyle {\begin{array}{*10c}\delta \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}\delta \theta \end{array}}}}} (\Omega ^{wc}_{RFFM}(\mathbf {x}^{(i)}, y^{(i)})) \end{equation}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>where,
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation*} \begin{split} &amp; \frac{{{\scriptstyle {\begin{array}{*10c}\delta \end{array}}}}}{{{\scriptstyle {\begin{array}{*10c}\delta \theta \end{array}}}}} \Omega ^{wc}_{RFFM}(\mathbf {x}, y)= \left\lbrace \begin{array}{@{}l@{\quad }l@{}}-y &amp; \text{if } \theta = w_{0} \\ -yx_{j} + \eta _{j} \text{sgn}(w_{j}) &amp; \text{if } \theta = w_{j} \\ -yv_{k, f_{j}}x_{j} x_{k} + \\\text{sgn}(v_{j,f_{k}}) |v_{k, f_{j}}| \rho _{j} \rho _{k} &amp; \text{if } \theta = v_{j,f_{k}}. \end{array}\right. \end{split}\end{equation*}</span><br />
          </div>
        </div>The final algorithm for RFFM differs from RFM in the core weight update steps in Algorithm 1 . Also, similar to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], we perform updates only on the non zero dimensions of the weight matrix to avoid unnecessary computation.
        <p></p>
      </section>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experiments</h2>
        </div>
      </header>
      <p>In this section we investigate the effectiveness of RFM and RFFM against their non-robust counterparts. In particular, we (i) evaluate the prediction quality of robust classifiers on original and perturbed datasets, (ii) examine the noise resilience arising from the choice of uncertainty sets, (iii) empirically compare different initialization strategies for the weight matrix, (iv) assess the impact of hyperparameters on model performance, (v) explore isotonic regression for calibration of classifiers, and (vi) study model convergence rate with increased parallelism. Our experimental findings reveal that by incorporating the notion of robustness, the resulting classifiers take a slight performance hit for the unperturbed datasets, but outperform the original formulations significantly when presented with noisy measurements.</p>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Experimental Setup</h3>
          </div>
        </header>
        <section id="sec-14">
          <p><em>4.1.1 Dataset Description.</em> We evaluate our formulations on three publicly available real-world datasets. These encompass both clickthrough rate (CTR) and conversion rate (CVR) prediction settings, which are two central problems for large scale user response prediction.</p>
          <ul class="list-no-style">
            <li id="list8" label="•"><strong>Criteo CTR Prediction</strong> Released for a Kaggle competition in 2014, this dataset has become an important benchmark for CTR estimation. The training data comprises of 45 million ad impressions served to users along with their online footprint in the form of 13 integer features and 26 hashed categorical features. Label indicates whether a user subsequently clicked on the ad or not. One-hot encoding of the categorical variables results in a feature space of size ∼ 10<sup>6</sup>.<br /></li>
            <li id="list9" label="•"><strong>Avazu CTR Prediction</strong> This dataset was released as part of a Kaggle challenge by Avazu advertising platform. It contains ten days of clickthrough data on mobile devices. The feature set comprises of signals like hour of the day, banner position, site id, device model, etc.<br /></li>
            <li id="list10" label="•"><strong>Criteo Conversion Logs</strong> This dataset consists of conversion feedback signals for a portion of Criteo's ad traffic. In each row of the dataset, features represent an ad served to a user and a conversion timestamp label indicates when the user converted. If there is no purchase by the user, the field is empty. It is used widely for standardization of CVR algorithms.<br /></li>
          </ul>
          <p>The dataset statistics are summarized in Table <a class="tbl" href="#tab2">2</a>. For brevity, we sometimes refer to Criteo click and conversion datasets as CriClick and CriConv respectively. In addition to the performance evaluation on these computational advertising datasets, we include a case study on a credit card fraud detection dataset in Section <a class="sec" href="#sec-24">5</a> to highlight that RFM and RFFM can characterize noise across domains.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span> <span class="table-title">Summary Statistics of Datasets.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Dataset</th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\#$</span></span> Instances</th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\#$</span></span> Features</th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\#$</span></span> Fields</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">Criteo CTR Prediction</td>
                  <td style="text-align:center;">45,840,617</td>
                  <td style="text-align:center;">10<sup>6</sup></td>
                  <td style="text-align:center;">39</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Avazu CTR Prediction</td>
                  <td style="text-align:center;">40,428,967</td>
                  <td style="text-align:center;">10<sup>6</sup></td>
                  <td style="text-align:center;">33</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Criteo Conversion Logs</td>
                  <td style="text-align:center;">15,898,883</td>
                  <td style="text-align:center;">10<sup>4</sup></td>
                  <td style="text-align:center;">17</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-15">
          <p><em>4.1.2 Evaluation Metric.</em> For maximizing the efficiency of an ad campaign, the class probabilities estimated by a classifier need to be well calibrated since they directly impact the subsequent bidding for auctions. Hence we use logloss as the benchmarking metric for assessing model quality. Logloss (also known as logistic loss or cross entropy loss) is defined for the binary classification setting as:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation*} -\frac{1}{m}\sum _{i =1}^{m}{y^{(i)}\log (p^{(i)}) + (1-y^{(i)})\log (1-p^{(i)})}\end{equation*}</span><br />
            </div>
          </div>where <em>p</em> <sup>(<em>i</em>)</sup> is the probability or the confidence estimate assigned by a classifier for sample <em>i</em> belonging to the positive class, and <em>y</em> <sup>(<em>i</em>)</sup> ∈ {0, 1} is the true label. Logloss metric possesses an information theoretic interpretation of being the cross entropy between the true and predicted class distributions. An ideal model will have zero logloss. Lower values of this metric imply less divergence from true labels and hence superior model performance.
          <p></p>
        </section>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Implementation Details</h3>
          </div>
        </header>
        <p>We have implemented RFM and RFFM on Apache Spark [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], a distributed computing framework facilitating efficient parallelization, which is crucial for timely processing of the current massive datasets. Spark provides fault tolerant data storage abstraction: RDD (Resilient Distributed Dataset), which is an immutable collection of data partitioned across cluster nodes. The data is stored in-memory, which is highly favorable for iterative workloads.</p>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Spark implementation workflow for robust factorization machines.</span>
          </div>
        </figure>
        <p></p>
        <p>We employ <em>iterative parameter mixing</em> strategy for performing distributed stochastic gradient descent (SGD) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>]. Figure <a class="fig" href="#fig3">3</a> outlines the implementation workflow. (1) Input data is partitioned across executors. (2) An initial weight matrix is broadcasted to all data partitions. Each node performs a single update of minibatch SGD on the subset of data it contains. (3) After every epoch, models learnt independently by the nodes are averaged. (4) The resulting global model is broadcasted again. (5) The algorithm terminates when error between successive iterations falls below a threshold. This distributed training strategy demonstrates fast convergence owing to the synchronous nature of model updates.</p>
        <p>Memory requirement of our formulations is proportional to the number of features in a dataset. Distributed SGD adds an additional latency in terms of model transmission cost over the network after each epoch. Model compactness is therefore imperative for driving efficient performance. Owing to high dimensionality of the feature space, we resort to the hashing trick [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>], which uses a simple hash function to restrict the number of feature values.</p>
        <p>In our experimentation, we use 80<span class="inline-equation"><span class="tex">$\%$</span></span> of the data for training and 10% each for constructing validation and test sets. Also, since our goal is to examine the difference between the robust and non-robust variants, we refrain ourselves from delving into feature engineering and focus exclusively on the model specific aspects. The code and dataset links for our implementation are available on Dropbox<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a> for experimental reproducibility.</p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Choice of Uncertainty Sets</h3>
          </div>
        </header>
        <p>Having made the design choice of considering box-type uncertainty around data points in order to facilitate independence amongst covariates, the next critical step is to associate uncertainty bounds (<em>η</em>, <em>ρ</em>) with each training example. A straightforward approach is <em>absolute assignment</em> i.e. keeping these variables as additional parameters whose optimal values can be determined by parameter tuning. However, this is an expensive solution which would explode the hyperparameter space and render the possibility of obtaining the best performing model infeasible under practical settings. Another approach which seems appealing at first is to have <em>field specific</em> uncertainty values so that the number of newly introduced parameters is bounded. This approach however has the following drawbacks: it is tightly coupled with the dataset at hand and no direct link can be established between parameters selected and noise-proofness of the model procured after training.</p>
        <p>These concerns encourage us to adopt the strategy of <em>relative assignment</em> for our experimentation. In this approach we select two positive real valued parameters (<span class="inline-equation"><span class="tex">$\eta _{\%}$</span></span> , <span class="inline-equation"><span class="tex">$\rho _{\%}$</span></span> ) such that for every measurement <em>γ</em>, the effective linear and pairwise uncertainty bounds are given by (<span class="inline-equation"><span class="tex">$\eta _{\%}*\gamma$</span></span> , <span class="inline-equation"><span class="tex">$\rho _{\%}*\gamma$</span></span> ). This simple trick significantly brings down the size of parameters to tune and at the same time retains the feasibility of assigning variable sized hyperrectangles around the data. Under this formulation, larger measurements are associated with higher variability or lower confidence. Additionally, we threshold the uncertainty bounds to moderate the incorporated noise. As we shall present in the results below, this methodology of designing uncertainty sets has a nice interpretability in terms of cost incurred by incorporating robustness v/s resistance gained under noisy settings.</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Performance Comparison</h3>
          </div>
        </header>
        <p>We compare the performance of RFM and RFFM models against the original factorization machine formulations on the conversion and click datasets. We particularly focus on the relative behavior under noisy settings. Gaussian distribution is a popular noise model in signal processing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>]. On similar lines, we simulate noise in original datasets by adding a Gaussian perturbation <span class="inline-equation"><span class="tex">$\mathcal {N} \sim (\mu _{noise}, \sigma _{noise})$</span></span> to the test data. We vary the noise parameters and examine the goodness of classifiers for both original and perturbed versions of datasets. The results are presented in Table <a class="tbl" href="#tab3">3</a>. By adhering to worst case loss minimization, the robust classifiers take a conservative view even for the original datasets, resulting in a higher logloss as compared to the non-robust equivalents. However, when subjected to noise, the average performance degradation of robust classifiers is remarkably lower than the FMs and FFMs. In the risk sensitive domain of user modeling where signals captured might not be representative of complete user intent, this graceful degradation is a desirable property.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">Comparative Analysis of Robust Formulations. <span class="inline-equation"><span class="tex">$\mathcal {L}_{M}$</span></span> indicates logloss of model <em>M</em>. For Gaussian perturbation, the loss is averaged over <em>μ<sub>noise</sub></em> ∈ {0.0001, 0.001, 0.01, 0.1, 1.0}, <em>σ<sub>noise</sub></em> ∈ {0.01, 0.1, 0.3, 0.5, 0.7, 0.9} and over <em>λ<sub>noise</sub></em> ∈ {0.01, 0.1, 0.2, 0.3, 0.5, 0.6, 0.8, 1.0} for Poisson noise model. Lower logloss indicates better model. Robust counterparts take performance hit for original datasets but outperform the base classifiers under perturbed settings.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td><img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-img2.png" class="img-responsive" alt="" longdesc="" /></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The levers offered by robust formulations for regulating uncertainty bounds are <span class="inline-equation"><span class="tex">$(\eta _{\%} , \rho _{\%})$</span></span> . Higher values imply higher uncertainty accounted for by the trained classifiers and hence greater immunity against noise. We train multiple RFM and RFFM models by varying <span class="inline-equation"><span class="tex">$(\eta _{\%} , \rho _{\%})$</span></span> and study the relative reduction in logloss against the non-robust variants for different noise configurations. This relative reduction is given by <span class="inline-equation"><span class="tex">$\Delta _{logloss}\%=$</span></span> <span class="inline-equation"><span class="tex">$\frac{(\mathcal {L}_{Original} - \mathcal {L}_{Robust})}{\mathcal {L}_{Original}}*$</span></span> 100 , where <span class="inline-equation"><span class="tex">$\mathcal {L}_{{Original}{6pt}}$</span></span> and <span class="inline-equation"><span class="tex">$\mathcal {L}_{{Robust}{5pt}}$</span></span> indicate loss under original and robust formulations respectively. As is evident from Figure <a class="fig" href="#fig4">4</a>, for each <span class="inline-equation"><span class="tex">$(\eta _{\%} , \rho _{\%}$</span></span> ) the robust classifier starts off with a higher logloss w.r.t. baseline (<span class="inline-equation"><span class="tex">$\Delta _{logloss}\%$</span></span> &lt; 0) when the <em>σ<sub>noise</sub></em> is low. However, on increasing the standard deviation of noise, the reduction in logloss is pronounced and goes as high as 40% for some cases. These findings not only reinforce the fact that the proposed classifiers indeed demonstrate superiority under noisy measurements, but also present an interesting tradeoff between aiming for high robustness and the price to be paid in the unperturbed settings.</p>
        <figure id="fig4">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span> <span class="figure-title">Study of classifier behavior for different <span class="inline-equation"><span class="tex">$(\eta _{\%} , \rho _{\%}$</span></span> ) when subjected to perturbed variants of the original dataset during the test phase. A Gaussian noise with <em>μ<sub>noise</sub></em> = 0.1 with varying <em>σ<sub>noise</sub></em> ∈ {0.01, 0.1, 0.3, 0.5, 0.7, 0.9} is added to the test samples. Higher values of <em>Δ<sub>logloss</sub></em> % indicates superior noise resilience.</span>
          </div>
        </figure>
        <p>In traditional signal processing systems, Poisson process is another widely used model for capturing noise. In an attempt to provide a comprehensive treatment to performance study under noise, we experiment with this noise model as well. Our findings are aligned with the insights procured for Gaussian noise model. We provide an interesting subset of results on the Avazu dataset in Figure <a class="fig" href="#fig5">5</a>.</p>
        <figure id="fig5">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span> <span class="figure-title">Relative logloss reduction offered by robust formulations under Poisson noise for <em>λ<sub>noise</sub></em> ∈ {0.01, 0.1, 0.2, 0.3, 0.5, 0.6, 0.8, 1.0}.</span>
          </div>
        </figure>
        <p>These results reinforce the fact that the robust formulations are indeed able to withstand the potential incompleteness and corruption in response prediction datasets. Here we would like to reiterate the fact that our formulations are generic and can be applied to any domain where data uncertainty is a concern. The model designer can select <span class="inline-equation"><span class="tex">$(\eta _{\%} , \rho _{\%})$</span></span> parameters in accordance with the degree of uncertainty, for the problem at hand.</p>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Discussion</h3>
          </div>
        </header>
        <section id="sec-20">
          <p><em>4.5.1 Initialization Strategy.</em> Non-convexity of the optimization problem for factorization machines makes the selection of initial weight matrix pivotal to the optimality of results obtained. The traditional FMs employ Gaussian distribution for initializing model weights. Laplace distribution has recently been proposed [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] as a superior initializer owing to the fact that it has a higher peak than Gaussian and consequently results in a better fit for sparse datasets. For FFMs, sampling from Uniform distribution is another commonly adopted initialization approach [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>]. We investigate the impact of different initialization strategies. The results summarized in Table <a class="tbl" href="#tab4">4</a> indicate that Laplace distribution outperforms Gaussian and Uniform distributions in terms of logloss for both RFM and RFFM across the three datasets.</p>
          <div class="table-responsive" id="tab4">
            <div class="table-caption">
              <span class="table-number">Table 4:</span> <span class="table-title">Performance comparison of different initialization approaches. Logloss <span class="inline-equation"><span class="tex">$\mathcal {L}_{S}$</span></span> is being incurred under initialization strategy <em>S</em>.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Dataset</th>
                  <th style="text-align:center;">Model</th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\mathcal {L}_{Gaussian}$</span></span></th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\mathcal {L}_{Laplace}$</span></span></th>
                  <th style="text-align:center;"><span class="inline-equation"><span class="tex">$\mathcal {L}_{Uniform}$</span></span></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">CriClick</td>
                  <td style="text-align:center;">RFM</td>
                  <td style="text-align:center;">0.4538</td>
                  <td style="text-align:center;">0.4532</td>
                  <td style="text-align:center;">0.4541</td>
                </tr>
                <tr style="border-bottom: solid 2px;">
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RFFM</td>
                  <td style="text-align:center;">0.4619</td>
                  <td style="text-align:center;">0.4582</td>
                  <td style="text-align:center;">0.4598</td>
                </tr>
                <tr>
                  <td style="text-align:center;">CriConv</td>
                  <td style="text-align:center;">RFM</td>
                  <td style="text-align:center;">0.3809</td>
                  <td style="text-align:center;">0.3780</td>
                  <td style="text-align:center;">0.3875</td>
                </tr>
                <tr style="border-bottom: solid 2px;">
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RFFM</td>
                  <td style="text-align:center;">0.3784</td>
                  <td style="text-align:center;">0.3769</td>
                  <td style="text-align:center;">0.3774</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Avazu</td>
                  <td style="text-align:center;">RFM</td>
                  <td style="text-align:center;">0.3944</td>
                  <td style="text-align:center;">0.3942</td>
                  <td style="text-align:center;">0.3952</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RFFM</td>
                  <td style="text-align:center;">0.3952</td>
                  <td style="text-align:center;">0.3939</td>
                  <td style="text-align:center;">0.3941</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-21">
          <p><em>4.5.2 Impact of Hyperparameters.</em> Parameter tuning is an important step for deriving optimal performance from any learning algorithm. From Figure <a class="fig" href="#fig6">6</a>(a), we observe that model (RFM/RFFM) performance improves with number of epochs <em>t</em>, though there is a diminishing returns property evident in improvement, which seems intuitive. This trend is consistent for all the datasets.</p>
          <p>Gradient descent approaches are sensitive to the selection of learning rate <em>α</em>. As can be observed from Figure <a class="fig" href="#fig6">6</a>(b), for the same number of epochs, choosing smaller step size results in higher logloss since not enough exploration has been performed in the loss function landscape. On the other hand, selecting very high values of <em>α</em> might result in skipping the minima altogether leading to an increased loss value.</p>
          <figure id="fig6">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig6.jpg" class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span> <span class="figure-title">Effect of hyperparameters.</span>
            </div>
          </figure>
          <p></p>
          <p>Regularization parameter plays a key role in preventing model overfitting. RFMs and RFFMs exhibit less sensitivity to changes in the value of <em>λ</em> as illustrated in Figure <a class="fig" href="#fig6">6</a>(c). This observation suggests that robustness inherently imposes some degree of regularization.</p>
          <p>Higher number of latent factors <em>p</em> results in models possessing better generalization ability. However, for the distributed implementation of gradient descent, a large value of <em>p</em> translates into increased weight matrix serialization overhead and network communication cost for model synchronization among nodes. The reduction in logloss with <em>p</em> and the corresponding increase in training time are depicted in Figure <a class="fig" href="#fig6">6</a>(d).</p>
          <figure id="fig7">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig7.jpg" class="img-responsive" alt="" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 7:</span> <span class="figure-title">Classifier calibration using isotonic regression.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-22">
          <p><em>4.5.3 Model Calibration.</em> Training models that give accurate probabilistic outputs is central to the user modeling problem since it plays a major role in subsequent bidding. Calibration plots a.k.a. reliability diagrams serve as useful visualization tools for assessing goodness of model predictions with respect to true posterior probabilities. For each bucket of model prediction, the mean true probability is the fraction of positive samples in the bucket. The output of a perfectly calibrated classifier can be represented as a diagonal line on the reliability curve. To calibrate the model outputs, we employ isotonic regression technique [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] of univariate curve fitting with monotonicity constraints, where model outputs serve as regressors and actual label is the dependent variable.</p>
          <p>We calibrate the probability estimates generated by RFMs, RFFMs and their non-robust counterparts and investigate the relative improvement in logloss. After this postprocessing, we observe a higher marginal improvement in the calibration quality of robust classifiers. Figure <a class="fig" href="#fig7">7</a>(a) depicts the reliability curve for Avazu FFM and RFFM before and after applying isotonic regression. The percentage logloss reduction achieved as a result of calibration for the three datasets is presented in Figure <a class="fig" href="#fig7">7</a>(b).</p>
        </section>
        <section id="sec-23">
          <p><em>4.5.4 Impact of Parallelism on Model Convergence.</em> Degree of parallelism has an inverse relationship with model convergence rate. Increasing the number of RDD partitions results in gradient descent being applied on smaller subsets of data and hence the averaged global model, procured after each epoch, is less stable. The downside of keeping lesser partitions is that each parallel worker is delegated with large number of samples, which increases the time taken per iteration. Figure <a class="fig" href="#fig8">8</a> demonstrates the classic tradeoff between training time speedup v/s classification quality.</p>
          <figure id="fig8">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig8.jpg" class="img-responsive" alt="" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 8:</span> <span class="figure-title">Effect of parallelism on convergence rate.</span>
            </div>
          </figure>
        </section>
      </section>
    </section>
    <section id="sec-24">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Case Study: Fraud Detection</h2>
        </div>
      </header>
      <figure id="fig9">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/images/www2018-157-fig9.jpg" class="img-responsive" alt="Figure 9" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 9:</span> <span class="figure-title"><em>Δ<sub>logloss</sub></em> % for perturbed variants of fraud detection dataset where <em>μ<sub>noise</sub></em> ∈ {0.05, 0.10, 0.15, 0.20, 0.30}. Logloss is averaged over <em>σ<sub>noise</sub></em> ∈ {0.001, 0.01, 0.1, 0.2, 0.3, 0.5, 0.7} for each <em>μ<sub>noise</sub></em> .</span>
        </div>
      </figure>
      <p>The proposed formulations RFM and RFFM add a significant value for user response prediction under perturbed settings, as established by the experiments in Section <a class="sec" href="#sec-12">4</a>. However, these are generic predictors, not restricted to the computational advertising domain and can be employed in any noise-sensitive classification scenario. To substantiate this claim, we test our formulations in the field of credit card fraud detection [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>]. The dataset comprises of 284,807 anonymized credit card transactions and the challenge is to label them as fraudulent or genuine. The feature set is composed of 28 PCA transformed numerical variables with 0.172% of transactions labeled as fraud. Absence of categorical features renders RFM (or FM) and RFFM (or FFM) formulations equivalent for this problem. The metric <em>Δ<sub>logloss</sub></em> % (as defined in Section <a class="sec" href="#sec-18">4.4</a>), which captures the logloss reduction achieved by the robust variants w.r.t. original FMs, exhibits dramatic improvement as the magnitude of Gaussian noise increases. This is evident from Figure <a class="fig" href="#fig9">9</a>, demonstrating superior noise resilience offered by RFMs. These findings are encouraging since they highlight domain independence of our formulations.</p>
    </section>
    <section id="sec-25">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion and Future Work</h2>
        </div>
      </header>
      <p>The ever increasing customer touchpoints and the associated noise sources have created a pressing need to design algorithms which take into account input uncertainty for user modeling. To this end, we have proposed novel robust formulations for factorization machines and field-aware factorization machines. The distributed Spark based implementation for RFM and RFFM seamlessly scales to massive real-world datasets. Experimental evidence establishes a consistently superior noise resilience of the proposed formulations. This opens up new avenues for utilizing the combined power of robust optimization and traditional factorization machine models. As a future work, benchmarking the effectiveness of RFMs and RFFMs across a breadth of classification settings is a promising area of investigation. Exploring other choices of uncertainty models, like ellipsoidal and conic models is another interesting research direction. Applying RO principles to tree based ensembles and deep learning is yet another unexplored territory. The pursuit of the question of whether incorporating the notion of worst case loss minimization for these highly expressive models results in higher generalization power, might reveal deeper insights about the models themselves.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Adroll. 2016. <em><em>Factorization Machines</em></em> . <a href="http://tech.adroll.com/blog/data-science/2015/08/25/factorization-machines.html" target="_blank">http://tech.adroll.com/blog/data-science/2015/08/25/factorization-machines.html</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">Dimitris Bertsimas, David&nbsp;B. Brown, and Constantine Caramanis. 2011. Theory and Applications of Robust Optimization. <em><em>SIAM Rev.</em></em> 53, 3 (Aug. 2011), 464–501. <a class="link-inline force-break" href="https://doi.org/10.1137/080734510" target="_blank">https://doi.org/10.1137/080734510</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Alexey Borisov, Ilya Markov, Maarten de Rijke, and Pavel Serdyukov. 2016. A Neural Click Model for Web Search. In <em><em>Proceedings of the 25th International Conference on World Wide Web</em></em> (WWW ’16). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 531–541. <a class="link-inline force-break" href="https://doi.org/10.1145/2872427.2883033" target="_blank">https://doi.org/10.1145/2872427.2883033</a>
        </li>
        <li id="BibPLXBIB0004" label="[4]">Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and Scalable Response Prediction for Display Advertising. <em><em>ACM Trans. Intell. Syst. Technol.</em></em> 5, 4, Article 61 (Dec. 2014), 34&nbsp;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2532128" target="_blank">https://doi.org/10.1145/2532128</a>
        </li>
        <li id="BibPLXBIB0005" label="[5]">Comscore. 2015. <em><em>Lessons Learned: Maximizing Returns with Digital Media</em></em> . <a href="https://www.comscore.com/Insights/Presentations-and-Whitepapers/2015/Lessons-Learned-Maximizing-Returns-with-Digital-Media" target="_blank">https://www.comscore.com/Insights/Presentations-and-Whitepapers/2015/Lessons-Learned-Maximizing-Returns-with-Digital-Media</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">Criteo. 2016. <em><em>The State of Cross-Device Commerce</em></em> . <a href="http://www.criteo.com/resources/cross-device-commerce-report-h2-2016/" target="_blank">http://www.criteo.com/resources/cross-device-commerce-report-h2-2016/</a>
        </li>
        <li id="BibPLXBIB0007" label="[7]">Laurent El&nbsp;Ghaoui, Gert R.&nbsp;G. Lanckriet, and Georges Natsoulis. 2003. <em><em>Robust Classification with Interval Data</em></em> . Technical Report UCB/CSD-03-1279. EECS Department, University of California, Berkeley. <a href="http://www2.eecs.berkeley.edu/Pubs/TechRpts/2003/5772.html" target="_blank">http://www2.eecs.berkeley.edu/Pubs/TechRpts/2003/5772.html</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">Emarketer. 2015. <em><em>When Will Mobile Marketers Move Beyond Basic Measurement?</em></em> <a href="https://www.emarketer.com/Article/Will-Mobile-Marketers-Move-Beyond-Basic-Measurement/1012600" target="_blank">https://www.emarketer.com/Article/Will-Mobile-Marketers-Move-Beyond-Basic-Measurement/1012600</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">He&nbsp;Xinran et. al. 2014. Practical Lessons from Predicting Clicks on Ads at Facebook. In <em><em>Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</em></em> (ADKDD’14). ACM, New York, NY, USA, Article 5, 9&nbsp;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2648584.2648589" target="_blank">https://doi.org/10.1145/2648584.2648589</a>
        </li>
        <li id="BibPLXBIB0010" label="[10]">McMahan et. al. 2013. Ad Click Prediction: A View from the Trenches. In <em><em>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> (KDD ’13). ACM, New York, NY, USA, 1222–1230. <a class="link-inline force-break" href="https://doi.org/10.1145/2487575.2488200" target="_blank">https://doi.org/10.1145/2487575.2488200</a>
        </li>
        <li id="BibPLXBIB0011" label="[11]">Bram&nbsp;L. Gorissen, İhsan Yanikoğlu, and Dick den Hertog. 2015. A practical guide to robust optimization. <em><em>Omega</em></em> 53, Supplement C (2015), 124 – 137. <a class="link-inline force-break" href="https://doi.org/10.1016/j.omega.2014.12.006" target="_blank">https://doi.org/10.1016/j.omega.2014.12.006</a>
        </li>
        <li id="BibPLXBIB0012" label="[12]">Yuchin Juan, Damien Lefortier, and Olivier Chapelle. 2017. Field-aware Factorization Machines in a Real-world Online Advertising System. In <em><em>Proceedings of the 26th International Conference on World Wide Web Companion</em></em> (WWW ’17 Companion). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 680–688. <a class="link-inline force-break" href="https://doi.org/10.1145/3041021.3054185" target="_blank">https://doi.org/10.1145/3041021.3054185</a>
        </li>
        <li id="BibPLXBIB0013" label="[13]">Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware Factorization Machines for CTR Prediction. In <em><em>Proceedings of the 10th ACM Conference on Recommender Systems</em></em> (RecSys ’16). ACM, New York, NY, USA, 43–50. <a class="link-inline force-break" href="https://doi.org/10.1145/2959100.2959134" target="_blank">https://doi.org/10.1145/2959100.2959134</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Sungchul Kim, Nikhil Kini, Jay Pujara, Eunyee Koh, and Lise Getoor. 2017. Probabilistic Visitor Stitching on Cross-Device Web Logs. In <em><em>Proceedings of the 26th International Conference on World Wide Web</em></em> (WWW ’17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 1581–1589. <a class="link-inline force-break" href="https://doi.org/10.1145/3038912.3052711" target="_blank">https://doi.org/10.1145/3038912.3052711</a>
        </li>
        <li id="BibPLXBIB0015" label="[15]">Kevin&nbsp;J. Lang, Benjamin Moseley, and Sergei Vassilvitskii. 2012. Handling Forecast Errors While Bidding for Display Advertising. In <em><em>Proceedings of the 21st International Conference on World Wide Web</em></em> (WWW ’12). ACM, New York, NY, USA, 371–380. <a class="link-inline force-break" href="https://doi.org/10.1145/2187836.2187887" target="_blank">https://doi.org/10.1145/2187836.2187887</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Hoai&nbsp;An Le&nbsp;Thi, Xuan&nbsp;Thanh Vo, and Tao Pham&nbsp;Dinh. 2013. <em><em>Robust Feature Selection for SVMs under Uncertain Data</em></em> . Springer Berlin Heidelberg, Berlin, Heidelberg, 151–165. <a class="link-inline force-break" href="https://doi.org/10.1007/978-3-642-39736-3_12" target="_blank">https://doi.org/10.1007/978-3-642-39736-3_12</a>
        </li>
        <li id="BibPLXBIB0017" label="[17]">Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed Training Strategies for the Structured Perceptron. In <em><em>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</em></em> (HLT ’10). Association for Computational Linguistics, Stroudsburg, PA, USA, 456–464. <a href="http://dl.acm.org/citation.cfm?id=1857999.1858068" target="_blank">http://dl.acm.org/citation.cfm?id=1857999.1858068</a>
        </li>
        <li id="BibPLXBIB0018" label="[18]">Mahdi&nbsp;Pakdaman Naeini and Gregory&nbsp;F. Cooper. 2016. Binary Classifier Calibration Using an Ensemble of Near Isotonic Regression Models. <em><em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em></em> (2016), 360–369.</li>
        <li id="BibPLXBIB0019" label="[19]">Z. Pan, E. Chen, Q. Liu, T. Xu, H. Ma, and H. Lin. 2016. Sparse Factorization Machines for Click-through Rate Prediction. In <em><em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em></em> . 400–409. <a class="link-inline force-break" href="https://doi.org/10.1109/ICDM.2016.0051" target="_blank">https://doi.org/10.1109/ICDM.2016.0051</a>
        </li>
        <li id="BibPLXBIB0020" label="[20]">A.&nbsp;D. Pozzolo, O. Caelen, R.&nbsp;A. Johnson, and G. Bontempi. 2015. Calibrating Probability with Undersampling for Unbalanced Classification. In <em><em>2015 IEEE Symposium Series on Computational Intelligence</em></em> . 159–166. <a class="link-inline force-break" href="https://doi.org/10.1109/SSCI.2015.33" target="_blank">https://doi.org/10.1109/SSCI.2015.33</a>
        </li>
        <li id="BibPLXBIB0021" label="[21]">Y. Qu, H. Cai, K. Ren, W. Zhang, Y. Yu, Y. Wen, and J. Wang. 2016. Product-Based Neural Networks for User Response Prediction. In <em><em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em></em> . 1149–1154. <a class="link-inline force-break" href="https://doi.org/10.1109/ICDM.2016.0151" target="_blank">https://doi.org/10.1109/ICDM.2016.0151</a>
        </li>
        <li id="BibPLXBIB0022" label="[22]">Steffen Rendle. 2010. Factorization Machines. In <em><em>Proceedings of the 2010 IEEE International Conference on Data Mining</em></em> (ICDM ’10). IEEE Computer Society, Washington, DC, USA, 995–1000. <a class="link-inline force-break" href="https://doi.org/10.1109/ICDM.2010.127" target="_blank">https://doi.org/10.1109/ICDM.2010.127</a>
        </li>
        <li id="BibPLXBIB0023" label="[23]">Christopher Riederer, Yunsung Kim, Augustin Chaintreau, Nitish Korula, and Silvio Lattanzi. 2016. Linking Users Across Domains with Location Data: Theory and Validation. In <em><em>Proceedings of the 25th International Conference on World Wide Web</em></em> (WWW ’16). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 707–719. <a class="link-inline force-break" href="https://doi.org/10.1145/2872427.2883002" target="_blank">https://doi.org/10.1145/2872427.2883002</a>
        </li>
        <li id="BibPLXBIB0024" label="[24]">V. Tuzlukov. 2002. <em><em>Signal Processing Noise</em></em> . CRC Press. <a href="https://books.google.co.in/books?id=x6hoBG_MAYIC" target="_blank">https://books.google.co.in/books?id=x6hoBG_MAYIC</a>
        </li>
        <li id="BibPLXBIB0025" label="[25]">Jun Wang, Weinan Zhang, and Shuai Yuan. 2016. Display Advertising with Real-Time Bidding (RTB) and Behavioural Targeting. <em><em>CoRR</em></em> abs/1610.03013(2016). <a href="../../../data/deliveryimages.acm.org/10.1145/3190000/3186148/arxiv:1610.03013" target="_blank">arxiv:1610.03013</a> <a href="http://arxiv.org/abs/1610.03013" target="_blank">http://arxiv.org/abs/1610.03013</a>
        </li>
        <li id="BibPLXBIB0026" label="[26]">Matei Zaharia, Reynold&nbsp;S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael&nbsp;J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016. Apache Spark: A Unified Engine for Big Data Processing. <em><em>Commun. ACM</em></em> 59, 11 (Oct. 2016), 56–65. <a class="link-inline force-break" href="https://doi.org/10.1145/2934664" target="_blank">https://doi.org/10.1145/2934664</a>
        </li>
        <li id="BibPLXBIB0027" label="[27]">Qian Zhao, Yue Shi, and Liangjie Hong. 2017. GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees. In <em><em>Proceedings of the 26th International Conference on World Wide Web</em></em> (WWW ’17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 1311–1319. <a class="link-inline force-break" href="https://doi.org/10.1145/3038912.3052668" target="_blank">https://doi.org/10.1145/3038912.3052668</a>
        </li>
        <li id="BibPLXBIB0028" label="[28]">Martin Zinkevich, Markus Weimer, Lihong Li, and Alex&nbsp;J. Smola. 2010. Parallelized Stochastic Gradient Descent. In <em><em>Advances in Neural Information Processing Systems 23</em></em> , J.&nbsp;D. Lafferty, C.&nbsp;K.&nbsp;I. Williams, J.&nbsp;Shawe-Taylor, R.&nbsp;S. Zemel, and A.&nbsp;Culotta (Eds.). Curran Associates, Inc., 2595–2603. <a href="http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent.pdf" target="_blank">http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent.pdf</a>
        </li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Both authors contributed equally to the paper</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://www.kaggle.com/c/criteo-display-ad-challenge">https://www.kaggle.com/c/criteo-display-ad-challenge</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="https://www.kaggle.com/c/avazu-ctr-prediction">https://www.kaggle.com/c/avazu-ctr-prediction</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="https://www.dropbox.com/sh/ny6puvtopl98339/AACExLZ0waDL_ibWhfNItJfGa?dl=0">https://www.dropbox.com/sh/ny6puvtopl98339/AACExLZ0waDL_ibWhfNItJfGa?dl=0</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3178876.3186148">https://doi.org/10.1145/3178876.3186148</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
