<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>An Attention Factor Graph Model for Tweet Entity
  Linking</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186012'>https://doi.org/10.1145/3178876.3186012</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186012'>https://w3id.org/oa/10.1145/3178876.3186012</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">An Attention Factor Graph Model
          for Tweet Entity Linking</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Chenwei</span> <span class=
          "surName">Ran</span>, Department of Computer Science and
          Technology, Tsinghua University, <a href=
          "mailto:rcw14@mails.tsinghua.edu.cn">rcw14@mails.tsinghua.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Wei</span> <span class=
          "surName">Shen</span><a class="fn" href="#fn1" id=
          "foot-fn1"><sup>*</sup></a>, College of Computer and
          Control Engineering, Nankai University, <a href=
          "mailto:shenwei@nankai.edu.cn">shenwei@nankai.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Jianyong</span> <span class=
          "surName">Wang</span>, Department of Computer Science and
          Technology, Tsinghua University, Jiangsu, Collaborative
          Innovation Center for Language Ability, Jiangsu Normal
          University, <a href=
          "mailto:jianyong@tsinghua.edu.cn">jianyong@tsinghua.edu.cn</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186012"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186012</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The rapid expansion of Twitter has attracted
        worldwide attention. With more than 500 million tweets
        posted per day, Twitter becomes an invaluable information
        and knowledge source. Many Twitter related tasks have been
        studied, such as event extraction, hashtag recommendation,
        and topic detection. A critical step in understanding and
        mining information from Twitter is to disambiguate entities
        in tweets, i.e., tweet entity linking. It is a challenging
        task because tweets are short, noisy, and fresh. Many
        tweet-specific signals have been found to solve the tweet
        entity linking problem, such as user interest, temporal
        popularity, location information and so on. However, two
        common weaknesses exist in previous work. First, most
        proposed models are not flexible and extendable to fit new
        signals. Second, their scalability is not good enough to
        handle the large-scale social network like Twitter. In this
        work, we formalize the tweet entity linking problem into a
        factor graph model which has shown its effectiveness and
        efficiency in many other applications. We also propose
        selective attention over entities to increase the
        scalability of our model, which brings linear complexity.
        To adopt the attention mechanism in the factor graph, we
        propose a new type of nodes called pseudo-variable nodes to
        solve the asymmetry attention problem caused by the
        undirected characteristic of the factor graph. We evaluated
        our model on two different manually annotated tweet
        datasets. The experimental results show that our model
        achieves better performance in terms of both effectiveness
        and efficiency compared with the state-of-the-art
        approaches.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Information extraction;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Twitter</small>,</span>
          <span class="keyword"><small>knowledge
          graph</small>,</span> <span class="keyword"><small>entity
          linking</small>,</span> <span class=
          "keyword"><small>factor graph model</small>,</span>
          <span class="keyword"><small>attention-based
          model</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Chenwei Ran, Wei Shen, and Jianyong Wang. 2018. An
          Attention Factor Graph Model for Tweet Entity Linking. In
          <em>WWW 2018: The 2018 Web Conference,</em> <em>April
          23–27, 2018, Lyon, France.</em> ACM, New York, NY, USA,
          10 pages. <a href=
          "https://doi.org/10.1145/3178876.3186012" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186012</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>The rapid expansion of Twitter<a class="fn" href="#fn2"
      id="foot-fn2"><sup>1</sup></a>, an online social networking
      and microblogging service, has attracted worldwide attention.
      While tweets (messages posted in Twitter) are restricted to
      140 characters, there are more than 330 million monthly
      active users and 500 million tweets per day about topics
      ranging from daily life to breaking news in Twitter.
      Researchers also take note of this invaluable information and
      knowledge source. Many Twitter related tasks have been
      studied, such as event extraction [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0014">14</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>], hashtag recommendation
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0013">13</a>], and topic
      detection [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>].</p>
      <p>A critical step in understanding and mining information
      from Twitter is disambiguating entities in tweets, i.e.,
      linking mentions in tweets to their corresponding entities in
      the knowledge graph (e.g., Wikipedia). It is a challenging
      task. Characteristics of tweets can be summarized to three
      points [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>]:
      1. <strong>short</strong>, each tweet is restricted to 140
      characters; 2. <strong>noisy</strong>, informal acronyms,
      spoken language writing style and typos are common in tweets;
      3.<strong>fresh</strong>, the newly emerging entity-related
      information in tweets may have not been included in the
      knowledge graph. All these characteristics make it hard to
      compute the similarity between text around the mention and
      the document describing the entity, which is an important
      feature when linking entities in traditional document entity
      linking. It also means there are fewer entities in the same
      tweet (as shown in [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>], each tweet averagely contains only
      0.76 entities), while topical coherence between different
      entities within the same tweet is another important feature
      in previous work.</p>
      <p>Although it seems hard to leverage these traditional
      features in tweet entity linking, various novel
      tweet-specific signals have been found. For example, user
      interest is shown to be a powerful signal in tweet entity
      linking. Shen et al. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>] show that users in Twitter have
      their own interests. Thus, tweets from the same user have
      topical coherence which is helpful for tweet entity linking.
      Hua et al. [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0011">11</a>]
      consider the scenario that some information seeking users
      (rather than content generators) post tweets rarely, and
      claim that users having followee-follower relationships also
      have similar interests, i.e., their tweets are topical
      coherent. Temporal signal is also helpful to achieve a better
      performance due to the fresh characteristic of tweets. For
      instance, Fang and Chang [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>] and Hua et al. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>] both use the temporal
      popularity of entities in a specific time period instead of
      the general prior popularity. In addition, location
      information [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>],
      hashtags [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0020">20</a>]
      and extra posts [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>] are also shown to be helpful signals
      for the tweet entity linking task.</p>
      <p>However, two common weaknesses exist in these previous
      work. First, most of these work propose particular models,
      which are well-designed to leverage some signals. Therefore,
      these proposed models are not flexible and extendable to fit
      new signals. That is to say, it is hard to combine various
      effective signals mentioned above into a single model.
      Second, most of these proposed models are not scalable. As
      there are hundred millions of tweets posted every day,
      scalability is very essential for a tweet entity linking
      system. When topical coherence between different entities are
      considered, model learning and inference usually become
      NP-hard problem. Some work propose approximate algorithms to
      find the best configuration [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>], but they are easy to fall into a
      poor local optimum.</p>
      <p>In order to overcome these weaknesses, we leverage a
      factor graph model to solve the tweet entity linking problem
      in this paper. The factor graph model has been successfully
      applied in many applications, such as knowledge base
      alignment [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0030">30</a>],
      social relationship mining [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0029">29</a>], and social influence analysis
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>]. A factor
      graph model defines a factorization probabilistic
      distribution. It is an undirected graph comprising variable
      and factor nodes. Each factor node represents a function over
      the variables it connects to. As the factor function can be
      defined over arbitrary sets of variables, we could use almost
      every signals mentioned above as features. The factorization
      form of the probabilistic distribution makes the learning and
      inference in a factor graph model effective and efficient.
      The most difficult part of the learning and inference in a
      factor graph model is to calculate marginal probabilities. It
      is intractable because the graphical structure can be
      arbitrary and may contain cycles. Loopy belief propagation
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>] is a
      commonly used approximate algorithm to solve this problem.
      Empirically it can get a great approximation and has
      polynomial complexity in proportion to the number and average
      degree of variable nodes.</p>
      <p>We emphasize the importance of scalability for a tweet
      entity linking system. The polynomial complexity when
      learning and inferring in a factor graph model is efficient
      for many applications, but it is not good enough for a tweet
      entity linking system. Here we explain the reason under the
      scenario only considering one user. User interest is one of
      the most important signals when disambiguating entities from
      one user, which means we should measure the topical coherence
      between every two entities from the same user. When we apply
      the factor graph model in the tweet entity linking problem,
      each mention has a corresponding entity variable node in the
      factor graph. If we consider the topical coherence between
      every two entities, there are factor nodes connecting every
      two entity variable nodes. It makes the average degree of
      variable nodes equal to the number of variable nodes. In
      other words, the complexity when learning and inferring will
      be in proportion to the square of the average number of
      mentions extracted from one user, which is unacceptable for a
      large-scale social network like Twitter.</p>
      <p>To achieve better scalability, we adopt the attention
      mechanism in the factor graph model. We argue that it is
      unnecessary to measure the topical coherence between every
      two entities from the same user. When we disambiguate an
      entity, we only need to consider the topical coherence
      between it and the entities it should pay attention to. A
      remaining problem is how to handle the asymmetry attention in
      the factor graph model, and we will solve it using the
      pseudo-variable nodes.</p>
      <p><strong>Contributions.</strong> The main contributions of
      this paper are summarized as follows.</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We leverage a factor graph model
        to deal with the task of tweet entity linking. It is
        effective, efficient, and flexible enough to combine many
        different signals together.<br /></li>
        <li id="list2" label="•">To the best of our knowledge, it
        is the first time that the attention mechanism is adopted
        in the factor graph model. With the proposed selective
        attention over entities, our attention factor graph model
        can reach linear complexity. We also propose a new type of
        nodes called pseudo-variable nodes to solve the asymmetry
        attention problem.<br /></li>
        <li id="list3" label="•">To verify the effectiveness and
        efficiency of our model, we evaluated it over two different
        tweet datasets. The experimental results show that the
        proposed model clearly achieves better performance than
        other state-of-the-art competitors.<br /></li>
      </ul>
      <p>The rest of this paper is organized as follows. In the
      next section, we introduce related work. In Section <a class=
      "sec" href="#sec-9">3</a>, we give the formal definition of
      the tweet entity linking problem and introduce notations used
      in this paper. In Section <a class="sec" href=
      "#sec-10">4</a>, we present a general factor graph model for
      tweet entity linking and its learning and inference
      procedures. Then in Section <a class="sec" href=
      "#sec-15">5</a>, we explain how to add the attention
      mechanism into the factor graph. We evaluate our model in
      Section <a class="sec" href="#sec-19">6</a>. Finally in
      Section <a class="sec" href="#sec-26">7</a> we give a
      conclusion of this work.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Tweet
            Entity Linking</h3>
          </div>
        </header>
        <p>In the early time, a large amount of research on entity
        linking [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0001">1</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0010">10</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0025">25</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0026">26</a>] have
        been conducted over news documents. However, these
        approaches do not work well when they are applied to the
        tweet data as tweets are short, noisy, and fresh.
        Researchers turn to find new signals from rich meta-data of
        tweets. Among various tweet-specific signals, user interest
        may be the most powerful one. Shen et al. [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0027">27</a>] found that the entities
        from the same user are topical coherent, as users have
        their own underlying topic distribution based on their
        interest. Hua et al. [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>] further leveraged the
        followee-follower relationship between users to avoid the
        difficulty and inaccuracy of modeling user interest from
        limited amount of tweets. Time stamp is another useful
        signal for tweet entity linking. Fang and Chang [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0011">11</a>] and Hua
        et al. [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0005">5</a>]
        both found that the temporal popularity of entities is a
        better feature than the general prior popularity because it
        can capture the recency characteristic of tweets. They
        computed the temporal popularity with the occurrence number
        of entities from the historical tweet data. As the entity
        mentioned in tweets is unknown due to the ambiguity of
        mentions, they adopted a variant of the EM algorithm.
        Another challenge in the calculation of the temporal signal
        is the sparsity of data. They smoothed the temporal
        popularity over time binning and entities respectively.
        Besides, there are many other useful signals such as
        locations [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0005">5</a>], attached hashtags [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>] and extra posts
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0009">9</a>]. Some
        studies [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0005">5</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0008">8</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0031">31</a>] also
        found that merging the tasks of mention detection and
        entity disambiguation into a joint task can achieve
        improvements. Two common weaknesses exist in these work.
        First, most of these models are not flexible and extendable
        for new features. Second, most of them cannot scale up well
        to a large social network such as Twitter.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Factor
            Graph Model</h3>
          </div>
        </header>
        <p>The factor graph model has been successfully applied in
        many applications for large-scale data, such as knowledge
        base alignment [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>], social relationship mining
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0029">29</a>] and
        social influence analysis [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0028">28</a>]. Tang et al. [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0029">29</a>] studied the social
        relationship classification problem on a publication
        dataset. Wang et al. [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>] studied the cross-lingual
        knowledge linking problem on a dataset constructed from
        Wikipedia. The datasets they used are both large-scale. In
        this paper, we apply the factor graph model to the tweet
        entity linking problem.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span>
            Attention-based Model</h3>
          </div>
        </header>
        <p>The idea of selective attention is inspired by Lin et
        al. [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>]. They studied the relation
        extraction task using a sentence-level convolutional neural
        network. They proposed the selective attention over
        instances to overcome the wrong labeling problem in distant
        supervision. Besides, the attention-based model adopted in
        deep neural network has been applied to various areas such
        as speech recognition [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>] and image classification [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>]. To the
        best of our knowledge, this is the first time that the
        attention mechanism is adopted in the factor graph
        model.</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Preliminaries
          and notations</h2>
        </div>
      </header>
      <p>In this section, we first briefly introduce some basic
      concepts and define the task of tweet entity linking. Then we
      present notations employed in the remaining of the paper.</p>
      <p><strong>Tweet.</strong> <em>Tweets are the data source of
      the tweet entity linking task. The collection of tweets is
      denoted by <em>T</em> and the collection of users posting
      tweets is denoted by <em>U</em></em> .</p>
      <p>The main characteristic of tweets is the restriction of
      140 characters which is different from general Web documents.
      Nonetheless, tweets have rich meta-data including user
      information, retweet (re-posting of a tweet) relationships,
      posting times and locations, attached hashtags, images, and
      URLs. We use the content of tweets and the corresponding user
      information in this work, while other signals could be easily
      added to our model which will be discussed in details in the
      next section. For the tweet collection <em>T</em>, we use the
      lowercase of its denotation to represent an element in it
      (i.e., <em>t</em> ∈ <em>T</em>) and the subscript to index
      the element in it (i.e., <em>t<sub>i</sub></em> means the
      <em>i</em>-th tweet in <em>T</em>). We also represent the
      element of other collections in this paper following the same
      way.</p>
      <p><strong>Mention.</strong> <em>A mention is a textual
      phrase referring to some entity extracted from the content of
      tweets. The collection of mentions is denoted by
      <em>M</em>.</em></p>
      <p><strong>Entity.</strong> <em>An entity is a unique
      real-world object. The collection of entities is denoted by
      <em>E</em>.</em></p>
      <p>Although an entity is unique, it may have many different
      surface forms. For example, the New York City can also be
      called ”Big Apple”. In the meanwhile, the same textual phrase
      may refer to different entities. For instance, ”Jordan” could
      refer to the brand Air Jordan, the basketball player Michael
      Jordan, or the country Jordan. Thus, there is a many-to-many
      correspondence between mentions and entities. However, with
      certain context, a mention has its clear and unique
      semantically corresponding entity. To find the correct target
      entity, tweet entity linking is required.</p>
      <p><strong>Knowledge Graph.</strong> <em>A knowledge graph is
      a machine-readable set of knowledge.</em></p>
      <p>Here knowledge refers to entities, their semantic
      categories and attributes, and the relationships between
      entities. The knowledge graph we adopt in this paper is
      Wikipedia<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>2</sup></a>.</p>
      <p><strong>Tweet Entity Linking.</strong> <em>Given a mention
      <em>m</em> ∈ <em>M</em> extracted from a tweet <em>t</em> ∈
      <em>T</em>, it could refer to different entities. We call
      them candidate entities of mention <em>m</em> and denote them
      as <em>E<sub>m</sub></em> ⊆<em>E</em>. The goal of tweet
      entity linking is to identify the semantically corresponding
      target entity <em>e</em> ∈ <em>E<sub>m</sub></em> for the
      mention <em>m</em> with the context of <em>m</em> and the
      meta-data of tweet <em>t</em>.</em></p>
      <p>A typical entity linking system consists of three modules:
      mention detection, candidate generation, and candidate
      ranking. In this paper we focus on the candidate ranking task
      and take the detected mentions and the candidate entities for
      each mention as input. The task of mention detection (also
      named entity recognition) for tweets has been studied by
      several work [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>]. The candidate entities of a mention
      can be collected from entity pages, redirect pages,
      disambiguation pages, and anchor phrases in Wikipedia
      articles [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0026">26</a>].
      Specially, mention <em>m</em> may be unlinkable, i.e., the
      corresponding entity of mention <em>m</em> does not exist in
      the knowledge graph. Some methods link such a mention to a
      special denotation NIL. In this paper, we assume that all the
      detected mentions are linkable with Wikipedia. The method for
      unlinkable mention detection is left for future research.</p>
      <p><strong>Factor Graph.</strong> <em>A factor graph can be
      represented as <em>G</em> = (<em>V</em>, <em>F</em>,
      <em>D</em>), where <em>V</em> is the set of variable nodes,
      <em>F</em> is the set of factor nodes, and <em>D</em> is the
      set of edges.</em></p>
      <p>In a factor graph, a variable node <em>v<sub>i</sub></em>
      ∈ <em>V</em> represents a random variable, and a factor node
      <em>f<sub>j</sub></em> ∈ <em>F</em> represents a function
      <em>f<sub>j</sub></em> (<em>S<sub>j</sub></em> ) where
      <em>S<sub>j</sub></em> ⊆<em>V</em> is a set of variable
      nodes. There is an undirected edge between a factor node
      <em>f<sub>j</sub></em> and each variable node
      <em>v<sub>k</sub></em> ∈ <em>S<sub>j</sub></em> . The two
      types of nodes in a factor graph form a bipartite and
      undirected graph.</p>
      <p>An illustration of formalizing the tweet entity linking
      problem into a factor graph model is shown in Fig. <a class=
      "fig" href="#fig1">1</a>. Under the scenario of tweet entity
      linking, all variable nodes <em>V</em> in a factor graph
      <em>G</em> is divided into two subsets <em>X</em> and
      <em>Y</em> (i.e., <em>V</em> = <em>X</em>∪<em>Y</em>),
      corresponding to the observed and hidden variables
      respectively. A mention <em>m<sub>i</sub></em> ∈ <em>M</em>
      is mapped to an observed variable <em>x<sub>i</sub></em> and
      a hidden variable <em>y<sub>i</sub></em> . Thus the factor
      graph <em>G</em> has 2 · |<em>M</em>| variable nodes in
      total. <em>x<sub>i</sub></em> represents the mention
      <em>m<sub>i</sub></em> and its context. It has only one state
      as it is observed. <em>y<sub>i</sub></em> represents the
      semantically corresponding entity of <em>m<sub>i</sub></em> .
      It has <span class="inline-equation"><span class="tex">$K_i =
      |E_{m_i}|$</span></span> possible states each of which
      represents a candidate entity that <em>m<sub>i</sub></em> may
      refer to. We also call <em>x</em> as mention variable and
      <em>y</em> as entity variable in this paper. For learning and
      inference, the correct corresponding entity of some mention
      is labeled. Thus <em>Y</em> can be further divided into two
      subsets <em>Y<sup>L</sup></em> and <em>Y<sup>U</sup></em>
      (i.e., <em>Y</em> = <em>Y<sup>L</sup></em>
      ∪<em>Y<sup>U</sup></em> ), corresponding to the labeled and
      unlabeled hidden variables respectively.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186012/images/www2018-21-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The factor graph model for tweet entity
          linking.</span>
        </div>
      </figure>
      <p></p>
      <p>When learning and inferring with the factor graph model,
      an important algorithm is the loopy belief propagation
      algorithm. This algorithm involves passing messages on the
      factor graph. We denote the message from variable node
      <em>v<sub>i</sub></em> to factor node <em>f<sub>j</sub></em>
      as <em>μ</em> <sub><em>i</em> → <em>j</em></sub>
      (<em>v<sub>i</sub></em> ). There are <em>K<sub>i</sub></em>
      possible states that variable <em>v<sub>i</sub></em> can
      take, thus the message results in a vector of length
      <em>K<sub>i</sub></em> . Similarly, the message from factor
      node <em>f<sub>j</sub></em> to variable node
      <em>v<sub>i</sub></em> is denoted as <em>λ</em>
      <sub><em>j</em> → <em>i</em></sub> (<em>v<sub>i</sub></em> )
      which results in a vector of length <em>K<sub>i</sub></em> as
      well.</p>
      <p>We propose a new type of nodes called pseudo-variable
      nodes for the factor graph in this paper. In our model, each
      hidden variable has a corresponding pseudo-variable node. We
      denote the corresponding pseudo-variable node of hidden
      variable <em>y<sub>i</sub></em> as <span class=
      "inline-equation"><span class="tex">$y^\prime
      _i$</span></span> .</p>
      <p>All notations we use in this paper are summarized in Table
      <a class="tbl" href="#tab1">1</a>.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Summary of notations.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">Notation</th>
              <th style="text-align:center;">Definition</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;"><em>T</em></td>
              <td style="text-align:center;">A tweet
              collection</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>U</em></td>
              <td style="text-align:center;">A user collection</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>M</em></td>
              <td style="text-align:center;">A mention collection
              extracted from tweets <em>T</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>M<sub>u</sub></em>
              ⊆<em>M</em></td>
              <td style="text-align:center;">The mentions extracted
              from tweets posted by a user <em>u</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>E</em></td>
              <td style="text-align:center;">The entity collection
              in a knowledge graph</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>E<sub>m</sub></em>
              ⊆<em>E</em></td>
              <td style="text-align:center;">The candidate entities
              of a mention <em>m</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>G</em> =
              (<em>V</em>, <em>F</em>, <em>D</em>)</td>
              <td style="text-align:center;">A factor graph</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>V</em> =
              <em>X</em>∪<em>Y</em></td>
              <td style="text-align:center;">Variable nodes</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>F</em></td>
              <td style="text-align:center;">Factor nodes</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>D</em></td>
              <td style="text-align:center;">Edges</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>S<sub>j</sub></em>
              ⊆<em>V</em></td>
              <td style="text-align:center;">The variable nodes
              connecting to the factor <em>f<sub>j</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>X</em></td>
              <td style="text-align:center;">Observed
              variables</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>Y</em> =
              <em>Y<sup>L</sup></em> ∪<em>Y<sup>U</sup></em></td>
              <td style="text-align:center;">Hidden variables</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <em>K<sub>i</sub></em></td>
              <td style="text-align:center;">The number of
              <em>y<sub>i</sub></em> ’s possible states</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <em>Y<sup>L</sup></em></td>
              <td style="text-align:center;">The hidden variables
              with label</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <em>Y<sup>U</sup></em></td>
              <td style="text-align:center;">The hidden variables
              without label</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>μ</em>
              <sub><em>i</em> → <em>j</em></sub>
              (<em>v<sub>i</sub></em> )</td>
              <td style="text-align:center;">The message from the
              variable <em>v<sub>i</sub></em> to the factor
              <em>f<sub>j</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>λ</em>
              <sub><em>j</em> → <em>i</em></sub>
              (<em>v<sub>i</sub></em> )</td>
              <td style="text-align:center;">The message from the
              factor <em>f<sub>j</sub></em> to the variable
              <em>v<sub>i</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;">Θ</td>
              <td style="text-align:center;">Weighting
              parameters</td>
            </tr>
            <tr>
              <td style="text-align:center;">Φ</td>
              <td style="text-align:center;">Feature functions</td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$y^\prime
              _i$</span></span></td>
              <td style="text-align:center;">The corresponding
              pseudo-variable node of the hidden variable
              <em>y<sub>i</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {R}_i(y_j)$</span></span></td>
              <td style="text-align:center;">potential topical
              coherence between the variable <em>y<sub>i</sub></em>
              and <em>y<sub>j</sub></em></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Factor Graph
          Model</h2>
        </div>
      </header>
      <p>In this section, we begin with the description of the
      model features we use. Then we explain the learning and
      inference algorithm for the factor graph model in
      details.</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Features</h3>
          </div>
        </header>
        <p>The factor functions in a factor graph can be
        instantiated in different ways. In this paper, we use
        exponential-linear functions. Specifically, we define the
        factor function <em>f<sub>j</sub></em> as:</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} f_j(S_j) =
            exp\lbrace {\Theta }^T {\Phi }(S_j)\rbrace
            \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>where Θ = (<em>θ</em> <sub>1</sub>, <em>θ</em>
        <sub>2</sub>, ...) is a weighting vector and Φ is a vector
        of feature functions. As we mentioned before, the observed
        variable <em>x<sub>i</sub></em> corresponds to the mention
        <em>m<sub>i</sub></em> and the hidden variable
        <em>y<sub>i</sub></em> corresponds to the entity that
        <em>m<sub>i</sub></em> semantically refers to. In the
        following we use the terms mention <em>x<sub>i</sub></em>
        and entity <em>y<sub>i</sub></em> for simplicity. In this
        paper, we consider two feature functions:
        <p></p>
        <ol class="list-no-style">
          <li id="list4" label="(1)">Prior popularity. We leverage
          the anchor links in Wikipedia to compute the popularity
          of the entity <em>y<sub>i</sub></em> given the mention
          <em>x<sub>i</sub></em> and define the first feature
          function as:
            <div class="table-responsive" id="eq2">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation} \varphi
                _1(x_i, y_i) = \frac{count(x_i, y_i)}{count(x_i)}
                \end{equation}</span><br />
                <span class="equation-number">(2)</span>
              </div>
            </div>where <em>count</em>(<em>x<sub>i</sub></em> )
            represents the number of mention <em>x<sub>i</sub></em>
            occurring as the surface form of an anchor link in
            Wikipedia and <em>count</em>(<em>x<sub>i</sub></em> ,
            <em>y<sub>i</sub></em> ) represents the number of
            anchor links with the surface form
            <em>x<sub>i</sub></em> pointing to the entity
            <em>y<sub>i</sub></em> .<br />
          </li>
          <li id="list5" label="(2)">Topical coherence based on
          user interest. As shown in previous work, mentions from
          the same user have topical coherence. We adopt the
          Wikipedia Link-based Measure(WLM) described in
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0021">21</a>] to measure the topical
            coherence between entities. Given
            <em>y<sub>i</sub></em> and <em>y<sub>j</sub></em> whose
            corresponding mentions are extracted from the same
            user's tweets, we define the second feature function
            as:
            <div class="table-responsive" id="eq3">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation} \varphi
                _2(y_i, y_j) = 1 - \frac{log(max(|A_{y_i}|,
                |A_{y_j}|) - log (|A_{y_i} \cap A_{y_j}|)}{log(|A|)
                - log(min(|A_{y_i}|, |A_{y_j}|))}
                \end{equation}</span><br />
                <span class="equation-number">(3)</span>
              </div>
            </div>where <em>A</em> is the collection of all
            Wikipedia articles and <em>A<sub>y</sub></em> is the
            collection of articles containing a link to entity
            <em>y</em>.<br />
          </li>
        </ol>
        <p>Although we only consider two feature functions in this
        paper, we claim that our model is flexible and extendable
        for new features. For example, if we want to leverage the
        temporal popularity, we can add a new feature function with
        regard to <em>x<sub>i</sub></em> and <em>y<sub>i</sub></em>
        to capture this signal. If we want to leverage the topical
        coherence based on hashtags, we can add a new feature
        function with regard to <em>y<sub>i</sub></em> and
        <em>y<sub>j</sub></em> whose corresponding mention are
        extracted from tweets having the same attached hashtag.
        More generally, the feature function can be defined over
        arbitrary sets of variables while the learning and
        inference algorithm can still work.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Learning</h3>
          </div>
        </header>
        <p>Given a factor graph <em>G</em>, the joint distribution
        over hidden variables <em>Y</em> is defined as:</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(Y|G) =
            \frac{1}{Z}\prod _{j}f_j(S_j) = \frac{1}{Z}exp\lbrace
            {\Theta }^T\sum _{j} {\Phi }(S_j)\rbrace =
            \frac{1}{Z}exp\lbrace {\Theta }^T {\Psi }\rbrace
            \end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where <em>Z</em> = ∑ <sub><em>Y</em></sub>
        <em>exp</em>{Θ <sup><em>T</em></sup> Ψ} is a special
        normalization factor with zero nodes, Ψ = ∑
        <sub><em>j</em></sub> Φ(<em>S<sub>j</sub></em> ) is the
        aggregation of feature functions over all factor nodes.
        <p></p>
        <p>Learning a factor graph model is to estimate an optimum
        parameter configuration Θ<sup>*</sup>, so that the
        log-likelihood of given labeled data is maximized. The
        log-likelihood objective function is defined as:</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathcal
            {O}({\Theta }) &amp;= log~p(Y^L|G) = log\sum
            _{Y|Y^L}\frac{1}{Z}exp\lbrace {\Theta }^T {\Psi
            }\rbrace \nonumber \\ &amp;=log\sum _{Y|Y^L}exp\lbrace
            {\Theta }^T {\Psi }\rbrace - log\sum _{Y}exp\lbrace
            {\Theta }^T {\Psi }\rbrace \end{align}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>where <em>Y<sup>L</sup></em> denotes the hidden
        variables with known labels and
        <em>Y</em>|<em>Y<sup>L</sup></em> is a labeling
        configuration of <em>Y</em> inferred from
        <em>Y<sup>L</sup></em> . The gradient decent algorithm is a
        commonly used method to maximize the objective function.
        The gradient for parameters Θ is calculated as:
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \frac{\partial
            \mathcal {O}({\Theta })}{\partial {\Theta }} &amp;=
            \frac{\partial log\sum _{Y|Y^L}exp\lbrace {\Theta }^T
            {\Psi }\rbrace - log\sum _{Y}exp\lbrace {\Theta }^T
            {\Psi }\rbrace }{\partial {\Theta }} \nonumber \\
            &amp;= \frac{\sum _{Y|Y^L} exp\lbrace {\Theta }^T {\Psi
            }\rbrace \cdot {\Psi }}{\sum _{Y|Y^L} exp\lbrace
            {\Theta }^T {\Psi }\rbrace } - \frac{\sum _{Y}
            exp\lbrace {\Theta }^T {\Psi }\rbrace \cdot {\Psi
            }}{\sum _{Y} exp\lbrace {\Theta }^T {\Psi }\rbrace }
            \nonumber \\ &amp;=\mathbb {E}_{p_{ {\Theta
            }}(Y|Y^L,G)} {\Psi } - \mathbb {E}_{p_{ {\Theta }}(Y)}
            {\Psi } \end{align}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\mathbb {E}_{p_{ {\Theta }}(Y|Y^L,G)} {\Psi
        }$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathbb {E}_{p_{
        {\Theta }}(Y)} {\Psi }$</span></span> are two expectations
        of Ψ with regard to the probabilistic distribution
        <em>p</em> <sub>Θ</sub>(<em>Y</em>|<em>Y<sup>L</sup></em> ,
        <em>G</em>) and <em>p</em> <sub>Θ</sub>(<em>Y</em>)
        respectively. To obtain the expectation of Ψ, we need to
        calculate the marginal probabilities
        <em>p</em>(<em>y<sub>i</sub></em> ) and
        <em>p</em>(<em>y<sub>i</sub></em> , <em>y<sub>j</sub></em>
        ) and then sum over all hidden variables. Exact marginal
        probabilities of a factor graph is intractable to be
        calculated as the graphical structure of the factor graph
        can be arbitrary and may contain cycles. Loopy belief
        propagation (LBP for short) [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0023">23</a>] is a commonly used algorithm to
        approximate marginal probabilities for a graphical model.
        We briefly introduce the LBP algorithm here.
        <p></p>
        <p>The main idea of the LBP algorithm is passing messages
        on the factor graph. There are two types of messages:</p>
        <ol class="list-no-style">
          <li id="list6" label="(1)">A message from the variable
          node <em>v<sub>i</sub></em> to the factor node
          <em>f<sub>j</sub></em> , denoted as:
            <div class="table-responsive" id="eq7">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation} \mu _{i
                \rightarrow j}(v_i) = \prod _{f_k:v_i \in S_k, f_k
                \not= f_j }\lambda _{k \rightarrow i}(v_i)
                \end{equation}</span><br />
                <span class="equation-number">(7)</span>
              </div>
            </div>which means the message from
            <em>v<sub>i</sub></em> to <em>f<sub>j</sub></em> is the
            product of the messages from neighboring factor nodes
            of <em>v<sub>i</sub></em> except <em>f<sub>j</sub></em>
            . Specially, if <em>f<sub>j</sub></em> is the only
            neighboring factor node of <em>v<sub>i</sub></em> , the
            message is set to the uniform distribution.<br />
          </li>
          <li id="list7" label="(2)">A message from the factor node
          <em>f<sub>j</sub></em> to the variable node
          <em>v<sub>i</sub></em> , denoted as:
            <div class="table-responsive" id="eq8">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation} \lambda
                _{j \rightarrow i}(v_i) = \sum
                _{S_j:v_i=x_i}f_j(S_j) \prod _{v_k \in S_j
                \backslash \lbrace v_i\rbrace }\mu _{k \rightarrow
                j}(v_k) \end{equation}</span><br />
                <span class="equation-number">(8)</span>
              </div>
            </div>which means the message from
            <em>f<sub>j</sub></em> to <em>v<sub>i</sub></em> is the
            product of the factor <em>f<sub>j</sub></em> with the
            messages from all other connecting variable nodes,
            marginalized over all variable nodes except
            <em>v<sub>i</sub></em> (i.e., <em>S<sub>j</sub></em>
            \{<em>v<sub>i</sub></em> }). Specially, if
            <em>S<sub>j</sub></em> = {<em>v<sub>i</sub></em> }, the
            message <em>λ</em> <sub><em>j</em> → <em>i</em></sub>
            (<em>v<sub>i</sub></em> ) = <em>f<sub>j</sub></em>
            (<em>v<sub>i</sub></em> ).<br />
          </li>
        </ol>
        <p>If the variable <em>v<sub>i</sub></em> has a label
        <em>v<sub>k</sub></em> , the message it sends to any factor
        nodes is 1 if <em>v<sub>i</sub></em> =
        <em>v<sub>k</sub></em> and 0 otherwise. As the messages are
        defined recursively, we initialize all messages to the
        uniform distribution and then update the messages according
        to Eq. <a class="eqn" href="#eq7">7</a> and Eq. <a class=
        "eqn" href="#eq8">8</a> iteratively. With the final
        messages, we can compute the marginal probability of the
        variable <em>v<sub>i</sub></em> as:</p>
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(v_i) \propto
            \prod _{f_k:v_i \in S_k}\lambda _{k \rightarrow i}(v_i)
            \end{equation}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <p>We can also compute the marginal probability of the
        variable set <em>S<sub>j</sub></em> involved in the factor
        <em>f<sub>j</sub></em> as:</p>
        <div class="table-responsive" id="eq10">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(S_j) \propto
            f_j(S_j)\prod _{v_i \in S_j}\mu _{i \rightarrow i}(v_i)
            \end{equation}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>
        <p></p>
        <p>Then we use a two-step variation of the LBP algorithm to
        obtain the gradient, one step for calculating expectation
        <span class="inline-equation"><span class="tex">$\mathbb
        {E}_{p_{ {\Theta }}(Y|Y^L,G)} {\Psi }$</span></span> and
        the other step for calculating expectation <span class=
        "inline-equation"><span class="tex">$\mathbb {E}_{p_{
        {\Theta }}(Y)} {\Psi }$</span></span> . The learning
        algorithm also can be extended to a distributed learning
        version with a graph segmentation algorithm such as
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0012">12</a>]. Readers
        could refer to [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0029">29</a>] for details of the learning
        algorithm.</p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span>
            Inference</h3>
          </div>
        </header>
        <p>After we learned the optimal parameters Θ<sup>*</sup>,
        we can infer the best label of each hidden variable (i.e.,
        the entity that mention semantically refers to). The best
        label could be the state with the highest marginal
        probability according to Eq. <a class="eqn" href=
        "#eq9">9</a>. Alternatively, a better way is to find the
        best label as a whole, i.e., find a label configuration
        which maximizes the joint probability defined in Eq.
        <a class="eqn" href="#eq4">4</a>:</p>
        <div class="table-responsive" id="eq11">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} Y^* =
            {argmax}_{Y|Y^L}p(Y|G, {\Theta }^*)
            \end{equation}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>
        <p></p>
        <p>We can also use the LBP algorithm to solve this problem
        just with a little modification. The idea is simple:
        replace the ∑ operator with the <em>max</em> operator in
        the definition of messages. The modified algorithm is also
        named the max-sum algorithm.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Complexity
            Analysis</h3>
          </div>
        </header>
        <p>We analyze the time complexity of the learning and
        inference algorithm, more specifically, the LBP algorithm.
        The time cost of the LBP algorithm depends on the message
        passing process. In our model, the hidden variable
        corresponding to the mention from user <em>u</em> connects
        to |<em>M<sub>u</sub></em> | factors. Thus in one
        iteration, the total number of message passing is</p>
        <div class="table-responsive" id="eq12">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \displaystyle
            \sum _{u}|M_{u}|^2 \ge |U|(\frac{\sum
            _{u}M_{u}}{|U|})^2 = |U||\overline{M_u}|^2
            \end{equation}</span><br />
            <span class="equation-number">(12)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$|\overline{M_u}|$</span></span> represents the
        average number of mentions extracted from the tweets posted
        by each user. Thus, time complexity of the learning and
        inference algorithm has a lower bound <span class=
        "inline-equation"><span class="tex">$\Omega
        (|U||\overline{M_u}|^2)$</span></span> .
        <p></p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Selective
          Attention Over Entities</h2>
        </div>
      </header>
      <p>In the previous section, we have discussed the intuitive
      idea of how to leverage the topical coherence based on user
      interest in the factor graph model, and the time complexity
      of the learning and inference algorithm has a lower bound
      <span class="inline-equation"><span class="tex">$\Omega
      (|U||\overline{M_u}|^2)$</span></span> . In this section, we
      argue that it is unnecessary to measure the topical coherence
      between every two entity variables corresponding to the same
      user. Here we consider the scenario that we disambiguate a
      mention <em>m</em> while the labeled entity of other mentions
      from the same user is already known. Obviously if a labeled
      entity is not topically coherent with any candidate entity of
      <em>m</em>, we do not need to consider this entity. In the
      factor graph model, that is to say, we do not need to connect
      these two entity variables with a factor node. Thus, we
      introduce the selective attention over entities for the
      factor graph model.</p>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Attention
            Measure</h3>
          </div>
        </header>
        <p>When introducing the selective attention over entities
        for the factor graph model, the first challenge is that
        most entity variables in the factor graph model are
        unlabeled which make it hard to decide whether two entity
        variables are topically coherent. Therefore, we turn to
        measure the potential topical coherence between two entity
        variables. Given the entity variable <em>y<sub>i</sub></em>
        , we propose three ways to measure its potential topical
        coherence to another entity variable <em>y<sub>j</sub></em>
        :</p>
        <p><strong>Sum:</strong> The sum value of topical coherence
        between the entity variables <em>y<sub>i</sub></em> and
        <em>y<sub>j</sub></em> :</p>
        <div class="table-responsive" id="eq13">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {R}_{i}^{1}(y_j) = \sum _{y_i, y_j}\varphi _2(y_i, y_j)
            \end{equation}</span><br />
            <span class="equation-number">(13)</span>
          </div>
        </div>
        <p></p>
        <p><strong>Average:</strong> The average value of topical
        coherence between the entity variables
        <em>y<sub>i</sub></em> and <em>y<sub>j</sub></em> :</p>
        <div class="table-responsive" id="eq14">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {R}_{i}^{2}(y_j) = \frac{\sum _{y_i, y_j}\varphi
            _2(y_i, y_j)}{K_i \cdot K_j}
            \end{equation}</span><br />
            <span class="equation-number">(14)</span>
          </div>
        </div>
        <p></p>
        <p><strong>Max:</strong> The maximal value of topical
        coherence between the entity variables
        <em>y<sub>i</sub></em> and <em>y<sub>j</sub></em> :</p>
        <div class="table-responsive" id="eq15">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {R}_{i}^{3}(y_j) = Max_{y_j}\varphi _2(y_i, y_j)
            \end{equation}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>
        <p></p>
        <p>Then we choose the top-<span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> entity variables with respect to Eq.
        <a class="eqn" href="#eq13">13</a>, Eq. <a class="eqn"
        href="#eq14">14</a> or Eq. <a class="eqn" href=
        "#eq15">15</a> as the entity variables that
        <em>y<sub>i</sub></em> should pay attention to.</p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span>
            Pseudo-variable Node</h3>
          </div>
        </header>
        <p>The second challenge is the asymmetry attention problem,
        which means the situation that the variable
        <em>y<sub>i</sub></em> should pay attention to the variable
        <em>y<sub>j</sub></em> while <em>y<sub>j</sub></em> should
        not pay attention to <em>y<sub>i</sub></em> . Obviously, we
        can add a factor connecting to <em>y<sub>i</sub></em> and
        <em>y<sub>j</sub></em> if they should pay attention to each
        other. We can remove the factor connecting to
        <em>y<sub>i</sub></em> and <em>y<sub>j</sub></em> if they
        should not pay attention to each other. However, we cannot
        handle the asymmetry attention problem in the factor graph
        model directly because there is no directed edge in a
        factor graph.</p>
        <p>We have a further discussion about what ”attention”
        exactly means in the factor graph model here. When we say
        <em>y<sub>i</sub></em> should pay attention to
        <em>y<sub>j</sub></em> , we are expecting there is a factor
        whose neighbors are <em>y<sub>i</sub></em> and
        <em>y<sub>j</sub></em> , and its value will change when the
        state of <em>y<sub>j</sub></em> changes. When we say
        <em>y<sub>i</sub></em> should not pay attention to
        <em>y<sub>j</sub></em> , we are expecting there is no such
        a factor. Now consider the situation of asymmetry
        attention. It is required that there is a factor whose
        value will change when the state of <em>y<sub>j</sub></em>
        changes and there is no such a factor whose value will
        change when the state of <em>y<sub>i</sub></em> changes at
        the same time.</p>
        <p>Thus, we propose a new type of nodes called
        pseudo-variable nodes. In the attention factor graph model,
        each hidden variable <em>y</em> has a corresponding
        pseudo-variable node <em>y</em>′. The pseudo-variable node
        <em>y</em>′ is an observed node, whose state is set as:</p>
        <div class="table-responsive" id="eq16">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} y^\prime =
            argmax_{y}P(y|Y^L, G) \end{equation}</span><br />
            <span class="equation-number">(16)</span>
          </div>
        </div>
        <p></p>
        <p>Now we explain how to use the pseudo-variable node to
        solve the asymmetry attention problem with the example
        shown in Fig. <a class="fig" href="#fig2">2</a>. In this
        example, we assume that <em>y</em> <sub>4</sub> should pay
        attention to <em>y</em> <sub>3</sub> while <em>y</em>
        <sub>3</sub> should not pay attention to <em>y</em>
        <sub>4</sub>. Thus, we add a factor connecting <em>y</em>
        <sub>4</sub> with <span class=
        "inline-equation"><span class="tex">$y_3^\prime$</span></span>
        instead of <em>y</em> <sub>3</sub>. When the state of
        <em>y</em> <sub>4</sub> changes, the value of this factor
        will change. When the state of <em>y</em> <sub>3</sub>
        changes, the value of this factor will not change as the
        state of <span class="inline-equation"><span class=
        "tex">$y_3^\prime$</span></span> does not change.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186012/images/www2018-21-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">An example for the
            pseudo-variable node.</span>
          </div>
        </figure>
        <p></p>
        <p>A remaining problem is how to let the state of
        <em>y</em>′ and the marginal probability
        <em>P</em>(<em>y</em>|<em>Y<sup>L</sup></em> , <em>G</em>)
        affect each other. We propose a variation of the LBP
        algorithm to solve this problem. We first fix the states of
        the pseudo-variable nodes and approximate the marginal
        probabilities, and then we update the states of the
        pseudo-variable nodes iteratively. The proposed algorithm
        is depicted in Algo. 1.</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Complexity
            Analysis</h3>
          </div>
        </header>
        <p>We analyze the time complexity of our attention factor
        graph model. As each variable should pay attention to at
        most <span class="inline-equation"><span class=
        "tex">$\mathcal {K}$</span></span> other variables, there
        are at most <span class="inline-equation"><span class=
        "tex">$\mathcal {K}$</span></span> factors that each
        variable connects to. Thus, time complexity of our
        attention factor graph model has an upper bound
        <span class="inline-equation"><span class="tex">$\mathcal
        {O}(\mathcal {K}|U||\overline{M_u}|)$</span></span> . When
        <span class="inline-equation"><span class="tex">$\mathcal
        {K} \le |\overline{M_u}|$</span></span> , the time cost of
        our model is technically less than the general factor graph
        model. As <span class="inline-equation"><span class=
        "tex">$\mathcal {K}$</span></span> can be considered as a
        constant value, the upper bound of the time complexity can
        also be represented as <span class=
        "inline-equation"><span class="tex">$\mathcal {O}(\mathcal
        {K}|U||\overline{M_u}|) = \mathcal {O}(|M|)$</span></span>
        which is a linear complexity with respect to the number of
        mentions in <em>M</em>.</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186012/images/www2018-21-img1.svg"
        class="img-responsive" alt="" longdesc="" /></p>
      </section>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span>
          Experiments</h2>
        </div>
      </header>
      <p>To verify the effectiveness and efficiency of our
      attention factor graph model, we present a thorough
      experimental study in this section. We firstly describe the
      experimental setting in Section <a class="sec" href=
      "#sec-20">6.1</a>. Then we evaluate the effectiveness of our
      proposed model in Section <a class="sec" href=
      "#sec-24">6.2</a> and the efficiency and scalability in
      Section <a class="sec" href="#sec-25">6.3</a> on two
      different tweet datasets.</p>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span>
            Experimental Setting</h3>
          </div>
        </header>
        <section id="sec-21">
          <header>
            <div class="title-info">
              <h4><span class="section-number">6.1.1</span>
              Wikipedia Dataset</h4>
            </div>
          </header>
          <p>In our experiments, we use Wikipedia as our knowledge
          graph and regard the set of articles in Wikipedia as the
          entity collection. We downloaded the August 2017 version
          of English Wikipedia dump, which contains 5.5 million
          article pages, 7.9 million redirect pages and more than
          78.3 million anchor links according to the Wikimedia
          Statistics project<a class="fn" href="#fn4" id=
          "foot-fn4"><sup>3</sup></a>. We also leverage the open
          source toolkit WikipediaMiner<a class="fn" href="#fn5"
          id="foot-fn5"><sup>4</sup></a> to generate candidate
          entities for mentions and calculate features including
          prior popularity (Eq. <a class="eqn" href="#eq2">2</a>)
          and topical coherence (Eq. <a class="eqn" href=
          "#eq3">3</a>).</p>
        </section>
        <section id="sec-22">
          <div class="title-info">
            <h4><span class="section-number">6.1.2</span> Tweet
            Dataset</h4>
          </div>
          <p>The first tweet dataset we use is shared by Shen et
          al. in [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0027">27</a>]. We call this dataset as SHEN13.
          Shen et al. randomly sampled 20 users from Twitter and
          annotated at most 200 recent tweets for each user.
          Finally, SHEN13 contains 3,818 tweets and 2,203 linkable
          mentions with annotation.</p>
          <p>We also use the tweets collected by Feng et al. in
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0007">7</a>] to create the second gold
          standard dataset. Feng et al. crawled 36.7 million tweets
          posted by 5.26 million users using Twitter's streaming
          API from October 2014 to December 2014. We call this
          dataset as FENG14. We randomly sampled a quarter of these
          tweets and then randomly chose 43 users each of which
          post more than one tweet. Finally, we obtained 3,458
          tweets from 43 users. We manually annotated these tweets
          and obtained 1,036 linkable mentions with annotation.</p>
          <p>The statistics of these two tweet datasets used in the
          experiments are shown in Table <a class="tbl" href=
          "#tab2">2</a>. Although both of them are created from
          tweets, there are some clear difference. Both the average
          numbers of mentions with respect to each tweet and each
          user in FENG14 are smaller than SHEN13.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span>
              <span class="table-title">Statistics of tweet
              datasets.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Dataset</th>
                  <th style="text-align:center;">#users</th>
                  <th style="text-align:center;">#tweets</th>
                  <th style="text-align:center;">#mentions</th>
                  <th style="text-align:center;">
                  |<em>M<sub>u</sub></em> |</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">SHEN13</td>
                  <td style="text-align:center;">20</td>
                  <td style="text-align:center;">3818</td>
                  <td style="text-align:center;">2203</td>
                  <td style="text-align:center;">110</td>
                </tr>
                <tr>
                  <td style="text-align:center;">FENG14</td>
                  <td style="text-align:center;">43</td>
                  <td style="text-align:center;">3458</td>
                  <td style="text-align:center;">1036</td>
                  <td style="text-align:center;">25</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-23">
          <header>
            <div class="title-info">
              <h4><span class="section-number">6.1.3</span>
              Evaluation Method</h4>
            </div>
          </header>
          <p>Considering different attention measures, our
          attention factor graph model has several variations:</p>
          <p><strong>ATT-FULL</strong>: When we set the max
          attention number <span class=
          "inline-equation"><span class="tex">$\mathcal
          {K}$</span></span> to a large number, the attention
          factor graph model will fall back into the general factor
          graph model (i.e., the factor graph model without the
          selective attention over entities). We use this variation
          to evaluate the necessity of the attention mechanism.</p>
          <p><strong>ATT-SUM</strong>: It selects the attention
          targets according to the attention measure
          <strong>Sum</strong> (Eq. <a class="eqn" href=
          "#eq13">13</a>).</p>
          <p><strong>ATT-AVG</strong>: It selects the attention
          targets according to the attention measure
          <strong>Avg</strong> (Eq. <a class="eqn" href=
          "#eq14">14</a>).</p>
          <p><strong>ATT-MAX</strong>: It selects the attention
          targets according to the attention measure
          <strong>Max</strong> (Eq. <a class="eqn" href=
          "#eq15">15</a>).</p>
          <p>We compare them with the following baseline methods in
          terms of accuracy and efficiency:</p>
          <p><strong>POP:</strong> It is the baseline method that
          only leverages the prior popularity to disambiguate
          entities. In this method, we choose the candidate entity
          which has the max prior popularity as the best label.</p>
          <p><strong>KAURI:</strong> It is the state-of-the-art
          method proposed in [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0027">27</a>]. This model constructs a graph
          from all candidate entities. Each node in the graph
          represents a candidate entity and it has an initial
          interest score estimated by prior probability, context
          similarity, and topical coherence. Then a PageRank-like
          algorithm is proposed to propagate the interest score
          between candidate entities. Finally the best label for a
          mention is the entity which has the maximum final
          interest score.</p>
          <p>To quantitatively evaluate the proposed model, we
          consider two aspects: effectiveness and efficiency. For
          effectiveness evaluation, we consider two-fold
          cross-validation. More specifically, we construct the
          factor graph model with all mentions. Then we hide the
          labels of half nodes to learn the parameters and infer
          the best labels for the other part. We evaluate the
          methods mentioned above in terms of accuracy. For
          efficiency evaluation, we examine the execution time of
          the model learning.</p>
          <p>Our attention factor graph model needs two inputs: the
          learning rate <em>η</em> and the max attention number
          <span class="inline-equation"><span class="tex">$\mathcal
          {K}$</span></span> . The learning rate <em>η</em> for two
          weighting parameters is set to (0.01, 0.1). The max
          attention number <span class=
          "inline-equation"><span class="tex">$\mathcal
          {K}$</span></span> is set to 110 and 25 for SHEN13 and
          FENG14 respectively. We will also analyze the impact of
          <span class="inline-equation"><span class="tex">$\mathcal
          {K}$</span></span> to our model's performance.</p>
          <p>All the algorithms were implemented in Java with the
          support of JAMA<a class="fn" href="#fn6" id=
          "foot-fn6"><sup>5</sup></a> for fast matrix
          manipulations, and the experiments were carried out on a
          personal computer with Intel Xeon CPU E3-1230v3 (3.30GHz)
          and 16 GB memory. In this paper, we do not utilize the
          distributed learning and just consider single machine
          implementation.</p>
        </section>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span>
            Effectiveness</h3>
          </div>
        </header>
        <p>In this subsection, we study the effectiveness of our
        attention factor graph model under different
        configurations, and compare them with some baselines. The
        experimental result of effectiveness performance with
        different methods over two tweet datasets is shown in Table
        <a class="tbl" href="#tab3">3</a>. We present both the
        number of correctly linked mentions and the accuracy. It
        can be observed that all the variations of our model
        outperform the baselines significantly. It can be seen that
        user interest is a very powerful signal, as it helps our
        model nearly achieve a 10% improvement in terms of accuracy
        compared with the baseline <strong>POP</strong> on both two
        datasets. Compared with the state-of-the-art method
        <strong>KAURI</strong>, our model can achieve a 6%
        improvement on the dataset SHEN13 and a 4.8% improvement on
        the dataset FENG14. As they both leverage the user interest
        signal, the reason of such significant improvements is that
        the factor graph model can capture the relatedness between
        different variables better and infer a better global
        optimal configuration. Besides, we can see that different
        variations of our attention factor graph model outperform
        the general factor graph model <strong>ATT-FULL</strong>
        slightly. Among them, the variation
        <strong>ATT-AVG</strong> achieves the best performance on
        the dataset SHEN13 and the variation
        <strong>ATT-MAX</strong> achieves the best performance on
        the dataset FENG14. However, the variation
        <strong>ATT-SUM</strong> results in a performance reduction
        on both datasets in comparison with the general factor
        graph model <strong>ATT-FULL</strong>.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Effectiveness performance with different
            methods.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;" rowspan="2">
                Method</th>
                <th colspan="2" style="text-align:center;">
                SHEN13</th>
                <th colspan="2" style="text-align:center;">
                FENG14</th>
              </tr>
              <tr>
                <th style="text-align:center;">#Correct</th>
                <th style="text-align:center;">Acc. (%)</th>
                <th style="text-align:center;">#Correct</th>
                <th style="text-align:center;">Acc. (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">POP</td>
                <td style="text-align:center;">1760</td>
                <td style="text-align:center;">79.9</td>
                <td style="text-align:center;">816</td>
                <td style="text-align:center;">78.8</td>
              </tr>
              <tr>
                <td style="text-align:center;">KAURI</td>
                <td style="text-align:center;">1890</td>
                <td style="text-align:center;">85.8</td>
                <td style="text-align:center;">868</td>
                <td style="text-align:center;">83.7</td>
              </tr>
              <tr>
                <td style="text-align:center;">ATT-FULL</td>
                <td style="text-align:center;">2016</td>
                <td style="text-align:center;">91.5</td>
                <td style="text-align:center;">915</td>
                <td style="text-align:center;">88.3</td>
              </tr>
              <tr>
                <td style="text-align:center;">ATT-SUM</td>
                <td style="text-align:center;">2001</td>
                <td style="text-align:center;">90.8</td>
                <td style="text-align:center;">903</td>
                <td style="text-align:center;">87.2</td>
              </tr>
              <tr>
                <td style="text-align:center;">ATT-AVG</td>
                <td style="text-align:center;">
                <strong>2022</strong></td>
                <td style="text-align:center;">
                <strong>91.8</strong></td>
                <td style="text-align:center;">912</td>
                <td style="text-align:center;">88.0</td>
              </tr>
              <tr>
                <td style="text-align:center;">ATT-MAX</td>
                <td style="text-align:center;">2018</td>
                <td style="text-align:center;">91.6</td>
                <td style="text-align:center;">
                <strong>917</strong></td>
                <td style="text-align:center;">
                <strong>88.5</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>We further investigate the impact of different attention
        measures and different max attention number <span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> . The experimental result is shown in
        Fig. <a class="fig" href="#fig3">3</a>. As we mentioned
        before, the variations with different attention measures
        will fall back into the general factor graph model when
        <span class="inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> is set to a large number. It can be seen
        from Fig. <a class="fig" href="#fig3">3</a> that the
        accuracy of the variations with different attention
        measures all converges to the accuracy of the variation
        <strong>ATT-FULL</strong> on both datasets when the max
        attention number <span class="inline-equation"><span class=
        "tex">$\mathcal {K}$</span></span> increases. We can have
        three main observations based on the experimental result.
        First, our attention factor graph model achieves a better
        performance when the max attention number <span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> increases, as more attention targets
        bring more helpful information for disambiguation. Second,
        our attention factor graph model can still achieve a
        relatively good performance when <span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> is small. Our model outperforms the
        baseline <strong>KAURI</strong> when <span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> is set to 30 on the dataset SHEN13 and
        10 on the dataset FENG14, as these two numbers are both
        smaller than the number <span class=
        "inline-equation"><span class=
        "tex">$|\overline{M_u}|$</span></span> of two datasets
        respectively. Third, the variation <strong>ATT-MAX</strong>
        outperforms other two variations, especially when
        <span class="inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> is set to a small number. We demonstrate
        that it is an effective measure to find the mentions that
        we should pay attention to when we disambiguate a mention
        <em>m</em>.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186012/images/www2018-21-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Effectiveness performance
            with different attention measures and different max
            attention number <span class=
            "inline-equation"><span class="tex">$\mathcal
            {K}$.</span></span></span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-25">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.3</span> Efficiency
            and Scalability</h3>
          </div>
        </header>
        <p>In this subsection, we study the efficiency and
        scalability of our attention factor graph model. As the
        time complexity of the attention factor graph model does
        not depend on the attention measure, we only compare the
        variation <strong>ATT-MAX</strong> with the general factor
        graph model <strong>ATT-FULL</strong>. More specifically,
        we report the average running time for one iteration in the
        learning algorithm with different experimental settings on
        both datasets. We first demonstrate the running time with
        different max attention number <span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> in Fig. <a class="fig" href=
        "#fig4">4</a>. As we mentioned before, the attention factor
        graph model will fall back into the general factor graph
        model when <span class="inline-equation"><span class=
        "tex">$\mathcal {K}$</span></span> is set to a large
        number. We can see that our attention factor graph model is
        much more efficient than the general factor graph model.
        The variation <strong>ATT-MAX</strong> with our settings of
        the max attention number <span class=
        "inline-equation"><span class="tex">$\mathcal
        {K}$</span></span> described before results in a 16.4%
        (89.2ms) and 43.6% (41.3ms) relative reduction of the
        running time compared with <strong>ATT-FULL</strong> on two
        datasets respectively. We can also observe that the average
        running time is approximately linear to the max attention
        number <span class="inline-equation"><span class=
        "tex">$\mathcal {K}$</span></span> , which verifies the
        complexity analysis in Section <a class="sec" href=
        "#sec-18">5.3</a>. Thus, we can achieve a better efficiency
        by further decreasing the max attention number just with a
        slight decrease of accuracy.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186012/images/www2018-21-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Efficiency performance with
            different max attention number <span class=
            "inline-equation"><span class="tex">$\mathcal
            {K}$.</span></span></span>
          </div>
        </figure>
        <p></p>
        <p>Finally, we study the scalability of our attention
        factor graph model. We compare the average running time of
        different models with different size of datasets. The
        experimental result is shown in Fig. <a class="fig" href=
        "#fig5">5</a>. It can be observed that the average running
        time of the model <strong>ATT-MAX</strong> is approximately
        linear to the size of datasets while the curve of the model
        <strong>ATT-FULL</strong> is more like the quadratic
        function. This observation verifies our complexity analysis
        in both Section <a class="sec" href="#sec-14">4.4</a> and
        Section <a class="sec" href="#sec-18">5.3</a>. Thus, it can
        be said that our attention factor graph model can scale up
        better than the origin factor graph model in the
        large-scale network like Twitter.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186012/images/www2018-21-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Scalability performance of
            different methods with different size of
            datasets.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-26">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Conclusions</h2>
        </div>
      </header>
      <p>In this paper, we study the problem of tweet entity
      linking. We formalize the tweet entity linking problem into a
      factor graph model, which has been successfully applied to
      many applications. We consider two features (prior popularity
      and topical coherence based on user interest) in our model.
      Nevertheless, we claim that our model is flexible and
      extendable for new features as factor nodes can be defined on
      arbitrary sets of variables while the learning and inference
      algorithm still work. To achieve better scalability, we
      propose to adopt the attention mechanism in the factor graph
      model. We propose a new type of nodes called pseudo-variable
      nodes to solve the asymmetry attention problem caused by the
      undirected characteristic of the factor graph. Experimental
      results verify the effectiveness and efficiency of our model
      on two tweet datasets.</p>
      <p>There are some important future directions of this work.
      First, add more tweet-specific signals to the factor graph
      model. Second, formalize the problem of tweet entity linking
      into a semi-supervised framework. As we can only label
      extremely small part of data in Twitter, and Tang et al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>] has shown
      the advantage of semi-supervised learning, a semi-supervised
      framework for tweet entity linking may be also helpful and
      promising. Finally, find a better attention measure. As the
      change of variables that need attention can influence the
      final results directly, a better attention measure is also
      very important.</p>
    </section>
    <section id="sec-27">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>Chenwei Ran and Jianyong Wang were supported in part by
      National Natural Science Foundation of China under Grant No.
      61532010 and National Basic Research Program of China (973
      Program) under Grant No. 2014CB340505. Wei Shen was supported
      in part by National Natural Science Foundation of China under
      Grant No. 61502253.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Razvan&nbsp;C Bunescu
        and Marius Pasca. 2006. Using Encyclopedic Knowledge for
        Named entity Disambiguation.. In <em><em>EACL</em></em> ,
        Vol.&nbsp;6. 9–16.</li>
        <li id="BibPLXBIB0002" label="[2]">Jan Chorowski, Dzmitry
        Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
        End-to-End Continuous Speech Recognition Using
        Attention-Based Recurrent NN: First Results. In
        <em><em>NIPS 2014 Workshop on Deep Learning</em></em>
        .</li>
        <li id="BibPLXBIB0003" label="[3]">Silviu Cucerzan. 2007.
        Large-Scale Named Entity Disambiguation Based on Wikipedia
        Data. In <em><em>Proceedings of the 2007 Joint Conference
        on Empirical Methods in Natural Language Processing and
        Computational Natural Language Learning</em></em> .</li>
        <li id="BibPLXBIB0004" label="[4]">Diego&nbsp;Marinho de
        Oliveira, Alberto&nbsp;HF Laender, Adriano Veloso, and
        Altigran&nbsp;S da Silva. 2013. FS-NER: a Lightweight
        Filter-Stream Approach to Named Entity Recognition on
        Twitter Data. In <em><em>Proceedings of the 22nd
        International Conference on World Wide Web</em></em> .
        International World Wide Web Conferences Steering
        Committee, 597–604.</li>
        <li id="BibPLXBIB0005" label="[5]">Yuan Fang and Ming-Wei
        Chang. 2014. Entity Linking on Microblogs with Spatial and
        Temporal Signals. <em><em>Transactions of the Association
        for Computational Linguistics</em></em> 2 (2014),
        259–272.</li>
        <li id="BibPLXBIB0006" label="[6]">Wei Feng and Jianyong
        Wang. 2012. Incorporating Heterogeneous Information for
        Personalized Tag Recommendation in Social Tagging Systems.
        In <em><em>Proceedings of the 18th ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining</em></em>
        . ACM, 1276–1284.</li>
        <li id="BibPLXBIB0007" label="[7]">Wei Feng, Chao Zhang,
        Wei Zhang, Jiawei Han, Jianyong Wang, Charu Aggarwal, and
        Jianbin Huang. 2015. STREAMCUBE: Hierarchical
        Spatio-Temporal Hashtag Clustering for Event Exploration
        over the Twitter Stream. In <em><em>2015 IEEE 31st
        International Conference on Data Engineering</em></em> .
        IEEE, 1561–1572.</li>
        <li id="BibPLXBIB0008" label="[8]">Stephen Guo, Ming-Wei
        Chang, and Emre Kiciman. 2013. To Link or Not to Link? A
        Study on End-to-End Tweet Entity Linking. In
        <em><em>Proceedings of The 2013 Conference of the North
        American Chapter of the Association for Computational
        Linguistics: Human Language Technologies</em></em> .
        1020–1030.</li>
        <li id="BibPLXBIB0009" label="[9]">Yuhang Guo, Bing Qin,
        Ting Liu, and Sheng Li. 2013. Microblog Entity Linking by
        Leveraging Extra Posts. In <em><em>Proceedings of the 2013
        Conference on Empirical Methods in Natural Language
        Processing</em></em> . 863–868.</li>
        <li id="BibPLXBIB0010" label="[10]">Johannes Hoffart,
        Mohamed&nbsp;Amir Yosef, Ilaria Bordino, Hagen Fürstenau,
        Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan
        Thater, and Gerhard Weikum. 2011. Robust Disambiguation of
        Named Entities in Text. In <em><em>Proceedings of the
        Conference on Empirical Methods in Natural Language
        Processing</em></em> . ACL, 782–792.</li>
        <li id="BibPLXBIB0011" label="[11]">Wen Hua, Kai Zheng, and
        Xiaofang Zhou. 2015. Microblog Entity Linking with Social
        Temporal Context. In <em><em>Proceedings of the 2015 ACM
        SIGMOD International Conference on Management of
        Data</em></em> . ACM, 1761–1775.</li>
        <li id="BibPLXBIB0012" label="[12]">George Karypis and
        Vipin Kumar. 1995. METIS–Unstructured Graph Partitioning
        and Sparse Matrix Ordering System, Version 2.0.
        (1995).</li>
        <li id="BibPLXBIB0013" label="[13]">Dominik Kowald,
        Subhash&nbsp;Chandra Pujari, and Elisabeth Lex. 2017.
        Temporal Effects on Hashtag Reuse in Twitter: A
        Cognitive-Inspired Hashtag Recommendation Approach. In
        <em><em>Proceedings of the 26th International Conference on
        World Wide Web</em></em> . International World Wide Web
        Conferences Steering Committee, 1401–1410.</li>
        <li id="BibPLXBIB0014" label="[14]">Kathy Lee, Ashequl
        Qadir, Sadid&nbsp;A Hasan, Vivek Datla, Aaditya Prakash,
        Joey Liu, and Oladimeji Farri. 2017. Adverse Drug Event
        Detection in Tweets with Semi-Supervised Convolutional
        Neural Networks. In <em><em>Proceedings of the 26th
        International Conference on World Wide Web</em></em> .
        International World Wide Web Conferences Steering
        Committee, 705–714.</li>
        <li id="BibPLXBIB0015" label="[15]">Chenliang Li, Jianshu
        Weng, Qi He, Yuxia Yao, Anwitaman Datta, Aixin Sun, and
        Bu-Sung Lee. 2012. Twiner: Named Entity Recognition in
        Targeted Twitter Stream. In <em><em>Proceedings of the 35th
        international ACM SIGIR Conference on Research and
        Development in Information Retrieval</em></em> . ACM,
        721–730.</li>
        <li id="BibPLXBIB0016" label="[16]">Yankai Lin, Shiqi Shen,
        Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural
        Relation Extraction with Selective Attention over
        Instances. In <em><em>Proceedings of the 54th Annual
        Meeting of the Association for Computational Linguistics.
        ACL</em></em> .</li>
        <li id="BibPLXBIB0017" label="[17]">Xiaohua Liu, Yitong Li,
        Haocheng Wu, Ming Zhou, Furu Wei, and Yi Lu. 2013. Entity
        Linking for Tweets.. In <em><em>Proceedings of the 51st
        Annual Meeting of the Association for Computational
        Linguistics</em></em> . 1304–1311.</li>
        <li id="BibPLXBIB0018" label="[18]">Xiaohua Liu, Shaodian
        Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing Named
        Entities in Tweets. In <em><em>Proceedings of the 49th
        Annual Meeting of the Association for Computational
        Linguistics</em></em> . ACL, 359–367.</li>
        <li id="BibPLXBIB0019" label="[19]">Michael Mathioudakis
        and Nick Koudas. 2010. Twittermonitor: Trend Detection over
        The Twitter Stream. In <em><em>Proceedings of the 2010 ACM
        SIGMOD International Conference on Management of
        Data</em></em> . ACM, 1155–1158.</li>
        <li id="BibPLXBIB0020" label="[20]">Edgar Meij, Wouter
        Weerkamp, and Maarten De&nbsp;Rijke. 2012. Adding Semantics
        to Microblog Posts. In <em><em>Proceedings of the fifth ACM
        International Conference on Web Search and Data
        Mining</em></em> . ACM, 563–572.</li>
        <li id="BibPLXBIB0021" label="[21]">David Milne and
        Ian&nbsp;H. Witten. 2008. An Effective, Low-Cost Measure of
        Semantic Relatedness Obtained from Wikipedia links. In
        <em><em>WIKIAI</em></em> .</li>
        <li id="BibPLXBIB0022" label="[22]">Volodymyr Mnih, Nicolas
        Heess, Alex Graves, and others. 2014. Recurrent Models of
        Visual Attention. In <em><em>Advances in Neural Information
        Processing Systems</em></em> . 2204–2212.</li>
        <li id="BibPLXBIB0023" label="[23]">Kevin&nbsp;P Murphy,
        Yair Weiss, and Michael&nbsp;I Jordan. 1999. Loopy Belief
        Propagation for Approximate Inference: An Empirical Study.
        In <em><em>Proceedings of the 15th Conference on
        Uncertainty in Artificial Intelligence</em></em> . Morgan
        Kaufmann Publishers Inc., 467–475.</li>
        <li id="BibPLXBIB0024" label="[24]">Alan Ritter, Evan
        Wright, William Casey, and Tom Mitchell. 2015. Weakly
        Supervised Extraction of Computer Security Events from
        Twitter. In <em><em>Proceedings of the 24th International
        Conference on World Wide Web</em></em> . International
        World Wide Web Conferences Steering Committee,
        896–905.</li>
        <li id="BibPLXBIB0025" label="[25]">Wei Shen, Jianyong
        Wang, and Jiawei Han. 2015. Entity linking with a Knowledge
        Base: Issues, Techniques, and Solutions. <em><em>IEEE
        Transactions on Knowledge and Data Engineering</em></em>
        27, 2(2015), 443–460.</li>
        <li id="BibPLXBIB0026" label="[26]">Wei Shen, Jianyong
        Wang, Ping Luo, and Min Wang. 2012. Linden: Linking Named
        Entities with Knowledge Base via Semantic Knowledge. In
        <em><em>Proceedings of the 21st International Conference on
        World Wide Web</em></em> . International World Wide Web
        Conferences Steering Committee, 449–458.</li>
        <li id="BibPLXBIB0027" label="[27]">Wei Shen, Jianyong
        Wang, Ping Luo, and Min Wang. 2013. Linking Named Entities
        in Tweets with Knowledge Base via User Interest Modeling.
        In <em><em>Proceedings of the 19th ACM SIGKDD International
        Conference on Knowledge Discovery and Data Mining</em></em>
        . ACM, 68–76.</li>
        <li id="BibPLXBIB0028" label="[28]">Jie Tang, Jimeng Sun,
        Chi Wang, and Zi Yang. 2009. Social Influence Analysis in
        Large-Scale Networks. In <em><em>Proceedings of the 15th
        ACM SIGKDD International Conference on Knowledge Discovery
        and Data Mining</em></em> . ACM, 807–816.</li>
        <li id="BibPLXBIB0029" label="[29]">Wenbin Tang, Honglei
        Zhuang, and Jie Tang. 2011. Learning to Infer Social Ties
        in Large Networks. In <em><em>Proceedings of the 2011
        European Conference on Machine Learning and Knowledge
        Discovery in Databases</em></em> . Springer, 381–397.</li>
        <li id="BibPLXBIB0030" label="[30]">Zhichun Wang, Juanzi
        Li, Zhigang Wang, and Jie Tang. 2012. Cross-Lingual
        Knowledge Linking across Wiki Knowledge Bases. In
        <em><em>Proceedings of the 21st International Conference on
        World Wide Web</em></em> . International World Wide Web
        Conferences Steering Committee, 459–468.</li>
        <li id="BibPLXBIB0031" label="[31]">Yi Yang and Ming-Wei
        Chang. 2015. S-MART: Novel Tree-based Structured Learning
        Algorithms Applied to Tweet Entity Linking. In
        <em><em>Proceedings of the 53rd Annual Meeting of the
        Association for Computational Linguistics and the 7th
        International Joint Conference on Natural Language
        Processing</em></em> . ACL, 504–513.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>*</sup></a>Corresponding
    author</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://twitter.com">https://twitter.com</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://en.wikipedia.org">https://en.wikipedia.org</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://stats.wikimedia.org/EN/TablesWikipediaEN.htm">https://stats.wikimedia.org/EN/TablesWikipediaEN.htm</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/dnmilne/wikipediaminer">https://github.com/dnmilne/wikipediaminer</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class=
    "link-inline force-break" href=
    "http://math.nist.gov/javanumerics/jama">http://math.nist.gov/javanumerics/jama</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23–27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186012">https://doi.org/10.1145/3178876.3186012</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
