<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>VIZ-Wiki: Generating Visual Summaries to Factoid Threads in Community Question Answering Services</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186986'>https://doi.org/10.1145/3184558.3186986</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186986'>https://w3id.org/oa/10.1145/3184558.3186986</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">VIZ-Wiki: Generating Visual Summaries to Factoid Threads in Community Question Answering Services</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Tanya</span> <span class="surName">Chowdhury</span>, IIIT-Delhi, <a href="mailto:tanya14109@iiitd.ac.in">tanya14109@iiitd.ac.in</a>
        </div>
        <div class="author">
          <span class="givenName">Aashay</span> <span class="surName">Mittal</span>, IIIT-Delhi, <a href="mailto:aashay14001@iiitd.ac.in">aashay14001@iiitd.ac.in</a>
        </div>
        <div class="author">
          <span class="givenName">Tanmoy</span> <span class="surName">Chakraborty</span>, IIIT-Delhi, <a href="mailto:tanmoy@iiitd.ac.in">tanmoy@iiitd.ac.in</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186986" target="_blank">https://doi.org/10.1145/3184558.3186986</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>In this demo, we present VIZ-Wiki, a browser extension which generates an overview of summarizable threads in Question Answering forums. It reduces a user's effort to go through lengthy text-based, sarcastic and highly critiqued answers. Our tool can be used to collect community opinion from popular discussion sites like Quora, Yahoo! Answers, Reddit etc. as well as topic-centric ones such as Askubuntu, Stackoverflow. We rely on textual information of these forums to extract insightful summaries for a reader.</small></p>
        <p><small>VIZ-Wiki provides users a pie-graph view marking popular choices when such a question link is raised. A button guides them to detailed statistics and relevant list of answers. It further highlights sentences relevant to an answer choice in the text. VIZ-Wiki deals with answers contradicted by other users, prioritizes highly-recommended ones and avoids sarcasm. We test our model on the factoid questions dataset of Yahoo! Answers and obtain a macro precision of 0.6 on displayed answers and a macro recall of 0.69, beating the baseline significantly. To the best of our knowledge, <em>VIZ-Wiki is the first attempt to visualize answers for questions in community question answering services</em>. <strong>In the spirit of reproducibility, we have released the code and a demonstration video public at <a href="http://goo.gl/cyx3EF">http://goo.gl/cyx3EF</a> and <a href="https://youtu.be/XNmRa_jtmC8">https://youtu.be/XNmRa_jtmC8</a> respectively</strong>.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Information retrieval query processing;</strong> <em>Information retrieval;</em> Information extraction;</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Factoid questions; Summarization; CQA services</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Tanya Chowdhury, Aashay Mittal, and Tanmoy Chakraborty. 2018. VIZ-Wiki: Generating Visual Summaries to Factoid Threads in Community Question Answering Services. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 4 Pages. <a href="https://doi.org/10.1145/3184558.3186986" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186986</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Community question answering (CQA) services have of late gained a lot of popularity. Readers browse through Quora, Reddit in leisure time. Stackoverflow is immensely popular amongst the programming community to discuss common errors and optimal strategies. Community forums such as Stackexchange and Mathoverflow exist to cater to specific scientific communities.</p>
      <p><strong>Motivation:</strong> Most of these forums are open to join and post. They are usually unmoderated and often unstructured. Few questions have answers running in hundreds, and it is difficult for the readers to browse through all to obtain opinion. Also being open, these forums are often subject to slurs, hatred, and used as a medium for propaganda. Most forums allow users to support an answer through an ‘upvote’, and the most upvoted answer is awarded the best answer. Users often write lengthy paragraphs for factoid queries and deviate from the original topic. Little work exists to summarize these threads in order to filter out useful information from these knowledge bases.</p>
      <p>The concept of Answer-Wikis<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> in CQA was introduced by Quora. It is a bulleted list allowing the community to collaborate on an answer summary or an aggregated answer for a particular question. According to Quora, these wikis are aimed at building a comprehensive collection of uncontroversial, factual information. A bulleted list of answers and their visual representation for factoid questions, would make it easier for a user to understand the answers at a glance. To the best of our knowledge, Answer Wikis on Quora are manually curated, which requires unanonymous edits and staff moderation<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>. A user is able to create an Answer Wiki only when the question has at least 3 answers.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186986/images/www18companion-226-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Question thread from Yahoo! Answers (question noun headword in yellow and annotated answers in green).</span>
        </div>
      </figure><strong>System Architecture:</strong> VIZ-Wiki is an online tool which automatically generates a visual summary of answers for factoid questions posted in CQA forums (as shown in Figure <a class="fig" href="#fig1">1</a>). It follows a novel architecture, right from the data-preprocessing step to the presentation of summary results (see Figure <a class="fig" href="#fig2">2</a>). When a CQA question is loaded with VIZ-Wiki, a button is shown, by clicking on which the question type is determined and the answers are parsed. The sentiment of the question is gauged and answer sentences with opposite sentiments are removed. An attempt is made to find candidate answers via Named Entity Recognizer (NER), Anaphora Resolver and Dependency Parser. Answer phrases with <em>nsubj</em> relation with the noun headword are marked as candidate answers. Candidate answers from different user-written answers are merged, and the most popular choices in the community are displayed through a pie-graph (see Figures <a class="fig" href="#fig3">3</a> and <a class="fig" href="#fig4">4</a>).
      <p></p>
      <p>Since we did not receive permission to scrape Quora, we build a generic model and test it on Yahoo! Answers (with suitable permission). Experimental results with human judgment show that VIZ-Wiki performs significantly well in extracting a meaningful list of answers, achieving a macro precision of 0.6 on displayed answers and a macro recall of 0.69 .</p>
      <figure id="fig2">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186986/images/www18companion-226-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class="figure-title">System architecture of VIZ-Wiki.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>Previous work mostly attempted to find similar questions in CQA forums [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]. A <em>similar question database</em><a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> was also made available by Quora for analysis on this problem. However, it only comprises of similar question pairs, and it is not permissible to scrape corresponding answers from Quora. Liu et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>] analysed the quality of answers and suggested an optimal strategy to choose the best answer. Nakov et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] focused on ranking of answers in these online forums based on their content and quality.</p>
      <p>Not much work has been done in generating answer overviews for questions in CQA forums. Pande et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] attempted to solve the problem by adding missing valuable information to the ”best answer”. Song et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] summarized non-factoid answers in CQA services. They employed a sparse coding based summarization strategy based on document expansion.</p>
      <p>A significant amount of research has been done in extracting answers for factoid questions given a corpus of relevant documents. Wang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] wrote a survey on answer extraction techniques for factoid question answering.</p>
      <p>VIZ-Wiki is different from others in many aspects. It analyzes the sentiment of the question to remove opposing sentiment answers, which are otherwise detected by a Named Entity Recognizer. To deal with question-type classifier's and NER's failure in detecting an answer, VIZ-Wiki uses anaphora to resolve the text followed by dependency parsing. Unlike previous work, VIZ-Wiki creates visual summaries and bulleted lists which can be understood at a glance.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Methodology and Framework</h2>
        </div>
      </header>
      <p>In this section, we describe individual modules of VIZ-Wiki. Figure <a class="fig" href="#fig2">2</a> shows the overall system framework of VIZ-Wiki.</p>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Data Acquisition</h3>
          </div>
        </header>
        <p>We choose our dataset from the official factoid Queries dataset<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> provided by <em>Yahoo! Answers</em><a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a> (with suitable permission). Also since the official factoid questions dataset primarily contains one correct answer per question (not survey questions), we took only 15% of our final dataset from them and the rest was randomly taken from Yahoo! Answers. When a user clicks on the VIZ-Wiki button on a question link page, we scrape the question and pass it on to the next module (question type classifier).</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Question Type Classifier</h3>
          </div>
        </header>
        <p>Li et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] suggested that all factoid questions can be classified into 6 coarse classes. We build a supervised question type classifier to identify which class a question belongs to. We use the dataset provided by Li and Roth, containing 5,500 hand-tagged questions to train our model. Each question is labeled as one of the following 6 coarse-grained categories – ‘abbreviation’, ‘entity’, ‘description’, ‘human’, ‘location’, and ‘numeric’. Here, we train a supervised model on this dataset. We use training features such as words of the question, their POS tags, named-entities, the Wh-word, the noun-head chunk (the first noun following the Wh-word) and the verb-head chunk (the first verb following the Wh-word). We also add Word2Vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] information to take into consideration semantic similarity amongst different question words. We feed these features to an SVM classifier<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> and use standard grid-search for hyper-parameter optimization. We validate our model with a test set of 500 questions released in TREC’10<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a>. We then run this pre-trained model on our dataset. If the question type is identified as ‘description’, our pipeline does not attempt to find an answer for that question; otherwise we proceed to the next module (answer type identification).</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Answer Filtering</h3>
          </div>
        </header>
        <p>Certain answers in CQA forums may deviate from the actual point. E.g., one of the answers to the question “What is your least favorite Oscar winning film?” <a class="fn" href="#fn8" id="foot-fn8"><sup>8</sup></a> goes like: “Faves: Halle Berry's for Monster's Ball. Even though she forgot to mention ...(contd)”. As a result, we analyse the sentiment of the question and answer statements and rule out mismatched sentences from further processing. We use Serendio [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] to classify the question into ‘Positive’, ‘Negative’ or ‘Neutral’. Serendio offers a normalized sentence sentiment score per class using Sentiwordnet [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] on feature words. The winner class is the class with the maximum score. We define a sentiment as strongly ‘Positive’ or ‘Negative’ if the dominant class score in a sentence is greater than 0.4. For such sentiment polar questions, we analyze the sentiment of its answer statements. We remove occurrences of strongly ‘Negative’ sentiment sentences from questions classified strongly ‘Positive’ and vice-versa.</p>
        <p>Answers usually comprise of: one word, one phrase, list of phrases or paragraphs. We directly mark one word, one phrase and list of phrases as candidate answers. It is impossible to process huge paragraphs of text at run time. As a result, we extract important sentences from lengthy answers. We use a naive algorithm for important sentence extraction as follows. We run a POS-tagger on the answers. Next, we assign a score to all sentences based on its constituent words. We assign a weight of 2 to each proper noun and a weight of 1 to each common noun (singular/plural) <a class="fn" href="#fn9" id="foot-fn9"><sup>9</sup></a>. We define a threshold and assign it a value proportional to the answer length and calculated score. Sentences with score above this threshold are marked important. We then proceed with a list of important sentences to the next module.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Named Entity Recognition</h3>
          </div>
        </header>
        <p>Wang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] mentioned that all answers to factoid questions could be detected by NER. In this step, we apply Stanford NER [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] to: one phrase long answers, answers with a bulleted-list of phrases and important sentences extracted from paragraph-based answers. Stanford NER provides three types of named-entity classifiers – a 3-class NER (Organization, Location, Person), a 4-class NER (Organization, Location, Person, Misc) and a 7-class NER (Location, Organization, Date, Money, Person, Percent, Time). We observed that if our question type belongs to one of the classes provided by the 3-class classifier, then an answer was more likely to be detected by the 3-class NER than the 4 or 7-class NERs. As a result, if the question type is one of ‘human’, ‘entity’ or ‘location’, we use the 3-class NER. If the question type is ‘entity’ and we do not find an answer with the 3 class tagger, we roll back and employ the 7-class NER. In other cases, we directly use the 7-class NER. This model seems to give the best results (F-score of 0.7) on our data.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span> Anaphora Resolution</h3>
          </div>
        </header>
        <p>We use BART toolkit [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] on paragraph-based answers for anaphora resolution. Note that we consider entire paragraphs for anaphora resolution instead of important sentences, in case no answer is found yet. The REST-based web service of BART returns co-reference chains with each chain having a unique SetID attribute. We replace pronouns in the text with the nouns that they refers to. The next module, i.e., the dependency parser is used to extract answers in anaphora resolved text.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.6</span> Dependency Parsing</h3>
          </div>
        </header>
        <p>Next we identify the noun headword in the Question. For this we run a dependency parser on the question statement. The word which has a subject relation with the Wh-word, directly or via a determiner, is tagged as the noun headword. In case there exists no Wh-word in the question, we set hand-based rules to determine the noun head chunk.</p>
        <p>We then convert the answer to a list of sentences. We iterate over this list and generate dependency trees for each sentence. These dependency trees are stored as a list of dependency relations, i.e., three element tuples. We look for dependency relations common between the question and answer sentences. In case there is a match, we look for the noun headword in the sentence. If the noun headword exists, then the word with which it has a subject relation is a candidate answer. In case a direct subject relation does not exist, we add all nearest noun candidates via determiners to the list of candidate answers.</p>
        <p>We tested with two dependency parsers: Stanford Dependency Parser [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] and Stanford CoreNLP Dependency Parser [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. The CoreNLP parser seemed to perform marginally better in our case.</p>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186986/images/www18companion-226-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Primary view of VIZ-Wiki UI.</span>
          </div>
        </figure>
        <figure id="fig4">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186986/images/www18companion-226-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span> <span class="figure-title">Extended view of VIZ-Wiki UI.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.7</span> Merging of Answers</h3>
          </div>
        </header>
        <p>At the end of the pipeline, we obtain a list of candidate answer tuples for a particular question; one tuple from each community-user answer. We now create a histogram with every candidate-answer as the key, and a count of number of actual answers in which it has appeared as the value. In this process, we find that often candidate answers, due to spelling variation, receive different keys. As a result we set a minimum edit distance (2), below which we club histogram keys and add their counts. We then send the candidate answers and their counts to the display module of VIZ-Wiki.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.8</span> Display</h3>
          </div>
        </header>
        <p>We pick the top answers that cumulatively contribute to approximately 60% of the results and display their names and percentages in a pie chart. We club all other answers into an ‘Others’ category. In case of less that 5 answers, we plot all answers in the chart. We name this sub-class of candidate answers as display answers. This is followed to avoid overcrowding of pie chart with multiple low-frequency candidate answers. A button directs the user to a comprehensive list of less popular answers (see Figures <a class="fig" href="#fig3">3</a> and <a class="fig" href="#fig4">4</a>). We use <em>CanvasJs</em><a class="fn" href="#fn10" id="foot-fn10"><sup>10</sup></a>, a responsive HTML5 charting library, to visualize pie-charts in VIZ-Wiki. When the user clicks on one of the pie chart labels, we highlight content related to that answer choice, in the text.</p>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Evaluation</h2>
        </div>
      </header>
      <p>For evaluating VIZ-Wiki, we built a dataset of 500 factoid questions from Yahoo! Answers. Since in the official Yahoo! answers factoid questions dataset, each question primarily has one correct answer (not survey questions), we took only 15% of our final dataset from them and the rest was randomly generated from Yahoo! Answers. Three annotators manually tagged the questions into one of the six categories as mentioned in Section <a class="sec" href="#sec-8">3.2</a> (inter-annotator agreement was 0.79). This forms the ground-truth for question type classification. Similarly, six annotators generated wikis for our dataset answers. The generated wikis do not take into account candidate answer popularity and ranking. Answers with multiple occurrences have been reported only once. An answer is marked to be a candidate answer if it is favorably referred to in at least half its occurrences. Again we consider this set of annotated Wikis as the ground-truth in the last stage, for evaluating final answers. We consider the output of Stanford NER as a <strong>baseline system</strong>.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class="table-title">Performance of the question type classification module of VIZ-Wiki after the addition of each feature.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;"><strong>Sl. No.</strong></td>
              <td style="text-align:center;"><strong>Features Used</strong></td>
              <td style="text-align:center;"><strong>F-Score</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;">(1)</td>
              <td style="text-align:center;">Sentence unigrams</td>
              <td style="text-align:center;">0.52</td>
            </tr>
            <tr>
              <td style="text-align:center;">+ (2)</td>
              <td style="text-align:center;">POS-Tags of words</td>
              <td style="text-align:center;">0.58</td>
            </tr>
            <tr>
              <td style="text-align:center;">+ (3)</td>
              <td style="text-align:center;">Word2Vec vectors of sentence unigrams</td>
              <td style="text-align:center;">0.67</td>
            </tr>
            <tr>
              <td style="text-align:center;">+ (4)</td>
              <td style="text-align:center;">Noun &amp; verb chunks and Wh-Words</td>
              <td style="text-align:center;">0.71</td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Evaluation of Question Type Classifier</h3>
          </div>
        </header>
        <p>As stated earlier, VIZ-Wiki uses unigrams, POS tags of unigrams, Word2Vec representation of words, weighted noun &amp; verb chunks and Wh-words features for question type classification. Table &nbsp;<a class="tbl" href="#tab1">1</a> shows the F-score of VIZ-Wiki Question Type Classifier with the addition of each feature. The Question Type classifier module of VIZ-Wiki achieves a F-score of 0.71.</p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Evaluation of Answer Extraction Model</h3>
          </div>
        </header>
        <p>We compare VIZ-Wiki with the baseline model by two relevance measures - (i) <strong>Precision:</strong> We find the precision of displayed answers as the fraction of correct answers among the set of answers returned. (ii) <strong>Recall:</strong> We calculate the recall value of the computed candidate answers against the hand tagged ground truth wikis as the fraction of correct answers which are returned. Note that we ignore answer ranking and popularity while computing precision and recall.</p>
        <p>The accuracy is measured for each question. Table &nbsp;<a class="tbl" href="#tab2">2</a> shows the micro and macro statistics of the competing models. We observe that VIZ-Wiki outperforms the baseline by a significant margin for all the measures.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Performance of baseline and VIZ-Wiki for extracting final answers w.r.t Micro Precison (Mi-P), Macro Precision (Ma-P), Micro Recall (Mi-R) and Macro Recall (Ma-R).</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;"><strong>Model</strong></td>
                <td style="text-align:center;"><strong>Mi-P</strong></td>
                <td style="text-align:center;"><strong>Ma-P</strong></td>
                <td style="text-align:center;"><strong>Mi-R</strong></td>
                <td style="text-align:center;"><strong>Ma-R</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">Baseline</td>
                <td style="text-align:center;">0.60</td>
                <td style="text-align:center;">0.59</td>
                <td style="text-align:center;">0.51</td>
                <td style="text-align:center;">0.52</td>
              </tr>
              <tr>
                <td style="text-align:center;">VIZ-Wiki</td>
                <td style="text-align:center;">0.60</td>
                <td style="text-align:center;">0.60</td>
                <td style="text-align:center;">0.67</td>
                <td style="text-align:center;">0.69</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion and Future Work</h2>
        </div>
      </header>
      <p>Question answering forums could deploy built-in visual summarizers to ease user browsing. We developed VIZ-Wiki that can automate this summarization process. VIZ-Wiki was further tested by human experts. The proposed framework can be extended to non-factoid description-based question threads.</p>
      <p>Often questions on these online forums ask for procedures to do a task (eg: WikiHow). In future, we would try to come up with a model which gives a graph-based depiction of steps, combining significant answers via abstractive summarization.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>The work was partially supported by the Ramanujan Fellowship, SERB-DST, Govt. of India.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.. In <em><em>LREC</em></em> , Vol.&nbsp;10. 2200–2204.</li>
        <li id="BibPLXBIB0002" label="[2]">Samuel Broscheit, Massimo Poesio, Simone&nbsp;Paolo Ponzetto, Kepa&nbsp;Joseba Rodriguez, Lorenza Romano, Olga Uryupina, Yannick Versley, and Roberto Zanoli. 2010. BART: A multilingual anaphora resolution system. In <em><em>Proceedings of the 5th international workshop on semantic evaluation</em></em> . ACL, 104–107.</li>
        <li id="BibPLXBIB0003" label="[3]">Marie-Catherine De&nbsp;Marneffe, Bill MacCartney, Christopher&nbsp;D Manning, <em>et al.</em> 2006. Generating typed dependency parses from phrase structure parses. In <em><em>Proceedings of LREC</em></em> , Vol.&nbsp;6. Genoa Italy, 449–454.</li>
        <li id="BibPLXBIB0004" label="[4]">Jenny&nbsp;Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In <em><em>Proceedings of the 43rd annual meeting on association for computational linguistics</em></em> . ACL, 363–370.</li>
        <li id="BibPLXBIB0005" label="[5]">Jiwoon Jeon, W&nbsp;Bruce Croft, and Joon&nbsp;Ho Lee. 2005. Finding similar questions in large question and answer archives. In <em><em>Proceedings of the 14th ACM international conference on Information and knowledge management</em></em> . ACM, 84–90.</li>
        <li id="BibPLXBIB0006" label="[6]">Xin Li and Dan Roth. 2002. Learning question classifiers. In <em><em>Proceedings of the 19th international conference on Computational linguistics-Volume 1</em></em> . Association for Computational Linguistics, 1–7.</li>
        <li id="BibPLXBIB0007" label="[7]">Mingrong Liu, Yicen Liu, and Qing Yang. 2010. <em><em>Predicting Best Answerers for New Questions in Community Question Answering</em></em> . Springer Berlin Heidelberg, Berlin, Heidelberg, 127–138.</li>
        <li id="BibPLXBIB0008" label="[8]">Christopher&nbsp;D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven&nbsp;J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In <em><em>Association for Computational Linguistics (ACL) System Demonstrations</em></em> . 55–60.</li>
        <li id="BibPLXBIB0009" label="[9]">Tomas Mikolov, Quoc&nbsp;V Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. <em><em>arXiv preprint arXiv:1309.4168</em></em> (2013).</li>
        <li id="BibPLXBIB0010" label="[10]">Preslav Nakov, Doris Hoogeveen, Lluís Màrquez, Alessandro Moschitti, Hamdy Mubarak, Timothy Baldwin, and Karin Verspoor. 2017. SemEval-2017 task 3: Community question answering. In <em><em>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</em></em> . 27–48.</li>
        <li id="BibPLXBIB0011" label="[11]">Prabu Palanisamy, Vineet Yadav, and Harsha Elchuri. 2013. Serendio: Simple and Practical lexicon based approach to Sentiment Analysis. In <em><em>proceedings of Second Joint Conference on Lexical and Computational Semantics</em></em> . 543–548.</li>
        <li id="BibPLXBIB0012" label="[12]">Vinay Pande, Tanmoy Mukherjee, and Vasudeva Varma. 2013. <em><em>Summarizing Answers for Community Question Answer Services</em></em> . Springer Berlin Heidelberg, Berlin, Heidelberg, 151–161.</li>
        <li id="BibPLXBIB0013" label="[13]">Hongya Song, Zhaochun Ren, Shangsong Liang, Piji Li, Jun Ma, and Maarten de Rijke. 2017. Summarizing answers in non-factoid community question-answering. In <em><em>WSDM</em></em> . ACM, 405–414.</li>
        <li id="BibPLXBIB0014" label="[14]">Mengqiu Wang. 2006. A survey of answer extraction techniques in factoid question answering. <em><em>Computational Linguistics</em></em> 1, 1 (2006).</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://www.quora.com/What-is-an-Answer-Wiki-on-Quora-1">https://www.quora.com/What-is-an-Answer-Wiki-on-Quora-1</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a href="https://goo.gl/m9LrRY">https://goo.gl/m9LrRY</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs">https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>VIZ-Wiki can be employed on Quora threads too. However, scraping Quora isn't permissible. Hence we were unable to include our analysis on the Quora dataset.</p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break" href="https://answers.yahoo.com/">https://answers.yahoo.com/</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>We experimented with many classifiers such as decision Tree, Logistic Regression, K-NN, and got the best results from SVM.</p>
    <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a><a class="link-inline force-break" href="http://trec.nist.gov/data/web10.html">http://trec.nist.gov/data/web10.html</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a><a class="link-inline force-break" href="https://answers.yahoo.com/question/index?qid=20130218073751AAG6zGa">https://answers.yahoo.com/question/index?qid=20130218073751AAG6zGa</a></p>
    <p id="fn9"><a href="#foot-fn9"><sup>9</sup></a><a href="https://medium.com/@acrosson/summarize-documents-using-tf-idf-bdee8f60b71">https://medium.com/@acrosson/summarize-documents-using-tf-idf-bdee8f60b71</a></p>
    <p id="fn10"><a href="#foot-fn10"><sup>10</sup></a><a href="https://canvasjs.com/">https://canvasjs.com/</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18 Companion,, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License.<br />
      ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186986">https://doi.org/10.1145/3184558.3186986</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
