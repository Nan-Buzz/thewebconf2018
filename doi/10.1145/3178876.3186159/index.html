<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>How to Impute Missing Ratings? Claims, Solution, and Its
  Application to Collaborative Filtering</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186159'>https://doi.org/10.1145/3178876.3186159</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186159'>https://w3id.org/oa/10.1145/3178876.3186159</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">How to Impute Missing Ratings?
          Claims, Solution, and Its Application to Collaborative
          Filtering</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Youngnam</span> <span class=
          "surName">Lee</span>, Hanyang University, Seoul, Korea,
          <a href=
          "mailto:youngnam.lee@vuno.co">youngnam.lee@vuno.co</a>
        </div>
        <div class="author">
          <span class="givenName">Sang-Wook</span> <span class=
          "surName">Kim</span>, Hanyang University, Seoul, Korea,
          <a href=
          "mailto:wook@hanyang.ac.kr">wook@hanyang.ac.kr</a>
        </div>
        <div class="author">
          <span class="givenName">Sunju</span> <span class=
          "surName">Park</span>, Yonsei University, Seoul, Korea,
          <a href=
          "mailto:boxenju@yonsei.ac.kr">boxenju@yonsei.ac.kr</a>
        </div>
        <div class="author">
          <span class="givenName">Xing</span> <span class=
          "surName">Xie</span>, Microsoft Research Asia, Beijing,
          China, <a href=
          "mailto:xing.xie@microsoft.com">xing.xie@microsoft.com</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186159"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186159</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Data sparsity is one of the biggest problems
        faced by collaborative filtering used in recommender
        systems. <em>Data imputation</em> alleviates the data
        sparsity problem by inferring missing ratings and imputing
        them to the original rating matrix. In this paper, we
        identify the limitations of existing data imputation
        approaches and suggest three new claims that all data
        imputation approaches should follow to achieve high
        recommendation accuracy. Furthermore, we propose a
        deep-learning based approach to compute imputed values that
        satisfies all three claims. Based on our hypothesis that
        most pre-use preferences (e.g., impressions) on items lead
        to their post-use preferences (e.g., ratings), our approach
        tries to understand via deep learning how pre-use
        preferences lead to post-use preferences differently
        depending on the characteristics of users and items.
        Through extensive experiments on real-world datasets, we
        verify our three claims and hypothesis, and also
        demonstrate that our approach significantly outperforms
        existing state-of-the-art approaches.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Recommender systems;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Recommender Systems;
          Collaborative Filtering; Data Sparsity; Data
          Imputation</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Youngnam Lee, Sang-Wook Kim, Sunju Park, and Xing Xie.
          2018. How to Impute Missing Ratings? Claims, Solution,
          and Its Application to Collaborative Filtering. In
          <em>Proceedings of The Web Conference 2018 (WWW
          2018).</em> ACM, New York, NY, USA, 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186159" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186159</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>The <em>recommender system</em> (RS), which recommends a
      set of items personalized to a target user that she/he is
      likely to prefer, has been extensively studied in the
      academic society as well as aggressively utilized by many
      companies such as Google, Netflix, eBay, and Amazon
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>][<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>][<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>][<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>].
      <em>Collaborative filtering</em> (CF) is one of the most
      successful and widely-used approaches in RS [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0033">33</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0029">29</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0014">14</a>]. In order to provide the
      top-<em>N</em> recommendations, CF relies on past user
      behaviors. For example, it learns the rating of a user on an
      item based on the rating given by other users similar to
      her/him in preferences.</p>
      <p>The majority of the entries in a user-item rating matrix
      <em>R</em> are <em>missing ratings</em>, because most users
      give ratings only on a small number of items [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0043">43</a>]. The sparsity of the
      rating matrix makes it difficult for CF to learn the ratings
      of a user on items, thereby contributing to the poor
      recommendation accuracy of CF. This <em>data sparsity
      problem</em> is one of the biggest problems faced by CF. To
      alleviate the data sparsity problem, a wide variety of
      approaches, such as acquiring additional information from a
      trust network [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] or crowdsourcing [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0042">42</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0008">8</a>] and performing data
      imputation [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>], have been extensively studied.</p>
      <p><em>Data imputation</em> aims to infer missing ratings and
      to impute them to the original rating matrix. This approach
      has been widely used, because it does not require additional
      data from other sources (e.g., trust networks and
      crowdsourcing). Since the imputed value is not a <em>real
      rating</em> but an <em>inferred rating</em>, however, there
      may exist an error. Inferring the imputed value of a missing
      rating accurately is the key to reducing such errors and
      eventually improving the accuracy of CF.</p>
      <p>Depending on the imputed values, existing data imputation
      approaches can be classified into two groups: one (called
      <em>group M</em>) that infers missing ratings <em>with
      multiple values</em> [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>] and the other (called <em>group
      S</em>) [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href=
      "#BibPLXBIB0043">43</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>] that infers missing ratings <em>with
      a single value</em>. Both groups have their limitations.
      Group M is able to capture the fact that a preference of user
      <em>u</em> on item <em>i</em> could be different, depending
      on the characteristics of <em>u</em> and <em>i</em>. However,
      it mostly assigns missing ratings with <em>high values</em>,
      which contradicts the observation that users tend to have low
      preferences on items with missing ratings [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0043">43</a>]. Group S could avoid the
      problem of group M, because it gives a low rating on most
      items with missing ratings, but fails to reflect the
      characteristics of users and items in imputed ratings.</p>
      <p>In this paper, we suggest three new claims to overcome the
      limitations of existing data imputation approaches and verify
      our claims through experiments with real-world datasets.</p>
      <ul class="list-no-style">
        <li id="list1" label="•">Claim 1 (C1): Most of the imputed
        values should be a <em>low rating</em>, because the items
        with missing ratings are mostly <em>uninteresting
        items.</em><br /></li>
        <li id="list2" label="•">Claim 2 (C2): The imputed values
        of interesting items should be <em>higher</em> than those
        of <em>uninteresting items.</em><br /></li>
        <li id="list3" label="•">Claim 3 (C3): The imputed values
        should be within the range that <em>users can give</em> to
        items.<br /></li>
      </ul>
      <p>Although the three claims are simple, they are <em>easily
      applicable</em> to any data imputation approaches and
      <em>very effective</em> in providing high recommendation
      accuracy. We show the validity of three claims via extensive
      experiments in Section 3. Furthermore, we propose a novel
      approach that satisfies all three claims. The key idea of
      this approach is to infer the post-use preference using the
      pre-use preference based on the hypothesis below.</p>
      <p><strong>Hypothesis:</strong> Most pre-use preferences on
      items lead to their post-use preferences.</p>
      <p>User <em>u</em>’s <em>pre-use preference</em> on item
      <em>i</em> (e.g., impression) is determined by <em>i</em>’s
      <em>external features</em> that <em>u</em> can obtain
      <em>without actually using</em> it, such as the genre or
      director of a movie. On the other hand, <em>u</em>’s
      <em>post-use preference</em> on <em>i</em> (e.g., rating) is
      determined by <em>i</em>’s <em>inherent features</em> that
      <em>u</em> does not know <em>before using it</em>, such as
      the storyline or choreography of a movie. After using the
      item, <em>u</em> expresses her/his post-use preference on
      <em>i through a rating</em>.</p>
      <p>Note that, the inherent and external features are <em>not
      independent</em>. Most inherent features are tightly related
      to external features. For example, the storyline or
      choreography of a movie is mostly under the influence of the
      director and the genre's characteristics. In this paper, we
      show the recommendation using <em>pre-use preferences
      only</em> achieves accuracy higher than classic
      recommendation approaches (e.g., itemKNN and SVD). In
      addition, it is possible to improve the recommendation
      accuracy further by considering the characteristics of the
      users and the items. Because the degree to which the pre-use
      preference leads to the post-use preference is different
      depending on the characteristics of the users and the
      items.</p>
      <p>Our imputation approach is as follows. First, it infers
      the pre-use preferences on all user-item pairs based on the
      original rating matrix <em>R</em>. Second, it extracts the
      features (i.e., characteristics) of the user and the item
      using the <em>variational autoencoder</em> (VAE). Third, it
      trains the model that infers the post-use preference based on
      the pre-use preference and the extracted features using the
      <em>multi-layer perceptron</em> (MLP). Finally, we infer the
      post-use preference for every missing rating (i.e.,
      <em>r</em> <sub><em>u</em>, <em>i</em></sub> = null) using
      the trained model, and impute the inferred post-use
      preferences to <em>R</em>. To show the effectiveness of the
      proposed approach, we perform experiments with three
      real-life datasets. The results demonstrate that our approach
      universally and consistently outperforms existing
      state-of-the-art approaches.</p>
      <p>In summary, the main contributions of this paper are
      listed as follows:</p>
      <ul class="list-no-style">
        <li id="list4" label="•">We identify the limitations of
        existing data imputation approaches.<br /></li>
        <li id="list5" label="•">We suggest three new claims that
        all data imputation approaches should follow to achieve
        high recommendation accuracy.<br /></li>
        <li id="list6" label="•">We propose a novel
        deep-learning-based approach that satisfies all three
        claims and relies on a hypothesis that most pre-use
        preferences on items lead to their post-use
        preferences.<br /></li>
        <li id="list7" label="•">Through extensive experiments on
        real-world datasets, we verify our claims and hypothesis,
        and also demonstrate that our proposed approach outperforms
        significantly existing state-of-the-art
        approaches.<br /></li>
      </ul>
      <p>The paper is organized as follows. Section 2 reviews
      existing approaches of data imputation and points out their
      limitations. Section 3 validates our claims via extensive
      experiments. Section 4 proposes our imputation approach,
      based on the variational autoencoder and the multi-layer
      perception, to an accurate inference of imputed values and
      explains its components in detail. Section 5 shows the
      superiority of our approach in comparison with existing
      state-of-the-art approaches via experimental evaluation.
      Finally, Section 6 summarizes and concludes the paper.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186159/images/www2018-168-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Distribution of existing ratings and
          imputed values in the three real-world datasets.</span>
        </div>
      </figure>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186159/images/www2018-168-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">The accuracy with different imputed values
          of interesting items in the three real-world
          datasets.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Limitation of
          Existing Approaches</h2>
        </div>
      </header>
      <p>Existing data imputation approaches can be categorized
      into two groups. The first group (group M), including AutAI
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>] and AdaM
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0027">27</a>], infers
      missing ratings as <em>multi-values</em>. After performing CF
      (e.g., userKNN, itemKNN) based on existing ratings, it
      determines missing ratings as the values predicted by CF. The
      intuition of group M is that <em>missing ratings would be
      similar to existing ratings.</em> On the other hand, the
      second group (group S), including Zero-injection [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>] and
      AllRank [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0043">43</a>],
      infers missing ratings as a <em>single-value</em>: 0 in
      Zero-injection and 2 in AllRank. The intuition of group S is
      that, if <em>r</em> <sub><em>u</em>, <em>i</em></sub> of user
      <em>u</em> on items <em>i</em> in <em>R</em> is missing, it
      <em>implicitly</em> represents <em>u</em>’s <em>negative
      preference</em> on <em>i</em>. Therefore, it imputes a single
      <em>low</em> rating to missing ratings.</p>
      <p>According to [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>], a user has a <em>small</em> number
      of <em>interesting items</em> and a very <em>large</em>
      number of <em>uninteresting items</em>. Interesting items are
      those items that a user would be interested in and expected
      to use eventually (i.e., user's pre-use preference on the
      item is high). Uninteresting items are such items that a user
      <em>would not be interested in</em> and thus not be expected
      to use (i.e., user's pre-use preference on the item is low).
      Due to time and cost constraints, a user experiences only a
      few interesting items; most items are left as uninteresting
      items.</p>
      <p>Keeping the above characteristics of missing ratings in
      mind, we analyze the limitations of existing data imputation
      approaches. Despite being <em>mostly uninteresting
      items</em>, most of missing ratings are inferred as <em>high
      ratings</em> by group M, because existing ratings in
      <em>R</em> are mostly high ratings. Compared to group M,
      group S has a high accuracy of recommendation because it
      focuses on a large number of uninteresting items: it regards
      <em>all (or most)</em> items having missing ratings as
      uninteresting items. However, the approaches of group S have
      two limitations. First, it ignores a small number of
      interesting items. <em>Regardless of whether item i is an
      interesting item or uninteresting item</em>, missing rating
      <em>r</em> <sub><em>u</em>, <em>i</em></sub> in <em>R</em> is
      inferred as the <em>same</em> single-value [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0043">43</a>][<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0003">3</a>]. Second, Zero-injection
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>] and
      pureSVD [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>]
      in group S impute missing ratings as 0. Note that the meaning
      of the imputed value is the rating that a user is likely to
      give to an item. However, 0 is even lower than 1, the lowest
      rating that the user can give. Therefore, imputing 0
      <em>exaggerates</em> users’ negative preference.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Validation of
          Our Claims</h2>
        </div>
      </header>
      <p>In this section, we show the limitations of existing data
      imputation approaches and verify the validity of our claims
      suggested in Section 1 via a series of experiments.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span>
            Experimental Setup</h3>
          </div>
        </header>
        <p>We use three real-world datasets, whose statistics are
        reported in Table 1. Because of clear differences in
        sparsity, the number of ratings, and rating distribution,
        the datasets are effective for cross-checking the
        validation of our experiments. The MovieLens is a rating
        dataset collected and made available from the MovieLens web
        site<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a> by the GroupLens Research. The
        CiaoDVD is a crawled dataset in 2013 from the DVD
        category<a class="fn" href="#fn2" id=
        "foot-fn2"><sup>2</sup></a> of Ciao that is the leading
        platform to get outstanding reviews from real consumers on
        millions of products. The Watcha is a dataset privately
        released in 2014 by an online movie-streaming
        company<a class="fn" href="#fn3" id=
        "foot-fn3"><sup>3</sup></a> similar to Netflix. The ratings
        of the three real-world datasets are all in integer values
        from 1 (worst) to 5 (best). In our experiments, if the
        dataset is not explicitly mentioned, the MovieLens dataset
        is used by default, which has been widely used for
        evaluating CF algorithms.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Statistics of three real-world
            datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">MovieLens</th>
                <th style="text-align:center;">CiaoDVD</th>
                <th style="text-align:center;">Watcha</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Sparsity</td>
                <td style="text-align:center;">95<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
                <td style="text-align:center;">99<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
                <td style="text-align:center;">97<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
              </tr>
              <tr>
                <td style="text-align:center;"># of users</td>
                <td style="text-align:center;">943</td>
                <td style="text-align:center;">996</td>
                <td style="text-align:center;">1,391</td>
              </tr>
              <tr>
                <td style="text-align:center;"># of items</td>
                <td style="text-align:center;">1,682</td>
                <td style="text-align:center;">1,359</td>
                <td style="text-align:center;">1,927</td>
              </tr>
              <tr>
                <td style="text-align:center;"># of ratings</td>
                <td style="text-align:center;">100,000</td>
                <td style="text-align:center;">18,640</td>
                <td style="text-align:center;">101,073</td>
              </tr>
              <tr>
                <td style="text-align:center;">Avg. of all
                ratings</td>
                <td style="text-align:center;">
                <strong>3.56</strong></td>
                <td style="text-align:center;">
                <strong>4.03</strong></td>
                <td style="text-align:center;">
                <strong>3.68</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">Avg. # of ratings
                per user</td>
                <td style="text-align:center;">106.04</td>
                <td style="text-align:center;">18.72</td>
                <td style="text-align:center;">72.66</td>
              </tr>
              <tr>
                <td style="text-align:center;">Min. # of ratings
                per user</td>
                <td style="text-align:center;">20</td>
                <td style="text-align:center;">1</td>
                <td style="text-align:center;">16</td>
              </tr>
              <tr>
                <td style="text-align:center;">Max. # of ratings
                per user</td>
                <td style="text-align:center;">737</td>
                <td style="text-align:center;">319</td>
                <td style="text-align:center;">513</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>For the top-<em>N</em> recommendation, we vary the value
        of <em>N</em> (<em>N</em> = 5, 10, and 20) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0009">9</a>]. Three metrics are used
        to measure the accuracy of the top-<em>N</em>
        recommendation: precision, recall, and normalized
        Discounted Cumulative Gain (nDCG) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0010">10</a>]. For a user <em>u</em>,
        precision <em>P<sub>u</sub></em> @<em>N</em> and recall
        <em>R<sub>u</sub></em> @<em>N</em> are computed by
        <span class="inline-equation"><span class=
        "tex">$\frac{\vert Rel_{u} \cap Rec_{u} \vert }{\vert
        Rec_{u} \vert }$</span></span> and <span class=
        "inline-equation"><span class="tex">$\frac{\vert Rel_{u}
        \cap Rec_{u} \vert }{\vert Rel_{u} \vert }$</span></span> ,
        respectively, where <em>Rec<sub>u</sub></em> denotes a set
        of <em>N</em> items recommended by CF to <em>u</em>, and
        <em>Rel<sub>u</sub></em> denotes a set of items considered
        relevant to <em>u</em>. We consider the items with rating 5
        (best) as relevant items, (i.e., ground truth), following
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0003">3</a>]. nDCG is
        used to reflect <em>ranked positions</em> of items in
        <em>Rec<sub>u</sub></em> . Let <em>y<sub>k</sub></em>
        represent a binary variable for the <em>k</em>-th item
        <em>i<sub>k</sub></em> in <em>Rec<sub>u</sub></em> . If
        <em>i<sub>k</sub></em> ∈ <em>Rel<sub>u</sub></em> ,
        <em>y<sub>k</sub></em> is set as one; otherwise,
        <em>y<sub>k</sub></em> is set as zero. Then,
        <em>nDCG<sub>u</sub></em> @<em>N</em> is computed by
        <span class="inline-equation"><span class=
        "tex">$\frac{DCG_{u}@N}{IDCG_{u}@N}$</span></span> , where
        <span class="inline-equation"><span class="tex">$DCG_{u}@N=
        \sum _{k=1}^{N}\frac{2^{y_{k}}-1}{\log
        _{2}(k+1)}$</span></span> , and <em>IDCG<sub>u</sub></em>
        @<em>N</em> means an ideal <em>DCG<sub>u</sub></em>
        @<em>N</em> where <em>y<sub>k</sub></em> is set as one for
        every item <em>i<sub>k</sub></em> ∈
        <em>Rec<sub>u</sub></em> . Finally, <em>P</em>@<em>N</em>,
        <em>R</em>@<em>N</em>, and <em>nDCG</em>@<em>N</em> are the
        averages of <em>P<sub>u</sub></em> @<em>N</em>,
        <em>R<sub>u</sub></em> @<em>N</em> and
        <em>nDCG<sub>u</sub></em> @<em>N</em> for <em>all
        users</em>, respectively. In our experiments, if the metric
        is not explicitly mentioned, <em>P</em>@5 is used because
        other metrics and other <em>N</em> showed very similar
        tendencies. For all experimental results, we have performed
        five cross-validations.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Validating
            C1</h3>
          </div>
        </header>
        <p>C1 is based on the limitation of group M. Despite being
        mostly uninteresting items, most of missing ratings are
        <em>inferred high</em> by group M, because existing ratings
        in rating matrix <em>R</em> are mostly high [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0043">43</a>][<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0021">21</a>]. Therefore, we verify
        the following: (1) <em>existing ratings and imputed values
        inferred by CF are high ratings</em>, and (2) <em>most
        imputed values should be low ratings</em>.</p>
        <p>To verify (1), Figure 1 shows the distribution of
        existing ratings and imputed values inferred by CF<a class=
        "fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>. As shown,
        both existing ratings and imputed values are <em>mostly
        higher</em> than or equal to 3. To verify (2), we show that
        the accuracy of recommendations is improved when imputed
        values are low ratings. First, we impute the <em>same</em>
        value (e.g., 1 to 5) to rating matrix <em>R</em> and then
        recommend top-<em>N</em> items for each user by CF. To
        demonstrate the validity of C1 independently of CF methods,
        we use the most widely used itemKNN and SVD in CF. In
        Figure 2, the accuracy of recommendations is high only when
        imputed values are <em>low ratings</em> (i.e., 1 and 2) in
        both itemKNN and SVD. Also, the accuracy of group S that
        imputes low values is <em>much higher</em> than that of
        group M that imputes high values as in Table 2. We use AdaM
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0027">27</a>] and
        Zero-injection [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>] for comparisons because they show
        the highest accuracy in each group. Through these three
        experiments, we identify the limitation of group M and also
        validate C1.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Comparison of group M and group S
            (MovieLens).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metrics</th>
                <th style="text-align:center;">AdaM (group M)</th>
                <th>Zero-injection (group S)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.051</td>
                <td><strong>0.207</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.046</td>
                <td><strong>0.166</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.042</td>
                <td><strong>0.125</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.043</td>
                <td><strong>0.218</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.076</td>
                <td><strong>0.325</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.132</td>
                <td><strong>0.450</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.058</td>
                <td><strong>0.274</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.066</td>
                <td><strong>0.297</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.085</td>
                <td><strong>0.332</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Validating
            C2</h3>
          </div>
        </header>
        <p>C2 is based on the first limitation of group S. Group S
        ignores a small number of interesting items. Whether item
        <em>i</em> is an interesting item or an uninteresting item
        to user <em>u</em>, all the missing ratings <em>r</em>
        <sub><em>u</em>, <em>i</em></sub> are imputed by a
        <em>single-value</em>. For example, pureSVD [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0003">3</a>] and AllRank [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0043">43</a>] impute 0
        and 2 to rating matrix <em>R</em> regardless of interesting
        and uninteresting items, respectively. C2 asserts that,
        although the number of interesting items is small, the
        accuracy of recommendations is improved when it is
        considered. To verify C2, we show that the accuracy of
        AllRank and pureSVD is improved when C2 is applied to them.
        More specifically, we let AllRank and pureSVD impute higher
        ratings to <em>interesting items</em> than uninteresting
        items while having the rest of their schemes remain the
        same. We define interesting items as the top 10% of missing
        ratings according to the pre-use preferences [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0021">21</a>].</p>
        <p>In Table 3, <em>I<sub>un</sub></em> indicates the
        imputed values for uninteresting items. Thus,
        <em>I<sub>un</sub></em> +0.5 is 2.5 in AllRank and 0.5 in
        pureSVD; <em>I<sub>un</sub></em> +1.0 and
        <em>I<sub>un</sub></em> +0.5 (resp. <em>I<sub>un</sub></em>
        -1.0 and <em>I<sub>un</sub></em> -0.5) mean that the
        imputed values of interesting items are higher (resp.
        lower) than those of uninteresting items;
        <em>I<sub>un</sub></em> +0 means that the imputed values of
        interesting items and those of uninteresting items are
        same, that is, original AllRank and pureSVD. Table 3
        reports that the accuracy of AllRank and pureSVD is
        improved when they impute <em>higher</em> values to
        interesting items than those to uninteresting items and is
        decreased when they impute lower values to interesting
        items than those to uninteresting items in terms of all
        metrics. The experimental result identifies the first
        limitation of group S and validates C2.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">The accuracy with respect to different
            imputed values of interesting items (MovieLens).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metrics</th>
                <th colspan="5" style="text-align:left;">
                  AllRank (<em>I<sub>un</sub></em> = 2)
                  <hr />
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">(l)2-6</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> -1.0</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> -0.5</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> +0</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> +0.5</td>
                <td><em>I<sub>un</sub></em> +1.0</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.081</td>
                <td style="text-align:center;">0.128</td>
                <td style="text-align:center;">0.184</td>
                <td style="text-align:center;">
                <strong>0.207</strong></td>
                <td><strong>0.207</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.063</td>
                <td style="text-align:center;">0.100</td>
                <td style="text-align:center;">0.147</td>
                <td style="text-align:center;">
                <strong>0.164</strong></td>
                <td><strong>0.164</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.047</td>
                <td style="text-align:center;">0.075</td>
                <td style="text-align:center;">0.108</td>
                <td style="text-align:center;">
                <strong>0.122</strong></td>
                <td><strong>0.123</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.067</td>
                <td style="text-align:center;">0.119</td>
                <td style="text-align:center;">0.195</td>
                <td style="text-align:center;">
                <strong>0.216</strong></td>
                <td><strong>0.219</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.099</td>
                <td style="text-align:center;">0.174</td>
                <td style="text-align:center;">0.287</td>
                <td style="text-align:center;">
                <strong>0.322</strong></td>
                <td><strong>0.324</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.141</td>
                <td style="text-align:center;">0.241</td>
                <td style="text-align:center;">0.397</td>
                <td style="text-align:center;">
                <strong>0.446</strong></td>
                <td><strong>0.446</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.103</td>
                <td style="text-align:center;">0.170</td>
                <td style="text-align:center;">0.247</td>
                <td style="text-align:center;">
                <strong>0.278</strong></td>
                <td><strong>0.281</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.104</td>
                <td style="text-align:center;">0.175</td>
                <td style="text-align:center;">0.264</td>
                <td style="text-align:center;">
                <strong>0.297</strong></td>
                <td><strong>0.300</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.114</td>
                <td style="text-align:center;">0.192</td>
                <td style="text-align:center;">0.294</td>
                <td style="text-align:center;">
                <strong>0.330</strong></td>
                <td><strong>0.334</strong></td>
              </tr>
            </tbody>
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="5" style="text-align:left;">
                  pureSVD (<em>I<sub>un</sub></em> = 0)
                  <hr />
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">(l)2-6</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> -1.0</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> -0.5</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> +0</td>
                <td style="text-align:center;">
                <em>I<sub>un</sub></em> +0.5</td>
                <td><em>I<sub>un</sub></em> +1.0</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.113</td>
                <td style="text-align:center;">0.146</td>
                <td style="text-align:center;">0.161</td>
                <td style="text-align:center;">
                <strong>0.173</strong></td>
                <td><strong>0.181</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.087</td>
                <td style="text-align:center;">0.115</td>
                <td style="text-align:center;">0.128</td>
                <td style="text-align:center;">
                <strong>0.138</strong></td>
                <td><strong>0.147</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.066</td>
                <td style="text-align:center;">0.087</td>
                <td style="text-align:center;">0.099</td>
                <td style="text-align:center;">
                <strong>0.106</strong></td>
                <td><strong>0.112</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.102</td>
                <td style="text-align:center;">0.142</td>
                <td style="text-align:center;">0.163</td>
                <td style="text-align:center;">
                <strong>0.178</strong></td>
                <td><strong>0.189</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.150</td>
                <td style="text-align:center;">0.215</td>
                <td style="text-align:center;">0.246</td>
                <td style="text-align:center;">
                <strong>0.268</strong></td>
                <td><strong>0.289</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.212</td>
                <td style="text-align:center;">0.302</td>
                <td style="text-align:center;">0.360</td>
                <td style="text-align:center;">
                <strong>0.388</strong></td>
                <td><strong>0.415</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.147</td>
                <td style="text-align:center;">0.190</td>
                <td style="text-align:center;">0.214</td>
                <td style="text-align:center;">
                <strong>0.230</strong></td>
                <td><strong>0.242</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.151</td>
                <td style="text-align:center;">0.202</td>
                <td style="text-align:center;">0.229</td>
                <td style="text-align:center;">
                <strong>0.246</strong></td>
                <td><strong>0.263</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.166</td>
                <td style="text-align:center;">0.225</td>
                <td style="text-align:center;">0.260</td>
                <td style="text-align:center;">
                <strong>0.280</strong></td>
                <td><strong>0.298</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Validating
            C3</h3>
          </div>
        </header>
        <p>C3 is based on the second limitation of group S.
        Zero-injection in group S infers missing ratings as 0
        without considering the meaning of imputed values. Note
        that the meaning of an imputed value is the rating that a
        user is likely to give to an item. However, 0 is lower than
        1, which is the lowest rating that users can give.
        Therefore, imputing 0 <em>exaggerates</em> users’ negative
        preference. C3 asserts that the imputed values should be
        within the range that users can give to items in reality.
        To verify C3, we let the approaches which impute 0 (i.e.,
        Zero-injection and pureSVD) impute values within the
        <em>rating range</em> (i.e., from 1 to 5), rather than 0
        while letting the rest of their approaches remain the
        same.</p>
        <p>Table 4 shows the accuracy with respect to different
        imputed values ranging from 1 to 5. Both Zero-injection and
        pureSVD show higher accuracy when imputed values are 1, 2,
        and 3 rather than 0. This result identifies that imputing 0
        <em>exaggerates</em> users’ negative preference too much
        and thus validates C3. In addition, the accuracy drops
        <em>drastically</em> when the imputed values are 4 and 5,
        which confirms again that <em>most of the missing ratings
        are uninteresting items.</em></p>
        <p>The validation of our claims in this section clearly
        points out the limitations of existing imputation
        approaches and demonstrates the possibility of improving
        the accuracy of recommendation when our <em>three
        claims</em> are satisfied.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">The accuracy with respect to different
            imputed values ranging from 0 to 5 (MovieLens).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metrics</th>
                <th colspan="6" style="text-align:left;">
                  Zero-injection
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">(l)2-7</th>
                <th style="text-align:center;">0</th>
                <th style="text-align:center;">1</th>
                <th style="text-align:center;">2</th>
                <th style="text-align:center;">3</th>
                <th style="text-align:center;">4</th>
                <th>5</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.207</td>
                <td style="text-align:center;">0.224</td>
                <td style="text-align:center;">
                <strong>0.234</strong></td>
                <td style="text-align:center;">0.224</td>
                <td style="text-align:center;">0.159</td>
                <td>0.031</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.166</td>
                <td style="text-align:center;">0.175</td>
                <td style="text-align:center;">
                <strong>0.182</strong></td>
                <td style="text-align:center;">0.176</td>
                <td style="text-align:center;">0.127</td>
                <td>0.03</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.125</td>
                <td style="text-align:center;">0.130</td>
                <td style="text-align:center;">
                <strong>0.134</strong></td>
                <td style="text-align:center;">0.131</td>
                <td style="text-align:center;">0.096</td>
                <td>0.03</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.218</td>
                <td style="text-align:center;">0.235</td>
                <td style="text-align:center;">
                <strong>0.244</strong></td>
                <td style="text-align:center;">0.230</td>
                <td style="text-align:center;">0.148</td>
                <td>0.029</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.325</td>
                <td style="text-align:center;">0.337</td>
                <td style="text-align:center;">
                <strong>0.347</strong></td>
                <td style="text-align:center;">0.330</td>
                <td style="text-align:center;">0.224</td>
                <td>0.058</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.450</td>
                <td style="text-align:center;">0.464</td>
                <td style="text-align:center;">
                <strong>0.468</strong></td>
                <td style="text-align:center;">0.453</td>
                <td style="text-align:center;">0.323</td>
                <td>0.124</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.274</td>
                <td style="text-align:center;">0.303</td>
                <td style="text-align:center;">
                <strong>0.316</strong></td>
                <td style="text-align:center;">0.304</td>
                <td style="text-align:center;">0.209</td>
                <td>0.037</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.297</td>
                <td style="text-align:center;">0.319</td>
                <td style="text-align:center;">
                <strong>0.331</strong></td>
                <td style="text-align:center;">0.319</td>
                <td style="text-align:center;">0.219</td>
                <td>0.046</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.332</td>
                <td style="text-align:center;">0.353</td>
                <td style="text-align:center;">
                <strong>0.362</strong></td>
                <td style="text-align:center;">0.350</td>
                <td style="text-align:center;">0.244</td>
                <td>0.067</td>
              </tr>
            </tbody>
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="5" style="text-align:left;">
                  pureSVD
                  <hr />
                </th>
                <th></th>
              </tr>
              <tr>
                <th style="text-align:center;">(l)2-7</th>
                <th style="text-align:center;">0</th>
                <th style="text-align:center;">1</th>
                <th style="text-align:center;">2</th>
                <th style="text-align:center;">3</th>
                <th style="text-align:center;">4</th>
                <th>5</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.161</td>
                <td style="text-align:center;">0.167</td>
                <td style="text-align:center;">0.184</td>
                <td style="text-align:center;">
                <strong>0.194</strong></td>
                <td style="text-align:center;">0.105</td>
                <td>0.005</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.128</td>
                <td style="text-align:center;">0.136</td>
                <td style="text-align:center;">0.147</td>
                <td style="text-align:center;">
                <strong>0.149</strong></td>
                <td style="text-align:center;">0.08</td>
                <td>0.004</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.099</td>
                <td style="text-align:center;">0.103</td>
                <td style="text-align:center;">0.108</td>
                <td style="text-align:center;">
                <strong>0.111</strong></td>
                <td style="text-align:center;">0.06</td>
                <td>0.003</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.163</td>
                <td style="text-align:center;">0.176</td>
                <td style="text-align:center;">0.195</td>
                <td style="text-align:center;">
                <strong>0.197</strong></td>
                <td style="text-align:center;">0.076</td>
                <td>0.002</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.246</td>
                <td style="text-align:center;">0.264</td>
                <td style="text-align:center;">0.287</td>
                <td style="text-align:center;">
                <strong>0.276</strong></td>
                <td style="text-align:center;">0.113</td>
                <td>0.003</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.360</td>
                <td style="text-align:center;">0.379</td>
                <td style="text-align:center;">0.397</td>
                <td style="text-align:center;">
                <strong>0.383</strong></td>
                <td style="text-align:center;">0.161</td>
                <td>0.006</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.214</td>
                <td style="text-align:center;">0.224</td>
                <td style="text-align:center;">0.247</td>
                <td style="text-align:center;">
                <strong>0.259</strong></td>
                <td style="text-align:center;">0.13</td>
                <td>0.005</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.229</td>
                <td style="text-align:center;">0.242</td>
                <td style="text-align:center;">0.264</td>
                <td style="text-align:center;">
                <strong>0.268</strong></td>
                <td style="text-align:center;">0.128</td>
                <td>0.005</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.260</td>
                <td style="text-align:center;">0.274</td>
                <td style="text-align:center;">0.294</td>
                <td style="text-align:center;">
                <strong>0.295</strong></td>
                <td style="text-align:center;">0.137</td>
                <td>0.005</td>
              </tr>
            </tbody>
          </table>
        </div>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186159/images/www2018-168-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Overview of our
            approach.</span>
          </div>
        </figure>
      </section>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Proposed
          Imputation Approach</h2>
        </div>
      </header>
      <p>In this section, we present our imputation approach in
      detail: the challenges (Section 4.1); the overview of the
      proposed approach (Section 4.2); how to infer the pre-use
      preference of a user on an item (Section 4.3); how to extract
      the features of a pair of a user and an item (Section 4.4);
      finally, how to infer the post-use preference of a user on an
      item for imputation (Section 4.5).</p>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Challenges</h3>
          </div>
        </header>
        <p>How do we determine post-use preferences from pre-use
        preferences? A straightforward answer would be simple: to
        use pre-use preferences as post-use preferences. This,
        however, would not work for two reasons. First, their
        scales are different. Pre-use preferences are ranged from 0
        to 1 [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>], while post-use preferences are
        from 1 to 5. Second, even if the scales are converted,
        post-use preferences should be based not only on pre-use
        preferences but also on the characteristics of a target
        user and a target item. Although most pre-use preferences
        on items lead to their post-use preferences, the degree of
        correlation depends on the user and the item.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Overview</h3>
          </div>
        </header>
        <p>Our approach consists of the following four steps.
        First, we infer the pre-use preferences on all user-item
        pairs (Figure 3(a)). Note that, even if a user <em>u</em>’s
        pre-use preference on an item <em>i</em> and a user
        <em>v</em>’s pre-use preference on an item <em>j</em> is
        equal, <em>u</em>’s post-use preference on <em>i</em> and
        <em>v</em>’s post-use preference on <em>j</em> could be
        different, depending on the characteristics of the users
        (<em>u</em> and <em>v</em>) and the items (<em>i</em> and
        <em>j</em>). To address this issue, we extract the features
        (i.e., characteristics) of the user and the item (Figure
        3(b)). Next, we train the model which infers the post-use
        preference using those features and the pre-use preference
        together. Specifically, the model is trained with the
        existing ratings in <em>R</em> and infers <em>u</em>’s
        post-use preference on <em>i</em>, given <em>u</em>’s
        pre-use preference on <em>i</em> and features of <em>u</em>
        and <em>i</em> (Figure 3(c)). Finally, we infer the
        post-use preference for every missing rating (i.e.,
        <em>r</em> <sub><em>u</em>, <em>i</em></sub> = null) using
        the trained model and impute the inferred post-use
        preferences to <em>R</em> to solve the data sparsity
        problem.</p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Inference
            of Pre-use Preferences</h3>
          </div>
        </header>
        <p>Inferring the pre-use preference of a user on an
        <em>item with an existing rating</em> is straightforward.
        <em>Intersting item</em> <sub><em>u</em>, <em>i</em></sub>
        is the item <em>i</em> that a user <em>u</em> would be
        interested in and expected to experience. Therefore,
        <em>u</em>’s pre-use preference on <em>i</em> with an
        <em>existing rating</em> should be high. On the other hand,
        inferring <em>u</em>’s pre-use preference of <em>i with a
        missing rating</em> is non-trivial, because missing ratings
        encompass both interesting and uninteresting items.</p>
        <p>This is the <em>One-Class Collaborative Filtering (OCCF)
        problem</em> [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0039">39</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0040">40</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0044">44</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0023">23</a>]. The OCCF problem occurs when the
        rating is a single, usually positive feedback, such as
        purchases and bookmarks. A cell in the feedback matrix is
        either missing a rating (i.e., null) or indicating
        interesting (i.e., 1). Therefore, ambiguity arises in the
        interpretation of missing ratings. A missing rating may
        mean <em>positive feedback</em>; the user was not aware of
        the existence of the item but would have liked it if she/he
        had known. If a missing rating means <em>negative
        feedback</em>, the user knew the existence of the item but
        did not like it [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0038">38</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0028">28</a>].</p>
        <p>To infer pre-use preferences (Figure 3(a)) on those
        items with missing ratings, we first convert a rating
        matrix <em>R</em> to a binary matrix <em>B</em> [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href=
        "#BibPLXBIB0031">31</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>]. In matrix <em>B</em>, <em>b</em>
        <sub><em>u</em>, <em>i</em></sub> is 1 if <em>r</em>
        <sub><em>u</em>, <em>i</em></sub> exists (i.e.,
        <em>intersting item</em> <sub><em>u</em>, <em>i</em></sub>
        ); <em>b</em> <sub><em>u</em>, <em>i</em></sub> is 0 if
        <em>r</em> <sub><em>u</em>, <em>i</em></sub> is missing
        (i.e., <em>unintersting</em>  <em>item</em>
        <sub><em>u</em>, <em>i</em></sub> ). Since not all missing
        ratings indicate uninteresting items, however, we employ
        <em>weighted Alternative Least Squares</em> (wALS)
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0037">37</a>]. Given
        matrix <em>B</em>, wALS aims at approximating <em>B</em>
        with a pre-use preference matrix <em>C</em>, where
        <em>c</em> <sub><em>u</em>, <em>i</em></sub> ∈ [0,1]
        indicates <em>u</em>’s pre-use preference on <em>i</em>,
        with minimizing the loss function in Eq. (1). The closer to
        1, the higher the pre-use preference is.</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {L}(C) = \sum _{u}\sum
            _{i}w_{u,i}(b_{u,i}-c_{u,i})^{2},
            \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>where <em>w</em> <sub><em>u</em>, <em>i</em></sub> ∈
        [0,1] indicates the weight of <em>b</em> <sub><em>u</em>,
        <em>i</em></sub> . The weight matrix <em>W</em> assignment
        is based on one of the most successful and widely-used
        approaches [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>]. If <em>b</em> <sub><em>u</em>,
        <em>i</em></sub> is 1 (i.e., observed rating), <em>w</em>
        <sub><em>u</em>, <em>i</em></sub> is assigned with the
        highest weight, 1. If <em>b</em> <sub><em>u</em>,
        <em>i</em></sub> is 0 (i.e., missing rating), <em>w</em>
        <sub><em>u</em>, <em>i</em></sub> is proportional to the
        number of ratings given by <em>u</em>. In [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0012">12</a>], it is demonstrated, as
        a user has a more number of positive feedbacks, it is more
        likely that she/he does not like the other (i.e., missing)
        items. Therefore, the missing rating of such a user is more
        likely to indicate a negative feedback.
        <p></p>
        <p>Since matrix <em>C</em> is a matrix multiplication of a
        matrix <em>X</em> ∈ <span class=
        "inline-equation"><span class="tex">$\mathbb {R}^{m\times
        d}$</span></span> and a matrix <em>Y</em> ∈ <span class=
        "inline-equation"><span class="tex">$\mathbb {R}^{n\times
        d}$</span></span> , we can re-write the loss function in
        Eq. (1) as Eq. (2). <em>d</em> is a low rank of matrix
        <em>B</em>, and <em>m</em> and <em>n</em> are the numbers
        of users and items, respectively [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0034">34</a>][<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0016">16</a>][<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0041">41</a>].</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{multline} \mathcal
            {L}(X,Y) = \sum _{u} \sum _{i} w_{u,i} \lbrace
            (b_{u.i}-X_{u(\cdot)}Y_{i(\cdot)}^{T})^{2} + \\ \lambda
            _{1}(\Vert X_{u(\cdot)} \Vert _{F}^{2} + \Vert
            Y_{i(\cdot)} \Vert _{F}^{2}) \rbrace
            ,\end{multline}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where ‖ · ‖ <sub><em>F</em></sub> denotes the
        Frobenius norm and <em>λ</em> <sub>1</sub> is the
        regularization parameter (<em>λ</em> <sub>1</sub> = 0.015,
        following [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>]). In order to minimize the loss
        function, wALS first initializes matrix <em>X</em> with
        random values (step 1). Then, while fixing <em>X</em>, it
        computes <em>Y</em> using Eq. (3) (step 2). After solving
        <em>Y</em>, it computes <em>X</em> while fixing <em>Y</em>,
        using Eq. (4) (step 3). wALS repeats steps 2 and 3 until
        convergence. After the convergence, each <em>c</em>
        <sub><em>u</em>, <em>i</em></sub> in <em>C</em> has user
        <em>u</em>’s pre-use preference inferred on item
        <em>i</em>.
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{multline} X_{u(\cdot)} =
            B_{u(\cdot)}\widetilde{W}_{u(\cdot)}Y \lbrace
            Y^{T}\widetilde{W}_{u(\cdot)}Y +\lambda _{1}(\sum
            _{i}w_{u,i})I\rbrace ^{-1}, \\ \forall 1 \le u \le
            m,\end{multline}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{multline} Y_{i(\cdot)} =
            B_{(\cdot)i}^T\widetilde{W}_{(\cdot)i}X \lbrace
            X^{T}\widetilde{W}_{(\cdot)i}X +\lambda _{1}(\sum
            _{u}w_{u,i})I\rbrace ^{-1}, \\ \forall 1 \le i \le
            n,\end{multline}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\widetilde{W}_{u(\cdot)}$</span></span> is a
        diagonal matrix with elements of <em>W</em>
        <sub><em>u</em>(·)</sub> on the diagonal, and matrix
        <em>I</em> is a <em>d</em> × <em>d</em> identity matrix.
        <p></p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Extracting
            User and Item Features</h3>
          </div>
        </header>
        <p>We use the <em>variational autoencoder</em> (VAE)
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0032">32</a>], one of
        the most popular approaches in the generative model, to
        extract the features of user <em>u</em> and the item
        <em>i</em> (Figure 3(b)) [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>][<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>][<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0035">35</a>]. VAE allows learning from a large
        dataset compared to other generative model approaches
        (e.g., Markov chain Monte Carlo sampling) and shows better
        reconstructive performance. Being unsupervised learning,
        VAE does not require labeled data (i.e., ratings) during
        training and thus is able to utilize all user-item pairs in
        <em>R</em> [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>]. The goal of VAE is to maximize the
        probability distribution of data <em>x</em> in Eq. (5).</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} P(x;\theta) =
            \int P(x|{\it z};\theta)P({\it z}){\it dz}.
            \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p></p>
        <p>In our case, <em>x</em> is a pair of user and item
        vectors. User vectors and item vectors are rows and
        columns, respectively, of rating matrix <em>R</em>, where
        missing ratings are treated as 0s. <em>z</em> is the latent
        feature of a user-item pair and is used as an input when
        inferring the imputed value. High <em>P</em>(<em>x</em>)
        indicates <em>z</em> represents data <em>x</em> better.
        Therefore, maximizing <em>P</em>(<em>x</em>) is important
        in improving the accuracy of our approach.</p>
        <p>However, the integral is intractable in Eq. (5), as it
        needs to be evaluated over all of <em>z</em>. To solve this
        problem, VAE approximates the posterior distribution using
        variational inference. As a result, Eq. (5) can be
        rewritten as Eq. (6). Eq. (6) is a loss function of VAE and
        can be rewritten as Eqs. (7) and (8), since the KL
        divergence is non-negative.</p>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \log P(x) =
            \mathit {D}_{\mathit {KL}}(Q(\mathit
            {z}|x;\phi)||P(\mathit {z}|x;\theta)) + \mathcal
            {L}(\theta ,\phi ;x) \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{multline} \log P(x) \ge
            \mathcal {L}(\theta ,\phi ;x) \\ = \mathbb
            {E}_{Q(\mathit {z}|x;\phi)}[-\log Q(\mathit {z}|x;\phi)
            + \log P(x,{\it z};\theta)]\end{multline}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{multline} \mathcal
            {L}(\theta ,\phi ;x) = -\mathit {D}_{\mathit
            {KL}}(Q(\mathit {z}|x;\phi)||P(\mathit {z};\theta)) +
            \\ \mathbb {E}_{Q(\mathit {z}|x;\phi)}[\log P(x|\mathit
            {z};\theta)]\end{multline}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>
        <p></p>
        <p>Given <em>x</em> corresponding to a pair of <em>u</em>
        and <em>i</em>, our approach samples <em>z</em>, which
        minimizes Eq. (8), for <em>x</em> by using
        <em>P</em>(<em>z</em>|<em>x</em>; <em>θ</em>) learned by
        VAE. As shown in Figure 3(b), the sampled <em>z</em>,
        together with <em>u</em>’s pre-use preference on <em>i</em>
        is used when inferring <em>u</em>’s post-use preference on
        <em>i</em>.</p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Inference
            of Post-use Preferences</h3>
          </div>
        </header>
        <p>To infer <em>u</em>’s <em>post-use preference</em>
        (Figure 3(c)), we use the <em>features</em> of user
        <em>u</em> and item <em>i</em> extracted by VAE and
        <em>u</em>’s <em>pre-use preference</em> on <em>i</em>. Our
        approach is to train the model to predict how <em>u</em>’s
        pre-use preference on <em>i</em> leads to <em>u</em>’s
        post-use preference on <em>i</em> according to the features
        of <em>u</em> and <em>i</em>. For learning, we use the
        multi-layer perceptron (MLP) that uses multiple layers and
        non-linear functions. Since it is supervised learning, MLP
        requires labeled data (i.e., post-use preferences or
        ratings). In our case, MLP uses existing ratings in
        <em>R</em> for training. The model parameters ϕ is trained
        by minimizing the loss function in Eq (9).</p>
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \mathcal
            {L}(\varphi) = \sum _{u}\sum
            _{i}(r_{u,i}-\hat{r}_{u,i})^{2},
            \end{equation}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <p>where <em>r</em> <sub><em>u</em>, <em>i</em></sub> is
        the existing rating and <span class=
        "inline-equation"><span class=
        "tex">$\hat{r}_{u,i}$</span></span> is the value predicted
        by the model. If <span class="inline-equation"><span class=
        "tex">$\hat{r}_{u,i}$</span></span> is less than 1 and
        larger than 5 (rarely occurring cases), it is truncated to
        1 and 5, respectively, following C3. After training the
        model, we predict the post-use preference for every missing
        rating (i.e., <em>r</em> <sub><em>u</em>, <em>i</em></sub>
        = null) and impute the predicted values to <em>R</em>.</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.6</span>
            Discussions</h3>
          </div>
        </header>
        <p>Our approach is <em>CF-agnostic</em>, that is,
        applicable to any CF methods in the recommendation, because
        it requires the imputation to the rating matrix only.
        Furthermore, the recommendation accuracy increases as it
        satisfies our three claims. Figure 4 reports the
        distribution of imputed values by our approach, indicating
        that most imputed values are low ratings (C1). Figure 5
        shows pre-use preferences are strongly (positively)
        correlated to post-use preferences. Because our approach
        infers post-use preferences by referring to pre-use
        preferences, the imputed values (i.e., inferred post-use
        preferences) of interesting items are mostly higher than
        those of uninteresting items (C2). Imputed values are in
        the range of ratings that any user actually can give to
        items (C3).</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186159/images/www2018-168-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Distribution of imputed
            values inferred by our approach (MovieLens).</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span>
          Experiments</h2>
        </div>
      </header>
      <p>In this section, we verify our hypothesis and then
      evaluate the effectiveness of our approach with real-world
      datasets. All experimental setup is equal to that stated in
      Section 3. Our experiments are to answer the following two
      key questions:</p>
      <ul class="list-no-style">
        <li id="list8" label="•">Q1: Do pre-use preferences lead to
        post-use preferences? (Is our hypothesis valid?)<br /></li>
        <li id="list9" label="•">Q2: How much does our approach
        outperform existing state-of-the-art approaches? (in
        various situations with data sparsity, including basic
        top-<em>N</em> recommendation, long-tail item's
        recommendation, and recommendation to cold-start
        users)<br /></li>
      </ul>
      <figure id="fig5">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186159/images/www2018-168-fig5.jpg"
        class="img-responsive" alt="Figure 5" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 5:</span> <span class=
          "figure-title">Correlation analysis between pre-use
          preferences and post-use preferences (MovieLens).</span>
        </div>
      </figure>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Q1.
            Validating Our Hypothesis</h3>
          </div>
        </header>
        <p>This section presents two experiments whose results
        support the validity of our hypothesis that pre-use
        preferences on items mostly lead to their post-use
        preferences. In the first experiment, we directly compare
        the correlation between pre-use preferences and post-use
        preferences on items. A positive correlation signifies that
        our hypothesis is valid.</p>
        <p>Figure 5 reports the correlation between pre-use
        preferences and post-use preferences on items, where the
        <em>x</em>-axis indicates the range of pre-use preferences
        on items and the <em>y</em>-axis does the average of
        post-use preferences on items having the pre-use
        preferences in the corresponding range. It clearly shows
        the positive correlation between the two axes, especially
        in the range of pre-use preferences between 0.4 and 1.0.
        Given that most items (93.3% in exact) belong to the
        pre-use preferences range between 0.4 and 1.0, this result
        supports that our hypothesis is valid.</p>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">The accuracy of top-<em>N</em>
            recommendations using pre-use preferences
            (MovieLens).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metrics</th>
                <th style="text-align:center;">itemKNN</th>
                <th style="text-align:center;">SVD</th>
                <th style="text-align:center;">top-<em>N</em>
                pre-use preferences</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.039</td>
                <td style="text-align:center;">0.069</td>
                <td style="text-align:center;">
                <strong>0.192</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.041</td>
                <td style="text-align:center;">0.059</td>
                <td style="text-align:center;">
                <strong>0.151</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.039</td>
                <td style="text-align:center;">0.048</td>
                <td style="text-align:center;">
                <strong>0.115</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.030</td>
                <td style="text-align:center;">0.056</td>
                <td style="text-align:center;">
                <strong>0.202</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.059</td>
                <td style="text-align:center;">0.091</td>
                <td style="text-align:center;">
                <strong>0.299</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.111</td>
                <td style="text-align:center;">0.144</td>
                <td style="text-align:center;">
                <strong>0.421</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.043</td>
                <td style="text-align:center;">0.084</td>
                <td style="text-align:center;">
                <strong>0.254</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.053</td>
                <td style="text-align:center;">0.089</td>
                <td style="text-align:center;">
                <strong>0.272</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.071</td>
                <td style="text-align:center;">0.105</td>
                <td style="text-align:center;">
                <strong>0.306</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In the second experiment, we show that a high level of
        accuracy can be achieved <em>even when only pre-use
        preferences are used</em> in top-<em>N</em>
        recommendations. In other words, each user is recommended
        with the items having the top-<em>N pre-use
        preferences</em> inferred for her/him, without any CF. If
        our hypothesis is valid, the recommendation should provide
        reasonable accuracy. We compare the accuracy of
        recommendation based on the top-<em>N</em> pre-use
        preferences with two existing CF approaches, itemKNN
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0005">5</a>] and SVD
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0034">34</a>]. Both
        itemKNN and SVD use existing ratings only, without
        employing any data imputation.</p>
        <p>Table 5 reports the accuracy results. Surprisingly, the
        recommendation using pre-use preferences only (i.e., no
        further CF) achieves accuracy, 389% higher than itemKNN and
        176% higher than SVD, in terms of <em>P</em>@5. Indeed, its
        remarkable accuracy is comparable to that of existing
        state-of-the-art approaches. Based on the results of two
        experiments above, as the answer to Q1, it is verified that
        pre-use preferences lead to post-use preferences.</p>
      </section>
      <section id="sec-21">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Q2.
            Performance Comparison</h3>
          </div>
        </header>
        <p>This section evaluates how much our approach outperforms
        existing state-of-the-art approaches. The accuracy
        evaluation is conducted under various situations, including
        basic top-<em>N</em> recommendation, long-tail item's
        recommendation, and recommendation to cold-start users. To
        demonstrate the effectiveness of our approach independently
        of CF methods, we use the most widely used itemKNN and SVD
        in CF. We compare two variants of our approach with three
        existing state-of-the-art approaches (i.e., pureSVD
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0003">3</a>], AllRank
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0043">43</a>], and
        Zero-injection [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>]) in group S because the accuracy
        of group S is higher than those of group M as shown in
        Table 2.</p>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class=
            "table-title">Comparison of existing state-of-the-art
            approaches and our approach.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">Imputed values</th>
                <th style="text-align:center;">Imputing
                targets</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">pureSVD</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">All missing
                ratings</td>
              </tr>
              <tr>
                <td style="text-align:center;">AllRank</td>
                <td style="text-align:center;">2</td>
                <td style="text-align:center;">All missing
                ratings</td>
              </tr>
              <tr>
                <td style="text-align:center;">Zero-injection</td>
                <td style="text-align:center;">0</td>
                <td style="text-align:center;">Ratings only for
                unintersting items</td>
              </tr>
              <tr>
                <td style="text-align:center;">Our approach</td>
                <td style="text-align:center;">[1, 5]</td>
                <td style="text-align:center;">All missing
                ratings</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab7">
          <div class="table-caption">
            <span class="table-number">Table 7:</span> <span class=
            "table-title">The accuracy of long-tail item's
            recommendations (MovieLens).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metrics</th>
                <th style="text-align:center;">pureSVD</th>
                <th style="text-align:center;">AllRank</th>
                <th style="text-align:center;">Zero-</th>
                <th style="text-align:center;">Our approach+</th>
                <th style="text-align:center;">Our approach+</th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">injection</th>
                <th style="text-align:center;">itemKNN</th>
                <th style="text-align:center;">SVD</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.099</td>
                <td style="text-align:center;">0.104</td>
                <td style="text-align:center;">0.108</td>
                <td style="text-align:center;">
                <strong>0.112</strong></td>
                <td style="text-align:center;">
                <strong>0.122</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.076</td>
                <td style="text-align:center;">0.082</td>
                <td style="text-align:center;">0.087</td>
                <td style="text-align:center;">
                <strong>0.088</strong></td>
                <td style="text-align:center;">
                <strong>0.096</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.057</td>
                <td style="text-align:center;">0.062</td>
                <td style="text-align:center;">0.066</td>
                <td style="text-align:center;">
                <strong>0.068</strong></td>
                <td style="text-align:center;">
                <strong>0.072</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.183</td>
                <td style="text-align:center;">0.199</td>
                <td style="text-align:center;">0.190</td>
                <td style="text-align:center;">
                <strong>0.194</strong></td>
                <td style="text-align:center;">
                <strong>0.220</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.133</td>
                <td style="text-align:center;">0.144</td>
                <td style="text-align:center;">0.144</td>
                <td style="text-align:center;">
                <strong>0.145</strong></td>
                <td style="text-align:center;">
                <strong>0.160</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.191</td>
                <td style="text-align:center;">0.202</td>
                <td style="text-align:center;">0.206</td>
                <td style="text-align:center;">
                <strong>0.209</strong></td>
                <td style="text-align:center;">
                <strong>0.227</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.127</td>
                <td style="text-align:center;">0.137</td>
                <td style="text-align:center;">0.138</td>
                <td style="text-align:center;">
                <strong>0.142</strong></td>
                <td style="text-align:center;">
                <strong>0.158</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.132</td>
                <td style="text-align:center;">0.143</td>
                <td style="text-align:center;">0.145</td>
                <td style="text-align:center;">
                <strong>0.146</strong></td>
                <td style="text-align:center;">
                <strong>0.163</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.146</td>
                <td style="text-align:center;">0.157</td>
                <td style="text-align:center;">0.160</td>
                <td style="text-align:center;">
                <strong>0.162</strong></td>
                <td style="text-align:center;">
                <strong>0.180</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab8">
          <div class="table-caption">
            <span class="table-number">Table 8:</span> <span class=
            "table-title">The accuracy of top-<em>N</em>
            recommendations to cold-start users (MovieLens).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metrics</th>
                <th colspan="5" style="text-align:left;">
                  <em>k</em>=5
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">(l)2-6</th>
                <th style="text-align:center;">pureSVD</th>
                <th style="text-align:center;">AllRank</th>
                <th style="text-align:center;">Zero</th>
                <th style="text-align:center;">Our approach+</th>
                <th style="text-align:center;">Our approach+</th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">injection</th>
                <th style="text-align:center;">itemKNN</th>
                <th style="text-align:center;">SVD</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.054</td>
                <td style="text-align:center;">0.063</td>
                <td style="text-align:center;">0.029</td>
                <td style="text-align:center;">
                <strong>0.079</strong></td>
                <td style="text-align:center;">
                <strong>0.080</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.047</td>
                <td style="text-align:center;">0.053</td>
                <td style="text-align:center;">0.020</td>
                <td style="text-align:center;">
                <strong>0.066</strong></td>
                <td style="text-align:center;">
                <strong>0.062</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.040</td>
                <td style="text-align:center;">0.045</td>
                <td style="text-align:center;">0.012</td>
                <td style="text-align:center;">
                <strong>0.053</strong></td>
                <td style="text-align:center;">
                <strong>0.053</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.053</td>
                <td style="text-align:center;">0.062</td>
                <td style="text-align:center;">0.040</td>
                <td style="text-align:center;">
                <strong>0.083</strong></td>
                <td style="text-align:center;">
                <strong>0.078</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.096</td>
                <td style="text-align:center;">0.103</td>
                <td style="text-align:center;">0.063</td>
                <td style="text-align:center;">
                <strong>0.137</strong></td>
                <td style="text-align:center;">
                <strong>0.125</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.144</td>
                <td style="text-align:center;">0.165</td>
                <td style="text-align:center;">0.079</td>
                <td style="text-align:center;">
                <strong>0.215</strong></td>
                <td style="text-align:center;">
                <strong>0.207</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.066</td>
                <td style="text-align:center;">0.075</td>
                <td style="text-align:center;">0.043</td>
                <td style="text-align:center;">
                <strong>0.107</strong></td>
                <td style="text-align:center;">
                <strong>0.097</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.079</td>
                <td style="text-align:center;">0.085</td>
                <td style="text-align:center;">0.048</td>
                <td style="text-align:center;">
                <strong>0.118</strong></td>
                <td style="text-align:center;">
                <strong>0.107</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.094</td>
                <td style="text-align:center;">0.105</td>
                <td style="text-align:center;">0.052</td>
                <td style="text-align:center;">
                <strong>0.141</strong></td>
                <td style="text-align:center;">
                <strong>0.132</strong></td>
              </tr>
            </tbody>
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="5" style="text-align:left;">
                  <em>k</em>=10
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">(l)2-6</th>
                <th style="text-align:center;">pureSVD</th>
                <th style="text-align:center;">AllRank</th>
                <th style="text-align:center;">Zero-</th>
                <th style="text-align:center;">Our approach+</th>
                <th style="text-align:center;">Our approach+</th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">injection</th>
                <th style="text-align:center;">itemKNN</th>
                <th style="text-align:center;">SVD</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;"><em>P</em>@5</td>
                <td style="text-align:center;">0.066</td>
                <td style="text-align:center;">0.071</td>
                <td style="text-align:center;">0.063</td>
                <td style="text-align:center;">
                <strong>0.080</strong></td>
                <td style="text-align:center;">
                <strong>0.090</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@10</td>
                <td style="text-align:center;">0.057</td>
                <td style="text-align:center;">0.064</td>
                <td style="text-align:center;">0.053</td>
                <td style="text-align:center;">
                <strong>0.072</strong></td>
                <td style="text-align:center;">
                <strong>0.075</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>P</em>@20</td>
                <td style="text-align:center;">0.048</td>
                <td style="text-align:center;">0.053</td>
                <td style="text-align:center;">0.038</td>
                <td style="text-align:center;">
                <strong>0.063</strong></td>
                <td style="text-align:center;">
                <strong>0.063</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@5</td>
                <td style="text-align:center;">0.071</td>
                <td style="text-align:center;">0.072</td>
                <td style="text-align:center;">0.077</td>
                <td style="text-align:center;">
                <strong>0.087</strong></td>
                <td style="text-align:center;">
                <strong>0.096</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@10</td>
                <td style="text-align:center;">0.119</td>
                <td style="text-align:center;">0.132</td>
                <td style="text-align:center;">0.122</td>
                <td style="text-align:center;">
                <strong>0.153</strong></td>
                <td style="text-align:center;">
                <strong>0.152</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>R</em>@20</td>
                <td style="text-align:center;">0.190</td>
                <td style="text-align:center;">0.192</td>
                <td style="text-align:center;">0.180</td>
                <td style="text-align:center;">
                <strong>0.248</strong></td>
                <td style="text-align:center;">
                <strong>0.246</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>nDCG</em>@5</td>
                <td style="text-align:center;">0.086</td>
                <td style="text-align:center;">0.092</td>
                <td style="text-align:center;">0.084</td>
                <td style="text-align:center;">
                <strong>0.111</strong></td>
                <td style="text-align:center;">
                <strong>0.118</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@10</td>
                <td style="text-align:center;">0.099</td>
                <td style="text-align:center;">0.109</td>
                <td style="text-align:center;">0.097</td>
                <td style="text-align:center;">
                <strong>0.129</strong></td>
                <td style="text-align:center;">
                <strong>0.132</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>nDCG</em>@20</td>
                <td style="text-align:center;">0.122</td>
                <td style="text-align:center;">0.128</td>
                <td style="text-align:center;">0.113</td>
                <td style="text-align:center;">
                <strong>0.159</strong></td>
                <td style="text-align:center;">
                <strong>0.161</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Figure 6 shows that two variants of our approach
        universally and consistently outperform existing imputation
        approaches with all datasets and all metrics. This
        indicates the imputed values inferred by our approach are
        more beneficial than those by other existing imputation
        approaches since they satisfy all the three claims.
        Moreover, Our approach+SVD achieves 16.4% higher accuracy
        than Zero-injection, which has been reported to provide the
        best accuracy [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>] among existing imputation
        approaches, in terms of <em>P</em>@5 in MovieLens.</p>
        <figure id="fig6">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186159/images/www2018-168-fig6.jpg"
          class="img-responsive" alt="Figure 6" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 6:</span>
            <span class="figure-title">The accuracy of basic
            top-<em>N</em> recommendations (MovieLens, CiaoDVD, and
            Watcha).</span>
          </div>
        </figure>
        <p></p>
        <p>We also validate our approach under the situations where
        the problem of data sparsity is even more serious: (1)
        long-tail item's (i.e., items having a few ratings)
        recommendation and (2) recommendation to cold-start users
        (i.e., users giving a few ratings) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0009">9</a>]. To set up the first
        situation, we divide items into <em>top-head items</em> and
        <em>long-tail items</em> by their popularity (i.e., the
        number of ratings they have) [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>]. For MovieLens, top-head items are
        5.9% of all items (i.e., 100 items) but involve 30% of all
        ratings. Then, we only recommend those items belonging to
        long-tail items to users. We set up the second situation,
        following [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0009">9</a>]
        and [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0036">36</a>]. We randomly select 200 users as
        cold-start users. Then, we also randomly select a small
        <em>k</em> number of ratings (<em>k</em> is set as 5 and
        10) for each cold-start user and discard the rest of the
        ratings. Considering the average number of ratings per user
        in Table 1, the selective values of <em>k</em> are small.
        We finally recommend items only to the cold-start
        users.</p>
        <p>Tables 7 and 8 show the accuracy of recommendations
        under these two data sparsity situations in MovieLens.
        Similar to the results of basic top-<em>N</em>
        recommendation, two variants of our approach universally
        and consistently outperform existing imputation approaches
        with all metrics. The overall results provide the answer to
        Q2.</p>
      </section>
    </section>
    <section id="sec-22">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span>
          Conclusions</h2>
        </div>
      </header>
      <p>In this paper, we identified the limitations of existing
      data imputation approaches and suggested three new claims
      that all data imputation approaches should follow for more
      accurate recommendations. We also hypothesized that most
      pre-use preferences on items lead to their post-use
      preferences. Based on our hypothesis, we proposed a novel
      imputation approach based on deep learning. Through extensive
      experiments with real-world datasets, we demonstrated that
      our proposed approach is quite effective, achieving up to
      16.4% higher accuracy compared to the <em>best</em> among the
      state-of-the-art approaches. Our approach also outperforms
      all the other approaches under more difficult settings, such
      as long-tail item's recommendation and recommendation to
      cold-start users.</p>
    </section>
    <section id="sec-23">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This work was supported by the National Research
      Foundation of Korea (NRF) grant funded by the Korea
      government (MSIT: Ministry of Science and ICT) (No.
      NRF-2017R1A2B3004581). Also, this work was supported by the
      Naver Corporation, where Jung-Tae Lee and Jaeho Choi gave us
      good comments in a practical point of view, which helped us
      greatly in performing this research successfully.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">G. Adomavicius and A.
        Tuzhilin. 2005. Toward the Next Generation of Recommender
        Systems: A Survey of the State-of-the-Art and Possible
        Extensions. <em><em>IEEE Trans. on Knowledge and Data
        Engineering</em></em> 17 (2005), 734–749.</li>
        <li id="BibPLXBIB0002" label="[2]">S. Chang, F.&nbsp;M.
        Harper, and L.&nbsp;G. Terveen. 2016. Crowd-Based
        Personalized Natural Language Explanations for
        Recommendations. In <em><em>Proc. ACM Int'l Conf. on
        Recommender Systems</em></em> . 175–182.</li>
        <li id="BibPLXBIB0003" label="[3]">P. Cremonesi, Y. Koren,
        and R. Turrin. 2010. Performance of Recommender Algorithms
        on Top-N Recommendation Tasks. In <em><em>Proc. ACM Int'l
        Conf. on Recommender Systems</em></em> . 39–46.</li>
        <li id="BibPLXBIB0004" label="[4]">C. Doersch. 2016.
        Tutorial on Variational Autoencoders. <em><em>arXiv
        preprint arXiv:1606.05908</em></em> (2016).</li>
        <li id="BibPLXBIB0005" label="[5]">B.&nbsp;Sarwar et
        al.2001. Item-Based Collaborative Filtering Recommendation
        Algorithms. In <em><em>Proc. Int'l Conf. on World Wide
        Web</em></em> . 285–295.</li>
        <li id="BibPLXBIB0006" label="[6]">D.&nbsp;Kingma et
        al.2014a. Semi-supervised Learning with Deep Generative
        Models. In <em><em>Advances in Neural Information
        Processing Systems</em></em> . 3581–3589.</li>
        <li id="BibPLXBIB0007" label="[7]">D.&nbsp;K.&nbsp;Chae et
        al.2018a. On Identifying k-Nearest Neighbors in
        Neighborhood Models for Efficient and Effective
        Collaborative Filtering. <em><em>Neurocomputing</em></em>
        278(2018), 124–143.</li>
        <li id="BibPLXBIB0008" label="[8]">J.&nbsp;Lee et al.2013a.
        Alleviating the Sparsity in Collaborative Filtering using
        Crowdsourcing. In <em><em>Proc. of Workshop on
        Crowdsourcing and Human Computation for Recommender
        Systems</em></em> .</li>
        <li id="BibPLXBIB0009" label="[9]">J.&nbsp;Lee et al.2016a.
        Improving the Accuracy of Top-N Recommendation using a
        Preference Model. <em><em>Information Sciences</em></em>
        348(2016), 290–304.</li>
        <li id="BibPLXBIB0010" label="[10]">J.&nbsp;W.&nbsp;Ha et
        al.2012a. Top-N Recommendation through Belief Propagation.
        In <em><em>Proc. ACM Int'l Conf. on Information and
        Knowledge Management</em></em> . 2343–2346.</li>
        <li id="BibPLXBIB0011" label="[11]">M.&nbsp;H.&nbsp;Jang et
        al.2016b. PIN-TRUST: Fast Trust Propagation Exploiting
        Positive, Implicit, and Negative Information. In
        <em><em>Proc. ACM Int'l Conf. on Information and Knowledge
        Management</em></em> . 629–638.</li>
        <li id="BibPLXBIB0012" label="[12]">R.&nbsp;Pan et al.2008.
        One-Class Collaborative Filtering. In <em><em>Proc. IEEE
        Int'l Conf. on Data Mining</em></em> . 502–511.</li>
        <li id="BibPLXBIB0013" label="[13]">S.&nbsp;C.&nbsp;Lee et
        al.2015a. A Graph-Based Recommendation Framework for
        Price-Comparison Services. In <em><em>Proc. Int'l Conf. on
        World Wide Web</em></em> . 59–60.</li>
        <li id="BibPLXBIB0014" label="[14]">S.&nbsp;C.&nbsp;Lee et
        al.2017. A Single-Step Approach to Recommendation
        Diversification. In <em><em>Proc. Int'l Conf. on World Wide
        Web</em></em> . 809–810.</li>
        <li id="BibPLXBIB0015" label="[15]">S.&nbsp;Rendle et
        al.2009. BPR: Bayesian Personalized Ranking from Implicit
        Feedback. In <em><em>Proc. Int'l Conf. on Uncertainty in
        Artificial Intelligence</em></em> . 452–461.</li>
        <li id="BibPLXBIB0016" label="[16]">V.&nbsp;Sindhwani et
        al.2010a. One-Class Matrix Completion with Low-Density
        Factorizations. In <em><em>Proc. IEEE Int'l Conf. on Data
        Mining</em></em> . 1055–1060.</li>
        <li id="BibPLXBIB0017" label="[17]">W.&nbsp;S.&nbsp;Hwang
        et al.2013b. Exploiting Trustors as well as Trustees in
        Trust-Based Recommendation. In <em><em>Proc. ACM Int'l
        Conf. on Information and Knowledge Management</em></em> .
        1893–1896.</li>
        <li id="BibPLXBIB0018" label="[18]">W.&nbsp;S.&nbsp;Hwang
        et al.2014b. Data Imputation using a Trust Network for
        Recommendation. In <em><em>Proc. Int'l Conf. on World Wide
        Web</em></em> . 299–300.</li>
        <li id="BibPLXBIB0019" label="[19]">W.&nbsp;S.&nbsp;Hwang
        et al.2015b. On Exploiting Trustors in Trust-Based
        Recommendation. <em><em>Journal of Internet
        Technology</em></em> 16, 4 (2015), 755–765.</li>
        <li id="BibPLXBIB0020" label="[20]">W.&nbsp;S.&nbsp;Hwang
        et al.2016c. Efficient Recommendation Methods using
        Category Experts for a Large Dataset. <em><em>Information
        Fusion</em></em> 28(2016), 75–82.</li>
        <li id="BibPLXBIB0021" label="[21]">W.&nbsp;S.&nbsp;Hwang
        et al.2016d. “Told You I Didn't Like It”: Exploiting
        Uninteresting Items for Effective Collaborative Filtering.
        In <em><em>Proc. IEEE Int'l Conf. on Data
        Engineering</em></em> . 349–360.</li>
        <li id="BibPLXBIB0022" label="[22]">X.&nbsp;He et al.2016e.
        Fast Matrix Factorization for Online Recommendation with
        Implicit Feedback. In <em><em>Proc. ACM Int'l Conf. on
        Research and Development in Information Retrieval</em></em>
        . 549–558.</li>
        <li id="BibPLXBIB0023" label="[23]">Y.&nbsp;C.&nbsp;Lee et
        al.2018b. gOCCF: Graph-Theoretic One-Class Collaborative
        Filtering Based on Uninteresting Items. In <em><em>Proc.
        AAAI Conf. on Artificial Intelligence</em></em> .</li>
        <li id="BibPLXBIB0024" label="[24]">Y.&nbsp;Li et al.2010b.
        Improving One-Class Collaborative Filtering by
        Incorporating rich User Information. In <em><em>Proc. ACM
        Int'l Conf. on Information and Knowledge
        Management</em></em> . 959–968.</li>
        <li id="BibPLXBIB0025" label="[25]">Y.&nbsp;Pu et al.2016f.
        Variational Autoencoder for Deep Learning of Images, Labels
        and Captions. In <em><em>Advances in Neural Information
        Processing Systems</em></em> . 2352–2360.</li>
        <li id="BibPLXBIB0026" label="[26]">Y.&nbsp;Ren et
        al.2012b. The Efficient Imputation Method for
        Neighborhood-Based Collaborative Filtering. In
        <em><em>Proc. ACM Int'l Conf. on Information and Knowledge
        Management</em></em> . 684–693.</li>
        <li id="BibPLXBIB0027" label="[27]">Y.&nbsp;Ren et
        al.2013c. AdaM: Adaptive-Maximum Imputation for
        Neighborhood-Based Collaborative Filtering. In
        <em><em>Proc. IEEE Int'l Conf. on Advances in Social
        Networks Analysis and Mining</em></em> . 628–635.</li>
        <li id="BibPLXBIB0028" label="[28]">Y.&nbsp;Yao et
        al.2014c. Dual-Regularized One-Class Collaborative
        Filtering. In <em><em>Proc. ACM Int'l Conf. on Information
        and Knowledge Management</em></em> . 759–768.</li>
        <li id="BibPLXBIB0029" label="[29]">Z.&nbsp;Gantner et
        al.2011. MyMediaLite: A Free Recommender System Library. In
        <em><em>Proc. ACM Int'l Conf. on Recommender
        Systems</em></em> . 305–308.</li>
        <li id="BibPLXBIB0030" label="[30]">R. He and J. McAuley.
        2016. Ups and Downs: Modeling the Visual Evolution of
        Fashion Trends with One-Class Collaborative Filtering. In
        <em><em>Proc. Int'l Conf. on World Wide Web</em></em> .
        507–517.</li>
        <li id="BibPLXBIB0031" label="[31]">Y. Hu, Y. Koren, and C.
        Volinsky. 2008. Collaborative Filtering for Implicit
        Feedback Datasets. In <em><em>Proc. IEEE Int'l Conf. on
        Data Mining</em></em> . 263–272.</li>
        <li id="BibPLXBIB0032" label="[32]">D. Kingma and M.
        Welling. 2013. Auto-Encoding Variational Bayes. In
        <em><em>arXiv preprint arXiv:1312.6114</em></em> .</li>
        <li id="BibPLXBIB0033" label="[33]">Y. Koren. 2008.
        Factorization Meets the Neighborhood: A Multifaceted
        Collaborative Filtering Model. In <em><em>Proc. ACM Int'l
        Conf. on Knowledge Discovery and Data Mining</em></em> .
        426–434.</li>
        <li id="BibPLXBIB0034" label="[34]">Y. Koren, R. Bell, and
        C. Volinsky. 2009. Matrix Factorization Techniques for
        Recommender Systems. <em><em>Computer</em></em> 42(2009),
        30–37.</li>
        <li id="BibPLXBIB0035" label="[35]">X. Li and J. She. 2017.
        Collaborative Variational Autoencoder for Recommender
        Systems. In <em><em>Proc. of ACM Int'l Conf. on Knowledge
        Discovery and Data Mining</em></em> . 305–314.</li>
        <li id="BibPLXBIB0036" label="[36]">H. Ma, I. King, and
        M.&nbsp;R. Lyu. 2007. Effective Missing Data Prediction for
        Collaborative Filtering. In <em><em>Proc. ACM Int'l Conf.
        on Research and Development in Information
        Retrieval</em></em> . 39–46.</li>
        <li id="BibPLXBIB0037" label="[37]">S. Nathan and J. Tommi.
        2003. Weighted Low-Rank Approximations. In <em><em>Proc.
        Int'l Conf. on Machine Learning</em></em> . 720–727.</li>
        <li id="BibPLXBIB0038" label="[38]">R. Pan and M. Scholz.
        2009. Mind the Gaps: Weighting the Unknown in Large-Scale
        One-Class Collaborative Filtering. In <em><em>Proc. ACM
        Int'l Conf. on Knowledge Discovery and Data
        Mining</em></em> . 667–676.</li>
        <li id="BibPLXBIB0039" label="[39]">W. Pan and L. Chen.
        2013. GBPR: Group Preference Based Bayesian Personalized
        Ranking for One-Class Collaborative Filtering. In
        <em><em>Proc. Int'l Joint Conf. on Artificial
        Intelligence</em></em> . 2691–2697.</li>
        <li id="BibPLXBIB0040" label="[40]">S. Rendle and C.
        Freudenthaler. 2014. Improving Pairwise Learning for Item
        Recommendation from Implicit Feedback. In <em><em>Proc.
        Int'l Conf. on Web Search and Data Mining</em></em> .
        273–282.</li>
        <li id="BibPLXBIB0041" label="[41]">R. Salakhutdinov and A.
        Mnih. 2008. Probabilistic Matrix Factorization. In
        <em><em>Advances in Neural Information Processing
        Systems</em></em> . 1257–1264.</li>
        <li id="BibPLXBIB0042" label="[42]">B. Smyth, R. Rafter,
        and S. Banks. 2016. Harnessing Crowdsourced Recommendation
        Preference Data from Casual Gameplay. In <em><em>Proc. ACM
        Int'l Conf. on User Modeling Adaptation and
        Personalization</em></em> . 95–104.</li>
        <li id="BibPLXBIB0043" label="[43]">H. Steck. 2010.
        Training and Testing of Recommender Systems on Data Missing
        Not at Random. In <em><em>Proc. ACM Int'l Conf. on
        Knowledge Discovery and Data Mining</em></em> .
        713–722.</li>
        <li id="BibPLXBIB0044" label="[44]">M. Volkovs and
        G.&nbsp;W. Yu. 2015. Effective Latent Models for Binary
        Feedback in Recommender Systems. In <em><em>Proc. ACM Int'l
        Conf. on Research and Development in Information
        Retrieval</em></em> . 313–322.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" target="_blank" href=
    "http://movielens.org">movielens.org</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" target="_blank" href=
    "http://dvd.ciao.co.uk">dvd.ciao.co.uk</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class=
    "link-inline force-break" target="_blank" href=
    "http://watcha.net">watcha.net</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>We used itemKNN
    [<a class="bib" data-trigger="hover" data-toggle="popover"
    data-placement="top" href="#BibPLXBIB0005">5</a>] in CF here;
    we also observed the same tendency when we used userKNN and
    other CF.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyons, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186159">https://doi.org/10.1145/3178876.3186159</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
