<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3185998'>https://doi.org/10.1145/3178876.3185998</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3185998'>https://w3id.org/oa/10.1145/3178876.3185998</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Yashoteja</span>      <span class="surName">Prabhu</span>,     Indian Institute of Technology Delhi, <a href="mailto:yashoteja.prabhu@gmail.com">yashoteja.prabhu@gmail.com</a>     </div>     <div class="author">     <span class="givenName">Anil</span>      <span class="surName">Kag</span>,     Microsoft Research India, <a href="mailto:anilkagak2@gmail.com">anilkagak2@gmail.com</a>     </div>     <div class="author">     <span class="givenName">Shrutendra</span>      <span class="surName">Harsola</span>,     Microsoft Bing Ads, <a href="mailto:shharsol@microsoft.com">shharsol@microsoft.com</a>     </div>     <div class="author">     <span class="givenName">Rahul</span>      <span class="surName">Agrawal</span>, <a href="mailto:Rahul.Agrawal@microsoft.com">Rahul.Agrawal@microsoft.com</a>     </div>     <div class="author">     <span class="givenName">Manik</span>      <span class="surName">Varma</span>, <a href="mailto:manik@microsoft.com">manik@microsoft.com</a>     </div>         <Affiliation id="aff2"/>         <Affiliation id="aff4"/>         <Affiliation id="aff6"/>     <Affiliation id="aff7"/>     <Affiliation id="aff8"/>    </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3185998" target="_blank">https://doi.org/10.1145/3178876.3185998</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>This paper develops the Parabel&#x00A0;algorithm for extreme multi-label learning where the objective is to learn classifiers that can annotate each data point with the most relevant subset of labels from an extremely large label set. The state-of-the-art 1-vs-All based DiSMEC and PPDSparse algorithms are the most accurate but can take upto months for training and prediction as they learn and apply an independent linear classifier per label. Consequently, they do not scale to large datasets with millions of labels. Parabel&#x00A0;addresses both limitations by learning a balanced label hierarchy such that: (a) the 1-vs-All classifiers in the leaf nodes of the label hierarchy can be trained on a small subset of the training set thereby reducing the training time to a few hours on a single core of a standard desktop and (b) novel points can be classified by traversing the learned hierarchy in logarithmic time and applying the 1-vs-All classifiers present in just the leaf thereby reducing the prediction time to a few milliseconds per test point. This allows Parabel&#x00A0;to scale to tasks considered infeasible for DiSMEC and PPDSparse such as predicting the subset of 7 million Bing queries that might lead to a click on a given ad-landing page for dynamic search advertising.</small>     </p>     <p>     <small>Experiments on multiple benchmark datasets revealed that Parabel&#x00A0;could be almost as accurate as PPDSparse and DiSMEC while being upto 1,000x faster at training and upto 40x-10,000x faster at prediction. Furthermore, Parabel&#x00A0;was demonstrated to significantly improve dynamic search advertising on Bing by more than doubling the ad recall and improving the click-through rate by 20%. Source code for Parabel&#x00A0;can be downloaded from [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>].</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Extreme classification</small>, </span>     <span class="keyword">      <small> dynamic search advertising</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik Varma. 2018. Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018 (WWW 2018),</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3185998" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3185998</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>     <strong>Objective</strong>: This paper on extreme classification develops the Parabel&#x00A0;algorithm whose predictions are almost as accurate as the state-of-the-art DiSMEC and PPDSparse classifiers while being up to 1,000x faster at training and 40x-10,000x faster at prediction. This allows Parabel&#x00A0;to efficiently and accurately predict the subset of 7 million Bing queries that might lead to a click on a given ad-landing page for Dynamic Search Advertising (DSA).</p>    <p>     <strong>Extreme Classification</strong>: Extreme multi-label learning addresses the problem of automatically annotating each data point with the most relevant <em>subset</em> of labels from an extremely large label set. For instance, there are more than a million labels (tags) on Wikipedia and one might wish to build an extreme multi-label classifier that tags a new article or web page with the subset of most relevant Wikipedia labels. Note that multi-label learning is distinct from multi-class classification which aims to predict a single mutually exclusive label.</p>    <p>     <strong>DSA</strong>: Advertisers are required to provide only a product description, or an ad-landing page, in DSA. The search engine automates everything else including generating the ad-copy, customizing the ad-title and ad-landing page to the search query, generating the bid-phrases and bid-values, <em>etc</em>. This provides a number of benefits to the advertiser including eliminating time-consuming and expensive tasks, reducing the deployment time for new and updated campaigns, more accurate targeting of users, <em>etc</em>.</p>    <p>A central problem in DSA is to determine the subset of search engine queries that might lead to a click on the given product description or ad-landing page. The predictions need to be highly accurate as it is not possible for either the advertiser to manually verify the predictions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] or to automatically match them against the advertiser supplied bid-phrases&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>]. Inaccurate predictions therefore decrease user satisfaction, search engine revenue and conversions for the advertiser. At the same time, the predictions need to be made in milliseconds per test point so as to handle the large, and constantly evolving, corpus of billions of ads. Furthermore, low training costs are critical as multiple prediction models need to be trained, and frequently re-trained, for various markets with different languages and ever changing query distributions.</p>    <p>Traditional approaches to DSA are based on landing page summarization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], translation and query language models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>], keyword suggestion based on Wikipedia concepts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>] and matching queries to the ad title based on deep embeddings&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] <em>etc.</em> This paper formulates DSA as a extreme multi-label learning task with each of the top 7 million monetizable queries on Bing being treated as a separate label. Extreme classifiers are then trained to take in a bag-of-words representation of the product description or ad-landing page as input and predict the subset of relevant Bing queries as output.</p>    <p>     <strong>DiSMEC and PPDSparse</strong>: DiSMEC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>] and PPDSparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>] make significantly more accurate predictions than all other extreme classifiers. They independently train a linear classifier per label based on the 1-vs-All approach and predict the label if the corresponding classifier fires for a given test point. Unfortunately, this leads to prohibitive prediction costs for web-scale datasets involving millions of labels and billions of test points. Training at this scale can also take months for these classifiers due to the large number labels and datapoints. These limitations make DiSMEC, PPDSparse and all other extreme classifiers infeasible for DSA.</p>    <p>     <strong>Parabel&#x00A0;</strong>: Parabel&#x00A0;addresses these limitations by learning a balanced tree over the labels such that similar labels end up together at the leaves. Each internal node partitions the labels allocated to it based on label similarity using a novel label representation. Leaf nodes contain 1-vs-All classifiers trained on a small subset of the training points containing only those examples having at least one leaf node label. This allows Parabel&#x00A0;to train up to 1,000x faster than DiSMEC and PPDSparse without any significant loss in prediction accuracy. Furthermore, the prediction time is cut down from linear to logarithmic in the number of labels allowing Parabel&#x00A0;to be up to 40x and 10,000x faster than PPDSparse and DiSMEC respectively.</p>    <p>Parabel&#x00A0;is also more scalable and accurate than all other extreme classifiers including state-of-the-art tree based approaches such as PfastreXML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. In particular, Parabel&#x00A0;can have up to 20x lower training time, 15x lower model size and 8% higher prediction accuracy as compared to PfastreXML. It achieves this by: (a) learning a small ensemble of up to 3 trees with powerful (1-vs-All) leaf node classifiers rather than a large ensemble of 50 PfastreXML trees with weak (constant) leaf node classifiers and (b) by learning trees which partition labels at each internal node rather than data points. This makes Parabel&#x00A0;much more suited to web-scale applications, such as DSA, because of its high prediction accuracy, low training time, low prediction time and the fact that expensive hardware with lots of RAM is no longer needed for either training or prediction. Specifically, Parabel&#x00A0;was found to be 6% more accurate at predicting queries from ad landing pages as compared to PfastreXML. Furthermore, Parabel&#x00A0;more than doubled the ad recall and increased the click-through rate by 20% as compared to the BM25 baseline.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>     <strong>Extreme multi-label learning</strong>: Much progress has recently been made in developing extreme multi-label learning algorithms based on trees&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>], embeddings&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] and 1-vs-All approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. Of these, 1-vs-All and tree based approaches are directly relevant to this paper.</p>    <p>     <strong>1-vs-All approaches</strong>: 1-vs-All approaches such as DiSMEC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], PD-Sparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>], PPDSparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>] and XML-CNN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] have the advantages of high prediction accuracy and low model size but suffer from high training and prediction times. These approaches learn a separate linear classifier per label. This leads to high prediction times as millions of classifiers need to be evaluated for each test point. As a result, 1-vs-All approaches are unable to meet the latency and throughput requirements of real-world applications including DSA. Some heuristics have been proposed to speed up the prediction times of 1-vs-All approaches based on trees&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] and label filters&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. Unlike Parabel&#x00A0;, such approaches are applied post hoc after the base linear classifiers have been trained and can therefore lead to decreased prediction accuracies and increased training times. Such approaches have therefore not been shown to yield state-of-the-art accuracies on the large scale problems considered in this paper. 1-vs-All approaches also have high training costs as each linear classifier has to be trained over millions of negative data points which might not be relevant for that label. PD-Sparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>] and PPDSparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>] address this issue to a limited extent by sampling the negative examples through primal and dual sparsity preserving optimization. For instance, PPDSparse has been shown to achieve almost the same prediction accuracy as the state-of-the-art DiSMEC with 100x faster training. Unfortunately, these speedups are insufficient to allow PPDSparse to train on large problems with millions of labels where it takes months to train. In contrast, Parabel&#x00A0;can match DiSMEC&#x0027;s accuracies while scaling to problems involving 7 million labels and making predictions in milliseconds.</p>    <p>     <strong>Tree approaches</strong>: Tree approaches to extreme classification have the advantages of low training and prediction times but have high model sizes and poor prediction accuracies. Poor prediction accuracies generally result from the use of weak classifiers in the leaf nodes. State-of-the-art approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] therefore learn large tree ensembles which slightly increase the prediction accuracy over a single tree but also greatly increase the model size. Parabel&#x00A0;differs by learning powerful 1-vs-All classifiers in its leaf nodes thereby allowing it to get significantly higher prediction accuracies using an ensemble of at most 3 trees. Another significant difference is that Parabel&#x00A0;partitions labels, rather than data points, in its internal nodes. While trees that partition labels have been studied extensively in the multi-class literature&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], most of these formulations do not extend straight forwardly to the multi-label setting and have not been shown to scale to millions of classes due to high training costs. In the multi-label setting, the Probabilistic Label Tree (PLT)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] and the HOMER&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] approaches are perhaps the most closely related to Parabel&#x00A0;. Parabel&#x00A0;improves over PLT by learning the label hierarchy rather than using a random one and by using a much more efficient optimization procedure. As demonstrated in Section&#x00A0;<a class="sec" href="#sec-17">4</a>, Parabel&#x00A0;can be 10x faster to train than PLT as well as up to 10% more accurate. As compared to HOMER, Parabel&#x00A0;uses a much faster label tree learning algorithm based on a more informative label feature representation. Furthermore, Parabel&#x00A0;&#x2019;s training and prediction algorithms are theoretically well-principled unlike HOMER&#x0027;s.</p>    <p>     <strong>Deep learning</strong>: Deep learning approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>] focussing on learning representations for extreme classification have also been explored in the literature. Learning features is orthogonal to the focus of this paper though it should be noted that Parabel&#x00A0;could be used to replace the tree&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] or 1-vs-All&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] classifiers used in these approaches for even better results.</p>    <p>     <strong>DSA</strong>: This paper focusses on the problem of determining the subset of search engine queries that might lead to a click on a given ad landing page. Various approaches have been proposed for this task in the organic search literature including information retrieval based methods such as BM25&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], probabilistic methods and topic models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] and deep learning&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. Unfortunately, such techniques have been found to not work well for ad landing pages which are pithy as compared to the regular web pages for which these techniques have been designed. Techniques have also been proposed specifically for sponsored search including those based on landing page summarization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], translation and query language models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] and keyword suggestion based on Wikipedia concepts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>]. Some of these approaches suffer from low coverage and might not be able to effectively leverage historical click data. As can be seen in Section&#x00A0;<a class="sec" href="#sec-17">4</a>, Parabel&#x00A0;is able to significantly increase click-through-rates and coverage on the task of DSA in Bing.</p>   </section>   <section id="sec-11">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Parabel&#x00A0;</h2>     </div>    </header>    <p>Parabel&#x00A0;&#x2019;s key technical contribution is a procedure for efficiently learning a balanced label hierarchy based on a novel label representation. Another important technical novelty is a hierarchical softmax model for multi-label data trained over such a label hierarchy. By effectively leveraging the balanced label tree and strong 1-vs-All classifiers in the leaf nodes, Parabel&#x00A0;cuts down the training and prediction complexities from linear to logarithmic in the number of labels as compared to the state-of-the-art 1-vs-All classifiers with no significant loss in accuracies.</p>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Architecture</h3>     </div>     </header>     <p>Parabel&#x00A0;learns a small ensemble of up to 3 label trees. Each label tree is grown by recursively partitioning the labels into 2 almost balanced groups. Nodes become leaves and are not partitioned further when they contain less than <em>M</em> = 100 (a user tunable hyper-parameter) labels. The leaf nodes contain linear 1-vs-All classifiers, one for each label in the leaf, trained on only those examples having at least one leaf node label.</p>     <p>A test point with unknown labels needs to traverse the label tree during prediction. Each internal node therefore learns two linear classifiers indicating whether a test point should be passed down to the left or right child or both. A test point therefore traverses multiple paths through the label tree to reach multiple leaves. The 1-vs-All classifiers in these leaves are evaluated to determine the marginal probabilities that the corresponding labels are relevant to the test point. Predictions are made by averaging these probabilities across the various trees of the ensemble.</p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Learning the Label Hierarchy</h3>     </div>     </header>     <p>This subsection describes the procedure used to partition the labels at an internal node. The procedure is applied recursively starting from the root node containing all the labels. Node partitioning is terminated when a node contains fewer than <em>M</em> = 100 labels. Training of the leaf node classifiers is discussed in Subsection&#x00A0;<a class="sec" href="#sec-15">3.4</a>.</p>     <p>     <strong>Label representation and similarity:</strong> A measure of label similarity is needed for partitioning the labels allocated to an internal node. Parabel&#x00A0;represents a label by the mean of the training points containing the label. Given a set of <em>N</em> training points <span class="inline-equation"><span class="tex">$\{(\bf x_i,\bf y_i)_{i=1}^{N}\}$</span>     </span> with <em>D</em> dimensional feature vectors <span class="inline-equation"><span class="tex">${\bf x}_i \in \mathbb {R}^D$</span>     </span> and <em>L</em> dimensional label vectors <span class="inline-equation"><span class="tex">${\bf y}_i \in \lbrace 0,1\rbrace ^L$</span>     </span>, the feature representation of label <em>l</em> is <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{align} {\bf v}_l = {\bf v}^{\prime }_l/\Vert {bf v}^{\prime }_l\Vert _2 \,\,\,\,\, \mbox{where,} \,\,\, {\bf v}^{\prime }_l = \sum _{i=1}^N y_{il} {\bf x}_i \end{align} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> and the similarity between labels <em>l</em> and <em>j</em> can be determined as <span class="inline-equation"><span class="tex">${\bf v}_l^{\top } {\bf v}_j$</span>     </span>. Note that the label features are needed just as an intermediate representation to construct the label tree and do not contribute to Parabel&#x00A0;&#x2019;s final model size. Further note that the features for all the labels can be computed efficiently in <span class="inline-equation"><span class="tex">$O(N\hat{L}\hat{D})$</span>     </span> time where <span class="inline-equation"><span class="tex">$\hat{L}$</span>     </span> and <span class="inline-equation"><span class="tex">$\hat{D}$</span>     </span> are the average label and feature sparsities respectively. This allows label similarity to be determined orders of magnitude more efficiently as compared to alternatives proposed in the literature such as those based on the label confusion matrix&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>].</p>     <p>The proposed representation is based on the intuition that two labels are similar if they are active in similar training points. In DSA, two queries (labels) are similar according to the proposed representation if they lead to clicks on very similar ads (training points). For example, the similarity between queries &#x201D;victorian style floor tiles&#x201D; and &#x201D;tiles are us&#x201D; is 0.97 as they lead to clicks on similar ads whereas the similarity between &#x201D;victorian style floor tiles&#x201D; and &#x201D;domino pizza&#x201D; is 0.66 as they are dissimilar. Parabel&#x00A0;&#x2019;s similarity measure might also be seen as an efficient proxy for the label confusion matrix since two similar labels, with similar training points, are likely to have high inter-class confusion. Furthermore, Parabel&#x00A0;&#x2019;s label representation might be better suited for dealing with tail labels which have very few training points. The estimated label confusion matrix might be unreliable for such labels due to classifier overfitting and limited validation data. On the other hand, Parabel&#x00A0;&#x2019;s error in estimating the label&#x0027;s mean vector might be lower given the small amount of training data. Parabel&#x00A0;&#x2019;s label representation is also more suitable than naive alternatives such as <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \tilde{{\bf v}}_l = [y_{1l},y_{2l},\cdots ,y_{Nl}] \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> For similar tail labels <em>l</em> and <em>j</em>, <span class="inline-equation"><span class="tex">${\bf v}_l^{\top } {\bf v}_j$</span>     </span> might be high whereas <span class="inline-equation"><span class="tex">$\tilde{{\bf v}}_l^{\top } \tilde{{\bf v}}_j$</span>     </span> might be 0 if the labels <em>l</em> and <em>j</em> are active for similar, but not the same, training points. This leads to significantly higher accuracies for Parabel&#x00A0;as demonstrated in Section&#x00A0;<a class="sec" href="#sec-17">4</a>.</p>     <p>     <strong>Label partitioning:</strong> The labels present at an internal node are partitioned into 2 equal groups by clustering them using the constrained spherical <em>k</em> = 2-means objective <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{align} 	\max_{\substack{{\bf\mu}_{\pm} \in \mathbb{R}^D , {\bf\alpha} \in \{-1,1\}^L}} ~~\frac{1}{L}\sum_{l=1}^L \Big(\frac{1+\alpha_l}{2}{\bf \mu}_+^{\top}{\bf v}_l + \frac{1-\alpha_l}{2}{\bf \mu}_-^{\top}{\bf v}_l\Big) \end{align} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>     <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{align} 	\mbox{s.t.}~~ \|{\bf \mu}	_{\pm}\|_2=1,\quad -1 \le \sum_{l=1}^L \alpha_l \le 1 \end{align} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where it has been assumed without loss of generality that the node has <em>L</em> labels and <em>&#x03B1;<sub>l</sub>     </em> = +1 indicates that label <em>l</em> has been assigned to the positive cluster with mean <span class="inline-equation"><span class="tex">${\bf \mu}_+$</span>     </span> and <em>&#x03B1;<sub>l</sub>     </em> = &#x2212;1 indicates that it has been assigned to the negative cluster with mean <span class="inline-equation"><span class="tex">${\bf \mu }_-$</span>     </span>. The constraint on <span class="inline-equation"><span class="tex">${\bf \alpha }$</span>     </span> ensures that the sizes of the two partitions are utmost one apart though this could be relaxed to a user tunable hyper-parameter <em>&#x03B7;</em> if the label structure was known to be imbalanced <em>a priori</em>.</p>     <p>The optimization problem in (<a class="eqn" href="#eq3">3</a>) is NP-hard&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]. Parabel&#x00A0;therefore employs the following alternating minimization algorithm which converges to a local optimum. The algorithm is initialized by sampling <span class="inline-equation"><span class="tex">${\bf \mu }_{\pm }$</span>     </span> from <span class="inline-equation"><span class="tex">$\lbrace {\bf v}_1,{\bf v}_2,\cdots ,{bf v}_L\rbrace$</span>     </span> uniformly at random without replacement. The following two steps are then repeated in each iteration of the algorithm until convergence. In the first step of each iteration, (<a class="eqn" href="#eq3">3</a>) is maximized while keeping <span class="inline-equation"><span class="tex">${\bf \mu }_{\pm }$</span>     </span> fixed. It is straightforward to show that the optimal <span class="inline-equation"><span class="tex">$\alpha _l^* = \mbox{Sign}\big (\mbox{Rank}\big (({\bf \mu }_+-{\bf \mu }_-)^{\top }{\bf v}_l\big)-\frac{L+1}{2}\big)$</span>     </span>, with <span class="inline-equation"><span class="tex">$\alpha _l^*=\mbox{Sign}({\bf \mu }_+-{\bf \mu }_-)^{\top }{\bf v}_l)$</span>     </span> for the label ranked <span class="inline-equation"><span class="tex">$\frac{L+1}{2}$</span>     </span> when <em>L</em> is odd. In the second step, each <em>&#x03B1;<sub>l</sub>     </em> is fixed and (<a class="eqn" href="#eq3">3</a>) maximized with respect to <span class="inline-equation"><span class="tex">${\bf \mu }_{\pm }$</span>     </span> to get <span class="inline-equation"><span class="tex">${\bf \mu }_{\pm }^* = {\bf \mu }^{\prime }_{\pm }/\Vert {\bf \mu }^{\prime }_{\pm }\Vert$</span>     </span> where <span class="inline-equation"><span class="tex">${\bf \mu }^{\prime }_{\pm } = \sum \limits _{l:\alpha _l={\pm 1}} {\bf v}_l$</span>     </span>. Convergence is reached when the <span class="inline-equation"><span class="tex">${\bf \alpha }$</span>     </span> assignments do not change from the previous iteration though, empirically, it was observed that good results could be obtained by terminating the algorithm when the clustering objective does not increase by more than 10<sup>&#x2212; 4</sup> from the previous iteration. The derivation of the update equations and the proof of convergence to a local optimum are given in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. Note that the proposed algorithm turns out to be a more efficient version of the constrained <em>k</em>-means algorithm for general <em>k</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] where <span class="inline-equation"><span class="tex">${\bf \alpha }^*$</span>     </span> can be obtained in closed form rather than solving by a linear program. The label partitioning procedure is applied recursively starting from the root node till all the trees are fully grown. Note that distinct trees are learnt as different random initializations lead to different solutions of (<a class="eqn" href="#eq3">3</a>).</p>     <p>     <strong>Comparison to other approaches:</strong> Parabel&#x00A0;&#x2019;s label tree is learnt in time <span class="inline-equation"><span class="tex">$O(\hat{D}L\log (L))$</span>     </span>. This is significantly faster than partitioning labels using graph cuts on the label confusion matrix&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>] which would not lead to balanced trees and where the time taken to partition just the root node would be <em>O</em>(<em>L</em>     <sup>2</sup>). The algorithms in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>] introduce balance constraints while partitioning labels but their optimization problems take much longer to solve than Parabel&#x00A0;&#x2019;s <em>k</em>-means as each iteration involves fitting an SVM. As a result, none of these methods have been shown to scale to large problems involving millions of labels and training points. Even in terms of accuracy, Parabel&#x00A0;&#x2019;s label tree turns out to be significantly better than other scalable approaches such as constructing random label trees or learning trees by partitioning labels using the proposed constrained <em>k</em> = 2-means algorithm on <span class="inline-equation"><span class="tex">$\tilde{{\bf v}}_l$</span>     </span>&#x00A0;(<a class="eqn" href="#eq2">2</a>) label representation as demonstrated in the Section&#x00A0;<a class="sec" href="#sec-17">4</a>. Finally, Parabel&#x00A0;should not be confused with LPSR&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], which also uses <em>k</em>-means to learn its trees albeit by clustering data points rather than partitioning labels.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> A Hierarchical Probabilistic Model</h3>     </div>     </header>     <p>Parabel&#x00A0;extends the hierarchical softmax model frequently used in extreme multi-class classification to the multi-label setting. Parabel&#x00A0;models <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf x})$</span>     </span> for multi-label <span class="inline-equation"><span class="tex">${\bf y} \in \lbrace 0,1\rbrace ^L$</span>     </span> based on the learnt label tree and without necessarily assuming label independence.</p>     <p>     <strong>Notation:</strong> Given a label tree, let <span class="inline-equation"><span class="tex">$\mathcal {I}=\lbrace 1,\ldots ,N_{\mathcal {I}}\rbrace$</span>     </span> denote the set of <span class="inline-equation"><span class="tex">$N_{\mathcal {I}}$</span>     </span> internal nodes and <span class="inline-equation"><span class="tex">$\mathcal {L}=\lbrace N_{\mathcal {I}}+1,\ldots ,N_{\mathcal {I}}+N_{\mathcal {L}}\rbrace$</span>     </span> denote the set of <span class="inline-equation"><span class="tex">$N_{\mathcal {L}}$</span>     </span> leaf nodes in the tree. Furthermore, let <span class="inline-equation"><span class="tex">$\mathcal {C}_n$</span>     </span> be the set of child nodes of an internal node <em>n</em>. Note that, while each internal node has only 2 children in the label tree proposed in Subsection&#x00A0;<a class="sec" href="#sec-13">3.2</a>, Parabel&#x00A0;&#x2019;s probabilistic model holds for <em>k</em>-ary trees and thus it is not explicitly assumed that <span class="inline-equation"><span class="tex">$|\mathcal {C}_n|=2$</span>     </span>. Finally, let <span class="inline-equation"><span class="tex">${\bf y}_n$</span>     </span> be a vector of binary random variables denoting whether the labels in the leaf node <em>n</em> are relevant to a given data point <span class="inline-equation"><span class="tex">${\bf x}$</span>     </span>.</p>     <p>     <strong>Discriminative model:</strong> Parabel&#x00A0;models the probability for each set of labels <span class="inline-equation"><span class="tex">${\bf y} \in \lbrace 0,1\rbrace ^L$</span>     </span> to be relevant to a given data point <span class="inline-equation"><span class="tex">${\bf x}$</span>     </span>. A label set can be sampled from such a probability distribution as follows. At each internal node, starting at the root, a probability distribution is sampled to determine which child nodes should be traversed. This procedure is applied recursively until a set of leaf nodes is reached. The set of relevant labels is then obtained by sampling from the label distributions in the reached leaves.</p>     <p>To be more precise, let the binary random variable <em>z<sub>n</sub>     </em> take the value 1 if node <em>n</em> was traversed and 0 otherwise and let <span class="inline-equation"><span class="tex">${\bf z}_{\mathcal {C}_n}$</span>     </span> be the set of the indicator <em>z</em> variables of the children of node <em>n</em>. Furthermore, let <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> denote the set all the <em>z</em> variables in the tree so that <span class="inline-equation"><span class="tex">$\bar{{\bf z}}_n={\bf z} \setminus z_n$</span>     </span> and <span class="inline-equation"><span class="tex">$\bar{{\bf z}}_{\mathcal {C}_n} = {\bf z} \setminus {\bf z}_{\mathcal {C}_n}$</span>     </span> denote the complements of <em>z<sub>n</sub>     </em> and <span class="inline-equation"><span class="tex">${\bf z}_{\mathcal {C}_n}$</span>     </span>respectively. Then, tree traversal at the internal node <em>n</em> happens by sampling from <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf z}_{\mathcal {C}_n}|z_n=1,{\bf x})$</span>     </span>. The set of relevant labels <span class="inline-equation"><span class="tex">${\bf y}$</span>     </span> is then generated by sampling labels from each leaf node <em>n</em> that has been reached according to <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}_n|z_n=1,{\bf x})$</span>     </span>. Let <span class="inline-equation"><span class="tex">$\mathcal {Z}_{{\bf y}}$</span>     </span> denote the set of all the configurations of <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> which could have led to <span class="inline-equation"><span class="tex">${\bf y}$</span>     </span> being sampled. Then, Parabel&#x00A0;&#x2019;s probabilistic model is given by <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb{P}({\bf y}|{\bf x}) 	&= \sum_{{\bf z}} \mathbb{P}({\bf y}|{\bf z},{\bf x})\mathbb{P}({\bf z}|{\bf x}) \end{align} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div>     <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{align}= \sum_{{\bf z} \in \mathcal{Z}_{{\bf y}}}\prod_{n \in \mathcal{L}:z_n=1}\mathbb{P}({\bf y}_n|z_n=1,{\bf x})\prod_{n \in \mathcal{I}:z_n=1}\mathbb{P}({\bf z}_{\mathcal{C}_n}|z_n=1,{\bf x}) \end{align} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> and is based on the following assumptions and theorem:</p>     <p>     <strong>Unvisited node assumption:</strong> This assumption formalizes the observation that the children of an unvisited internal node will never be traversed and that the labels in an unvisited leaf node will never be sampled. This implies <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb{P}({\bf z}_{\mathcal{C}_n} = {\bf 0}|z_n=0,{\bf x}) = 1 \forall n \in \mathcal{I}\end{align} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>     <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb{P}({\bf y}_n={\bf 0}|z_n=0,{\bf x}) = 1 \forall n \in \mathcal{L} \end{align} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>     </p>     <p>     <strong>Node independence assumptions:</strong> Parabel&#x00A0;assumes that the probability distribution at a reached node, whether internal or leaf, is sampled independently of all other nodes so that <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb{P}({\bf y}_n|z_n=1,{\bf x}) = \mathbb{P}({\bf y}_n|z_n=1,{\bf x},\bar{{\bf z}}_n, \bar{{\bf y}}_n)\quad \forall n \in \mathcal{L}\end{align} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>     <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{align} \mathbb{P}({\bf z}_{\mathcal{C}_n}|z_n=1,{\bf x}) = \mathbb{P}({\bf z}_{\mathcal{C}_n}| z_n=1, {\bf x} ,\bar{{\bf z}}_{\mathcal{C}_n}\setminus z_n, {\bf y} )\quad\forall n \in \mathcal{I} \end{align} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\bar{{\bf y}}_n$</span>     </span> denotes the set of all labels not present at node <em>n</em>.</p>     <p>Note that these assumptions do not imply label independence. The leaf node assumption&#x00A0;(<a class="eqn" href="#eq9">9</a>) allows Parabel&#x00A0;to model arbitrary correlations between the labels within a leaf while also modeling weaker correlations between label sets across leaves. In particular, labels in a leaf node <em>n</em> can have any correlation structure as long as they can be sampled efficiently from <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}_n|z_n=1,{\bf x})$</span>     </span>. This allows Parabel&#x00A0;to model the labels &#x201D;victorian style floor tiles&#x201D; and &#x201D;tiles are us&#x201D; as dependent if required. The assumption also does not imply that labels in different leaves are independent &#x2013; they are only conditionally independent given <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span>. This allows labels such as &#x201D;tiles are us&#x201D; and &#x201D;domino pizza&#x201D; to be modeled as being mutually exclusive. Finally, this also allows the set of pizza labels to be predicted independently from the set of dessert labels given a restaurant&#x0027;s ad landing page.</p>     <p>The internal node assumption&#x00A0;(<a class="eqn" href="#eq10">10</a>) implies that the decision about which child nodes to traverse at a given internal node is taken independently of the decisions taken at all other nodes. This is a natural assumption which places only mild restrictions on label correlations and is commonly made in most tree algorithms.</p>     <div class="theorem" id="enc1">     <Label>Theorem 3.1.</Label>     <p>      <strong>Tree factorization:</strong> Given a label tree, if the node independence and unvisited node assumptions hold at all the tree nodes, then for a label vector <span class="inline-equation"><span class="tex">${\bf y}$</span>      </span> and an indicator vector <span class="inline-equation"><span class="tex">${\bf z} \in \mathcal {Z}_{{y}}$</span>      </span>      <div class="table-responsive" id="eq11">       <div class="display-equation">        <span class="tex mytex">\begin{align} \mathbb{P}({\bf y}|{\bf z},{\bf x}) = \prod_{n \in \mathcal{L}:z_n=1} \mathbb{P}({\bf y}_n|z_n=1,{\bf x}) \end{align} </span>        <br/>        <span class="equation-number">(11)</span>       </div>      </div>      <div class="table-responsive" id="eq12">       <div class="display-equation">        <span class="tex mytex">\begin{align} \mathbb{P}({\bf z}|{\bf x}) = \prod_{n \in \mathcal{I}:z_n=1} \mathbb{P}({\bf z}_{\mathcal{C}_n}|z_n=1,{\bf x}) \end{align} </span>        <br/>        <span class="equation-number">(12)</span>       </div>      </div>     </p>     </div>     <div class="proof" id="proof1">     <Label>Proof.</Label>     <p> The proof is presented in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. &#x25A1;</p>     </div>     <p>     <strong>Discussion:</strong> Assume a balanced <em>k</em>-ary label tree over <em>L</em> labels grown such that each leaf node has at most <em>M</em> labels. Then the worst case space complexity of Parabel&#x00A0;&#x2019;s probabilistic model for this tree is <span class="inline-equation"><span class="tex">$O(\frac{L}{M}(2^k + 2^M))$</span>     </span> where it has been assumed that each leaf and internal node distributions take space <em>O</em>(2<sup>      <em>M</em>     </sup>) and <em>O</em>(2<sup>      <em>k</em>     </sup>) respectively. This can be tractable for small values of <em>M</em> and <em>k</em> particularly as compared to the <em>O</em>(2<sup>      <em>L</em>     </sup>) space complexity needed for modeling any general multi-label distribution. Assuming that only <em>O</em>(log&#x2009;(<em>L</em>)) labels are active for a given data point, the cost of sampling these labels from the proposed probabilistic model is <em>O</em>(log&#x2009;(<em>L</em>)2<sup>      <em>M</em>     </sup> + log&#x2009;<sup>2</sup>(<em>L</em>)2<sup>      <em>k</em>     </sup>) which is also tractable for small <em>M</em> and <em>k</em>. Unfortunately, evaluating <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf x})$</span>     </span> for a given <span class="inline-equation"><span class="tex">${\bf x}$</span>     </span> and <span class="inline-equation"><span class="tex">${\bf y}$</span>     </span> is intractable even for small <em>M</em> and <em>k</em> in the most general setting when the leaf node distributions are allowed to not predict any labels and the internal node distributions have a non zero probability of none of the children being traversed. Evaluating <span class="inline-equation"><span class="tex">$\mathbb {P}({y}|{\bf x})$</span>     </span> in this case would require marginalizing over the exponential configurations of <span class="inline-equation"><span class="tex">${\bf z} \in \mathcal {Z}_{{\bf y}}$</span>     </span> which could have led to <span class="inline-equation"><span class="tex">${\bf y}$</span>     </span> being sampled. This can make training intractable. Parabel&#x00A0;can overcome this limitation in 2 ways (see Subsection&#x00A0;<a class="sec" href="#sec-15">3.4</a>). The first is by maximizing a lower bound approximation of the log likelihood. The second is by prohibiting leaf nodes from not predicting any labels and by forcing internal nodes to choose at least 1 child for traversal. The marginalization over <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> would then collapse as only a single configuration of <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> could have led to <span class="inline-equation"><span class="tex">${\bf y}$</span>     </span> being sampled from the model. Note that this restriction implies that labels within a leaf node cannot be independent and that the children of an internal node cannot be selected independently for traversal. The proposed model therefore differs from the probabilistic label tree (PLT)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] which models just the marginal distributions of the individual labels. Nevertheless Parabel&#x00A0;can be reduced to PLT by assuming the lower bound approximation and that labels are independent within a leaf and by also assuming that children are traversed independently of each other. Various other instantiations of Parabel&#x00A0;&#x2019;s probabilistic model can be achieved based on different internal and leaf node distributions. Results for some these models are presented in Section&#x00A0;<a class="sec" href="#sec-17">4</a> and the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. </p>    </section>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Training</h3>     </div>     </header>     <p>     <strong>MAP estimation:</strong> Training Parabel&#x00A0;involves learning the parameters of the internal and leaf node distributions <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf z}|{\bf x})$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{z},{\bf x})$</span>     </span> respectively. Let <span class="inline-equation"><span class="tex">${\bf \Theta }= \lbrace {\bf \Theta }_{\mathcal {I}},{\bf \Theta }_{\mathcal {L}}\rbrace$</span>     </span> denote Parabel&#x00A0;&#x2019;s internal and leaf node distribution parameters and assume a training set of <em>N</em> independent and identically distributed points <span class="inline-equation"><span class="tex">$\lbrace ({\bf x}_i,{\bf y}_i)_{i=1}^N\rbrace$</span>     </span> with <span class="inline-equation"><span class="tex">${\bf x}_i \in \mathbb {R}^D$</span>     </span> and <span class="inline-equation"><span class="tex">${\bf y}_i \in \lbrace 0,1\rbrace ^L$</span>     </span>. Then, based on (<a class="eqn" href="#eq6">6</a>) and Theorem&#x00A0;<a class="enc" href="#enc1">3.1</a>, the maximum <em>a posteriori</em> estimate of <span class="inline-equation"><span class="tex">${\bf \Theta }$</span>     </span> can then be obtained as <div class="table-responsive" id="eq13">      <div class="display-equation">       <span class="tex mytex">\begin{align} {\bf \Theta}^* = \arg\max_{{\bf \Theta}} \mathbb{P}({\bf \Theta})~\prod_{i=1}^N \mathbb{P}({\bf y}_i|{\bf x}_i,{\bf \Theta}) \end{align} </span>       <br/>       <span class="equation-number">(13)</span>      </div>     </div>     <div class="table-responsive" id="eq14">      <div class="display-equation">       <span class="tex mytex">\begin{align} = \arg\max_{\bf \Theta} \mathbb{P}({\bf \Theta})~\prod_{i=1}^N \sum_{\bf z}\mathbb{P}({\bf y}_i|{\bf z},{\bf x}_i,{\bf \Theta})\mathbb{P}({\bf z}|{\bf x}_i,{\bf \Theta})\end{align} </span>       <br/>       <span class="equation-number">(14)</span>      </div>     </div>     <div class="table-responsive" id="eq15">      <div class="display-equation">       <span class="tex mytex">\begin{align} \arg\max_{{\bf \Theta}_{\mathcal{I}},{\bf \Theta}_{\mathcal{L}}} \mathbb{P}({\bf \Theta}_{\mathcal{I}})~\mathbb{P}({\bf \Theta}_{\mathcal{L}})~\prod_{i=1}^N \sum_{{\bf z} \in \mathcal{Z}_{{\bf y}_i}} \Big(\end{align} </span>       <br/>       <span class="equation-number">(15)</span>      </div>     </div>     <div class="table-responsive" id="eq16">      <div class="display-equation">       <span class="tex mytex">\begin{align} \prod_{n \in \mathcal{L}:z_n=1}\mathbb{P}({\bf y}_{in}|z_n=1,{\bf x}_i,{\bf \Theta}_{\mathcal{L}}) \prod_{n \in \mathcal{I}:z_n=1} \mathbb{P}({\bf z}_{\mathcal{C}_n}|z_n=1,{\bf x}_i,{\bf \Theta}_{\mathcal{I}})\Big) \end{align} </span>       <br/>       <span class="equation-number">(16)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\bf y}_{in}$</span>     </span> is a vector such that <em>y<sub>inl</sub>     </em> = 1 if label <em>l</em> in node <em>n</em> is relevant to <span class="inline-equation"><span class="tex">${\bf x}_i$</span>     </span> and 0 otherwise; <em>z<sub>n</sub>     </em> = 1 if node <em>n</em> was traversed and 0 otherwise; and <span class="inline-equation"><span class="tex">$z_{\mathcal {C}_nj}=1$</span>     </span> if child <em>j</em> of node <em>n</em> was traversed and 0 otherwise.</p>     <p>     <strong>Marginalization:</strong> Marginalization over <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> is intractable for arbitrary choices of <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf z},{x},{\bf \Theta })$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf z}|{\bf x},{\bf \Theta })$</span>     </span> as an exponential number of <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> could have been used to generate a given <span class="inline-equation"><span class="tex">${\bf y}_i$</span>     </span>. One way of addressing this limitation is to choose <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf z}|{\bf x},{\bf \Theta })$</span>     </span> such that tree traversal cannot be terminated at an internal node and choose <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf z},{\bf x},{\bf \Theta })$</span>     </span> such that at least one label is selected by each visited leaf. The marginalization over <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> then collapses as only a single <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> could have been used to generate the given <span class="inline-equation"><span class="tex">${\bf y}_i$</span>     </span>. This model is explored in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a> &#x00A0;as it has the side effect of not allowing the child nodes of an internal node to be chosen independently thereby impacting model size. An alternative is to allow general <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf z}|{\bf x},{\bf \Theta })$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf z},{\bf x},{\bf \Theta })$</span>     </span> but then lower bound the marginalization by choosing the sparsest <span class="inline-equation"><span class="tex">${\bf z}$</span>     </span> corresponding to all visited paths terminating at leaf nodes each predicting at least one label. In either case, let <span class="inline-equation"><span class="tex">${\bf z} = {\bf z}_i$</span>     </span> represent the unique or the sparsest path generating <span class="inline-equation"><span class="tex">${\bf y}_i$</span>     </span>. Then, the MAP estimation of (<a class="eqn" href="#eq13">13</a>) reduces to the following two independent optimization problems over the internal and leaf nodes respectively <div class="table-responsive" id="eq17">      <div class="display-equation">       <span class="tex mytex">\begin{align} \min_{{\bf \Theta}_{\mathcal{I}}}~-\log\mathbb{P}({\bf \Theta}_{\mathcal{I}}) -~\sum_{i=1}^N \sum_{n \in \mathcal{I}:z_{in}=1} \log\mathbb{P}({\bf z}_{i\mathcal{C}_n}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{I}})\end{align} </span>       <br/>       <span class="equation-number">(17)</span>      </div>     </div>     <div class="table-responsive" id="eq18">      <div class="display-equation">       <span class="tex mytex">\begin{align} \min_{{\bf \Theta}_{\mathcal{L}}}~-\log\mathbb{P}({\bf \Theta}_{\mathcal{L}}) -~\sum_{i=1}^N \sum_{n \in \mathcal{L}:z_{in}=1} \log\mathbb{P}({\bf y}_{in}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{L}}) \end{align} </span>       <br/>       <span class="equation-number">(18)</span>      </div>     </div> which in turn are themselves comprised of the following independent optimization problems one per each node <div class="table-responsive" id="eq19">      <div class="display-equation">       <span class="tex mytex">\begin{align} \min_{{\bf \Theta}_{\mathcal{I}n}}~-\log\mathbb{P}({\bf \Theta}_{\mathcal{I}n}) -~\sum_{i:z_{in}=1} \log\mathbb{P}({\bf z}_{i\mathcal{C}_n}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{I}n})\end{align} </span>       <br/>       <span class="equation-number">(19)</span>      </div>     </div>     <div class="table-responsive" id="eq20">      <div class="display-equation">       <span class="tex mytex">\begin{align} \min_{{\bf \Theta}_{\mathcal{L}n}}~-\log\mathbb{P}({\bf \Theta}_{\mathcal{L}n}) -~\sum_{i:z_{in}=1} \log\mathbb{P}({\bf y}_{in}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{L}n}) \end{align} </span>       <br/>       <span class="equation-number">(20)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">${\bf \Theta}_{\mathcal{I}}=\{{\bf \Theta}_{\mathcal{I}1},\ldots,{\bf \Theta}_{\mathcal{I}|\mathcal{I}|}\}$</span>     </span> and <span class="inline-equation"><span class="tex">${\bf \Theta}_{\mathcal{L}}=\{{\bf \Theta}_{\mathcal{L}1},\ldots,{\bf \Theta}_{\mathcal{L}|\mathcal{L}|}\}$</span>     </span>.</p>     <p>     <strong>Instantiation:</strong> Different instantiations of the proposed probabilistic model can be obtained by different choices of the internal and leaf node distributions some of which are explored in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. The following specific instantiation is referred to as Parabel&#x00A0;for the reminder of this paper. It is assumed that each internal node <em>n</em> learns <em>k</em> independent linear classifiers parameterized by <span class="inline-equation"><span class="tex">${\bf \Theta}_{\mathcal{I}n} = \{{\bf w}_{n1},\ldots,{\bf w}_{nk}\}$</span>     </span> in order to determine if each of its <em>k</em> children should be traversed or not. It is further assumed that the distribution in leaf node <em>n</em> over the labels contained within, say {1, &#x2026;, <em>M</em>}, corresponds to 1-vs-All linear classifiers parameterized by <span class="inline-equation"><span class="tex">${\bf \Theta}_{\mathcal{L}n} = \{{\bf w}_{n1},\ldots,{\bf w}_{nM}\}$</span>     </span>. Let <em>L</em>(<em>x</em>) = log&#x2009;(1 + exp&#x2009;(&#x2212; <em>x</em>)) denote the logistic loss. Assuming <span class="inline-equation"><span class="tex">$-\log\mathbb{P}({\bf \Theta}_{\mathcal{I}n})=\sum_{j=1}^k \frac{\|{\bf w}_{nj}\|^2}{2C}$, $-\log\mathbb{P}({\bf \Theta}_{\mathcal{L}n})=\sum_{j=1}^M\frac{\|{\bf w}_{nj}\|^2}{2C}$</span>     </span>, <span class="inline-equation"><span class="tex">$-\log\mathbb{P}({\bf z}_{i\mathcal{C}_n}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{I}n}) = -\sum_{j=1}^k \mathscr{L}((2z_{i\mathcal{C}_nj}-1){\bf w}_{nj}^{\top}{\bf x}_i)$</span>     </span>, <span class="inline-equation"><span class="tex">$-\log\mathbb{P}({\bf y}_{in}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{L}n}) = -\sum_{j=1}^M \mathscr{L}((2y_{inj}-1) {\bf w}_{nj}^{\top}{\bf x}_i)$</span>     </span> and <span class="inline-equation"><span class="tex">$-\log\mathbb{P}({\bf y}_{in}|z_{in}=1,{\bf x}_i,{\bf \Theta}_{\mathcal{L}n}) = -\sum_{j=1}^M \mathscr{L}((2y_{inj}-1){\bf w}_{nj}^{\top}{\bf x}_i)$</span>     </span> leads to the following independent optimization problems <div class="table-responsive" id="eq21">      <div class="display-equation">       <span class="tex mytex">\begin{align} \min_{{\bf w}_{nj}} \frac{1}{2}\|{\bf w}_{nj}\|^2 + C\sum_{i:z_{in}=1} \mathscr{L}((2z_{i\mathcal{C}_nj}-1){\bf w}_{nj}^{\top}{\bf x}_i)\end{align} </span>       <br/>       <span class="equation-number">(21)</span>      </div>     </div>      <div class="table-responsive" id="eq22">      <div class="display-equation">       <span class="tex mytex">\begin{align} \min_{{\bf w}_{nj}} \frac{1}{2}\|{\bf w}_{nj}\|^2 + C\sum_{i:z_{in}=1} \mathscr{L}((2y_{inj}-1){\bf w}_{nj}^{\top}{\bf x}_i) \end{align} </span>       <br/>       <span class="equation-number">(22)</span>      </div>     </div> comprising a separate problem for each pair of an internal node and its child node and each pair of a leaf node and its label. Note that each of these optimization problems is convex and therefore Parabel&#x00A0;&#x2019;s overall optimization problem is also convex and can be optimized efficiently.</p>     <p>This instantiation was found to have the best trade off between training time, prediction time, model size and prediction accuracy, particularly for low dimensional dense features obtained by deep learning. However, for high dimensional, sparse, bag-of-words features, it was found that replacing the log loss in (<a class="eqn" href="#eq21">21</a>) and (<a class="eqn" href="#eq22">22</a>) by the squared hinge loss resulted in even higher prediction accuracy though the loss does not arise from a valid probability distribution.</p>     <p>     <strong>Efficient training:</strong> Training starts by learning a <em>k</em> = 2-ary label tree such that each leaf node contains at most <em>M</em> = 100 labels (see Subsection&#x00A0;<a class="sec" href="#sec-13">3.2</a>). This has computational complexity <span class="inline-equation"><span class="tex">$O(kL\log (L)\hat{D})$</span>     </span> where <span class="inline-equation"><span class="tex">$\hat{D}$</span>     </span> is the average feature sparsity (<span class="inline-equation"><span class="tex">$\hat{D}=D$</span>     </span> for dense features). Given the learnt label tree and training point <em>i</em>, <span class="inline-equation"><span class="tex">${\bf z}_i$</span>     </span> is obtained by connecting the root to all the leaves containing any label relevant to point <em>i</em>. A point can therefore traverse multiple child nodes at any given internal node. Therefore at every internal node, according to (<a class="eqn" href="#eq21">21</a>), each child trains an independent linear classifier to separate the training points reaching the child from the training points reaching only its siblings. Now the computational complexity of training all internal node classifiers is <span class="inline-equation"><span class="tex">$O(Nk\log ^2(L)\hat{D})$</span>     </span> where it has been assumed that <em>O</em>(log&#x2009;(<em>L</em>)) labels are relevant to a data point on average. An independent linear classifier is also trained for each label in a leaf separating the training points containing the label from the other training points reaching the leaf according to (<a class="eqn" href="#eq22">22</a>). The computational complexity of training all leaf node classifiers is therefore <span class="inline-equation"><span class="tex">$O(NM\log (L)\hat{D})$</span>     </span> with <em>M</em> being typically larger than <em>k</em>log&#x2009;(<em>L</em>). Both internal and leaf node classifiers can be learnt efficiently by optimizing (<a class="eqn" href="#eq21">21</a>) and (<a class="eqn" href="#eq22">22</a>) by Liblinear&#x0027;s&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] co-ordinate ascent algorithm in the dual space. Parabel&#x00A0;&#x2019;s overall complexity is therefore <span class="inline-equation"><span class="tex">$O(NM\log (L)\hat{D})$</span>     </span> which can be orders of magnitude lower than that of other 1-vs-All approaches. For instance, DiSMEC&#x0027;s training complexity is <span class="inline-equation"><span class="tex">$O(NL\hat{D})$</span>     </span> as each of its 1-vs-All classifiers are trained on all <em>N</em> points. In contrast, Parabel&#x00A0;&#x2019;s 1-vs-All leaf node classifiers are trained on only <span class="inline-equation"><span class="tex">$O(NM\log (L)\hat{D}/L)$</span>     </span> points on average. Like Parabel&#x00A0;, PD-Sparse and PPDSparse also reduce the number of negative training points for each 1-vs-All classifier but result in much poorer prediction accuracies for an equivalent number of negatives. Furthermore, PPDSparse&#x0027;s negative sampling heuristic is heavily dependent on feature sparsity and is not effective for low-dimensional dense deep learning features.</p>     <p>Finally, note that Parabel&#x00A0;is embarrassingly parallelizable as each of the problems in (<a class="eqn" href="#eq21">21</a>) and (<a class="eqn" href="#eq22">22</a>) can be optimized independently. This can make Parabel&#x00A0;&#x2019;s training even faster than the single core implementation numbers reported in the experiments.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.5</span> Prediction</h3>     </div>     </header>     <p>Predictions need to be made very efficiently in many extreme classification applications. This makes previous 1-vs-All approaches infeasible as their prediction complexity is linear in the number of labels. Parabel&#x00A0;&#x2019;s prediction complexity is logarithmic in the number of labels and it can make predictions in milliseconds even on the largest datasets. Parabel&#x00A0;achieves this by optimizing its prediction algorithm for metrics such as precision@<em>k</em> and nDCG@<em>k</em> which are defined over the top ranked relevant predictions alone. Such metrics are commonly used to evaluate extreme classification algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] as they align better with real-world ranking, recommendation and tagging applications, including DSA, as compared to traditional metrics such as the subset zero-one loss and the hamming loss.</p>     <p>     <strong>Algorithm:</strong> Parabel&#x00A0;predicts a ranking of relevant labels by sorting the marginal label probabilities <span class="inline-equation"><span class="tex">$\mathbb {P}(y_l=1|{\bf x})$</span>     </span> in decreasing order of magnitude. Evaluating <span class="inline-equation"><span class="tex">$\mathbb {P}(y_l|{\bf x})$</span>     </span> for all labels would incur <em>O</em>(<em>L</em>) costs. Fortunately, it is not necessary to evaluate <span class="inline-equation"><span class="tex">$\mathbb {P}(y_l=1|{\bf x})$</span>     </span> for all labels since precision@<em>k</em> and nDCG@<em>k</em> are evaluated over just the top <em>k</em> relevant predictions. Parabel&#x00A0;therefore reduces its prediction cost by focussing on getting just the top <em>k</em> ranking right.</p>     <p>A novel point <span class="inline-equation"><span class="tex">${\bf x}$</span>     </span> is passed down Parabel&#x00A0;&#x2019;s label tree by starting at the root and applying internal node classifiers indicating which of the node&#x0027;s children should be traversed. The procedure is recursed until a set of leaf nodes is reached. Upon reaching leaf node <em>n</em>, the marginal probabilities of all the labels in that leaf node can be evaluated based on the following theorem</p>     <div class="theorem" id="enc2">     <Label>Theorem 3.2.</Label>     <p> Given a joint probability distribution <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf x})$</span>      </span> defined as in&#x00A0;(<a class="eqn" href="#eq6">6</a>) over a label tree, and a label <em>l</em> in a leaf node <em>n</em>, the marginal probability of label <em>l</em> being relevant to a data point with feature vector <span class="inline-equation"><span class="tex">${\bf x}$</span>      </span> is given by <div class="table-responsive" id="Xeq1">       <div class="display-equation">        <span class="tex mytex">\begin{equation} \mathbb {P}(y_l=1|{\bf x}) = \mathbb {P}(y_l=1|z_n=1,{\bf x})\prod _{\hat{n} \in \mathcal {A}_n}\mathbb {P}(z_{\hat{n}}=1|z_{\mathcal {P}_{\hat{n}}}=1,{x}) \end{equation} </span>        <br/>        <span class="equation-number">(23)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$\mathcal {A}_n$</span>      </span> is the set of ancestors of node <em>n</em> apart from the root and <span class="inline-equation"><span class="tex">$\mathcal {P}_{\hat{n}}$</span>      </span> is the parent of <span class="inline-equation"><span class="tex">$\hat{n}$</span>      </span>.</p>     </div>     <div class="proof" id="proof2">     <Label>Proof.</Label>     <p> The proof is available in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. </p>     </div>     <p>The first term in the expression is the marginal probability of label <em>l</em> being relevant to point <span class="inline-equation"><span class="tex">${\bf x}$</span>     </span> as returned by the leaf node classifier while all the probabilities in the product over the ancestors are generated by the internal node classifiers. An aggregate score for each predicted label <em>l</em> is then generated by averaging the label&#x0027;s marginal probability across all the trees in the ensemble as <span class="inline-equation"><span class="tex">$s_l = \frac{1}{T}\sum _{t=1}^T \mathbb {P}_t(y_l=1|{\bf x})$</span>     </span> where <span class="inline-equation"><span class="tex">$\mathbb {P}_t$</span>     </span> is the probability returned by tree <em>t</em>. The labels with the top <em>k</em> scores are finally predicted.</p>     <p>     <strong>Beam search:</strong> The proposed prediction algorithm could have <em>O</em>(<em>L</em>) complexity in the worst case as all paths could potentially be traversed. Parabel&#x00A0;addresses this limitation by traversing at most <em>P</em> paths in a breadth-first manner. The top <em>P</em> paths with the highest path probabilities are greedily retained while going from one tree level to the next. The value <em>P</em> = 2<em>k</em> was empirically found to work well for top <em>k</em> metrics such as precision@<em>k</em> and nDCG@<em>k</em> since paths outside the beam search have low probabilities of contributing labels with ranks less than or equal to <em>k</em>. The computational complexity of the tree traversal is therefore reduced to <span class="inline-equation"><span class="tex">$O(\log (\frac{L}{M})(2k)\hat{D})$</span>     </span> while the computational complexity of evaluating the leaf node classifiers is <span class="inline-equation"><span class="tex">$O(M(2k)\hat{D})$</span>     </span>.</p>     <p>     <strong>Optimality:</strong> Parabel&#x00A0;&#x2019;s prediction algorithm can be shown to be optimal (modulo beam search) for evaluation metrics which decompose over individual labels and which can be computed over the top <em>k</em> relevant predictions alone. This is based on the observation that the marginal label probabilities are sufficient for optimizing such metrics and is illustrated in the specific case of precision@<em>k</em> and nDCG@<em>k</em> by the following theorem:</p>     <div class="theorem" id="enc3">     <Label>Theorem 3.3.</Label>     <p> Let <span class="inline-equation"><span class="tex">$\mathbb {P}({\bf y}|{\bf x})$</span>      </span> be a joint probability distribution that a label subset <span class="inline-equation"><span class="tex">${\bf y}$</span>      </span> is relevant to <span class="inline-equation"><span class="tex">${\bf x}$</span>      </span>. Then for a given test point <span class="inline-equation"><span class="tex">${\bf x}$</span>      </span>, the optimal ranking of labels <span class="inline-equation"><span class="tex">${\bf r}({\bf x}) \in \Pi (L)$</span>      </span> which maximizes the expected gain over the above distribution as measured in terms of precision@<em>k</em> or nDCG@<em>k</em> is given by sorting the labels according to the decreasing order of marginal probability of relevance to <span class="inline-equation"><span class="tex">${\bf x}$</span>      </span>, <em>i.e</em>      <span class="inline-equation"><span class="tex">$\mathbb {P}(y_l=1|{\bf x})$</span>      </span>, as follows: <div class="table-responsive" id="Xeq2">       <div class="display-equation">        <span class="tex mytex">\begin{equation} {r}({\bf x}) = Rank(\lbrace \mathbb {P}(y_l=1|{\bf x})\rbrace _{l=1}^L) \end{equation} </span>        <br/>        <span class="equation-number">(24)</span>       </div>      </div>     </p>     </div>     <div class="proof" id="proof3">     <Label>Proof.</Label>     <p> The proof is available in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. </p>     </div>    </section>   </section>   <section id="sec-17">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Experiments</h2>     </div>    </header>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Dataset statistics</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:center;">Dataset</th>       <th style="text-align:center;">Train</th>       <th style="text-align:center;">Features</th>       <th style="text-align:center;">Labels</th>       <th style="text-align:center;">Test</th>       <th style="text-align:center;">Avg. labels</th>       <th style="text-align:center;">Avg. points</th>      </tr>      <tr>       <th style="text-align:left;"/>       <th style="text-align:center;">        <em>N</em>       </th>       <th style="text-align:center;">        <em>D</em>       </th>       <th style="text-align:center;">        <em>L</em>       </th>       <th style="text-align:center;">        <em>M</em>       </th>       <th style="text-align:center;">per point</th>       <th style="text-align:center;">per label</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">EURLex-4K</td>       <td style="text-align:right;">15,539</td>       <td style="text-align:right;">5,000</td>       <td style="text-align:right;">3,993</td>       <td style="text-align:right;">3,809</td>       <td style="text-align:right;">5.31</td>       <td style="text-align:right;">448.57</td>      </tr>      <tr>       <td style="text-align:left;">WikiLSHTC-325K</td>       <td style="text-align:right;">1,778,351</td>       <td style="text-align:right;">1,617,899</td>       <td style="text-align:right;">325,056</td>       <td style="text-align:right;">587,084</td>       <td style="text-align:right;">3.26</td>       <td style="text-align:right;">23.74</td>      </tr>      <tr>       <td style="text-align:left;">Wiki-500K</td>       <td style="text-align:right;">1,813,391</td>       <td style="text-align:right;">2,381,304</td>       <td style="text-align:right;">501,070</td>       <td style="text-align:right;">783,743</td>       <td style="text-align:right;">4.77</td>       <td style="text-align:right;">24.75</td>      </tr>      <tr>       <td style="text-align:left;">Amazon-670K</td>       <td style="text-align:right;">490,449</td>       <td style="text-align:right;">135,909</td>       <td style="text-align:right;">670,091</td>       <td style="text-align:right;">153,025</td>       <td style="text-align:right;">5.38</td>       <td style="text-align:right;">5.17</td>      </tr>      <tr>       <td style="text-align:left;">Amazon-3M</td>       <td style="text-align:right;">1,717,899</td>       <td style="text-align:right;">337,067</td>       <td style="text-align:right;">2,812,281</td>       <td style="text-align:right;">742,507</td>       <td style="text-align:right;">36.17</td>       <td style="text-align:right;">31.64</td>      </tr>      <tr>       <td style="text-align:left;">Ads-2M</td>       <td style="text-align:right;">11,966,195</td>       <td style="text-align:right;">4,091,864</td>       <td style="text-align:right;">2,078,535</td>       <td style="text-align:right;">2,988,996</td>       <td style="text-align:right;">2.57</td>       <td style="text-align:right;">14.82</td>      </tr>      <tr>       <td style="text-align:left;">Ads-7M</td>       <td style="text-align:right;">9,042,996</td>       <td style="text-align:right;">3,977,303</td>       <td style="text-align:right;">6,969,674</td>       <td style="text-align:right;">2,261,297</td>       <td style="text-align:right;">14.75</td>       <td style="text-align:right;">19.13</td>      </tr>     </tbody>     </table>    </div>    <div class="table-responsive" id="tab2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Parabel&#x00A0;is significantly faster at training and prediction than the state-of-the-art while almost achieving leading precision@<em>k</em> values. Results are reported for Parabel&#x00A0;with <em>T</em> = 1 and <em>T</em> = 3 trees trained using the log loss (l), the squared hinge loss (s).</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:left;">Method</th>       <th style="text-align:center;">&#x00A0;P1 (%)&#x00A0;</th>       <th style="text-align:center;">&#x00A0;P3 (%)&#x00A0;</th>       <th style="text-align:center;">&#x00A0;P5 (%)&#x00A0;</th>       <th style="text-align:center;">Training</th>       <th style="text-align:center;">Test time</th>       <th style="text-align:center;">Model</th>      </tr>      <tr>       <th style="text-align:left;"/>       <th style="text-align:center;"/>       <th style="text-align:center;"/>       <th style="text-align:center;"/>       <th style="text-align:center;">time (hr)</th>       <th style="text-align:center;">/ point (ms)</th>       <th style="text-align:center;">size (GB)</th>      </tr>     </thead>     <tbody>      <tr>       <td colspan="7" style="text-align:center;">        <strong>EURLex-4K</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">75.45</td>       <td style="text-align:center;">62.70</td>       <td style="text-align:center;">52.51</td>       <td style="text-align:center;">0.087</td>       <td style="text-align:center;">3.92</td>       <td style="text-align:center;">0.41</td>      </tr>      <tr>       <td style="text-align:left;">SLEEC</td>       <td style="text-align:center;">79.26</td>       <td style="text-align:center;">64.30</td>       <td style="text-align:center;">52.33</td>       <td style="text-align:center;">0.062</td>       <td style="text-align:center;">7.57</td>       <td style="text-align:center;">0.13</td>      </tr>      <tr>       <td style="text-align:left;">LEML</td>       <td style="text-align:center;">63.40</td>       <td style="text-align:center;">50.35</td>       <td style="text-align:center;">41.28</td>       <td style="text-align:center;">0.64</td>       <td style="text-align:center;">3.53</td>       <td style="text-align:center;">0.035</td>      </tr>      <tr>       <td style="text-align:left;">WSABIE</td>       <td style="text-align:center;">68.55</td>       <td style="text-align:center;">55.11</td>       <td style="text-align:center;">45.12</td>       <td style="text-align:center;">0.20</td>       <td style="text-align:center;">0.39</td>       <td style="text-align:center;">0.018</td>      </tr>      <tr>       <td style="text-align:left;">CPLST</td>       <td style="text-align:center;">72.28</td>       <td style="text-align:center;">58.16</td>       <td style="text-align:center;">47.73</td>       <td style="text-align:center;">2.20</td>       <td style="text-align:center;">6.82</td>       <td style="text-align:center;">0.018</td>      </tr>      <tr>       <td style="text-align:left;">CS</td>       <td style="text-align:center;">58.52</td>       <td style="text-align:center;">45.51</td>       <td style="text-align:center;">32.47</td>       <td style="text-align:center;">1.52</td>       <td style="text-align:center;">6.71</td>       <td style="text-align:center;">0.018</td>      </tr>      <tr>       <td style="text-align:left;">PD-Sparse</td>       <td style="text-align:center;">76.43</td>       <td style="text-align:center;">60.37</td>       <td style="text-align:center;">49.72</td>       <td style="text-align:center;">0.041</td>       <td style="text-align:center;">        <strong>0.12</strong>       </td>       <td style="text-align:center;">0.31</td>      </tr>      <tr>       <td style="text-align:left;">DiSMEC</td>       <td style="text-align:center;">83.67</td>       <td style="text-align:center;">70.70</td>       <td style="text-align:center;">59.14</td>       <td style="text-align:center;">0.094</td>       <td style="text-align:center;">7.05</td>       <td style="text-align:center;">0.04</td>      </tr>      <tr>       <td style="text-align:left;">PPDSparse</td>       <td style="text-align:center;">        <strong>83.83</strong>       </td>       <td style="text-align:center;">        <strong>70.72</strong>       </td>       <td style="text-align:center;">        <strong>59.21</strong>       </td>       <td style="text-align:center;">        <strong>0.015</strong>       </td>       <td style="text-align:center;">1.14</td>       <td style="text-align:center;">0.065</td>      </tr>      <tr>       <td style="text-align:left;">XML-CNN</td>       <td style="text-align:center;">76.38</td>       <td style="text-align:center;">62.81</td>       <td style="text-align:center;">51.41</td>       <td style="text-align:center;">0.28</td>       <td style="text-align:center;">0.18</td>       <td style="text-align:center;">0.017</td>      </tr>      <tr>       <td style="text-align:left;">PLT</td>       <td style="text-align:center;">76.58</td>       <td style="text-align:center;">62.99</td>       <td style="text-align:center;">52.16</td>       <td style="text-align:center;">1.30</td>       <td style="text-align:center;">8.64</td>       <td style="text-align:center;">        <strong>0.012</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-l-T=3</td>       <td style="text-align:center;">81.65</td>       <td style="text-align:center;">68.58</td>       <td style="text-align:center;">57.60</td>       <td style="text-align:center;">0.031</td>       <td style="text-align:center;">1.38</td>       <td style="text-align:center;">0.035</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">80.20</td>       <td style="text-align:center;">67.88</td>       <td style="text-align:center;">56.69</td>       <td style="text-align:center;">0.005</td>       <td style="text-align:center;">0.57</td>       <td style="text-align:center;">0.0087</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">81.94</td>       <td style="text-align:center;">69.01</td>       <td style="text-align:center;">57.78</td>       <td style="text-align:center;">0.016</td>       <td style="text-align:center;">1.2</td>       <td style="text-align:center;">0.026</td>      </tr>      <tr>       <td colspan="7" style="text-align:center;">        <strong>WikiLSHTC-325K</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">56.05</td>       <td style="text-align:center;">36.79</td>       <td style="text-align:center;">27.09</td>       <td style="text-align:center;">7.42</td>       <td style="text-align:center;">1.80</td>       <td style="text-align:center;">9.37</td>      </tr>      <tr>       <td style="text-align:left;">SLEEC</td>       <td style="text-align:center;">54.83</td>       <td style="text-align:center;">33.42</td>       <td style="text-align:center;">23.85</td>       <td style="text-align:center;">18.65</td>       <td style="text-align:center;">5.67</td>       <td style="text-align:center;">4.39</td>      </tr>      <tr>       <td style="text-align:left;">PD-Sparse</td>       <td style="text-align:center;">61.26</td>       <td style="text-align:center;">39.48</td>       <td style="text-align:center;">28.79</td>       <td style="text-align:center;">38.67</td>       <td style="text-align:center;">        <strong>0.17</strong>       </td>       <td style="text-align:center;">0.69</td>      </tr>      <tr>       <td style="text-align:left;">DiSMEC</td>       <td style="text-align:center;">        <strong>64.94</strong>       </td>       <td style="text-align:center;">42.71</td>       <td style="text-align:center;">31.5</td>       <td style="text-align:center;">749.43</td>       <td style="text-align:center;">2621.55</td>       <td style="text-align:center;">3.79</td>      </tr>      <tr>       <td style="text-align:left;">PPDSparse</td>       <td style="text-align:center;">64.08</td>       <td style="text-align:center;">41.26</td>       <td style="text-align:center;">30.12</td>       <td style="text-align:center;">3.93</td>       <td style="text-align:center;">37.76</td>       <td style="text-align:center;">5.14</td>      </tr>      <tr>       <td style="text-align:left;">PLT</td>       <td style="text-align:center;">45.67</td>       <td style="text-align:center;">29.13</td>       <td style="text-align:center;">21.95</td>       <td style="text-align:center;">9.91</td>       <td style="text-align:center;">1.37</td>       <td style="text-align:center;">        <strong>0.52</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-l-T=3</td>       <td style="text-align:center;">64.78</td>       <td style="text-align:center;">42.91</td>       <td style="text-align:center;">31.61</td>       <td style="text-align:center;">0.72</td>       <td style="text-align:center;">1.31</td>       <td style="text-align:center;">5.08</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">62.59</td>       <td style="text-align:center;">41.16</td>       <td style="text-align:center;">30.33</td>       <td style="text-align:center;">        <strong>0.29</strong>       </td>       <td style="text-align:center;">0.94</td>       <td style="text-align:center;">1.06</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">64.54</td>       <td style="text-align:center;">        <strong>42.92</strong>       </td>       <td style="text-align:center;">        <strong>31.89</strong>       </td>       <td style="text-align:center;">0.86</td>       <td style="text-align:center;">2.02</td>       <td style="text-align:center;">3.17</td>      </tr>      <tr>       <td colspan="7" style="text-align:center;">        <strong>DSA-7M</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">28.09</td>       <td style="text-align:center;">25.79</td>       <td style="text-align:center;">23.21</td>       <td style="text-align:center;">306</td>       <td style="text-align:center;">23</td>       <td style="text-align:center;">410</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">30.67</td>       <td style="text-align:center;">28.26</td>       <td style="text-align:center;">25.26</td>       <td style="text-align:center;">        <strong>27.83</strong>       </td>       <td style="text-align:center;">        <strong>3.21</strong>       </td>       <td style="text-align:center;">        <strong>14.08</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">        <strong>32.67</strong>       </td>       <td style="text-align:center;">        <strong>30.13</strong>       </td>       <td style="text-align:center;">        <strong>27.23</strong>       </td>       <td style="text-align:center;">80.10</td>       <td style="text-align:center;">9.30</td>       <td style="text-align:center;">42.12</td>      </tr>      <tr>       <td colspan="7" style="text-align:center;">        <strong>Wiki-500K</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">59.52</td>       <td style="text-align:center;">40.24</td>       <td style="text-align:center;">30.72</td>       <td style="text-align:center;">49.24</td>       <td style="text-align:center;">7.72</td>       <td style="text-align:center;">63.59</td>      </tr>      <tr>       <td style="text-align:left;">DiSMEC</td>       <td style="text-align:center;">        <strong>70.20</strong>       </td>       <td style="text-align:center;">        <strong>50.60</strong>       </td>       <td style="text-align:center;">        <strong>39.70</strong>       </td>       <td style="text-align:center;">7495.75</td>       <td style="text-align:center;">9355</td>       <td style="text-align:center;">14.76</td>      </tr>      <tr>       <td style="text-align:left;">PPDSparse</td>       <td style="text-align:center;">70.16</td>       <td style="text-align:center;">50.57</td>       <td style="text-align:center;">39.66</td>       <td style="text-align:center;">28.53</td>       <td style="text-align:center;">123.7</td>       <td style="text-align:center;">3.99</td>      </tr>      <tr>       <td style="text-align:left;">XML-CNN</td>       <td style="text-align:center;">59.85</td>       <td style="text-align:center;">39.28</td>       <td style="text-align:center;">29.81</td>       <td style="text-align:center;">117.23</td>       <td style="text-align:center;">23.20</td>       <td style="text-align:center;">3.71</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-l-T=3</td>       <td style="text-align:center;">67.83</td>       <td style="text-align:center;">48.30</td>       <td style="text-align:center;">37.58</td>       <td style="text-align:center;">9.73</td>       <td style="text-align:center;">4.73</td>       <td style="text-align:center;">15.87</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">66.32</td>       <td style="text-align:center;">47.02</td>       <td style="text-align:center;">36.47</td>       <td style="text-align:center;">        <strong>2.76</strong>       </td>       <td style="text-align:center;">        <strong>4.69</strong>       </td>       <td style="text-align:center;">        <strong>1.97</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">68.22</td>       <td style="text-align:center;">48.85</td>       <td style="text-align:center;">38.09</td>       <td style="text-align:center;">8.06</td>       <td style="text-align:center;">7.01</td>       <td style="text-align:center;">5.93</td>      </tr>      <tr>       <td colspan="7" style="text-align:center;">        <strong>Amazon-670K</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">39.46</td>       <td style="text-align:center;">35.81</td>       <td style="text-align:center;">33.05</td>       <td style="text-align:center;">3.32</td>       <td style="text-align:center;">4.75</td>       <td style="text-align:center;">9.80</td>      </tr>      <tr>       <td style="text-align:left;">SLEEC</td>       <td style="text-align:center;">35.05</td>       <td style="text-align:center;">31.25</td>       <td style="text-align:center;">28.56</td>       <td style="text-align:center;">11.33</td>       <td style="text-align:center;">18.51</td>       <td style="text-align:center;">7.08</td>      </tr>      <tr>       <td style="text-align:left;">DiSMEC</td>       <td style="text-align:center;">        <strong>45.37</strong>       </td>       <td style="text-align:center;">        <strong>40.40</strong>       </td>       <td style="text-align:center;">        <strong>36.96</strong>       </td>       <td style="text-align:center;">372.66</td>       <td style="text-align:center;">1414.15</td>       <td style="text-align:center;">3.75</td>      </tr>      <tr>       <td style="text-align:left;">PPDSparse</td>       <td style="text-align:center;">45.32</td>       <td style="text-align:center;">40.37</td>       <td style="text-align:center;">36.92</td>       <td style="text-align:center;">1.71</td>       <td style="text-align:center;">66.09</td>       <td style="text-align:center;">6.00</td>      </tr>      <tr>       <td style="text-align:left;">XML-CNN</td>       <td style="text-align:center;">35.39</td>       <td style="text-align:center;">31.93</td>       <td style="text-align:center;">29.32</td>       <td style="text-align:center;">52.23</td>       <td style="text-align:center;">16.18</td>       <td style="text-align:center;">1.49</td>      </tr>      <tr>       <td style="text-align:left;">PLT</td>       <td style="text-align:center;">36.65</td>       <td style="text-align:center;">32.12</td>       <td style="text-align:center;">28.85</td>       <td style="text-align:center;">3.25</td>       <td style="text-align:center;">1.71</td>       <td style="text-align:center;">0.76</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-l-T=3</td>       <td style="text-align:center;">44.00</td>       <td style="text-align:center;">39.41</td>       <td style="text-align:center;">36.03</td>       <td style="text-align:center;">0.69</td>       <td style="text-align:center;">1.81</td>       <td style="text-align:center;">3.58</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">43.22</td>       <td style="text-align:center;">37.95</td>       <td style="text-align:center;">33.99</td>       <td style="text-align:center;">        <strong>0.12</strong>       </td>       <td style="text-align:center;">        <strong>0.68</strong>       </td>       <td style="text-align:center;">        <strong>0.69</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">44.84</td>       <td style="text-align:center;">39.75</td>       <td style="text-align:center;">35.93</td>       <td style="text-align:center;">0.36</td>       <td style="text-align:center;">1.61</td>       <td style="text-align:center;">2.08</td>      </tr>      <tr>       <td colspan="7" style="text-align:center;">        <strong>Amazon-3M</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">43.83</td>       <td style="text-align:center;">41.81</td>       <td style="text-align:center;">40.09</td>       <td style="text-align:center;">15.74</td>       <td style="text-align:center;">4.05</td>       <td style="text-align:center;">36.79</td>      </tr>      <tr>       <td style="text-align:left;">DiSMEC</td>       <td style="text-align:center;">        <strong>47.77</strong>       </td>       <td style="text-align:center;">        <strong>44.96</strong>       </td>       <td style="text-align:center;">        <strong>42.80</strong>       </td>       <td style="text-align:center;">4955.24</td>       <td style="text-align:center;">16430</td>       <td style="text-align:center;">39.71</td>      </tr>      <tr>       <td style="text-align:left;">PPDSparse</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;"> &#x2248; 3406.00</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">-</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-l-T=3</td>       <td style="text-align:center;">43.16</td>       <td style="text-align:center;">40.61</td>       <td style="text-align:center;">38.67</td>       <td style="text-align:center;">10.37</td>       <td style="text-align:center;">1.78</td>       <td style="text-align:center;">65.95</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">45.72</td>       <td style="text-align:center;">42.89</td>       <td style="text-align:center;">40.72</td>       <td style="text-align:center;">        <strong>1.62</strong>       </td>       <td style="text-align:center;">        <strong>0.83</strong>       </td>       <td style="text-align:center;">        <strong>10.75</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">47.25</td>       <td style="text-align:center;">44.38</td>       <td style="text-align:center;">42.23</td>       <td style="text-align:center;">4.81</td>       <td style="text-align:center;">1.91</td>       <td style="text-align:center;">32.27</td>      </tr>      <tr>       <td colspan="7" style="text-align:center;">        <strong>DSA-2M</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">PfastreXML</td>       <td style="text-align:center;">28.52</td>       <td style="text-align:center;">17.05</td>       <td style="text-align:center;">12.5</td>       <td style="text-align:center;">431.53</td>       <td style="text-align:center;">7.51</td>       <td style="text-align:center;">417.5</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=1</td>       <td style="text-align:center;">31.68</td>       <td style="text-align:center;">19.47</td>       <td style="text-align:center;">14.35</td>       <td style="text-align:center;">        <strong>7.78</strong>       </td>       <td style="text-align:center;">        <strong>1.92</strong>       </td>       <td style="text-align:center;">        <strong>9.47</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">        <strong>34.47</strong>       </td>       <td style="text-align:center;">        <strong>21.07</strong>       </td>       <td style="text-align:center;">        <strong>15.53</strong>       </td>       <td style="text-align:center;">23.35</td>       <td style="text-align:center;">5.73</td>       <td style="text-align:center;">28.39</td>      </tr>     </tbody>     </table>    </div>    <div class="table-responsive" id="tab4">     <div class="table-caption">     <span class="table-number">Table 3:</span>     <span class="table-title">Alternative choices of Parabel&#x00A0;&#x2019;s components leads to worse performance. Please see the main text for details.</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:left;">Method</th>       <th style="text-align:center;">EURLex-4K</th>       <th style="text-align:center;">WikiLSHTC-325K</th>       <th style="text-align:center;">Amazon-670K</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">Weak leaf</td>       <td style="text-align:center;">36.71</td>       <td style="text-align:center;">22.40</td>       <td style="text-align:center;">30.74</td>      </tr>      <tr>       <td style="text-align:left;">Weak internal</td>       <td style="text-align:center;">50.06</td>       <td style="text-align:center;">22.44</td>       <td style="text-align:center;">27.53</td>      </tr>      <tr>       <td style="text-align:left;">PLT tree</td>       <td style="text-align:center;">55.70</td>       <td style="text-align:center;">21.73</td>       <td style="text-align:center;">28.58</td>      </tr>      <tr>       <td style="text-align:left;">HOMER features</td>       <td style="text-align:center;">57.18</td>       <td style="text-align:center;">23.99</td>       <td style="text-align:center;">32.03</td>      </tr>      <tr>       <td style="text-align:left;">Parabel&#x00A0;-s-T=3</td>       <td style="text-align:center;">        <strong>57.78</strong>       </td>       <td style="text-align:center;">        <strong>31.89</strong>       </td>       <td style="text-align:center;">        <strong>35.93</strong>       </td>      </tr>     </tbody>     </table>    </div>    <div class="table-responsive" id="tab5">     <div class="table-caption">     <span class="table-number">Table 4:</span>     <span class="table-title">The relative improvement of Parabel&#x00A0;over BM25 on a live deployment of Dynamic Search Advertising on Bing.</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:left;">Algorithm</th>       <th style="text-align:center;">Relative</th>       <th style="text-align:center;">Relative</th>       <th style="text-align:center;">Relative</th>       <th style="text-align:center;">Relative</th>      </tr>      <tr>       <th style="text-align:left;"/>       <th style="text-align:center;">Ad Recall (%)</th>       <th style="text-align:center;">CTR (%)</th>       <th style="text-align:center;">BR (%)</th>       <th style="text-align:center;">QOA (%)</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">Parabel&#x00A0;</td>       <td style="text-align:center;">420</td>       <td style="text-align:center;">120</td>       <td style="text-align:center;">68</td>       <td style="text-align:center;">104</td>      </tr>     </tbody>     </table>    </div>    <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185998/images/www2018-7-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Parabel&#x00A0;can the quantity, quality and diversity of predicted queries from ad landing pages for DSA on Bing.</span>     </div>    </figure>    <p>     <strong>Datasets:</strong> Experiments were carried out on datasets containing up to 12 million training points, 4 million dimensional features and 7 million labels (see Table&#x00A0;<a class="tbl" href="#tab1">1</a> for dataset statistics). The applications considered include tagging Wikipedia articles (Wiki-500K&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] and WikiLSHTC-325K&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>]), item-to-item recommendation of Amazon products (Amazon-3M&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>] and Amazon-670K&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>]), relevant query prediction for a given ad landing page (DSA-2M and DSA-7M) and document tagging (EUR-Lex-4K&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]). All datasets, apart from DSA-2M and DSA-7M, can be downloaded from The Extreme Classification Repository&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. The DSA datasets were created by mining the Bing logs. Each ad landing page was represented by a bag-of-words feature vector and the subset of 2M/7M queries that led to a click on the page became its labels.</p>    <p>     <strong>Algorithms:</strong> Parabel&#x00A0;was compared to a number of leading extreme classification algorithms including DiSMEC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], PPDSparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>], PD-Sparse&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>] and XML-CNN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>] (1-vs-All based approaches), PfastreXML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] and PLT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] (tree based approaches) and LEML&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>], WSABIE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], CPLST&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>], CS&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] and SLEEC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] (embedding based approaches). All algorithms were trained on the bag-of-words feature representation provided on The Extreme Classification Repository apart from XML-CNN which is a deep learning method and which learns its own features from the raw text directly.</p>    <p>The implementations of all algorithms were provided by their authors apart from CPLST and CS. These algorithms were implemented by us while ensuring that the published results could be reproduced. Results have been reported for only those datasets to which an implementation scales. In addition, results have not been reported for XML-CNN on WikiLSHTC as the raw text was not available.</p>    <p>Parabel&#x00A0;&#x2019;s implementation normalizes the features to unit L2-norm so as to help deal with documents of different length. Results are reported for internal and leaf classifiers trained using both the log loss and the squared hinge loss. All classifier weights less than 0.5 for log loss and 0.1 for squared hinge loss are clipped to 0 as recommended in DiSMEC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. The log loss was found to be better suited than the squared hinge loss for bag-of-words features evaluated using propensity-scored metrics and for deep learning features using standard metrics (please see the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>) while the squared hinge loss worked better for bag-of-words features using standard metrics.</p>    <p>     <strong>Hyperparameters:</strong> Parabel&#x00A0;has 4 hyperparameters: (a) the number of trees (<em>T</em>); (b) the maximum number of paths that can be traversed in a tree (<em>P</em>); (c) the maximum number of labels in a leaf node (<em>M</em>) and (d) the misclassification penalty for all the internal and leaf node classifiers (<em>C</em>). The default parameter settings of <em>M</em> = 100, <em>P</em> = 10 and <em>C</em> = 10 for log loss and <em>C</em> = 1 or squared hinge loss were used in all the experiments so as to reduce training time by eliminating hyperparameter sweeps (though tuning could have increased Parabel&#x00A0;&#x2019;s prediction accuracy). Results are reported for <em>T</em> = 1 &#x2212; 3 trees on the benchmark datasets and for <em>T</em> = 5 trees on DSA. The hyperparameters of the other algorithms were set as suggested by their authors wherever applicable and by fine grained validation otherwise.</p>    <p>     <strong>Results on repository datasets:</strong> Table&#x00A0;<a class="tbl" href="#tab2">2</a> compares Parabel&#x00A0;&#x2019;s performance to leading 1-vs-All and tree classifiers using the popular precision@<em>k</em> metric with <em>k</em> = 1, 3, 5. Results for nDCG@<em>k</em> and propensity-scored variants&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] are reported in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. All experiments were run on an Intel Xeon 2.5 GHz processor with 64 GB RAM, except for XML-CNN training and prediction which were carried out on a Nvidia GTX TITAN X GPU. Parabel&#x00A0;could be up to 1,000x faster at training and 10,000x faster at prediction than DiSMEC while having prediction accuracies that were lower by 1.0% on the Amazon datasets and by 1.6% on Wiki-500K. PPDSparse and XML-CNN, the other leading 1-vs-All approaches, could not scale to Amazon-3M and were estimated to also be approximately 1000x slower to train than Parabel&#x00A0;. On the smaller datasets, PPDSparse was found to be 5x slower at training and 40x slower at prediction than Parabel&#x00A0;while having prediction accuracies that were lower by 1.8% on WikiLSHTC-325K but higher by 1.5% on Wiki-500K and by 1.0% on Amazon-670K. Similarly XML-CNN, the deep learning based approach, was up to 150x slower at training, 10x slower at prediction and had up to 8% lower prediction accuracies than Parabel&#x00A0;. Finally, Parabel&#x00A0;was also up to 10x faster to train, had up to 15x lower model size and had up to 10% higher prediction accuracies than the leading tree based method PfastreXML.</p>    <p>     <strong>Parabel&#x00A0;variants</strong>: Table&#x00A0;<a class="tbl" href="#tab4">3</a> demonstrates that Parabel&#x00A0;&#x2019;s precision@5 decreases if its components are replaced by variants. In particular, the prediction accuracy can drop by up to 10% if Parabel&#x00A0;&#x2019;s learnt label tree is replaced by PLT&#x0027;s random tree (Random tree). Furthermore, replacing Parabel&#x00A0;&#x2019;s label representation by HOMER&#x0027;s label representation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>](HOMER features) could lead to an 8% drop in precision@5. Finally, note that training time can be substantially reduced by replacing Parabel&#x00A0;&#x2019;s strong 1-vs-All classifiers with weak Rocchio&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] classifiers in the internal (Weak internal) and leaf (Weak leaf) nodes. However, accuracies could drop by up to 9% in either case. Finally, the results of varying Parabel&#x00A0;&#x2019;s hyperparameters including the number of trees (<em>T</em>), the maximum number of labels in a leaf (<em>M</em>) and the number of paths taken (<em>P</em>) are reported in the <a class="link-inline force-break" href="http://supplementary material">supplementary material</a>. </p>    <p>     <strong>Dynamic Search Advertising:</strong> Table&#x00A0;<a class="tbl" href="#tab2">2</a> also reports results on 2 small DSA datasets &#x2013; DSA-2M having many more training points than labels and DSA-7M having almost the same number of training points and labels. None of the existing extreme classifiers could scale to these datasets apart from PfastreXML. Unfortunately, PfastreXML&#x0027;s performance was significantly worse than Parabel&#x00A0;&#x2019;s on all metrics. Parabel&#x00A0;was <span class="inline-equation"><span class="tex">$4-6\%$</span>     </span> more accurate, was 4-20x faster to train, had 10-15x lower model size and was 1.3-2.4x faster at prediction. Parabel&#x00A0;was therefore found to be much better suited for a live deployment on Bing based on much larger datasets which were beyond the scaling capabilities of PfastreXML.</p>    <p>Table&#x00A0;<a class="tbl" href="#tab5">4</a> compares Parabel&#x00A0;&#x2019;s performance to the baseline BM25 algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] on a live deployment of DSA on Bing. BM25 is a popularly used information retrieval model which ranks documents, for a given query, based on how frequently the query tokens occur in a given document. The performance of both algorithms was evaluated based on the click-through rate (CTR), bounce rate (BR) which is the percentage of times a user returns immediately to the search engine after clicking an ad, ad recall which is the percentage of ads which are clicked by at least one user and quality of ad recommendations (QOA) which measures the goodness of ad recommendations according to a query-ad relevance model trained on human labelled data. As can be seen, Parabel&#x00A0;&#x2019;s ad recall is 300% higher, CTR is 20% higher and BR is 30% lower as compared to the BM25 straw man. Including Parabel&#x00A0;in the Bing DSA ensemble generated 75% additional ad recall and 42% additional clicks when deployed in production.</p>    <p>Figure&#x00A0;<a class="fig" href="#fig1">1</a> shows some qualitative examples where Parabel&#x00A0;does better than the Bing ensemble. In general, it was observed that extreme classifiers such as Parabel&#x00A0;were able to predict more queries for pithy ads than the traditional approaches in the Bing ensemble. Parabel&#x00A0;&#x2019;s predictions were also found to be more diverse as can be seen in the &#x201D;Socofy loafers&#x201D; ad. Finally, Parabel&#x00A0;was also able to avoid predicting irrelevant queries such as &#x201D;product design questions&#x201D; and &#x201D;top nn10 questions&#x201D; for the &#x201D;Gu10 LED Bulbs&#x201D; ad as it learnt not to place emphasis on the feature corresponding to the &#x201D;question&#x201D; in the ad landing page.</p>   </section>   <section id="sec-18">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Conclusions</h2>     </div>    </header>    <p>This paper developed the Parabel&#x00A0;algorithm for extreme multi-label learning. Parabel&#x00A0;learnt a small ensemble of 1-3 highly accurate trees almost matching state-of-the-art prediction accuracies with training and prediction costs that were logarithmic in the number of labels. Parabel&#x00A0;&#x2019;s key technical contributions include: a novel way of learning balanced label trees based on an easily computed and informative label representation which were found to be better than previously proposed representations and tree growing procedures; a novel probabilistic hierarchical multi-label model which generalizes hierarchical softmax to the multi-label setting and highly scalable algorithms for efficient Parabel&#x00A0;training and prediction. Experiments revealed that Parabel&#x00A0;could be many orders of magnitude faster at training and prediction as compared to leading 1-vs-All extreme classifiers without a significant loss in accuracy. Parabel&#x00A0;was also found to be superior to leading tree classifiers on all metrics. This made Parabel&#x00A0;much more suited for a live deployment of Dynamic Search Advertising on Bing where it significantly increased the ad recall and click-through rate.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">[n. d.]. Code for Parabel. <a href="http://manikvarma.org/code/Parabel/download.html." target="http://manikvarma.org/code/Parabel/download.html.">http://manikvarma.org/code/Parabel/download.html.</a> ([n. d.]).</li>     <li id="BibPLXBIB0002" label="[2]">[n. d.]. The Extreme Classification Repository. <a href="http://manikvarma.org/downloads/XC/XMLRepository.html." target="http://manikvarma.org/downloads/XC/XMLRepository.html.">http://manikvarma.org/downloads/XC/XMLRepository.html.</a> ([n. d.]).</li>     <li id="BibPLXBIB0003" label="[3]">R. Agrawal, A. Gupta, Y. Prabhu, and M. Varma. 2013. Multi-label Learning with Millions of Labels: Recommending Advertiser Bid Phrases for Web Pages. In <em>      <em>WWW</em>.     </em></li>     <li id="BibPLXBIB0004" label="[4]">R. Babbar and B. Shoelkopf. 2017. DiSMEC-Distributed Sparse Machines for Extreme Multi-label Classification. In <em>      <em>WSDM</em>.     </em></li>     <li id="BibPLXBIB0005" label="[5]">S. Bengio, J. Weston, and D. Grangier. 2010. Label Embedding Trees for Large Multi-class Tasks. In <em>      <em>NIPS</em>.     </em>163&#x2013;171.</li>     <li id="BibPLXBIB0006" label="[6]">A. Bertoni, M. Goldwurm, J. Lin, and F. Sacc&#x00E0;. 2012. Size Constrained Distance Clustering: Separation Properties and Some Complexity Results. 115 (2012), 125&#x2013;139.</li>     <li id="BibPLXBIB0007" label="[7]">K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain. 2015. Sparse Local Embeddings for Extreme Multi-label Classification. In <em>      <em>NIPS</em>.     </em></li>     <li id="BibPLXBIB0008" label="[8]">P.&#x00A0;S. Bradley, K.&#x00A0;P. Bennett, and A. Demiriz. 2000. <em>      <em>Constrained K-Means Clustering</em>.     </em>Technical Report. MSR-TR-2000-65, Microsoft Research.</li>     <li id="BibPLXBIB0009" label="[9]">Y.&#x00A0;N. Chen and H.&#x00A0;T. Lin. 2012. Feature-aware Label Space Dimension Reduction for Multi-label Classification. In <em>      <em>NIPS</em>.     </em></li>     <li id="BibPLXBIB0010" label="[10]">Y. Choi, M. Fontoura, E. Gabrilovich, V. Josifovski, M. R.&#x00A0;Mediano, and B. Pang. [n. d.]. Using landing pages for sponsored search ad selection. In <em>      <em>WWW 2010</em>.     </em></li>     <li id="BibPLXBIB0011" label="[11]">M. Ciss&#x00E9;, N. Usunier, T. Arti&#x00E8;res, and P. Gallinari. 2013. Robust Bloom Filters for Large MultiLabel Classification Tasks. In <em>      <em>NIPS</em>.     </em></li>     <li id="BibPLXBIB0012" label="[12]">J. Deng, S. Satheesh, A.&#x00A0;C. Berg, and L. Fei-Fei. 2011. Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition. In <em>      <em>NIPS</em>.     </em>567&#x2013;575.</li>     <li id="BibPLXBIB0013" label="[13]">R.&#x00A0;E. Fan, K.&#x00A0;W. Chang, C.&#x00A0;J. Hsieh, X.&#x00A0;R. Wang, and C.&#x00A0;J. Lin. 2008. LIBLINEAR: A library for large linear classification. <em>      <em>JMLR</em>     </em> (2008).</li>     <li id="BibPLXBIB0014" label="[14]">T. Gao and D. Koller. [n. d.]. Discriminative Learning of Relaxed Hierarchy for Large-scale Visual Recognition. In <em>      <em>ICCV</em>.      </em>2072&#x2013;2079.</li>     <li id="BibPLXBIB0015" label="[15]">D. Hsu, S. Kakade, J. Langford, and T. Zhang. 2009. Multi-Label Prediction via Compressed Sensing. In <em>      <em>NIPS</em>.     </em></li>     <li id="BibPLXBIB0016" label="[16]">P.&#x00A0;S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L.&#x00A0;P. Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In <em>      <em>CIKM</em>.     </em></li>     <li id="BibPLXBIB0017" label="[17]">H. Jain, Y. Prabhu, and M. Varma. 2016. Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking &#x0026;#38; Other Missing Label Applications. In <em>      <em>KDD</em>.     </em></li>     <li id="BibPLXBIB0018" label="[18]">K. Jasinska, K. Dembczynski, R. Busa-Fekete, K. Pfannschmidt, T. Klerx, and E. H&#x00FC;llermeier. 2016. Extreme F-measure Maximization Using Sparse Probability Estimates. In <em>      <em>ICML</em>.     </em> 1435&#x2013;1444.</li>     <li id="BibPLXBIB0019" label="[19]">Y. Jernite, A. Choromanska, and D. Sontag. 2017. Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation. In <em>      <em>ICML</em>.     </em></li>     <li id="BibPLXBIB0020" label="[20]">K.&#x00A0;S. Jones, S. Walker, and S.&#x00A0;E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments. <em>      <em>Inf. Process. Manage.</em>     </em>(2000).</li>     <li id="BibPLXBIB0021" label="[21]">Z. Lin, G. Ding, M. Hu, and J. Wang. 2014. Multi-label Classification via Feature-aware Implicit Label Space Encoding. In <em>      <em>ICML</em>.     </em></li>     <li id="BibPLXBIB0022" label="[22]">J. Liu, W. Chang, Y. Wu, and Y. Yang. 2017. Deep Learning for Extreme Multi-label Text Classification. In <em>      <em>SIGIR</em>.     </em> 115&#x2013;124.</li>     <li id="BibPLXBIB0023" label="[23]">C.&#x00A0;D. Manning, P. Raghavan, and H. Sch&#x00FC;tze. 2008. <em>      <em>Introduction to Information Retrieval</em>     </em>. Cambridge University Press, New York, NY, USA.</li>     <li id="BibPLXBIB0024" label="[24]">J. McAuley and J. Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In <em>      <em>RecSys</em>.     </em></li>     <li id="BibPLXBIB0025" label="[25]">E.&#x00A0;L. Mencia and J. F&#x00FC;rnkranz. 2008. Efficient pairwise multilabel classification for large-scale problems in the legal domain. In <em>      <em>SIGIR</em>.     </em></li>     <li id="BibPLXBIB0026" label="[26]">P. Mineiro and N. Karampatziakis. 2015. Fast Label Embeddings for Extremely Large Output Spaces. In <em>      <em>ECML</em>.     </em></li>     <li id="BibPLXBIB0027" label="[27]">A. Niculescu-Mizil and E. Abbasnejad. 2017. Label Filters for Large Scale Multilabel Classification. In <em>      <em>International Conference on Artificial Intelligence and Statistics</em>.     </em> 1448&#x2013;1457.</li>     <li id="BibPLXBIB0028" label="[28]">Y. Prabhu, A. Kag, S. Gopinath, K. Dahiya, S. Harsola, R. Agrawal, and M. Varma. 2018. Extreme multi-label learning with label features for warm-start tagging, ranking and recommendation. In <em>      <em>WSDM</em>.     </em></li>     <li id="BibPLXBIB0029" label="[29]">Y. Prabhu and M. Varma. 2014. FastXML: A fast, accurate and stable tree-classifier for extreme multi-label learning. In <em>      <em>KDD</em>.     </em></li>     <li id="BibPLXBIB0030" label="[30]">S. Ravi, A.&#x00A0;Z. Broder, E. Gabrilovich, V. Josifovski, S. Pandey, and B. Pang. [n. d.]. Automatic generation of bid phrases for online advertising. In <em>      <em>WSDM 2010</em>.     </em></li>     <li id="BibPLXBIB0031" label="[31]">Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. 2014. Learning semantic representations using convolutional neural networks for web search. In <em>      <em>WWW</em>.     </em></li>     <li id="BibPLXBIB0032" label="[32]">S. Si, H. Zhang, S.&#x00A0;S. Keerthi, D. Mahajan, I.&#x00A0;S. Dhillon, and C.&#x00A0;J. Hsieh. 2017. Gradient Boosted Decision Trees for High Dimensional Sparse Output. In <em>      <em>ICML</em>.     </em> 3182&#x2013;3190.</li>     <li id="BibPLXBIB0033" label="[33]">Y. Tagami. 2017. AnnexML: Approximate Nearest Neighbor Search for Extreme Multi-label Classification. In <em>      <em>KDD</em>.     </em> 455&#x2013;464.</li>     <li id="BibPLXBIB0034" label="[34]">Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2008. Effective and efficient multilabel classification in domains with large number of labels. In <em>      <em>Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data</em>.     </em></li>     <li id="BibPLXBIB0035" label="[35]">X. Wei and W.&#x00A0;B. Croft. 2006. LDA-based document models for ad-hoc retrieval. In <em>      <em>SIGIR</em>.     </em></li>     <li id="BibPLXBIB0036" label="[36]">J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie: Scaling Up To Large Vocabulary Image Annotation. In <em>      <em>IJCAI</em>.     </em></li>     <li id="BibPLXBIB0037" label="[37]">J. Weston, A. Makadia, and H. Yee. 2013. Label Partitioning For Sublinear Ranking. In <em>      <em>ICML</em>.     </em></li>     <li id="BibPLXBIB0038" label="[38]">C. Xu, D. Tao, and C. Xu. 2016. Robust Extreme Multi-label Learning. In <em>      <em>KDD</em>.     </em> 1275&#x2013;1284.</li>     <li id="BibPLXBIB0039" label="[39]">I.&#x00A0;E.&#x00A0;H. Yen, X. Huang, W. Dai, P. Ravikumar, I. Dhillon, and E. Xing. 2017. PPDsparse: A Parallel Primal-Dual Sparse Method for Extreme Classification. In <em>      <em>KDD</em>.     </em> 545&#x2013;553.</li>     <li id="BibPLXBIB0040" label="[40]">I.&#x00A0;E.&#x00A0;H. Yen, X. Huang, P. Ravikumar, K. Zhong, and I.&#x00A0;S. Dhillon. 2016. PD-Sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification. In <em>      <em>ICML</em>.     </em></li>     <li id="BibPLXBIB0041" label="[41]">W.&#x00A0;T. Yih, J. Goodman, and V.&#x00A0;R. Carvalho. [n. d.]. Finding advertising keywords on web pages. In <em>      <em>WWW 2006</em>.     </em></li>     <li id="BibPLXBIB0042" label="[42]">H.&#x00A0;F. Yu, P. Jain, P. Kar, and I.&#x00A0;S. Dhillon. 2014. Large-scale Multi-label Learning with Missing Labels. In <em>      <em>ICML</em>.     </em></li>     <li id="BibPLXBIB0043" label="[43]">W. Zhang, D. Wang, G. Xue, and H. Zha. 2012. Advertising Keywords Recommendation for Short-Text Web Pages Using Wikipedia. <em>      <em>ACM TIST</em>     </em> (2012).</li>     <li id="BibPLXBIB0044" label="[44]">W. Zhang, L. Wang, J. Yan, X. Wang, and H. Zha. 2017. Deep Extreme Multi-label Learning. <em>      <em>CoRR</em>     </em> (2017).</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3185998">https://doi.org/10.1145/3178876.3185998</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
