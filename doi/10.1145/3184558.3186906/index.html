<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186906'>https://doi.org/10.1145/3184558.3186906</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186906'>https://w3id.org/oa/10.1145/3184558.3186906</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Ting-Yu</span>      <span class="surName">Yen</span>     Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617 Taiwan     </div>         <div class="author">     <span class="givenName">Yang-Yin</span>      <span class="surName">Lee</span>     Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617 Taiwan     </div>         <div class="author">     <span class="givenName">Hen-Hsen</span>      <span class="surName">Huang</span>     Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617 Taiwan     </div>         <div class="author">     <span class="givenName">Hsin-Hsi</span>      <span class="surName">Chen</span>     Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617 Taiwan     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186906" target="_blank">https://doi.org/10.1145/3184558.3186906</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>While<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> recent word embedding models demonstrate their abilities to capture syntactic and semantic information, the demand for sense level embedding is getting higher. In this study, we propose a novel joint sense embedding learning model that retrofits word representation into sense representation from contextual and ontological information. The experiment shows the effectiveness and robustness of our model that outperforms previous approaches in four public available benchmark datasets.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>KEYWORDS:</small>     </span>     <span class="keyword">      <small>Semantic relatedness</small>, </span>     <span class="keyword">      <small>sense embedding</small>, </span>     <span class="keyword">      <small>joint sense retrofitting</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>Ting-Yu Yen, Yang-Yin Lee, Hen-Hsen Huang and Hsin-Hsi Chen. 2018. That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information. In <em>Proceedings of The 2018 Web Conference Companion (WWW'2018 Companion). ACM, New York, NY, USA, 3 pages.</em> <a href="https://doi.org/10.1145/3184558.3186906" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186906</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec1">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> INTRODUCTION</h2>     </div>    </header>    <p>Recently, there has been an intensive research activity in studying word embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib1">1</a>]. However, most of the word embedding models use one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib2">2</a>&#x2013;<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib4">4</a>]. In this research, we propose a novel joint sense embedding learning algorithm that retrofits a trained word embedding using contextual and ontological information.</p>    <p>Our proposed <em>joint sense retrofitting</em> is a post-processing method for generating low-dimensional sense embedding inspired from <em>sense retro</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib3">3</a>]. Although some studies adopt ontological information into sense embedding model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib2">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib3">3</a>], it is the first time of employing the ontological and contextual information simultaneously. Given a trained word embedding and a lexical ontology that contains sense level relationships (e.g., synonym, hypernym, etc.), our model generates new sense vectors via constraining the distance between the sense vector and its word form vector, its sense neighbors and its contextual neighbors. In the experiment, we show that our proposed joint sense retrofitting model outperforms previous approaches in four benchmark datasets, and demonstrates robustness from two ontologies, WordNet<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> and Roget&#x0027;s Thesaurus<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> .</p>   </section>   <section id="sec2">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> JOINT SENSE RETROFITTING</h2>     </div>    </header>    <p>Let <span class="inline-equation"><span class="tex">$V = \{ {{w_1}, \ldots ,{w_n}} \}$ </span>     </span> be a vocabulary of a trained word embedding and <span class="inline-equation"><span class="tex">$| V |$ </span>     </span> be its size. The matrix <span class="inline-equation"><span class="tex">$\hat Q$ </span>     </span> will be the pre-trained collection of vector representations <span class="inline-equation"><span class="tex"/>     </span>, where <span class="inline-equation"><span class="tex">$d$ </span>     </span> is the dimensionality of a word vector. Each <span class="inline-equation"><span class="tex">${w_i} \in V$ </span>     </span> is learned using a standard word embedding technique (e.g., GloVe [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib1">1</a>]). Let <span class="inline-equation"><span class="tex">${\rm{\Omega }} = ( {T,E} )$ </span>     </span> be an ontology that contains the semantic relationship, where <span class="inline-equation"><span class="tex">$T = \{ {{t_1}, \ldots ,{t_m}} \}$ </span>     </span> is a set of senses and <span class="inline-equation"><span class="tex">$| T |$ </span>     </span> is total number of senses. The edge <span class="inline-equation"><span class="tex">$( {i,j} ) \in E$ </span>     </span> indicates a semantic relationship of interest (e.g., synonym) between <span class="inline-equation"><span class="tex">${t_i}$ </span>     </span> and <span class="inline-equation"><span class="tex">${t_j}$ </span>     </span>. The edge set <span class="inline-equation"><span class="tex">$E$ </span>     </span> can be further split into two disjoint subsets <span class="inline-equation"><span class="tex">${E_s}$ </span>     </span> and <span class="inline-equation"><span class="tex">${E_c}$ </span>     </span>. <span class="inline-equation"><span class="tex">$( {i,j} ) \in {E_s}$ </span>     </span> if and only if there is more than one sense of the word form of <span class="inline-equation"><span class="tex">${t_j}$ </span>     </span>, while <span class="inline-equation"><span class="tex">$( {i,j} ) \in {E_c}$ </span>     </span> if and only if <span class="inline-equation"><span class="tex">${t_j}$ </span>     </span> has only one sense. In our model, we use the word form vector to represent the neighbors of the <span class="inline-equation"><span class="tex">${t_i}$ </span>     </span>s in <span class="inline-equation"><span class="tex">${E_c}$ </span>     </span>. Those neighbors are viewed as contextual neighbors as they learned from the context of a corpus. We use <span class="inline-equation"><span class="tex">${\hat q_{{t_j}}}$ </span>     </span> to denote the word form vector of <span class="inline-equation"><span class="tex">${t_j}$ </span>     </span> (one should notice that <span class="inline-equation"><span class="tex">${\hat q_{{t_j}}}$ </span>     </span> and <span class="inline-equation"><span class="tex">${\hat q_{{t_k}}}$ </span>     </span> may map to the same vector representation even if <span class="inline-equation"><span class="tex">$j \ne k$ </span>     </span>). Then the objective of our joint sense retrofitting model is to learn a new matrix <span class="inline-equation"><span class="tex">${\rm{S}} = ( {{s_1}, \ldots ,{s_m}} )$ </span>     </span> such that each new sense vector is close to (1) its word form vertex, (2) its sense neighbors, and (3) its contextual neighbors. The objective to be minimized is: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{equation*}\mathop \sum \limits_{i = 1}^m \left[ {{\alpha _i}{s_i} - {{\hat q}_{{t_i}}}^2 + \mathop \sum \limits_{\left( {i,j} \right) \in {E_c}} {\beta _{ij}}{s_i} - {{\hat q}_{{t_j}}}^2 + \mathop \sum \limits_{\left( {i,k} \right) \in {E_s}} {\gamma _{ik}}{s_i} - {s_k}^2} \right]\left( 1 \right)\end{equation*}</span>      <br/>     </div>     </div>where <span class="inline-equation"><span class="tex">$\alpha ,\beta $ </span>     </span> and <span class="inline-equation"><span class="tex">$\gamma $ </span>     </span> control the relative strength of the sense relations. We therefore apply an iterative updating method to the solution of the above convex objective function [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib5">5</a>]. Initially, the sense vectors are set to their corresponding word form vectors (i.e., <span class="inline-equation"><span class="tex">${s_i} \leftarrow {\hat q_{{t_i}}}\;\forall i$ </span>     </span> ). Then in the following iterations, the updating formula for <span class="inline-equation"><span class="tex">${s_i}$ </span>     </span> would be: <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\begin{equation*} {s_i} = \frac{{\mathop \sum \nolimits_{k:\left( {i,k} \right) \in {E_s}} {\gamma _{ik}}{s_k} + \mathop \sum \nolimits_{j:\left( {i,j} \right) \in {E_c}} {\beta _{ij}}{{\hat q}_{{t_j}}} + {\alpha _i}{{\hat q}_{{t_i}}}}}{{\mathop \sum \nolimits_{k:\left( {i,k} \right) \in {E_s}} {\gamma _{ik}} + \mathop \sum \nolimits_{j:\left( {i,j} \right) \in {E_c}} {\beta _{ij}} + {\alpha _i}}}\left( 2 \right)\end{equation*}</span>      <br/>     </div>     </div>    </p>    <p>Experimentally, 10 iterations are sufficient to minimize the objective function from a set of starting vectors to produce effective sense retrofitted vectors.</p>   </section>   <section id="sec3">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> DATASETS AND EXPERIMENTAL SETUP</h2>     </div>    </header>    <p>We downloaded four benchmark datasets from the web: MEN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib6">6</a>], MTurk [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib7">7</a>], Rare Words (RW) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib8">8</a>] and WordSim353 (WS353) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib9">9</a>]. For measuring the semantic similarity between a word pair in the datasets, we adopt the sense evaluation metrics AvgSim and MaxSim [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib4">4</a>]. We select GloVe as our pre-trained word embedding model, which is trained on Wikipedia and Gigaword-5 (6B tokens, 400k vocab, uncased, 50d vectors). In testing phase, if a test dataset has missing words, we use the average of all sense vectors to represent the missing word. Note that our reported results of vanilla sense embedding may be slightly different from other researches due to the treatment of missing words. However, within this research the reported performance can be compared due to the same missing word processing method. We adopt two ontologies in our experiment: WordNet (WN) and Roget&#x0027;s 21st Century Thesaurus (Roget). In WN, the relation specific weights <span class="inline-equation"><span class="tex">$\beta $ </span>     </span>s and <span class="inline-equation"><span class="tex">$\gamma $ </span>     </span>s are set to 1.0 for synonyms and 0.5 for hypernyms or hyponyms. Unlike WN, Roget does not have the synset type. As a result, we manually built a synonym ontology from the resource. In Roget, there are three levels of synonym relationship, and we set <span class="inline-equation"><span class="tex">$\beta $ </span>     </span>s and <span class="inline-equation"><span class="tex">$\gamma $ </span>     </span>s to 1.0, 0.6 and 0.3 for the nearest to the farthest synonyms, respectively. For each sense, <span class="inline-equation"><span class="tex">$\alpha $ </span>     </span> is set to the sum of all the relation specific weights <span class="inline-equation"><span class="tex">$\beta $ </span>     </span>s and <span class="inline-equation"><span class="tex">$\gamma $ </span>     </span>s. Table <a class="tbl" href="#tb1">1</a> shows a summary of the benchmark datasets and their relationship with the ontologies. In Table <a class="tbl" href="#tb1">1</a>, row 3 and row 4 are the number of words that are both listed in the datasets and the ontologies. The word counts in WN and Roget are 83,118 and 47,229, respectively.</p>    <div class="table-responsive" id="tb1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Summarization of the benchmark datasets</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:center;"/>       <th style="text-align:center;">MEN</th>       <th style="text-align:center;">MTurk</th>       <th style="text-align:center;">RW</th>       <th style="text-align:center;">WS353</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">Pair count</td>       <td style="text-align:center;">3,000</td>       <td style="text-align:center;">287</td>       <td style="text-align:center;">2,034</td>       <td style="text-align:center;">353</td>      </tr>      <tr>       <td style="text-align:left;">Word count</td>       <td style="text-align:center;">751</td>       <td style="text-align:center;">499</td>       <td style="text-align:center;">2,951</td>       <td style="text-align:center;">437</td>      </tr>      <tr>       <td style="text-align:left;">WN</td>       <td style="text-align:center;">751</td>       <td style="text-align:center;">444</td>       <td style="text-align:center;">2,502</td>       <td style="text-align:center;">415</td>      </tr>      <tr>       <td style="text-align:left;">Roget</td>       <td style="text-align:center;">705</td>       <td style="text-align:center;">382</td>       <td style="text-align:center;">2,152</td>       <td style="text-align:center;">411</td>      </tr>     </tbody>     </table>    </div>   </section>   <section id="sec4">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> RESULTS AND DISCUSSION</h2>     </div>    </header>    <p>Table <a class="tbl" href="#tb2">2</a> shows the spearman correlation (<span class="inline-equation"><span class="tex">${\rm{\rho }} \times 100$ </span>     </span>) of AvgSim and MaxSim between human scores and sense embedding&#x0027;s scores on the benchmark datasets. We compare our proposed model (<em>joint</em>) with vanilla GloVe embedding and the sense retro model (<em>retro</em>) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#bib3">3</a>]. For vanilla GloVe, we directly compute the cosine similarity of a word pair&#x0027;s vectors, which can be seen as a special case of AvgSim/MaxSim. From Table <a class="tbl" href="#tb2">2</a>, we find that our proposed <em>joint</em> model outperforms <em>retro</em> and GloVe in all the datasets. Interestingly, although WN is bigger and covers more words than Roget, in our model the average performance with Roget is better than WN. Surprisingly, the RW&#x0027;s performance declined with the WN ontology. The possible reason might be WN pays more attention to common sense words than rarely occurred words. From the viewpoint of ontology, the <em>retro</em> model&#x0027;s performance declines in all the datasets with the smaller Roget, showing the dependency on the ontology size. In contrast, the <em>joint</em> model performs well in both the smaller Roget and larger WN ontologies, showing the robustness of our proposed model.</p>    <div class="table-responsive" id="tb2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">      <span class="inline-equation"><span class="tex">${\bf{\rho }} \times 100$ </span>      </span> of (MaxSim / AvgSim) on test datasets</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:center;"/>       <th style="text-align:center;">MEN</th>       <th style="text-align:center;">MTurk</th>       <th style="text-align:center;">RW</th>       <th style="text-align:center;">WS353</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">GloVe</td>       <td style="text-align:center;">65.7</td>       <td style="text-align:center;">61.9</td>       <td style="text-align:center;">30.3</td>       <td style="text-align:center;">50.3</td>      </tr>      <tr>       <td style="text-align:left;">retro-WN [3]</td>       <td style="text-align:center;">62.4 / 67.7</td>       <td style="text-align:center;">57.4 / 60.1</td>       <td style="text-align:center;">15.1 / 26.9</td>       <td style="text-align:center;">43.9 / 51.1</td>      </tr>      <tr>       <td style="text-align:left;">retro-Roget [3]</td>       <td style="text-align:center;">48.7 / 52.1</td>       <td style="text-align:center;">47.3 / 49.3</td>       <td style="text-align:center;">24.4 / 26.1</td>       <td style="text-align:center;">27.8 / 29.4</td>      </tr>      <tr>       <td style="text-align:left;">joint-WN</td>       <td style="text-align:center;">64.0 / <strong>68.9</strong>       </td>       <td style="text-align:center;">57.3 / 62.1</td>       <td style="text-align:center;">20.1 / 28.5</td>       <td style="text-align:center;">47.2 / 49.6</td>      </tr>      <tr>       <td style="text-align:left;">joint-Roget</td>       <td style="text-align:center;">        <strong>66.5 /</strong> 67.5</td>       <td style="text-align:center;">        <strong>62.0 / 62.6</strong>       </td>       <td style="text-align:center;">        <strong>32.3 / 32.5</strong>       </td>       <td style="text-align:center;">        <strong>50.9 / 52.8</strong>       </td>      </tr>     </tbody>     </table>    </div>   </section>   <section id="sec5">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> CONCLUSIONS</h2>     </div>    </header>    <p>In summary, we propose a novel joint sense retrofitting model that utilizes the contextual and ontological information. The sense embedding is learned iteratively via constraining the distance between the sense vector and its word form vector, its sense neighbors and its contextual neighbors. Experimentally, our proposed model outperforms previous models in four benchmark datasets. We provide the source code for the model at https://github.com/y95847frank/Joint-Retrofitting.</p>   </section>  </section>  <section class="back-matter">   <section id="ack-001">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This research was partially supported by Ministry of Science and Technology, Taiwan, under grants MOST-105-2221-E-002-154-MY3, MOST-106-2923-E-002-012-MY3 and MOST-107-2634-F-002-011.</p>   </section>   <section id="bib-sec-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="bib1" label="[1]">J. Pennington, R. Socher and C.D. Manning 2014. Glove: Global vectors for word representation. <em>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014)</em>. 12, (2014), 1532&#x2013;1543.</li>     <li id="bib2" label="[2]">I. Iacobacci, M.T. Pilehvar and R. Navigli 2015. SensEmbed: learning sense embeddings for word and relational similarity. <em>Proceedings of ACL</em> (2015), 95&#x2013;105.</li>     <li id="bib3" label="[3]">S.K. Jauhar, C. Dyer and E. Hovy 2015. Ontologically grounded multi-sense representation learning for semantic vector space models. <em>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em> (2015), 683&#x2013;693.</li>     <li id="bib4" label="[4]">J. Reisinger and R.J. Mooney 2010. Multi-prototype vector-space models of word meaning. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (2010), 109&#x2013;117.</li>     <li id="bib5" label="[5]">Y. Bengio, O. Delalleau and N. Le Roux 2006. Label Propagation and Quadratic Criterion. <em>Semi-Supervised Learning</em>. O. Chapelle, B. Sch&#x00F6;lkopf, and A. Zien, eds. MIT Press. 193&#x2013;216.</li>     <li id="bib6" label="[6]">E. Bruni, N.-K. Tran and M. Baroni 2014. Multimodal Distributional Semantics. <em>J. Artif. Intell. Res.(JAIR)</em>. 49, (2014), 1&#x2013;47.</li>     <li id="bib7" label="[7]">K. Radinsky, E. Agichtein, E. Gabrilovich and S. Markovitch 2011. A word at a time: computing word relatedness using temporal semantic analysis. <em>Proceedings of the 20th international conference on World wide web</em> (2011), 337&#x2013;346.</li>     <li id="bib8" label="[8]">T. Luong, R. Socher and C.D. Manning 2013. Better Word Representations with Recursive Neural Networks for Morphology. <em>CoNLL</em> (2013), 104&#x2013;113.</li>     <li id="bib9" label="[9]">L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman and E. Ruppin 2001. Placing search in context: The concept revisited. <em>Proceedings of the 10th international conference on World Wide Web</em> (2001), 406&#x2013;414.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>These authors contributed equally to this work.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://wordnet.princeton.edu">https://wordnet.princeton.edu</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="http://www.thesaurus.com">http://www.thesaurus.com</a>   </p>   <div class="bibStrip">    <p>     <CopyrightStatement/>    </p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p> ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186906">https://doi.org/10.1145/3184558.3186906</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
