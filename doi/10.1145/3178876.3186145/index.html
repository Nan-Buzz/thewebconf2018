<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Aspect-Aware Latent Factor Model: Rating Prediction with
  Ratings and Reviews</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186145'>https://doi.org/10.1145/3178876.3186145</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186145'>https://w3id.org/oa/10.1145/3178876.3186145</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Aspect-Aware Latent Factor Model:
          Rating Prediction with Ratings and Reviews</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Zhiyong</span> <span class=
          "surName">Cheng</span>, National University of Singapore,
          <a href=
          "mailto:jason.zy.cheng@gmail.com">jason.zy.cheng@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Ying</span> <span class=
          "surName">Ding</span>, Vipshop Inc., USA, <a href=
          "mailto:ian.yingding@gmail.com">ian.yingding@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Lei</span> <span class=
          "surName">Zhu</span>, Shandong Normal University, China,
          <a href=
          "mailto:leizhu0608@gmail.com">leizhu0608@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Mohan</span> <span class=
          "surName">Kankanhalli</span>, National University of
          Singapore, <a href=
          "mailto:mohan@comp.nus.edu.sg">mohan@comp.nus.edu.sg</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186145"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186145</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Although latent factor models (e.g., matrix
        factorization) achieve good accuracy in rating prediction,
        they suffer from several problems including cold-start,
        non-transparency, and suboptimal recommendation for local
        users or items. In this paper, we employ textual review
        information with ratings to tackle these limitations.
        Firstly, we apply a proposed aspect-aware topic model (ATM)
        on the review text to model user preferences and item
        features from different <em>aspects</em>, and estimate the
        <em>aspect importance</em> of a user towards an item. The
        aspect importance is then integrated into a novel
        aspect-aware latent factor model (ALFM), which learns
        user's and item's latent factors based on ratings. In
        particular, ALFM introduces a weighted matrix to associate
        those latent factors with the same set of aspects
        discovered by ATM, such that the latent factors could be
        used to estimate aspect ratings. Finally, the overall
        rating is computed via a linear combination of the aspect
        ratings, which are weighted by the corresponding aspect
        importance. To this end, our model could alleviate the data
        sparsity problem and gain good interpretability for
        recommendation. Besides, an aspect rating is weighted by an
        aspect importance, which is dependent on the targeted
        user's preferences and targeted item's features. Therefore,
        it is expected that the proposed method can model a user's
        preferences on an item more accurately for each user-item
        pair locally. Comprehensive experimental studies have been
        conducted on 19 datasets from Amazon and Yelp 2017
        Challenge dataset. Results show that our method achieves
        significant improvement compared with strong baseline
        methods, especially for users with only few ratings.
        Moreover, our model could interpret the recommendation
        results in depth.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Social recommendation;</strong>
        <strong>Personalization;</strong> <strong>Recommender
        systems;</strong> <em>Collaborative filtering;</em> •
        <strong>Computing methodologies</strong> → <strong>Topic
        modeling;</strong> <strong>Factor
        analysis;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Aspect-aware</small>,</span>
          <span class="keyword"><small>Matrix
          Factorization</small>,</span> <span class=
          "keyword"><small>Recommendation</small>,</span>
          <span class="keyword"><small>Review-aware</small>,</span>
          <span class="keyword"><small>Topic Model</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Zhiyong Cheng, Ying Ding, Lei Zhu, and Mohan Kankanhalli.
          2018. Aspect-Aware Latent Factor Model: Rating Prediction
          with Ratings and Reviews. In <em>WWW 2018: The 2018 Web
          Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186145" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186145</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>When making comments on an item (e.g., <em>product</em>,
      <em>movie</em>, and <em>restaurant</em>) in the online
      review/business websites, such as Yelp and Amazon, reviewers
      also provide an overall rating, which indicates their overall
      preference or satisfaction towards the corresponding items.
      Hence, predicting users’ overall ratings to unrated items or
      <em>personalized rating prediction</em> is an important
      research problem in recommender systems. Latent factor models
      (e.g., matrix factorization&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0009">9</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>]) are the most widely used
      and successful techniques for rating prediction, as
      demonstrated by the Netflix Prize contest&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>]. These
      methods characterize user's interests and item's features
      using <em>latent factors</em> inferred from rating patterns
      in user-item rating records. As a typical collaborative
      filtering technique, the performance of MF suffers when the
      ratings of items or users are insufficient (also known as the
      cold-start problem)&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>]. Besides, a rating only indicates
      the overall satisfaction of a user towards an item, it cannot
      explain the underlying rationale. For example, a user could
      give a restaurant a high rating because of its delicious food
      or due to its nice ambience. Most existing MF models cannot
      provide such fine-grained analysis. Therefore, relying solely
      on ratings makes these methods hard to explicitly and
      accurately model users’ preferences&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0017">17</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0035">35</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0036">36</a>].</p>
      <p>Moreover, MF cannot achieve optimal rating prediction
      locally for each user-item pair, because it learns the latent
      factors of users (p<sub>u</sub>) and items (q<sub>i</sub>)
      via a global optimization strategy&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0010">10</a>]. In other words,
      p<sub>u</sub> and q<sub>i</sub> are optimized to achieve a
      global optimization over all the user-item ratings in the
      training dataset.<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a> As a result, the performance
      could be severely compromised locally for individual users or
      items. MF predicts an unknown rating by the dot product of
      the targeted user <em>u</em>’s and item <em>i</em>’s latent
      factors (e.g., p<sub>u</sub> <sup><em>T</em></sup>
      q<sub>i</sub>). The overall rating of a user towards an item
      (<span class="inline-equation"><span class=
      "tex">$\hat{r}_{u,i}$</span></span> ) is decided by the
      importance/contribution of all factors. Take the
      <em>k</em>-th factor as an example, its contribution is
      <em>p</em> <sub><em>u</em>, <em>k</em></sub> *<em>q</em>
      <sub><em>i</em>, <em>k</em></sub> . For accurate prediction,
      it is important to accurately capture the importance of each
      latent factor for a user towards an item. It is well-known
      that different users may care about different
      <em>aspects</em> of an item. For example, in the domain of
      restaurants, some users care more about the taste of
      <em>food</em> while others pay more attention to the
      <em>ambience</em>. Even for the same aspect, the preference
      of users could be different from each other. For example, in
      the <em>food</em> aspect, some users like <em>Chinese
      cuisines</em> while some others favor <em>Italian
      cuisines</em>. Similarly, the characteristics of items on an
      aspect could also be different from each other. Thus, it is
      possible that “a user <em>u</em> prefers item <em>i</em> but
      dislikes item <em>j</em> on a specific aspect”, while
      “another user <em>u</em>′ favors item <em>j</em> more than
      item <em>i</em> on this aspect”. Therefore, in MF, the
      importance of a latent factor for users towards an item
      should be treated differently. At first glance, MF achieves
      the goal as the influence of a factor (e.g., <em>k</em>-th
      factor) is dependent on both <em>p</em> <sub><em>u</em>,
      <em>k</em></sub> and <em>q</em> <sub><em>i</em>,
      <em>k</em></sub> (i.e., <em>p</em> <sub><em>u</em>,
      <em>k</em></sub> *<em>q</em> <sub><em>i</em>,
      <em>k</em></sub> ). However, it is suboptimal to model the
      importance of a factor by a fixed value of an item or a user.
      In fact, MF treats each factor of an item with the same
      importance to all users (i.e., <em>q</em> <sub><em>i</em>,
      <em>k</em></sub> ); and similarly, each factor of a user is
      equally important to all items (i.e., <em>p</em>
      <sub><em>u</em>, <em>k</em></sub> ) in rating prediction.
      Take the previous example, “<em>a user <em>u</em> prefers
      item <em>i</em> but dislikes item <em>j</em> on an
      aspect</em>”, i.e., a factor (e..g, <em>k</em>) in MF), which
      means <em>p</em> <sub><em>u</em>, <em>k</em></sub>
      *<em>q</em> <sub><em>i</em>, <em>k</em></sub> should be
      larger than <em>p</em> <sub><em>u</em>, <em>k</em></sub>
      *<em>q</em> <sub><em>j</em>, <em>k</em></sub> (i.e.,
      <em>p</em> <sub><em>u</em>, <em>k</em></sub> *<em>q</em>
      <sub><em>i</em>, <em>k</em></sub> &gt; <em>p</em>
      <sub><em>u</em>, <em>k</em></sub> *<em>q</em>
      <sub><em>j</em>, <em>k</em></sub> ). On the other hand,
      “<em>user <em>u</em>′ favors item <em>j</em> more than item
      <em>i</em> on this aspect</em>”, thus <span class=
      "inline-equation"><span class="tex">$p_{u^{\prime
      },k}*q_{j,k}$</span></span> should be larger than
      <span class="inline-equation"><span class="tex">$p_{u^{\prime
      },k}*q_{i,k}$</span></span> (i.e., <span class=
      "inline-equation"><span class="tex">$p_{u^{\prime
      },k}*q_{i,k} {\lt}p_{u^{\prime },k}*q_{j,k}$</span></span> ).
      Because the values of <em>p</em> <sub><em>u</em>,
      <em>k</em></sub> and <span class=
      "inline-equation"><span class="tex">$p_{u^{\prime
      },k}$</span></span> are kept the same when predicting
      ratings, it is impossible for MF to satisfy the local
      requirements <em>p</em> <sub><em>u</em>, <em>k</em></sub>
      *<em>q</em> <sub><em>i</em>, <em>k</em></sub> &gt; <em>p</em>
      <sub><em>u</em>, <em>k</em></sub> *<em>q</em>
      <sub><em>j</em>, <em>k</em></sub> and <span class=
      "inline-equation"><span class="tex">$p_{u^{\prime
      },k}*q_{i,k} {\lt}p_{u^{\prime },k}*q_{j,k}$</span></span>
      simultaneously for these user-item pairs. A straightforward
      solution is to assign different weights (e.g., <em>w</em>
      <sub><em>u</em>, <em>i</em>, <em>k</em></sub> ) to different
      user-item pairs (e.g., <em>p</em> <sub><em>u</em>,
      <em>k</em></sub> *<em>q</em> <sub><em>i</em>,
      <em>k</em></sub> ). However, how to compute a proper weight
      for each user-item pair is challenging.</p>
      <p>A large amount of research effort has been devoted to deal
      with these weaknesses of MF methods. For example, various
      types of side information have been incorporated into MF to
      alleviate the cold-start problem, such as
      tags&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0030">30</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0038">38</a>], social
      relations&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>], reviews&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0039">39</a>], and visual
      features&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>]. Among them, the accompanied review
      of a rating contains important complementary information. It
      not only encodes the information of user preferences and item
      features but also explains the underlying reasons for the
      rating. Therefore, in recent years, many models have been
      developed to exploit reviews with ratings to tackle the
      cold-start problem and also enhance the explainability of MF,
      such as HFT&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>], CTR&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0032">32</a>], RMR&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>], and RBLT&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>]. However,
      a limitation of these models is that they all assume an
      <em>one-to-one correspondence relationship</em> between
      latent topics (learned from reviews) and latent factors
      (learned from ratings), which not only limits their
      flexibility on modeling reviews and ratings but also may not
      be optimal. In addition, they cannot deal with the suboptimal
      recommendation for local users or items in MF. In fact, very
      few studies in literature have considered this problem.</p>
      <p>In this paper, we focus on the problem of <em>personalized
      rating prediction</em> and attempt to tackle the above
      limitations together by utilizing reviews with ratings.
      Specifically, an <em>A</em>spect-aware <em>T</em>opic
      <em>M</em>odel (ATM) is proposed to extract <em>latent
      topics</em> from reviews, which are used to model users’
      preferences and items’ features in different
      <em>aspects</em>. In particular, each <em>aspect</em> of
      users/items is represented as a probability distribution of
      latent topics. Based on the results, the relative importance
      of an aspect (i.e., <em>aspect importance</em>) for a user
      towards an item can be computed. Subsequently, the aspect
      importance is integrated into a developed
      <em>A</em>spect-aware <em>L</em>atent <em>F</em>actor
      <em>M</em>odel (ALFM) to estimate <em>aspect ratings</em>. In
      particular, a weight matrix is introduced in ALFM to
      associate the latent factors to the same set of aspects
      discovered by ATM. In this way, our model avoids referring to
      external sentiment analysis tools for aspect rating
      prediction as in&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0042">42</a>]. The overall rating is obtained by a
      linear combination of the <em>aspect ratings</em>, which are
      weighted by the importance of corresponding aspects (i.e.,
      <em>aspect importance</em>). Note that the latent topics and
      latent factors in our model are not linked directly; instead,
      they are correlated via the <em>aspects</em> indirectly.
      Therefore, the number of latent topics and latent factors
      could be different and separately optimized to model reviews
      and ratings respectively, which is fundamentally different
      from the <em>one-to-one</em> mapping in previous
      models&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0002">2</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0039">39</a>]. Besides,
      our model could learn an aspect importance for each user-item
      pair, namely, assigning a different weight to each <em>p</em>
      <sub><em>u</em>, <em>k</em></sub> *<em>q</em>
      <sub><em>i</em>, <em>k</em></sub> , and thus could alleviate
      the suboptimal local recommendation problem and achieve
      better performance.</p>
      <p>A set of experimental studies has been conducted on 19
      real-world datasets from Yelp and Amazon to validate the
      effectiveness of our proposed model. Experimental results
      show that our model significantly outperforms the
      state-of-the-art methods which also use both reviews and
      ratings for rating prediction. Besides, our model also
      obtains better results for users with few ratings,
      demonstrating the advantages of our model on alleviating the
      cold-start problem. Furthermore, we illustrate the
      interpretability of our model on recommendation results with
      examples. In summary, the main contributions of this work
      include:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We propose a novel aspect-aware
        latent factor model, which could effectively combine
        reviews and ratings for rating prediction. Particularly,
        our model relaxes the constraint of one-to-one mappings
        between the latent topics and latent factors in previous
        models and thus could achieve better
        performance.<br /></li>
        <li id="list2" label="•">Our model could automatically
        extract explainable aspects, and learn the aspect
        importance/weights for different user-item pairs. By
        associating latent factors with aspects, the aspect weights
        are integrated with latent factors for rating prediction.
        Thus, the proposed model could alleviate the suboptimal
        problem of MF for individual user-item pairs.<br /></li>
        <li id="list3" label="•">We conduct comprehensive
        experimental studies to evaluate the effectiveness of our
        model. Results show that our model is significantly better
        than previous approaches on tasks of rating prediction,
        recommendation for sparse data, and recommendation
        interpretability.<br /></li>
      </ul>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <p>A comprehensive review on the recommender system is beyond
      the scope of this work. We mainly discuss the works which
      utilize both reviews and ratings for rating prediction. Some
      works assume that the review is available when predicting the
      rating score, such as SUIT&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>], LARAM&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0033">33</a>], and recent
      DeepCoNN&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0043">43</a>]. However, in real world
      recommendation settings, the task should be predicting
      ratings for the uncommented and unrated items. Therefore, the
      review is unavailable when predicting ratings. We broadly
      classify the approaches for the targeted task in three
      categories: (1) sentiment-based, (2) topic-based, and (3)
      deep learning-based. Our approach falls into the second
      category.</p>
      <p><strong>Sentiment-based.</strong> These works analyze
      user's sentiments on items in reviews to boost the rating
      prediction performance, such as&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0027">27</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0028">28</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0042">42</a>]. For example,
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0027">27</a>]
      estimated a sentiment score for each review to build a
      user-item sentiment matrix, then a traditional collaborative
      filtering method was applied. Zhang et al.&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0042">42</a>] analyzed
      the sentiment polarities of reviews and then jointly
      factorize the user–item rating matrix. These methods rely on
      the performance of external NLP tools for sentiment analysis
      and thus are not self-contained.</p>
      <p><strong>Topic-based.</strong> These approaches extract
      latent topics or aspects from reviews. An early
      work&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0014">14</a>]
      in this direction relied on domain knowledge to manually
      label reviews into different aspects, which requires
      expensive domain knowledge and high labor cost. Later on,
      most works attempt to extract latent topics or aspects from
      reviews automatically&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0039">39</a>]. A general approach of these methods
      is to extract latent topics from reviews using topic
      models&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0023">23</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0039">39</a>] or
      non-negative MF&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0029">29</a>] and learn latent factors from
      ratings using MF methods. HFT&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>] and
      TopicMF&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>] link the latent topics and latent
      factors by using a defined transform function.
      ITLFM&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0039">39</a>]
      and RBLT&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>] assume that the latent topics and
      latent factors are in the same space, and linearly combine
      them to form the latent representations for users and items
      to model the ratings in MF. CTR&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0032">32</a>] assumes that the latent
      factors of items depend on the latent topic distributions of
      their text, and adds a latent variable to offset the topic
      distributions of items when modeling the ratings.
      RMR&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0023">23</a>]
      also learns item's features using topic models on reviews,
      while it models ratings using a mixture of Gaussian rather
      than MF methods. Diao et al.&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>] propose an integrated
      graphical model called JMARS to jointly model aspects,
      ratings and sentiments for movie rating prediction. Those
      models all assume an one-to-one mapping between the learned
      latent topics from reviews and latent factors from ratings.
      Although we adopt the same strategy to extract latent topics
      and learn latent factors, our model does not have the
      constraint of one-to-one mapping. Besides, Zhang et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0042">42</a>]
      extracted aspects by decomposing the user–item rating matrix
      into item–aspect and user–aspect matrices. He et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0017">17</a>]
      extracted latent topics from reviews by modeling the
      user-item-aspect relation with a tripartite graph.</p>
      <p><strong>Deep learning-based</strong>. Recently, there has
      been a trend of applying deep learning techniques in
      recommendation&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>]. For example, He et al. generalized
      matrix factorization and factorization machines to neural
      collaborative filtering and achieved promising
      performance&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>]. Textual reviews have also been used
      in deep learning models for recommendation&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0040">40</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0041">41</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0043">43</a>]. The most
      related works in this direction are DeepCoNN&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0043">43</a>] and
      TransNet&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>], which apply deep techniques to
      reviews for rating prediction. In DeepCoNN, reviews are first
      processed by two CNNs to learn user's and item's
      representations, which are then concatenated and passed into
      a regression layer for rating prediction. A limitation of
      DeepCoNN is that it uses reviews in the testing phase.
      &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0006">6</a>]
      shows that the performance of DeepCoNN decreases greatly when
      reviews are unavailable in the testing phase. To deal with
      the problem, TransNet&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>] extends DeepCoNN by introducing an
      additional layer to simulate the review corresponding to the
      target user-item pair. The generated review is then used for
      rating prediction.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Notations and their definitions</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;">Notation</th>
              <th style="text-align:left;">Definition</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {D}$</span></span></td>
              <td style="text-align:left;">corpus with reviews and
              ratings</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>d</em>
              <sub><em>u</em>, <em>i</em></sub></td>
              <td style="text-align:left;">review document of user
              <em>u</em> to item <em>i</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>s</em></td>
              <td style="text-align:left;">a sentence in a review
              <em>d</em> <sub><em>u</em>, <em>i</em></sub></td>
            </tr>
            <tr>
              <td style="text-align:left;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {U}$</span></span> , <span class=
              "inline-equation"><span class="tex">$\mathcal
              {I}$</span></span> , <span class=
              "inline-equation"><span class="tex">$\mathcal
              {A}$</span></span></td>
              <td style="text-align:left;">user set, item set, and
              aspect set, respectively</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>M</em>, <em>N</em>,
              <em>A</em></td>
              <td style="text-align:left;">number of users, items,
              and aspects, respectively</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>N</em>
              <sub><em>w</em>, <em>s</em></sub></td>
              <td style="text-align:left;">number of words in a
              sentence <em>s</em></td>
            </tr>
            <tr style="border-top: solid 2px">
              <td style="text-align:left;"><em>K</em></td>
              <td style="text-align:left;">number of latent topics
              in ATM</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>y</em></td>
              <td style="text-align:left;">an indicator variable in
              ATM</td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <em>a<sub>s</sub></em></td>
              <td style="text-align:left;">assigned aspect
              <em>a</em> to sentence <em>s</em></td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <em>π<sub>u</sub></em></td>
              <td style="text-align:left;">the parameter of
              Bernoulli distribution <em>P</em>(<em>y</em> =
              0)</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>η</em></td>
              <td style="text-align:left;">Beta priors (<em>η</em>
              = {<em>η</em> <sub>0</sub>, <em>η</em>
              <sub>1</sub>})</td>
            </tr>
            <tr>
              <td style="text-align:left;">α<sub>u</sub>,
              α<sub>i</sub></td>
              <td style="text-align:left;">Dirichlet priors for
              aspect-topic distributions</td>
            </tr>
            <tr>
              <td style="text-align:left;">γ<sub>u</sub>,
              γ<sub>i</sub></td>
              <td style="text-align:left;">Dirichlet priors for
              aspect distributions</td>
            </tr>
            <tr>
              <td style="text-align:left;">β<sub>w</sub></td>
              <td style="text-align:left;">Dirichlet priors for
              topic-word distributions</td>
            </tr>
            <tr>
              <td style="text-align:left;">θ<sub>u, a</sub></td>
              <td style="text-align:left;">user's aspect-topic
              distribution: denoting user's preference on
              <em>a</em></td>
            </tr>
            <tr>
              <td style="text-align:left;">ψ<sub>i, a</sub></td>
              <td style="text-align:left;">item's aspect-topic
              distribution: denoting item's features on
              <em>a</em></td>
            </tr>
            <tr>
              <td style="text-align:left;">λ<sub>u</sub>,
              λ<sub>v</sub></td>
              <td style="text-align:left;">aspect distributions of
              user and item, respectively</td>
            </tr>
            <tr>
              <td style="text-align:left;">ϕ<sub>w</sub></td>
              <td style="text-align:left;">topic-text word
              distribution</td>
            </tr>
            <tr style="border-top: solid 2px">
              <td style="text-align:left;"><em>f</em></td>
              <td style="text-align:left;">number of latent factors
              in ALFM</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>μ</em>
              <sub>·</sub></td>
              <td style="text-align:left;">regularization
              coefficients</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>b</em>
              <sub>·</sub></td>
              <td style="text-align:left;">bias terms, e.g.,
              <em>b<sub>u</sub></em> , <em>b<sub>i</sub></em> ,
              <em>b</em> <sub>0</sub></td>
            </tr>
            <tr>
              <td style="text-align:left;">
              <em>w<sub>a</sub></em></td>
              <td style="text-align:left;">weight vector for aspect
              <em>a</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>p<sub>u</sub></em> ,
              <em>q<sub>i</sub></em></td>
              <td style="text-align:left;">latent factors of user
              <em>u</em> and item <em>i</em>, respectively</td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>r</em>
              <sub><em>u</em>, <em>i</em></sub></td>
              <td style="text-align:left;">rating of user
              <em>u</em> to item <em>i</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>r</em>
              <sub><em>u</em>, <em>i</em>, <em>a</em></sub></td>
              <td style="text-align:left;">aspect rating of user
              <em>u</em> towards item <em>i</em> on aspect
              <em>a</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>ρ</em>
              <sub><em>u</em>, <em>i</em>, <em>a</em></sub></td>
              <td style="text-align:left;">aspect importance of
              <em>a</em> for <em>u</em> with respect to
              <em>i</em></td>
            </tr>
            <tr>
              <td style="text-align:left;"><em>s</em>
              <sub><em>u</em>, <em>i</em>, <em>a</em></sub></td>
              <td style="text-align:left;">the degree of item
              <em>i</em>’s attributes matching user <em>u</em>’s
              preference</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:left;">on aspect
              <em>a</em></td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Proposed
          Model</h2>
        </div>
      </header>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Problem
            Setting</h3>
          </div>
        </header>
        <p>Let <span class="inline-equation"><span class=
        "tex">$\mathcal {D}$</span></span> be a collection of
        reviews of item set <span class=
        "inline-equation"><span class="tex">$\mathcal
        {I}$</span></span> from a specific category (e.g.,
        restaurant) written by a set of users <span class=
        "inline-equation"><span class="tex">$\mathcal
        {U}$</span></span> , and each review comes with an overall
        rating <em>r</em> <sub><em>u</em>, <em>i</em></sub> to
        indicate the overall satisfaction of user <em>u</em> to
        item <em>i</em>. The primary goal is to predict the unknown
        ratings of items that the users have not reviewed yet. A
        review <em>d</em> <sub><em>u</em>, <em>i</em></sub> is a
        piece of text which describes opinions of user <em>u</em>
        on different aspects <span class=
        "inline-equation"><span class="tex">$a \in \mathcal
        {A}$</span></span> towards item <em>i</em>, such as
        <em>food</em> for <em>restaurants</em>. In this paper, we
        only consider the case that all the items are from the same
        category, i.e., they share the same set of aspects
        <span class="inline-equation"><span class="tex">$\mathcal
        {A}$</span></span> . Aspects that users care for items are
        latent and learned from reviews by our proposed topic
        model, in which each aspect is represented as a
        distribution of the same set (e.g., <em>K</em>) of latent
        topics. Table&nbsp;<a class="tbl" href="#tab1">1</a> lists
        the key notations. Before introducing our method, we would
        like to first clarify the concepts of <em>aspects</em>,
        <em>latent topics</em>, and <em>latent factors</em>.</p>
        <ul class="list-no-style">
          <li id="list4" label="•"><strong>Aspect</strong> - it is
          a high-level semantic concept, which represents the
          attribute of items that users commented on in reviews,
          such as <em>“food”</em> for <em>restaurant</em> and
          <em>“battery”</em> for <em>mobile phones</em>.<br /></li>
          <li id="list5" label="•"><strong>Latent topic &amp;
          latent factor</strong> - in our context, both concepts
          represent a more fine-grained concept than
          <em>“aspect”</em>. A latent topic or factor can be
          regarded as a <em>sub-aspect</em> of an item. For
          instance, for the “food” aspect, a related latent topic
          could be “<em>breakfast</em>” or “<em>Italian
          cuisine</em>”. We adopt the terminology of <em>latent
          topic</em> in topic models and <em>latent factor</em> in
          matrix factorization. Accordingly, “latent topics” are
          discovered by topic model on reviews, and “latent
          factors” are learned by matrix factorization on
          ratings.<br /></li>
        </ul>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span>
            Aspect-aware Latent Factor Model</h3>
          </div>
        </header>
        <p>Based on the observations that (1) different users may
        care for different aspects of an item and (2) users’
        preferences may differ from each other for the same aspect,
        we claim that the overall satisfaction of a user <em>u</em>
        towards an item <em>i</em> (i.e., the overall rating
        <em>r</em> <sub><em>u</em>, <em>i</em></sub> ) depends on
        <em>u</em>’s satisfaction on each aspect <em>a</em> of
        <em>i</em> (i.e., <em>aspect rating r</em> <sub><em>u</em>,
        <em>i</em>, <em>a</em></sub> ) and the importance of each
        aspect (of <em>i</em>) to <em>u</em> (i.e., <em>aspect
        importance ρ</em> <sub><em>u</em>, <em>i</em>,
        <em>a</em></sub> ). Based on the assumptions, the overall
        rating <em>r</em> <sub><em>u</em>, <em>i</em></sub> can be
        predicted as:</p>
        <div class="table-responsive" id="Xeq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \hat{r}_{u,i}
            = \sum _{a\in \mathcal {A}} \overbrace{\rho
            _{u,i,a}}^{{\mbox{aspect importance}}}
            \underbrace{r_{u,i,a}}_{{\mbox{aspect rating}}}
            \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <p></p>
        <section id="sec-10">
          <p><em>3.2.1 Aspect rating estimation..</em> Aspect
          rating (i.e., <em>r</em> <sub><em>u</em>, <em>i</em>,
          <em>a</em></sub> ) reflects the satisfaction of a user
          <em>u</em> towards an item <em>i</em> on the aspect
          <em>a</em>. To receive a high aspect rating <em>r</em>
          <sub><em>u</em>, <em>i</em>, <em>a</em></sub> , an item
          should at least possess the characteristics/attributes in
          which the user is interested in this aspect. Moreover,
          the item should satisfy user's expectations on these
          attributes in this aspect. In other words, the item's
          attributes on this aspect should be of high quality such
          that the user likes it. Take the “food” aspect as an
          example, for a user who likes Chinese cuisines, to
          receive a high rating on the <em>“food”</em> aspect from
          the user, a restaurant should provide Chinese dishes and
          the dishes should suit the user's taste. Based on user's
          text reviews, we can learn users’ preferences and items’
          characteristics on each aspect and measure <em>how the
          attributes of an item <em>i</em> on aspect “<em>a</em>”
          suit a user <em>u</em>’s requirements on this
          aspect</em>, denoted by <em>s</em> <sub><em>u</em>,
          <em>i</em>, <em>a</em></sub> . We compute <em>s</em>
          <sub><em>u</em>, <em>i</em>, <em>a</em></sub> based on
          results of the proposed Aspect-aware Topic Model (ATM)
          (described in Sect.&nbsp;<a class="sec" href=
          "#sec-12">3.3</a>), in which user's preferences and
          item's characteristics on each aspect are modeled as
          multinomial distributions of latent topics, denoted by
          θ<sub>u, a</sub> and ψ<sub>i, a</sub>, respectively.
          <em>s</em> <sub><em>u</em>, <em>i</em>, <em>a</em></sub>
          ∈ [0, 1] is then computed as :</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} s_{u,i,a} =
              1-JSD({\theta _{u,a}}, {\psi _{i,a}})
              \end{equation}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>where <em>JSD</em>(θ<sub>u, a</sub>, ψ<sub>i,
          a</sub>) denotes the Jensen–Shannon
          divergence&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0013">13</a>] between θ<sub>u, a</sub> and
          ψ<sub>i, a</sub>. Notice that a large value of <em>s</em>
          <sub><em>u</em>, <em>i</em>, <em>a</em></sub> does not
          mean a high rating <em>r</em> <sub><em>u</em>,
          <em>i</em>, <em>a</em></sub> - an item providing all the
          features that a user <em>u</em> requires does not mean
          that it satisfies <em>u</em>’s expectations, since the
          provided ones could be of low quality. For instance, a
          restaurant provides all the Chinese dishes the user
          <em>u</em> likes (i.e., high score <em>s</em>
          <sub><em>u</em>, <em>i</em>, <em>a</em></sub> ), but
          these dishes taste bad from <em>u</em>’s opinion (i.e.,
          low rating <em>r</em> <sub><em>u</em>, <em>i</em>,
          <em>a</em></sub> ). Therefore, we can expect that for
          this restaurant: users discuss its Chinese dishes in
          reviews with negative opinions and thus give low ratings.
          Instead of analyzing the review sentiments for aspect
          rating estimation by using external NLP tools (such
          as&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0042">42</a>]), we refer to the matrix
          factorization (MF)&nbsp;[<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0021">21</a>] technique.
          <p></p>
          <p>MF maps users and items into a latent factor space and
          represents users’ preferences and items’ features by
          <em>f</em>-dim latent factor vectors (i.e., <span class=
          "inline-equation"><span class="tex">$ {p_u} \in \mathbb
          {R}^{f \times 1}$</span></span> and <span class=
          "inline-equation"><span class="tex">$ {q_i} \in \mathbb
          {R}^{f \times 1}$</span></span> ). The dot product of the
          user's and item's vectors (p<sub>u</sub>
          <sup><em>T</em></sup> q<sub>i</sub>) characterizes the
          user's overall interests on the item's characteristics,
          and is thus used to predict the rating <em>r</em>
          <sub><em>u</em>, <em>i</em></sub> . To extend MF for
          aspect rating prediction, we introduce a binary matrix
          <span class="inline-equation"><span class="tex">$ {W} \in
          \mathbb {R}^{f \times A}$</span></span> to associate the
          latent factors to different aspects, where <em>A</em> is
          the number of aspects considered. We call this model
          aspect-aware latent factor model (ALFM), in which the
          weight vector w<sub>a</sub> in the <em>a</em>-th column
          of W indicates which factors are related to the aspect
          <em>a</em>. Thus, p<sub>u, a</sub> =
          w<sub>a</sub>⊙p<sub>u</sub> denotes user's interests in
          the aspect <em>a</em>, where ⊙ represents element-wise
          product between vectors. Therefore, (p<sub>u, a</sub>)
          <sup><em>T</em></sup> (q<sub>i, a</sub>) represents the
          aspect rating of user <em>u</em> to item <em>i</em> on
          aspect <em>a</em>. Finally, we integrate the matching
          results of aspects (i.e., <em>s</em> <sub><em>u</em>,
          <em>i</em>, <em>a</em></sub> ) into ALFM to estimate the
          aspect ratings:</p>
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} r_{u,i,a} =
              s_{u,i,a} \cdot ({w_a} \odot {p_u})^T({w_a} \odot
              {q_i}) \end{equation}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>As a high aspect rating <em>r</em> <sub><em>u</em>,
          <em>i</em>, <em>a</em></sub> requires large values of
          both <em>s</em> <sub><em>u</em>, <em>i</em>,
          <em>a</em></sub> and (w<sub>a</sub>⊙p<sub>u</sub>)
          <sup><em>T</em></sup> (w<sub>a</sub>⊙q<sub>i</sub>), it
          is expected that the results learned from reviews could
          guide the learning of latent factors.
          <p></p>
        </section>
        <section id="sec-11">
          <p><em>3.2.2 Aspect importance estimation..</em> We rely
          on user reviews to estimate <em>ρ</em> <sub><em>u</em>,
          <em>i</em>, <em>a</em></sub> , as users often discuss
          their interest topics of aspects in reviews, such as
          different <em>cuisines</em> in the <em>food</em> aspect.
          In general, the more a user comments on an aspect in
          reviews, the more important this aspect is (to this
          user). Thus, we estimate the importance of an aspect
          according to the possibility of a user writing review
          comments on this aspect. When writing a review, some
          users tend to write comments from the aspects according
          to their own preferences, while others like commenting on
          the most notable features of the targeted item. Based on
          this consideration, we introduce (1)
          <em>π<sub>u</sub></em> to denote the probability of user
          <em>u</em> commenting an item based on his own preference
          and (2) <em>λ</em> <sub><em>u</em>, <em>a</em></sub>
          (<span class="inline-equation"><span class="tex">$\sum
          _{a\in \mathcal {A}}\lambda _{u,a}=1$</span></span> ) to
          denote the probability of user <em>u</em> commenting on
          the aspect <em>a</em> based on his own preference.
          Accordingly, (1 − <em>π<sub>u</sub></em> ) denotes the
          probability of the user commenting from the item
          <em>i</em>’s characteristics (<span class=
          "inline-equation"><span class="tex">$\sum _{a\in \mathcal
          {A}}\lambda _{i,a}=1$</span></span> ), and <em>λ</em>
          <sub><em>i</em>, <em>a</em></sub> is the probability of
          user <em>u</em> commenting item <em>i</em> from the
          item's characteristics on the aspect <em>a</em>. Thus,
          the probability of a user <em>u</em> commenting an item
          <em>i</em> on an aspect <em>a</em> (i.e., <em>ρ</em>
          <sub><em>u</em>, <em>i</em>, <em>a</em></sub> ) is:</p>
          <div class="table-responsive" id="eq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \rho
              _{u,i,a} = \pi _{u}\lambda _{u,a} + (1-\pi
              _{u})\lambda _{i,a} \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div><em>λ</em> <sub><em>u</em>, <em>a</em></sub> ,
          <em>λ</em> <sub><em>i</em>, <em>a</em></sub> , and
          <em>π<sub>u</sub></em> are estimated by ATM, which
          simulates the process of a user writing a review, as
          detailed in the next subsection.
          <p></p>
        </section>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Aspect-aware Topic Model</h3>
          </div>
        </header>
        <p>Given a corpus <span class=
        "inline-equation"><span class="tex">$\mathcal
        {D}$</span></span> , which contains reviews of users
        towards items <span class="inline-equation"><span class=
        "tex">$\lbrace d_{u,i}|d_{u,i} \in \mathcal {D}, u \in
        \mathcal {U}, i \in \mathcal {I}\rbrace$</span></span> , we
        assume that a set of latent topics (i.e., <em>K</em>
        topics) covers all the topics that users discuss in the
        reviews. λ<sub>u</sub> is a probability distribution of
        aspects in user <em>u</em>’s preferences, in which each
        value <em>λ</em> <sub><em>u</em>, <em>a</em></sub> denotes
        the relative importance of an aspect <em>a</em> to the user
        <em>u</em>. Similarly, λ<sub>i</sub> is the probability
        distribution of aspects in item <em>i</em>’s
        characteristics, in which each value <em>λ</em>
        <sub><em>i</em>, <em>a</em></sub> denotes the importance of
        an aspect <em>a</em> to the item <em>i</em>. As the
        <em>K</em> latent topics cover all the topics discussed in
        reviews, an aspect will only relate to some of the latent
        topics closely. For example, topic “<em>breakfast</em>” is
        closely related to aspect “food”, while it is not related
        to aspects like “<em>service</em>” or “<em>price</em>”. The
        relation between aspects and topics is also represented by
        a probabilistic distribution, i.e., θ<sub>u, a</sub> for
        users and ψ<sub>i, a</sub> for items. More detailedly, the
        interests of a user <em>u</em> in a specific aspect
        <em>a</em> is represented by θ<sub>u, a</sub>, which is a
        multinomial distribution of the latent topics; the
        characteristics of an item <em>i</em> in a specific aspect
        <em>a</em> is represented by ψ<sub>i, a</sub>, which is
        also a multinomial distribution of the same set of latent
        topics. θ<sub>u, a</sub> is determined based on all the
        reviews <span class="inline-equation"><span class=
        "tex">$\lbrace d_{u,i} | i \in \mathcal
        {I}\rbrace$</span></span> of user <em>u</em> writing for
        items. ψ<sub>i, a</sub> is learned from all the reviews
        <span class="inline-equation"><span class="tex">$\lbrace
        d_{u,i} | u \in \mathcal {U}\rbrace$</span></span> of
        <em>i</em> written by users. A latent topic is a
        multinomial distribution of text words in reviews. Based on
        these assumptions, we propose an aspect-aware topic model
        ATM to estimate the parameters {λ<sub>i</sub>,
        λ<sub>i</sub>, θ<sub>u, a</sub>, ψ<sub>i, a</sub>,
        <em>π<sub>u</sub></em> } by simulating the generation of
        the corpus <span class="inline-equation"><span class=
        "tex">$\mathcal {D}$</span></span> .</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186145/images/www2018-154-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">The graphical representation
            of the ATM model.</span>
          </div>
        </figure>
        <p></p>
        <p>The graphical representation of ATM is shown in
        Fig.&nbsp;<a class="fig" href="#fig1">1</a>. In the figure,
        the shaded circles indicate observed variables, while the
        unshaded ones represent the latent variables. ATM mimics
        the processing of writing a review sentence by sentence. A
        sentence usually discusses the same topic <em>z</em>, which
        could be from user's preferences or from item's
        characteristics. To decide the topic <em>z<sub>s</sub></em>
        for a sentence <em>s</em>, our model introduces an
        indicator variable <em>y</em> ∈ {0, 1} based on a Bernoulli
        distribution, which is parameterized by
        <em>π<sub>u</sub></em> . Specifically, when <em>y</em> = 0,
        the sentence is generated from user's preference;
        otherwise, it is generated according to item <em>i</em>’s
        characteristics. <em>π<sub>u</sub></em> is user-dependent,
        indicating the tendency to comment from <em>u</em>’s
        personal preferences or from the item <em>i</em>’s
        characteristics is determined by <em>u</em>’s personality.
        The generation process of ATM is shown in Algorithm&nbsp;1.
        Let <em>a<sub>s</sub></em> denote the aspect assigned to a
        sentence <em>s</em>. If <em>y</em> = 0,
        <em>a<sub>s</sub></em> is drawn from <em>λ<sub>u</sub></em>
        and <em>z<sub>s</sub></em> is then generated from
        <em>u</em>’s preferences on aspect <em>a<sub>s</sub></em> :
        <span class="inline-equation"><span class="tex">$ {\theta
        _{u,a_s}}$</span></span> ; otherwise, if <em>y</em> = 1,
        <em>a<sub>s</sub></em> is drawn from <em>λ<sub>i</sub></em>
        and <em>z<sub>s</sub></em> is then generated from
        <em>i</em>’s characteristics on aspect
        <em>a<sub>s</sub></em> : <span class=
        "inline-equation"><span class="tex">$ {\psi
        _{i,a_s}}$</span></span> . Then all the words <em>w</em> in
        sentence <em>s</em> is generated from
        <em>z<sub>s</sub></em> according to the word distribution:
        <span class="inline-equation"><span class="tex">$ {\phi
        _{z_s, w}}$</span></span> .</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186145/images/www2018-154-img1.svg"
        class="img-responsive" alt="" longdesc="" /></p>
        <p>In ATM, α<sub>u</sub>, α<sub>i</sub>, γ<sub>u</sub>,
        γ<sub>i</sub>, β, and <em>η</em> are pre-defined
        hyper-parameters and set to be symmetric for simplicity.
        Parameters need to be estimated including λ<sub>i</sub>,
        λ<sub>i</sub>, θ<sub>u, a</sub>, ψ<sub>i, a</sub>, and
        <em>π<sub>u</sub></em> . Different approximate inference
        methods have been developed for parameter estimation in
        topic models, such as variation inference&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0005">5</a>] and
        collapsed Gibbs sampling&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>]. We apply collapsed Gibbs sampling
        to infer the parameters, since it has been successfully
        applied in many large scale applications of topic
        models&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0008">8</a>]. Due to the space limitation, we
        omit the detailed inference steps in this paper.</p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Model
            Inference</h3>
          </div>
        </header>
        <p>With the results of ATM, <em>ρ</em> <sub><em>u</em>,
        <em>i</em>, <em>a</em></sub> and <em>s</em>
        <sub><em>u</em>, <em>i</em>, <em>a</em></sub> can be
        computed using Eq.&nbsp;<a class="eqn" href="#eq2">4</a>
        and &nbsp;<a class="eqn" href="#eq1">2</a>, respectively.
        With the consideration of bias terms (i.e.,
        <em>b<sub>u</sub></em> , <em>b<sub>i</sub></em> ,
        <em>b</em> <sub>0</sub>) in ALFM, the overall rating can be
        estimated as<a class="fn" href="#fn2" id=
        "foot-fn2"><sup>2</sup></a>,</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \hat{r}_{u,i}
            = \sum _{a\in \mathcal {A}}(\rho _{u,i,a}\cdot
            s_{u,i,a} \cdot ({w_a} \odot {p_u})^T({w_a} \odot
            {q_i})) + b_u + b_i + b_0 \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>where <em>b</em> <sub>0</sub> is the average rating,
        <em>b<sub>u</sub></em> and <em>b<sub>i</sub></em> are user
        and item biases, respectively. The estimation of parameters
        is to minimize the rating prediction error in the training
        dataset. The optimization objective function is
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \begin{split}
            \underset{p*,q*}{\text{min}} \frac{1}{2}\sum _{u,i}
            (r_{u,i} &amp; -\hat{r}_{u,i})^2 + \frac{\mu _u}{2} ||
            {p_u}||_2^2 + \frac{\mu _i}{2} || {q_i}||_2^2 \\ &amp;
            + \mu _w \sum _a|| {w_a}||_1 + \frac{\mu _b}{2}
            (||b_u||_2^2 + ||b_i||_2^2); \end{split}
            \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>where || · ||<sub>2</sub> denotes the ℓ<sub>2</sub>
        norm for preventing model overfitting, and || ·
        ||<sub>1</sub> denotes the ℓ<sub>1</sub> norm.
        <em>μ<sub>u</sub></em> , <em>μ<sub>i</sub></em> ,
        <em>μ<sub>w</sub></em> , and <em>μ<sub>b</sub></em> are
        regularization parameters, which are tunable
        hyper-parameters. In practice, we relax the binary
        requirement of w<sub>a</sub> by using ℓ
        <sub><em>l</em></sub> norm. It is well known that ℓ
        <sub><em>l</em></sub> regularization yields sparse solution
        of the weights&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>]. The ℓ<sub>2</sub> regularization
        of p<sub>u</sub> and q<sub>i</sub> prevents them to have
        arbitrarily large values, which would lead to arbitrarily
        small values of w<sub>a</sub>.
        <p></p>
        <p><strong>Optimization.</strong> We use the stochastic
        gradient descent (SGD) algorithm to learn the parameters by
        optimizing the objective function in Eq.&nbsp;<a class=
        "eqn" href="#eq4">6</a>. In each step of SGD, the localized
        optimization is performed on a rating <em>r</em>
        <sub><em>u</em>, <em>i</em></sub> . Let <em>L</em> denote
        the loss, and the gradients of parameters are given as
        follows:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \frac{\partial
            L}{\partial p_u}&amp;=\sum _{i=1}^{N}(\sum _{a}\rho
            _{u,i,a}s_{u,i,a}w_a^2)(\hat{r}_{u,i}-r_{u,i})q_i + \mu
            _u p_u \\\frac{\partial L}{\partial q_i}&amp;=\sum
            _{u=1}^{M}(\sum _{a}\rho
            _{u,i,a}s_{u,i,a}w_a^2)(\hat{r}_{u,i}-r_{u,i})p_u + \mu
            _i q_i \\\frac{\partial L}{\partial w_a}&amp;=\sum
            _{u=1}^{M}\sum _{i=1}^{N}\rho
            _{u,i,a}s_{u,i,a}(\hat{r}_{u,i}-r_{u,i})p_uq_iw_a +
            \frac{\mu _w w_a}{\sqrt
            {(w_a^2+\epsilon)}}\end{align*}</span><br />
          </div>
        </div>Here, we omit the gradients of <em>b<sub>u</sub></em>
        and <em>b<sub>i</sub></em> , as they are the same as in the
        standard biased MF&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>]. <em>M</em> and <em>N</em> are the
        total number of users and items in the dataset. Notice that
        in the deriving of the gradient for <em>w<sub>a</sub></em>
        , we use <span class="inline-equation"><span class=
        "tex">$\sqrt {w_a^2+\epsilon }$</span></span> in place of
        ||<em>w<sub>a</sub></em> ||<sub>1</sub>, because
        ℓ<sub>1</sub> norm is not differentiable at 0. ϵ can be
        regarded as a “smoothing parameter” and is set to 10<sup>−
        6</sup> in our implementation.
        <p></p>
      </section>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experimental
          Study</h2>
        </div>
      </header>
      <p>To validate the assumptions when designing the model and
      evaluate our proposed model, we conducted comprehensive
      experimental studies to answer the following questions:</p>
      <ul class="list-no-style">
        <li id="list6" label="•">
          <strong>RQ1:</strong> How do the important parameters
          (e.g., the number of latent topics and latent factors)
          affect the performance of our model? More importantly, is
          the setting <em>f</em> = <em>K</em> optimal, which is a
          default assumption for many previous models?
          (Sect.&nbsp;<a class="sec" href="#sec-17">4.3</a>)<br />
        </li>
        <li id="list7" label="•">
          <strong>RQ2:</strong> Can our ALFM model outperform the
          state-of-the-art recommendation methods, which consider
          both ratings and reviews, on rating prediction?
          (Sect.&nbsp;<a class="sec" href="#sec-18">4.4</a>)<br />
        </li>
        <li id="list8" label="•">
          <strong>RQ3:</strong> Compared to other methods which
          also use textual reviews and ratings, how does our ALFM
          model perform on the cold-start setting when users have
          only few ratings? (Sect.&nbsp;<a class="sec" href=
          "#sec-19">4.5</a>)<br />
        </li>
        <li id="list9" label="•">
          <strong>RQ4:</strong> Can our model explicitly interpret
          the reasons for a high or low rating?
          (Sect.&nbsp;<a class="sec" href="#sec-20">4.6</a>)<br />
        </li>
      </ul>
      <div class="table-responsive" id="tab2">
        <div class="table-caption">
          <span class="table-number">Table 2:</span> <span class=
          "table-title">Statistics of the evaluation
          datasets</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">Datasets</th>
              <th style="text-align:center;">#users</th>
              <th style="text-align:center;">#items</th>
              <th style="text-align:center;">#ratings</th>
              <th style="text-align:center;">Sparsity</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">Instant Video</td>
              <td style="text-align:center;">4,902</td>
              <td style="text-align:center;">1,683</td>
              <td style="text-align:center;">36,486</td>
              <td style="text-align:center;">0.9956</td>
            </tr>
            <tr>
              <td style="text-align:center;">Automotive</td>
              <td style="text-align:center;">2,788</td>
              <td style="text-align:center;">1,835</td>
              <td style="text-align:center;">20,218</td>
              <td style="text-align:center;">0.9960</td>
            </tr>
            <tr>
              <td style="text-align:center;">Baby</td>
              <td style="text-align:center;">17,177</td>
              <td style="text-align:center;">7,047</td>
              <td style="text-align:center;">158,311</td>
              <td style="text-align:center;">0.9987</td>
            </tr>
            <tr>
              <td style="text-align:center;">Beauty</td>
              <td style="text-align:center;">19,766</td>
              <td style="text-align:center;">12,100</td>
              <td style="text-align:center;">196,325</td>
              <td style="text-align:center;">0.9992</td>
            </tr>
            <tr>
              <td style="text-align:center;">Cell Phones</td>
              <td style="text-align:center;">24,650</td>
              <td style="text-align:center;">10,420</td>
              <td style="text-align:center;">189,255</td>
              <td style="text-align:center;">0.9993</td>
            </tr>
            <tr>
              <td style="text-align:center;">Clothing</td>
              <td style="text-align:center;">34,447</td>
              <td style="text-align:center;">23,026</td>
              <td style="text-align:center;">277,324</td>
              <td style="text-align:center;">0.9997</td>
            </tr>
            <tr>
              <td style="text-align:center;">Digital Music</td>
              <td style="text-align:center;">5,426</td>
              <td style="text-align:center;">3,568</td>
              <td style="text-align:center;">64,475</td>
              <td style="text-align:center;">0.9967</td>
            </tr>
            <tr>
              <td style="text-align:center;">Grocery</td>
              <td style="text-align:center;">13,979</td>
              <td style="text-align:center;">8,711</td>
              <td style="text-align:center;">149,434</td>
              <td style="text-align:center;">0.9988</td>
            </tr>
            <tr>
              <td style="text-align:center;">Health</td>
              <td style="text-align:center;">34,850</td>
              <td style="text-align:center;">18,533</td>
              <td style="text-align:center;">342,262</td>
              <td style="text-align:center;">0.9995</td>
            </tr>
            <tr>
              <td style="text-align:center;">Home &amp;
              Kitchen</td>
              <td style="text-align:center;">58,901</td>
              <td style="text-align:center;">28,231</td>
              <td style="text-align:center;">544,239</td>
              <td style="text-align:center;">0.9997</td>
            </tr>
            <tr>
              <td style="text-align:center;">Musical
              Instruments</td>
              <td style="text-align:center;">1,397</td>
              <td style="text-align:center;">900</td>
              <td style="text-align:center;">10,216</td>
              <td style="text-align:center;">0.9919</td>
            </tr>
            <tr>
              <td style="text-align:center;">Office Products</td>
              <td style="text-align:center;">4,798</td>
              <td style="text-align:center;">2,419</td>
              <td style="text-align:center;">52,673</td>
              <td style="text-align:center;">0.9955</td>
            </tr>
            <tr>
              <td style="text-align:center;">Patio</td>
              <td style="text-align:center;">1,672</td>
              <td style="text-align:center;">962</td>
              <td style="text-align:center;">13,077</td>
              <td style="text-align:center;">0.9919</td>
            </tr>
            <tr>
              <td style="text-align:center;">Pet Supplies</td>
              <td style="text-align:center;">18,070</td>
              <td style="text-align:center;">8,508</td>
              <td style="text-align:center;">155,692</td>
              <td style="text-align:center;">0.9990</td>
            </tr>
            <tr>
              <td style="text-align:center;">Sports &amp;
              Outdoors</td>
              <td style="text-align:center;">31,176</td>
              <td style="text-align:center;">18,355</td>
              <td style="text-align:center;">293,306</td>
              <td style="text-align:center;">0.9995</td>
            </tr>
            <tr>
              <td style="text-align:center;">Tools &amp; Home</td>
              <td style="text-align:center;">15,438</td>
              <td style="text-align:center;">10,214</td>
              <td style="text-align:center;">133,414</td>
              <td style="text-align:center;">0.9992</td>
            </tr>
            <tr>
              <td style="text-align:center;">Toys &amp; Games</td>
              <td style="text-align:center;">17,692</td>
              <td style="text-align:center;">11,924</td>
              <td style="text-align:center;">166,180</td>
              <td style="text-align:center;">0.9992</td>
            </tr>
            <tr>
              <td style="text-align:center;">Video Games</td>
              <td style="text-align:center;">22,348</td>
              <td style="text-align:center;">10,672</td>
              <td style="text-align:center;">228,164</td>
              <td style="text-align:center;">0.9990</td>
            </tr>
            <tr>
              <td style="text-align:center;">Yelp 2017</td>
              <td style="text-align:center;">169,257</td>
              <td style="text-align:center;">63,300</td>
              <td style="text-align:center;">1,659,678</td>
              <td style="text-align:center;">0.9998</td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Dataset
            Description</h3>
          </div>
        </header>
        <p>We conducted experiments on two publicly accessible
        datasets that provide user review and rating information.
        The first dataset is Amazon Product Review dataset
        collected by&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>]<a class="fn" href="#fn3" id=
        "foot-fn3"><sup>3</sup></a>, which contains product reviews
        and metadata from Amazon. This dataset has been widely used
        for rating prediction with reviews and ratings in previous
        studies&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>]. The dataset is organized into 24
        product categories. In this paper, we used 18 categories
        (See Table&nbsp;<a class="tbl" href="#tab2">2</a>) and
        focus on the 5-core version, with at least 5 reviews for
        each user or item. The other dataset is from Yelp Dataset
        Challenge 2017<a class="fn" href="#fn4" id=
        "foot-fn4"><sup>4</sup></a>, which includes reviews of
        local business in 12 metropolitan areas across 4 countries.
        For the Yelp 2017 dataset, we also processed it to keep
        users and items with at least 5 reviews. From each review
        in these datasets, we extract the corresponding “userID”,
        “itemID”, a rating score (from 1 to 5 rating stars), and a
        textual review for experiments. Notice that for all the
        datasets, we checked and removed the duplicates, and then
        filtered again to keep them as 5-core. Besides, we removed
        the infrequent terms in the reviews for each
        dataset.<a class="fn" href="#fn5" id=
        "foot-fn5"><sup>5</sup></a> Some statistics of the datasets
        are shown in Table&nbsp;<a class="tbl" href=
        "#tab2">2</a>.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Experimental Settings</h3>
          </div>
        </header>
        <p>For each dataset, we randomly split it into training,
        validation, and testing set with ratio 80:10:10 for each
        user as in&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>]. Because we take the 5-core
        dataset where each user has at least 5 interactions, we
        have at least 3 interactions per user for training, and at
        least 1 interaction per user for validation and testing.
        Note that we only used the review information in the
        training set, because the reviews in the validation or
        testing set are unavailable during the prediction process
        in real scenarios. The number of aspect is set to 5 in
        experiments.<a class="fn" href="#fn6" id=
        "foot-fn6"><sup>6</sup></a></p>
        <p><strong>Baselines:</strong> We compare the proposed
        <strong>ALFM</strong> model with the following baselines.
        It is worth noting that these methods are tuned on the
        validation dataset to obtain their optimal hyper-parameter
        settings for fair comparisons.</p>
        <ul class="list-no-style">
          <li id="list10" label="•">
            <strong>BMF&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0021">21</a>].</strong> It is a standard MF
            method with the consideration of bias terms (i.e., user
            biases and item biases). This method only leverages
            ratings when modeling users’ and items’ latent factors.
            It is typically a strong baseline model in
            collaborative filtering&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0021">21</a>,
            <a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0023">23</a>].<br />
          </li>
          <li id="list11" label="•">
            <strong>HFT&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0026">26</a>].</strong> It models ratings
            with MF and review text with latent topic model (e.g.,
            LDA&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0005">5</a>]). We use it as a representative
            of the methods which use an exponential transformation
            function to link the latent topics with latent factors,
            such as TopicMF&nbsp;[<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0002">2</a>]. The topic distribution
            can be modeled on either users or items. We use the
            topic distribution based on items, since it achieves
            better results. Note that in experiments, we add bias
            terms into HFT, which can achieve better
            performance.<br />
          </li>
          <li id="list12" label="•"><strong>CTR&nbsp;[<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href=
          "#BibPLXBIB0032">32</a>].</strong> This method also
          utilizes both review and rating information. It uses a
          topic model to learn the topic distribution of items,
          which is then used as the latent factors of items in MF
          with an addition of a latent variable.<br /></li>
          <li id="list13" label="•"><strong>RMR&nbsp;[<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href=
          "#BibPLXBIB0023">23</a>].</strong> This method also uses
          both ratings and reviews. Different from HFT and CTR,
          which use MF to model rating, it uses a mixture of
          Gaussian to model the ratings.<br /></li>
          <li id="list14" label="•">
            <strong>RBLT&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0031">31</a>].</strong> This method is the
            most recent method, which also uses MF to model ratings
            and LDA to model review texts. Instead of using an
            exponential transformation function to link the latent
            topics and latent factors (as in HFT&nbsp;[<a class=
            "bib" data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0026">26</a>]),
            this method linearly combines the latent factors and
            latent topics to represent users and items, with the
            assumption that the dimensions of topics and latent
            factors are equal and in the same latent space. The
            same strategy is also adopted by ITLFM&nbsp;[<a class=
            "bib" data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0039">39</a>].
            Here, we use RBLT as a representative method for this
            strategy.<br />
          </li>
          <li id="list15" label="•">
            <strong>TransNet&nbsp;[<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0006">6</a>].</strong> This method
            adopts neural network frameworks for rating prediction.
            In this model, the reviews of users and items are used
            as input to learn the latent representations of users
            and items. More descriptions about this method could be
            found in Section&nbsp;<a class="sec" href=
            "#sec-6">2</a>. We used the codes published by the
            authors in our experiments and tuned the parameters as
            described in&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0006">6</a>].<br />
          </li>
        </ul>
        <p>The standard root-mean-square error
        (<strong>RMSE</strong>) is adopted in evaluation. A smaller
        RMSE value indicates better performance.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Comparisons of adopted methods in terms
            of RMSE with <em>f</em> = <em>K</em> = 5.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Datasets</th>
                <th style="text-align:left;">BMF</th>
                <th style="text-align:left;">HFT</th>
                <th style="text-align:left;">CTR</th>
                <th style="text-align:left;">RMR</th>
                <th style="text-align:left;">RBLT</th>
                <th style="text-align:left;">TransNet</th>
                <th style="text-align:left;">ALFM</th>
                <th colspan="6" style="text-align:center;">
                  Improvement(%)
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">(a)</th>
                <th style="text-align:left;">(b)</th>
                <th style="text-align:left;">(c)</th>
                <th style="text-align:left;">(d)</th>
                <th style="text-align:left;">(e)</th>
                <th style="text-align:left;">(f)</th>
                <th style="text-align:left;">(g)</th>
                <th style="text-align:left;">g vs. a</th>
                <th style="text-align:left;">g vs. b</th>
                <th style="text-align:left;">g vs. c</th>
                <th style="text-align:left;">g vs. d</th>
                <th style="text-align:left;">g vs. e</th>
                <th style="text-align:left;">g vs. f</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Instant Video</td>
                <td style="text-align:left;">1.162</td>
                <td style="text-align:left;">0.999</td>
                <td style="text-align:left;">1.014</td>
                <td style="text-align:left;">1.039</td>
                <td style="text-align:left;">0.978</td>
                <td style="text-align:left;">0.996</td>
                <td style="text-align:left;">
                <strong>0.967</strong></td>
                <td style="text-align:left;">16.79</td>
                <td style="text-align:left;">3.19</td>
                <td style="text-align:left;">4.63*</td>
                <td style="text-align:left;">6.94*</td>
                <td style="text-align:left;">1.12**</td>
                <td style="text-align:left;">2.88</td>
              </tr>
              <tr>
                <td style="text-align:left;">Automotive</td>
                <td style="text-align:left;">1.032</td>
                <td style="text-align:left;">0.968</td>
                <td style="text-align:left;">1.016</td>
                <td style="text-align:left;">0.997</td>
                <td style="text-align:left;">0.924</td>
                <td style="text-align:left;">0.918</td>
                <td style="text-align:left;">
                <strong>0.885</strong></td>
                <td style="text-align:left;">14.26*</td>
                <td style="text-align:left;">8.58**</td>
                <td style="text-align:left;">12.86*</td>
                <td style="text-align:left;">11.19*</td>
                <td style="text-align:left;">4.24**</td>
                <td style="text-align:left;">3.56*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Baby</td>
                <td style="text-align:left;">1.359</td>
                <td style="text-align:left;">1.112</td>
                <td style="text-align:left;">1.144</td>
                <td style="text-align:left;">1.178</td>
                <td style="text-align:left;">1.122</td>
                <td style="text-align:left;">1.110</td>
                <td style="text-align:left;">
                <strong>1.076</strong></td>
                <td style="text-align:left;">20.83**</td>
                <td style="text-align:left;">3.24</td>
                <td style="text-align:left;">5.98*</td>
                <td style="text-align:left;">8.66**</td>
                <td style="text-align:left;">4.11</td>
                <td style="text-align:left;">3.05*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Beauty</td>
                <td style="text-align:left;">1.342</td>
                <td style="text-align:left;">1.132</td>
                <td style="text-align:left;">1.171</td>
                <td style="text-align:left;">1.190</td>
                <td style="text-align:left;">1.117</td>
                <td style="text-align:left;">1.123</td>
                <td style="text-align:left;">
                <strong>1.082</strong></td>
                <td style="text-align:left;">19.39**</td>
                <td style="text-align:left;">4.47</td>
                <td style="text-align:left;">7.65**</td>
                <td style="text-align:left;">9.12**</td>
                <td style="text-align:left;">3.18**</td>
                <td style="text-align:left;">3.65**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Phones</td>
                <td style="text-align:left;">1.432</td>
                <td style="text-align:left;">1.216</td>
                <td style="text-align:left;">1.271</td>
                <td style="text-align:left;">1.289</td>
                <td style="text-align:left;">1.220</td>
                <td style="text-align:left;">1.207</td>
                <td style="text-align:left;">
                <strong>1.167</strong></td>
                <td style="text-align:left;">18.47**</td>
                <td style="text-align:left;">3.98*</td>
                <td style="text-align:left;">8.18*</td>
                <td style="text-align:left;">9.4**</td>
                <td style="text-align:left;">4.33</td>
                <td style="text-align:left;">3.27**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Clothing</td>
                <td style="text-align:left;">1.073</td>
                <td style="text-align:left;">1.103</td>
                <td style="text-align:left;">1.142</td>
                <td style="text-align:left;">1.145</td>
                <td style="text-align:left;">1.073</td>
                <td style="text-align:left;">1.064</td>
                <td style="text-align:left;">
                <strong>1.032</strong></td>
                <td style="text-align:left;">3.8**</td>
                <td style="text-align:left;">6.47**</td>
                <td style="text-align:left;">9.65</td>
                <td style="text-align:left;">9.9**</td>
                <td style="text-align:left;">3.86**</td>
                <td style="text-align:left;">2.96*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Digital Music</td>
                <td style="text-align:left;">1.093</td>
                <td style="text-align:left;">
                <strong>0.918</strong></td>
                <td style="text-align:left;">0.921</td>
                <td style="text-align:left;">0.960</td>
                <td style="text-align:left;">
                <strong>0.918</strong></td>
                <td style="text-align:left;">1.061</td>
                <td style="text-align:left;">0.920</td>
                <td style="text-align:left;">15.82</td>
                <td style="text-align:left;">-0.15</td>
                <td style="text-align:left;">0.13*</td>
                <td style="text-align:left;">4.49**</td>
                <td style="text-align:left;">-0.15**</td>
                <td style="text-align:left;">4.13**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Grocery</td>
                <td style="text-align:left;">1.192</td>
                <td style="text-align:left;">1.016</td>
                <td style="text-align:left;">1.045</td>
                <td style="text-align:left;">1.061</td>
                <td style="text-align:left;">1.012</td>
                <td style="text-align:left;">1.022</td>
                <td style="text-align:left;">
                <strong>0.982</strong></td>
                <td style="text-align:left;">17.66**</td>
                <td style="text-align:left;">3.36**</td>
                <td style="text-align:left;">6.07</td>
                <td style="text-align:left;">7.46**</td>
                <td style="text-align:left;">3.01**</td>
                <td style="text-align:left;">3.94*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Health</td>
                <td style="text-align:left;">1.263</td>
                <td style="text-align:left;">1.073</td>
                <td style="text-align:left;">1.105</td>
                <td style="text-align:left;">1.135</td>
                <td style="text-align:left;">1.070</td>
                <td style="text-align:left;">1.114</td>
                <td style="text-align:left;">
                <strong>1.042</strong></td>
                <td style="text-align:left;">17.48*</td>
                <td style="text-align:left;">2.83</td>
                <td style="text-align:left;">5.65*</td>
                <td style="text-align:left;">8.20</td>
                <td style="text-align:left;">2.56**</td>
                <td style="text-align:left;">6.46**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Home &amp;
                Kitchen</td>
                <td style="text-align:left;">1.297</td>
                <td style="text-align:left;">1.083</td>
                <td style="text-align:left;">1.123</td>
                <td style="text-align:left;">1.149</td>
                <td style="text-align:left;">1.086</td>
                <td style="text-align:left;">1.123</td>
                <td style="text-align:left;">
                <strong>1.049</strong></td>
                <td style="text-align:left;">19.16**</td>
                <td style="text-align:left;">3.15**</td>
                <td style="text-align:left;">6.62</td>
                <td style="text-align:left;">8.7**</td>
                <td style="text-align:left;">3.41**</td>
                <td style="text-align:left;">6.61**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Musical
                Instruments</td>
                <td style="text-align:left;">1.004</td>
                <td style="text-align:left;">0.972</td>
                <td style="text-align:left;">0.979</td>
                <td style="text-align:left;">0.983</td>
                <td style="text-align:left;">0.946</td>
                <td style="text-align:left;">0.901</td>
                <td style="text-align:left;">
                <strong>0.893</strong></td>
                <td style="text-align:left;">11.08</td>
                <td style="text-align:left;">8.17**</td>
                <td style="text-align:left;">8.83**</td>
                <td style="text-align:left;">9.2**</td>
                <td style="text-align:left;">5.61</td>
                <td style="text-align:left;">0.95</td>
              </tr>
              <tr>
                <td style="text-align:left;">Office Products</td>
                <td style="text-align:left;">1.025</td>
                <td style="text-align:left;">0.879</td>
                <td style="text-align:left;">0.898</td>
                <td style="text-align:left;">0.934</td>
                <td style="text-align:left;">0.872</td>
                <td style="text-align:left;">0.898</td>
                <td style="text-align:left;">
                <strong>0.848</strong></td>
                <td style="text-align:left;">17.29**</td>
                <td style="text-align:left;">3.55**</td>
                <td style="text-align:left;">5.61*</td>
                <td style="text-align:left;">9.26**</td>
                <td style="text-align:left;">2.77**</td>
                <td style="text-align:left;">5.67**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Patio</td>
                <td style="text-align:left;">1.180</td>
                <td style="text-align:left;">1.041</td>
                <td style="text-align:left;">1.062</td>
                <td style="text-align:left;">1.077</td>
                <td style="text-align:left;">1.032</td>
                <td style="text-align:left;">1.046</td>
                <td style="text-align:left;">
                <strong>1.001</strong></td>
                <td style="text-align:left;">15.19**</td>
                <td style="text-align:left;">3.84*</td>
                <td style="text-align:left;">5.7*</td>
                <td style="text-align:left;">7.07*</td>
                <td style="text-align:left;">2.96</td>
                <td style="text-align:left;">4.33**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Pet Supplies</td>
                <td style="text-align:left;">1.367</td>
                <td style="text-align:left;">1.137</td>
                <td style="text-align:left;">1.177</td>
                <td style="text-align:left;">1.200</td>
                <td style="text-align:left;">1.139</td>
                <td style="text-align:left;">1.149</td>
                <td style="text-align:left;">
                <strong>1.099</strong></td>
                <td style="text-align:left;">19.64**</td>
                <td style="text-align:left;">3.41*</td>
                <td style="text-align:left;">6.67*</td>
                <td style="text-align:left;">8.41</td>
                <td style="text-align:left;">3.54**</td>
                <td style="text-align:left;">4.38**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Sports &amp;
                Outdoors</td>
                <td style="text-align:left;">1.130</td>
                <td style="text-align:left;">0.970</td>
                <td style="text-align:left;">0.998</td>
                <td style="text-align:left;">1.019</td>
                <td style="text-align:left;">0.964</td>
                <td style="text-align:left;">0.990</td>
                <td style="text-align:left;">
                <strong>0.933</strong></td>
                <td style="text-align:left;">17.42**</td>
                <td style="text-align:left;">3.8*</td>
                <td style="text-align:left;">6.47</td>
                <td style="text-align:left;">8.4*</td>
                <td style="text-align:left;">3.2**</td>
                <td style="text-align:left;">5.77**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Tools &amp; Home</td>
                <td style="text-align:left;">1.168</td>
                <td style="text-align:left;">1.013</td>
                <td style="text-align:left;">1.047</td>
                <td style="text-align:left;">1.090</td>
                <td style="text-align:left;">1.011</td>
                <td style="text-align:left;">1.041</td>
                <td style="text-align:left;">
                <strong>0.974</strong></td>
                <td style="text-align:left;">16.63**</td>
                <td style="text-align:left;">3.90</td>
                <td style="text-align:left;">6.98</td>
                <td style="text-align:left;">10.68**</td>
                <td style="text-align:left;">3.7**</td>
                <td style="text-align:left;">6.51**</td>
              </tr>
              <tr>
                <td style="text-align:left;">Toys &amp; Games</td>
                <td style="text-align:left;">1.072</td>
                <td style="text-align:left;">0.926</td>
                <td style="text-align:left;">0.948</td>
                <td style="text-align:left;">0.974</td>
                <td style="text-align:left;">0.923</td>
                <td style="text-align:left;">0.951</td>
                <td style="text-align:left;">
                <strong>0.902</strong></td>
                <td style="text-align:left;">15.81**</td>
                <td style="text-align:left;">2.59*</td>
                <td style="text-align:left;">4.82**</td>
                <td style="text-align:left;">7.39**</td>
                <td style="text-align:left;">2.3**</td>
                <td style="text-align:left;">5.11*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Video Games</td>
                <td style="text-align:left;">1.321</td>
                <td style="text-align:left;">1.096</td>
                <td style="text-align:left;">1.115</td>
                <td style="text-align:left;">1.150</td>
                <td style="text-align:left;">1.094</td>
                <td style="text-align:left;">1.123</td>
                <td style="text-align:left;">
                <strong>1.070</strong></td>
                <td style="text-align:left;">19.02*</td>
                <td style="text-align:left;">2.43</td>
                <td style="text-align:left;">4.03**</td>
                <td style="text-align:left;">6.97*</td>
                <td style="text-align:left;">2.24**</td>
                <td style="text-align:left;">4.77*</td>
              </tr>
              <tr>
                <td style="text-align:left;">Yelp 2017</td>
                <td style="text-align:left;">1.415</td>
                <td style="text-align:left;">1.174</td>
                <td style="text-align:left;">1.233</td>
                <td style="text-align:left;">1.266</td>
                <td style="text-align:left;">1.202</td>
                <td style="text-align:left;">1.190</td>
                <td style="text-align:left;">
                <strong>1.155</strong></td>
                <td style="text-align:left;">18.35*</td>
                <td style="text-align:left;">1.60**</td>
                <td style="text-align:left;">6.33**</td>
                <td style="text-align:left;">8.74*</td>
                <td style="text-align:left;">3.88**</td>
                <td style="text-align:left;">2.92*</td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:left;">Average</td>
                <td style="text-align:left;">1.207</td>
                <td style="text-align:left;">1.044</td>
                <td style="text-align:left;">1.074</td>
                <td style="text-align:left;">1.097</td>
                <td style="text-align:left;">1.037</td>
                <td style="text-align:left;">1.049</td>
                <td style="text-align:left;">
                <strong>1.004</strong></td>
                <td style="text-align:left;">14.56**</td>
                <td style="text-align:left;">2.84*</td>
                <td style="text-align:left;">7.16**</td>
                <td style="text-align:left;">8.31*</td>
                <td style="text-align:left;">3.37**</td>
                <td style="text-align:left;">4.26**</td>
              </tr>
            </tbody>
            <tfoot>
              <tr>
                <td colspan="14">The improvements with * are
                significant with <em>p</em> − <em>value</em> &lt;
                0.05, and the improvements with ** are significant
                with <em>p</em> − <em>value</em> &lt; 0.01 with a
                two-tailed paired t-test.</td>
              </tr>
            </tfoot>
          </table>
        </div>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186145/images/www2018-154-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Effects of factors and
            topics in our model.</span>
          </div>
        </figure>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186145/images/www2018-154-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Effects of #factors v.s.
            #topics.</span>
          </div>
        </figure>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Effect of
            Important Parameters (RQ1)</h3>
          </div>
        </header>
        <p>In this subsection, we analyze the influence of <em>the
        number of latent factors</em> and <em>the number of latent
        topics</em> on the final performance of ALFM. As we know,
        in MF, more latent factors will lead to better performance
        unless overfitting occurs&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0021">21</a>]; while the optimal
        number of latent topics in topic models (e.g., LDA) is
        dependent on the datasets&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0001">1</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0004">4</a>]. Accordingly, the
        optimal number of latent topics in topic model and the
        optimal number of latent factors in MF should be tuned
        separately. However, in the previous latent factor models
        (e.g., HFT, TopicMF&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>], RMR, CTR, and RBLT), the number of
        factors (i.e., #factors) and the number of topics (i.e.,
        #topics) are assumed to be the same, and thus cannot be
        optimized separately. Since our model does not have such
        constraint, we studied the effects of #factors and #topics
        individually. Fig.&nbsp;<a class="fig" href="#fig2">2</a>
        show the performance variations with the change of #factors
        and #topics by setting the other one to 5. We only
        visualize the performance variations of three datasets, due
        to the space limitation and the similar performance
        variation behaviors of other datasets. From the figure, we
        can see that with the increase of #factors, RMSE
        consistently decreases although the degree of decline is
        small. Notice that in our model, the rating prediction
        still relies on MF technique (Eq.&nbsp;<a class="eqn" href=
        "#eq3">5</a>). Therefore, the increase of #factors could
        lead to better representation capability and thus more
        accurate prediction. In contrast, the optimal number of
        latent topics is different from dataset to dataset.</p>
        <p>To better visualize the impact of #factors and #topics,
        we also present 3D figures by varying the number of factors
        and topics in {5, 10, 15, 20, 25}, as shown in
        Fig.&nbsp;<a class="fig" href="#fig3">3</a>. In this
        figure, we use the performance of three datasets as
        illustration. From the figure, we can see that the optimal
        numbers of topics and latent factors are varied across
        different datasets. In general, more latent factors usually
        lead to better performance, while the optimal number of
        latent topics is dependent on the reviews of different
        datasets. This also reveals that setting #factors and
        #topics to be the same may not be optimal.</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Model
            Comparison (RQ2)</h3>
          </div>
        </header>
        <p>We show the performance comparisons of our ALFM with all
        the baseline methods in Table&nbsp;<a class="tbl" href=
        "#tab3">3</a>, where the best prediction result on each
        dataset is in bold. For fair comparison, we set the number
        of latent factors (<em>f</em>) and the number of latent
        topics (<em>K</em>) to be the same as <em>f</em> =
        <em>K</em> = 5. Notice that our model could obtain better
        performance when setting <em>f</em> and <em>K</em>
        differently. Still, ALFM achieves the best results on 18
        out of the 19 datasets. Compared with BMF, which only uses
        ratings, we achieve much better prediction performance
        (16.49% relative improvement on average). More importantly,
        our model outperforms CTR and RMR with large margins -
        6.28% and 8.18% relative improvements on average,
        respectively. Compared to the recently proposed RBLT and
        TransNet, ALFM can still achieve 3.37% and 4.26% relative
        improvement on average respectively with significance
        testing. It is worth mentioning that HFT achieves better
        performance than RMR and comparable performance with recent
        RBLT, because we added bias terms to the original HFT
        in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>]. TransNet applies neural networks,
        which has exhibited strong capabilities on representation
        learning, in reviews to learn users’ preferences and items’
        characteristics for rating prediction. However, it may
        suffer from (1) noisy information in reviews, which would
        deteriorate the performance; and (2) errors introduced when
        generating fake reviews for rating prediction, which will
        also cause bias in the final performance. Compared to those
        baselines, the advantage of ALFM is that it models users’
        preferences on different aspects; and more importantly, it
        captures a user's specific attention on each aspect of a
        targeted item. The substantial improvement of ALFM over
        those baselines demonstrates the benefits of modeling
        users’ specific preferences on each aspect of different
        items.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186145/images/www2018-154-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Gain in RMSE for user with
            limited training data on two individual datasets and
            overall 19 datasets.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Cold-Start
            Setting (RQ3)</h3>
          </div>
        </header>
        <p>As shown in Table&nbsp;<a class="tbl" href=
        "#tab2">2</a>, the datasets are usually very sparse in
        practical systems. It is inherently difficult to provide
        satisfactory recommendation based on limited ratings. In
        the matrix factorization model, given a few ratings, the
        penalty function tends to push <em>q<sub>u</sub></em> and
        <em>p<sub>i</sub></em> towards zero. As a result, such
        users and items are modeled only with the bias
        terms&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0023">23</a>]. Therefore, matrix factorization
        easily suffers from the cold-start problem. By integrating
        reviews in users’ and items’ latent factor learning, our
        model could alleviate the problem of cold-start to a great
        extent, since reviews contain rich information about user
        preferences and item features.</p>
        <p>To demonstrate the capability of our model in dealing
        with users with very limited ratings, we randomly split the
        datasets into training, validation, and testing sets in
        ratio 80:10:10 based on the number of ratings in each set.
        In this setting, it is not guaranteed that a user has at
        least 3 ratings in the training set. It is possible that a
        user has no rating in the training set. For the users
        without any ratings in the training set, we also removed
        them in the testing set. Then we evaluate the performance
        of users who have the number of ratings from 1 to 10 in the
        training set. In Fig.&nbsp;<a class="fig" href=
        "#fig4">4</a>, we show the <strong>Gain in RMSE</strong>
        (<em>y</em>-axis) grouped by the number of ratings
        (<em>x</em>-axis) of users in the training set. The value
        of <strong>Gain in RMSE</strong> is equal to the average
        RMSE of baselines <em>minus</em> that of our model (e.g.,
        “BMF-TALFM”). A positive value indicates that our model
        achieves better prediction. As we can see, our ALFM model
        substantially improves the prediction accuracy compared
        with the BMF model. More importantly, our model also
        outperforms all the other baselines which also utilize
        reviews. This demonstrates that our model is more effective
        in exploiting reviews and ratings, because it learns user's
        preferences and item's features in different aspects and is
        capable of estimating the aspect weights based on the
        targeted user's preferences and targeted item's
        features.</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Top ten words of each aspect for a user
            (index 1511) from <em>Clothing</em>. Each column is
            corresponding to an aspect attached with an
            “interpretation” label.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Value</th>
                <th style="text-align:center;">Comfort</th>
                <th style="text-align:center;">Accessories</th>
                <th style="text-align:center;">Shoes</th>
                <th style="text-align:center;">Clothing</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">price</td>
                <td style="text-align:center;">size</td>
                <td style="text-align:center;">ring</td>
                <td style="text-align:center;">socks</td>
                <td style="text-align:center;">shirt</td>
              </tr>
              <tr>
                <td style="text-align:center;">color</td>
                <td style="text-align:center;">fit</td>
                <td style="text-align:center;">pretty</td>
                <td style="text-align:center;">foot</td>
                <td style="text-align:center;">back</td>
              </tr>
              <tr>
                <td style="text-align:center;">quality</td>
                <td style="text-align:center;">wear</td>
                <td style="text-align:center;">dress</td>
                <td style="text-align:center;">boots</td>
                <td style="text-align:center;">bra</td>
              </tr>
              <tr>
                <td style="text-align:center;">worth</td>
                <td style="text-align:center;">comfortable</td>
                <td style="text-align:center;">time</td>
                <td style="text-align:center;">comfort</td>
                <td style="text-align:center;">top</td>
              </tr>
              <tr>
                <td style="text-align:center;">cute</td>
                <td style="text-align:center;">bra</td>
                <td style="text-align:center;">beautiful</td>
                <td style="text-align:center;">sandals</td>
                <td style="text-align:center;">feel</td>
              </tr>
              <tr>
                <td style="text-align:center;">comfortable</td>
                <td style="text-align:center;">small</td>
                <td style="text-align:center;">gift</td>
                <td style="text-align:center;">walk</td>
                <td style="text-align:center;">soft</td>
              </tr>
              <tr>
                <td style="text-align:center;">fits</td>
                <td style="text-align:center;">color</td>
                <td style="text-align:center;">earrings</td>
                <td style="text-align:center;">toe</td>
                <td style="text-align:center;">black</td>
              </tr>
              <tr>
                <td style="text-align:center;">ring</td>
                <td style="text-align:center;">fits</td>
                <td style="text-align:center;">compliments</td>
                <td style="text-align:center;">pairs</td>
                <td style="text-align:center;">jeans</td>
              </tr>
              <tr>
                <td style="text-align:center;">dress</td>
                <td style="text-align:center;">perfect</td>
                <td style="text-align:center;">chain</td>
                <td style="text-align:center;">hold</td>
                <td style="text-align:center;">pants</td>
              </tr>
              <tr>
                <td style="text-align:center;">shirt</td>
                <td style="text-align:center;">material</td>
                <td style="text-align:center;">jewelry</td>
                <td style="text-align:center;">strap</td>
                <td style="text-align:center;">tight</td>
              </tr>
              <tr>
                <td style="text-align:center;">material</td>
                <td style="text-align:center;">long</td>
                <td style="text-align:center;">shoes</td>
                <td style="text-align:center;">pockets</td>
                <td style="text-align:center;">material</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">Interpretation for why the “user 1511”
            rated “item 1” and “item 2” with 5 and 2, respectively,
            from <em>Clothing</em>.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style=
                "text-align:center; border-right: solid 2px">
                Aspects</th>
                <th style="text-align:center;">Value</th>
                <th style="text-align:center;">Comfort</th>
                <th style="text-align:center;">Accessories</th>
                <th style="text-align:center;">Shoes</th>
                <th style="text-align:center;">Clothing</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style=
                "text-align:center; border-right: solid 2px">
                Importance (1)</td>
                <td style="text-align:center;">0.621</td>
                <td style="text-align:center;">0.042</td>
                <td style="text-align:center;">0.241</td>
                <td style="text-align:center;">0.001</td>
                <td>0.095</td>
              </tr>
              <tr>
                <td style=
                "text-align:center; border-right: solid 2px">
                Matching (1)</td>
                <td style="text-align:center;">0.982</td>
                <td style="text-align:center;">0.596</td>
                <td style="text-align:center;">0.660</td>
                <td style="text-align:center;">0.759</td>
                <td>0.638</td>
              </tr>
              <tr>
                <td style=
                "text-align:center; border-right: solid 2px">
                Polarity (1)</td>
                <td style="text-align:center; color:red">
                <strong>+</strong></td>
                <td style="text-align:center; color:blue">
                <strong>-</strong></td>
                <td style="text-align:center; color:red">
                <strong>+</strong></td>
                <td style="text-align:center; color:blue">
                <strong>-</strong></td>
                <td style="text-align:center; color:red">
                <strong>+</strong></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style=
                "text-align:center; border-right: solid 2px">
                Importance (2)</td>
                <td style="text-align:center;">0.621</td>
                <td style="text-align:center;">0.042</td>
                <td style="text-align:center;">0.241</td>
                <td style="text-align:center;">0.001</td>
                <td>0.094</td>
              </tr>
              <tr>
                <td style=
                "text-align:center; border-right: solid 2px">
                Matching (2)</td>
                <td style="text-align:center;">0.920</td>
                <td style="text-align:center;">0.303</td>
                <td style="text-align:center;">0.362</td>
                <td style="text-align:center;">1.000</td>
                <td>0.638</td>
              </tr>
              <tr>
                <td style=
                "text-align:center; border-right: solid 2px">
                Polarity (2)</td>
                <td style="text-align:center; color:blue">
                <strong>-</strong></td>
                <td style="text-align:center; color:blue">
                <strong>-</strong></td>
                <td style="text-align:center; color:blue">
                <strong>-</strong></td>
                <td style="text-align:center; color:blue">
                <strong>-</strong></td>
                <td style="text-align:center; color:red">
                <strong>+</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.6</span>
            Interpretability (RQ4)</h3>
          </div>
        </header>
        <p>In our ALFM model, a user's preference on an item is
        decomposed into user's preference on different aspects and
        the importance of those aspects. An aspect is represented
        as a distribution of latent topics discovered based on
        reviews. A user's attitude/sentiment on an aspect of the
        targeted item is decided by the latent factors (learned
        from ratings) associating with the aspect. Based on the
        topic distribution of an aspect (<span class=
        "inline-equation"><span class="tex">$ {\theta
        _{u,a_s}}$</span></span> ) and the word distribution of
        topics (ϕ<sub>w</sub>), we can semantically represent an
        aspect by the top words in this aspect. Specifically, the
        probability of a word <em>w</em> in an aspect
        <em>a<sub>s</sub></em> of a user <em>u</em> can be computed
        as <span class="inline-equation"><span class="tex">$\sum
        _{k=1}^K\theta _{u,a_s,k}\phi _{k,w}$</span></span> . The
        top 10 aspect words (#aspect = 5) of “user 1511” from
        <em>Clothing</em> dataset discovered by our model are shown
        in Table&nbsp;<a class="tbl" href="#tab4">4</a>. Notice
        that in order to obtain a better visualization of each
        aspect, we removed the “background” words that belong to
        more than 3 aspects. As shown in Table&nbsp;<a class="tbl"
        href="#tab4">4</a>, the five aspects can be semantically
        interpreted to “value”<a class="fn" href="#fn7" id=
        "foot-fn7"><sup>7</sup></a>, “comfort”, “accessories”,
        “shoes”, and “clothing”. Next, we illustrate the
        interpretability of our ALFM model on high or low ratings
        by examples from the same dataset. Table&nbsp;<a class=
        "tbl" href="#tab5">5</a> shows the aspect importance (i.e.,
        <em>ρ</em> <sub><em>u</em>, <em>i</em>, <em>a</em></sub> in
        Eq.&nbsp;<a class="eqn" href="#eq2">4</a>) of the “user
        1511” , the aspect matching scores (i.e., <em>s</em>
        <sub><em>u</em>, <em>i</em>, <em>a</em></sub> in
        Eq.&nbsp;<a class="eqn" href="#eq1">2</a>) as well as
        sentiment polarity (obtained by Eq.&nbsp;<a class="eqn"
        href="#eq3">5</a>) on the five aspects with respect to
        “item 1” and “item 2” in <em>Clothing</em> dataset. From
        the results, we can see that “user 1511” pays more
        attention to “Value” and “Accessories” aspects. On the
        “Value” aspect, both “item 1” and “item 2” highly match her
        preference, however, she has a positive sentiment on “item
        1” while a negative sentiment on “item 2”.<a class="fn"
        href="#fn8" id="foot-fn8"><sup>8</sup></a> For the
        “Accessories” aspect, “item 1” has a higher matching score
        than “item 2”; and more importantly, the sentiment is
        positive on “item 1” while negative on “item 2”. As a
        result, “user 1511” rated “item 1” with 5 while rated “item
        2” with 2. From the examples, we can see that our model
        could provide explanations for the recommendations in depth
        with <em>aspect semantics</em>, <em>aspect matching
        score</em>, as well as <em>aspect ratings</em> (which shows
        sentiment polarity).</p>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span>
          Conclusions</h2>
        </div>
      </header>
      <p>In this paper, we proposed an aspect-aware latent factor
      model for rating prediction by effectively combining reviews
      and ratings. Our model correlates the latent topics learned
      from review text and the latent factors learned from ratings
      based on the same set of aspects, which are discovered from
      textual reviews. Accordingly, our model does not have the
      constraint of one-to-one mapping between latent factors and
      latent topics as previous models (e.g., HFT, RMR, RBLT,
      etc.), and thus could achieve better user preference and item
      feature modeling. Besides, our model is able to estimate
      aspect ratings and assign weights to different aspects. The
      aspect weight is dependent on each user-item pair, since it
      is estimated based on user's personal preferences on the
      corresponding aspect towards an item. Experimental results on
      19 real-world datasets show that our model greatly improves
      the rating prediction accuracy compared to the
      state-of-the-art methods, especially for users who have few
      ratings. With the extracted aspects from textual reviews,
      estimated aspect weights, and aspect ratings, our model could
      provide interpretation for recommendation results in great
      detail.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-22">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This research is supported by the National Research
      Foundation, Prime Minister's Office, Singapore under its
      International Research Centre in Singapore Funding
      Initiative. The authors would like to thank Rose Catherine
      Kanjirathinkal (from CMU)’s great help on fine-tuning the
      results of TransNet on all datasets.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">R. Arun, V. Suresh,
        CE&nbsp;V. Madhavan, and MN&nbsp;N. Murthy. 2010. On
        finding the natural number of topics with latent dirichlet
        allocation: Some observations. In <em><em>Pacific-Asia
        Conference on Knowledge Discovery and Data Mining</em></em>
        . Springer, 391–402.</li>
        <li id="BibPLXBIB0002" label="[2]">Y. Bao, H. Fang, and J.
        Zhang. 2014. TopicMF: Simultaneously exploiting ratings and
        reviews for recommendation. In <em><em>Proceedings of the
        28th AAAI Conference on Artificial Intelligence</em></em> .
        2–8.</li>
        <li id="BibPLXBIB0003" label="[3]">R.&nbsp;M Bell and Y.
        Koren. 2007. Lessons from the Netflix prize challenge.
        <em><em>ACM SIGKDD Explorations Newsletter</em></em> 9, 2
        (2007), 75–79.</li>
        <li id="BibPLXBIB0004" label="[4]">David&nbsp;M Blei. 2012.
        Probabilistic topic models. <em><em>Commun. ACM</em></em>
        55, 4 (2012), 77–84.</li>
        <li id="BibPLXBIB0005" label="[5]">D.&nbsp;M. Blei,
        A.&nbsp;Y. Ng, and M.&nbsp;I. Jordan. 2003. Latent
        dirichlet allocation. <em><em>Journal of Machine Learning
        Research</em></em> 3 (2003), 993–1022.</li>
        <li id="BibPLXBIB0006" label="[6]">R. Catherine and W.
        Cohen. 2017. TransNets: Learning to Transform for
        Recommendation. In <em><em>Proceedings of the 11th ACM
        Conference on Recommender Systems</em></em> .
        ACM,288–296.</li>
        <li id="BibPLXBIB0007" label="[7]">Z. Cheng and J. Shen.
        2016. On effective location-aware music recommendation.
        <em><em>ACM Trans. Inf. Syst.</em></em> 34, 2 (2016),
        13:1–13:32.</li>
        <li id="BibPLXBIB0008" label="[8]">Z. Cheng, J. Shen, L.
        Nie, T.-S. Chua, and M. Kankanhalli. 2017. Exploring
        user-specific information in music retrieval. In
        <em><em>Proceedings of the 40th International ACM SIGIR
        Conference on Research and Development in Information
        Retrieval</em></em> . ACM,655–664.</li>
        <li id="BibPLXBIB0009" label="[9]">Z. Cheng, J. Shen, L.
        Zhu, M. Kankanhalli, and L. Nie. 2017. Exploiting music
        play sequence for music recommendation. In
        <em><em>Proceedings of the 26th International Joint
        Conference on Artificial Intelligence</em></em> .
        3654–3660.</li>
        <li id="BibPLXBIB0010" label="[10]">E. Christakopoulou and
        G. Karypis. 2016. Local item-item models for top-n
        recommendation. In <em><em>Proceedings of the 10th ACM
        Conference on Recommender Systems</em></em> .
        ACM,67–74.</li>
        <li id="BibPLXBIB0011" label="[11]">P. Covington, J. Adams,
        and E. Sargin. 2016. Deep neural networks for youtube
        recommendations. In <em><em>Proceedings of the 10th ACM
        Conference on Recommender Systems</em></em> .
        ACM,191–198.</li>
        <li id="BibPLXBIB0012" label="[12]">Q. Diao, M. Qiu, C.-Y.
        Wu, A.&nbsp;J Smola, J. Jiang, and C. Wang. 2014. Jointly
        modeling aspects, ratings and sentiments for movie
        recommendation (jmars). In <em><em>Proceedings of the 20th
        ACM SIGKDD international conference on Knowledge discovery
        and data mining</em></em> . ACM,193–202.</li>
        <li id="BibPLXBIB0013" label="[13]">D.&nbsp;M. Endres and
        J.&nbsp;E Schindelin. 2003. A new metric for probability
        distributions. <em><em>IEEE Trans. Inf. Theory</em></em>
        49, 7 (2003), 1858–1860.</li>
        <li id="BibPLXBIB0014" label="[14]">G. Ganu, N. Elhadad,
        and A. Marian. 2009. Beyond the Stars: Improving Rating
        Predictions using Review Text Content. In
        <em><em>Proceedings of the 12th International Workshop on
        the Web and Databases</em></em> , Vol.&nbsp;9. Citeseer,
        1–6.</li>
        <li id="BibPLXBIB0015" label="[15]">T.&nbsp;L Griffiths and
        M. Steyvers. 2004. Finding scientific topics.
        <em><em>Proceedings of the National Academy of
        Sciences</em></em> 101, Suppl 1(2004), 5228–5235.</li>
        <li id="BibPLXBIB0016" label="[16]">R. He and J. McAuley.
        2016. VBPR: visual bayesian personalized ranking from
        implicit feedback. In <em><em>Proceedings of the 30th AAAI
        Conference on Artificial Intelligence</em></em> .
        144–150.</li>
        <li id="BibPLXBIB0017" label="[17]">X. He, T. Chen, M.-Y.
        Kan, and X. Chen. 2015. Trirank: Review-aware explainable
        recommendation by modeling aspects. In <em><em>Proceedings
        of the 24th ACM International on Conference on Information
        and Knowledge Management</em></em> . ACM,1661–1670.</li>
        <li id="BibPLXBIB0018" label="[18]">X. He and T.-S. Chua.
        2017. Neural factorization machines for sparse predictive
        analytics. In <em><em>Proceedings of the 40th International
        ACM SIGIR conference on Research and Development in
        Information Retrieval</em></em> . ACM,355–364.</li>
        <li id="BibPLXBIB0019" label="[19]">X. He, L. Liao, H.
        Zhang, L. Nie, X. Hu, and T.-S. Chua. 2017. Neural
        collaborative filtering. In <em><em>Proceedings of the 26th
        International Conference on World Wide Web</em></em> .
        International World Wide Web Conferences Steering
        Committee, 173–182.</li>
        <li id="BibPLXBIB0020" label="[20]">X. He, H. Zhang, M.-Y.
        Kan, and T.-S. Chua. 2016. Fast matrix factorization for
        online recommendation with implicit feedback. In
        <em><em>Proceedings of the 39th International ACM SIGIR
        conference on Research and Development in Information
        Retrieval</em></em> . ACM,549–558.</li>
        <li id="BibPLXBIB0021" label="[21]">Y. Koren, R. Bell, and
        C. Volinsky. 2009. Matrix factorization techniques for
        recommender systems. <em><em>Computer</em></em> 42, 8
        (2009), 30–37.</li>
        <li id="BibPLXBIB0022" label="[22]">F. Li, S. Wang, S. Liu,
        and M. Zhang. 2014. SUIT: A supervised user-item based
        topic model for sentiment analysis. In <em><em>Proceedings
        of the 28th AAAI Conference on Artificial
        Intelligence</em></em> . 1636–1642.</li>
        <li id="BibPLXBIB0023" label="[23]">G. Ling, M. Lyu, and I.
        King. 2014. Ratings meet reviews, a combined approach to
        recommend. In <em><em>Proceedings of the 8th ACM Conference
        on Recommender systems</em></em> . ACM,105–112.</li>
        <li id="BibPLXBIB0024" label="[24]">H. Ma, D. Zhou, C. Liu,
        M. Lyu, and I. King. 2011. Recommender systems with social
        regularization. In <em><em>Proceedings of the fourth ACM
        international conference on Web search and data
        mining</em></em> . ACM,287–296.</li>
        <li id="BibPLXBIB0025" label="[25]">J. Mairal, F. Bach, J.
        Ponce, and G. Sapiro. 2010. Online learning for matrix
        factorization and sparse coding. <em><em>Journal of Machine
        Learning Research</em></em> 11, Jan (2010), 19–60.</li>
        <li id="BibPLXBIB0026" label="[26]">J. McAuley and J.
        Leskovec. 2013. Hidden factors and hidden topics:
        understanding rating dimensions with review text. In
        <em><em>Proceedings of the 7th ACM conference on
        Recommender systems</em></em> . ACM,165–172.</li>
        <li id="BibPLXBIB0027" label="[27]">N. Pappas and A.
        Popescu-Belis. 2013. Sentiment analysis of user comments
        for one-class collaborative filtering over ted talks. In
        <em><em>Proceedings of the 36th international ACM SIGIR
        conference on Research and development in information
        retrieval</em></em> . ACM,773–776.</li>
        <li id="BibPLXBIB0028" label="[28]">Š. Pero and T. Horváth.
        2013. Opinion-driven matrix factorization for rating
        prediction. In <em><em>International Conference on User
        Modeling, Adaptation, and Personalization</em></em> .
        Springer, 1–13.</li>
        <li id="BibPLXBIB0029" label="[29]">L. Qiu, S. Gao, W.
        Cheng, and J. Guo. 2016. Aspect-based latent factor model
        by integrating ratings and reviews for recommender system.
        <em><em>Knowledge-Based Systems</em></em> 110 (2016),
        233–243.</li>
        <li id="BibPLXBIB0030" label="[30]">Y. Shi, M. Larson, and
        A. Hanjalic. 2013. Mining contextual movie similarity with
        matrix factorization for context-aware recommendation.
        <em><em>ACM Trans. Intell. Syst. Technol.</em></em> 4, 1
        (2013), 16.</li>
        <li id="BibPLXBIB0031" label="[31]">Y. Tan, M. Zhang, Y.
        Liu, and S. Ma. 2016. Rating-boosted latent topics:
        Understanding users and items with ratings and reviews. In
        <em><em>Proceedings of the 25th International Joint
        Conference on Artificial Intelligence</em></em> .
        2640–2646.</li>
        <li id="BibPLXBIB0032" label="[32]">C. Wang and D. Blei.
        2011. Collaborative topic modeling for recommending
        scientific articles. In <em><em>Proceedings of the 17th ACM
        SIGKDD International conference on Knowledge Discovery and
        Data Mining</em></em> . ACM,448–456.</li>
        <li id="BibPLXBIB0033" label="[33]">H. Wang, Y. Lu, and C.
        Zhai. 2011. Latent aspect rating analysis without aspect
        keyword supervision. In <em><em>Proceedings of the 17th ACM
        SIGKDD international conference on Knowledge discovery and
        data mining</em></em> . ACM,618–626.</li>
        <li id="BibPLXBIB0034" label="[34]">X. Wang, X. He, L. Nie,
        and T.-S. Chua. 2017. Item silk road: Recommending items
        from information domains to social users. In
        <em><em>Proceedings of the 40th International ACM SIGIR
        conference on Research and Development in Information
        Retrieval</em></em> . ACM,185–194.</li>
        <li id="BibPLXBIB0035" label="[35]">X. Wang, X. He, L. Nie,
        and T.-S. Chua. 2018. TEM: Tree-enhanced embedding model
        for explainable recommendation. In <em><em>Proceedings of
        the 27th International Conference on World Wide
        Web</em></em> . International World Wide Web Conferences
        Steering Committee.</li>
        <li id="BibPLXBIB0036" label="[36]">Y. Wu and M. Ester.
        2015. FLAME: A probabilistic model combining aspect based
        opinion mining and collaborative filtering. In
        <em><em>Proceedings of the Eighth ACM International
        Conference on Web Search and Data Mining</em></em> .
        ACM,199–208.</li>
        <li id="BibPLXBIB0037" label="[37]">H. Zhang, F. Shen, W.
        Liu, X. He, H. Luan, and T.-S. Chua. 2016. Discrete
        collaborative filtering. In <em><em>Proceedings of the 39th
        International ACM SIGIR conference on Research and
        Development in Information Retrieval</em></em> .
        ACM,325–334.</li>
        <li id="BibPLXBIB0038" label="[38]">H. Zhang, Z.-J. Zha, Y.
        Yang, S. Yan, Y. Gao, and T.-S. Chua. 2014.
        Attribute-augmented semantic hierarchy: towards a unified
        framework for content-based image retrieval. <em><em>ACM
        Transactions on Multimedia Computing, Communications, and
        Applications</em></em> 11, 1s (2014), 21:1–21:21.</li>
        <li id="BibPLXBIB0039" label="[39]">W. Zhang and J. Wang.
        2016. Integrating topic and latent factors for scalable
        personalized review-based rating prediction. <em><em>IEEE
        Trans. Knowledge Data Eng.</em></em> 28, 11 (2016),
        3013–3027.</li>
        <li id="BibPLXBIB0040" label="[40]">W. Zhang, Q. Yuan, J.
        Han, and J. Wang. 2016. Collaborative multi-level embedding
        learning from reviews for rating prediction. In
        <em><em>Proceedings of the 25th International Joint
        Conference on Artificial Intelligence</em></em> .
        2986–2992.</li>
        <li id="BibPLXBIB0041" label="[41]">Y. Zhang, Q. Ai, X.
        Chen, and W.&nbsp;B. Croft. 2017. Joint representation
        learning for top-n recommendation with heterogeneous
        information sources. In <em><em>Proceedings of the 2017 ACM
        on Conference on Information and Knowledge
        Management</em></em> . ACM,1449–1458.</li>
        <li id="BibPLXBIB0042" label="[42]">Y. Zhang, G. Lai, M.
        Zhang, Y. Zhang, Y. Liu, and S. Ma. 2015. Explicit factor
        models for explainable recommendation based on phrase-level
        sentiment analysis. In <em><em>Proceedings of the 24th ACM
        International on Conference on Information and Knowledge
        Management</em></em> . ACM,1661–1670.</li>
        <li id="BibPLXBIB0043" label="[43]">L. Zheng, V. Noroozi,
        and P.&nbsp;S Yu. 2017. Joint deep modeling of users and
        items using reviews for recommendation. In
        <em><em>Proceedings of the Tenth ACM International
        Conference on Web Search and Data Mining</em></em> .
        ACM,425–434.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>In the paper,
    unless otherwise specified, notations in bold style denote
    matrices or vectors, and the ones in normal style denote
    scalars.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>In our
    experiments, we tried to normalize <em>ρ</em> <sub><em>u</em>,
    <em>i</em>, <em>a</em></sub> or <em>ρ</em> <sub><em>u</em>,
    <em>i</em>, <em>a</em></sub> · <em>s</em> <sub><em>u</em>,
    <em>i</em>, <em>a</em></sub> in Eq.&nbsp;<a class="eqn" href=
    "#eq3">5</a>, but no improvement has been observed.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a><a class=
    "link-inline force-break" href=
    "http://www.yelp.com/dataset_challenge/">http://www.yelp.com/dataset_challenge/</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>The thresholds
    of infrequent terms varied across different datasets. For
    example, for the “Yelp 2017” dataset, which is relatively
    large, a term that appears less than 10 times in reviews is
    defined as an infrequent term; and the thresholds are smaller
    for relatively small datasets (e.g., the threshold is 5 for the
    “Music Instruments” dataset).)</p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>We tuned the
    number of aspects from 1 to 8 for all the datasets, and found
    that the performance does not change much unless setting the
    aspect number to 1 or 2.</p>
    <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a>“Value” means
    value for money</p>
    <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>As a reminder,
    the aspect matching is based on the reviews. It is possible
    that both item 1 and item 2 contains comments on aspect
    “value”. However, “item 1” has a high value while “item 2” has
    a low value.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186145">https://doi.org/10.1145/3178876.3186145</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
