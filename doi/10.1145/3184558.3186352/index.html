<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Discovering Connotations as Labels for Weakly Supervised Image-Sentence Data</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3184558.3186352"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186352'>https://doi.org/10.1145/3184558.3186352</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186352'>https://w3id.org/oa/10.1145/3184558.3186352</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Discovering Connotations as Labels for Weakly Supervised Image-Sentence Data</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Aditya</span> <span class="surName">Mogadala</span>, Karlsruhe Institute of Technology, Karlsruhe, Germany, <a href="mailto:aditya.mogadala@kit.edu">aditya.mogadala@kit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Bhargav</span> <span class="surName">Kanuparthi</span><a class="fn" href="#fn1" id="foot-fn1"><sup>⁎</sup></a>, BITS, Hyderabad, India, <a href="mailto:f20140527@hyderabad.bits-pilani.ac.in">f20140527@hyderabad.bits-pilani.ac.in</a>
        </div>
        <div class="author">
          <span class="givenName">Achim</span> <span class="surName">Rettinger</span>, Karlsruhe Institute of Technology, Karlsruhe, Germany, <a href="mailto:rettinger@kit.edu">rettinger@kit.edu</a>
        </div>
        <div class="author">
          <span class="givenName">York</span> <span class="surName">Sure-Vetter</span>, Karlsruhe Institute of Technology, Karlsruhe, Germany, <a href="mailto:york.sure-vetter@kit.edu">york.sure-vetter@kit.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186352" target="_blank">https://doi.org/10.1145/3184558.3186352</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Growth of multimodal content on the web and social media has generated abundant weakly aligned image-sentence pairs. However, it is hard to interpret them directly due to intrinsic <em>“intension”</em>. In this paper, we aim to annotate such image-sentence pairs with connotations as labels to capture the intrinsic <em>“intension”</em>. We achieve it with a connotation multimodal embedding model (CMEM) using a novel loss function. It's unique characteristics over previous models include: (i) the exploitation of multimodal data as opposed to only visual information, (ii) robustness to outlier labels in a multi-label scenario and (iii) works effectively with large-scale weakly supervised data. With extensive quantitative evaluation, we exhibit the effectiveness of CMEM for detection of multiple labels over other state-of-the-art approaches. Also, we show that in addition to annotation of image-sentence pairs with connotation labels, byproduct of our model inherently supports cross-modal retrieval i.e. image query - sentence retrieval.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Web searching and information discovery;</strong> • <strong>Computing methodologies</strong> → <strong>Neural networks;</strong> <em>Image representations;</em> <em>Learning settings;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Image-Sentence Connotation Labels</small>,</span> <span class="keyword"><small>Weakly Supervised Deep Learning</small>,</span> <span class="keyword"><small>Multi-label Prediction</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Aditya Mogadala, Bhargav Kanuparthi, Achim Rettinger, and York Sure-Vetter. 2018. Discovering Connotations as Labels for Weakly Supervised Image-Sentence Data. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018 (WWW ’18 Companion),</em> <em>Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3186352" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186352</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Vast amount of visual data is created daily and major chunk of it is found on the web and social media. Many approaches are built to leverage such data (e.g. Flickr<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>) for building datasets&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>] by employing human efforts to filter the noisy images and annotate them with object categories. However, human involvement includes cost and also acquire other problems such as incompleteness and bias&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>]. Hence, an alternative approach would be is to learn visual features and object detectors directly without using any manual labeling.</p>
      <p>Hitherto, some approaches have explored the idea of automatically leveraging different types of web data from sources constituting only images&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>] and images accompanied with text&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>] to build visual models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>]. Although, it is asserted that the data is automatically extracted (e.g. search engines) and trained. Models are generally subjected to bias added by sources from which they are acquired. For example, image search engines (e.g. Google) usually concentrate on acquiring high-precision over recall and hence rank those images higher where a single object is centered with a clean background. In this case, images obtained may contain false positives but images themselves are not very complex to interpret i.e. images represent objects which can be easily localized.</p>
      <p>However, other forms of web data (e.g. social media) usually contain complex images which can be refereed with labels denoted by different connotations. Linguistically, a connotation is refereed to an idea that a word may hold which is in addition to its main or literal meaning (i.e. denotation). Pertaining images, it denotes that an image can also be described with connotations (e.g. abstract meaning) in addition to their usual denotations (e.g. visual objects depicting WordNet categories). Also from the perspective of logic and semantics, connotation refers to <em>intension</em><a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]. Figure&nbsp;<a class="fig" href="#fig1">1</a> shows sample image-tweet pairs where an image-tweet pair augmented with connotations along with their denotations is better interpretable when compared against rest. It is evident that adding connotations to complex images usually found on social media platforms is beneficial. Howbeit, most part of current research usually concentrate only building visual models that handle denotations and only learn from images.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186352/images/www18companion-114-fig1.jpg" class="img-responsive" alt="" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Augmenting connotations acquired from an imagetweet pair along with denotations provide better comprehension of “intension”. Boxes in red denote ground-truth “intension” observed in the image-tweet pair. Denotations are captured only from an image with a commercial image recognition API.<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a></span>
        </div>
      </figure>
      <p></p>
      <p>Hence in this paper, we aim to add such diversity in labels by harnessing large-scale data. Usually, standard web-scale image datasets (e.g. YFCC100M&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]) have shorter textual context and provide only denotations. However, connotations can be acquired only from the larger textual context. Therefore, our first goal is to acquire such image-textual data which provide such a context. Specifically, 1) we leverage Twitter<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a> to collect weakly supervised image-tweet pairs data that provides such a context. Since manual annotation of images-tweet pairs at the scale of Twitter is tedious. We leverage semantic annotation and disambiguation approaches&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] for generating connotations. 2) Second, an architecture based on embedding models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] is leveraged to capture correlation between image-tweets and connotations by learning their common space representation. Further, for any given input image-tweet, connotations are ranked according to the dot product between connotation and image-tweet embeddings. 3) Lastly, byproduct of our model is used to perform cross-modal retrieval to compare its effectiveness with other similar approaches.</p>
      <p>We believe that this work will provide a new direction for exploiting social media data to achieve varied visual tasks without human labeling effort. In rest of the paper, Section&nbsp;<a class="sec" href="#sec-6">2</a> presents related work and the Section&nbsp;<a class="sec" href="#sec-10">3</a> describes our approach to learn features from image-tweet pairs and then learn a connotation model to rank the connotations. Further, experimental setup Section&nbsp;<a class="sec" href="#sec-19">4</a> present the preliminaries about dataset and evaluation measure, While experimental results are presented in Section&nbsp;<a class="sec" href="#sec-30">5</a> followed by the conclusion and future work.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>Our related work can be drawn from many closely aligned areas.</p>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Labeling Images with Webly Supervised Learning</h3>
          </div>
        </header>
        <p>There has been a long standing interest in mining visual data from the web&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. Many approaches&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] have focused there efforts on either cleaning the web data by leveraging pre-trained models built from datasets created with human supervision (e.g. ImageNet&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]) or aimed to automatically discover hidden patterns to train models directly from it&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>]. Our work also focuses on the later objective and has an intent to tackle noise involved in such scenarios for building effectual models. Already, some approaches&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>] have handled similar challenges by filtering the noise when learning visual models. However, we differ from them by directly not learning visual representation models (e.g. CNNs&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>]) as we understand that learning from CNN with noisy labeled data is still an open problem. But, we leverage multimodal data to address the challenge. Also, aforementioned approaches only operate with single label per image, while we predict multiple labels per image.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Cross-Modal Retrieval with Images and Text</h3>
          </div>
        </header>
        <p>One of the closely aligned research fields is cross-modal retrieval with images and text. Over the past few years many approaches are proposed for cross-modal retrieval concerning images and textual forms observed in variable lengths such as phrases, sentences and paragraphs. Most of these early proposed approaches belong to subspace learning methods which learn a common space for cross-modal data, in which the similarity between the modalities is measured using varied distance metrics. Several of such subspace learning methods exists such as Canonical Correlation analysis (CCA)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>] etc.</p>
        <p>However, subspace learning methods are generally susceptible to scaling challenges. To overcome such issues, probabilistic graphical model (PGM) based approaches are proposed such as correspondence Latent Dirichlet Allocation (Corr-LDA)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] and their variations. Howbeit, these approaches also faced drawback as exact inference in general is intractable and has to depend on the approximate inference methods, such as variational inference, expectation propagation, or Gibbs sampling.</p>
        <p>Deep neural network based methods overcame challenges observed in subspace learning and PGM models by designing robust techniques that can scale to large data and also avoid intractable inference issues. Approaches such as deep restricted boltzmann machine (Deep RBM)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>], deep canonical correlation analysis (DCCA)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>], correspondence autoencoder (Corr-AE)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] and deep visual-semantic embeddings&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] used multimodal inputs to learn representations of common spaces.</p>
        <p>Our approach falls in-line with the family of deep learning methods and is proximal to the visual-semantic embedding approaches. However, our model goal is bigger than performing common space learning, we aim to predict of multiple labels by leveraging common space of each multimodal pair.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Hashtag Prediction</h3>
          </div>
        </header>
        <p>Prediction of connotations which captures intension in social media data is also closely aligned with the hashtag prediction&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] or recommendation&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>]. Hashtags are regularly observed to capture authors sentiment or comprehension on a particular topic. However, they are usually illustrated with n-grams or abbreviations and sometimes difficult to interpret when compared with semantically enriched connotation labels.</p>
        <p>Nevertheless, initially several approaches have leveraged deep neural networks to build their models only with social media text (e.g. Tweets) for prediction or recommendation. However, these approaches pursued different paths to achieve their goal. Weston et al.,&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] composed semantic embeddings from hashtags, while Dhingra et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] utilized character-based embeddings and Gong et al.,&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>] used attention-based CNN. Only recently, using hashtags for image tagging was explored. Denton et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] proposed a 3-way multiplicative gating approach, where the image model is conditioned on the user metadata on Facebook dataset. While, Park et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>] Context Sequence Memory Network (CSMN) model mainly built for personalized image captioning to predict hashtags on Instagram dataset. However, none of the aforementioned approaches leveraged multimodal social media data for utilizing larger contexts. Also, none of the hashtags were semantically enriched for better interpretation.</p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Approach</h2>
        </div>
      </header>
      <p>Let <span class="inline-equation"><span class="tex">$\mathcal {S} = \lbrace (I_j,T_j),Y_j\rbrace ^N_{j=1}$</span></span> be our dataset with (<em>I<sub>j</sub></em> , <em>T<sub>j</sub></em> ) the <em>j</em>-th image-tweet pair and <span class="inline-equation"><span class="tex">$Y_j \subseteq \mathcal {Y}$</span></span> the automatically extracted corresponding connotations set, where <span class="inline-equation"><span class="tex">$\mathcal {Y} \overset{\Delta }{=} \lbrace 1, 2, \ldots , K\rbrace$</span></span> is set of all possible connotations. Each image-tweet can have different number of connotations (<em>I<sub>j</sub></em> , <em>T<sub>j</sub></em> ) = |<em>Y<sub>j</sub></em> |.</p>
      <p>Our goal is now to learn a ranking model R(I,T,Y) that computes the confidence scores to all connotations to rank relevant connotations for a given image-tweet pair. We further decompose R(I,T,Y) = f(g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)),<em>E<sub>y</sub></em> ) where <span class="inline-equation"><span class="tex">$E_{\mathcal {Y}} \in \mathbb {R}^{d \times K}$</span></span> denote connotation label embeddings matrix and g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)): <span class="inline-equation"><span class="tex">$\mathbb {R}^I \times \mathbb {R}^T \rightarrow \mathbb {R}^d$</span></span> computation model to add tweet bias to image representations. Further, f(g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)),Y): <span class="inline-equation"><span class="tex">$(\mathbb {R}^d, \mathbb {R}^d) \rightarrow \mathbb {R}^K$</span></span> computes dot product between g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)) and connotation embedding matrix <span class="inline-equation"><span class="tex">$E_{\mathcal {Y}}$</span></span> for finding confidence scores of relevant connotations <em>Y</em>. We now adapt a Convolutional Neural Network (CNN)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] for an image representation (<em>Φ</em>(<em>I</em>)), character-level long short-term memory (charLSTM)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>] for the tweet representation (<em>Ψ</em>(<em>T</em>)) and a novel loss function for learning R(I,T,Y) model.</p>
      <p>In the following, we provide details of individual components of the ranking model R(I,T,Y).</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Image-Tweet Bilinear Model</h3>
          </div>
        </header>
        <p>Aim of the image-tweet bilinear model is to compute g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)). Initially, we present architectures used for extracting feature representations from both image (I) and tweet (T) i.e. <em>Φ</em>(<em>I</em>) and <em>Ψ</em>(<em>T</em>) respectively followed by the bilinear model.</p>
        <section id="sec-12">
          <p><em>3.1.1 Tweet Representation.</em> Tweets (<em>T</em>) are sequences upto 140 characters with inherent semantic and syntactic meaning. Encoding a tweet into a embedding vector (<span class="inline-equation"><span class="tex">$\mathbb {R}^T$</span></span> ) can encapsulate the compositional structure of the entire tweet. Thus, we propose to leverage charLSTM i.e. <em>Ψ</em>(<em>T</em>, <em>Θ</em>) to build embedding for each tweet, where <em>Θ</em> represent parameters of charLSTM. Initially, characters in a tweet are read sequentially to be further fed as input to an charLSTM encoder for encoding a tweet it into a <span class="inline-equation"><span class="tex">$\mathbb {R}^T$</span></span> vector.</p>
        </section>
        <section id="sec-13">
          <p><em>3.1.2 Image Representation.</em> For representing images (<em>I</em>) into fixed vector (<span class="inline-equation"><span class="tex">$\mathbb {R}^I$</span></span> ). We use pre-trained CNN on ImageNet classes as a feature extractor i.e. <em>Φ</em>(<em>I</em>) to obtain the image embeddings from the raw image. The image vectors of dimensionality <span class="inline-equation"><span class="tex">$\mathbb {R}^I$</span></span> are extracted from the final fully connected layer of the network without the top Softmax layer.</p>
        </section>
        <section id="sec-14">
          <p><em>3.1.3 Tweet-Biased Image Representation.</em> Image and tweet representations belong to two different feature spaces and do not share any common representation. To associate image and tweet representations, image-tweet bilinear model gives a simple method for leveraging tweet information by adding a tweet dependent bias term to the image embedding. In particular, the tweet-biased image embedding g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)): <span class="inline-equation"><span class="tex">$\mathbb {R}^I \times \mathbb {R}^T \rightarrow \mathbb {R}^d$</span></span> is defined by Equation&nbsp;<a class="eqn" href="#eq1">1</a>.</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} g(\Phi (I),\Psi (T)) = W_I^T\Phi (I)+ W_T^T\Psi (T) \end{equation}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>where <span class="inline-equation"><span class="tex">$W_I \in \mathbb {R}^{I \times d}$</span></span> and <span class="inline-equation"><span class="tex">$W_T \in \mathbb {R}^{T \times d}$</span></span> are image and tweet parameter matrices respectively.
          <p></p>
        </section>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Connotation Multimodal Embedding Model</h3>
          </div>
        </header>
        <p>The connotation multimodal embedding model (CMEM) denoted using the function f(g(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>)),<span class="inline-equation"><span class="tex">$E_{\mathcal {Y}}$</span></span> ;<em>θ</em>) <span class="inline-equation"><span class="tex">$\in \mathbb {R}^K$</span></span> learns a joint embedding space for connotation embedding matrix (<span class="inline-equation"><span class="tex">$E_{\mathcal {Y}}$</span></span> ) and tweet-biased image representations (<em>g</em>(<em>Φ</em>(<em>I</em>), <em>Ψ</em>(<em>T</em>))) to rank connotations. Figure&nbsp;<a class="fig" href="#fig2">2</a> presents the overall model.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186352/images/www18companion-114-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Connotation Multimodal embedding model with its different constituents. <span class="inline-equation"><span class="tex">$\mathbb {R}^d$</span></span> refers to the final d-dimension representation of image-tweet pair in-line with the connotation embeddings dimensions. ⊙ denote element-wise dot product.</span>
          </div>
        </figure>
        <p></p>
        <p>To learn parameters of f(·) <span class="inline-equation"><span class="tex">$\in \mathbb {R}^K$</span></span> , an optimization problem is solved using the loss function (<em>l</em>) given by Equation&nbsp;<a class="eqn" href="#eq2">2</a>.</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \min _{\theta } \frac{1}{N}\sum _{n=1}^N l(f(g(\Phi (I_n),\Psi (T_n)),E_{\mathcal {Y}};\theta),Y_n) + \lambda ||\theta ||^2_2 \end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where <em>θ</em> refers to the parameters of the CMEM.
        <p></p>
        <p>Furthermore, we design the loss function (<em>l</em>) in a manner to leverage large datasets and enforce f(·) to produce results whose values for true connotations are greater than those for negative connotations for any given image-tweet pair. In particular, pairwise rank loss (PRL)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] suits such a criteria and is given by Equation&nbsp;<a class="eqn" href="#eq3">3</a>.</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} l_{prl} = \sum _{\hat{y} \notin Y_i}\sum _{y \in Y_i} max(0,\alpha + f_{\hat{y}}(\cdot)-f_{y}(\cdot)) \end{equation}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>where <span class="inline-equation"><span class="tex">$\hat{y}$</span></span> represent negative connotations for any given positive connotation <em>y</em>, <em>alpha</em> is the hyper-parameter that denotes margin. However, <em>l<sub>prl</sub></em> is not smooth everywhere and thus makes it difficult to optimize.
        <p></p>
        <p>Therefore for CMEM, we propose to explore three different losses that provides better theoretical guarantees than the <em>l<sub>prl</sub></em> and makes easier for optimization. In the following, we first present the two existing techniques based on pairwise rank loss (i.e. WARP&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>] and LSEP&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>]) and then present our proposed loss function.</p>
        <section id="sec-16">
          <p><em>3.2.1 Weighted Approximate Rank Pairwise (WARP) Loss.</em> Weston et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>] extended pairwise rank loss provided in Equation&nbsp;<a class="eqn" href="#eq3">3</a> by adding weights on violations with Weighted Approximate Rank Pairwise (WARP) loss given by Equation&nbsp;<a class="eqn" href="#eq4">4</a>.</p>
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} l_{warp} = \sum _{\hat{y} \notin Y_i}\sum _{y \in Y_i} w(r_{i}^y) max(0,\alpha + f_{\hat{y}}(\cdot)-f_{y}(\cdot)) \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>where w(·) denote monotonically increasing function and <span class="inline-equation"><span class="tex">$r_{i}^y$</span></span> is the predicted rank of the positive connotation <em>y</em>. The intuition is that if the positive connotation is ranked lower, then the violation should be penalized higher. However, due its non-smoothness it is not differentiable everywhere and makes optimization difficult.
          <p></p>
        </section>
        <section id="sec-17">
          <p><em>3.2.2 Log-Sum-Exp Pairwise (LSEP) loss.</em> Addressing issues in <em>l<sub>prl</sub></em> and <em>l<sub>warp</sub></em> such as non-smoothness, adaptable margins etc., Li et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>] proposed Log-Sum-Exp pairwise (LSEP) loss by modifying exponential pairwise rank loss (<em>l<sub>epl</sub></em> )&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>] given by the Equation&nbsp;<a class="eqn" href="#eq5">5</a>.</p>
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} l_{lsep} = log\Bigg (1+\sum _{\hat{y} \notin Y_i}\sum _{y \in Y_i} exp(f_{\hat{y}}(\cdot)-f_{y}(\cdot))\Bigg) \end{equation}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>LSEP is expected to provide flexibility to the learning problem by allowing adaptable margins per sample pair and also making it smooth everywhere. Also, LSEP do not use weight function w(·) as it is expected have implicit weight effect to penalize the lower ranked positive connotations harder. Although, LSEP have many advantages such as it can linearly scale with vocabulary size with negative sampling technique&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>] and provide better numeric stability. Nevertheless, it still lack two key abilities. (1) <em>l<sub>lsep</sub></em> is not <em>α</em>-convex&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>]. This means that we cannot place a bound on how long gradient descent takes to converge. Howbeit, it partially mitigates the problem with regularization (2) <em>l<sub>lsep</sub></em> uses variant of logistic loss, thus making it sensitive to outliers in the data and assigns large loss values to them. We aim to overcome such challenges with our proposed penalized-logistic-sum pairwise (PLSP) loss.
          <p></p>
        </section>
        <section id="sec-18">
          <p><em>3.2.3 Penalized-Logistic-Sum Pairwise (PLSP) Loss.</em> It can be comprehended from aforementioned sections that pairwise ranking approaches dependent on variants of hinge loss (e.g. <em>l<sub>prl</sub></em> , <em>l<sub>warp</sub></em> ), exponential loss (e.g. <em>l<sub>epl</sub></em> ) and logistic loss (e.g. <em>l<sub>lesp</sub></em> ). Our proposed approach is the variant of truncated logistic loss and is expected to be <em>alpha</em>-convex while being robust to outliers. Equation&nbsp;<a class="eqn" href="#eq6">6</a> shows the loss function <em>l<sub>plsp</sub></em> .</p>
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} l_{plsp} = log\Bigg (1+\sum _{\hat{y} \notin Y_i}\sum _{y \in Y_i} exp\Big (\frac{min(f_{\hat{y}}(\cdot)-f_{y}(\cdot),s)}{max(f_{\hat{y}}(\cdot)-f_{y}(\cdot),-s)}\Big)\Bigg) \end{equation}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>where <em>s</em> &lt; 0 denotes location of truncation. Important property of <em>l<sub>plsp</sub></em> is that the denominator value in the exponential cannot get extremely small because it is lower bounded by <em>s</em>. Similarly, numerator of the equation cannot get cannot get extremely big. Therefore, it makes <em>l<sub>plsp</sub></em> robust to noise of outliers and also smooth everywhere due to exponential.
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experimental Setup</h2>
        </div>
      </header>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Dataset</h3>
          </div>
        </header>
        <p>In this section, we introduce a new dataset called <em>TwitterBrexit</em> collected from Twitter.</p>
        <section id="sec-21">
          <p><em>4.1.1 Dataset Procurement.</em> In the following, we present varied stages involved in creation of the dataset.</p>
          <p><strong>Tweets Collection</strong> is specific to a domain i.e. Brexit in our case. This is undertaken to reduce noise in the collection and to ensure connotations set is interpretable. Otherwise, we will end up procuring randomly distributed labels and could lead to uninterpretable results. Initially, we attained seed topic words using Google trends<a class="fn" href="#fn6" id="foot-fn6"><sup>5</sup></a> during the period of May 2015 and May 2016 for searching Twitter. Topic words such as Brexit, Immigration, Racism, Theresa, etc., are then used as queries to Twitter search API&nbsp;<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a> for collecting tweets. This step is iterated several times until a long list of tweets are acquired.</p>
          <p><strong>Tweets pruning</strong> is performed to acquire only those tweets with corresponding images. We found that only 25% of the tweets collected are accompanied with images. Further, pruned image-tweet pairs is again processed to eliminate junk, tweets without words, English only tweets and duplicates.</p>
        </section>
        <section id="sec-22">
          <p><em>4.1.2 Dataset Peculiarity.</em> The new dataset introduced in this paper is peculiar and also challenging to process when compared against other similar datasets on the following aspects. First, dataset is collected from the social media platform. Hence, the language usually used will be informal comprising grammatical mistakes and large vocabularies. However, there are also additional characteristics which is helpful for highlighting information present in the image-tweet pair such as hashtags. Second, the association between the image and tweet is often loosely connected. Hence, they are weakly supervised. Third, dataset is useful for large-scale training and can be exploited to test robustness of multi-label classifiers.</p>
        </section>
        <section id="sec-23">
          <p><em>4.1.3 Creation of Connotations.</em> For acquiring connotations for images present in the collection, text aligned with images is leveraged. An usual strategy to make sense or extract intension from the social media text is by annotating them with semantic enricher's. Acquired connotations are considered as a brief summarization of the content present in the text. Hence, connotations also support better information interpretation. We leveraged semantic annotation and disambiguation tool&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>] to obtain such labels. Since these labels are acquired from the text aligned with images, labels are also expected to describe images. However, they are not preferable for direct learning of image recognition models. As they are extracted automatically without human supervision and hence can induce noise into learning. In total, the dataset contained ∼ 30k distinct connotations as labels. The mean number of labels per tweet was 2.3 with a standard deviation of 1.3. A large fraction of labels describe the content of the image, with many synonyms. Others describe abstract meaning representing possible intension in the image content (e.g. Economics, Xenophobia).</p>
          <p>The distribution of labels in the dataset is far from uniform: the top 10 labels account for 47% of the total and ∼ 27k of them appear less than 10 times throughout the whole dataset. It is difficult to predict infrequent labels, so we limit top 1387 labels which have appeared at least 25 times in the dataset to create a balanced version of the dataset. Figure&nbsp;<a class="fig" href="#fig3">3</a> shows sample annotations, while Figure&nbsp;<a class="fig" href="#fig4">4</a> presents top-50 frequent labels in the entire dataset.</p>
          <figure id="fig3">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186352/images/www18companion-114-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span> <span class="figure-title">Example connotations as labels observed for different variants of images. For example, “OCR” variant represent images which contain text and “Abstract” denote concepts close to real-world entities.</span>
            </div>
          </figure>
          <figure id="fig4">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186352/images/www18companion-114-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span> <span class="figure-title">Top-50 frequent labels and their distribution.</span>
            </div>
          </figure>
          <p></p>
          <p>In total, our collection comprises around 160,004 image-tweet pairs for training, 10,000 for validation and 10220 for testing.</p>
        </section>
        <section id="sec-24">
          <p><em>4.1.4 Applications of the Dataset.</em> We leverage the dataset mainly for multi-label prediction. However, we also show that it is also useful for cross-modal retrieval.</p>
        </section>
      </section>
      <section id="sec-25">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Evaluation Measures</h3>
          </div>
        </header>
        <p>To measure the effectiveness of discovering connotations as labels for images. We use different measures such as recall, multi-label accuracy, Hamming loss&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] and coverage.</p>
        <section id="sec-26">
          <p><em>4.2.1 Recall@k (R@k).</em> measures of the fraction of relevant connotation labels for each test image-tweet pair that are ranked in the top <em>k</em> given by Equation&nbsp;<a class="eqn" href="#eq7">7</a>.</p>
          <div class="table-responsive" id="eq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \text{Accuracy} = \frac{1}{|q|} \sum _{j=1}^{|q|}\frac{|X_j \bigcap Y_j|}{|Y_j|} \end{equation}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>where <em>X<sub>j</sub></em> refers to the predicted correct labels and <em>Y<sub>j</sub></em> the ground-truth labels for the <em>j</em>-th query.
          <p></p>
        </section>
        <section id="sec-27">
          <p><em>4.2.2 Multi-label Accuracy@k (ML-A@k).</em> measures the proportion of predicted correct labels that are ranked in the top <em>k</em> to the total number of ground-truth labels for a given image query. Overall accuracy is the average across all queries given by Equation&nbsp;<a class="eqn" href="#eq8">8</a>.</p>
          <div class="table-responsive" id="eq8">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \text{Accuracy} = \frac{1}{|q|} \sum _{j=1}^{|q|}\frac{|X_j \bigcap Y_j|}{|X_j \bigcup Y_j|} \end{equation}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>Higher the value of accuracy, better the performance.
          <p></p>
        </section>
        <section id="sec-28">
          <p><em>4.2.3 Hamming Loss (HL).</em> measures how many times on an average the relevance of an instance to a class label is incorrectly predicted. Also, hamming loss consider both prediction error (i.e. prediction of incorrect label) and missing error (i.e. missing out the relevant label) normalized over total number of classes and examples given by Equation&nbsp;<a class="eqn" href="#eq9">9</a></p>
          <div class="table-responsive" id="eq9">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \text{HL} = \frac{1}{|q|N} \sum _{j=1}^{|q|}\sum _{l=1}^{N}[\mathcal {F}(l \in X_j \wedge l \notin Y_j) + \mathcal {F}(l \notin X_j \wedge l \in Y_j)] \end{equation}</span><br />
              <span class="equation-number">(9)</span>
            </div>
          </div>where <span class="inline-equation"><span class="tex">$\mathcal {F}$</span></span> refers to the indicator function and <em>l</em> to the semantic labels. In practice, smaller the value of HL, better the performance.
          <p></p>
        </section>
        <section id="sec-29">
          <p><em>4.2.4 Coverage.</em> evaluates how much one needs to traverse the ranked list of labels on average to cover all the relevant labels of the sample and is provided by Equation&nbsp;<a class="eqn" href="#eq10">10</a>.</p>
          <div class="table-responsive" id="eq10">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \text{Cov} = \frac{1}{|q|} \sum _{j=1}^{|q|} max(rank (X_j))-1 \end{equation}</span><br />
              <span class="equation-number">(10)</span>
            </div>
          </div>Smaller the value of coverage, better the performance.
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-30">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experiments</h2>
        </div>
      </header>
      <section id="sec-31">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Implementation</h3>
          </div>
        </header>
        <p>As discussed in aforementioned sections, important constituents of our CMEM are CNN, charLSTM, connotation embeddings and a loss function. For image representation, we explored two different CNN models, mainly VGG16&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>] and ResNet50&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] pre-trained on the ImageNet ILSVRC dataset&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] by extracting features of dimensions 4096 and 2048 respectively from the final fully connected layer of the network without the top Softmax layer. For the charLSTM, we initialized character embeddings with 512 dimensions using Glorot uniform. Connotations acquired from semantic enricher are Wikipedia titles (i.e. concepts) which are also observed in DBpedia<a class="fn" href="#fn8" id="foot-fn8"><sup>7</sup></a>. Therefore, we leveraged wiki2vec<a class="fn" href="#fn9" id="foot-fn9"><sup>8</sup></a> to obtain 256 and 512 dimension embeddings for concepts. CMEM is now trained using Adam optimizer&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] with gradient clipping having maximum norm of 1.0 for 10 epochs. The weight decay <em>λ</em> in the regularization term of Equation&nbsp;<a class="eqn" href="#eq2">2</a> is set to 5e-5.</p>
      </section>
      <section id="sec-32">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Results and Discussion</h3>
          </div>
        </header>
        <section id="sec-33">
          <p><em>5.2.1 Baselines.</em> We design our baselines based on the usage of varied loss functions with CMEM. For example, CMEM-<em>warp</em> represents our CMEM with the WARP loss.</p>
        </section>
        <section id="sec-34">
          <p><em>5.2.2 Quantitative Analysis (Label Prediction).</em> To conduct our evaluation, we leveraged the <em>TwitterBrexit</em> dataset mentioned in the Section&nbsp;<a class="sec" href="#sec-20">4.1</a>. Different approaches are evaluated based on measures such as recall at 10 (R@10), accuracy at 10 (ML-A@10) and hamming loss (HL). Table&nbsp;<a class="tbl" href="#tab1">1</a> shows the results attained. We can notice that the CMEM-<em>prl</em> performed poorly when compared against all other models. However, when only advanced models like CMEM-<em>warp</em> and CMEM-<em>lsep</em> are compared, it can be observed that CMEM-<em>lsep</em> outperforms CMEM-<em>warp</em> on both recall and accuracy. Howbeit, for HL there seems to have no significant difference. This can be attributed to better optimization achieved with <em>l<sub>lsep</sub></em> .</p>
          <p>Furthermore, we can perceive that CMEM with our proposed loss i.e. CMEM-<em>plsp</em> performs particularly well in terms of accuracy, recall and HL when compared against other baselines. Results also convey that our proposed loss in CMEM was particularly robust to outliers and could leverage that with significant gains in both recall and accuracy. Also, few more observations can be made about visual features and dimensions of connotation embeddings. ResNet50 performs better than VGG16, while larger dimensions for connotation embeddings perform better than their counterparts with lesser dimensions.</p>
          <p>Figure&nbsp;<a class="fig" href="#fig5">5</a> shows average precision-recall (PR) curves that allow us to comprehend the effect of label prediction. It can be perceived that our CMEM-<em>plsp</em> outperforms others suggesting the robustness of our <em>l<sub>plsp</sub></em> loss with CMEM when compared to other baselines.</p>
          <div class="table-responsive" id="tab1">
            <div class="table-caption">
              <span class="table-number">Table 1:</span> <span class="table-title">Connotation Label prediction Results on <em>TwitterBrexit</em>. R@10, ML-A@10 represent percentages (%). Bold denote best, while underline represent second best.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Loss Function</th>
                  <th style="text-align:left;">Connotation Embeddings</th>
                  <th style="text-align:left;">CNN Architecture</th>
                  <th style="text-align:left;">R@10</th>
                  <th style="text-align:left;">ML-A@10</th>
                  <th style="text-align:left;">HL</th>
                  <th>Cov</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">CMEM-<em>prl</em></td>
                  <td style="text-align:left;">256</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">18.11</td>
                  <td style="text-align:left;">35.42</td>
                  <td style="text-align:left;">0.416</td>
                  <td>12.61</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">18.17</td>
                  <td style="text-align:left;">35.84</td>
                  <td style="text-align:left;">0.412</td>
                  <td>12.54</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">512</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">18.84</td>
                  <td style="text-align:left;">36.10</td>
                  <td style="text-align:left;">0.408</td>
                  <td>12.46</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">18.90</td>
                  <td style="text-align:left;">36.54</td>
                  <td style="text-align:left;">0.406</td>
                  <td>12.44</td>
                </tr>
                <tr>
                  <td style="text-align:left;">CMEM-<em>warp</em></td>
                  <td style="text-align:left;">256</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">18.95</td>
                  <td style="text-align:left;">36.69</td>
                  <td style="text-align:left;">0.406</td>
                  <td>12.37</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">19.02</td>
                  <td style="text-align:left;">36.78</td>
                  <td style="text-align:left;">0.404</td>
                  <td>12.38</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">512</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">19.10</td>
                  <td style="text-align:left;">37.80</td>
                  <td style="text-align:left;">0.396</td>
                  <td>12.26</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">19.24</td>
                  <td style="text-align:left;">38.24</td>
                  <td style="text-align:left;">0.389</td>
                  <td>12.10</td>
                </tr>
                <tr>
                  <td style="text-align:left;">CMEM-<em>lsep</em></td>
                  <td style="text-align:left;">256</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">19.22</td>
                  <td style="text-align:left;">38.15</td>
                  <td style="text-align:left;">0.390</td>
                  <td>12.14</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">19.30</td>
                  <td style="text-align:left;">38.84</td>
                  <td style="text-align:left;">0.382</td>
                  <td>12.09</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">512</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">19.44</td>
                  <td style="text-align:left;">39.16</td>
                  <td style="text-align:left;">0.374</td>
                  <td>11.98</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">19.51</td>
                  <td style="text-align:left;">39.40</td>
                  <td style="text-align:left;">0.371</td>
                  <td>11.93</td>
                </tr>
                <tr>
                  <td style="text-align:left;">CMEM-<em>plsp</em> (ours)</td>
                  <td style="text-align:left;">256</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;">19.54</td>
                  <td style="text-align:left;">39.84</td>
                  <td style="text-align:left;">0.369</td>
                  <td>11.88</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;">19.63</td>
                  <td style="text-align:left;">40.05</td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">0.368</span></td>
                  <td>11.85</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">512</td>
                  <td style="text-align:left;">VGG16</td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">19.63</span></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">40.08</span></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">0.368</span></td>
                  <td><span style="text-decoration: underline;">11.84</span></td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">ResNet50</td>
                  <td style="text-align:left;"><strong>19.68</strong></td>
                  <td style="text-align:left;"><strong>40.26</strong></td>
                  <td style="text-align:left;"><strong>0.366</strong></td>
                  <td><strong>11.79</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <figure id="fig5">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186352/images/www18companion-114-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span> <span class="figure-title">Average Precision-Recall Curve.</span>
            </div>
          </figure>
        </section>
        <section id="sec-35">
          <p><em>5.2.3 Quantitative Analysis (Cross-modal Retrieval).</em> A natural consequence of CMEM is learning of parameters <em>W<sub>I</sub></em> and <em>W<sub>T</sub></em> . During learning, both of them are updated jointly when optimized w.r.t connotations. Hence, they inherently share correlation between image and tweets.</p>
          <p>In this section, we evaluate their effectiveness with image query for tweet retrieval and compare them with standard subspace learning methods such as canonical correlation analysis (CCA) and its variants (i.e. regularized CCA (RCCA)) using Mean rank. We also explored other representations for the text such as latent Dirichlet allocation (LDA)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>]. The LDA model is trained with 50 topics to represent each tweet with 50-dimensional LDA feature by the topic assignment probability distributions. Table&nbsp;<a class="tbl" href="#tab2">2</a> shows the comparison of different approaches for image to tweet retrieval.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span> <span class="table-title">Mean rank (<strong>lower the better</strong>) using different percentage (%) of image queries for retrieval. RCCA-* represent different regularization (100, 1000). Underline represents second best. All results are reported using ResNet50 as image features.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th colspan="12" style="text-align:center;">Image → Tweet Retrieval</th>
                </tr>
                <tr>
                  <th style="text-align:left;">Measures</th>
                  <th style="text-align:left;">Methods</th>
                  <th style="text-align:left;">10</th>
                  <th style="text-align:left;">20</th>
                  <th style="text-align:left;">30</th>
                  <th style="text-align:left;">40</th>
                  <th style="text-align:left;">50</th>
                  <th style="text-align:left;">60</th>
                  <th style="text-align:left;">70</th>
                  <th style="text-align:left;">80</th>
                  <th style="text-align:left;">90</th>
                  <th>100</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Mean Rank</td>
                  <td style="text-align:left;">CCA-LDA</td>
                  <td style="text-align:left;">5371</td>
                  <td style="text-align:left;">5570</td>
                  <td style="text-align:left;">5627</td>
                  <td style="text-align:left;">5767</td>
                  <td style="text-align:left;">5779</td>
                  <td style="text-align:left;">5753</td>
                  <td style="text-align:left;">5774</td>
                  <td style="text-align:left;">5770</td>
                  <td style="text-align:left;">5752</td>
                  <td>5766</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">RCCA-100-LDA</td>
                  <td style="text-align:left;">4902</td>
                  <td style="text-align:left;">5083</td>
                  <td style="text-align:left;">5224</td>
                  <td style="text-align:left;">5303</td>
                  <td style="text-align:left;">5312</td>
                  <td style="text-align:left;">5309</td>
                  <td style="text-align:left;">5312</td>
                  <td style="text-align:left;">5306</td>
                  <td style="text-align:left;">5304</td>
                  <td>5315</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">RCCA-1000-LDA</td>
                  <td style="text-align:left;">4873</td>
                  <td style="text-align:left;">5060</td>
                  <td style="text-align:left;">5203</td>
                  <td style="text-align:left;">5260</td>
                  <td style="text-align:left;">5269</td>
                  <td style="text-align:left;">5267</td>
                  <td style="text-align:left;">5272</td>
                  <td style="text-align:left;">5262</td>
                  <td style="text-align:left;">10263</td>
                  <td>5275</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">CCA-charLSTM</td>
                  <td style="text-align:left;">3616</td>
                  <td style="text-align:left;">3989</td>
                  <td style="text-align:left;">4130</td>
                  <td style="text-align:left;">4347</td>
                  <td style="text-align:left;">4409</td>
                  <td style="text-align:left;">4515</td>
                  <td style="text-align:left;">4572</td>
                  <td style="text-align:left;">4627</td>
                  <td style="text-align:left;">4661</td>
                  <td>4690</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">RCCA-100-charLSTM</td>
                  <td style="text-align:left;">3637</td>
                  <td style="text-align:left;">3965</td>
                  <td style="text-align:left;">4125</td>
                  <td style="text-align:left;">4328</td>
                  <td style="text-align:left;">4397</td>
                  <td style="text-align:left;">4504</td>
                  <td style="text-align:left;">4565</td>
                  <td style="text-align:left;">4613</td>
                  <td style="text-align:left;">4650</td>
                  <td>4682</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">RCCA-1000-charLSTM</td>
                  <td style="text-align:left;">3708</td>
                  <td style="text-align:left;">3909</td>
                  <td style="text-align:left;">4181</td>
                  <td style="text-align:left;">4377</td>
                  <td style="text-align:left;">4451</td>
                  <td style="text-align:left;">4551</td>
                  <td style="text-align:left;">4615</td>
                  <td style="text-align:left;">4649</td>
                  <td style="text-align:left;">4683</td>
                  <td>4719</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">CMEM-<em>prl</em></td>
                  <td style="text-align:left;">2618</td>
                  <td style="text-align:left;">2986</td>
                  <td style="text-align:left;">3124</td>
                  <td style="text-align:left;">3341</td>
                  <td style="text-align:left;">3403</td>
                  <td style="text-align:left;">3509</td>
                  <td style="text-align:left;">3562</td>
                  <td style="text-align:left;">3617</td>
                  <td style="text-align:left;">3652</td>
                  <td>3688</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">CMEM-<em>warp</em></td>
                  <td style="text-align:left;">2627</td>
                  <td style="text-align:left;">2959</td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3112</span></td>
                  <td style="text-align:left;">3318</td>
                  <td style="text-align:left;">3381</td>
                  <td style="text-align:left;">3502</td>
                  <td style="text-align:left;">3551</td>
                  <td style="text-align:left;">3603</td>
                  <td style="text-align:left;">3641</td>
                  <td>3672</td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">CMEM-<em>lsep</em></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">2608</span></td>
                  <td style="text-align:left;"><strong>2898</strong></td>
                  <td style="text-align:left;"><strong>3101</strong></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3277</span></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3311</span></td>
                  <td style="text-align:left;"><strong>3481</strong></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3515</span></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3549</span></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3583</span></td>
                  <td><span style="text-decoration: underline;">3619</span></td>
                </tr>
                <tr>
                  <td style="text-align:left;"></td>
                  <td style="text-align:left;">CMEM-<em>plsp</em></td>
                  <td style="text-align:left;"><strong>2588</strong></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">2908</span></td>
                  <td style="text-align:left;">3121</td>
                  <td style="text-align:left;"><strong>3227</strong></td>
                  <td style="text-align:left;"><strong>3286</strong></td>
                  <td style="text-align:left;"><span style="text-decoration: underline;">3496</span></td>
                  <td style="text-align:left;"><strong>3488</strong></td>
                  <td style="text-align:left;"><strong>3529</strong></td>
                  <td style="text-align:left;"><strong>3575</strong></td>
                  <td><strong>3596</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-36">
          <p><em>5.2.4 Qualitative Analysis (Label Prediction).</em> Figure&nbsp;<a class="fig" href="#fig6">6</a> presents sample results attained with CMEM using different loss functions. It can be seen that connotations extracted shows better intension from the image-tweet pair when compared against using only denotations captured from the image.</p>
          <figure id="fig6">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186352/images/www18companion-114-fig6.jpg" class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span> <span class="figure-title">Sample qualitative results (red: False positives, Blue: True positives) attained with CMEM using different loss functions. Top-2 ranked connotations obtained are presented for each case. CMEM-<em>plsp</em> (Best) refers to the best model obtained from Table&nbsp;<a class="tbl" href="#tab1">1</a>. Denotations are captured only using images with a commercial image recognition API<a class="fn" href="#fn10" id="foot-fn10"><sup>9</sup></a>.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-37">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion and Future Work</h2>
        </div>
      </header>
      <p>In this paper, we presented an approach to automatically extract connotations as labels for images by leveraging weakly supervised image-tweet data. We showed that the approach is scalable to many new classes and can support large scale image recognition as required in Web scenarios. In future, we aim to extend the approach to varied domains and check its generalization ability. Also, we would like to address other problems such as label inter-dependency and sparsity.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. 2013. Deep canonical correlation analysis. In <em><em>International Conference on Machine Learning</em></em> . 1247–1255.</li>
        <li id="BibPLXBIB0002" label="[2]">David&nbsp;M Blei and Michael&nbsp;I Jordan. 2003. Modeling annotated data. In <em><em>Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</em></em> . ACM, 127–134.</li>
        <li id="BibPLXBIB0003" label="[3]">Kalina Bontcheva and Dominic Rout. 2014. Making sense of social media streams through semantics: a survey. <em><em>Semantic Web</em></em> 5, 5 (2014), 373–403.</li>
        <li id="BibPLXBIB0004" label="[4]">Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In <em><em>Proceedings of the 24th international conference on Machine learning</em></em> . ACM, 129–136.</li>
        <li id="BibPLXBIB0005" label="[5]">Rudolf Carnap. 1988. <em><em>Meaning and necessity: a study in semantics and modal logic</em></em> . University of Chicago Press.</li>
        <li id="BibPLXBIB0006" label="[6]">Xinlei Chen and Abhinav Gupta. 2015. Webly supervised learning of convolutional networks. In <em><em>Proceedings of the IEEE International Conference on Computer Vision</em></em> . 1431–1439.</li>
        <li id="BibPLXBIB0007" label="[7]">Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. 2013. Neil: Extracting visual knowledge from web data. In <em><em>Proceedings of the IEEE International Conference on Computer Vision</em></em> . 1409–1416.</li>
        <li id="BibPLXBIB0008" label="[8]">Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In <em><em>Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</em></em> . IEEE, 248–255.</li>
        <li id="BibPLXBIB0009" label="[9]">Emily Denton, Jason Weston, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. 2015. User conditional hashtag prediction for images. In <em><em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> . ACM, 1731–1740.</li>
        <li id="BibPLXBIB0010" label="[10]">Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, and William&nbsp;W Cohen. 2016. Tweet2vec: Character-based distributed representations for social media. <em><em>arXiv preprint arXiv:1605.03481</em></em> (2016).</li>
        <li id="BibPLXBIB0011" label="[11]">André Elisseeff and Jason Weston. 2002. A kernel method for multi-labelled classification. In <em><em>Advances in neural information processing systems</em></em> . 681–687.</li>
        <li id="BibPLXBIB0012" label="[12]">Fangxiang Feng, Xiaojie Wang, and Ruifan Li. 2014. Cross-modal retrieval with correspondence autoencoder. In <em><em>Proceedings of the 22nd ACM international conference on Multimedia</em></em> . ACM, 7–16.</li>
        <li id="BibPLXBIB0013" label="[13]">Andrea Frome, Greg&nbsp;S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, <em>et al.</em> 2013. Devise: A deep visual-semantic embedding model. In <em><em>Advances in neural information processing systems</em></em> . 2121–2129.</li>
        <li id="BibPLXBIB0014" label="[14]">Yoav Goldberg and Omer Levy. 2014. word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method. <em><em>arXiv preprint arXiv:1402.3722</em></em> (2014).</li>
        <li id="BibPLXBIB0015" label="[15]">Yuyun Gong and Qi Zhang. 2016. Hashtag Recommendation Using Attention-Based Convolutional Neural Network.. In <em><em>IJCAI</em></em> . 2782–2788.</li>
        <li id="BibPLXBIB0016" label="[16]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In <em><em>Proceedings of the IEEE conference on computer vision and pattern recognition</em></em> . 770–778.</li>
        <li id="BibPLXBIB0017" label="[17]">Yoon Kim, Yacine Jernite, David Sontag, and Alexander&nbsp;M Rush. 2016. Character-Aware Neural Language Models.. In <em><em>AAAI</em></em> . 2741–2749.</li>
        <li id="BibPLXBIB0018" label="[18]">Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <em><em>arXiv preprint arXiv:1412.6980</em></em> (2014).</li>
        <li id="BibPLXBIB0019" label="[19]">Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David&nbsp;A Shamma, <em>et al.</em> 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. <em><em>International Journal of Computer Vision</em></em> 123, 1 (2017), 32–73.</li>
        <li id="BibPLXBIB0020" label="[20]">Yuncheng Li, Yale Song, and Jiebo Luo. 2017. Improving Pairwise Ranking for Multi-label Image Classification. <em><em>arXiv preprint arXiv:1704.03135</em></em> (2017).</li>
        <li id="BibPLXBIB0021" label="[21]">Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C&nbsp;Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In <em><em>European conference on computer vision</em></em> . Springer, 740–755.</li>
        <li id="BibPLXBIB0022" label="[22]">Ishan Misra, C Lawrence&nbsp;Zitnick, Margaret Mitchell, and Ross Girshick. 2016. Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels. In <em><em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em></em> . 2930–2939.</li>
        <li id="BibPLXBIB0023" label="[23]">Aditya Mogadala and Achim Rettinger. 2015. Multi-modal Correlated Centroid Space for Multi-lingual Cross-Modal Retrieval. In <em><em>European Conference on Information Retrieval</em></em> . Springer, 68–79.</li>
        <li id="BibPLXBIB0024" label="[24]">Cesc&nbsp;Chunseong Park, Byeongchang Kim, and Gunhee Kim. 2017. Attend to You: Personalized Image Captioning with Context Sequence Memory Networks. <em><em>arXiv preprint arXiv:1704.06485</em></em> (2017).</li>
        <li id="BibPLXBIB0025" label="[25]">NN Pascu. 1979. Alpha-close-to-convex functions. In <em><em>Romanian-Finnish Seminar on Complex Analysis</em></em> . Springer, 331–335.</li>
        <li id="BibPLXBIB0026" label="[26]">Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, <em>et al.</em> 2015. Imagenet large scale visual recognition challenge. <em><em>International Journal of Computer Vision</em></em> 115, 3 (2015), 211–252.</li>
        <li id="BibPLXBIB0027" label="[27]">Jieying She and Lei Chen. 2014. Tomoha: Topic model-based hashtag recommendation on twitter. In <em><em>Proceedings of the 23rd International Conference on World Wide Web</em></em> . ACM, 371–372.</li>
        <li id="BibPLXBIB0028" label="[28]">Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. <em><em>arXiv preprint arXiv:1409.1556</em></em> (2014).</li>
        <li id="BibPLXBIB0029" label="[29]">Nitish Srivastava and Ruslan&nbsp;R Salakhutdinov. 2012. Multimodal learning with deep boltzmann machines. In <em><em>Advances in neural information processing systems</em></em> . 2222–2230.</li>
        <li id="BibPLXBIB0030" label="[30]">Bart Thomee, David&nbsp;A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: The new data in multimedia research. <em><em>Commun. ACM</em></em> 59, 2 (2016), 64–73.</li>
        <li id="BibPLXBIB0031" label="[31]">Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. 2017. Learning From Noisy Large-Scale Datasets With Minimal Supervision. <em><em>arXiv preprint arXiv:1701.01619</em></em> (2017).</li>
        <li id="BibPLXBIB0032" label="[32]">Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: learning to rank with joint word-image embeddings. <em><em>Machine learning</em></em> 81, 1 (2010), 21–35.</li>
        <li id="BibPLXBIB0033" label="[33]">Jason Weston, Sumit Chopra, and Keith Adams. 2014. #TagSpace: Semantic Embeddings from Hashtags. In <em><em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em></em> . 1822–1827.</li>
        <li id="BibPLXBIB0034" label="[34]">Yan Xia, Xudong Cao, Fang Wen, and Jian Sun. 2014. Well begun is half done: Generating high-quality seeds for automatic image dataset construction from web. In <em><em>European Conference on Computer Vision</em></em> . Springer, 387–400.</li>
        <li id="BibPLXBIB0035" label="[35]">Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. 2015. Learning from massive noisy labeled data for image classification. In <em><em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em></em> . 2691–2699.</li>
        <li id="BibPLXBIB0036" label="[36]">Yazhou Yao, Jian Zhang, Fumin Shen, Xian-Sheng Hua, Jingsong Xu, and Zhenmin Tang. 2017. Exploiting Web Images for Dataset Construction: A Domain Robust Approach. <em><em>IEEE Transactions on Multimedia</em></em> (2017).</li>
        <li id="BibPLXBIB0037" label="[37]">Lei Zhang and Achim Rettinger. 2014. X-LiSA: cross-lingual semantic annotation. <em><em>Proceedings of the VLDB Endowment</em></em> 7, 13 (2014), 1693–1696.</li>
        <li id="BibPLXBIB0038" label="[38]">Min-Ling Zhang and Zhi-Hua Zhou. 2006. Multilabel neural networks with applications to functional genomics and text categorization. <em><em>IEEE transactions on Knowledge and Data Engineering</em></em> 18, 10(2006), 1338–1351.</li>
        <li id="BibPLXBIB0039" label="[39]">Yan-Tao Zheng, Ming Zhao, Yang Song, Hartwig Adam, Ulrich Buddemeier, Alessandro Bissacco, Fernando Brucher, Tat-Seng Chua, and Hartmut Neven. 2009. Tour the world: building a web-scale landmark recognition engine. In <em><em>Computer vision and pattern recognition, 2009. CVPR 2009. IEEE conference on</em></em> . IEEE, 1085–1092.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Work done while doing an internship at Institute AIFB.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="https://www.flickr.com/">https://www.flickr.com/</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a>Not to be confused with “intention”.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class="link-inline force-break" href="https://clarifai.com/">https://clarifai.com/</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class="link-inline force-break" href="https://twitter.com/">https://twitter.com/</a></p>
    <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class="link-inline force-break" href="https://trends.google.com/trends/">https://trends.google.com/trends/</a></p>
    <p id="fn7"><a href="#foot-fn7"><sup>6</sup></a><a class="link-inline force-break" href="https://developer.twitter.com/en/docs/tweets/search/overview/basic-search">https://developer.twitter.com/en/docs/tweets/search/overview/basic-search</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>7</sup></a><a class="link-inline force-break" href="http://wiki.dbpedia.org/">http://wiki.dbpedia.org/</a></p>
    <p id="fn9"><a href="#foot-fn9"><sup>8</sup></a><a class="link-inline force-break" href="https://github.com/idio/wiki2vec">https://github.com/idio/wiki2vec</a></p>
    <p id="fn10"><a href="#foot-fn10"><sup>9</sup></a><a class="link-inline force-break" href="https://clarifai.com/">https://clarifai.com/</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186352">https://doi.org/10.1145/3184558.3186352</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
