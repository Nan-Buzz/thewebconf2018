<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>    <link rel="cite-as" href="https://doi.org/10.1145/3184558.3186980"/>
</head>  <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186980'>https://doi.org/10.1145/3184558.3186980</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186980'>https://w3id.org/oa/10.1145/3184558.3186980</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Yanyan</span>      <span class="surName">Wang</span>     School of Computer Science, Northwestern Polytechnical University     </div>     <div class="author">     <span class="givenName">Qun</span>      <span class="surName">Chen</span>     School of Computer Science, Northwestern Polytechnical University     </div>     <div class="author">     <span class="givenName">Xin</span>      <span class="surName">Liu</span>     School of Computer Science, Northwestern Polytechnical University     </div>     <div class="author">     <span class="givenName">Murtadha</span>      <span class="surName">Ahmed</span>     School of Computer Science, Northwestern Polytechnical University     </div>     <div class="author">     <span class="givenName">Zhanhuai</span>      <span class="surName">Li</span>     School of Computer Science, Northwestern Polytechnical University     </div>     <div class="author">     <span class="givenName">Wei</span>      <span class="surName">Pan</span>     School of Computer Science, Northwestern Polytechnical University     </div>     <div class="author">     <span class="givenName">Hailong</span>      <span class="surName">Liu</span>     School of Computer Science, Northwestern Polytechnical University, <a href="mailto:wangyanyan@mail., chenbenben@, liuxin@mail., murtadha@mail., lizhh@, panwei1002@, liuhailong@nwpu.edu.cn">wangyanyan@mail., chenbenben@, liuxin@mail., murtadha@mail., lizhh@, panwei1002@, liuhailong@nwpu.edu.cn</a>     </div>         <Affiliation id="aff2">Key Laboratory of Big Data Storage and Management, Northwestern Polytechnical University, Ministry of Industry and Information Technology, 1 Dongxiang RoadXi&#x0027;an Shaanxi, P.R.China 710029</Affiliation>    </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186980" target="_blank">https://doi.org/10.1145/3184558.3186980</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>The state-of-the-art techniques for aspect-level sentiment analysis focus on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their practical performance may fall short of expectations due to semantic complexity of natural languages. Motivated by the observation that linguistic hints (e.g. explicit sentiment words and shift words) can be strong indicators of sentiment, we present a joint framework, SenHint, which integrates the output of deep neural networks and the implication of linguistic hints into a coherent reasoning model based on Markov Logic Network (MLN). In SenHint, linguistic hints are used in two ways: (1) to identify easy instances, whose sentiment can be automatically determined by machine with high accuracy; (2) to capture implicit relations between aspect polarities. We also empirically evaluate the performance of SenHint on both English and Chinese benchmark datasets. Our experimental results show that SenHint can effectively improve accuracy compared with the state-of-the-art alternatives.</small>     </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Deep neural networks; Linguistic hints; Aspect-level sentiment analysis</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yanyan Wang, Qun Chen, Xin Liu, Murtadha Ahmed, Zhanhuai Li, Wei Pan, and Hailong Liu. 2018. SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 4 Pages. <a href="https://doi.org/10.1145/3184558.3186980" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186980</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>Aspect-level sentiment analysis, the task of extracting opinions expressed towards different aspects of an entity, is highly valuable to both consumers and businesses [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. For example, given a review that covers two sentences <em>&#x201D;The phone has a great resolution. But it can not last long to watch videos&#x201D;</em> which evaluates the phone from two aspects <em>display</em> and <em>battery</em>. Our goal in this task is to identify the polarity of each aspect.</p>    <p>The state-of-the-art techniques focus on features modeling using a variety of deep neural networks, most popular among them is Attention-based LSTM with Aspect Embedding (ATAE-LSTM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>]. Despite they can achieve performance improvement compared with previous ones (e.g., lexicon-based [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] and SVM-based approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]) , their practical performance may still fall short of expectations due to semantic complexity of natural languages.</p>    <p>Natural languages richly provide many useful linguistic hints that can effectively enhance the performance of sentiment analysis. A sentence may contain very explicit sentiment words, which are highly indicative of sentiment. In the running example, the presence of the positive sentiment word &#x201C;great&#x201D;, together with the absence of any negative word, strongly suggest that the sentiment of the first sentence is positive. It may also contain shift words, e.g., <em>but</em> and <em>however</em>, which are highly indicative of polarity relation. Again in the running example, the word &#x201C;But&#x201D; at the beginning of the second sentence strongly indicates that its polarity is opposite to the polarity of the first sentence. In contrast, the absence of shift words usually suggests that the polarities of two sentences are similar. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186980/images/www18companion-220-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">An example of SenHint.</span>     </div>     </figure>    </p>    <p>In this demo, we present a joint framework for aspect-level sentiment analysis, SenHint, which integrates deep neural networks and linguistic hints into a coherent reasoning model. Its basic idea is to design a Markov Logic Network (MLN) for jointly modeling explicit sentiment, the output of deep neural networks, and implicit polarity relations. We note that it is <em>not</em> new to leverage linguistic hints for sentiment analysis. For instance, the classical lexicon-based approaches [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] used the hints of sentiment words for polarity reasoning; the hints of shift words have also been used to tune the performance of deep neural networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. However, <em>SenHint</em> is a novel joint framework based on MLN that can integrate the output of DNN and the implication of linguistic hints in a single model. Compared with previous approaches, SenHint also uses linguistic hints in different ways. It only uses sentiment words to determine the polarities of easy instances, whose explicit sentiment can be automatically determined by machine with high accuracy. It captures implicit relations between aspect polarities by the hints of shift words and encodes them as weighted first-order logic formulas in a MLN. The major contributions of this demo can be summarized as follows:</p>    <ol class="list-no-style">     <li id="list1" label="(1)">We propose SenHint, a joint framework for aspect-level sentiment analysis based on MLN;<br/></li>     <li id="list2" label="(2)">We present the techniques of encoding the output of deep neural networks and aspect polarity relations in a MLN;<br/></li>     <li id="list3" label="(3)">We empirically evaluate the performance of SenHint on both English and Chinese benchmark datasets. Our experiments show that SenHint can effectively improve accuracy compared with state-of-the-art alternatives.<br/></li>    </ol>    <p>The rest of this demo is organized as follows: Section&#x00A0;<a class="sec" href="#sec-4">2</a> defines the task and presents the SenHint framework. Section&#x00A0;<a class="sec" href="#sec-10">3</a> presents the empirical evaluation results and details demo plan.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> SenHint Framework</h2>     </div>    </header>    <p>In aspect-level sentiment analysis, the aspect, also called aspect category, is specified by a pair of entity type and attribute label. Take the phone review above as an example, there are two aspect categories in the sentences: <em>(display, quality)</em> and <em>(battery, operation performance)</em>, where <em>display</em> and <em>battery</em> are entity types, <em>quality</em> and <em>operation performance</em> are attribute labels. Following the literature [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], we view entity type as aspect. Our goal is to predict sentiment polarity of the aspects given a sentence and its pre-specified aspects.</p>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Framework Overview</h3>     </div>     </header>     <p>SenHint seamlessly integrates the output of DNN and the implication of linguistic hints into a unified reasoning model based on Markov Logic Network (MLN) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. MLN has been widely applied to infer uncertain knowledge, e.g., Deepdive [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] and ProbKB [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. A MLN <em>L</em> is a set of pairs {(<em>F<sub>i</sub>     </em>, <em>w<sub>i</sub>     </em>)}, where <em>F<sub>i</sub>     </em> is a rule in first-order logic rule and <em>w<sub>i</sub>     </em> is a real number that expresses some level of confidence on this rule. Inference on MLN consists of two steps: grounding and marginal inference. The grounding process constructs a <em>ground factor graph</em> based on the rules, which defines a probability distribution over random variables. SenHint uses the marginal probability of a variable for identifying the polarity of a aspect. This process is called marginal inference of probabilistic graphical models.</p>     <p>An example of SenHint, including first-order logic rules and ground factor graph, has been shown in Figure 1. In the graph, aspect polarities are represented by variables (round nodes in the figure), and the influences of DNN output and linguistic implication are represented by factors (box nodes in the figure). The value of a variable indicates its corresponding aspect sentiment. There are two types of variables: <em>evidence variables</em> and <em>infer variables</em>. Evidence variables correspond to the easy aspect polarities that have been determined by explicit linguistic hints. They participate in the MLN inference, but their values are specified beforehand and remain unchanged in the inference process. The values of infer variables should instead be inferred based on the constructed MLN. Additionally, there are three types of factors: <em>DNN factor</em>, <em>similar factor</em> and <em>opposite factor</em>. The DNN factor simulates the effect of DNN output on aspect polarity. The similar factor and opposite factor represent implicit relations between aspect polarities.</p>     <p>Note that in SenHint, the values of evidence variables (easy aspect polarities) and implicit relations between aspect polarities are estimated based on extracted linguistic hints. In the following subsections, we will describe: 1) how to represent the output of DNN by weighted first-order rule (Subsection&#x00A0;<a class="sec" href="#sec-6">2.2</a>); 2) how to capture explicit aspect polarities using the hints of sentiment lexicon; (Subsection&#x00A0;<a class="sec" href="#sec-7">2.3</a>); 3) how to extract implicit relations between aspect polarities and represent them by weighted first-order logic rules; (Subsection&#x00A0;<a class="sec" href="#sec-8">2.4</a>); 4) how to perform joint inference on MLN (Subsection&#x00A0;<a class="sec" href="#sec-9">2.5</a>).</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Modeling DNN Output</h3>     </div>     </header>     <p>For aspect-level sentiment analysis, the approach of ATAE-LSTM [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] have attracted much attention due to its empirically improved performance compared with alternative DNNs [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. ATAE-LSTM uses attention mechanism to concentrate on different parts of a sentence when different aspects are taken as input. Its output can indicate the influence resulting from multiple levels of features. In this demo, we train a ATAE-LSTM model and then encode its results into MLN. However, it can be observed that other DNNs can be similarly incorporated into MLN.</p>     <p>In order to integrate the influence of DNN output into a MLN, we design a rule, expressed by <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w(p): \ dnn\_prob(t, p)\rightarrow positive(t) \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> , in which the left-hand side (LHS), <span class="inline-equation"><span class="tex">$dnn\_prob(t, p)$</span>     </span>, predicates that the probability of aspect polarity <em>t</em> being positive is equal to the value of <em>p</em>, and the right-hand side (RHS), <em>positive</em>(<em>t</em>), is a boolean variable indicating whether an aspect polarity <em>t</em> is positive. The weight function <em>w</em>(<em>p</em>) denotes the level of confidence on the rule. Observing that the relationship between the weight <em>w</em> and the probability <em>p</em> (for a boolean variable <em>x</em> being true) can be expressed by <em>p</em>(<em>x</em> = 1) = <em>e<sup>w</sup>     </em>/(1 + <em>e<sup>w</sup>     </em>), we define the rule weight as <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w(p)=ln(\frac{p}{1-p}). \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> According to Eq.&#x00A0;<a class="eqn" href="#eq2">2</a>, <em>w</em>(<em>p</em>) > 0 if 0.5 < <em>p</em> < 1.0; otherwise, if 0 < <em>p</em> < 0.5, <em>w</em>(<em>p</em>) < 0. Note that <em>w</em>(<em>p</em>) takes the value of 10 when <em>p</em> = 1.0, and the value of -10 when <em>p</em> = 0. In the case of <em>w</em>(<em>p</em>) > 0, a zero value of <em>positive</em>(<em>t</em>) would invoke a cost penalty. In the case of <em>w</em>(<em>p</em>) < 0, a positive value for <em>positive</em>(<em>t</em>) would instead invoke a cost penalty.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Identifying Explicit Polarities</h3>     </div>     </header>     <p>The existing lexicon-based approaches typically use the sum of sentiment values of all sentiment words in the sentence to reason about polarity. In a sentiment lexicon, the score of a sentiment word indicates its intensity of sentiment. For instance, in the lexicon of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>], the scores of sentiment words are normalized into the interval of [-4,4]: a positive (negative) score indicates positive (negative) polarity and the polarity intensity increases with the absolute value of score.</p>     <p>Unfortunately, the lexicon-based approaches usually fail to detect the true sentiment of a sentence in two ambiguous cases: 1) the sentence does not contain explicit sentiment words; 2) the sentiment words in the sentence hold conflicting polarities. SenHint considers a sentiment word as <em>explicit</em> if and only if the absolute value of its score exceeds a pre-specified threshold (e.g., 1.0 in our experiment). Note that an explicit sentiment word can be positive or negative. SenHint considers an aspect polarity as <em>easy instance</em> if and only if the sentence containing the aspect polarity satisfies the following two conditions:</p>     <ul class="list-no-style">     <li id="list4" label="&#x2022;">The sentence does not contain sentiment words with conflicting polarities;<br/></li>     <li id="list5" label="&#x2022;">The sentence contains explicit sentiment words;<br/></li>     </ul>     <p>According to the above two conditions, the sentence, &#x201C;The phone is great.&#x201D;, is an easy instance, because it contains only positive words and it contains the explicit word &#x201C;great&#x201D;. In contrast, the sentence, &#x201C;To be honest, i am a little disappointed and considering returning it&#x201D;, is <em>not</em> an easy intance, because it contains both the positive word &#x201C;honest&#x201D; and the negative word &#x201C;disappointed&#x201D;.</p>     <p>Since negation words can effectively reverse sentiment word polarity, SenHint takes negation into consideration while reasoning about word sentiment. For each sentiment word, SenHint checks whether there is any negation word in the three words before it. If a negation word is found, word sentiment would be reversed. As a conclusion of this subsection, if an aspect polarity in a sentence is considered as easy instance, its polarity is determined by the sentiment of the explicit word in the sentence. It is modeled as evidence variables in MLN. Its polarity result would <em>not</em> be affected by MLN inference, but it could help MLN to detect the true polarity of more challenging instances.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Modeling Implicit Polarity Relations</h3>     </div>     </header>     <p>It is common in natural languages to connect two sentences with opposite polarities by shift words. In contrast, the absence of shift words usually indicates that two neighboring sentences have similar polarities. In practice, shift words can appear within a sentence (inner-sentence) or between two sentences (inter-sentence).</p>     <p>In the inner-sentence case, if a sentence contains multiple aspects and no shift word, the aspect polarities within the sentence are supposed to have the same polarity. However, SenHint does not use shift words to capture opposite polarity relations, because it is very challenging to determine whether a shift word is used to connect two aspect polarities or it is only a connecting word within a phrase describing an aspect polarity. For example, the sentence &#x201D;I&#x0027;m not sure if it was covered in the description, but it does have a backlit keyboard and I&#x0027;m getting about an 8 hour battery life on one charge&#x201D; contains the shift word of &#x201C;but&#x201D;, but the two aspect polarities (e.g., <em>keyboard</em> and <em>battery</em>) are similar.</p>     <p>In the inter-sentence case, the reasoning process of SenHint works as follows: 1) if two neighboring sentences do not contain any shift word, their respective aspect polarities are supposed to be similar; 2) if neither of them contains any inner-sentence shift word but they are connected by a shift word, their respective aspect polarities are supposed to be opposite.</p>     <p>In SenHint, the influence of <em>similar</em> relations on aspect polarities is represented by the following two rules: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w_s: \ positive(t_1), similar(t_1, t_2) \rightarrow positive(t_2) \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> , and <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w_s: \ !positive(t_1), similar(t_1, t_2) \rightarrow !positive(t_2) \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> , in which <em>w<sub>s</sub>     </em> denotes a positive weight of rule (<em>w<sub>s</sub>     </em> = 2 in our experiments), and !<em>positive</em>(<em>t</em>     <sub>1</sub>) denotes the negation of a boolean variable. Similarly, the influence of <em>opposite</em> relations on aspect polarities is represented by <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w_o: \ positive(t_1), opposite(t_1, t_2) \rightarrow !positive(t_2) \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> , and <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} w_o: \ !positive(t_1), opposite(t_1, t_2) \rightarrow positive(t_2) \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> , in which <em>w<sub>o</sub>     </em> denotes a positive weight (<em>w<sub>o</sub>     </em> = <em>w<sub>s</sub>     </em> = 2 in our experiments). Note that in MLN, positive rule weight means that if LHS is true, RHS tends to be also true; otherwise, rule is violated with a cost penalty.</p>    </section>    <section id="sec-9">     <header>     <div class="title-info">      <h3>       <span class="section-number">2.5</span> Joint Inference</h3>     </div>     </header>     <p>SenHint considers the output of DNN as a feature of an inference variable and transforms their relationship into a unary factor in the grounding phase. It also transforms the implicit relations defined in Eq.&#x00A0;<a class="eqn" href="#eq3">3</a>, &#x00A0;<a class="eqn" href="#eq4">4</a>, &#x00A0;<a class="eqn" href="#eq5">5</a> and &#x00A0;<a class="eqn" href="#eq6">6</a> into binary factors between variables. In principle, each rule should be transformed into a type of factors in the factor graph. However, the two types of factors corresponding to the rules specifying similar relation (or opposite relation) are similar; for the sake of presentation simplicity, we only show one factor between two aspect polarities in this demo. The ground factor graph then defines a probability distribution over its variables [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>]. As typical in MLN inference, SenHint uses Gibbs sampling[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] for inference. In our experiments, we use the inference engine of DeepDive [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>]. More details on grounding and inference are omitted here due to space limit, but can be found in our technical report [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186980/images/www18companion-220-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Screenshot for SenHint.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Experiments and Demo Plan</h2>     </div>    </header>    <p>We empirically evaluate the performance of SenHint on the benchmark datasets of SemEval 2016 Task 5 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>], including the domains laptop (English), phone (Chinese) and camera (Chinese). Our experiments perform 2-class classification to label an aspect polarity as <em>positive</em> or <em>negative</em>. Note that the laptop dataset contains neutral instances. But they are ignored in our experiments. The sentiment lexicons for English and Chinese datasets are provided by VADER <a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> and the website <a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> respectively. For English data, the word vectors are initialized by Glove <a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. For Chinese data, we use jieba <a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> to tokenize sentences and obtain word embeddings from Baidu <a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Evaluation results for ATAE-LSTM and SenHint.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:center;">domain</td>       <td style="text-align:center;">SemEval</td>       <td style="text-align:center;">ATAE-</td>       <td colspan="3" style="text-align:center;">Our approach<hr/>       </td>      </tr>      <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">2016 (top)</td>       <td style="text-align:center;">LSTM</td>       <td style="text-align:center;">SH-rel</td>       <td style="text-align:center;">SH-lex</td>       <td style="text-align:center;">SenHint</td>      </tr>      <tr>       <td style="text-align:center;">laptop</td>       <td style="text-align:center;">&#x2014;</td>       <td style="text-align:center;">77.35%</td>       <td style="text-align:center;">80%</td>       <td style="text-align:center;">79.20%</td>       <td style="text-align:center;">        <strong>81.46</strong>%</td>      </tr>      <tr>       <td style="text-align:center;">phone</td>       <td style="text-align:center;">73.35%</td>       <td style="text-align:center;">73.91%</td>       <td style="text-align:center;">76.37%</td>       <td style="text-align:center;">77.88%</td>       <td style="text-align:center;">        <strong>79.58</strong>%</td>      </tr>      <tr>       <td style="text-align:center;">camera</td>       <td style="text-align:center;">80.46%</td>       <td style="text-align:center;">80.87%</td>       <td style="text-align:center;">82.33%</td>       <td style="text-align:center;">86.07%</td>       <td style="text-align:center;">        <strong>87.11</strong>%</td>      </tr>     </tbody>     </table>    </div>    <p>we compare SenHint with the top performer reported on the SemEval 2016 benchmark datasets [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] as well as the state-of-the-art approach of ATAE-LSTM. To help readers to better understand the influences of respective components in SenHint, we also report the results for applying only implicit polarity relations (SH-rel), and the results for applying only explicit sentiment (SH-lex). Note that the statistics on the identified explicit sentiment and extracted polarity relations are omitted here due to space limit, but can be found in our technical report [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. The detailed evaluation results are presented in Table 1. It can be observed that: 1) our implementation of ATAE-LSTM achieves performance very similar to the reported top performance at [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]; 2) both explicit sentiment and implicit polarity relations can effectively improve accuracy; 3) <em>SenHint outperforms the existing alternatives by more than 4% on</em> laptop<em>and by 5%-6% on</em> phone<em>and</em> camera<em>, which are considerable if we consider the widely recognized challenges of sentiment analysis.</em> Note that the results for the 2-class classification on the domain laptop is not provided in SemEval 2016, so we ignore it in our comparision.</p>    <p>     <strong>Demo Plan.</strong> We have implemented a propotype system, whose GUI is sketched in Figure 2, it consists of five panels:</p>    <ul class="list-no-style">     <li id="list6" label="&#x2022;"><em>Deploy.</em> It initializes the system settings. Users can load lexicon, negation words and shift words;<br/></li>     <li id="list7" label="&#x2022;"><em>DNN.</em> It uses a pre-trained DNN to predict the probability distribution for a test dataset;<br/></li>     <li id="list8" label="&#x2022;"><em>Lexicon.</em> It demonstrates the results of identified explicit sentiment;<br/></li>     <li id="list9" label="&#x2022;"><em>Relation.</em> It demonstrates the results of implicit polarity relations between aspects;<br/></li>     <li id="list10" label="&#x2022;"><em>Inference.</em> It demonstrates the grounded factor graph of MLN and the final polarity analysis results.<br/></li>    </ul>    <p>The demo will be run on a laptop. The attendees will be invited to perform the polarity analysis task using different test datasets. They will be able to zoom in the results at each processing step. We have recorded a video for the demo, which can be accessed via the link: <a class="link-inline force-break"     href="https://www.youtube.com/watch?v=3FN6GOVIBlc">https://www.youtube.com/watch?v=3FN6GOVIBlc</a>. </p>   </section>  </section>  <section class="back-matter">   <section id="sec-11">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work is supported by the Ministry of Science and Technology of China, National Key Research and Development Program (2016YFB1000703), National Natural Science Foundation of China (61332006, 61732014, 61672432, 61472321 and 61502390).</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Yang Chen and Daisy&#x00A0;Zhe Wang. 2014. Knowledge expansion over probabilistic knowledge bases. In <em>      <em>Proceedings of the 2014 ACM SIGMOD international conference on Management of data</em>     </em>. ACM, 649&#x2013;660.</li>     <li id="BibPLXBIB0002" label="[2]">Pedro Domingos and Daniel Lowd. 2009. Markov logic: An interface layer for artificial intelligence. <em>      <em>Synthesis Lectures on Artificial Intelligence and Machine Learning</em>     </em>3, 1(2009), 1&#x2013;155.</li>     <li id="BibPLXBIB0003" label="[3]">Sepp Hochreiter and J&#x00FC;rgen Schmidhuber. 1997. Long short-term memory. <em>      <em>Neural computation</em>     </em>9, 8 (1997), 1735&#x2013;1780.</li>     <li id="BibPLXBIB0004" label="[4]">Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. 2016. Harnessing deep neural networks with logic rules. <em>      <em>arXiv preprint arXiv:1603.06318</em>     </em>(2016).</li>     <li id="BibPLXBIB0005" label="[5]">Clayton&#x00A0;J Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In <em>      <em>Eighth international AAAI conference on weblogs and social media</em>     </em>.</li>     <li id="BibPLXBIB0006" label="[6]">Tony Mullen and Nigel Collier. 2004. Sentiment Analysis using Support Vector Machines with Diverse Information Sources.. In <em>      <em>EMNLP</em>     </em>, Vol.&#x00A0;4. 412&#x2013;418.</li>     <li id="BibPLXBIB0007" label="[7]">Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, and Orphee&#x00A0;De Clercq. 2016. SemEval-2016 Task 5: Aspect Based Sentiment Analysis. In <em>      <em>International Workshop on Semantic Evaluation</em>     </em>. 19&#x2013;30.</li>     <li id="BibPLXBIB0008" label="[8]">Jaeho Shin, Sen Wu, Feiran Wang, Christopher De&#x00A0;Sa, Ce Zhang, and Christopher R&#x00E9;. 2015. Incremental knowledge base construction using deepdive. <em>      <em>Proceedings of the VLDB Endowment</em>     </em>8, 11 (2015), 1310&#x2013;1321.</li>     <li id="BibPLXBIB0009" label="[9]">Yanyan Wang, Qun Chen, Xin Liu, Murtadha Ahmed, Zhanhuai Li, Wei Pan, and Hailong Liu. 2018. <em>      <em>A Joint Framework for Aspect-level Sentiment Analysis by Deep Neutral Networks and Linguistic Hints(technical report)</em>     </em>. Technical Report. <a class="link-inline force-break"      href="http://www.wowbigdata.com.cn/SenHint/senhint-report.pdf"      target="_blank">http://www.wowbigdata.com.cn/SenHint/senhint-report.pdf</a></li>     <li id="BibPLXBIB0010" label="[10]">Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based LSTM for Aspect-level Sentiment Classification. In <em>      <em>Conference on Empirical Methods in Natural Language Processing</em>     </em>. 606&#x2013;615.</li>     <li id="BibPLXBIB0011" label="[11]">Ce Zhang. 2013. Towards high-throughput gibbs sampling at scale:a study across storage managers. In <em>      <em>ACM SIGMOD International Conference on Management of Data</em>     </em>. 397&#x2013;408.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>https://github.com/cjhutto/vaderSentiment</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break"     href="http://ir.dlut.edu.cn/EmotionOntologyDownload">http://ir.dlut.edu.cn/EmotionOntologyDownload</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class="link-inline force-break" href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>https://github.com/fxsjy/jieba</p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a><a class="link-inline force-break" href="http://pan.baidu.com/s/1jIb3yr8">http://pan.baidu.com/s/1jIb3yr8</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186980">https://doi.org/10.1145/3184558.3186980</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
