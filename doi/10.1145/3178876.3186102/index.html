<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>RaRE: Social Rank Regulated Large-scale Network
  Embedding</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3186102'>https://doi.org/10.1145/3178876.3186102</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186102'>https://w3id.org/oa/10.1145/3178876.3186102</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">RaRE: Social Rank Regulated
          Large-scale Network Embedding</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Yupeng</span> <span class=
          "surName">Gu</span>, University of California, Los
          Angeles, Los Angeles, CA, <a href=
          "mailto:ypgu@cs.ucla.edu">ypgu@cs.ucla.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Yizhou</span> <span class=
          "surName">Sun</span>, University of California, Los
          Angeles, Los Angeles, CA, <a href=
          "mailto:yzsun@cs.ucla.edu">yzsun@cs.ucla.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Yanen</span> <span class=
          "surName">Li</span>, Snapchat Inc., Venice, CA, <a href=
          "mailto:yanen.li@snap.com">yanen.li@snap.com</a>
        </div>
        <div class="author">
          <span class="givenName">Yang</span> <span class=
          "surName">Yang</span>, Zhejiang University, Hangzhou,
          China, <a href=
          "mailto:yangya@zju.edu.cn">yangya@zju.edu.cn</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186102"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186102</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Network embedding algorithms that map nodes in a
        network into a low-dimensional vector space are prevalent
        in recent years, due to their superior performance in many
        network-based tasks, such as clustering, classification,
        and link prediction. The main assumption of existing
        algorithms is that the learned latent representation for
        nodes should preserve the structure of the network, in
        terms of first-order or higher-order connectivity. In other
        words, nodes that are more similar will have higher
        probability to connect to each other. This phenomena is
        typically explained as homophily in network science.
        However, there is another factor usually neglected by the
        existing embedding algorithms, which is the popularity of a
        node. For example, celebrities in a social network usually
        receive numerous followers, which cannot be fully explained
        by the similarity of the two users. We denote this factor
        with the terminology “social rank”. We then propose a
        network embedding model that considers both of the two
        factors in link generation, and learn proximity-based
        embedding and social rank-based embedding separately.
        Rather than simply treating these two factors independent
        with each other, a carefully designed link generation model
        is proposed, which explicitly models the interdependency
        between these two types of embeddings. Experiments on
        several real-world datasets across different domains
        demonstrate the superiority of our novel network embedding
        model over the state-of-the-art methods.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Network
          embedding</small>,</span> <span class=
          "keyword"><small>social rank</small>,</span> <span class=
          "keyword"><small>representation learning</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Yupeng Gu, Yizhou Sun, Yanen Li, and Yang Yang. 2018.
          RaRE: Social Rank Regulated Large-scale Network
          Embedding. In <em>WWW 2018: The 2018 Web Conference,</em>
          <em>April 23–27, 2018 (WWW 2018),</em> <em>Lyon, France.
          ACM, New York, NY, USA</em> 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186102" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186102</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Studying the latent representation of nodes in information
      networks has been a prevalent topic recently. Latent
      representations, also known as latent features or embeddings,
      often reside in a lower dimensional continuous vector space,
      and are especially helpful in terms of understanding the
      nodes. A wide variety of applications can be achieved as a
      result, including classification, visualization, community
      detection and so on.</p>
      <p>Early methods include mapping nodes onto a lower
      dimensional manifold by finding the intrinsic dimensionality
      using spectral methods on graph adjacency matrix, such as
      locally linear embedding (LLE) [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>], Isomap [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0041">41</a>], multidimensional scaling
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>] and so on.
      Most of these methods do not scale for large networks. Later
      on more principled statistical models have been developed
      where node parameters are deduced by optimizing some global
      objective function. The intuition is rather straightforward:
      similarity in the graph should be preserved in the lower
      dimensional space. Proximity in the graph is usually embodied
      as neighbors: according to the homophily assumption
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>], entities
      that are connected in the graph represent some sort of
      similarity. Neighbors are also essential in random walk based
      methods [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0016">16</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0034">34</a>], where
      information and label propagate. As a result, entities that
      are connected in the original graph are often adjacent to
      each other in the latent space.</p>
      <p>However, this seemingly plausible approach has some severe
      drawbacks and naturally triggers several open questions.
      First of all, <em>is it consistently true that all links
      occur between similar nodes</em>? As per the preferential
      attachment process in network generation [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0004">4</a>], nodes are believed to
      have a higher chance to connect to (e.g. follow) high-degree
      nodes (e.g. celebrities) in general. For example, a new
      Twitter user may first choose to follow some well-known
      politicians or movie stars, regardless of being a fan or not.
      Those links are generated due to the high exposure and
      popularity of certain accounts, rather than their similar
      tastes (i.e. proximity of latent representation). For
      instance, famous politicians usually have a fair amount of
      followers on social networks, but certainly not 100% of the
      followers should be considered as similar to them in terms of
      political opinions. On the other hand, everyone has limited
      amount of energy and resources, which prevents many actually
      similar pairs from being present. During a literature review,
      famous groups of scholars in the corresponding field of study
      are usually considered first, while a vast majority of junior
      groups may be ignored, even though they are working on very
      similar topics. With all being said, node features inferred
      according to the homophily assumption will be a mixture of
      popularity and proximity factors, and thus are not desirable
      for clustering or classification tasks. In this work, we use
      a specific terminology “social rank” to denote the popularity
      factor, namely the position where an entity is ranked among
      the network, and “proximity-based representation” to denote
      the general embedding vector which denotes the opinions and
      preferences of an entity.</p>
      <p>The second question is, <em>should these two factors be
      considered as totally independent with each other</em>? From
      the case studies in the above question, we notice that how
      much proximity will contribute to link formation depends on
      the relative social status/rank between a pair of nodes. On
      one hand, when a link from a node to a more popular one is
      observed, it is somewhat likely to be explained by the
      popularity of the latter. On the other hand, when a link from
      a node to a less popular one is present, proximity factor
      should account for most of the intentions. For example, in a
      bibliography citation network, it is often more common for
      papers to cite very famous papers due to their substantial
      public attention. However, when a less popular paper is
      cited, it is almost certainly the case that it is essentially
      very relevant to the work. In sum, homophily itself does not
      suffice to explain the reason behind link generation, and we
      should decide <em>the extent</em> to which homophily is
      trusted considering the social rank of nodes. A principled
      methodology is desired to balance the effect of social rank
      and proximity factor in terms of network generation.</p>
      <p>In this paper we propose a Social <span style=
      "text-decoration: underline;">R</span> <span style=
      "text-decoration: underline;">a</span>nk <span style=
      "text-decoration: underline;">R</span>egulated Network
      <span style="text-decoration: underline;">E</span>mbedding
      (RaRE) model which incorporates both latent <em>rank
      factor</em> and latent <em>proximity-based factor</em> to
      interpret the network generation process. Our unified
      Bayesian framework models the probabilistic relationship
      between social rank, proximity-based representation and
      existence of a link. We discuss what portion is truly
      justified by the proximity between nodes given their social
      rank difference, which explains the generation of a link from
      a brand new perspective. Our method is also scalable to large
      networks. The contribution of our method can be summarized as
      follows:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We propose to solve the network
        embedding problem from a novel Bayesian perspective, which
        integrates both social rank and proximity-based
        embedding.<br /></li>
        <li id="list2" label="•">A brand new probabilistic link
        formation model is formulated that explicitly models the
        extent of contribution of proximity-based embedding under
        different relative social rank difference.<br /></li>
        <li id="list3" label="•">Our method is easily scalable to
        real-world large-scale networks which consist of millions
        of nodes.<br /></li>
      </ul>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Problem
          Definition</h2>
        </div>
      </header>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Background</h3>
          </div>
        </header>
        <p>Extracting latent representation of nodes (also known as
        embedding) in an information network is essential in
        understanding the relative position of each node in the
        network. A natural embedding is the row vector in the
        adjacency matrix where each dimension denotes the link
        status between a pair of nodes. Nevertheless, this plain
        strategy is seldom applicable to real-world tasks due to
        the computational complexity brought by its high
        dimensional representation, as well as its low
        representation power in terms of preserving network
        structure. In order to tackle this problem, traditional
        approaches extract dimensions with the biggest contribution
        to the data (e.g. PCA, SVD, IsoMap), or find lower
        dimensional representation of nodes by factorizing the
        adjacency matrix [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0028">28</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0042">42</a>].</p>
        <p>More recent approaches introduce the notion of “node
        embedding”, a low-dimensional vector representation of
        node, which embodies the latent merits and characteristics
        of an individual. The concept of embedding is very similar
        to word embeddings [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>], where every word is represented
        by a low dimensional vector. These embedding vectors are
        learned by preserving similarity in the corpus (i.e.
        between every word and its context) and similarity in the
        latent space (i.e. vector dot product). Levy and Goldberg
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0024">24</a>] also
        reveal the connection between matrix factorization and word
        embedding, arguing that estimating word embedding is
        equivalent to factorizing a pointwise mutual information
        matrix. In the realm of information networks, the concept
        of “context” no longer exists, and many researchers have
        proposed ways to define similar nodes in the network, such
        as <em>n</em>-hop neighbors or the nodes reachable from a
        random walk [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0034">34</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0040">40</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0043">43</a>]. Besides, the generation process
        also allows great flexibility in the modeling part, and
        different link generation approaches have been proposed
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0013">13</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0014">14</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0043">43</a>]. In sum,
        the lower dimensional representations are much more
        succinct while keeping the majority of information in the
        graph.</p>
        <p>However, none of the network embedding approaches
        explicitly interpret the meaning of the representation, and
        assume the link is generated based on the proximity of
        representations, or that proximity can propagate through
        links. We argue that, there is another essential factor
        (“social rank”) other than proximity that leads to the
        formation of a link, which is not homogeneous for connected
        nodes and should be detached from the general
        proximity-based embedding. Some matrix factorization
        methods model popularity in terms of bias (e.g. [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0020">20</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>]), but
        they simply treat proximity and popularity as two
        independent factors. Instead, we model the interdependency
        of the two factors in link formation explicitly. We will
        explain the necessity of modeling their interdependency in
        Section <a class="sec" href="#sec-9">3</a> and define our
        problem formally in the next paragraph.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Problem
            Definition</h3>
          </div>
        </header>
        <p>We define our embedding problem as follows. An
        information network can be formatted as <em>G</em> =
        (<em>V</em>, <em>E</em>), where <span class=
        "inline-equation"><span class="tex">$V = \lbrace u_n
        \rbrace _{n=1}^{N}$</span></span> is the set of vertices
        and <em>E</em>⊂<em>V</em> <sup>2</sup> is the set of edges.
        We use <em>e<sub>ij</sub></em> to denote the binary status
        of the link from <em>u<sub>i</sub></em> to
        <em>u<sub>j</sub></em> for unweighted networks, or the
        multiplicity of the link for weighted networks. Our goal is
        to infer both latent <em>proximity-based</em>
        representation <span class="inline-equation"><span class=
        "tex">$\lbrace {z}_v | v \in V\rbrace \subset \mathbb
        {R}^{K}$</span></span> and latent <em>rank</em>
        representation <span class="inline-equation"><span class=
        "tex">$\lbrace r_v | v \in V\rbrace \subset \mathbb
        {R^{+}}$</span></span> for nodes in the network. Similar to
        the ordinal numbers, our ranking assumes a higher social
        rank for a smaller value of <em>r</em>, and requires that
        all social ranks are positive. The rank <em>r</em> can also
        be considered as radius of a node in the visualization,
        where center nodes (smaller radius) are most
        influential.</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Approach</h2>
        </div>
      </header>
      <p>In this section, we illustrate our approach by introducing
      a general form of network generation prototype for unweighted
      networks, investigate the details, instantiate our final
      model using mathematical derivations, and extend it to
      general networks. We will discuss its scalability and
      relationship to existing models as well.</p>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Base
            Model</h3>
          </div>
        </header>
        <p>The most general model for the binary status of a link
        <em>e<sub>ij</sub></em> (1 if present, 0 otherwise) is a
        Bernoulli event with parameter</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(e_{ij}=1 |
            r_i, r_j, {z}_i, {z}_j) = f(r_i, r_j, {z}_i, {z}_j)
            \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>where <em>f</em>() is a probability function to be
        designed. First, we would like <em>f</em>() to encode the
        difference of the social ranks, and the most natural
        measure is the difference <em>r<sub>i</sub></em> −
        <em>r<sub>j</sub></em> . Besides, a recent work [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0002">2</a>] has also
        found that the probability of a friendship is a function of
        the difference of two users’ social ranks (independent of
        their absolute values), meaning that the function should be
        translation-invariant. We adopt their assumption and thus
        always study the difference of two entities’ ranks (denoted
        by <em>dr</em> = <em>r<sub>i</sub></em> −
        <em>r<sub>j</sub></em> ) in function <em>f</em>(). For the
        proximity-based representation, the homophily assumption
        has been widely used and accepted in related studies
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0018">18</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0029">29</a>], which
        posits that in information networks, most interactions
        occur between nodes with similar merits and
        characteristics. Therefore, with the hope that the
        representation z is a reflection of people's hidden
        characteristics, we design the probability to be a function
        of their Euclidean distance <em>dz</em> = ||z
        <sub><em>i</em></sub> − z <sub><em>j</em></sub>
        ||<sub>2</sub> in the lower dimensional space. In sum,
        Equation <a class="eqn" href="#eq1">1</a> becomes:
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(e_{ij}=1 |
            r_i, r_j, {z}_i, {z}_j) = f(dr, dz)
            \end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <p></p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig1.svg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">Left: traditional embedding
            models (e.g. [<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0030">30</a>, <a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0040">40</a>]), where <em>s</em>(·, ·)
            measures vector similarity. Right: graphical model
            representation of our model. Shadowed unit
            <em>e<sub>ij</sub></em> represents the observed
            variable (i.e. the status of the link).</span>
          </div>
        </figure>
        <p>A graphical model representation is illustrated in
        Figure <a class="fig" href="#fig1">1</a>. It would be
        controversial about the exact form of <em>f</em>() at this
        stage, therefore in order to have a more convincing
        formulation, we will investigate the distribution of
        parameters under different circumstances (i.e. when the
        link is present/absent) and derive <em>f</em>() from a
        Bayesian perspective.</p>
        <p>To reach the concrete form of the probability function,
        we define the conditional distribution
        <em>p</em>(<em>dr</em>, <em>dz</em>|<em>e<sub>ij</sub></em>
        ) = <em>p</em>(<em>dr</em>|<em>dz</em>,
        <em>e<sub>ij</sub></em> ) ·
        <em>p</em>(<em>dz</em>|<em>e<sub>ij</sub></em> ) in Section
        <a class="sec" href="#sec-11">3.2</a> and the prior
        distribution <em>p</em>(<em>r</em>) and <em>p</em>(z) in
        Section <a class="sec" href="#sec-12">3.3</a>. Finally, the
        exact form of <em>f</em>() is determined by Bayes’ rule in
        Section <a class="sec" href="#sec-13">3.4</a>.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Conditional
            Distributions</h3>
          </div>
        </header>
        <p><em>When the link is present,</em>. it is generally due
        to two reasons: (1) the link-receiver is famous (or at
        least more famous than the link-sender); or (2) the two
        individuals are similar (homophily). In other words, a
        majority of the links occur between pairs of nodes
        (<em>u<sub>i</sub></em> , <em>u<sub>j</sub></em> ) where
        <em>dr</em> = <em>r<sub>i</sub></em> −
        <em>r<sub>j</sub></em> is positive or <em>dz</em> = ||z
        <sub><em>i</em></sub> − z <sub><em>j</em></sub>
        ||<sub>2</sub> is small. From the graphical model in Figure
        <a class="fig" href="#fig1">1</a> (right), <em>dr</em> and
        <em>dz</em> are no longer independent when we have
        knowledge about the link <em>e<sub>ij</sub></em> , which is
        referred to as “explaining away” in Bayesian networks. In
        other words, <em>p</em>(<em>dr</em>,
        <em>dz</em>|<em>e<sub>ij</sub></em> ) ≠
        <em>p</em>(<em>dr</em>|<em>e<sub>ij</sub></em> ) ·
        <em>p</em>(<em>dz</em>|<em>e<sub>ij</sub></em> ) because
        they are conditionally dependent on <em>e<sub>ij</sub></em>
        . In particular, given the presence of a link,
        dissimilarity of two users (i.e. large <em>dz</em>) will
        increase our belief that <em>j</em> is more popular than
        <em>i</em> (i.e. positive <em>dr</em>). For example,
        sportsmen followed by a non-sporty person are likely to be
        very influential. On the other hand, proximity of z (i.e.
        small <em>dz</em>) will eliminate some possibility that
        <em>j</em> is popular (i.e. positive <em>dr</em>), thus
        shifting the mean of <em>dr</em> towards left. For example,
        followees of a sportholic might as well be some
        unspectacular players in his/her home team. Therefore, we
        assume the distribution of <em>dr</em> is a Gaussian with a
        positive mean, and <em>dz</em> follows a (truncated)
        Gaussian distribution where the peak is at <em>dz</em> =
        0:</p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(dr | dz,
            e_{ij} = 1) = \mathcal {N}(\mu \cdot h(dz), \sigma
            _R^2) \end{equation}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(dz | e_{ij}
            = 1) = \frac{1}{\mathcal {Z}} \cdot I_{\mathbb {R}^{+}}
            (dz) \cdot \mathcal {N}(0, \sigma _1^2)
            \end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where <em>μ</em> · <em>h</em>(<em>dz</em>)
        (<em>μ</em> &gt; 0) is the mean of <em>dr</em> conditioned
        on <em>dz</em> and <em>e<sub>ij</sub></em> = 1,
        <span class="inline-equation"><span class="tex">$\mathcal
        {Z}$</span></span> is the normalization term (<span class=
        "inline-equation"><span class="tex">$\mathcal
        {Z}=1/2$</span></span> obviously) and
        <em>I<sub>S</sub></em> (<em>x</em>) is the indicator
        function (takes value 1 if <em>x</em> ∈ <em>S</em> and 0
        otherwise). The hyper-parameter <em>μ</em> controls the
        scale of the mean, while <em>h</em>(<em>dz</em>) adjusts
        the mean with respect to different values of <em>dz</em>.
        <p></p>
        <p>Ideally, <em>dr</em> and <em>dz</em> in the above
        equations should follow our previous intuition exactly.
        Therefore, function <em>h</em>() should obey the following
        properties:</p>
        <ul class="list-no-style">
          <li id="list4" label="•">Non-negative, bounded and
          supported on <span class="inline-equation"><span class=
          "tex">$\mathbb {R}^{+}$</span></span> . In general, most
          of existing links are likely to occur from a node to a
          more influential one (i.e., <em>dr</em> =
          <em>r<sub>i</sub></em> − <em>r<sub>j</sub></em> ≥ 0). In
          other words, no matter what value <em>dz</em> takes, the
          mean of the rank difference (given the link is present)
          <span class="inline-equation"><span class="tex">$\mathbb
          {E}[dr]=\mu \cdot h(dz)$</span></span> should always be
          non-negative.<br /></li>
          <li id="list5" label="•">Non-decreasing monotonic. Given
          the existence of a link, the relative rank difference
          between two nodes should be more significant as their
          dissimilarity increases. This corresponds to the
          “explaining away” effect above.<br /></li>
          <li id="list6" label="•">Concave on some right-unbounded
          interval. From our intuition, as <em>dz</em> increases,
          the marginal gain of <span class=
          "inline-equation"><span class="tex">$\mathbb {E}[dr]=\mu
          \cdot h(dz)$</span></span> should be diminishing. In
          other words, <em>h</em> should be a concave function when
          <em>dz</em> exceeds some threshold.<br /></li>
        </ul>
        <p>Lots of families of functions have these properties, but
        a simple one of them we find to work well is <span class=
        "inline-equation"><span class="tex">$h(dz) =
        \frac{dz^2}{1+dz^2}$</span></span> . An illustration of the
        conditional probability distributions of <em>dr</em> (given
        <em>dz</em> and <em>e<sub>ij</sub></em> = 1) and
        <em>dz</em> (given <em>e<sub>ij</sub></em> = 1) is shown in
        Figure <a class="fig" href="#fig2">2</a> (left).</p>
        <p><em>When the link is absent,</em>. it does not
        necessarily mean that the two individuals are dissimilar,
        or one of them is not popular enough to be observed. In
        other words, there are various reasons why no link is
        observed between two nodes. Therefore, without much
        confidence in claiming any special characteristic of an
        absent link, we assume a Gaussian distribution on
        <em>dr</em> and <em>dz</em>, both centered at 0. In other
        words, <em>dr</em> and <em>dz</em> will have equal chance
        of being positive and negative. Meanwhile, the variance of
        <em>dz</em> should be large as we expect the distribution
        to be more uniform.</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(dr | dz,
            e_{ij} = 0) = \mathcal {N}(0, \sigma _R^2)
            \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p(dz | e_{ij}
            = 0) = \frac{1}{\mathcal {Z}} \cdot I_{\mathbb {R}^{+}}
            (dz) \cdot \mathcal {N}(0, \sigma _2^2)
            \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\mathcal {Z}=1/2$</span></span> is the normalization
        term.
        <p></p>
        <p>Note that we assume <span class=
        "inline-equation"><span class="tex">$\sigma _2^2 {\gt}
        \sigma _1^2$</span></span> (Equation <a class="eqn" href=
        "#eq4">4</a> and <a class="eqn" href="#eq6">6</a>), as the
        probability distribution function of <em>dz</em> should
        look much more flat when the link is absent (illustrated in
        Figure <a class="fig" href="#fig2">2</a> (b)). For
        simplicity, we assume the variance of <em>dr</em> remains
        the same as <span class="inline-equation"><span class=
        "tex">$\sigma _R^2$</span></span> for the two
        scenarios.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Conditional probability
            distributions of variable <em>dr</em> given <em>dz</em>
            and <em>e<sub>ij</sub></em> , and variable <em>dz</em>
            given <em>e<sub>ij</sub></em> .</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Prior
            Distributions</h3>
          </div>
        </header>
        <p>Social rank <em>r</em> reflects the ranking of actors in
        a network, and the most popular entities are assumed to
        have the smallest <em>r</em> values. Intuitively,
        influential nodes should always be fewer than ordinary
        nodes, which means there should be more nodes with large
        <em>r</em> values. Since power law is usually utilized to
        model a node's characteristics (e.g. ranks of individuals,
        foraging patterns of many species, etc.), we use the
        (truncated) long-tailed power law distribution as the prior
        distribution of the inverse of social rank, namely
        <span class="inline-equation"><span class="tex">$p(r) \sim
        (1/r)^{-k_R}$</span></span> (<em>k<sub>R</sub></em> &gt;
        0). Mathematically, a power law cannot be a well-defined
        probability distribution, but a distribution that is a
        truncated power law is possible: <span class=
        "inline-equation"><span class="tex">$p(r) = C \cdot
        (1/r)^{-k_R}$</span></span> when 1/<em>r</em> &gt;
        <em>r<sub>min</sub></em> . It is easy to reveal that the
        normalization factor <span class=
        "inline-equation"><span class="tex">$C = (k_R+1) \cdot
        r_{min}^{k_R+1}$</span></span> , and thus <span class=
        "inline-equation"><span class="tex">$p(r) = (k_R+1) \cdot
        r_{min}^{k_R+1} \cdot (1/r)^{-k_R}$</span></span> (0 &lt;
        <em>r</em> &lt; 1/<em>r<sub>min</sub></em> ).</p>
        <p>Proximity-based representation z should generally lie
        more uniformly on the <em>K</em> dimensional space,
        preferably not too far away from the origin. Therefore, a
        Gaussian prior is assumed on z: <span class=
        "inline-equation"><span class="tex">$p({z}) = \mathcal
        {N}(0, \sigma _Z^2 \cdot I_K)$</span></span> , where
        <em>I<sub>K</sub></em> denotes the <em>K</em> dimensional
        identity matrix. An illustration of the prior distributions
        is shown in Figure <a class="fig" href="#fig3">3</a>.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Left: prior of <em>r</em>.
            Right: prior of z (2D is used for visualization
            purpose).</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Objective
            Function and Optimization</h3>
          </div>
        </header>
        <p>Given the conditional and prior distribution of
        variables, we can formalize the function <em>f</em>() in
        the beginning (Equation (<a class="eqn" href="#eq2">2</a>))
        using Bayes’ rule, and thus finalize our objective.</p>
        <div class="table-responsive" id="Xeq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \begin{aligned} &amp; p(e_{ij} = 1 | dr, dz) =
            \frac{p(dr, dz, e_{ij}=1)}{\sum _{e=0}^{1} p(dr, dz,
            e_{ij}=e)} \\ =&amp; \frac{p(dr | dz, e_{ij} = 1) \cdot
            p(dz | e_{ij} = 1) \cdot p(e_{ij} = 1)}{\sum _{e=0}^{1}
            p(dr | dz, e_{ij} = e) \cdot p(dz | e_{ij} = e) \cdot
            p(e_{ij} = e)} = sigmoid(f_{ij}) \end{aligned}
            \end{equation}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>where
        <div class="table-responsive" id="Xeq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} f_{ij} = \log
            \frac{p(dr | dz, e_{ij} = 1) \cdot p(dz | e_{ij} = 1)
            \cdot p(e_{ij} = 1)}{p(dr | dz, e_{ij} = 0) \cdot p(dz
            | e_{ij} = 0) \cdot p(e_{ij} = 0)}
            \end{equation}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>and <em>sigmoid</em>(<em>x</em>) = 1/(1 + <em>e</em>
        <sup>− <em>x</em></sup> ). Combining Equations (<a class=
        "eqn" href="#eq3">3</a>)-(<a class="eqn" href=
        "#eq6">6</a>), we have
        <div class="table-responsive" id="Xeq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \begin{aligned} f_{ij} &amp;= \frac{\mu }{\sigma _R^2}
            \cdot dr \cdot h(dz) - \frac{\mu ^2}{2\sigma _R^2}
            \cdot h^2(dz) - (\frac{1}{2\sigma _1^2} -
            \frac{1}{2\sigma _2^2}) (dz)^2 \\ &amp;+ \log
            \frac{\sigma _2 \cdot p(e_{ij} = 1)}{\sigma _1 \cdot
            p(e_{ij} = 0)} \end{aligned}
            \end{equation}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <p>Since <em>h</em>(<em>dz</em>) &lt; 1, we ignore the
        insignificant second order term <em>h</em>
        <sup>2</sup>(<em>dz</em>) as an approximation for now. For
        short, we use <em>λ<sub>R</sub></em> to denote <span class=
        "inline-equation"><span class="tex">$\frac{\mu }{\sigma
        _R^2}$</span></span> (<em>μ</em> &gt;
        0⇒<em>λ<sub>R</sub></em> &gt; 0), <em>λ<sub>Z</sub></em> to
        denote <span class="inline-equation"><span class=
        "tex">$\frac{1}{2\sigma _1^2} - \frac{1}{2\sigma
        _2^2}$</span></span> (<span class=
        "inline-equation"><span class="tex">$\sigma _1^2 {\lt}
        \sigma _2^2 \Rightarrow \lambda _Z {\gt} 0$</span></span> )
        and <em>λ</em> <sub>0</sub> to denote <span class=
        "inline-equation"><span class="tex">$\log \frac{\sigma _2
        \cdot p(e_{ij} = 1)}{\sigma _1 \cdot p(e_{ij} =
        0)}$</span></span> (for sparse networks,
        <em>p</em>(<em>e<sub>ij</sub></em> = 1) ≪
        <em>p</em>(<em>e<sub>ij</sub></em> = 0)⇒<em>λ</em>
        <sub>0</sub> &lt; 0). Then the equation above is altered to
        the following:</p>
        <div class="table-responsive" id="Xeq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} f_{ij} =
            \lambda _R \cdot dr \cdot h(dz) - \lambda _Z \cdot
            (dz)^2 + \lambda _0 \end{equation}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>where <em>λ<sub>R</sub></em> , <em>λ<sub>Z</sub></em>
        , <em>λ</em> <sub>0</sub> are hyper-parameters that need to
        be pre-assigned. Intuitively, large <em>λ<sub>R</sub></em>
        indicates the rank factor is more important in the network;
        while a large <em>λ<sub>Z</sub></em> indicates the
        proximity-based factor is more important. <em>λ</em>
        <sub>0</sub> reflects the sparsity of the network. We find
        that our model is not sensitive to <em>λ</em> <sub>0</sub>,
        and the optimal values of <em>λ<sub>R</sub></em> and
        <em>λ<sub>Z</sub></em> can be found using cross validation.
        <p></p>
        <p>As an extension to weighted graphs (denoting the weight
        of a link by <em>w<sub>ij</sub></em> ), we treat the weight
        as multi-edges, and consider each edge independently. Thus
        the probability of observing a weighted edge is simply
        generalized to</p>
        <div class="table-responsive" id="Xeq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            p(e_{ij}=w_{ij} | dr,dz) := p(e_{ij}=1 |
            dr,dz)^{w_{ij}} . \end{equation}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>
        <p></p>
        <p>The model parameters are inferred using maximum a
        posteriori (MAP) estimation, i.e.,</p>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \begin{aligned} &amp; \lbrace r^*, {z}^*\rbrace =
            \mathop{argmax}\limits _{r, {z}} \log p(r, {z} | G) \\
            =&amp; \mathop{argmax}\limits _{r, {z}} \log p(G | r,
            {z}) + \log p(r, {z}) \\ =&amp; \mathop{argmax}\limits
            _{r, {z}} \log p(G | r, {z}) + \sum _{i=1}^{N} \log
            p(r_i) + \sum _{i=1}^{N} \log p({z_i}) . \end{aligned}
            \end{equation}</span><br />
            <span class="equation-number">(12)</span>
          </div>
        </div>
        <p></p>
        <p>Since each entity generally has more freedom to issue
        links (e.g. a webpage can contain arbitrary many
        hyperlinks; a person can follow/retweet as many
        people/tweets as she wants), in order to eliminate the
        effect of size, we define the weight
        <em>w<sub>ij</sub></em> as the number of occurrence from
        <em>i</em> to <em>j</em> normalized by <em>i</em>’s
        out-degree: <span class="inline-equation"><span class=
        "tex">$w_{ij} = \#(i \rightarrow j) /
        deg_{out}(i)$</span></span> .</p>
        <p>Edges in the graph are assumed to be generated
        independently. Therefore the likelihood of the graph is
        simply the product of the probabilities of all edges.
        Negative sampling is adopted in consideration of the
        efficiency issue [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>]; in other words, for each existing
        link <em>e<sub>ij</sub></em> , <em>k</em> non-existing
        links are sampled <span class=
        "inline-equation"><span class="tex">$\lbrace e_{il}=0
        \rbrace _{l \sim \mathcal {P}_0}$</span></span> . According
        to [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0031">31</a>], the background probability
        <span class="inline-equation"><span class="tex">$\mathcal
        {P}_0$</span></span> is set to <span class=
        "inline-equation"><span class="tex">$\mathcal {P}_0(n)
        \propto deg_{out}(n)^{0.75}$</span></span> . Denoting the
        sampled negative links by <span class=
        "inline-equation"><span class="tex">$S_0 = \cup _i \lbrace
        (i,l) | e_{il}=0 \rbrace _{l \sim \mathcal
        {P}_0}$</span></span> , Equation (<a class="eqn" href=
        "#eq7">12</a>) becomes</p>
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \begin{aligned} &amp; \lbrace r^*, {z}^*\rbrace =
            \mathop{argmax}\limits _{r, {z}} \Big (\sum
            _{(i,j):e_{ij}{\gt}0} w_{ij} \cdot \log p(e_{ij}=1 |
            dr,dz) \\ +&amp; \sum _{(i,j) \in S_0} \log \big
            (1-p(e_{ij}=1 | dr,dz) \big) ~+ \sum _{i=1}^{N} \log
            p(r_i) + \sum _{i=1}^{N} \log p({z_i}) \Big) \\ =&amp;
            \mathop{argmax}\limits _{r, {z}} \Big (\sum
            _{(i,j):e_{ij}{\gt}0 | dr,dz} w_{ij} \cdot \log
            sigmoid(f_{ij}) \\ +&amp; \sum _{(i,j) \in S_0} \log
            sigmoid(-f_{ij}) ~+ \sum _{i=1}^{N} \log p(r_i) + \sum
            _{i=1}^{N} \log p({z_i}) \Big) . \end{aligned}
            \end{equation}</span><br />
            <span class="equation-number">(13)</span>
          </div>
        </div>
        <p></p>
        <p>The optimization is done using stochastic gradient
        ascent.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span>
            Complexity</h3>
          </div>
        </header>
        <p>The first two summations in Equation (<a class="eqn"
        href="#eq8">13</a>) consist of <em>O</em>(<em>E</em> ·
        <em>K</em>) terms, where <em>E</em> is the number of edges
        in the graph, and <em>K</em> is the dimension of the
        proximity-based factor. The last two summations in Equation
        (<a class="eqn" href="#eq8">13</a>) consist of
        <em>O</em>(<em>N</em> · <em>K</em>) terms, where <em>N</em>
        is the number of nodes. Therefore the computational
        complexity for each epoch of the data is
        <em>O</em>((<em>E</em> + <em>N</em>) · <em>K</em>). In sum,
        the running time is linear to the number of edges and
        nodes, therefore is scalable to large networks. In
        practice, it takes only a few epoches to reach
        convergence.</p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.6</span> Relation to
            Other Models</h3>
          </div>
        </header>
        <p>Our model can be treated as a generalization of the well
        established Bradley-Terry model [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0010">10</a>] in the realm of ranking
        and pairwise comparison, which has been successfully
        applied in learning to rank [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>]. One parametrization of the
        Bradley-Terry model estimates the probability that the
        pairwise comparison <em>i</em>≻<em>j</em> (interpreted as
        “<em>i</em> is preferred to <em>j</em>”, or “<em>i</em>
        ranks higher than <em>j</em>”) is true as</p>
        <div class="table-responsive" id="Xeq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} P(i \succ j) =
            \frac{e^{s_i}}{e^{s_i}+e^{s_j}} = sigmoid(s_i-s_j)
            \end{equation}</span><br />
            <span class="equation-number">(14)</span>
          </div>
        </div>where <em>s<sub>i</sub></em> is a real-valued score
        assigned to <em>i</em> and will be inferred.
        <em>s<sub>i</sub></em> can be treated as the rank of
        <em>i</em>; for example, if <em>s<sub>i</sub></em> &gt;
        <em>s<sub>j</sub></em> , then
        <em>P</em>(<em>i</em>≻<em>j</em>) &gt; 0.5. This is
        essentially the same as the probability of a link
        <em>p</em>(<em>e<sub>ji</sub></em> = 1) in a special case
        of our model, where <em>λ<sub>R</sub></em> = 1,
        <em>λ<sub>Z</sub></em> = <em>λ</em> <sub>0</sub> = 0 and
        <em>h</em>(<em>dz</em>) = 1 (constant).
        <p></p>
      </section>
    </section>
    <section id="sec-16">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span>
          Experiments</h2>
        </div>
      </header>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Datasets</h3>
          </div>
        </header>
        <p>We use the following real-world datasets from different
        domains to test our new embedding algorithm:</p>
        <ul class="list-no-style">
          <li id="list7" label="•">Snapchat Friendship. Snapchat is
          a US based ephemeral photo-messaging application
          developed by camera company Snap Inc. Users can make
          bi-directional friend links with others. We extract all
          friendship relations from a relatively small and isolated
          country, resulting in a network with about 1.5 million
          total nodes (users) and about 66 millions edges
          (bi-directional friendship links).<br /></li>
          <li id="list8" label="•">Tencent Weibo Retweet
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0048">48</a>]. Tencent weibo is a Chinese
            microblogging service where users can post tweets and
            follow/retweet from others. We extract the complete
            retweet relationships on November 1st, 2011. Tweets
            that have been retweeted for less than 5 times are
            excluded. A directed edge (<em>u<sub>A</sub></em> ,
            <em>u<sub>B</sub></em> ) is added when a user
            <em>u<sub>A</sub></em> retweets from user
            <em>u<sub>B</sub></em> .<br />
          </li>
          <li id="list9" label="•">Venue Citation: We extract the
          paper citation links in the computer science domain and
          build a venue citations network from the Microsoft
          Academic Graph data [<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0036">36</a>]. A link from
            <em>venue<sub>A</sub></em> to
            <em>venue<sub>B</sub></em> indicates a citation from a
            paper published in <em>venue<sub>A</sub></em> to a
            paper published in <em>venue<sub>B</sub></em> . We do
            not differentiate each proceeding of the venues. Links
            are aggregated over a 10-year period of time
            (2007-2016) and it naturally becomes a weighted graph.
            Venues are manually labeled according to their field of
            study, and we look into the following eight categories:
            AI (artificial intelligence and machine learning), NET
            (network), SE (software engineering), CT (computer
            theory), CV (computer vision and graphics), DB
            (database), PL (programming languages) and DM (data
            mining). Some venues may have multiple labels.<br />
          </li>
          <li id="list10" label="•">Wikipedia Hyperlink<a class=
          "fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. An edge
          from <em>i</em> to <em>j</em> represents a hyperlink from
          wikipage <em>i</em> to wikipage <em>j</em>. Wikipages and
          their categories are structured in a collaborative
          hierarchical framework, and the folksonomy information is
          further cleaned according to [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0007">7</a>]. We
            only keep wikipages with clear hierarchical labels in
            the previous step. Top six categories that contain most
            webpages are picked for our classification task:
            <em>sports</em>, <em>politics</em>, <em>science</em>,
            <em>Christian</em>, <em>geography</em> and
            <em>musician</em>. Some wikipages may have multiple
            labels. Edge multiplicity is not available for this
            dataset.<br />
          </li>
          <li id="list11" label="•">Wikipedia
            Clickstream<a class="fn" href="#fn2" id=
            "foot-fn2"><sup>2</sup></a>. This dataset contains
            counts of (referer, resource) pairs extracted from the
            HTTP request logs of Wikipedia during Jan. 2017, where
            people navigate from one wikipage (i.e. referer) to
            another (i.e. resource). Node labels are obtained in
            the same way as the Wikipedia Hyperlink dataset.<br />
          </li>
        </ul>
        <p>The details about the above datasets can be found in
        Table <a class="tbl" href="#tab1">1</a>.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Dataset Statistics.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">#Nodes</th>
                <th style="text-align:center;">#Edges</th>
                <th style="text-align:center;">Weighted?</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Snapchat
                Friendship</td>
                <td style="text-align:center;">1.5M</td>
                <td style="text-align:center;">66M</td>
                <td style="text-align:center;">No</td>
              </tr>
              <tr>
                <td style="text-align:center;">Tencent Weibo
                Retweet</td>
                <td style="text-align:center;">842K</td>
                <td style="text-align:center;">1.9M</td>
                <td style="text-align:center;">Yes</td>
              </tr>
              <tr>
                <td style="text-align:center;">Venue Citation</td>
                <td style="text-align:center;">1.2K</td>
                <td style="text-align:center;">91K</td>
                <td style="text-align:center;">Yes</td>
              </tr>
              <tr>
                <td style="text-align:center;">Wikipedia
                Hyperlink</td>
                <td style="text-align:center;">488K</td>
                <td style="text-align:center;">5.5M</td>
                <td style="text-align:center;">No</td>
              </tr>
              <tr>
                <td style="text-align:center;">Wikipedia
                Clickstream</td>
                <td style="text-align:center;">2.4M</td>
                <td style="text-align:center;">15M</td>
                <td style="text-align:center;">No</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In all experiments, 90% of the links are randomly
        sampled as the training dataset. Hyper-parameters for prior
        distribution (introduced in Section <a class="sec" href=
        "#sec-12">3.3</a>) are set to <em>k<sub>R</sub></em> = 1.5,
        <em>r<sub>min</sub></em> = 0.1 and <span class=
        "inline-equation"><span class="tex">$\sigma
        _Z^2=10^6$</span></span> . We do not observe significance
        in terms of the evaluation metrics for different settings
        of <em>k<sub>R</sub></em> ∈ [1, 2],
        <em>r<sub>min</sub></em> ≤ 0.2 and <span class=
        "inline-equation"><span class="tex">$\sigma _Z^2 \ge
        10^4$</span></span> .</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Applications</h3>
          </div>
        </header>
        <section id="sec-19">
          <p><em>4.2.1 Classification.</em> We demonstrate the
          advantage of our embedding in terms of multi-label
          classification results. We compare with the following
          baseline results.</p>
          <ul class="list-no-style">
            <li id="list12" label="•">Matrix factorization
            techniques for recommender systems (MF) [<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0022">22</a>].
              Although this method is designed for recommender
              systems, we apply their method on the user-user
              affinity matrix and treat the low dimensional feature
              for users as embedding. Note that, the popularity of
              a user is explicitly modeled by an additional bias
              factor.<br />
            </li>
            <li id="list13" label="•">Graph Factorization (GF)
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0001">1</a>]. This is another matrix
            factorization based approach that factorizes the graph
            adjacency matrix in order to obtain the low dimensional
            vector representation for nodes.<br />
            </li>
            <li id="list14" label="•">Large-scale information
            network embedding (LINE) [<a class="bib"
              data-trigger="hover" data-toggle="popover"
              data-placement="top" href="#BibPLXBIB0040">40</a>].
              It embeds large information networks into lower
              dimensional vector spaces. The authors propose two
              measures of node proximity (i.e. 1st and 2nd order),
              and we will compare with both of them.<br />
            </li>
            <li id="list15" label="•">Node2vec [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0016">16</a>].
            This approach learns low-dimensional representations
            for nodes in a graph by optimizing a neighborhood
            preserving objective.<br />
            </li>
          </ul>
          <p>The lower dimensional feature vector for each entity
          is then used to predict its label. For fair comparison,
          we only use the proximity-based representation z as the
          feature in our model. We also try treating the
          concatenation of both <em>r</em> and z as the feature
          vector; however, since there is little correlation
          between rank <em>r</em> and label (e.g. the popularity of
          a person does not indicate her occupation), we report the
          performance using z only. The state-of-the-art
          Conditional Bernoulli Mixtures (CBM) model [<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0025">25</a>] is
          adopted as the multi-label classifier, where the authors
          kindly make their code available online<a class="fn"
          href="#fn3" id="foot-fn3"><sup>3</sup></a>. 90% of the
          nodes (labels as well as their vector representations)
          are randomly sampled for training. We use the following
          metrics for evaluation (denoting <em>Pred</em> as the set
          of predicted labels, and <em>True</em> as the set of
          ground truth labels for each entity):</p>
          <ul class="list-no-style">
            <li id="list16" label="•">Jaccard Index: the number of
            correctly predicted labels divided by the union of
            predicted and true labels: <span class=
            "inline-equation"><span class="tex">$J(Pred, True) =
            \frac{|Pred~ \cap ~True|}{|Pred~ \cup
            ~True|}$</span></span> . Larger values indicate better
            performance.<br /></li>
            <li id="list17" label="•">Hamming Loss: the fraction of
            the wrong labels to the total number of labels:
            <span class="inline-equation"><span class=
            "tex">$\frac{1}{L} \sum _{i=1}^{L} xor(Pred_i,
            True_i)$</span></span> , where <em>L</em> is the number
            of total labels. Smaller values indicate better
            performance.<br /></li>
            <li id="list18" label="•">F1 score: the harmonic mean
            of precision <span class="inline-equation"><span class=
            "tex">$\frac{|Pred~ \cap ~True|}{|Pred|}$</span></span>
            and recall <span class="inline-equation"><span class=
            "tex">$\frac{|Pred~ \cap ~True|}{|True|}$</span></span>
            . Larger values indicate better performance.<br /></li>
          </ul>
          <p>The classification results (average of the above
          metrics for each entity) are shown in Figures <a class=
          "fig" href="#fig4">4</a>-<a class="fig" href=
          "#fig6">6</a>. We only evaluate datasets where ground
          truth user labels are available (i.e., Venue Citation,
          Wikipedia Hyperlink and Wikipedia Clickstream).</p>
          <figure id="fig4">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig4.jpg"
            class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span>
              <span class="figure-title">Multilabel classification
              results on Venue Citation dataset.</span>
            </div>
          </figure>
          <figure id="fig5">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig5.jpg"
            class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span>
              <span class="figure-title">Multilabel classification
              results on Wikipedia Hyperlink dataset.</span>
            </div>
          </figure>
          <figure id="fig6">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig6.jpg"
            class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span>
              <span class="figure-title">Multilabel classification
              results on Wikipedia Clickstream dataset.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-20">
          <p><em>4.2.2 Link Prediction.</em> Although not all
          baseline methods explicitly mention the application in
          link prediction, they all assign a probability score to
          every pair of nodes, which can be sorted and evaluated
          using the area under the ROC curve (AUC) score.
          Specifically, 10% of the existing edges and non-existing
          edges are hidden from the training set, and their
          probabilities are examined by the model. For methods
          designed for undirected networks, the probability of a
          directed link <em>u<sub>i</sub></em> →
          <em>u<sub>j</sub></em> is simply regarded as that of the
          undirected dyad (<em>u<sub>i</sub></em> ,
          <em>u<sub>j</sub></em> ). The evaluation results are
          reported in Table <a class="tbl" href="#tab2">2</a>.</p>
          <p>Particularly, a significant improvement over the
          baseline methods is observed on Snapchat and Weibo
          dataset, as users tend to interact with others
          (especially celebrities) due to their popularity instead
          of proximity, which is never captured by most baseline
          methods. This observation agrees with our intuition. Note
          that by using our embedding algorithm (RaRE), a
          2-dimensional proximity-based embedding can already beat
          much higher-dimensional embedding (e.g., <em>K</em> = 32)
          for all the other baselines in almost all the
          datasets.</p>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span>
              <span class="table-title">Link prediction AUC (%) on
              all datasets.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th colspan="2" style="text-align:left;"></th>
                  <th colspan="5" style="text-align:center;">
                    Dimension of embedding <em>K</em>
                    <hr />
                  </th>
                </tr>
                <tr>
                  <th style="text-align:center;"></th>
                  <th style="text-align:center;"></th>
                  <th style="text-align:center;">2</th>
                  <th style="text-align:center;">4</th>
                  <th style="text-align:center;">8</th>
                  <th style="text-align:center;">16</th>
                  <th style="text-align:center;">32</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">Snapchat</td>
                  <td style="text-align:center;">MF</td>
                  <td style="text-align:center;">54.0</td>
                  <td style="text-align:center;">54.2</td>
                  <td style="text-align:center;">54.3</td>
                  <td style="text-align:center;">54.3</td>
                  <td style="text-align:center;">54.3</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">GF</td>
                  <td style="text-align:center;">63.1</td>
                  <td style="text-align:center;">66.7</td>
                  <td style="text-align:center;">69.5</td>
                  <td style="text-align:center;">72.1</td>
                  <td style="text-align:center;">73.5</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-1st</td>
                  <td style="text-align:center;">54.6</td>
                  <td style="text-align:center;">57.8</td>
                  <td style="text-align:center;">59.1</td>
                  <td style="text-align:center;">60.3</td>
                  <td style="text-align:center;">58.8</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-2nd</td>
                  <td style="text-align:center;">55.8</td>
                  <td style="text-align:center;">56.0</td>
                  <td style="text-align:center;">56.0</td>
                  <td style="text-align:center;">56.0</td>
                  <td style="text-align:center;">56.0</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">Node2vec</td>
                  <td style="text-align:center;">57.3</td>
                  <td style="text-align:center;">56.9</td>
                  <td style="text-align:center;">56.1</td>
                  <td style="text-align:center;">53.9</td>
                  <td style="text-align:center;">65.7</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RaRE</td>
                  <td style="text-align:center;">
                  <strong>86.7</strong></td>
                  <td style="text-align:center;">
                  <strong>92.1</strong></td>
                  <td style="text-align:center;">
                  <strong>94.0</strong></td>
                  <td style="text-align:center;">
                  <strong>94.8</strong></td>
                  <td style="text-align:center;">
                  <strong>94.6</strong></td>
                </tr>
                <tr>
                  <td style="text-align:center;">Tencent<br />
                  Weibo</td>
                  <td style="text-align:center;">MF</td>
                  <td style="text-align:center;">92.7</td>
                  <td style="text-align:center;">92.9</td>
                  <td style="text-align:center;">93.0</td>
                  <td style="text-align:center;">
                  <strong>93.1</strong></td>
                  <td style="text-align:center;">93.1</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">GF</td>
                  <td style="text-align:center;">71.7</td>
                  <td style="text-align:center;">76.9</td>
                  <td style="text-align:center;">76.7</td>
                  <td style="text-align:center;">76.3</td>
                  <td style="text-align:center;">76.8</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-1st</td>
                  <td style="text-align:center;">62.3</td>
                  <td style="text-align:center;">64.5</td>
                  <td style="text-align:center;">70.9</td>
                  <td style="text-align:center;">74.7</td>
                  <td style="text-align:center;">75.8</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-2nd</td>
                  <td style="text-align:center;">57.1</td>
                  <td style="text-align:center;">60.4</td>
                  <td style="text-align:center;">60.2</td>
                  <td style="text-align:center;">60.9</td>
                  <td style="text-align:center;">61.6</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">Node2vec</td>
                  <td style="text-align:center;">68.3</td>
                  <td style="text-align:center;">71.0</td>
                  <td style="text-align:center;">71.9</td>
                  <td style="text-align:center;">72.3</td>
                  <td style="text-align:center;">72.5</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RaRE</td>
                  <td style="text-align:center;">
                  <strong>95.3</strong></td>
                  <td style="text-align:center;">
                  <strong>95.2</strong></td>
                  <td style="text-align:center;">
                  <strong>94.2</strong></td>
                  <td style="text-align:center;">
                  <strong>93.1</strong></td>
                  <td style="text-align:center;">
                  <strong>96.6</strong></td>
                </tr>
                <tr>
                  <td style="text-align:center;">Venue<br />
                  Citation</td>
                  <td style="text-align:center;">MF</td>
                  <td style="text-align:center;">85.5</td>
                  <td style="text-align:center;">90.2</td>
                  <td style="text-align:center;">92.3</td>
                  <td style="text-align:center;">91.8</td>
                  <td style="text-align:center;">91.6</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">GF</td>
                  <td style="text-align:center;">78.0</td>
                  <td style="text-align:center;">87.1</td>
                  <td style="text-align:center;">92.6</td>
                  <td style="text-align:center;">93.7</td>
                  <td style="text-align:center;">
                  <strong>94.4</strong></td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-1st</td>
                  <td style="text-align:center;">55.0</td>
                  <td style="text-align:center;">56.4</td>
                  <td style="text-align:center;">63.0</td>
                  <td style="text-align:center;">79.8</td>
                  <td style="text-align:center;">80.0</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-2nd</td>
                  <td style="text-align:center;">64.4</td>
                  <td style="text-align:center;">74.2</td>
                  <td style="text-align:center;">80.0</td>
                  <td style="text-align:center;">81.2</td>
                  <td style="text-align:center;">81.5</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">Node2vec</td>
                  <td style="text-align:center;">81.0</td>
                  <td style="text-align:center;">85.3</td>
                  <td style="text-align:center;">89.4</td>
                  <td style="text-align:center;">90.9</td>
                  <td style="text-align:center;">91.2</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RaRE</td>
                  <td style="text-align:center;">
                  <strong>91.4</strong></td>
                  <td style="text-align:center;">
                  <strong>93.7</strong></td>
                  <td style="text-align:center;">
                  <strong>94.0</strong></td>
                  <td style="text-align:center;">
                  <strong>94.3</strong></td>
                  <td style="text-align:center;">94.2</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Wikipedia<br />
                  Hyperlink</td>
                  <td style="text-align:center;">MF</td>
                  <td style="text-align:center;">84.9</td>
                  <td style="text-align:center;">87.8</td>
                  <td style="text-align:center;">89.6</td>
                  <td style="text-align:center;">90.8</td>
                  <td style="text-align:center;">91.5</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">GF</td>
                  <td style="text-align:center;">80.4</td>
                  <td style="text-align:center;">88.7</td>
                  <td style="text-align:center;">93.5</td>
                  <td style="text-align:center;">95.6</td>
                  <td style="text-align:center;">96.6</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-1st</td>
                  <td style="text-align:center;">52.8</td>
                  <td style="text-align:center;">55.8</td>
                  <td style="text-align:center;">63.7</td>
                  <td style="text-align:center;">69.6</td>
                  <td style="text-align:center;">77.7</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-2nd</td>
                  <td style="text-align:center;">50.0</td>
                  <td style="text-align:center;">50.1</td>
                  <td style="text-align:center;">50.2</td>
                  <td style="text-align:center;">50.7</td>
                  <td style="text-align:center;">51.7</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">Node2vec</td>
                  <td style="text-align:center;">77.2</td>
                  <td style="text-align:center;">84.9</td>
                  <td style="text-align:center;">88.7</td>
                  <td style="text-align:center;">89.1</td>
                  <td style="text-align:center;">89.4</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RaRE</td>
                  <td style="text-align:center;">
                  <strong>97.5</strong></td>
                  <td style="text-align:center;">
                  <strong>97.6</strong></td>
                  <td style="text-align:center;">
                  <strong>97.7</strong></td>
                  <td style="text-align:center;">
                  <strong>97.8</strong></td>
                  <td style="text-align:center;">
                  <strong>97.8</strong></td>
                </tr>
                <tr>
                  <td style="text-align:center;">Wikipedia<br />
                  Clickstream</td>
                  <td style="text-align:center;">MF</td>
                  <td style="text-align:center;">63.4</td>
                  <td style="text-align:center;">68.6</td>
                  <td style="text-align:center;">72.1</td>
                  <td style="text-align:center;">74.5</td>
                  <td style="text-align:center;">76.9</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">GF</td>
                  <td style="text-align:center;">76.1</td>
                  <td style="text-align:center;">82.5</td>
                  <td style="text-align:center;">86.1</td>
                  <td style="text-align:center;">86.7</td>
                  <td style="text-align:center;">86.7</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-1st</td>
                  <td style="text-align:center;">73.8</td>
                  <td style="text-align:center;">78.1</td>
                  <td style="text-align:center;">78.6</td>
                  <td style="text-align:center;">78.8</td>
                  <td style="text-align:center;">78.8</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">LINE-2nd</td>
                  <td style="text-align:center;">69.3</td>
                  <td style="text-align:center;">71.2</td>
                  <td style="text-align:center;">72.4</td>
                  <td style="text-align:center;">73.0</td>
                  <td style="text-align:center;">73.5</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">Node2vec</td>
                  <td style="text-align:center;">82.9</td>
                  <td style="text-align:center;">87.2</td>
                  <td style="text-align:center;">88.1</td>
                  <td style="text-align:center;">89.6</td>
                  <td style="text-align:center;">89.0</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">RaRE</td>
                  <td style="text-align:center;">
                  <strong>90.7</strong></td>
                  <td style="text-align:center;">
                  <strong>94.4</strong></td>
                  <td style="text-align:center;">
                  <strong>94.7</strong></td>
                  <td style="text-align:center;">
                  <strong>94.3</strong></td>
                  <td style="text-align:center;">
                  <strong>93.7</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-21">
          <p><em>4.2.3 Embedding as Additional Features for
          Classification.</em> The lower dimensional feature vector
          for each entity can also serve as additional features for
          real-world applications. For example, in Snapchat, the
          gender information is not required at the time of
          registration, however, knowledge of gender is crucial for
          better user understanding, ADs targeting and content
          recommendation. Currently, gender information is
          predicted by a Gradient Boosted Decision Trees model with
          a few highly engineered features, which already has an
          accuracy of 93.5% (the true gender information is
          obtained from Bitmoji user avatar). The features are
          derived from first name of a user, country, historical
          usage behavior of the Snapchat app products such as Lens,
          User Story, Discover etc. In this classification task, we
          collected 258,014 labeled examples, and we split it into
          training set and test set using 0.7 to 0.3 ratio. We
          concatenate the learned embedding vector as additional
          features to the basic features we used for gender
          prediction, and report the accuracy in Table <a class=
          "tbl" href="#tab3">3</a> (accuracy = 1.0 - error rate).
          Given the high accuracy of the baseline model (93.5%),
          absolute accuracy lift of greater than 1% is considered
          very challenging and significant. As we can see from
          Table <a class="tbl" href="#tab3">3</a> that adding
          embedding vectors produced by our proposed method RaRE
          significantly outperforms other baselines for the gender
          prediction task. For example, when using 32 dimensional
          embeddings trained by RaRE, we observed 1.5% lift on the
          gender prediction accuracy over the current production
          model. The results show that the embedding information of
          a user carries useful signals for predicting the basic
          profile of a user such as gender.</p>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span>
              <span class="table-title">Gender prediction accuracy
              (%) on Snapchat dataset.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Method</th>
                  <th style="text-align:center;"><em>K</em> =
                  2</th>
                  <th style="text-align:center;"><em>K</em> =
                  4</th>
                  <th style="text-align:center;"><em>K</em> =
                  8</th>
                  <th style="text-align:center;"><em>K</em> =
                  16</th>
                  <th style="text-align:center;"><em>K</em> =
                  32</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">MF</td>
                  <td style="text-align:center;">93.3</td>
                  <td style="text-align:center;">93.4</td>
                  <td style="text-align:center;">93.5</td>
                  <td style="text-align:center;">93.7</td>
                  <td style="text-align:center;">94.0</td>
                </tr>
                <tr>
                  <td style="text-align:center;">GF</td>
                  <td style="text-align:center;">93.6</td>
                  <td style="text-align:center;">93.6</td>
                  <td style="text-align:center;">93.8</td>
                  <td style="text-align:center;">94.0</td>
                  <td style="text-align:center;">94.3</td>
                </tr>
                <tr>
                  <td style="text-align:center;">LINE-1st</td>
                  <td style="text-align:center;">93.5</td>
                  <td style="text-align:center;">93.5</td>
                  <td style="text-align:center;">93.6</td>
                  <td style="text-align:center;">93.7</td>
                  <td style="text-align:center;">93.7</td>
                </tr>
                <tr>
                  <td style="text-align:center;">LINE-2nd</td>
                  <td style="text-align:center;">93.5</td>
                  <td style="text-align:center;">93.5</td>
                  <td style="text-align:center;">93.7</td>
                  <td style="text-align:center;">93.7</td>
                  <td style="text-align:center;">93.8</td>
                </tr>
                <tr>
                  <td style="text-align:center;">Node2vec</td>
                  <td style="text-align:center;">94.2</td>
                  <td style="text-align:center;">94.2</td>
                  <td style="text-align:center;">94.3</td>
                  <td style="text-align:center;">94.3</td>
                  <td style="text-align:center;">94.5</td>
                </tr>
                <tr>
                  <td style="text-align:center;">RaRE</td>
                  <td style="text-align:center;">
                  <strong>94.5</strong></td>
                  <td style="text-align:center;">
                  <strong>94.6</strong></td>
                  <td style="text-align:center;">
                  <strong>94.7</strong></td>
                  <td style="text-align:center;">
                  <strong>94.9</strong></td>
                  <td style="text-align:center;">
                  <strong>95.0</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-22">
          <p><em>4.2.4 A Novel Polar Coordinate-based
          Visualization.</em> Visualization is another way to
          demonstrate the effectiveness of learned representation.
          A good embedding algorithm should be able to distinguish
          nodes of different labels by separating them in the
          vector representation space. Conventionally, a 2D or 3D
          vector representation is learned for each individual,
          which is treated as his/her coordinates and thus can be
          displayed in a scatter plot. We list the results of a few
          visualization methods in Figure <a class="fig" href=
          "#fig7">7</a>, and it is very clear that the
          proximity-based embedding of RaRE does well in detecting
          different computer science research communities.</p>
          <p>It is also interesting to reveal the visualization of
          the nodes by combining social rank and proximity-based
          representation in a unified plot. While many of the
          visualization approaches are capable of capturing much of
          the local structure (e.g. neighbors) as well as the
          global structure (e.g. clusters), they fail to identify
          the influential entities in the plot. In our method, we
          depict the coordinate of a node using the polar system,
          where the radius is simply its social rank <em>r</em>,
          and the angle <em>θ</em> is obtained from its
          proximity-based representation z by a simple
          transformation:</p>
          <div class="table-responsive" id="Xeq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \begin{aligned} \lbrace \theta \rbrace ^{*} =
              \mathop{argmin}\limits _{\theta } \sum _{(i,j) \in
              E_Z} \log \big (1 + e^{-\cos (\theta _i - \theta _j)}
              \big) + \sum _{(i,j) \in \hat{E_Z}} \log \big (1 +
              e^{\cos (\theta _i - \theta _j)} \big) \end{aligned}
              \end{equation}</span><br />
              <span class="equation-number">(15)</span>
            </div>
          </div>where cos (<em>θ<sub>i</sub></em> −
          <em>θ<sub>j</sub></em> ) reflects the similarity on the
          2D sphere, <em>E<sub>Z</sub></em> is the new set of edges
          defined in the space of proximity-based representation:
          <em>E<sub>Z</sub></em> = {(<em>i</em>, <em>j</em>): ~||z
          <sub><em>i</em></sub> − z <sub><em>j</em></sub>
          ||<sup>2</sup> &lt; <em>t</em>} and <span class=
          "inline-equation"><span class=
          "tex">$\hat{E_Z}$</span></span> is the corresponding
          non-existing pairs of nodes sampled using the same
          strategy as Section <a class="sec" href=
          "#sec-13">3.4</a>. The equation above can be considered
          as preserving the proximity in both spaces (minimizing
          the logit loss), as similar to various dimension
          reduction approaches. Here we pick <em>t</em> = 0.5 and
          <span class="inline-equation"><span class="tex">$ {z} \in
          \mathbb {R}^K$</span></span> where <em>K</em> = 2, and we
          do not observe significant variance in terms of these
          parameters.
          <p></p>
          <p>In the bottom right figure of Figure <a class="fig"
          href="#fig7">7</a> (polar coordinates from RaRE), we can
          clearly observe the most influential venues around the
          center, among which top conferences in different areas
          (e.g. CHI, WWW, ICSE, CVPR, SIGGRAPH, SIGMOD, INFOCOM,
          AAAI, KDD, VLDB, ICML, STOC) are successfully
          identified.</p>
          <figure id="fig7">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186102/images/www2018-111-fig7.jpg"
            class="img-responsive" alt="Figure 7" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 7:</span>
              <span class="figure-title">Visualization on Venue
              Citation dataset. These plots are best viewed in
              color<a class="fn" href="#fn4" id=
              "foot-fn4"><sup>4</sup></a>.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-23">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Related
          Work</h2>
        </div>
      </header>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span>
            Ranking</h3>
          </div>
        </header>
        <p>Mapping entities in the network to a spectrum of
        importance scores has been a very popular research topic
        for decades. The notion of influential nodes emerged from
        the large-scale World Wide Web (WWW), where it is vital for
        crawlers to start from important pages first [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0015">15</a>], or
        generally, an objective method is desired to measure
        human's interest in a collections of webpages. Various
        algorithms have been proposed, e.g. PageRank [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0032">32</a>], HITS
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0021">21</a>], and
        have been widely generalized towards various needs
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0011">11</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0017">17</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0027">27</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0045">45</a>]. These
        algorithms have an assumption in common: a random surfer is
        assumed to browse the web and click hyperlinks randomly,
        and the probability distribution of a webpage being visited
        will converge to a score related to its rank. Similar ideas
        are also applicable to information networks by defining the
        weights between entities [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0045">45</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0046">46</a>]. In addition to random walk-based
        approaches, many ranking methods are proposed based on
        Bayesian network and inference. Pal and Counts [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0033">33</a>] generate
        a list of features for microblogs based on their followers,
        number of hashtags and so on. Clusters are then revealed
        using a Gaussian mixture model, and the rank of a microblog
        is an aggregation of the rank of its features. Ball and
        Newman [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0002">2</a>]
        study several friendship networks, and find that
        unreciprocated friendships often consist of a lower-ranked
        individual claiming friendship with a higher-ranked one.
        Based on this assumption, they deduce such social rankings
        using maximum likelihood estimation.</p>
        <p>All the above approaches provide a global ranking for
        each entity, however, the ranking makes little sense when
        entities in several categories are mingled together without
        distinguishing the clustering information. For example, we
        seldom mention comparisons such as “a computer system
        conference is ranked higher than a database conference”.
        For heterogeneous networks where multiple types of entities
        or relations may exist, determining the network becomes
        even tricker and multiple ranking systems may occur
        according to different schemas or topics. Sun et al.
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0037">37</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0038">38</a>]
        associate heterogeneous network clustering and ranking
        together, and assume several rank distributions conditional
        on different cluster structures. Liu et al. [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0026">26</a>] propose a probabilistic
        generative model, which explains the network generation
        process from users and documents and is able to reveal the
        most related nodes with a given topic (query). However,
        additional information other than cluster labels is usually
        desired to understand the network. Our method, on the other
        hand, provides the lower dimensional proximity-based
        representation for each entity, which has wide applications
        including classification, clustering, visualization and so
        on.</p>
      </section>
      <section id="sec-25">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Network
            Embedding</h3>
          </div>
        </header>
        <p>Detecting latent representation (embedding) of nodes in
        a network is essential in understanding the opinions of
        individuals and desired for various machine learning tasks.
        Traditional approaches usually utilize the adjacency matrix
        in order to extract essential dimensions of the data
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0005">5</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0023">23</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0035">35</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0041">41</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0047">47</a>], which
        involve finding the eigenvalues of a matrix and thus not
        scalable for large networks. Matrix factorization finds an
        approximation of a matrix by the product of two lower rank
        matrices, and this technique has been popular especially in
        recommender systems [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0019">19</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>]. In terms of graph data, matrix
        factorization can be applied on the affinity matrix
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0001">1</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0030">30</a>], and
        each row of the low dimensional matrix naturally becomes
        the vector representation of the corresponding node.</p>
        <p>More recent approaches introduce embedding for nodes in
        a network, which is a low-dimensional vector that
        represents the latent characteristics of a node. These
        embedding vectors are learned by preserving similarity in
        the network and similarity in the latent Euclidean space.
        The notion of embedding originates from word embedding
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0031">31</a>], and
        Levy and Goldberg [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0024">24</a>] establishes the connection between
        matrix factorization and word embedding, arguing that
        estimating word embedding is equivalent to factorizing a
        pointwise mutual information matrix. Later on, researchers
        have discovered strategies to explain the generation of
        links from a probabilistic perspective, with the assumption
        that the likelihood of a link should be proportional to the
        similarity of both nodes (neighbors) [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0001">1</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0028">28</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0030">30</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0042">42</a>] or entities [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0008">8</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0009">9</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0014">14</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0044">44</a>]. More
        recent approaches generalize the notion of similar nodes to
        <em>n</em>-hop neighbors [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0034">34</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0040">40</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0043">43</a>]. Generally, links are assumed to
        be explained as the proximity between the representation of
        two actors (i.e. the “homophily” assumption [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0029">29</a>]). However, we often
        observe links to highly-ranked nodes (e.g. many users
        follow <em>celebrities</em> on Twitter, and scholars tend
        to cite <em>popular</em> works and authors in a
        bibliographic network). As a result, some nodes are poorly
        modeled by the homophily assumption. Embedding-based
        approaches ignore this seminal factor in link generation,
        which may lead to inaccurate estimation as a result. Some
        matrix factorization methods consider the popularity factor
        by introducing a bias term [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>], but they model these two factors
        independently, while neglecting the fact that the knowledge
        of one can affect the distribution of the other.</p>
      </section>
    </section>
    <section id="sec-26">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper we present a novel approach for information
      network embedding with consideration of individuals’ social
      ranks. From the graph generation perspective, we refine the
      latent representation of nodes on information network by
      analyzing the role of individuals in terms of their social
      rank. Moreover, we provide solid derivations on the reason
      behind a link in terms of both latent proximity-based
      representation as well as social rank of a node, which
      provides a brand new insight of the problem. We carefully
      design a framework that explicitly models the interdependency
      between these two types of embeddings. Finally, we evaluate
      our model on several real-world large-scale datasets, and the
      results on classification, link prediction and visualization
      demonstrate our advantage over the state-of-the-art network
      embedding methods.</p>
    </section>
    <section id="sec-27">
      <header>
        <div class="title-info">
          <h2>Acknowledgement</h2>
        </div>
      </header>
      <p>We would like to thank Snapchat Inc. for its gift funding.
      We would also like to share our gratitude to the anonymous
      reviewers for their precious comments. This work is partially
      supported by NSF CAREER Award #1741634 and #1453800.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">A.&nbsp;Ahmed,
        N.&nbsp;Shervashidze, S.&nbsp;Narayanamurthy,
        V.&nbsp;Josifovski, and A.&nbsp;J. Smola. Distributed
        large-scale natural graph factorization. In <em>Proceedings
        of the 22nd international conference on World Wide
        Web</em>, pages 37–48. ACM, 2013.</li>
        <li id="BibPLXBIB0002" label="[2]">B.&nbsp;Ball and
        M.&nbsp;E. Newman. Friendship networks and social status.
        <em>Network Science</em>, 1(01):16–30, 2013.</li>
        <li id="BibPLXBIB0003" label="[3]">A.&nbsp;Balmin,
        V.&nbsp;Hristidis, and Y.&nbsp;Papakonstantinou.
        Objectrank: Authority-based keyword search in databases. In
        <em>Proceedings of the Thirtieth international conference
        on Very large data bases-Volume 30</em>, pages 564–575.
        VLDB Endowment, 2004.</li>
        <li id="BibPLXBIB0004" label="[4]">A.-L. Barabási and
        R.&nbsp;Albert. Emergence of scaling in random networks.
        <em>science</em>, 286(5439):509–512, 1999.</li>
        <li id="BibPLXBIB0005" label="[5]">M.&nbsp;Belkin and
        P.&nbsp;Niyogi. Laplacian eigenmaps and spectral techniques
        for embedding and clustering. In <em>Advances in neural
        information processing systems</em>, pages 585–591,
        2002.</li>
        <li id="BibPLXBIB0006" label="[6]">R.&nbsp;M. Bell,
        Y.&nbsp;Koren, and C.&nbsp;Volinsky. The bellkor 2008
        solution to the netflix prize. <em>Statistics Research
        Department at AT&amp;T Research</em>, 2008.</li>
        <li id="BibPLXBIB0007" label="[7]">P.&nbsp;Boldi and
        C.&nbsp;Monti. Cleansing wikipedia categories using
        centrality. In <em>Proceedings of the 25th International
        Conference Companion on World Wide Web</em>, pages 969–974.
        International World Wide Web Conferences Steering
        Committee, 2016.</li>
        <li id="BibPLXBIB0008" label="[8]">A.&nbsp;Bordes,
        N.&nbsp;Usunier, A.&nbsp;Garcia-Duran, J.&nbsp;Weston, and
        O.&nbsp;Yakhnenko. Translating embeddings for modeling
        multi-relational data. In <em>Advances in neural
        information processing systems</em>, pages 2787–2795,
        2013.</li>
        <li id="BibPLXBIB0009" label="[9]">A.&nbsp;Bordes,
        J.&nbsp;Weston, R.&nbsp;Collobert, Y.&nbsp;Bengio,
        et&nbsp;al. Learning structured embeddings of knowledge
        bases. In <em>AAAI</em>, volume&nbsp;6, page&nbsp;6,
        2011.</li>
        <li id="BibPLXBIB0010" label="[10]">R.&nbsp;A. Bradley and
        M.&nbsp;E. Terry. Rank analysis of incomplete block
        designs: I. the method of paired comparisons.
        <em>Biometrika</em>, 39(3/4):324–345, 1952.</li>
        <li id="BibPLXBIB0011" label="[11]">U.&nbsp;Brandes and
        S.&nbsp;Cornelsen. Visual ranking of link structures.
        <em>J. Graph Algorithms Appl.</em>, 7(2):181–201,
        2003.</li>
        <li id="BibPLXBIB0012" label="[12]">C.&nbsp;Burges,
        T.&nbsp;Shaked, E.&nbsp;Renshaw, A.&nbsp;Lazier,
        M.&nbsp;Deeds, N.&nbsp;Hamilton, and G.&nbsp;Hullender.
        Learning to rank using gradient descent. In <em>Proceedings
        of the 22nd international conference on Machine
        learning</em>, pages 89–96. ACM, 2005.</li>
        <li id="BibPLXBIB0013" label="[13]">S.&nbsp;Cao,
        W.&nbsp;Lu, and Q.&nbsp;Xu. Grarep: Learning graph
        representations with global structural information. In
        <em>Proceedings of the 24th ACM International on Conference
        on Information and Knowledge Management</em>, pages
        891–900. ACM, 2015.</li>
        <li id="BibPLXBIB0014" label="[14]">S.&nbsp;Chang,
        W.&nbsp;Han, J.&nbsp;Tang, G.-J. Qi, C.&nbsp;C. Aggarwal,
        and T.&nbsp;S. Huang. Heterogeneous network embedding via
        deep architectures. In <em>Proceedings of the 21th ACM
        SIGKDD International Conference on Knowledge Discovery and
        Data Mining</em>, pages 119–128. ACM, 2015.</li>
        <li id="BibPLXBIB0015" label="[15]">J.&nbsp;Cho,
        H.&nbsp;Garcia-Molina, and L.&nbsp;Page. Efficient crawling
        through url ordering. <em>Computer Networks and ISDN
        Systems</em>, 30(1):161–172, 1998.</li>
        <li id="BibPLXBIB0016" label="[16]">A.&nbsp;Grover and
        J.&nbsp;Leskovec. node2vec: Scalable feature learning for
        networks. In <em>Proceedings of the 22nd ACM SIGKDD
        International Conference on Knowledge Discovery and Data
        Mining</em>, pages 855–864. ACM, 2016.</li>
        <li id="BibPLXBIB0017" label="[17]">T.&nbsp;H. Haveliwala.
        Topic-sensitive pagerank. In <em>Proceedings of the 11th
        international conference on World Wide Web</em>, pages
        517–526. ACM, 2002.</li>
        <li id="BibPLXBIB0018" label="[18]">P.&nbsp;D. Hoff,
        A.&nbsp;E. Raftery, and M.&nbsp;S. Handcock. Latent space
        approaches to social network analysis. <em>Journal of the
        american Statistical association</em>, 97(460):1090–1098,
        2002.</li>
        <li id="BibPLXBIB0019" label="[19]">Y.&nbsp;Hu,
        Y.&nbsp;Koren, and C.&nbsp;Volinsky. Collaborative
        filtering for implicit feedback datasets. In <em>Data
        Mining, 2008. ICDM’08. Eighth IEEE International Conference
        on</em>, pages 263–272. Ieee, 2008.</li>
        <li id="BibPLXBIB0020" label="[20]">C.&nbsp;C. Johnson.
        Logistic matrix factorization for implicit feedback data.
        <em>Advances in Neural Information Processing Systems</em>,
        27, 2014.</li>
        <li id="BibPLXBIB0021" label="[21]">J.&nbsp;M. Kleinberg.
        Authoritative sources in a hyperlinked environment.
        <em>Journal of the ACM (JACM)</em>, 46(5):604–632,
        1999.</li>
        <li id="BibPLXBIB0022" label="[22]">Y.&nbsp;Koren,
        R.&nbsp;Bell, and C.&nbsp;Volinsky. Matrix factorization
        techniques for recommender systems. <em>Computer</em>,
        42(8):30–37, 2009.</li>
        <li id="BibPLXBIB0023" label="[23]">J.&nbsp;B. Kruskal and
        M.&nbsp;Wish. <em>Multidimensional scaling</em>,
        volume&nbsp;11. Sage, 1978.</li>
        <li id="BibPLXBIB0024" label="[24]">O.&nbsp;Levy and
        Y.&nbsp;Goldberg. Neural word embedding as implicit matrix
        factorization. In <em>Advances in neural information
        processing systems</em>, pages 2177–2185, 2014.</li>
        <li id="BibPLXBIB0025" label="[25]">C.&nbsp;Li,
        B.&nbsp;Wang, V.&nbsp;Pavlu, and J.&nbsp;Aslam. Conditional
        bernoulli mixtures for multi-label classification. In
        <em>Proceedings of The 33rd International Conference on
        Machine Learning</em>, pages 2482–2491, 2016.</li>
        <li id="BibPLXBIB0026" label="[26]">L.&nbsp;Liu,
        J.&nbsp;Tang, J.&nbsp;Han, M.&nbsp;Jiang, and S.&nbsp;Yang.
        Mining topic-level influence in heterogeneous networks. In
        <em>Proceedings of the 19th ACM international conference on
        Information and knowledge management</em>, pages 199–208.
        ACM, 2010.</li>
        <li id="BibPLXBIB0027" label="[27]">L.&nbsp;Lü, Y.-C.
        Zhang, C.&nbsp;H. Yeung, and T.&nbsp;Zhou. Leaders in
        social networks, the delicious case. <em>PloS one</em>,
        6(6):e21202, 2011.</li>
        <li id="BibPLXBIB0028" label="[28]">L.&nbsp;v.&nbsp;d.
        Maaten and G.&nbsp;Hinton. Visualizing data using t-sne.
        <em>Journal of Machine Learning Research</em>,
        9(Nov):2579–2605, 2008.</li>
        <li id="BibPLXBIB0029" label="[29]">M.&nbsp;McPherson,
        L.&nbsp;Smith-Lovin, and J.&nbsp;M. Cook. Birds of a
        feather: Homophily in social networks. <em>Annual review of
        sociology</em>, pages 415–444, 2001.</li>
        <li id="BibPLXBIB0030" label="[30]">A.&nbsp;Menon and
        C.&nbsp;Elkan. Link prediction via matrix factorization.
        <em>Machine Learning and Knowledge Discovery in
        Databases</em>, pages 437–452, 2011.</li>
        <li id="BibPLXBIB0031" label="[31]">T.&nbsp;Mikolov,
        I.&nbsp;Sutskever, K.&nbsp;Chen, G.&nbsp;S. Corrado, and
        J.&nbsp;Dean. Distributed representations of words and
        phrases and their compositionality. In <em>Advances in
        neural information processing systems</em>, pages
        3111–3119, 2013.</li>
        <li id="BibPLXBIB0032" label="[32]">L.&nbsp;Page,
        S.&nbsp;Brin, R.&nbsp;Motwani, and T.&nbsp;Winograd. The
        pagerank citation ranking: Bringing order to the web.
        Technical report, Stanford InfoLab, 1999.</li>
        <li id="BibPLXBIB0033" label="[33]">A.&nbsp;Pal and
        S.&nbsp;Counts. Identifying topical authorities in
        microblogs. In <em>Proceedings of the fourth ACM
        international conference on Web search and data
        mining</em>, pages 45–54. ACM, 2011.</li>
        <li id="BibPLXBIB0034" label="[34]">B.&nbsp;Perozzi,
        R.&nbsp;Al-Rfou, and S.&nbsp;Skiena. Deepwalk: Online
        learning of social representations. In <em>Proceedings of
        the 20th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em>, pages 701–710. ACM,
        2014.</li>
        <li id="BibPLXBIB0035" label="[35]">S.&nbsp;T. Roweis and
        L.&nbsp;K. Saul. Nonlinear dimensionality reduction by
        locally linear embedding. <em>science</em>,
        290(5500):2323–2326, 2000.</li>
        <li id="BibPLXBIB0036" label="[36]">A.&nbsp;Sinha,
        Z.&nbsp;Shen, Y.&nbsp;Song, H.&nbsp;Ma, D.&nbsp;Eide,
        B.-j.&nbsp;P. Hsu, and K.&nbsp;Wang. An overview of
        microsoft academic service (mas) and applications. In
        <em>Proceedings of the 24th international conference on
        world wide web</em>, pages 243–246. ACM, 2015.</li>
        <li id="BibPLXBIB0037" label="[37]">Y.&nbsp;Sun,
        J.&nbsp;Han, P.&nbsp;Zhao, Z.&nbsp;Yin, H.&nbsp;Cheng, and
        T.&nbsp;Wu. Rankclus: integrating clustering with ranking
        for heterogeneous information network analysis. In
        <em>Proceedings of the 12th International Conference on
        Extending Database Technology: Advances in Database
        Technology</em>, pages 565–576. ACM, 2009.</li>
        <li id="BibPLXBIB0038" label="[38]">Y.&nbsp;Sun,
        Y.&nbsp;Yu, and J.&nbsp;Han. Ranking-based clustering of
        heterogeneous information networks with star network
        schema. In <em>Proceedings of the 15th ACM SIGKDD
        international conference on Knowledge discovery and data
        mining</em>, pages 797–806. ACM, 2009.</li>
        <li id="BibPLXBIB0039" label="[39]">J.&nbsp;Tang,
        J.&nbsp;Liu, M.&nbsp;Zhang, and Q.&nbsp;Mei. Visualizing
        large-scale and high-dimensional data. In <em>Proceedings
        of the 25th International Conference on World Wide
        Web</em>, pages 287–297. International World Wide Web
        Conferences Steering Committee, 2016.</li>
        <li id="BibPLXBIB0040" label="[40]">J.&nbsp;Tang,
        M.&nbsp;Qu, M.&nbsp;Wang, M.&nbsp;Zhang, J.&nbsp;Yan, and
        Q.&nbsp;Mei. Line: Large-scale information network
        embedding. In <em>Proceedings of the 24th International
        Conference on World Wide Web</em>, pages 1067–1077. ACM,
        2015.</li>
        <li id="BibPLXBIB0041" label="[41]">J.&nbsp;B. Tenenbaum,
        V.&nbsp;De&nbsp;Silva, and J.&nbsp;C. Langford. A global
        geometric framework for nonlinear dimensionality reduction.
        <em>science</em>, 290(5500):2319–2323, 2000.</li>
        <li id="BibPLXBIB0042" label="[42]">L.&nbsp;Van
        Der&nbsp;Maaten. Accelerating t-sne using tree-based
        algorithms. <em>Journal of machine learning research</em>,
        15(1):3221–3245, 2014.</li>
        <li id="BibPLXBIB0043" label="[43]">D.&nbsp;Wang,
        P.&nbsp;Cui, and W.&nbsp;Zhu. Structural deep network
        embedding. In <em>Proceedings of the 22nd ACM SIGKDD
        international conference on Knowledge discovery and data
        mining</em>, pages 1225–1234. ACM, 2016.</li>
        <li id="BibPLXBIB0044" label="[44]">Z.&nbsp;Wang,
        J.&nbsp;Zhang, J.&nbsp;Feng, and Z.&nbsp;Chen. Knowledge
        graph embedding by translating on hyperplanes. In
        <em>AAAI</em>, pages 1112–1119, 2014.</li>
        <li id="BibPLXBIB0045" label="[45]">J.&nbsp;Weng, E.-P.
        Lim, J.&nbsp;Jiang, and Q.&nbsp;He. Twitterrank: finding
        topic-sensitive influential twitterers. In <em>Proceedings
        of the third ACM international conference on Web search and
        data mining</em>, pages 261–270. ACM, 2010.</li>
        <li id="BibPLXBIB0046" label="[46]">Y.&nbsp;Yamaguchi,
        T.&nbsp;Takahashi, T.&nbsp;Amagasa, and H.&nbsp;Kitagawa.
        Turank: Twitter user ranking based on user-tweet graph
        analysis. In <em>International Conference on Web
        Information Systems Engineering</em>, pages 240–253.
        Springer, 2010.</li>
        <li id="BibPLXBIB0047" label="[47]">S.&nbsp;Yan,
        D.&nbsp;Xu, B.&nbsp;Zhang, H.-J. Zhang, Q.&nbsp;Yang, and
        S.&nbsp;Lin. Graph embedding and extensions: A general
        framework for dimensionality reduction. <em>IEEE
        transactions on pattern analysis and machine
        intelligence</em>, 29(1):40–51, 2007.</li>
        <li id="BibPLXBIB0048" label="[48]">Y.&nbsp;Yang,
        J.&nbsp;Tang, C.&nbsp;W.-k. Leung, Y.&nbsp;Sun,
        Q.&nbsp;Chen, J.&nbsp;Li, and Q.&nbsp;Yang. Rain: Social
        role-aware information diffusion. In <em>AAAI</em>, pages
        367–373, 2015.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://snap.stanford.edu/data/enwiki-2013.html">https://snap.stanford.edu/data/enwiki-2013.html</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream">https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/cheng-li/pyramid">https://github.com/cheng-li/pyramid</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Labels (colors)
    are identified by investigating the topic of venues within each
    class of KNN on the original one-hot encoding vector.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186102">https://doi.org/10.1145/3178876.3186102</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
