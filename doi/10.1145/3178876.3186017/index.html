<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Scalable Instance Reconstruction in Knowledge Bases via
  Relatedness Affiliated Embedding</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3178876.3186017'>https://doi.org/10.1145/3178876.3186017</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186017'>https://w3id.org/oa/10.1145/3178876.3186017</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Scalable Instance Reconstruction
          in Knowledge Bases via Relatedness Affiliated
          Embedding</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Richong</span> <span class=
          "surName">Zhang</span>, BDBC and SKLSDE, Beihang
          University, <a href=
          "mailto:zhangrc@act.buaa.edu.cn">zhangrc@act.buaa.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Junpeng</span> <span class=
          "surName">Li</span>, BDBC and SKLSDE, Beihang University,
          <a href=
          "mailto:lijp12@act.buaa.edu.cn">lijp12@act.buaa.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Jiajie</span> <span class=
          "surName">Mei</span>, BDBC and SKLSDE, Beihang
          University, <a href=
          "mailto:meijj@act.buaa.edu.cn">meijj@act.buaa.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Yongyi</span> <span class=
          "surName">Mao</span>, School of Electrical Engineering
          and Computer Science, University of Ottawa, <a href=
          "mailto:ymao@uottawa.ca">ymao@uottawa.ca</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186017"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186017</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The knowledge base (KB) completion problem is
        usually formulated as a link prediction problem. Such
        formulation is incapable of capturing certain application
        scenarios when the KB contains multi-fold relations. In
        this paper, we present a new formulation of KB completion,
        called instance reconstruction. Unlike its link-prediction
        counterpart, which has linear complexity in the size of the
        KB, this problem has its complexity behave as a high-degree
        polynomial. This presents a significant challenge in
        developing scalable instance reconstruction algorithms. In
        this paper, we present a novel knowledge embedding model
        (RAE) and build on it an instance reconstruction algorithm
        (SIR). The SIR algorithm utilizes schema-based filtering as
        well as “relatedness” filtering for complexity reduction.
        Here relatedness refers to the likelihood that two entities
        co-participate in a common instance, and the relatedness
        metric is learned from the RAE model. We show
        experimentally that SIR significantly reduces computation
        complexity without sacrificing reconstruction performance.
        The complexity reduction corresponds to reducing the KB
        size by 100 to 1000 folds.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Computing methodologies</strong>
        → <strong>Reasoning about belief and knowledge;</strong> •
        <strong>Mathematics of computing</strong> → <em>Graph
        algorithms;</em> • <strong>Information systems</strong> →
        <em>Data extraction and integration;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Link
          Prediction</small>,</span> <span class=
          "keyword"><small>Knowledge Base</small>,</span>
          <span class="keyword"><small>Multi-fold
          Relation</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Richong Zhang, Junpeng Li, Jiajie Mei, and Yongyi Mao.
          2018. Scalable Instance Reconstruction in Knowledge Bases
          via Relatedness Affiliated Embedding. In <em>Proceedings
          of The 2018 Web Conference (WWW 2018).</em> ACM, New
          York, NY, USA 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186017" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186017</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Knowledge bases (KBs), such as YAGO&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>], DBpedia[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>] and
      Freebase&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>], have been created for easy and rapid
      information retrieval and knowledge discovery. The enormous
      amount of linked knowledge therein enables the development of
      various applications&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>]. Notably this linked data is usually
      extracted from human-edited knowledge systems, such as
      Wikipedia. As a result, the consistency and the quality of
      the data vary significantly and the completeness of the
      extracted knowledge graph is hardly guaranteed. This makes KB
      completion an important direction of research.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186017/images/www2018-26-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Link prediction vs. instance
          reconstruction.</span>
        </div>
      </figure>
      <p></p>
      <p>In the literature, the problem of KB completion is usually
      formulated as a <em>link prediction</em> problem [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0014">14</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>]. Although
      such a problem is often formulated for binary relations, it
      can be generalized to multi-fold or <em>n</em>-ary relations
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0015">15</a>]. In a KB
      containing multi-fold relations, the link prediction problem
      can be formulated as solving for the entity missing from a
      given role of a given instance. Figure <a class="fig" href=
      "#fig1">1</a> (a) describes an example of the link prediction
      problem, where one wishes to answer the question “where did
      Kobe Bryant got married with Venessa Bryant”. Despite its
      wide adoption in the research literature, this formulation is
      limited in its scope of capturing practical scenarios,
      particularly for KB containing multi-fold relations. For
      example, as shown in Figure <a class="fig" href="#fig1">1</a>
      (b), one may wish to answer the question “where did Kobe
      Bryant got married and with whom”. That is, one may wish to
      reconstruct an instance in which entities are missing from
      all but one roles. We call such a problem the <em>instance
      reconstruction</em> problem.</p>
      <p>It is obvious that for binary relational data, the
      instance reconstruction problem reduces to the link
      prediction problem. However, for multi-fold relations, the
      two problems differ significantly. It is remarkable that the
      significant difference between the two problems has little to
      do with their mathematical nature. Rather this difference is
      fundamentally reflected by their computational complexities.
      For example, for a <em>m</em>-fold relation in a KB
      containing <em>N</em> entities, the complexity of a link
      prediction task is linear in <em>N</em>, whereas the
      complexity of instance reconstruction is in the order of
      <em>N</em> <sup><em>m</em> − 1</sup>. For KBs containing
      multi-fold relations, this complexity can be prohibitive when
      the number of entities is large and when some of the
      relations are non-binary. In fact, for a given set of
      instance reconstruction tasks, even when the fraction of
      non-binary tasks does not dominate, the complexity of the
      non-binary tasks can still dominate the overall complexity.
      For example, in the set of instance reconstruction tasks,
      even when there are a small fraction of tasks that are 6-ary,
      their complexity order of <em>N</em> <sup>5</sup> may
      dominate all tasks with lower arities. As a consequence, even
      for a modestly-sized set of instance-reconstruction tasks,
      the overall complexity can be prohibitive. As such the
      primary challenge in instance reconstruction is to develop an
      efficient reconstruction algorithm that scales well with the
      size of the KB.</p>
      <p>Since the instance reconstruction problem has a similar
      mathematical nature to the link prediction problem, our
      approach to this problem is to build into a link prediction
      model a complexity reduction mechanism. In the recent
      literature, various models have been developed for the link
      prediction purpose [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>], among which the KB embedding models
      have demonstrated to be the state of the art [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0004">4</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0008">8</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0014">14</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0016">16</a>]. We then select a KB
      embedding model and augment its loss function with an
      additional loss measuring the likelihood of two entities
      being “related”. Here two entities are said to be
      <em>related</em> if there is an instance involving both the
      entities. After this overall model is learned, it is then
      possible to first use the relatedness loss to filter all
      possible candidate entity pairs and then use the embedding
      loss to evaluate whether a combination of the remaining pairs
      may be spliced to form a solution instance. We note that the
      complexity of the filtering step is <em>O</em>(<em>N</em>
      <sup>2</sup>) and the complexity of splicing and evaluation
      is still <em>O</em>(<em>N</em> <sup><em>m</em> − 1</sup>).
      However, since filtering can significantly reduce the
      qualified entities to be paired, the overall complexity is in
      fact (ϵ<em>N</em>) <sup><em>m</em> − 1</sup> for a small
      constant ϵ. This makes the reconstruction algorithm much more
      scalable. Our experiments on a multi-fold relational dataset,
      JF17K [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0015">15</a>],
      show that the constant ϵ ranges from 0.001 to 0.0077. This
      corresponds to an effective reduction of the KB size by 100
      to 1000 folds.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Problem
          Statement</h2>
        </div>
      </header>
      <p>Let <span class="inline-equation"><span class=
      "tex">${\mathcal {N}}$</span></span> denote the set of all
      entities in a given knowledge base (KB). We will use
      <span class="inline-equation"><span class="tex">${\mathcal
      {R}}$</span></span> to index the set of all relations
      therein, namely, for each index <span class=
      "inline-equation"><span class="tex">$r\in {\mathcal
      {R}}$</span></span> , there is a relation
      <em>R<sub>r</sub></em> . Each relation <em>R<sub>r</sub></em>
      is associated with a set <span class=
      "inline-equation"><span class="tex">${\mathcal
      {M}}(r)$</span></span> of ordered role tuples {<em>ρ</em>
      <sub>1</sub>, <span class="inline-equation"><span class=
      "tex">$\rho _2, \ldots , \rho _{|{\mathcal
      {M}}(r)|}\rbrace$</span></span> , and the cardinality
      <span class="inline-equation"><span class="tex">$|{\mathcal
      {M}}(r)|$</span></span> of <span class=
      "inline-equation"><span class="tex">${\mathcal
      {M}}(r)$</span></span> is the arity of <em>R<sub>r</sub></em>
      . Each instance in the relation <em>R<sub>r</sub></em> is a
      vector (<span class="inline-equation"><span class="tex">$x_1,
      x_2, \ldots , x_{|{\mathcal {M}}(r)|}$</span></span> ) of
      entities, in which entity <em>x<sub>i</sub></em> corresponds
      to role <span class="inline-equation"><span class="tex">$\rho
      _i\in {\mathcal {M}}(r)$</span></span> . The relation
      <em>R<sub>r</sub></em> is then understood as the set of all
      such vectors. We note that each relation
      <em>R<sub>r</sub></em> is allowed to have an arbitrary
      arity.</p>
      <p>In practice, KBs are far from being complete, namely that
      <em>R<sub>r</sub></em> is not completely given. Instead it is
      revealed via an incomplete collection of instances in
      <em>R<sub>r</sub></em> , which we will denote by <span class=
      "inline-equation"><span class="tex">${\mathcal
      {T}}_r$</span></span> . The KB is then specified by
      <span class="inline-equation"><span class="tex">$G=({\mathcal
      {N}}, {\mathcal {R}}, \lbrace {\mathcal {T}}_r: r\in
      {\mathcal {R}}\rbrace)$</span></span> . We will use
      <em>G</em><sup>0</sup> to denote the underlying complete KB.
      i.e. <span class="inline-equation"><span class=
      "tex">$G^0:=({\mathcal {N}}, {\mathcal {R}}, \lbrace
      {\mathcal {R}}_r: r\in {\mathcal {R}}\rbrace)$</span></span>
      .</p>
      <p>Broadly speaking, a KB completion problem deals with
      inferring unobserved instances in <em>G</em><sup>0</sup>
      based on observed instances in <em>G</em>. Such a problem may
      be set up in at least two ways, <em>link prediction</em> as
      is standard, or <em>instance reconstruction</em> as we
      introduce in this paper.</p>
      <p>The link prediction problem can be formulated as follows.
      Let <span class="inline-equation"><span class="tex">$\rho
      ^\bullet \in {\mathcal {M}}(r)$</span></span> for a relation
      <em>R<sub>r</sub></em> , and suppose that
      <em>t</em><sup>*</sup> is a subvector of an instance
      <span class="inline-equation"><span class="tex">$t=(x_1, x_2,
      \ldots , x_{|{\mathcal {M}}(r)|})\in R_r$</span></span> ,
      where <em>t</em><sup>*</sup> contains all elements of
      <em>t</em> except for the one corresponding to role
      <em>ρ</em><sup>•</sup>. Then the link prediction problem is
      to recover <em>t</em> from <em>t</em><sup>*</sup> (Figure
      <a class="fig" href="#fig1">1</a> (a)), and
      <em>t</em><sup>*</sup> will be referred to as the
      <em>key</em> for the problem. The role <em>ρ</em><sup>•</sup>
      will be referred to as the <em>query role</em>.</p>
      <p>Due to the limited application scope of the link
      prediction problem, we now formulate another KB completion
      problem, the <em>instance reconstruction</em> problem. Let
      <span class="inline-equation"><span class="tex">$\rho ^*\in
      {\mathcal {M}}(r)$</span></span> for a relation
      <em>R<sub>r</sub></em> , and suppose that
      <em>x</em><sup>*</sup> is an entity of an instance
      <span class="inline-equation"><span class="tex">$t=(x_1, x_2,
      \ldots , x_{|{\mathcal {M}}(r)|})\in R_r$</span></span>
      corresponding to <em>ρ</em><sup>*</sup>. Then the instance
      reconstruction problem is to recover <em>t</em> from
      <em>x</em><sup>*</sup> (Figure <a class="fig" href=
      "#fig1">1</a> (b)), and, likewise, <em>x</em><sup>*</sup>
      will be referred to as the <em>key</em> for the problem.</p>
      <p>Apparently, an instance reconstruction problem for a given
      primary entity <em>x</em><sup>*</sup> can be resolved by
      repeatedly solving many link prediction problems. More
      precisely, this involves 1) going through each relation
      <em>R<sub>r</sub></em> 2) and for each <em>R<sub>r</sub></em>
      , testing <span class="inline-equation"><span class=
      "tex">${|{\mathcal {N}}|-1 \atopwithdelims ()|{\mathcal
      {M}}(r)|-1}$</span></span> entity combinations to see if they
      may form an instance of <em>R<sub>r</sub></em> . This entails
      a complexity <span class="inline-equation"><span class=
      "tex">$\sum \limits _{R_r \in {\mathcal {R}}}{|{\mathcal
      {N}}|-1 \atopwithdelims ()|{\mathcal
      {M}}(r)|-1}$</span></span> , which is prohibitive for large
      KB containing many relations. Thus the main challenge in
      instance reconstruction is dealing with its complexity.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Joint Modelling
          and Learning of Relatedness and Embedding</h2>
        </div>
      </header>
      <p>In this section, we will first introduce a new modelling
      framework, which we call <em>Relatedness Affiliated
      Embedding</em>, or <em>RAE</em>, which augments any KB
      embedding model with an additional loss that models the
      relatedness of entities. We will also describe a concrete
      example of an RAE model. We then describe the training
      algorithm of the proposed RAE model.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Relatedness
            Affiliated Embedding (RAE)</h3>
          </div>
        </header>
        <p>The RAE model is specified by a loss function which
        consists of two components, a “global embedding loss” and
        “relatedness loss”.</p>
        <section id="sec-9">
          <p><em>3.1.1 global embedding loss.</em> The <em>global
          embedding loss</em> function may be the optimization
          objective function in any parametrized KB embedding
          model. Thus this subsection serves two purposes. On one
          hand, we give a concise review of the KB embedding
          framework, and on the other hand, we will explain our
          embedding loss function.</p>
          <p>Given a KB <span class="inline-equation"><span class=
          "tex">$G=({\mathcal {N}}, {\mathcal {R}}, \lbrace
          {\mathcal {T}}_r: r\in {\mathcal
          {R}}\rbrace)$</span>,</span> the KB embedding problem is
          to represent <span class="inline-equation"><span class=
          "tex">${\mathcal {N}}$</span></span> and each
          <em>R<sub>r</sub></em> as quantities in a
          <em>K</em>-dimensional Euclidean space <span class=
          "inline-equation"><span class="tex">$U:={\mathbb
          {R}}^K$</span></span> such that the structure of the KB
          is preserved. Let function <span class=
          "inline-equation"><span class="tex">$\phi :{\mathcal {N}}
          \rightarrow {\mathbb {R}}^K$</span></span> be the desired
          embedding map which represents every entity <span class=
          "inline-equation"><span class="tex">$x\in {\mathcal
          {N}}$</span></span> as a vector <em>ϕ</em>(<em>x</em>) ∈
          <em>U</em>. For any instance <span class=
          "inline-equation"><span class="tex">$t:=(x_1, x_2, \ldots
          , x_{|{\mathcal {M}}(r)|}) \in R_r$</span>,</span> its
          image <em>ϕ</em>(<em>t</em>) under the embedding map is
          then <span class="inline-equation"><span class=
          "tex">$(\phi (x_1), \phi (x_2), \ldots ,\phi
          (x_{|{\mathcal {M}}(r)|}))$</span></span> . To model the
          structure of a relation <em>R<sub>r</sub></em> , one
          usually associates <em>R<sub>r</sub></em> with a loss
          function <em>f<sub>r</sub></em> on <span class=
          "inline-equation"><span class="tex">$U^{|{\mathcal
          {M}}(r)|}$</span></span> and desires
          <em>f<sub>r</sub></em> (<em>t</em>) to be near 0 for
          <em>t</em> ∈ <em>R<sub>r</sub></em> and to be large for
          <em>t</em> ∉ <em>R<sub>r</sub></em> .</p>
          <p>When the set <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}:=
          \bigcup _{r\in {\mathcal {R}}} {\mathcal
          {T}}_r$</span></span> is given, one may take a learning
          approach to decide what these functions <span class=
          "inline-equation"><span class="tex">$\lbrace f_r:r\in
          {\mathcal {R}}\rbrace$</span></span> and the embedding
          map <em>ϕ</em> should be by examining how the examples in
          <span class="inline-equation"><span class=
          "tex">${\mathcal {T}}$</span></span> look like. To fit
          the problem into a supervised learning regime, one may
          also create a set of negative examples <span class=
          "inline-equation"><span class="tex">${\mathcal
          {T}}^-$</span></span> . This can be done by taking the
          instances in each <span class=
          "inline-equation"><span class="tex">${\mathcal
          {T}}_r$</span></span> and randomly replacing an entity
          therein by another entity. This creates a set
          <span class="inline-equation"><span class=
          "tex">${\mathcal {T}}_r^-$</span>,</span> in which each
          element is a “non-instance” or negative example with
          respect to relation <em>R<sub>r</sub></em> with high
          probability. Then <span class=
          "inline-equation"><span class="tex">${\mathcal
          {T}}^-$</span></span> may be compiled as the union
          <span class="inline-equation"><span class="tex">$\bigcup
          _{r\in {\mathcal {R}}}{\mathcal {T}}_r^-$</span></span>
          .</p>
          <p>The key of designing an embedding model is designing
          the functional form of <em>f<sub>r</sub></em> . When this
          is done, we can set up a global embedding loss function
          <em>E<sub>I</sub></em> as</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray} E_I (\Theta
              _I,\phi) &amp; := &amp;\sum \limits _{r\in {\mathcal
              {R}}} \left(\sum \limits _{t\in {\mathcal {T}}_r}
              f_r(t) + \sum \limits _{t^-\in {\mathcal
              {T}}^{-}_{r}} \!\!\!\! {\bf ReLU}(C - f_r(t)) \right)
              \end{eqnarray}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>where <em>Θ<sub>I</sub></em> denotes <span class=
          "inline-equation"><span class="tex">$\lbrace \theta
          _r:r\in {\mathcal {R}}\rbrace$</span>,</span> <em>C</em>
          is a pre-specified threshold and <strong>ReLU</strong>(·)
          is the rectified linear unit.
          <p></p>
          <p>Existing KB embedding models use various forms of
          <em>f<sub>r</sub></em> [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0008">8</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0016">16</a>]. A majority of these embedding
          models assume that each relation is binary or two-fold,
          so that <em>f<sub>r</sub></em> has two variables. These
          models include TransE[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0004">4</a>], TransH[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0014">14</a>],
          TransR[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0008">8</a>], etc. For example, in TransH,
          each <em>f<sub>r</sub></em> is parameterized by a
          hyperplane with normal vector
          <strong>n<sub>r</sub></strong> together with the
          displacement vector <strong>b</strong>
          <sub><em>r</em></sub> on the plane:</p>
          <div class="table-responsive" id="eq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} f_r({\bf
              x},{\bf y}) : = \left|\left| {\mathbb {P}}_{{\bf
              n}_r}({\bf x})+ {\bf b}_r - {\mathbb {P}}_{{\bf
              n}_r}({\bf y}) \right|\right|^2
              \end{equation}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>where <span class="inline-equation"><span class=
          "tex">$\mathbb {P}_{{\bf n}_r}$</span></span> is the
          projection operator on the hyperplane with normal vector
          <strong>n</strong> <sub><em>r</em></sub> .
          <p></p>
          <p>Recognizing the limitation of these binary models, the
          authors of [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0015">15</a>] introduced the mTransH model by
          extending TransH to multi-fold relations. More
          specifically, the global embedding loss function
          <em>f<sub>r</sub></em> in mTransH is defined as:</p>
          <div class="table-responsive" id="eq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray} f_r({\bf
              x}_{\rho _1},{\bf x}_{\rho _2},\ldots ,{\bf x}_{\rho
              _{|{\mathcal {M}}(r)|}}) &amp;: = &amp;
              \!\left|\left| \sum \limits _{\rho \in {\mathcal
              {M}}(R_r)} {{\bf a}_{\rho }}{\mathbb {P}_{{\bf
              n}_r}}({\bf x}_\rho) + {\bf b}_r \right|\right|^2,
              \end{eqnarray}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>where <strong>n</strong> <sub><em>r</em></sub> is a
          unit vector and <strong>b</strong> <sub><em>r</em></sub>
          is a vector on the hyperplane with normal vector
          <strong>n</strong> <sub><em>r</em></sub> . In our RAE
          model, we will use the loss function in TransH (<a class=
          "eqn" href="#eq2">2</a>) or mTransH (<a class="eqn" href=
          "#eq3">3</a>) as our embedding loss function
          <em>f<sub>r</sub></em> , depending on whether the KB we
          deal with is binary or multi-fold. We note however that
          the RAE model is not restricted to these embedding loss
          functions, and virtually any multi-fold embedding model
          can be used.
          <p></p>
        </section>
        <section id="sec-10">
          <p><em>3.1.2 relatedness loss.</em> The <em>relatedness
          loss</em> function can be the optimization objective in
          any binary classification model. Here entities <em>x</em>
          and <em>y</em> are said to be <em>related</em>, denoted
          by <em>x</em> ∼ <em>y</em>, if there is a relation
          <em>R<sub>r</sub></em> and an instance <em>t</em> ∈
          <em>R<sub>r</sub></em> such that <em>x</em> and
          <em>y</em> are two components of vector <em>t</em>. And
          we write <em>p</em>(<em>x</em> ∼ <em>y</em>) as the
          probability for this event (under our to-be-specified
          relatedness model).</p>
          <p>Let <span class="inline-equation"><span class=
          "tex">${\mathcal {B}}^0$</span></span> denote the set of
          all related entity pairs in <em>G</em> <sup>0</sup>, and
          <span class="inline-equation"><span class="tex">$\mathcal
          {B}$</span></span> denote the subset of <span class=
          "inline-equation"><span class="tex">${\mathcal
          {B}}^0$</span></span> that is observed in <em>G</em>. The
          relatedness loss for every pair (<em>x</em>, <em>y</em>)
          of entities measures whether <em>x</em> and <em>y</em>
          are related, or if <span class=
          "inline-equation"><span class="tex">$(x, y)\in {\mathcal
          {B}}^0$</span></span> . Similar to the way that one
          constructs the negative examples set <span class=
          "inline-equation"><span class="tex">$\mathcal
          {T}^-$</span>,</span> we may create a set <span class=
          "inline-equation"><span class="tex">$\mathcal
          {B}^-$</span></span> of unrelated entity pairs. This will
          allow us to use <span class=
          "inline-equation"><span class="tex">$\mathcal
          {B}$</span></span> and <span class=
          "inline-equation"><span class="tex">$\mathcal
          {B}^-$</span></span> to train the relatedness model.</p>
          <p>In this work, we choose the relatedness loss to be the
          loss function of multi-layer perceptron (MLP) neural
          network, which we now specify.</p>
          <p>Let <em>w</em> <sub>1</sub> be a 2<em>K</em> ×
          <em>L</em> matrix and <em>w</em> <sub>2</sub> an
          <em>L</em> × 2 matrix. Let <span class=
          "inline-equation"><span class="tex">${\bf softmax}:
          {\mathbb {R}}^2\rightarrow {\mathbb {R}}^2$</span></span>
          be the softmax function on <span class=
          "inline-equation"><span class="tex">${\mathbb
          {R}}^2$</span></span> . That is, for every vector
          <span class="inline-equation"><span class="tex">$c=[c_1,
          c_2]^T\in {\mathbb {R}}^2$</span>,</span></p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ {\bf
              softmax}(c)_i:=\frac{\exp (c_i)}{\exp (c_1)+\exp
              (c_2)}, i=1, 2. \]</span><br />
            </div>
          </div>
          <p></p>
          <p>Let <span class="inline-equation"><span class=
          "tex">$b_1\in {\mathbb {R}}^L$</span></span> and
          <span class="inline-equation"><span class="tex">$b_2\in
          {\mathbb {R}}^2$</span></span> . Our MLP model is defined
          by the following sequence of function compositions. For
          any vector <span class="inline-equation"><span class=
          "tex">$v\in {\mathbb {R}}^{2K}$</span>,</span></p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ g:=w_1v+b_1; h:={\bf
              ReLU}(g); c:=w_2h+b_2; z:={\bf softmax}(c).
              \]</span><br />
            </div>
          </div>For any input <span class=
          "inline-equation"><span class="tex">$v\in {\mathbb
          {R}}^{2K}$</span>,</span> let
          <strong>MLP</strong>(<em>v</em>) denote the output of the
          MLP. For any pair of entities (<em>x</em>, <em>y</em>),
          let
          <div class="table-responsive" id="Xeq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} p(x\sim y):=
              {\bf MLP}(\phi (x), \phi (y)).
              \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>That is, if we supply the MLP with the
          concatenation of the embedding vectors for <em>x</em> and
          <em>y</em>, the output of the MLP models the likelihood
          that <em>x</em> and <em>y</em> are related. The parameter
          of the MLP (<em>w</em> <sub>1</sub>, <em>w</em>
          <sub>2</sub>, <em>b</em> <sub>1</sub>, <em>b</em>
          <sub>2</sub>) will be denoted by <em>Θ<sub>C</sub></em> .
          The overall relatedness loss function on the training
          sets <span class="inline-equation"><span class=
          "tex">${\mathcal {B}}$</span></span> and <span class=
          "inline-equation"><span class="tex">${\mathcal
          {B}}^-$</span></span> can then be naturally defined as:
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} E_C(\Theta
              _C, \phi): = -\!\!\!\!\!\!\sum \limits _{(x,y) \in
              {\mathcal {B}}} \!\!\!\!\log p(x\sim y) -
              \!\!\!\!\!\!\sum \limits _{(x^{\prime },y^{\prime })
              \in {\mathcal {B}}^-} \!\!\!\!\log (1-p(x^{\prime
              }\sim y^{\prime })) \end{equation}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>
          <p></p>
        </section>
        <section id="sec-11">
          <p><em>3.1.3 Overall RAE Model.</em> When the model for
          <em>p</em>(<em>x</em> ∼ <em>y</em>) is chosen, the
          overall loss function of the RAE model is defined as</p>
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} E(\Theta
              _{\it I}, \Theta _{\it C}, \phi):= E_{\it I}(\Theta
              _{\it I}, \phi) + \lambda E_{\it C}(\Theta _{\it C},
              \phi), \end{equation}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>where <em>λ</em> is a hyper-parameter governing the
          relative weighting of relatedness loss and embedding
          loss.
          <p></p>
          <p>In the RAE framework, one is free to choose a
          binary-classification model for <em>p</em>(<em>x</em> ∼
          <em>y</em>) for related and unrelated (<em>x</em>,
          <em>y</em>), as long as <em>p</em>(<em>x</em> ∼
          <em>y</em>) is made dependent of the embedding map
          <em>ϕ</em> in some sensible way.</p>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186017/images/www2018-26-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">(Top) Back propagation
              update of <em>Θ<sub>C</sub></em> ; (Bottom) Back
              propagation update of <em>v</em>.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> RAE
            Training Algorithm</h3>
          </div>
        </header>
        <p>The training of the RAE model can be formulated as
        solving the minimization problem with objective function
        <em>E</em>(<em>Θ<sub>I</sub></em> , <em>Θ<sub>C</sub></em>
        , <em>ϕ</em>). The minimizing configuration then gives us
        both an embedding model, which we may use to evaluate if a
        <span class="inline-equation"><span class="tex">$|{\mathcal
        {M}}(r)|$</span></span> -tuple qualifies for being an
        instance of <em>R<sub>r</sub></em> , and a classifier,
        which we may use to determine the probability that two
        entities are related.</p>
        <p>The summation form in the definition of the overall loss
        <em>E</em> and in <em>E<sub>I</sub></em> and
        <em>E<sub>C</sub></em> allows us to solve the problem via
        stochastic gradient descent (SGD). In this work, we
        developed a training algorithm, which is a combination of a
        three-way alternating minimization and SGD. More
        specifically, the training of the model consists of three
        elementary steps:</p>
        <p><span style=
        "text-decoration: underline;"><strong>Step-1</strong></span>
        : Update (<em>Θ<sub>I</sub></em> , <em>ϕ</em>) following
        the gradient of <em>E<sub>I</sub></em> with respect to
        <em>Θ<sub>I</sub></em> . This can be done by SGD on the
        cost function <em>E<sub>I</sub></em> .</p>
        <p><span style=
        "text-decoration: underline;"><strong>Step-2</strong></span>
        : Update <em>Θ<sub>C</sub></em> following the gradient of
        <em>E<sub>C</sub></em> with respect to
        <em>Θ<sub>C</sub></em> . This can be done by standard back
        propagation training of the neural network (Figure
        <a class="fig" href="#fig2">2</a>, Top).</p>
        <p><span style=
        "text-decoration: underline;"><strong>Step-3</strong></span>
        : Update <em>ϕ</em> following the gradient of
        <em>E<sub>C</sub></em> with respect to <em>ϕ</em>. This can
        also be done by back propagation, where instead of
        propagating error back to <em>Θ<sub>C</sub></em> , the
        error is propagated back to <em>v</em> =
        (<em>ϕ</em>(<em>x</em>), <em>ϕ</em>(<em>y</em>))
        (Figure&nbsp;<a class="fig" href="#fig2">2</a>, Bottom).
        This is essentially an SGD approach and is a modest
        innovation of this work.</p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> SCALABLE
          INSTANCE RECONSTRUCTION</h2>
        </div>
      </header>
      <figure id="fig3">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186017/images/www2018-26-fig3.jpg"
        class="img-responsive" alt="" longdesc="" />
      </figure>
      <p>As discussed earlier, the instance reconstruction problem
      entails significant and sometimes prohibitive complexity. A
      natural strategy to fix this problem is to use a “filtering”
      approach to drastically reduce the number of candidate tuples
      that will be evaluated under the embedding model. This gives
      rise to our proposed Scalable Instance Reconstruction (SIR)
      algorithm.</p>
      <p>For a given instance reconstruction task with key
      <em>x</em><sup>*</sup> (and in relation
      <em>R<sub>r</sub></em> ), the overall structure of the SIR
      algorithm is given in the pseudo code in Algorithm 1. The SIR
      algorithm contains three parts: filtering, splicing, and
      ranking. In this section, we first walk through this
      algorithm.</p>
      <p>We note that for notational convenience, in the
      description of Algorithm 1, each role in <span class=
      "inline-equation"><span class="tex">${\mathcal
      {M}}(r)$</span></span> is associated with an index
      <span class="inline-equation"><span class="tex">$i\in \lbrace
      1, 2, \ldots , |{\mathcal {M}}(r)|\rbrace$</span></span> and
      the key role <em>ρ</em><sup>*</sup> is
      <em>ρ</em><sub>1</sub>, namely, the role with the smallest
      index.</p>
      <figure id="fig4">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186017/images/www2018-26-fig4.jpg"
        class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class=
          "figure-title">EECG Example.</span>
        </div>
      </figure>
      <p></p>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Filtering</h3>
          </div>
        </header>
        <p>In this algorithm, we consider two types of filtering:
        schema-based filtering(SCH-FILTER) and Relatedness
        filtering(REL-FILTER).</p>
        <section id="sec-15">
          <p><em>4.1.1 Schema-Based Filtering: SCH-FILTER.</em>
          Schema-based filtering, which is denoted by subroutine
          SCH-FILTER, aims to reduce the number of entities to be
          considered in forming candidate instances. It leverages
          the type requirements on the entities dictated by the
          schema of a relation. More specifically, for the relation
          <em>R<sub>r</sub></em> of the to-be-reconstructed
          instance and a non-key role <span class=
          "inline-equation"><span class="tex">$\rho \in {\mathcal
          {M}}(r)\setminus \lbrace \rho ^*\rbrace$</span>,</span>
          the subroutine SCH-FILTER finds the entity types allowed
          at the role <em>ρ</em> by the schema of
          <em>R<sub>r</sub></em> , and returns a set of entities by
          excluding from <span class="inline-equation"><span class=
          "tex">${\mathcal {N}}$</span></span> the entities whose
          types are not compatible with allowed types.</p>
        </section>
        <section id="sec-16">
          <p><em>4.1.2 Relatedness filtering: REL-FILTER.</em>
          Relatedness filtering is treated as a subroutine
          REL-FILTER in Algorithm 1. The RAE model includes a
          relatedness model, in which any two entities participate
          in the same instance in the KB are considered related.
          When the RAE model is learned, the parameter <em>Θ</em>
          <sub>C</sub> for relatedness model <em>p</em>(<em>x</em>
          ∼ <em>y</em>) is determined. A natural approach is to
          compare <em>p</em>(<em>x</em> ∼ <em>y</em>) with a
          threshold <em>τ</em> to predict if the two entities are
          related. More specifically, given an entity <em>x</em>,
          an arbitrary set <span class=
          "inline-equation"><span class="tex">${\mathcal
          {U}}$</span></span> of entities, the learned parameter
          <em>Θ<sub>C</sub></em> and an acceptance threshold
          <em>τ</em>, the function REL-FILTER returns the set of
          all entity pairs (<em>x</em>, <em>y</em>) such that
          <span class="inline-equation"><span class="tex">$y\in
          {\mathcal {U}}$</span></span> and <em>p</em>(<em>x</em> ∼
          <em>y</em>) exceeds <em>τ</em>.</p>
        </section>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Splicing:
            SPLICE</h3>
          </div>
        </header>
        <p>The objective of pair splicing is to construct the set
        <span class="inline-equation"><span class="tex">${\mathcal
        {I}}(x^*)$</span></span> of all <em>candidate tuples</em>.
        It turns out that the problem of constructing candidate
        tuples can be formulated in an elegant graph-theoretic
        framework, which we now explain.</p>
        <p><strong>The Maximum Color-Matched Clique
        Problem</strong> An <em>edge-end-coloured graph</em>, or
        EECG, <span class="inline-equation"><span class=
        "tex">${\mathcal {G}}$</span></span> of colour set
        <span class="inline-equation"><span class="tex">${\mathcal
        {H}}$</span></span> is an undirected graph in which the two
        end points of each edge are coloured by two distinct
        colours in <span class="inline-equation"><span class=
        "tex">${\mathcal {H}}$</span></span> . For any given EECG
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> , we will use <span class=
        "inline-equation"><span class="tex">$V({\mathcal
        {G}})$</span></span> to denote the set of vertices of
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> . If <em>e</em> is an edge in an EECG
        whose two end points are coloured by colours <em>h</em> and
        <em>h</em>′ respectively, we use <em>v</em>(<em>e</em>;
        <em>h</em>) (resp. <em>v</em>(<em>e</em>; <em>h</em>′)) to
        denote the vertex connected by edge <em>e</em> through its
        end-point colour <em>h</em> (resp. <em>h</em>′). For any
        vertex <em>u</em> in an EECG, we will use <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}(u)$</span></span> to denote the set of all colours on
        the edge endpoints attached to <em>u</em>, and refer to
        <span class="inline-equation"><span class="tex">${\mathcal
        {H}}(u)$</span></span> as the colour set of vertex
        <em>u</em>. Figure <a class="fig" href="#fig4">3</a> shows
        an example of an EECG, where
        <em>v</em>(<em>e</em><sub>1</sub>; <em>o</em>) = 1,
        <em>v</em>(<em>e</em><sub>1</sub>; <em>r</em>) = 2,
        <span class="inline-equation"><span class="tex">${\mathcal
        {H}}(2) = \lbrace r\rbrace$</span></span> and <span class=
        "inline-equation"><span class="tex">${\mathcal {H}}(1) =
        \lbrace r, o\rbrace$</span></span> .</p>
        <p>Given an EECG <span class="inline-equation"><span class=
        "tex">${\mathcal {G}}$</span></span> of colour set
        <span class="inline-equation"><span class="tex">${\mathcal
        {H}}$</span></span> , a <em>colour-matched clique</em>, or
        CMC, is a sub-EECG <span class=
        "inline-equation"><span class="tex">${\mathcal {G}}^{\prime
        }$</span></span> of <span class=
        "inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> (namely <span class=
        "inline-equation"><span class="tex">${\mathcal {G}}^{\prime
        }$</span></span> contains a subset of the edges in
        <span class="inline-equation"><span class="tex">$\mathcal
        {G}$</span></span> ) satisfying the following conditions.
        (1) <span class="inline-equation"><span class=
        "tex">${\mathcal {G}}^{\prime }$</span></span> is complete.
        (2) The colour set <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}(u)$</span></span> of every vertex <em>u</em> in
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}^{\prime }$</span></span> is a singleton set; we will
        denote the single colour contained in <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}(u)$</span></span> by <em>h</em>(<em>u</em>) and call
        it the <em>colour</em> of <em>u</em>. (3) For every two
        different vertices <em>u</em> and <em>u</em>′ in
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}^{\prime }$</span>,</span> <em>h</em>(<em>u</em>) ≠
        <em>h</em>(<em>u</em>′), or every two vertices have
        different colours.</p>
        <p>The size of a CMC refers to the number of vertices it
        contains. In the EECG in Figure <a class="fig" href=
        "#fig4">3</a>, the sub-EECG consisting of edges
        {<em>e</em><sub>2</sub>, <em>e</em><sub>4</sub>,
        <em>e</em><sub>5</sub>} is a CMC of size 3, and every edge
        is a CMC of size 2. Note that by definition, in a CMC
        <span class="inline-equation"><span class="tex">${\mathcal
        {C}}$</span></span> of size <em>m</em>, the <em>m</em>
        vertices have <em>m</em> different colours, and we will use
        <span class="inline-equation"><span class="tex">${\mathcal
        {H}}({\mathcal {C}})$</span></span> to denote this set of
        <em>m</em> colours. A CMC <span class=
        "inline-equation"><span class="tex">${\mathcal
        {C}}$</span></span> in the EECG <span class=
        "inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> of colour set <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}$</span></span> will be called a <em>maximum CMC</em>,
        or <em>MCMC</em>, if <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}({\mathcal {C}}) = {\mathcal {H}}$</span></span> . The
        following results can be proved.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186017/images/www2018-26-fig5.jpg"
          class="img-responsive" alt="" longdesc="" />
        </figure>
        <p></p>
        <div class="lemma" id="enc1">
          <label>Lemma 1.</label>
          <p>For a given reconstruction task with key <em>x</em>
          <sup>*</sup>, interpret the set <span class=
          "inline-equation"><span class="tex">$\lbrace {\mathcal
          {P}}(x^*; \rho _i, \rho _j): \rho _i, \rho _j\in
          {\mathcal {M}}(r): i\ {\lt}\ j\rbrace$</span></span>
          (Algorithm 1) as an EECG <span class=
          "inline-equation"><span class="tex">${\mathcal
          {G}}(x^*)$</span></span> . Every related entity pair in
          <em>G</em> <sup>0</sup> can be interpreted as an
          endpoint-coloured edge. Under such interpretations, if
          the set of all related entities are contained in
          <span class="inline-equation"><span class=
          "tex">${\mathcal {G}}(x^*)$</span>,</span> then each
          instance in <em>G</em> <sup>0</sup> involving <em>x</em>
          <sup>*</sup> can be interpreted as an MCMC containing
          <em>x</em> <sup>*</sup>.</p>
        </div>
        <p>Under this lemma, the pair splicing problem can be
        equated with finding the set of all <em>MCMC</em>’s of
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}(x^*)$</span></span> .</p>
        <p><strong>A</strong> <em><strong>MCMC</strong></em>
        <strong>Finding Algorithm</strong> On an EECG <span class=
        "inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> of colour set <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}$</span></span> , for any given vertex <em>u</em> and
        any given colour <span class="inline-equation"><span class=
        "tex">$h\in {\mathcal {H}}$</span></span> , let
        <span class="inline-equation"><span class="tex">${\mathcal
        {V}}(u; h)$</span></span> be the set of all vertices
        <em>u</em>′ in <span class="inline-equation"><span class=
        "tex">${\mathcal {G}}$</span></span> such that there is an
        edge, say <em>e</em>, connecting <em>u</em> and <em>u</em>′
        and <em>v</em>(<em>e</em>; <em>h</em>) = <em>u</em>′. If
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}^{\prime }$</span></span> is a sub-EECG of <span class=
        "inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> and <em>u</em> is vertex in
        <span class="inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> but not in <span class=
        "inline-equation"><span class="tex">${\mathcal {G}}^{\prime
        }$</span></span> , then we use <span class=
        "inline-equation"><span class="tex">${\mathcal {G}}^{\prime
        }+u$</span></span> to denote the sub-EECG of <em>G</em>
        which consists of all edges and vertices in <span class=
        "inline-equation"><span class="tex">${\mathcal {G}}^{\prime
        }$</span></span> , vertex <em>u</em>, and the set of all
        edges connecting to <em>u</em> in <span class=
        "inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> .</p>
        <p>Using these notations, we present an algorithm SPLICE
        (Algorithm 2), which finds the set of all MCMC's for a
        given EECG <span class="inline-equation"><span class=
        "tex">${\mathcal {G}}$</span></span> with seed CMC
        {<em>x</em>}, where the colour set <span class=
        "inline-equation"><span class="tex">${\mathcal
        {H}}$</span></span> of <span class=
        "inline-equation"><span class="tex">${\mathcal
        {G}}$</span></span> is expressed as <span class=
        "inline-equation"><span class="tex">$\lbrace 1, 2, \ldots ,
        |{\mathcal {H}}|\rbrace$</span></span> and the colour of
        the seed CMC is taken as 1.</p>
        <div class="lemma" id="enc2">
          <label>Lemma 2.</label>
          <p>The algorithm SPLICE returns the set of all MCMC's of
          <span class="inline-equation"><span class=
          "tex">${\mathcal {G}}(x^*)$</span></span> .</p>
        </div>
        <p>The proof of the lemma can be established via
        induction.</p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Ranking:
            RANK</h3>
          </div>
        </header>
        <p>Upon obtaining the set <span class=
        "inline-equation"><span class="tex">${\mathcal
        {I}}(x^*)$</span></span> of candidate tuples, Algorithm 1
        finally executes the RANK subroutine, which performs the
        following computation in order.</p>
        <ol class="list-no-style">
          <li id="list4" label="(1)">Evaluating the embedding loss
          for each candidate tuple <span class=
          "inline-equation"><span class="tex">$t=(x_1, x_2, \ldots
          , x_{|{\mathcal {M}}(r)|})\in {\mathcal
          {I}}(x^*)$</span></span> using the learned embedding loss
          function <em>f<sub>r</sub></em> , namely, evaluating the
          quantity <span class="inline-equation"><span class=
          "tex">$f_r(\phi (x_1), \phi (x_2), \ldots , \phi
          (x_{|{\mathcal {M}}(r)|}))$</span></span> .<br /></li>
          <li id="list5" label="(2)">Ranking the tuples in
          <span class="inline-equation"><span class=
          "tex">${\mathcal {I}}(x^*)$</span></span> according to
          the their embedding loss, from low to high (low loss
          having a better ranking). The resulting ranked list is
          denoted by <em>L</em>(<em>x</em>
          <sup>*</sup>).<br /></li>
        </ol>
      </section>
    </section>
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experimental
          Study</h2>
        </div>
      </header>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span>
            Datasets</h3>
          </div>
        </header>
        <p>Since the objective of this paper is to develop a
        scalable model and algorithm for instance reconstruction,
        which is particularly challenging for multi-fold relational
        data, the primary dataset in our experiments is the JF17K
        dataset. Having complexity reduction as our main goal, it
        is also desirable to investigate the reconstruction
        performance of our approach and hope that this reduction in
        complexity entails no loss of performance, comparing with
        the standard embedding models. Such a comparison is however
        infeasible on the JF17K dataset, since using any
        traditional model even on a modestly sized KB with
        multi-fold relations would entail significant complexity.
        For this reason, we also experiment our approach and a
        traditional embedding model on the FB15K dataset, as a
        reference for performance comparison.</p>
        <section id="sec-21">
          <p><em>5.1.1 FB15K and JF17K.</em> FB15K, developed by
          &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0004">4</a>] and filtered from Freebase,
          contains binary relational data. Note that Freebase
          originally contains multi-fold relations. As discussed in
          &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0004">4</a>] and [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0015">15</a>], in FB15K, these data are
          converted to binary in a way much similar to how we
          define the relatedness relation ∼ . JF17K, developed in
          &nbsp;[<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0015">15</a>], is also filtered from Freebase,
          while having multi-fold relational structure preserved.
          JF17K dataset can be found at <a class=
          "link-inline force-break" href=
          "http://github.com/lijp12/SIR">http://github.com/lijp12/SIR</a>.</p>
          <p>Both datasets are further pre-processed using the
          schema of Freebase. The Freebase schema lists the set of
          all included relations and defines the expected entity
          types for the roles of each relation. Although FB15K
          converts multi-fold relations to binary relations, its
          schema can be retrieved from Freebase. The most common
          type in Freebase schema is “common.topic”; in FB15K and
          JF17K, more than 99% of the entities have this type
          label. This type, containing virtually no information, is
          removed from the schema. In addition, relations not
          included in the schema are also deleted. In total, 97
          relations from FB15K and 5 relations from JF17K are
          removed, together with the entities only involved in
          these relations. Each resulting dataset is partitioned
          into the training set <span class=
          "inline-equation"><span class="tex">${\mathcal
          {T}}$</span></span> and the testing set <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span></span> . Table 1 shows the statistics of
          FB15K and JF17K datasets.</p>
        </section>
        <section id="sec-22">
          <p><em>5.1.2 Derived Datasets.</em> Several datasets are
          derived from FB15K and JF17K.</p>
          <p><strong>Negative Training Sets</strong>
          <strong>(</strong> <span class=
          "inline-equation"><span class="tex">${\mathcal
          {T}}^-$</span></span> <strong>and</strong> <span class=
          "inline-equation"><span class="tex">${\mathcal
          {B}}^-$</span></span> <strong>)</strong>Negative training
          examples <span class="inline-equation"><span class=
          "tex">$\mathcal {T}^-$</span></span> and <span class=
          "inline-equation"><span class="tex">$\mathcal
          {B}^-$</span></span> are constructed as follows. For each
          positive training example <span class=
          "inline-equation"><span class="tex">$t\in \mathcal
          {T}$</span></span> (and resp. <span class=
          "inline-equation"><span class="tex">$ b\in \mathcal
          {B}$</span></span> ), we build a negative example in
          <span class="inline-equation"><span class="tex">$\mathcal
          {T}^-$</span></span> (and resp. <span class=
          "inline-equation"><span class="tex">$\mathcal
          {B}^-$</span></span> ), by substituting one entity in
          <em>t</em> (resp. in <em>b</em>) with another random
          entity.</p>
          <p><strong>Relatedness Prediction Testing Sets</strong>
          <strong>(</strong> <span class=
          "inline-equation"><span class="tex">${\mathcal {B}}_{\bf
          test}$</span></span> <strong>and</strong> <span class=
          "inline-equation"><span class="tex">${\mathcal {B}}_{\bf
          test}^-$</span></span> <strong>)</strong> For the purpose
          of evaluating the relatedness prediction performance
          under the relatedness model, we constructed two
          classification testing sets for FB15K and JF17K. The
          positive testing set <span class=
          "inline-equation"><span class="tex">${\mathcal {B}}_{\bf
          test}$</span></span> is constructed from <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span></span> . For FB15K, <span class=
          "inline-equation"><span class="tex">${\mathcal {B}}_{\bf
          test}$</span></span> is constructed by simply removing
          the relation label for each triple in <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span>,</span> which turns the triples into pairs.
          That is, <span class="inline-equation"><span class=
          "tex">$|{\mathcal {T}}_{\bf test}|=|{\mathcal {B}}_{\bf
          test}|$</span></span> . For JF17K, we turn each
          <em>m</em>-fold instance in <span class=
          "inline-equation"><span class="tex">$t\in {\mathcal
          {T}}_{\bf test}$</span></span> into <span class=
          "inline-equation"><span class="tex">${m \atopwithdelims
          ()2}$</span></span> testing pairs by pairing every two
          possible entities involved in <em>t</em>. The negative
          testing set <span class="inline-equation"><span class=
          "tex">${\mathcal {B}}^-_{\bf test}$</span></span> is
          constructed by randomly replacing one entity for each
          pair in <span class="inline-equation"><span class=
          "tex">${\mathcal {B}}_{\bf test}$</span></span> .</p>
          <p><strong>Testing Set of Keys</strong>
          <strong>(</strong><span class=
          "inline-equation"><span class="tex">${\mathcal
          {K}}$</span></span> <strong>)</strong> For each dataset
          (FB15K and JF17K), we extract a testing set <span class=
          "inline-equation"><span class="tex">${\mathcal
          {K}}$</span></span> of keys. Given a relation
          <em>R<sub>r</sub></em> and any of its role <span class=
          "inline-equation"><span class="tex">$\rho \in {\mathcal
          {M}}(r)$</span>,</span> let <em>γ</em>(<em>ρ</em>;
          <em>r</em>) be the number of distinct entities that
          participate in an instance of <span class=
          "inline-equation"><span class="tex">$\mathcal
          {T}_r$</span></span> as role <em>ρ</em>. We define the
          role <em>ρ</em> <sup>*</sup> that maximizes
          <em>γ</em>(<em>ρ</em>; <em>r</em>) as the <em>primary
          role</em> for relation <em>R<sub>r</sub></em> . Table 2
          shows the statistics about testing keys by relation
          arity. Note that FB15K dataset only contains binary
          relations, so its relation arity is just 2. For a given
          instance <em>t</em> ∈ <em>R<sub>r</sub></em> , let
          <em>x</em> <sup>*</sup> be the entity in <em>t</em> that
          is corresponding to the primary role of
          <em>R<sub>r</sub></em> . The entity <em>x</em>
          <sup>*</sup> is then taken as the key for reconstructing
          <em>t</em>. For example, in Figure <a class="fig" href=
          "#fig1">1</a>, the primary role for relation
          <tt>PeopleMarriage</tt> is <tt>PERSON</tt>, since the
          number of distinct people participating in this relation
          as role <tt>PERSON</tt> is larger than the number of
          distinct entities participating the relation in each of
          the other roles. As such, KobeBryant is chosen as the
          key. Using this procedure, we extract distinct keys from
          <span class="inline-equation"><span class=
          "tex">${\mathcal {T}}_{\bf test}$</span></span> to form
          the set <span class="inline-equation"><span class=
          "tex">${\mathcal {K}}$</span></span> of testing keys.</p>
        </section>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> General
            Setup and Model Training</h3>
          </div>
        </header>
        <p>We train the proposed RAE model on both JF17K and FB15K
        while training a baseline TransH model only on FB15K. The
        dimension of the embedding space for all models is chosen
        as 100.</p>
        <section id="sec-24">
          <p><em>5.2.1 Training of RAE Model on JF17K and
          FB15K.</em> The embedding loss <em>f<sub>r</sub></em> in
          the objective function of the RAE model is chosen as the
          mTransH loss for JF17K and TransH loss for FB15K. In the
          MLP network of the relatedness model, the input layer and
          the hidden layer contain 200 and 400 neurons
          respectively.</p>
          <div class="table-responsive" id="tab1">
            <div class="table-caption">
              <span class="table-number">Table 1:</span>
              <span class="table-title">Statistics of
              Datasets.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;">Dataset</th>
                  <th style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {N}|$</span></span></th>
                  <th style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {R}|$</span></span></th>
                  <th style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {T}|$</span></span></th>
                  <th style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {B}|$</span></span></th>
                  <th style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {T}_{{\bf test}}|$</span></span></th>
                  <th style="text-align:left;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {B}_{{\bf test}}|$</span></span></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">FB15K</td>
                  <td style="text-align:left;">14921</td>
                  <td style="text-align:left;">1243</td>
                  <td style="text-align:left;">455101</td>
                  <td style="text-align:left;">455101</td>
                  <td style="text-align:left;">55721</td>
                  <td style="text-align:left;">55721</td>
                </tr>
                <tr>
                  <td style="text-align:left;">JF17K</td>
                  <td style="text-align:left;">28645</td>
                  <td style="text-align:left;">322</td>
                  <td style="text-align:left;">76379</td>
                  <td style="text-align:left;">171559</td>
                  <td style="text-align:left;">24568</td>
                  <td style="text-align:left;">66615</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab2">
            <div class="table-caption">
              <span class="table-number">Table 2:</span>
              <span class="table-title">Breakdown of testing keys
              by relation arity.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">Dataset</th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {K}(2)|$</span></span></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {K}(3)|$</span></span></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {K}(4)|$</span></span></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {K}(5)|$</span></span></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$|\mathcal
                  {K}(6)|$</span></span></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$|{\mathcal
                  {K}}|$</span></span></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">FB15k</td>
                  <td style="text-align:center;">43180</td>
                  <td style="text-align:center;">0</td>
                  <td style="text-align:center;">0</td>
                  <td style="text-align:center;">0</td>
                  <td style="text-align:center;">0</td>
                  <td style="text-align:center;">43180</td>
                </tr>
                <tr>
                  <td style="text-align:center;">JF17K</td>
                  <td style="text-align:center;">5199</td>
                  <td style="text-align:center;">3277</td>
                  <td style="text-align:center;">776</td>
                  <td style="text-align:center;">87</td>
                  <td style="text-align:center;">7</td>
                  <td style="text-align:center;">9346</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>In the training of the model, the algorithm loops over
          the three steps for 10 outer iterations. In each outer
          iteration:</p>
          <ul class="list-no-style">
            <li id="list6" label="•">The Step-1 update is run for
            100 epochs. In each epoch, (<em>Θ<sub>I</sub></em> ,
            <em>ϕ</em>) is updated using mini-batch stochastic
            gradient descent. Each mini-batch contains 1000 pairs
            of positive and negative examples (in <span class=
            "inline-equation"><span class="tex">$\mathcal
            {T}$</span></span> and <span class=
            "inline-equation"><span class="tex">$\mathcal
            {T}^-$</span></span> ). The margin <em>C</em> for both
            TransH and mTransH is set to 0.5.<br /></li>
            <li id="list7" label="•">The Step-2 update is run for
            10 epochs. In each epoch, <em>Θ<sub>C</sub></em> is
            updated by going through the entire positive and
            negative training examples (in <span class=
            "inline-equation"><span class="tex">$\mathcal
            {B}$</span></span> and <span class=
            "inline-equation"><span class="tex">$\mathcal
            {B}^-$</span></span> ) once using mini-batched Back
            Propagation&nbsp;[<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0010">10</a>]. Each mini-batch contains 1000
            pairs of positive and negative training examples. The
            learning rate is set as 0.005.<br />
            </li>
            <li id="list8" label="•">The Step-3 update is run for 5
            epochs. Each epoch is structured as mini-batches in the
            same way as those in the Step-2 updates. For each
            mini-batch, only the embedding vectors of the involved
            entities are updated. The learning rate is set as
            0.001.<br /></li>
          </ul>
        </section>
        <section id="sec-25">
          <p><em>5.2.2 Training of TransH Model on FB15K.</em> The
          training of the TransH model on FB15K is structured
          similarly as the Step-1 update in training the RAE model
          on FB15K, except that in total 1000 epochs are run.</p>
        </section>
      </section>
      <section id="sec-26">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Relatedness
            Prediction</h3>
          </div>
        </header>
        <p>Applying the learned parameter <em>Θ</em><sub>C</sub> of
        RAE to the relatedness model <em>p</em>(<em>x</em> ∼
        <em>y</em>), we investigate the model's
        relatedness-prediction performance on the testing sets
        <span class="inline-equation"><span class="tex">${\mathcal
        {B}}_{\bf test}$</span></span> and <span class=
        "inline-equation"><span class="tex">${\mathcal {B}}^-_{\bf
        test}$</span></span> , over the entire range of the
        acceptance threshold value <em>τ</em>. For a given value of
        acceptance threhold <em>τ</em>, the true positive rate
        TP(<em>τ</em>) and false positive rate FP(<em>τ</em>) are
        used as the performance criteria.</p>
        <p>The behaviour of TP(<em>τ</em>) and FP(<em>τ</em>) on
        FB15K and JF17K is plotted as the Receiver Operation
        Characteristics (ROC) curves (namely, the trajectory of
        point (TP(<em>τ</em>),FP(<em>τ</em>)) as <em>τ</em>
        increases in [0, 1]) in Figure <a class="fig" href=
        "#fig6">4</a>. The small area above the ROC curves suggests
        an excellent prediction performance.</p>
        <p>Superimposed on the ROC plots in Figure <a class="fig"
        href="#fig6">4</a>, the value of <em>τ</em> is also plotted
        against its corresponding FP(<em>τ</em>). Observing that
        the TP and FP values at <em>τ</em> = 0.5 (Figure <a class=
        "fig" href="#fig6">4</a>), we conclude that <em>τ</em> =
        0.5 is a good choice and adopt it in the rest of the
        experiments.</p>
        <figure id="fig6">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186017/images/www2018-26-fig6.jpg"
          class="img-responsive" alt="Figure 6" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 6:</span>
            <span class="figure-title">Relatedness prediction
            performances. Left: FB15K, TP(0.5)=90.38%,
            FP(0.5)=4.37%. Right: JF17K, TP(0.5)=95.53%,
            FP(0.5)=5.44%.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.4</span> Link
            Prediction</h3>
          </div>
        </header>
        <p>In order to verify whether the RAE model affects the
        embedding performance of its component embedding model, we
        perform a set of link prediction tasks. The experiments are
        performed on both JF17K and FB15K. For each instance
        <span class="inline-equation"><span class="tex">$t \in
        {\mathcal {T}}_{\bf test}$</span></span> and each
        <em>ρ</em> of the instance, we extract a key <em>t</em>
        <sup>*</sup> by making <em>ρ</em> the query role. This
        gives us a set of keys for link prediction tasks. We then
        train separately a standard embedding model (TransH for
        FB15K and mTransH for JF17K), and use the model to obtain
        the link prediction performance. We also take the embedding
        model from the trained RAE model and obtain its link
        prediction performance. The standard Mean Rank and HIT@10
        metrics are used as performance measures.</p>
        <p>From Table &nbsp;<a class="tbl" href="#tab3">3</a>, it
        can be observed that the incorporation of the relatedness
        model does not degrade the traditional TransH/mTransH
        performance. In some cases, the HIT@10 and Mean Rank are
        even improved.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Link Prediction Performance.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <strong>FB15K</strong>
                  <hr />
                </th>
                <th colspan="2" style="text-align:center;">
                  <strong>JK17K</strong>
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;"></th>
                <th style="text-align:center;">
                <strong>TransH</strong></th>
                <th style="text-align:center;">
                <strong>RAE</strong></th>
                <th style="text-align:center;">
                <strong>mTransH</strong></th>
                <th style="text-align:center;">
                <strong>RAE</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">HIT@10</td>
                <td style="text-align:center;">44.30<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
                <td style="text-align:center;">44.27<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
                <td style="text-align:center;">49.74<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
                <td style="text-align:center;">50.35<span class=
                "inline-equation"><span class=
                "tex">$\%$</span></span></td>
              </tr>
              <tr>
                <td style="text-align:center;">Mean Rank</td>
                <td style="text-align:center;">171.24</td>
                <td style="text-align:center;">171.03</td>
                <td style="text-align:center;">237.82</td>
                <td style="text-align:center;">229.27</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-28">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.5</span> Instance
            Reconstruction</h3>
          </div>
        </header>
        <p>In our instance-reconstruction experiments, we are
        interested in investigating the complexity reduction
        attained by the SIR algorithm in knowledge base containing
        multi-fold relations. This makes JF17K our primary test
        bench. Additionally we wish to validate the performance of
        the SIR algorithm by comparing it with the baseline
        reconstruction algorithm that solely relies on KB embedding
        (mTransH). The latter is however infeasible since the
        baseline algorithm entails prohibitive complexity when
        applied to JF17K data. We then use FB15K for this purpose,
        on which the baseline algorithm is much less costly (note
        that although the FB15K dataset is binary in nature, the
        instance reconstruction problem is still well defined,
        except that it reduces to the link prediction problem). So
        we perform three instance-reconstruction experiments: SIR
        reconstruction on JF17K, SIR reconstruction on FB15K, and
        TransH-based reconstruction on FB15K. Note that in the
        TransH-based reconstruction for each key entity <em>x</em>
        <sup>*</sup>, the candidate triples are evaluated using the
        TransH loss function.</p>
        <p>For any dataset with the set <span class=
        "inline-equation"><span class="tex">${\mathcal
        {K}}$</span></span> of reconstruction keys, each algorithm
        under comparison outputs a ranked list
        <em>L</em>(<em>x</em> <sup>*</sup>) of candidate tuples for
        each key <span class="inline-equation"><span class=
        "tex">$x^*\in {\mathcal {K}}$</span></span> . For SIR,
        <em>L</em>(<em>x</em> <sup>*</sup>) is defined as in
        Algorithm 1, which may be denoted by <em>L</em>
        <sup><strong>SIR</strong></sup> (<em>x</em> <sup>*</sup>),
        whenever required, to distinguish from the list <em>L</em>
        <sup><strong>mTransH</strong></sup> (<em>x</em>
        <sup>*</sup>) or <em>L</em>
        <sup><strong>TransH</strong></sup> (<em>x</em>
        <sup>*</sup>) produced by the baseline algorithm. Note that
        <em>L</em> <sup><strong>mTransH</strong></sup> (<em>x</em>
        <sup>*</sup>) and <em>L</em>
        <sup><strong>TransH</strong></sup> (<em>x</em>
        <sup>*</sup>) contain all length-<span class=
        "inline-equation"><span class="tex">$|{\mathcal
        {M}}(r)|$</span></span> tuples in which the entity at role
        <em>ρ</em> <sup>*</sup> is <em>x</em> <sup>*</sup>.</p>
        <p>It is remarkable that the output lists
        <em>L</em>(<em>x</em> <sup>*</sup>) from the compared
        algorithms differ significantly in length and in content
        and that the ranking is done within their own respective
        lists. Thus one must be cautious about the performance
        metric used to compare the lists. Using usual metrics such
        as HIT@K or Mean Rank to directly compare the lists, which
        in fact favours the shorter list (namely <em>L</em>
        <sup><strong>SIR</strong></sup> (<em>x</em> <sup>*</sup>)),
        fails to give a fair comparison.</p>
      </section>
      <section id="sec-29">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.6</span> Instance
            Reconstruction Performance</h3>
          </div>
        </header>
        <section id="sec-30">
          <p><em>5.6.1 Performance Metrics.</em> For each key
          <em>x</em> <sup>*</sup>, the output of each
          reconstruction algorithm is a ranked list
          <em>L</em>(<em>x</em> <sup>*</sup>). We now introduce a
          generalized notion of rank for key <em>x</em>
          <sup>*</sup>, then explain what it means when it takes a
          particular form.</p>
          <p>Let <span class="inline-equation"><span class=
          "tex">${\mathcal {W}}(x^*)$</span></span> be a subset of
          <span class="inline-equation"><span class=
          "tex">${\mathcal {T}}_{\bf test}$</span></span> . For
          each key <span class="inline-equation"><span class=
          "tex">$x^*\in {\mathcal {K}}$</span>,</span> its average
          rank with respect to <span class=
          "inline-equation"><span class="tex">${\mathcal
          {W}}(x^*)$</span></span> is naturally defined as</p>
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} {\bf
              RNK}(x^*; {\mathcal {W}}(x^*))=1+\frac{1}{|{\mathcal
              {W}}(x^*)|}\sum \limits _{t\in {\mathcal {W}}(x^*)}
              l(t) \end{equation}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>where <em>l</em>(<em>t</em>) is the number of
          instances in <em>L</em>(<em>x</em> <sup>*</sup>) which
          have a rank value smaller (i.e., better) than the correct
          answer <em>t</em> and which are neither in <span class=
          "inline-equation"><span class="tex">${\mathcal
          {T}}$</span></span> nor in <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span></span> . Note that the tuples that are
          neither in <span class="inline-equation"><span class=
          "tex">${\mathcal {T}}$</span></span> nor in <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span></span> are incorrect answers if we assume
          that <span class="inline-equation"><span class=
          "tex">${\mathcal {T}}$</span></span> and <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span></span> contain all correct answers (in the
          sense of reflecting the ground truth) for the
          reconstruction tasks. Then <span class=
          "inline-equation"><span class="tex">${\bf RNK}(x^*;
          {\mathcal {W}}(x^*))-1$</span></span> measures on average
          how many incorrect answers are put ahead of the “correct
          answers” in list <em>L</em>(<em>x</em> <sup>*</sup>).
          <p></p>
          <p>In this work, we will take <span class=
          "inline-equation"><span class="tex">${\mathcal
          {W}}(x^*)$</span></span> in the following three
          forms.</p>
          <ul class="list-no-style">
            <li id="list9" label="•">“<strong>SIR-Covered</strong>”
            form, namely, <span class=
            "inline-equation"><span class="tex">${\mathcal
            {W}}(x^*) = {\mathcal {T}}_{\bf test} \cap L^{\bf
            SIR}(x^*)$</span></span> . In this case, only the
            testing instances that are included in SIR's list are
            regarded as “correct answers”.<br /></li>
            <li id="list10" label="•">“<strong>SIR-Missed</strong>”
            form, namely, <span class=
            "inline-equation"><span class="tex">${\mathcal
            {W}}(x^*) = {\mathcal {T}}_{\bf test} \setminus
            {L}^{\bf SIR}(x^*)$</span></span> . In this case, only
            the testing instances that are excluded in SIR's list
            are regarded as “correct answers”.<br /></li>
            <li id="list11" label="•">“<strong>All</strong>” form,
            namely, <span class="inline-equation"><span class=
            "tex">${\mathcal {W}}(x^*) = {\mathcal {T}}_{\bf
            test}$</span></span> . In this case, all testing
            instances are regarded as “correct answers”.<br /></li>
          </ul>
          <p>Further suppose that <span class=
          "inline-equation"><span class="tex">${\mathcal
          {W}}=\lbrace {\mathcal {W}}(x^*): x^*\in {\mathcal
          {K}}\rbrace$</span></span> . Then we define the mean rank
          <span class="inline-equation"><span class=
          "tex">$\overline{\bf RNK}({\mathcal {W}})$</span></span>
          with respect to <span class=
          "inline-equation"><span class="tex">$\mathcal
          {W}$</span></span> as</p>
          <div class="table-responsive" id="Xeq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \overline{\bf RNK}({\mathcal
              {W}})=\frac{1}{|{\mathcal {K}}|}\sum \limits _{x^*\in
              {\mathcal {K}}} {\bf RNK}(x^*; {\mathcal {W}}(x^*))
              \end{equation}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>where <span class="inline-equation"><span class=
          "tex">${\mathcal {W}}(x^*)$</span></span> can take any of
          the above three forms.
          <p></p>
          <p>The <span class="inline-equation"><span class=
          "tex">${\bf TopK} ({\mathcal {W}})$</span></span>
          performance with respect to <span class=
          "inline-equation"><span class="tex">${\mathcal
          {W}}$</span></span> is defined as the fraction of keys
          <em>x</em> <sup>*</sup> in <span class=
          "inline-equation"><span class="tex">${\mathcal
          {K}}$</span></span> with <span class=
          "inline-equation"><span class="tex">${\bf RNK}(x^*;
          {\mathcal {W}}(x^*))\le K$</span></span> . We will
          primarily use the <strong>TopK</strong> and <span class=
          "inline-equation"><span class="tex">$\overline{\bf
          RNK}$</span></span> metrics to measure and compare the
          performance of SIR and its baseline algorithms. We note
          that these metrics are only well defined for an algorithm
          when <span class="inline-equation"><span class=
          "tex">${\mathcal {W}}(x^*)$</span></span> is a subset of
          the algorithm's output list <em>L</em>(<em>x</em>
          <sup>*</sup>).</p>
          <p>To measure to what extent the candidate tuple lists
          produced by SIR cover the testing set <span class=
          "inline-equation"><span class="tex">${\mathcal {T}}_{\bf
          test}$</span>,</span> we also define a notion <em>SIR
          Coverage</em>, or <strong>Coverage</strong>, as</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ {\bf Coverage}:=
              \frac{\left|{\mathcal {T}}_{\bf test} \cap
              \left(\bigcup _{x^*\in {\mathcal {K}}} L^{\bf
              SIR}(x^*)\right)\right|}{\big |{\mathcal {T}}_{\bf
              test}\big |}. \]</span><br />
            </div>
          </div>
          <p></p>
        </section>
        <section id="sec-31">
          <p><em>5.6.2 Instance Reconstruction Performance
          Result.</em> The performance of SIR algorithm (overall
          and breakdown according to relation arity) on JF17K is
          shown in Table <a class="tbl" href="#tab4">4</a>. The
          <span class="inline-equation"><span class=
          "tex">$\overline{\bf RNK}$</span></span> performance
          might have appeared poorer than usually those seen in
          link prediction over binary relations. This is merely a
          consequence of much larger number of tuples existing as
          possible candidates for reconstruction in multi-fold
          relations. Notably, however, the <strong>TopK</strong>
          performance appear quite impressive. For example,
          reconstruction from 19.24% of the keys ranks the correct
          instances at the very top of its list; reconstruction
          from 41.29% of the keys ranks the correct instances
          within top 10 of its list. Additionally, the candidate
          list produced by the SIR algorithm covers 88.54% of the
          correct answers (i.e., testing instances).</p>
          <p>Nonetheless, from Table <a class="tbl" href=
          "#tab4">4</a> alone, no conclusion can be drawn whether
          SIR algorithm suffers from a loss of performance relative
          to more naive baseline. Such a conclusion can however be
          drawn from experiments on the FB15k dataset, as seen in
          Table <a class="tbl" href="#tab5">5</a>.</p>
          <div class="table-responsive" id="tab4">
            <div class="table-caption">
              <span class="table-number">Table 4:</span>
              <span class="table-title">Instance Reconstruction
              Performance of SIR on JF17K. <span class=
              "inline-equation"><span class="tex">${\mathcal
              {W}}(x^*)$</span></span> in SIR-Covered form.
              Coverage = 88.54%.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">arity</th>
                  <th style="text-align:center;">
                  <strong>Top1</strong></th>
                  <th style="text-align:center;">
                  <strong>Top10</strong></th>
                  <th style="text-align:center;">
                  <strong>Top20</strong></th>
                  <th style="text-align:center;">
                  <strong>Top50</strong></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class=
                  "tex">$\overline{\bf RNK}$</span></span></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">2</td>
                  <td style="text-align:center;">25.07%</td>
                  <td style="text-align:center;">52.68%</td>
                  <td style="text-align:center;">60.45%</td>
                  <td style="text-align:center;">74.63%</td>
                  <td style="text-align:center;">47.63</td>
                </tr>
                <tr>
                  <td style="text-align:center;">3</td>
                  <td style="text-align:center;">12.42%</td>
                  <td style="text-align:center;">27.29%</td>
                  <td style="text-align:center;">32.52%</td>
                  <td style="text-align:center;">41.21%</td>
                  <td style="text-align:center;">610.09</td>
                </tr>
                <tr>
                  <td style="text-align:center;">4</td>
                  <td style="text-align:center;">11.56%</td>
                  <td style="text-align:center;">29.48%</td>
                  <td style="text-align:center;">34.67%</td>
                  <td style="text-align:center;">41.88%</td>
                  <td style="text-align:center;">34844.11</td>
                </tr>
                <tr>
                  <td style="text-align:center;">5</td>
                  <td style="text-align:center;">2.78%</td>
                  <td style="text-align:center;">8.33%</td>
                  <td style="text-align:center;">9.72%</td>
                  <td style="text-align:center;">13.89%</td>
                  <td style="text-align:center;">10229.41</td>
                </tr>
                <tr>
                  <td style="text-align:center;">6</td>
                  <td style="text-align:center;">20.00%</td>
                  <td style="text-align:center;">60.00%</td>
                  <td style="text-align:center;">80.00%</td>
                  <td style="text-align:center;">100.00%</td>
                  <td style="text-align:center;">10.80</td>
                </tr>
                <tr>
                  <td style="text-align:center;">overall</td>
                  <td style="text-align:center;">19.24%</td>
                  <td style="text-align:center;">41.29%</td>
                  <td style="text-align:center;">47.89%</td>
                  <td style="text-align:center;">59.45%</td>
                  <td style="text-align:center;">2981.81</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab5">
            <div class="table-caption">
              <span class="table-number">Table 5:</span>
              <span class="table-title">Instance
              reconstruction(FB15K): performance of SIR and its
              baseline. Coverage=88.73%.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;"></th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class="tex">$\mathcal
                  {W}(x^{*})$</span></span></th>
                  <th style="text-align:center;">TransH</th>
                  <th style="text-align:center;">SIR</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">
                  <strong>Top10</strong></td>
                  <td style="text-align:center;">SIR-Covered</td>
                  <td style="text-align:center;">83.37%</td>
                  <td style="text-align:center;">92.23%</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">SIR-Missed</td>
                  <td style="text-align:center;">42.00%</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">All</td>
                  <td style="text-align:center;">78.09%</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;">
                  <strong>Top1</strong></td>
                  <td style="text-align:center;">SIR-Covered</td>
                  <td style="text-align:center;">0.38%</td>
                  <td style="text-align:center;">50.45%</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">SIR-Missed</td>
                  <td style="text-align:center;">3.78%</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">All</td>
                  <td style="text-align:center;">0.85%</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;"><span class=
                  "inline-equation"><span class=
                  "tex">$\overline{\bf RNK}$</span></span></td>
                  <td style="text-align:center;">SIR-Covered</td>
                  <td style="text-align:center;">10.96</td>
                  <td style="text-align:center;">4.15</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">SIR-Missed</td>
                  <td style="text-align:center;">252.55</td>
                  <td style="text-align:center;">-</td>
                </tr>
                <tr>
                  <td style="text-align:center;"></td>
                  <td style="text-align:center;">All</td>
                  <td style="text-align:center;">41.13</td>
                  <td style="text-align:center;">-</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>In Table <a class="tbl" href="#tab5">5</a>, the
          reconstruction performance of SIR is compared with its
          baseline TransH-based algorithm on FB15K. With respect to
          the SIR-Covered “correct answers”, SIR improves upon the
          baseline by a large margin in both <strong>TopK</strong>
          and <span class="inline-equation"><span class=
          "tex">$\overline{\bf RNK}$</span></span> metrics.
          Specifically, SIR reconstruction from over 50% of the
          keys ranks the “correct answers” at the very top of its
          list, contrasting the 0.38% <strong>Top1</strong>
          performance of TransH. In addition, SIR reconstruction
          ranks the “correct answers” at top 4.15 on average,
          whereas the TransH baseline ranks them at top 10.96 on
          average. Of course, SIR misses some “correct answers”. It
          is interesting to note however that with respect to these
          SIR-Missed answers, the TransH baseline also ranks them
          much more unfavourably than it does with the SIR-Covered
          answers. For example, it ranks SIR-Missed “correct
          answers” at top 252.55 in its list on average,
          significantly contrasting its <span class=
          "inline-equation"><span class="tex">$\overline{\bf
          RNK}$</span></span> performance of 10.96 on the
          SIR-Covered answers. That is, even though SIR misses some
          answers, which is a necessary tradeoff due to filtering,
          these answers are also unlikely to be selected by the
          baseline algorithm. On the other hand, for the answers
          that favoured by the baseline algorithm, SIR further
          promotes them on average.</p>
          <p>Although reconstruction performance advantage of SIR
          over TransH demonstrated in Table <a class="tbl" href=
          "#tab5">5</a> is observed on the FB15K dataset, one may
          expect that it extrapolates reasonably well to JF17K,
          since both datasets are extracted from the same knowledge
          base, Freebase.</p>
          <p>From these results, we conclude that instance
          reconstruction by the SIR algorithm does not appear to
          entail any performance loss. But it offers significant
          complexity advantage as we will show next.</p>
        </section>
      </section>
      <section id="sec-32">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.7</span> Instance
            Reconstruction Complexity: Analysis and Experimental
            Results</h3>
          </div>
        </header>
        <p>In this work, we are not only interested in
        characterizing the complexity of the proposed SIR
        algorithm, but we would also like to compare it against
        other baseline methods. Noting that SIR contains two key
        filtering mechanisms for complexity reduction, the
        schema-based filtering and relatedness filtering, we then
        choose the following baseline for complexity comparison:
        reconstruction with no filtering (NF), reconstruction with
        schema-based filtering only (SCH), and reconstruction with
        relatedness filtering only (REL).</p>
        <section id="sec-33">
          <p><em>5.7.1 Complexity Metrics for Instance
          Reconstruction.</em> For large knowledge bases, the
          computational complexity of instance reconstruction, both
          for the SIR algorithm and for the base embedding model is
          nearly exclusively dominated by splicing the candidate
          pairs into candidate instances and by evaluating the
          embedding loss for these tuples. This is because forming
          the candidate pairs requires a complexity of order no
          more than <em>N</em> <sup>2</sup>, whereas for splicing
          and loss evaluation, the complexity order is a
          high-degree polynomial in <em>N</em>. For example, in our
          experiments on JF17K dataset, the SIR algorithm
          (Algorithm 1) spends more than 99% of its total
          computation time on executing the SPLICE and RANK
          subroutines. Thus, the primary complexity measures are
          those concerning pair splicing and loss evaluation.</p>
          <p>As such, we define two fundamental notions of
          complexity measure: the splicing complexity
          (<strong>CmpSp</strong>) and the Evaluation Complexity
          (<strong>CmpEv</strong>). In reconstructing an instance
          in relation <em>R<sub>r</sub></em> from a given key
          <em>x</em> <sup>*</sup>, the splicing complexity
          <strong>CmpSp</strong>(<em>x</em> <sup>*</sup>) is
          defined as the total number of <span class=
          "inline-equation"><span class="tex">$|{\mathcal
          {M}}(r)|$</span></span> -tuples that need to be
          <em>examined</em> in order to produce the candidate
          instances set; the evaluation complexity
          <strong>CmpEv</strong>(<em>x</em> <sup>*</sup>) is
          defined as the total number of candidate instances that
          need to be evaluated under the embedding loss
          function.</p>
          <p>The two respective notions of complexity can be
          extended to various levels, which we now define:</p>
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} {\bf
              CmpX}(m): = \sum \limits _{x^*\in {\mathcal {K}}(m)}
              {\bf CmpX}(x^*) \end{equation}</span><br />
              <span class="equation-number">(9)</span>
            </div>
          </div>
          <div class="table-responsive" id="eq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} {\bf CmpX}:
              = \sum \limits _{m} {\bf CmpX}(m)
              \end{equation}</span><br />
              <span class="equation-number">(10)</span>
            </div>
          </div>
          <p></p>
          <p>where “X” can be either “<strong>Sp</strong>” or
          “<strong>Ev</strong>”. Obviously
          <strong>CmpX</strong>(<em>x</em> <sup>*</sup>),
          <strong>CmpX</strong>(<em>m</em>), and
          <strong>CmpX</strong> measure respectively the
          complexities for a particular reconstruction task
          <em>x</em> <sup>*</sup>, for all reconstruction tasks in
          <em>m</em>-fold relations, and for the entire set of
          reconstruction tasks.</p>
          <p>By definition, it is obvious that the various notions
          of Splicing Complexity are upper bounds of the respective
          notions of Evaluation Complexity.</p>
        </section>
        <section id="sec-34">
          <p><em>5.7.2 Instance Reconstruction: Complexity
          Analysis.</em> Using above notions of complexity, we now
          analyze the reconstruction complexities for NF (no
          filtering), SCH (schema-based filtering only), REL
          (relatedness filtering only) and SIR.</p>
          <div class="lemma" id="enc3">
            <label>Lemma 3.</label>
            <p>For NF, the Splicing Complexity and Evaluation
            Complexity coincide, which will be both denoted by
            <strong>CmpX</strong><sup><strong>NF</strong></sup> .
            Then</p>
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \begin{array}{lll}\bf
                CmpX^{\bf NF}(x^*) &amp;= &amp;N^{m-1}\\
                \end{array} \]</span><br />
              </div>
            </div>
            <p></p>
          </div>
          <p>Let <span class="inline-equation"><span class=
          "tex">${\mathcal {N}}(x^*; \rho)$</span></span> be that
          defined in Algorithm 1, namely, <span class=
          "inline-equation"><span class="tex">${\mathcal {N}}(x^*;
          \rho)$</span></span> is the set of all entities left for
          role <em>ρ</em> after schema-based filtering. Then the
          complexities of SCH can be expressed using this term.</p>
          <div class="lemma" id="enc4">
            <label>Lemma 4.</label>
            <p>For SCH, the Splicing Complexity and Evaluation
            Complexity coincide, which will be both denoted by
            <strong>CmpX</strong><sup><strong>SCH</strong></sup> .
            Then</p>
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \begin{array}{lll}\bf
                CmpX^{\bf SCH}(x^*) &amp;= &amp;\prod \limits
                _{\rho \in {\mathcal {M}}(r)\setminus \lbrace \rho
                ^*\rbrace }|{\mathcal {N}}(x^*; \rho)|\\
                \end{array} \]</span><br />
              </div>
            </div>
            <p></p>
          </div>
          <p>To determine the complexity for REL, for each non-key
          role <span class="inline-equation"><span class=
          "tex">$\rho \in {\mathcal {M}}(r)\setminus \lbrace \rho
          ^*\rbrace$</span>,</span> let <span class=
          "inline-equation"><span class="tex">$\widehat{\mathcal
          {N}}(x^*)$</span></span> be the set of all entities that
          are predicted to be related to <em>x</em> <sup>*</sup>
          after applying the relatedness filtering. Let
          <span class="inline-equation"><span class=
          "tex">$\widehat{\mathcal {P}}(x^*):=$</span></span>
          REL-FILTER(<span class="inline-equation"><span class=
          "tex">$\widehat{\mathcal {N}}(x^*)$</span>,</span>
          <span class="inline-equation"><span class=
          "tex">$\widehat{\mathcal {N}}(x^*)$</span></span> ). That
          is, <span class="inline-equation"><span class=
          "tex">$\widehat{\mathcal {P}}(x^*)$</span></span> is the
          set of all related entity pairs (<em>a</em>, <em>b</em>)
          where both <em>a</em> and <em>b</em> are related to
          <em>x</em> <sup>*</sup>. Interpreting <span class=
          "inline-equation"><span class="tex">$\widehat{\mathcal
          {P}}(x^*)$</span></span> as an undirected graph where
          each pair <span class="inline-equation"><span class=
          "tex">$(a, b) \in \widehat{\mathcal
          {P}}(x^*)$</span></span> is interpreted as an edge
          connecting entities (vertex) <em>a</em> and <em>b</em>,
          we will use <span class="inline-equation"><span class=
          "tex">${\mathcal {J}}(x^*, k; \widehat{\mathcal
          {P}}(x^*))$</span></span> to denote the set of all
          cliques in <span class="inline-equation"><span class=
          "tex">$\widehat{\mathcal {P}}(x^*)$</span></span> having
          <em>k</em> vertices.</p>
          <div class="lemma" id="enc5">
            <label>Lemma 5.</label>
            <p>For REL,</p>
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \begin{array}{lcl}\bf
                CmpSp^{\bf REL} (x^*) &amp; = &amp; \prod \limits
                _{\rho \in {\mathcal {M}}(r)\setminus \lbrace \rho
                ^*\rbrace }|\widehat{\mathcal {N}}(x^*)|\\
                \end{array} \]</span><br />
              </div>
            </div>
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \begin{array}{lcl}\bf
                CmpEv^{\bf REL}(x^*) &amp; = &amp; \Big |{\mathcal
                {J}}(x^*, |{\mathcal {M}}(r)|-1; \widehat{\mathcal
                {P}}(x^*))\Big |\\ \end{array} \]</span><br />
              </div>
            </div>
            <p></p>
          </div>
          <p>Let <span class="inline-equation"><span class=
          "tex">${\mathcal {I}}(x^*)$</span></span> be that defined
          in Algorithm 1, namely, <span class=
          "inline-equation"><span class="tex">${\mathcal
          {I}}(x^*)$</span></span> is the set of all candidate
          tuples resulted from SPLICE in the SIR algorithm.</p>
          <div class="lemma" id="enc6">
            <label>Lemma 6.</label>
            <p>For SIR,</p>
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \begin{array}{lcl}\bf
                CmpSp^{\bf SIR}(x^*) &amp; = &amp; \prod \limits
                _{\rho \in {\mathcal {M}}(r)\setminus \lbrace \rho
                ^*\rbrace } \left|\widetilde{\mathcal {N}}(x^*;
                \rho)\right|\\ \end{array} \]</span><br />
              </div>
            </div>
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \begin{array}{lcl}\bf
                CmpEv^{\bf SIR}(x^*) &amp; = &amp; |{\mathcal
                {I}}(x^*)|\\ \end{array} \]</span><br />
              </div>
            </div>
            <p></p>
          </div>
          <p>Since we have determined
          <strong>CmpX</strong>(<em>x</em> <sup>*</sup>) through
          the above lemmas, we can further get
          <strong>CmpX</strong>(<em>m</em>) and
          <strong>CmpX</strong> by definition (<a class="eqn" href=
          "#eq6">9</a>) and (<a class="eqn" href="#eq7">10</a>).
          The computation of the complexities of these algorithms
          usually requires counting the filtering results (except
          for the case of NF). As such, we will investigate the
          complexity comparison of these algorithms in our
          experiments.</p>
          <p>Among the four algorithms, NF is clearly the most
          naive baseline, comparing with which all other three
          algorithms have reduced complexity. It is important to
          note however that the complexity reduction attained by
          other algorithms, or by any algorithm is not via reducing
          the order of the polynomial complexity of NF. The
          complexity reduction is via significantly reducing the
          coefficients of the polynomial, or alternatively put, via
          “effectively” reducing the size of the KB. We now
          elaborate on this perspective.</p>
          <p>Now we can re-express
          <strong>CmpSp</strong><sup><strong>A</strong></sup> in
          the form of</p>
          <div class="table-responsive" id="eq8">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} {\bf
              CmpSp}^{\bf A} = \sum \limits _m |{\mathcal {K}}(m)|
              \big (\epsilon ^{\bf A}_m N\big)^{m-1}
              \end{equation}</span><br />
              <span class="equation-number">(11)</span>
            </div>
          </div>to match the polynomial expression <span class=
          "inline-equation"><span class="tex">$\sum \limits _m
          |{\mathcal {K}}(m)| N^{m-1}$</span></span> of NF
          complexity in Lemma <a class="enc" href="#enc3">3</a>.
          The term <span class="inline-equation"><span class=
          "tex">$\epsilon ^{\bf A}_m$</span></span> in the equation
          above has an interesting interpretation: <em>as far as
          <em>m</em>-fold relations are concerned, the KB size is
          effectively scaled down by a factor of</em> <span class=
          "inline-equation"><span class="tex">$\epsilon ^{\bf
          A}_m$</span></span> .
          <p></p>
          <div class="lemma" id="enc7">
            <label>Lemma 7.</label>
            <p>Let <em>m</em> be an arbitrary arity value. If in
            each instance reconstruction task from key <em>x</em>
            <sup>*</sup>, each <span class=
            "inline-equation"><span class="tex">$|{\mathcal
            {N}}^A(x^*; \rho)|$</span></span> is distributed
            independently and the distribution is independent of
            the relation <em>R<sub>r</sub></em> and <em>ρ</em>,
            then</p>
            <div class="table-responsive" id="eq9">
              <div class="display-equation">
                <span class="tex mytex">\begin{equation}
                \widetilde{\epsilon }^{\bf A}_m:=
                \frac{1}{|{\mathcal {K}}(m)|}\sum \limits _{x^*\in
                {\mathcal {K}}(m)} \left(\prod \limits _{\rho \in
                {\mathcal {M}}(r)\setminus (\rho ^*)}\frac{N^A(x^*;
                \rho)}{N} \right)^{\frac{1}{m-1}}
                \end{equation}</span><br />
                <span class="equation-number">(12)</span>
              </div>
            </div>
            <p></p>
            <p>is an unbiased estimate of <span class=
            "inline-equation"><span class="tex">$\epsilon ^{\bf
            A}_m$</span></span> .</p>
          </div>
          <p>Under this lemma, we may use <span class=
          "inline-equation"><span class="tex">$\widetilde{\epsilon
          }^{\bf A}_m$</span></span> to estimate by how much the
          algorithm A effectively reduces the KB size.</p>
        </section>
        <section id="sec-35">
          <p><em>5.7.3 Instance Reconstruction: Complexity Results
          (on JF17K).</em> In this subsection, we compare the
          complexity of SIR on JF17K with baselines that utilize
          schema-based filtering only (SCH), relatedness filtering
          only (REL), or no filtering (NF). The computation of the
          complexity metrics is based on the analytic tools
          developed above and data collected from experiments. In
          particular, complexity results for SIR and REL are
          obtained using the trained RAE model, whereas the results
          for NF and SCH are obtained using the base embedding
          model, mTransH.</p>
          <p>Tables <a class="tbl" href="#tab6">6</a> and <a class=
          "tbl" href="#tab7">7</a> report the Evaluation Complexity
          and Splicing Complexity for the compared schemes.
          Compared with the naive NF scheme, SCH, REL, and SIR
          reduce Evaluation Complexity by about 10<sup>11</sup>,
          10<sup>4</sup>, and 10<sup>13</sup> folds respectively.
          Similar orders of reduction are seen in Splicing
          Complexity. This suggests that schema-based filtering
          plays a more important role than relatedness filtering.
          Nonetheless both filtering schemes are required to make
          instance reconstruction complexity reduce to an
          affordable level.</p>
          <p>For NF, the complexity is dominated by highest-arity
          (arity-6) keys, although there are only 7 such keys
          (Table <a class="tbl" href="#tab2">2</a>). However when
          filtering schemes are applied, this dominance reduces. In
          fact, when both filtering schemes applied, namely, in
          SIR, arity-4 keys become dominant, noting that there are
          776 arity-4 keys, significantly more abundant than the
          arity-5 and arity-6 keys while also significantly scarcer
          than the arity-2 and arity-3 keys (Table <a class="tbl"
          href="#tab2">2</a>).</p>
          <p>The reduction factors <span class=
          "inline-equation"><span class="tex">$\widetilde{\epsilon
          }^{\bf SIR}_m$</span></span> shown in Table <a class=
          "tbl" href="#tab7">7</a> suggest that with respect to
          relations of various arities, SIR effectively reduces the
          KB size by a scaling factor of 10<sup>− 3</sup> to
          10<sup>− 2</sup>. But since arity-6 keys dominates the NF
          complexity, the overall reduction factor can be justified
          as close to 1.31 × 10<sup>− 3</sup>.</p>
          <div class="table-responsive" id="tab6">
            <div class="table-caption">
              <span class="table-number">Table 6:</span>
              <span class="table-title">Evaluation Complexity
              (JF17K).</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;">arity</th>
                  <th style="text-align:center;">NF</th>
                  <th style="text-align:center;">SCH</th>
                  <th style="text-align:center;">REL</th>
                  <th style="text-align:center;">SIR</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">2</td>
                  <td style="text-align:center;">1.49e+8</td>
                  <td style="text-align:center;">2.91e+6</td>
                  <td style="text-align:center;">1.96e+7</td>
                  <td style="text-align:center;">1.14e+6</td>
                </tr>
                <tr>
                  <td style="text-align:center;">3</td>
                  <td style="text-align:center;">2.69e+12</td>
                  <td style="text-align:center;">5.10e+8</td>
                  <td style="text-align:center;">3.19e+10</td>
                  <td style="text-align:center;">3.14e+7</td>
                </tr>
                <tr>
                  <td style="text-align:center;">4</td>
                  <td style="text-align:center;">1.82e+16</td>
                  <td style="text-align:center;">4.01e+11</td>
                  <td style="text-align:center;">≤ 4.76e+13</td>
                  <td style="text-align:center;">9.26e+9</td>
                </tr>
                <tr>
                  <td style="text-align:center;">5</td>
                  <td style="text-align:center;">5.86e+19</td>
                  <td style="text-align:center;">1.08e+11</td>
                  <td style="text-align:center;">≤ 2.18e+17</td>
                  <td style="text-align:center;">8.99e+7</td>
                </tr>
                <tr>
                  <td style="text-align:center;">6</td>
                  <td style="text-align:center;">1.35e+23</td>
                  <td style="text-align:center;">2.96e+12</td>
                  <td style="text-align:center;">≤ 3.27e+19</td>
                  <td style="text-align:center;">2.28e+6</td>
                </tr>
                <tr>
                  <td style="text-align:center;">overall</td>
                  <td style="text-align:center;">1.35e+23</td>
                  <td style="text-align:center;">3.47e+12</td>
                  <td style="text-align:center;">≤ 3.29e+19</td>
                  <td style="text-align:center;">9.38e+9</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="table-responsive" id="tab7">
            <div class="table-caption">
              <span class="table-number">Table 7:</span>
              <span class="table-title">Splicing Complexity
              (JF17K).</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:center;"><em>m</em></th>
                  <th style="text-align:center;">NF</th>
                  <th style="text-align:center;">SCH</th>
                  <th style="text-align:center;">REL</th>
                  <th style="text-align:center;">SIR</th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class=
                  "tex">$\tilde{\epsilon }_{m}^{\bf
                  SIR}$</span></span></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:center;">2</td>
                  <td style="text-align:center;">1.49e+8</td>
                  <td style="text-align:center;">2.91e+6</td>
                  <td style="text-align:center;">1.96e+7</td>
                  <td style="text-align:center;">1.14e+6</td>
                  <td style="text-align:center;">0.00766</td>
                </tr>
                <tr>
                  <td style="text-align:center;">3</td>
                  <td style="text-align:center;">2.69e+12</td>
                  <td style="text-align:center;">5.10e+8</td>
                  <td style="text-align:center;">4.61e+10</td>
                  <td style="text-align:center;">7.25e+7</td>
                  <td style="text-align:center;">0.00377</td>
                </tr>
                <tr>
                  <td style="text-align:center;">4</td>
                  <td style="text-align:center;">1.82e+16</td>
                  <td style="text-align:center;">4.01e+11</td>
                  <td style="text-align:center;">2.65e+14</td>
                  <td style="text-align:center;">6.65e+10</td>
                  <td style="text-align:center;">0.01122</td>
                </tr>
                <tr>
                  <td style="text-align:center;">5</td>
                  <td style="text-align:center;">5.86e+19</td>
                  <td style="text-align:center;">1.08e+11</td>
                  <td style="text-align:center;">2.74e+17</td>
                  <td style="text-align:center;">4.33e+9</td>
                  <td style="text-align:center;">0.00152</td>
                </tr>
                <tr>
                  <td style="text-align:center;">6</td>
                  <td style="text-align:center;">1.35e+23</td>
                  <td style="text-align:center;">2.96e+12</td>
                  <td style="text-align:center;">3.21e+20</td>
                  <td style="text-align:center;">2.13e+9</td>
                  <td style="text-align:center;">0.00131</td>
                </tr>
                <tr>
                  <td style="text-align:center;">overall</td>
                  <td style="text-align:center;">1.35e+23</td>
                  <td style="text-align:center;">3.47e+12</td>
                  <td style="text-align:center;">3.21e+20</td>
                  <td style="text-align:center;">7.31e+10</td>
                  <td style="text-align:center;">-</td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
      </section>
    </section>
    <section id="sec-36">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span>
          Conclusions</h2>
        </div>
      </header>
      <p>Relations in the real world are often multi-fold. Storing
      instances from multi-fold relations in their original
      (multi-fold) format has the advantage of better preserving
      their information content. We observe that knowledge bases
      (KB) containing multi-fold relations are however challenged
      by a new class of KB completion problem, the instance
      reconstruction problem. This paper formally introduces this
      problem and explains its complexity challenge. We also
      develop a new KB embedding model, called Relatedness
      Affiliated Embedding (RAE), and build on it a Scalable
      Instance Reconstruction (SIR) algorithm. We demonstrate
      experimentally that the SIR algorithm has significantly
      reduced complexity with virtually no loss of reconstruction
      performance.</p>
    </section>
    <section id="sec-37">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Acknowledgments</h2>
        </div>
      </header>
      <p>This work is supported partly by China 973 program (No.
      2014CB340 305, 2015CB358700), by the National Natural Science
      Foundation of China (No. 61772059, 61421003) and by the
      Beijing S&amp;T Committee (Z161100000216127). This paper is
      also supported by the State Key Laboratory of Software
      Development Environment of China and Beijing Advanced
      Innovation Center for Big Data and Brain Computing.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Sören Auer, Christian
        Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
        and Zachary Ives. 2007. <em><em>Dbpedia: A nucleus for a
        web of open data.</em></em> Springer.</li>
        <li id="BibPLXBIB0002" label="[2]">Kurt Bollacker, Colin
        Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
        2008. Freebase: a collaboratively created graph database
        for structuring human knowledge. In <em><em>Proceedings of
        the 2008 ACM SIGMOD international conference on Management
        of data.</em></em> ACM.</li>
        <li id="BibPLXBIB0003" label="[3]">Antoine Bordes, Xavier
        Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic
        matching energy function for learning with multi-relational
        data. <em><em>Machine Learning</em></em> 94, 2 (2014),
        233–259.</li>
        <li id="BibPLXBIB0004" label="[4]">Antoine Bordes, Nicolas
        Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana
        Yakhnenko. 2013. Translating embeddings for modeling
        multi-relational data. In <em><em>Advances in Neural
        Information Processing Systems.</em></em> 2787–2795.</li>
        <li id="BibPLXBIB0005" label="[5]">Antoine Bordes, Jason
        Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning
        structured embeddings of knowledge bases. In
        <em><em>Conference on Artificial
        Intelligence.</em></em></li>
        <li id="BibPLXBIB0006" label="[6]">David Liben-Nowell and
        Jon&nbsp;M. Kleinberg. 2007. The link-prediction problem
        for social networks. <em><em>JASIST</em></em> 58, 7 (2007),
        1019–1031.</li>
        <li id="BibPLXBIB0007" label="[7]">Yankai Lin, Zhiyuan Liu,
        Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. 2015.
        Modeling Relation Paths for Representation Learning of
        Knowledge Bases. In <em><em>Proceedings of the 2015
        Conference on Empirical Methods in Natural Language
        Processing.</em></em> Association for Computational
        Linguistics, Lisbon, Portugal.</li>
        <li id="BibPLXBIB0008" label="[8]">Yankai Lin, Zhiyuan Liu,
        Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity
        and Relation Embeddings for Knowledge Graph Completion. In
        <em><em>Proceedings of the Twenty-Ninth AAAI Conference on
        Artificial Intelligence</em></em> (<em>AAAI’15</em>).</li>
        <li id="BibPLXBIB0009" label="[9]">Alex Marin, Roman
        Holenstein, Ruhi Sarikaya, and Mari Ostendorf. 2014.
        Learning phrase patterns for text classification using a
        knowledge graph and unlabeled data. In <em><em>Fifteenth
        Annual Conference of the International Speech Communication
        Association.</em></em></li>
        <li id="BibPLXBIB0010" label="[10]">David&nbsp;E Rumelhart,
        Geoffrey&nbsp;E Hinton, and Ronald&nbsp;J Williams. 1988.
        Learning representations by back-propagating errors.
        <em><em>Cognitive modeling</em></em> 5, 3 (1988), 1.</li>
        <li id="BibPLXBIB0011" label="[11]">Richard Socher, Danqi
        Chen, Christopher&nbsp;D Manning, and Andrew Ng. 2013.
        Reasoning with neural tensor networks for knowledge base
        completion. In <em><em>Advances in Neural Information
        Processing Systems.</em></em> 926–934.</li>
        <li id="BibPLXBIB0012" label="[12]">Fabian&nbsp;M Suchanek,
        Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of
        semantic knowledge. In <em><em>Proceedings of the 16th
        international conference on World Wide Web.</em></em> ACM,
        697–706.</li>
        <li id="BibPLXBIB0013" label="[13]">Yi Tay, Anh&nbsp;Tuan
        Luu, Siu&nbsp;Cheung Hui, and Falk Brauer. 2017. Random
        Semantic Tensor Ensemble for Scalable Knowledge Graph Link
        Prediction. In <em><em>Proceedings of the Tenth ACM
        International Conference on Web Search and Data Mining,
        WSDM 2017, Cambridge, United Kingdom, February 6-10,
        2017.</em></em> 751–760.</li>
        <li id="BibPLXBIB0014" label="[14]">Zhen Wang, Jianwen
        Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph
        embedding by translating on hyperplanes. In
        <em><em>Proceedings of the Twenty-Eighth AAAI Conference on
        Artificial Intelligence.</em></em> 1112–1119.</li>
        <li id="BibPLXBIB0015" label="[15]">Jianfeng Wen, Jianxin
        Li, Yongyi Mao, Shini Chen, and Richong Zhang. 2016. On the
        Representation and Embedding of Knowledge Bases beyond
        Binary Relations. In <em><em>Proceedings of the
        Twenty-Fifth International Joint Conference on Artificial
        Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July
        2016.</em></em> 1300–1307.</li>
        <li id="BibPLXBIB0016" label="[16]">Han Xiao, Minlie Huang,
        and Xiaoyan Zhu. 2016. TransG : A Generative Model for
        Knowledge Graph Embedding. In <em><em>Proceedings of the
        54th Annual Meeting of the Association for Computational
        Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,
        Volume 1: Long Papers.</em></em></li>
        <li id="BibPLXBIB0017" label="[17]">Chenyan Xiong and Jamie
        Callan. 2015. EsdRank: Connecting Query and Documents
        Through External Semi-Structured Data. In
        <em><em>Proceedings of the 24th ACM International on
        Conference on Information and Knowledge
        Management</em></em> (<em>CIKM ’15</em>). ACM, New York,
        NY, USA, 951–960.</li>
        <li id="BibPLXBIB0018" label="[18]">Chenyan Xiong and Jamie
        Callan. 2015. Query Expansion with Freebase. In
        <em><em>Proceedings of the Fifth ACM International
        Conference on the Theory of Information
        Retrieval.</em></em> ACM.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186017">https://doi.org/10.1145/3178876.3186017</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
