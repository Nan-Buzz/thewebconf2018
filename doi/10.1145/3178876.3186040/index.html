<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Junwei</span>     <span class="surName">Pan</span>,     Oath Inc., <a href="mailto:jwpan@oath.com">jwpan@oath.com</a>    </div>    <div class="author">     <span class="givenName">Jian</span>     <span class="surName">Xu</span>,     TouchPal Inc., <a href="mailto:nayobux@gmail.com">nayobux@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Alfonso Lobos</span>     <span class="surName">Ruiz</span>,     UC Berkeley, <a href="mailto:alobos@berkeley.edu">alobos@berkeley.edu</a>    </div>    <div class="author">     <span class="givenName">Wenliang</span>     <span class="surName">Zhao</span>,     Oath Inc., <a href="mailto:wenliangz@oath.com">wenliangz@oath.com</a>    </div>    <div class="author">     <span class="givenName">Shengjun</span>     <span class="surName">Pan</span>,     Oath Inc., <a href="mailto:alanpan@oath.com">alanpan@oath.com</a>    </div>    <div class="author">     <span class="givenName">Yu</span>     <span class="surName">Sun</span>,     LinkedIn Corporation, <a href="mailto:ysun1@linkedin.com">ysun1@linkedin.com</a>    </div>    <div class="author">     <span class="givenName">Quan</span>     <span class="surName">Lu</span>,     Ablibaba Group, <a href="mailto:quan.lu@gmail.com">quan.lu@gmail.com</a>    </div>                                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186040" target="_blank">https://doi.org/10.1145/3178876.3186040</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Click-through rate (CTR) prediction is a critical task in online display advertising. The data involved in CTR prediction are typically multi-field categorical data, i.e., every feature is categorical and belongs to one and only one field. One of the interesting characteristics of such data is that features from one field often interact differently with features from different other fields. Recently, Field-aware Factorization Machines (FFMs) have been among the best performing models for CTR prediction by explicitly modeling such difference. However, the number of parameters in FFMs is in the order of feature number times field number, which is unacceptable in the real-world production systems. In this paper, we propose Field-weighted Factorization Machines (FwFMs) to model the different feature interactions between different fields in a much more memory-efficient way. Our experimental evaluations show that FwFMs can achieve competitive prediction performance with only as few as 4% parameters of FFMs. When using the same number of parameters, FwFMs can bring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Factorization methods;</strong> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Computational advertising;</strong> &#x2022;<strong> Theory of computation </strong>&#x2192; <em>Computational advertising theory;</em></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Display Advertising; CTR Prediction; Factorization Machines</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. 2018. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 9 Pages. <a href="https://doi.org/10.1145/3178876.3186040" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186040</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>Online display advertising is a multi-billion dollar business nowadays, with an annual revenue of 31.7 billion US dollars in fiscal year 2016, up 29% from fiscal year 2015&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. One of the core problems display advertising strives to solve is to deliver the right ads to the right people, in the right context, at the right time. Accurately predicting the click-through rate (CTR) is crucial to solve this problem and it has attracted much research attention in the past few years&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>].</p>    <p>The data involved in CTR prediction are typically <em>multi-field categorical data</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] which are also quite ubiquitous in many applications besides display advertising, such as recommender systems&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. Such data possess the following properties. First, all the features are categorical and are very sparse since many of them are identifiers. Therefore, the total number of features can easily reach millions or tens of millions. Second, every feature belongs to one and only one field and there can be tens to hundreds of fields. Table 1 is an example of a real-world multi-field categorical data set used for CTR prediction.</p>    <div class="table-responsive" id="tab1">    <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">An example of multi-field categorical data for CTR prediction. Each row is an ad impression. Column <tt>CLICK</tt> is the label indicating whether there is a click associated with this impression. Each of the rest columns is a field. Features are all categorical, e.g., <tt>Male</tt>, <tt>Female</tt>, <tt>Nike</tt>, <tt>Walmart</tt>, and each of them belongs to one and only one field, e.g., <tt>Male</tt> belongs to field <tt>GENDER</tt>, and <tt>Nike</tt> belongs to field <tt>ADVERTISER</tt>.</span>    </div>    <table class="table"> 			 <thead>      <tr>       <th style="text-align:center;">       <strong>CLICK</strong>       </th>       <th style="text-align:center;">       <strong>User_ID</strong>       </th>       <th style="text-align:center;">       <strong>GENDER</strong>       </th>       <th style="text-align:center;">       <strong>ADVERTISER</strong>       </th>       <th style="text-align:center;">       <strong>PUBLISHER</strong>       </th>      </tr> 			 </thead>     <tbody>      <tr>       <td style="text-align:center;">1</td>       <td style="text-align:center;">29127394</td>       <td style="text-align:center;">       <tt>Male</tt>       </td>       <td style="text-align:center;">       <tt>Nike</tt>       </td>       <td style="text-align:center;">       <tt>news.yahoo.com</tt>       </td>      </tr>      <tr>       <td style="text-align:center;">-1</td>       <td style="text-align:center;">89283132</td>       <td style="text-align:center;">       <tt>Female</tt>       </td>       <td style="text-align:center;">       <tt>Walmart</tt>       </td>       <td style="text-align:center;">       <tt>techcrunch.com</tt>       </td>      </tr>      <tr>       <td style="text-align:center;">-1</td>       <td style="text-align:center;">91213212</td>       <td style="text-align:center;">       <tt>Male</tt>       </td>       <td style="text-align:center;">       <tt>Gucci</tt>       </td>       <td style="text-align:center;">       <tt>nba.com</tt>       </td>      </tr>      <tr>       <td style="text-align:center;">-1</td>       <td style="text-align:center;">71620391</td>       <td style="text-align:center;">       <tt>Female</tt>       </td>       <td style="text-align:center;">       <tt>Uber</tt>       </td>       <td style="text-align:center;">       <tt>tripadviser.com</tt>       </td>      </tr>      <tr>       <td style="text-align:center;">1</td>       <td style="text-align:center;">39102740</td>       <td style="text-align:center;">       <tt>Male</tt>       </td>       <td style="text-align:center;">       <tt>Adidas</tt>       </td>       <td style="text-align:center;">       <tt>mlb.com</tt>       </td>      </tr>     </tbody>    </table>    </div>    <p>The properties of multi-field categorical data pose several unique challenges to building effective machine learning models for CTR prediction:</p>    <ol class="list-no-style">    <li id="list1" label="(1)"><strong>Feature interactions are prevalent and need to be specifically modeled</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. Feature conjunctions usually associate with the labels differently from individual features do. For example, the CTR of <tt>Nike</tt>&#x2019;s ads shown on <tt>nba.com</tt> is usually much higher than the average CTR of <tt>Nike</tt>&#x2019;s ads or the average CTR of ads shown on <tt>nba.com</tt>. This phenomenon is usually referred to as <em>feature interaction</em> in the literature&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. To avoid confusion and simplify discussion, in the rest of the paper, we unify and abuse the terminology <em>feature interaction strength</em> to represent the level of association between feature conjunctions and labels.<br/></li>    <li id="list2" label="(2)"><strong>Features from one field often interact differently with features from different other fields</strong>. For instance, we have observed that features from field <tt>GENDER</tt> usually have strong interactions with features from field <tt>ADVERTISER</tt> while their interactions with features from field <tt>DEVICE_TYPE</tt> are relatively weak. This might be attributed to the fact that users with a specific gender are more biased towards the ads they are viewing than towards the type of device they are using.<br/></li>    <li id="list3" label="(3)"><strong>Potentially high model complexity needs to be taken care of</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>]. The model parameters such as weights and embedding vectors need to be stored in memory in real-world production systems to enable real-time ad serving. As there are typically millions of features in practice, the model complexity needs to be carefully designed and tuned to fit the model into memory.<br/></li>    </ol>    <p>To resolve part of these challenges, researchers have built several solutions. Factorization Machines (FMs)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] and Field-aware Factorization Machines (FFMs)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>] are among the most successful ones. FMs tackle the first challenge by modeling the effect of pairwise feature interactions as the dot product of two embedding vectors. However, the field information is not leveraged in FMs at all. Recently, FFMs have been among the best performing models for CTR prediction and won two competitions hosted by Criteo and Avazu&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. FFMs learn different embedding vectors for each feature when the feature interacts with features from different other fields. In this way, the second challenge is explicitly tackled. However, the number of parameters in FFMs is in the order of feature number times field number, which can easily reach tens of millions or even more. This is unacceptable in real-world production systems. In this paper, we introduce Field-weighted Factorization Machines (FwFMs) to resolve all these challenges simultaneously. The main contributions of this paper can be summarized as follows:</p>    <ol class="list-no-style">    <li id="list4" label="(1)">Empirically we show that the average interaction strength of feature pairs from one field pair tends to be quite different from that of other field pairs. In other words, different field pairs have significantly different levels of association with the labels (i.e. clicks in CTR prediction). Following the same convention, we call this <em>field pair interactions</em>.<br/></li>    <li id="list5" label="(2)">Based on the above observation, we propose Field-weighted Factorization Machines (FwFMs). By introducing and learning a field pair weight matrix, FwFMs can effectively capture the heterogeneity of field pair interactions. Moreover, parameters in FwFMs are magnitudes fewer than those in FFMs, which makes FwFMs a preferable choice in real-world production systems.<br/></li>    <li id="list6" label="(3)">FwFMs are further augmented by replacing the binary representations of the linear terms with embedding vector representations. This novel treatment can effectively help avoid over-fittings and enhance prediction performance.<br/></li>    <li id="list7" label="(4)">We conduct comprehensive experiments on two real-world CTR prediction data sets to evaluate the performance of FwFMs against existing models. The results show that FwFMs can achieve competitive prediction performance with only as few as 4% parameters of FFMs. When using the same number of parameters, FwFMs outperform FFMs by up to 0.9% AUC lift.<br/></li>    </ol>    <p>The rest of the paper is organized as follows. Section&#x00A0;<a class="sec" href="#sec-9">2</a> provides the preliminaries of existing CTR prediction models that handle multi-field categorical data. In Section&#x00A0;<a class="sec" href="#sec-10">3</a>, we show that the interaction strengths of different field pairs are quite different, followed by detailed discussion of the proposed model in Section&#x00A0;<a class="sec" href="#sec-11">4</a>. Our experimental evaluation results are presented in Section&#x00A0;<a class="sec" href="#sec-14">5</a>. In Section&#x00A0;<a class="sec" href="#sec-24">6</a>, we show that FwFMs learn the field pair interaction strengths even better than FFMs. Section&#x00A0;<a class="sec" href="#sec-25">7</a> and Section&#x00A0;<a class="sec" href="#sec-26">8</a> discusses the related work and concludes the paper respectively.</p>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Preliminaries</h2>    </div>    </header>    <p>Logistic Regression (LR) is probably the most widely used machine learning model on multi-field categorical data for CTR prediction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>]. Suppose there are <em>m</em> unique features {<em>f</em>    <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>f<sub>m</sub>    </em>} and <em>n</em> different fields {<em>F</em>    <sub>1</sub>, &#x22C5;&#x22C5;&#x22C5;, <em>F<sub>n</sub>    </em>}. Since each feature belongs to one and only one field, to simplify notation, we use index <em>i</em> to represent feature <em>f<sub>i</sub>    </em> and <em>F</em>(<em>i</em>) to represent the field <em>f<sub>i</sub>    </em> belongs to. Given a data set S = {<em>y</em>    <sup>(<em>s</em>)</sup>, x<sup>(<em>s</em>)</sup>}, where <em>y</em>    <sup>(<em>s</em>)</sup> &#x2208; {1, &#x2212;1} is the label indicating click or non-click and x<sup>(<em>s</em>)</sup> &#x2208; {0, 1}<sup>     <em>m</em>    </sup> is the feature vector in which <span class="inline-equation"><span class="tex">$x_i^{(s)} = 1$</span>    </span> if feature <em>i</em> is active for this instance otherwise <span class="inline-equation"><span class="tex">$x_i^{(s)}=0$</span>    </span>, the LR model parameters w are estimated by minimizing the following regularized loss function: <div class="table-responsive" id="Xeq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \min _{{ w}}\;\lambda \Vert { w}\Vert _2^2 + \sum _{s=1}^{|S|}\log (1+\exp (-y^{(s)} \Phi _{LR}({ w}, { x}^{(s)}))) \end{equation} </span>      <br/>      <span class="equation-number">(1)</span>     </div>    </div> where <em>&#x03BB;</em> is the regularization parameter, and <div class="table-responsive" id="Xeq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \Phi _{LR}({ w}, { x}) = w_0 + \sum _{i=1}^m x_i w_i \end{equation} </span>      <br/>      <span class="equation-number">(2)</span>     </div>    </div> is a linear combination of individual features.</p>    <p>However, linear models are not sufficient for tasks such as CTR prediction in which feature interactions are crucial&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. A general way to address this problem is to add feature conjunctions. It has been shown that Degree-2 Polynomial (Poly2) models can effectively capture the effect of feature interactions[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. Mathematically, in the loss function of equation (1), Poly2 models consider replacing <em>&#x03A6;<sub>LR</sub>    </em> with <div class="table-responsive" id="Xeq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \Phi _{Poly2}({ w}, { x}) = w_0 + \sum _{i=1}^m x_i w_i + \sum _{i=1}^{m}\sum _{j=i+1}^m x_{i}x_{j}w_{h(i,j)} \end{equation} </span>      <br/>      <span class="equation-number">(3)</span>     </div>    </div> where <em>h</em>(<em>i</em>, <em>j</em>) is a function hashing <em>i</em> and <em>j</em> into a natural number in hashing space <em>H</em> to reduce the number of parameters. Otherwise the number of parameters in the model would be in the order of <em>O</em>(<em>m</em>    <sup>2</sup>).</p>    <p>Factorization Machines(FMs) learn an embedding vector <span class="inline-equation"><span class="tex">${\bf{\it v}}_i \in \mathbb {R}^K$</span>    </span> for each feature, where <em>K</em> is a hyper-parameter and is usually a small integer, e.g., 10. FMs model the interaction between two features <em>i</em> and <em>j</em> as the dot product of their corresponding embedding vectors <strong><em>v</em></strong><sub>     <em>i</em>    </sub>, <strong><em>v</em></strong><sub>     <em>j</em>    </sub>: <div class="table-responsive" id="Xeq4">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \Phi _{FMs}\left(({ w},{ v}),{ x} \right)= w_0 + \sum _{i=1}^m x_i w_i +\sum _{i=1}^{m}\sum _{j=i+1}^m x_{i}x_{j} \langle { v}_{i}, { v}_{j}\rangle \end{equation} </span>      <br/>      <span class="equation-number">(4)</span>     </div>    </div>    </p>    <p>FMs usually outperform Poly2 models in applications involving sparse data such as CTR prediction. The reason is that FMs can always learn some meaningful embedding vector for each feature as long as the feature itself appears enough times in the data, which makes the dot product a good estimate of the interaction effect of two features even if they have never or seldom occurred together in the data. However, FMs neglect the fact that a feature might behave differently when it interacts with features from different other fields. Field-aware Factorization Machines (FFMs) model such difference explicitly by learning <em>n</em> &#x2212; 1 embedding vectors for each feature, say <em>i</em>, and only using the corresponding one v<sub>     <em>i</em>, <em>F</em>(<em>j</em>)</sub> to interact with another feature <em>j</em> from field <em>F</em>(<em>j</em>): <div class="table-responsive" id="Xeq5">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \Phi _{FFMs}(({ w}, { v}), { x})= w_0 + \sum _{i=1}^m x_i w_i + \sum _{i=1}^{m}\sum _{j=i+1}^m x_{i}x_{j} \langle { v}_{i, F(j)}, { v}_{j, F(i)}\rangle \end{equation} </span>      <br/>      <span class="equation-number">(5)</span>     </div>    </div>    </p>    <p>Although FFMs have got significant performance improvement over FMs, their number of parameters is in the order of <em>O</em>(<em>mmK</em>). The huge number of parameters of FFMs is undesirable in the real-world production systems. Therefore, it is very appealing to design alternative approaches that are competitive and more memory-efficient.</p>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Interaction Strengths of Field Pairs</h2>    </div>    </header>    <p>In multi-field categorical data, every feature belongs to one and only one field. We are particularly interested in whether the strength of interactions are different at the field level. In other words, whether the average interaction strength between all feature pairs from a field pair is different from that of other field pairs.</p>    <p>For example, in the CTR prediction data, features from field <tt>ADVERTISER</tt> usually have strong interaction with features from field <tt>PUBLISHER</tt> since advertisers usually target a group of people with specific interest and the audience of publishers are naturally grouped by interests. On the other hand, features from field <tt>HOUR_OF_DAY</tt> tend to have little interaction with features from field <tt>DAY_OF_WEEK</tt>, which is not hard to understand since by intuition their interactions reveal little about the clicks.</p>    <p>To validate the heterogeneity of the field pair interactions, we use mutual information&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] between a field pair (<em>F<sub>k</sub>    </em>, <em>F<sub>l</sub>    </em>) and label variable <em>Y</em> to quantify the interaction strength of the field pair: <div class="table-responsive" id="eq1">     <div class="display-equation">      <span class="tex mytex">\begin{equation} MI((F_k, F_l), Y) = \sum _{(i,j) \in (F_k, F_l)} \sum _{y \in Y} p((i, j), y) log \frac{p((i, j), y)}{p(i,j)p(y)} \end{equation} </span>      <br/>      <span class="equation-number">(6)</span>     </div>    </div>    </p>    <figure id="fig1">    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186040/images/www2018-49-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 1:</span>     <span class="figure-title">Heat map of mutual information between each field pair and the label.</span>    </div>    </figure>    <p>Figure&#x00A0;<a class="fig" href="#fig1">1</a> is a visualization of the mutual information between each field pair and the label, computed from Oath CTR data. Unsurprisingly, the interaction strengths of different field pairs are quite different. Some field pairs have very strong interactions, such as (<tt>AD_ID, SUBDOMAIN</tt>), (<tt>CREATIVE_ID</tt>, <tt>PAGE_TLD</tt>) while some other field pairs have very weak interactions, such as (<tt>LAYOUT_ID</tt>, <tt>GENDER</tt>), (<tt>DAY_OF_WEEK</tt>, <tt>AD_POSITION_ID</tt>). The meanings of the these fields are explained in Section&#x00A0;<a class="sec" href="#sec-15">5.1</a>    </p>    <p>Although the analysis result is not surprising, we note that none of the existing models takes this field level interaction heterogeneity into consideration. This motivated us to build an effective machine learning model to capture the different interaction strengths of different field pairs.</p>   </section>   <section id="sec-11">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Field-weighted Factorization Machines (FwFMs)</h2>    </div>    </header>    <p>We propose to explicitly model the different interaction strengths of different field pairs. More specifically, the interaction of a feature pair <em>i</em> and <em>j</em> in our proposed approach is modeled as <div class="table-responsive">     <div class="display-equation">      <span class="tex mytex">\[ x_ix_j\langle {\bf{\it v}}_i, {\bf\it{ v}}_j\rangle r_{F(i), F(j)} \] </span>      <br/>     </div>    </div> where <strong><em>v</em></strong><sub>     <em>i</em>    </sub>, <strong><em>v</em></strong><sub>     <em>j</em>    </sub> are the embedding vectors of <em>i</em> and <em>j</em>, <em>F</em>(<em>i</em>), <em>F</em>(<em>j</em>) are the fields of feature <em>i</em> and <em>j</em>, respectively, and <span class="inline-equation"><span class="tex">$r_{F(i), F(j)} \in \mathbb {R}$</span>    </span> is a weight to model the interaction strength between field <em>F</em>(<em>i</em>) and <em>F</em>(<em>j</em>). We refer to the resulting model as the Field-weighted Factorization Machines(FwFMs): <div class="table-responsive" id="eq2">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \Phi _{FwFMs}(({ w},{ v}), { x}) = w_0 + \sum _{i=1}^m x_i w_i + \sum _{i=1}^m\sum _{j=i+1}^m x_{i} x_{j} \langle { v}_i, { v}_j \rangle r_{F(i), F(j)} \end{equation} </span>      <br/>      <span class="equation-number">(7)</span>     </div>    </div>    </p>    <p>FwFMs are extensions of FMs in the sense that we use additional weight <em>r</em>    <sub>     <em>F</em>(<em>i</em>), <em>F</em>(<em>j</em>)</sub> to explicitly capture different interaction strengths of different field pairs. FFMs can model this implicitly since they learn several embedding vectors for each feature <em>i</em>, each one <span class="inline-equation"><span class="tex">${ v}_{i, F_k}$</span>    </span> corresponds to one of other fields <em>F<sub>k</sub>    </em> &#x2260; <em>F</em>(<em>i</em>), to model its different interaction with features from different fields. However, the model complexity of FFMs is significantly higher than that of FMs and FwFMs.</p>    <div class="table-responsive" id="tab2">    <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">A summary of model complexities (ignoring the bias term <em>w</em>      <sub>0</sub>). <em>m</em> and <em>n</em> are feature number and field number respectively, <em>K</em> is the embedding vector dimension, and <em>H</em> is the hashing space size when hashing tricks are used for Poly2 models.</span>    </div>    <table class="table">     <thead>      <tr>       <th style="text-align:center;">Model</th>       <th style="text-align:center;">Number of Parameters</th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:center;">LR</td>       <td style="text-align:center;">       <em>m</em>       </td>      </tr>      <tr>       <td style="text-align:center;">Poly2</td>       <td style="text-align:center;">       <em>m</em> + <em>H</em>       </td>      </tr>      <tr>       <td style="text-align:center;">FMs</td>       <td style="text-align:center;">       <em>m</em> + <em>mK</em>       </td>      </tr>      <tr>       <td style="text-align:center;">FFMs</td>       <td style="text-align:center;">       <em>m</em> + <em>m</em>(<em>n</em> &#x2212; 1)<em>K</em>       </td>      </tr>      <tr>       <td style="text-align:center;">FwFMs</td>       <td style="text-align:center;">       <span class="inline-equation"><span class="tex">$m + mK + \frac{n(n-1)}{2}$</span>       </span>       </td>      </tr>     </tbody>    </table>    </div>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Model Complexity</h3>     </div>    </header>    <p>The number of parameters in FMs is <em>m</em> + <em>mK</em>, where <em>m</em> accounts for the weights for each feature in the linear part {<em>w<sub>i</sub>     </em>|<em>i</em> = 1, ..., <em>m</em>} and <em>mK</em> accounts for the embedding vectors for all the features {v<sub>      <em>i</em>     </sub>|<em>i</em> = 1, ..., <em>m</em>}. FwFMs use <em>n</em>(<em>n</em> &#x2212; 1)/2 additional parameters <span class="inline-equation"><span class="tex">$\lbrace r_{F_k, F_l} | k, l = 1,...,n\rbrace$</span>     </span> for each field pair so that the total number of parameters of FwFMs is <em>m</em> + <em>mK</em> + <em>n</em>(<em>n</em> &#x2212; 1)/2. For FFMs, the number of parameters is <em>m</em> + <em>m</em>(<em>n</em> &#x2212; 1)<em>K</em> since each feature has <em>n</em> &#x2212; 1 embedding vectors. Given that usually <em>n</em> &#x226A; <em>m</em>, the parameter number of FwFMs is comparable with that of FMs and significantly less than that of FFMs. In Table&#x00A0;<a class="tbl" href="#tab2">2</a> we compare the model complexity of all models mentioned so far.</p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Linear Terms</h3>     </div>    </header>    <p>In the linear terms <span class="inline-equation"><span class="tex">$\sum _{i=1}^m x_i w_i$</span>     </span> of equation (<a class="eqn" href="#eq2">7</a>), we learn a weight <em>w<sub>i</sub>     </em> for each feature to model its effect with the label, using binary variable <em>x<sub>i</sub>     </em> to represent feature <em>i</em>. However, the embedding vectors v<sub>      <em>i</em>     </sub> learned in the interaction terms should capture more information about feature <em>i</em>, therefore we propose to use <em>x<sub>i</sub>     </em>v<sub>      <em>i</em>     </sub> to represent each feature in the linear terms as well.</p>    <p>We can learn one linear weight vector w<sub>      <em>i</em>     </sub> for each feature and the linear terms become: <div class="table-responsive" id="Xeq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \sum _{i=1}^m x_i \langle { v}_i, { w}_i \rangle \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>    </p>    <p>There are <em>mK</em> parameters in the feature-wise linear weight vectors, and the total number of parameters is <span class="inline-equation"><span class="tex">$2mK + \frac{n(n-1)}{2}$</span>     </span>. Alternatively, we can learn one linear weight vector w<sub>      <em>F</em>(<em>i</em>)</sub> for each field and all features from the same field <em>F</em>(<em>i</em>) use the same linear weight vector. Then the linear terms can be formulated as: <div class="table-responsive" id="Xeq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \sum _{i=1}^m x_i \langle { v}_i, { w}_{F(i)} \rangle \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div>    </p>    <p>The parameter number of these kind of FwFMs is <span class="inline-equation"><span class="tex">$nK + mK + \frac{n(n-1)}{2}$</span>     </span>, which is almost the same as FwFMs with original linear weights since both <em>K</em> and <em>n</em> are usually in the order of tens.</p>    <p>In the rest of the paper, we denote FwFMs with original linear weights as FwFMs_LW, denote FwFMs with feature-wise linear weight vectors as FwFMs_FeLV, denote FwFMs with field-wise linear weight vectors as FwFMs_FiLV.</p>    </section>   </section>   <section id="sec-14">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>    </div>    </header>    <p>In this section we present our experimental evaluations results. We will first describe the data sets and implementation details in Section&#x00A0;<a class="sec" href="#sec-15">5.1</a> and &#x00A0;<a class="sec" href="#sec-16">5.2</a> respectively. In Section&#x00A0;<a class="sec" href="#sec-18">5.3.1</a> we compare FwFMs_LW, i.e., FwFMs using original linear weights, with LR, Poly2, FMs and FFMs. Then we further investigate the performance of FwFMs_LW and FFMs when they use the same number of parameters in Section&#x00A0;<a class="sec" href="#sec-19">5.3.2</a>. The enhancement brought by our novel linear term treatment is discussed in Section&#x00A0;<a class="sec" href="#sec-20">5.3.3</a>. Finally we show model hyper-parameter tuning details in Section&#x00A0;<a class="sec" href="#sec-21">5.4</a>.</p>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Data sets</h3>     </div>    </header>    <p>We use the following two data sets in our experiments.</p>    <ol class="list-no-style">     <li id="list8" label="(1)">Criteo CTR data set: This is the data set used for the Criteo Display Advertising Challenge&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0015">15</a>]. We split the data into training, validation and test sets randomly by 60%:20%:20%.<br/></li>     <li id="list9" label="(2)">Oath CTR data set: We use two-week display advertising click log from our ad serving system as the training set and the log of next day and the day after next day as validation and test set respectively.<br/></li>    </ol>    <p>The Criteo data set is already label balanced. For the Oath CTR data set, the ratio of positive samples (clicks) is the overall CTR, which is typically smaller than 1%. We downsample the negative examples so that the positive and negative samples are more balanced. The downsampling is not done for validation and test sets, since the evaluation should be applied to data sets reflecting the actual traffic distribution.</p>    <p>There are 26 anonymous categorical fields in Criteo data set. The Oath data set consists of 15 fields, which can be categorized into 4 groups: user side fields such as <tt>GENDER</tt>, <tt>AGE_BUCKET</tt>, <tt>USER_ID</tt>, publisher side fields such as <tt>PAGE_TLD</tt>, <tt>PUBLISHER_ID</tt>, <tt>SUBDOMAIN</tt>, advertiser side fields such as <tt>ADVERTISER_ID</tt>, <tt>AD_ID</tt>, <tt>CREATIVE_ID</tt>, <tt>LAYOUT_ID</tt> , <tt>LINE_ID</tt> and context side fields such as <tt>HOUR_OF_DAY</tt>, <tt>DAY_OF_WEEK</tt>, <tt>AD_POSITION_ID</tt> and <tt>DEVICE_TYPE_ID</tt>. The meanings of most fields are quite straightforward and we only explain some of them: <tt>PAGE_TLD</tt> denotes the top level domain of a web page and <tt>SUBDOMAIN</tt> denotes the sub domain of a web page. <tt>CREATIVE_ID</tt> denotes the identifier of a creative, while <tt>AD_ID</tt> identifies an ad that encapsulates a creative; the same creative may be assigned to different ads. <tt>DEVICE_TYPE_ID</tt> denotes whether this events happens on desktop, mobile or tablet. <tt>LAYOUT_ID</tt> denotes a specific ads size and <tt>AD_POSITION_ID</tt> denotes the position for ads in web page.</p>    <p>Furthermore, for both data sets we filter out all features which appear less than <em>&#x03C4;</em> times in the training set and replace them by a <tt>NULL</tt> feature, where <em>&#x03C4;</em> is set to 20 for Criteo data set and 10 for Oath data set. The statistics of the two data sets are shown in Table&#x00A0;<a class="tbl" href="#tab3">3</a>.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Statistics of training, validation and test sets of Criteo and Oath data sets respectively.</span>     </div>     <table class="table">      <thead>       <tr>       <th colspan="2" style="text-align:center;">Data set       </th>       <th style="text-align:right;">Samples</th>       <th style="text-align:center;">Fields</th>       <th style="text-align:center;">Features</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Criteo</td>       <td style="text-align:center;">Train</td>       <td style="text-align:right;">27,502,713</td>       <td style="text-align:center;">26</td>       <td style="text-align:center;">399,784</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">Validation</td>       <td style="text-align:right;">9,168,820</td>       <td style="text-align:center;">26</td>       <td style="text-align:center;">399,654</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">Test</td>       <td style="text-align:right;">9,169,084</td>       <td style="text-align:center;">26</td>       <td style="text-align:center;">399,688</td>       </tr>       <tr>       <td style="text-align:center;">Oath</td>       <td style="text-align:center;">Train</td>       <td style="text-align:right;">24,885,731</td>       <td style="text-align:center;">15</td>       <td style="text-align:center;">156,401</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">Validation</td>       <td style="text-align:right;">7,990,874</td>       <td style="text-align:center;">15</td>       <td style="text-align:center;">101,217</td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;">Test</td>       <td style="text-align:right;">8,635,361</td>       <td style="text-align:center;">15</td>       <td style="text-align:center;">100,515</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Implementations</h3>     </div>    </header>    <p>We use LibLinear&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>] to train Poly2 with hashing tricks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>] to hash feature conjunctions to a hashing space of 10<sup>7</sup>. All the other models are implemented in Tensorflow. We follow the implementation of LR and FMs in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>], and implement FFMs by ourselves. The architecture of FwFMs in Tensorflow is shown in Figure&#x00A0;<a class="fig" href="#fig2">2</a>. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186040/images/www2018-49-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Implementation for FwFMs in Tensorflow.</span>      </div>     </figure>    </p>    <p>The input is a sparse binary vector <span class="inline-equation"><span class="tex">${ x}_i \in \mathbb {R}^m$</span>     </span> with only <em>n</em> non-zero entries since there are <em>n</em> fields and each field has one and only one active feature for each sample. In the embedding layer, the input vector x<sub>      <em>i</em>     </sub> is projected into <em>n</em> embedding vectors. In the next layer the linear terms and interaction terms are computed from these <em>n</em> latent vectors. The linear terms layer simply concatenates the latent vectors of all active features. The interaction terms layer calculates the dot product &#x27E8;v<sub>      <em>i</em>     </sub>, v<sub>      <em>j</em>     </sub>&#x27E9; between embedding vectors of all pairs of active features. Then every node in the linear terms layer and interaction terms layer will be connected to the final output node, which will sum up the inputs from both linear terms layer and interaction terms layer with weights. Note that in FwFMs_FeLV, the weights between the linear terms layer and the final output is <em>w<sub>i</sub>     </em> while for FwFMs_FeLV and FwFM_FiLV the weights are w<sub>      <em>i</em>     </sub> and w<sub>      <em>F</em>(<em>i</em>)</sub> respectively. The weights between the interaction terms layer and the output node are <em>r</em>     <sub>      <em>F</em>(<em>i</em>), <em>F</em>(<em>j</em>)</sub>.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Comparison among models on Criteo and Oath CTR data sets.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">Models</th>       <th style="text-align:center;">Parameters</th>       <th colspan="3" style="text-align:center;">AUC<hr/>       </th>       </tr>       <tr>       <th style="text-align:center;"/>       <th style="text-align:center;"/>       <th style="text-align:center;">Training</th>       <th style="text-align:center;">Validation</th>       <th style="text-align:center;">Test</th>       </tr>       </thead>       <tbody>       <tr>       <td style="text-align:center;">LR</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 1<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 7, <em>t</em> = 15</td>       <td style="text-align:center;">0.8595</td>       <td style="text-align:center;">0.8503</td>       <td style="text-align:center;">0.8503</td>       </tr>       <tr>       <td style="text-align:center;">Poly2</td>       <td style="text-align:center;">        <em>s</em> = 7, <em>c</em> = 2</td>       <td style="text-align:center;">0.8652</td>       <td style="text-align:center;">0.8542</td>       <td style="text-align:center;">0.8523</td>       </tr>       <tr>       <td style="text-align:center;">FMs</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 5<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 6, <em>k</em> = 10, <em>t</em> = 10</td>       <td style="text-align:center;">0.8768</td>       <td style="text-align:center;">0.8628</td>       <td style="text-align:center;">0.8583</td>       </tr>       <tr>       <td style="text-align:center;">FFMs</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 1<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 7, <em>k</em> = 10, <em>t</em> = 3</td>       <td style="text-align:center;">        <strong>0.8833</strong>       </td>       <td style="text-align:center;">        <strong>0.8660</strong>       </td>       <td style="text-align:center;">        <strong>0.8624</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">FwFMs</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 1<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 5, <em>k</em> = 10, <em>t</em> = 15</td>       <td style="text-align:center;">0.8827</td>       <td style="text-align:center;">0.8659</td>       <td style="text-align:center;">0.8614</td>       </tr>       <tr>       <td colspan="5" style="text-align:center;">        <strong>(a) Oath data set</strong>        <hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">Models</td>       <td style="text-align:center;">Parameters</td>       <td colspan="3" style="text-align:center;">AUC<hr/>       </td>       </tr>       <tr>       <td style="text-align:center;"/>       <td style="text-align:center;"/>       <td style="text-align:center;">Training</td>       <td style="text-align:center;">Validation</td>       <td style="text-align:center;">Test</td>       </tr>       <tr>       <td style="text-align:center;">LR</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 5<em>e</em> &#x2212; 5, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 6, <em>t</em> = 14</td>       <td style="text-align:center;">0.7716</td>       <td style="text-align:center;">0.7657</td>       <td style="text-align:center;">0.7654</td>       </tr>       <tr>       <td style="text-align:center;">Poly2</td>       <td style="text-align:center;">        <em>s</em> = 7, <em>c</em> = 2</td>       <td style="text-align:center;">0.7847</td>       <td style="text-align:center;">0.7718</td>       <td style="text-align:center;">0.7710</td>       </tr>       <tr>       <td style="text-align:center;">FMs</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 1<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 6, <em>k</em> = 10, <em>t</em> = 10</td>       <td style="text-align:center;">0.7925</td>       <td style="text-align:center;">0.7759</td>       <td style="text-align:center;">0.7761</td>       </tr>       <tr>       <td style="text-align:center;">FFMs</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 5<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 7, <em>k</em> = 10, <em>t</em> = 3</td>       <td style="text-align:center;">        <strong>0.7989</strong>       </td>       <td style="text-align:center;">        <strong>0.7781</strong>       </td>       <td style="text-align:center;">        <strong>0.7768</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">FwFMs</td>       <td style="text-align:center;">        <em>&#x03B7;</em> = 1<em>e</em> &#x2212; 4, <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 6, <em>k</em> = 10, <em>t</em> = 8</td>       <td style="text-align:center;">0.7941</td>       <td style="text-align:center;">0.7772</td>       <td style="text-align:center;">0.7764</td>       </tr>       <tr>       <td colspan="5" style="text-align:center;">        <strong>(b) Criteo data set</strong>        <hr/>       </td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Performance Comparisons</h3>     </div>    </header>    <section id="sec-18">     <p><em>5.3.1 Comparison of FwFMs with Existing Models.</em> In this section we will conduct performance evaluation for FwFMs with original linear terms, i.e., FwFMs_LW. We will compare it with LR, Poly2, FMs and FFMs on two data sets mentioned above. For the parameters such as regularization coefficient <em>&#x03BB;</em>, and learning rate <em>&#x03B7;</em> in all models, we select those which leads to the best performance on the validation set and then use them in the evaluation on test set. Experiment results can be found in Table&#x00A0;<a class="tbl" href="#tab4">4</a>.</p>     <p>We observe that FwFMs can achieve better performance than LR, Poly2 and FMs on both data sets. The improvement comes from the fact that FwFMs explicitly model the different interaction strengths of field pairs, which would be discussed in Section&#x00A0;<a class="sec" href="#sec-24">6</a> in more details. FFMs can always get the best performance on training and validation sets. However, FwFMs&#x2019; performance on the both test sets are quite competitive with that of FwFMs. It suggests that FFMs are more vulnerable to overfittings than FwFMs.</p>    </section>    <section id="sec-19">     <p><em>5.3.2 Comparison of FwFMs and FFMs using the same number of parameters.</em> One critical drawback of using FFMs is that their number of parameters is in the order of <em>O</em>(<em>mnK</em>), which would be too large to fit in memory. There are two solutions to reduce the number of parameters in FFMs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>]: use a smaller <em>K</em> or use hashing tricks with a small hashing space <em>H</em>. Juan et. al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>] proposed to use <em>K<sub>FFMs</sub>      </em> = 2 for FFMs to get a good trade-off between prediction accuracy and the number of parameters. This reduce the number of parameters to (<em>n</em> &#x2212; 1)<em>mK<sub>FFMs</sub>      </em>. Juan et. al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>] also proposed to further reduce the number of parameters of FFMs by using hashing tricks with a small hashing space <em>H<sub>FFMs</sub>      </em> to reduce it to (<em>n</em> &#x2212; 1)<em>m<sub>FFMs</sub>K<sub>FFMs</sub>      </em>. In this section, we make a fair comparison of FwFMs with FFMs using the same number of parameters by choosing proper <em>k<sub>FFMs</sub>      </em> and <em>H<sub>FFMs</sub>      </em> so that the number of parameters of FFMs and FwFMs are the same, i.e., (<em>n</em> &#x2212; 1)<em>H<sub>FFMs</sub>K<sub>FFMs</sub>      </em> = <em>mK<sub>FwFMs</sub>      </em>, we do not count the parameters from the linear terms since they are negligible compared with the number of interaction terms. We choose <em>K<sub>FwFMs</sub>      </em> = 10 and <em>K<sub>FFMs</sub>      </em> = 2 and <em>K<sub>FFMs</sub>      </em> = 4 as described in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0012">12</a>]. The experimental results are shown in Table&#x00A0;<a class="tbl" href="#tab5">5</a>.</p>     <p>We observe that when using the same number of parameters, FwFMs get better performance on test sets of both Criteo and Oath data sets, with a lift of 0.70% and 0.45% respectively. We conclude that FFMs make lots of compromise on the prediction performance when reducing the feature numbers therefore FwFMs can outperform them significantly in such cases.</p>     <div class="table-responsive" id="tab5">      <div class="table-caption">       <span class="table-number">Table 5:</span>       <span class="table-title">Comparison of FFMs with FwFMs using same number of parameters. <em>K</em> is embedding vector dimension and <em>H</em> is the hashing space for hashing tricks.</span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:center;">Model</th>        <th colspan="3" style="text-align:center;">Oath data set<hr/>        </th>        <th colspan="3" style="text-align:center;">Criteo data set<hr/>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">Training</th>        <th style="text-align:center;">Validation</th>        <th style="text-align:center;">Test</th>        <th style="text-align:center;">Training</th>        <th style="text-align:center;">Validation</th>        <th style="text-align:center;">Test</th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;">FFMs(<span class="inline-equation"><span class="tex">$K=2,H=\frac{10}{14 \cdot 2}N$</span>         </span>)</td>        <td style="text-align:center;">0.8743</td>        <td style="text-align:center;">0.8589</td>        <td style="text-align:center;">0.8543</td>        <td style="text-align:center;">0.7817</td>        <td style="text-align:center;">0.7716</td>        <td style="text-align:center;">0.7719</td>       </tr>       <tr>        <td style="text-align:center;">FFMs(<span class="inline-equation"><span class="tex">$K=4,H=\frac{10}{14 \cdot 4}N$</span>         </span>)</td>        <td style="text-align:center;">0.8708</td>        <td style="text-align:center;">0.8528</td>        <td style="text-align:center;">0.8418</td>        <td style="text-align:center;">0.7697</td>        <td style="text-align:center;">0.7643</td>        <td style="text-align:center;">0.7641</td>       </tr>       <tr>        <td style="text-align:center;">FwFMs</td>        <td style="text-align:center;">         <strong>0.8827</strong>        </td>        <td style="text-align:center;">         <strong>0.8659</strong>        </td>        <td style="text-align:center;">         <strong>0.8614</strong>        </td>        <td style="text-align:center;">         <strong>0.7941</strong>        </td>        <td style="text-align:center;">         <strong>0.7772</strong>        </td>        <td style="text-align:center;">         <strong>0.7764</strong>        </td>       </tr>       </tbody>      </table>     </div>    </section>    <section id="sec-20">     <p><em>5.3.3 FwFMs with Different Linear Terms.</em> We conduct experiments to compare FwFMs with three different kinds of linear terms as mentioned in Section&#x00A0;<a class="sec" href="#sec-13">4.2</a>. Table&#x00A0;<a class="tbl" href="#tab6">6</a> lists the performance comparison between them. We observe that, FwFMs_LW and FwFMs_FeLV can achieve better performance on the training and validation set than FwFMs_FiLV. The reason is that those two models have more number of parameters in the linear weights(<em>m</em> and <em>mK</em>) than FwFMs_FiLV(<em>nK</em>), so they can fit the training and validation set better than FwFMs_FiLV. However, FwFMs_FiLV get the best results on the test set, which suggests that it has better generalization performance. Furthermore, when we compare FwFMs_FiLV with FFMs using the same number of parameters, the AUC lift on Oath data set and Criteo data set are 0.92% and 0.47% respectively.</p>     <div class="table-responsive" id="tab6">      <div class="table-caption">       <span class="table-number">Table 6:</span>       <span class="table-title">Performance of FwFMs with different linear terms.</span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:center;">Model</th>        <th colspan="3" style="text-align:center;">Oath data set<hr/>        </th>        <th colspan="3" style="text-align:center;">Criteo data set<hr/>        </th>       </tr>       <tr>        <th style="text-align:center;"/>        <th style="text-align:center;">Training</th>        <th style="text-align:center;">Validation</th>        <th style="text-align:center;">Test</th>        <th style="text-align:center;">Training</th>        <th style="text-align:center;">Validation</th>        <th style="text-align:center;">Test</th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;">FwFMs_LW</td>        <td style="text-align:center;">         <strong>0.8827</strong>        </td>        <td style="text-align:center;">0.8659</td>        <td style="text-align:center;">0.8614</td>        <td style="text-align:center;">0.7941</td>        <td style="text-align:center;">0.7772</td>        <td style="text-align:center;">0.7764</td>       </tr>       <tr>        <td style="text-align:center;">FwFMs_FeLV</td>        <td style="text-align:center;">0.8829</td>        <td style="text-align:center;">         <strong>0.8665</strong>        </td>        <td style="text-align:center;">0.8623</td>        <td style="text-align:center;">         <strong>0.7945</strong>        </td>        <td style="text-align:center;">         <strong>0.7774</strong>        </td>        <td style="text-align:center;">0.7763</td>       </tr>       <tr>        <td style="text-align:center;">FwFMs_FiLV</td>        <td style="text-align:center;">0.8799</td>        <td style="text-align:center;">0.8643</td>        <td style="text-align:center;">         <strong>0.8635</strong>        </td>        <td style="text-align:center;">0.7917</td>        <td style="text-align:center;">0.7766</td>        <td style="text-align:center;">         <strong>0.7766</strong>        </td>       </tr>       </tbody>      </table>     </div>    </section>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Hyper-parameter Tuning</h3>     </div>    </header>    <p>In this section we will show the impact of regularization coefficient <em>&#x03BB;</em>, embedding vector dimension <em>K</em> and learning rate <em>&#x03B7;</em> on FwFMs. All following evaluations are done for FwFMs_FiLV model on Oath validation set.</p>    <section id="sec-22">     <p><em>5.4.1 Regularization.</em> We add <em>L</em>      <sub>2</sub> regularizations of all parameters in FwFMs to the loss function to prevent over-fitting. Figure&#x00A0;<a class="fig" href="#fig3">3</a> shows the AUC on validation set using different <em>&#x03BB;</em>. We get the best performance on validation set using <em>&#x03BB;</em> = 1<em>e</em> &#x2212; 5.</p>    </section>    <section id="sec-23">     <p><em>5.4.2 Learning Rate and Embedding Vector Dimension.</em> We have done experiments to check the impact of learning rate <em>&#x03B7;</em> to the performance of FwFMs, and the results are shown in Figure&#x00A0;<a class="fig" href="#fig4">4</a>. It shows that by using a small <em>&#x03B7;</em>, we can keep improving the performance on validation set slowly in the first 20 epochs, while using a large <em>&#x03B7;</em> will improve the performance quickly and then lead to over-fitting. In all experiments on Oath data set we choose <em>&#x03B7;</em> = 1<em>e</em> &#x2212; 4. <figure id="fig3">       <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186040/images/www2018-49-fig3.jpg" class="img-responsive" alt="Figure 3"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">Impact of regularization coefficient <em>&#x03BB;</em> on FwFMs.</span>       </div>      </figure>      <figure id="fig4">       <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186040/images/www2018-49-fig4.jpg" class="img-responsive" alt="Figure 4"        longdesc=""/>       <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Impact of learning rate <em>&#x03B7;</em> on FwFMs.</span>       </div>      </figure>     </p>     <p>We conduct experiments to investigate the impact of embedding vector dimension <em>K</em> on the performance of FwFMs. Table&#x00A0;<a class="tbl" href="#tab7">7</a> shows that the AUC changes only a little when we use different <em>K</em>. We choose <em>K</em> = 10 in FwFMs since it gets best trade-off between performance and training time.</p>     <div class="table-responsive" id="tab7">      <div class="table-caption">       <span class="table-number">Table 7:</span>       <span class="table-title">Comparison of different embedding vector dimensions <em>K</em>       </span>      </div>      <table class="table">       <thead>       <tr>        <th style="text-align:center;">         <em>k</em>        </th>        <th style="text-align:center;">Train AUC</th>        <th style="text-align:center;">Validation AUC</th>        <th style="text-align:center;">Training time (s)</th>       </tr>       </thead>       <tbody>       <tr>        <td style="text-align:center;">5</td>        <td style="text-align:center;">0.8794</td>        <td style="text-align:center;">0.8621</td>        <td style="text-align:center;">320</td>       </tr>       <tr>        <td style="text-align:center;">10</td>        <td style="text-align:center;">0.8827</td>        <td style="text-align:center;">0.8659</td>        <td style="text-align:center;">497</td>       </tr>       <tr>        <td style="text-align:center;">15</td>        <td style="text-align:center;">0.8820</td>        <td style="text-align:center;">0.8644</td>        <td style="text-align:center;">544</td>       </tr>       <tr>        <td style="text-align:center;">20</td>        <td style="text-align:center;">0.8822</td>        <td style="text-align:center;">0.8640</td>        <td style="text-align:center;">636</td>       </tr>       <tr>        <td style="text-align:center;">30</td>        <td style="text-align:center;">0.8818</td>        <td style="text-align:center;">0.8647</td>        <td style="text-align:center;">848</td>       </tr>       <tr>        <td style="text-align:center;">50</td>        <td style="text-align:center;">0.8830</td>        <td style="text-align:center;">0.8652</td>        <td style="text-align:center;">1113</td>       </tr>       <tr>        <td style="text-align:center;">100</td>        <td style="text-align:center;">0.8830</td>        <td style="text-align:center;">0.8646</td>        <td style="text-align:center;">1728</td>       </tr>       <tr>        <td style="text-align:center;">200</td>        <td style="text-align:center;">0.8825</td>        <td style="text-align:center;">0.8646</td>        <td style="text-align:center;">3250</td>       </tr>       </tbody>      </table>     </div>    </section>    </section>   </section>   <section id="sec-24">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Study of Learned Field Interaction Strengths</h2>    </div>    </header>    <p>In this section, we compare FMs, FFMs and FwFMs in terms of their ability to capture interaction strengths of different field pairs. Our result shows that FwFMs can model the interaction strength much better than FMs and FFMs do thanks to the field pair interaction weight <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span>. <figure id="fig5">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186040/images/www2018-49-fig5.jpg" class="img-responsive" alt="Figure 5"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Heat maps of mutual informations (a) and learned field pair interaction strengths of FMs (b), FwFMs (c) and FFMs (d) on Oath CTR data set.</span>     </div>    </figure>    </p>    <p>In Section&#x00A0;<a class="sec" href="#sec-10">3</a> the interaction strengths of field pairs are quantified by mutual information and visualized in heat map (Figure&#x00A0;5a). To measure the learned field interaction strengths, we define the following metric: <div class="table-responsive" id="eq3">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \frac{\sum _{(i,j) \in (F_k, F_l)} I(i,j) \cdot \#(i,j)}{\sum _{(i,j) \in (F_k, F_l)} \#(i,j)} \end{equation} </span>      <br/>      <span class="equation-number">(10)</span>     </div>    </div> where <span class="inline-equation"><span class="tex">$\#(i,j)$</span>    </span> is the number of times feature pair (<em>i</em>, <em>j</em>) appears in the training data, <em>I</em>(<em>i</em>, <em>j</em>) is the learned strength of interaction between feature <em>i</em> and <em>j</em>. For FMs <em>I</em>(<em>i</em>, <em>j</em>) = |&#x27E8;<strong><em>v</em></strong><sub>     <em>i</em>    </sub>, <strong><em>v</em></strong><sub>     <em>j</em>    </sub>&#x27E9;| , for FFMs <span class="inline-equation"><span class="tex">$I(i,j) = |\langle { v}_{i, F_l}, { v}_{j, F_k} \rangle |$</span>    </span>, for FwFMs <span class="inline-equation"><span class="tex">$I(i,j) = |\langle { v}_i, { v}_j \rangle \cdot r_{F_k, F_l}|$</span>    </span>. Note that we sum up the absolute values of the inner product terms otherwise positive values and negative values would counteract with each others.</p>    <p>If a model can capture the interaction strengths of different field pairs well, we expect its learned field interaction strengths to be close to the mutual information as shown in equation&#x00A0;(<a class="eqn" href="#eq1">6</a>). To facilitate the comparison, in Figure&#x00A0;<a class="fig" href="#fig5">5</a> we plot the field interaction strengths learned by FMs, FFMs and FwFMs in the form of heatmap, along with heatmap of the mutual information between field pairs and labels. We also computed Pearson correlation coefficient to quantitatively measure how well the learned field interaction strengths match the mutual information.</p>    <p>From Figure&#x00A0;5b we observe that FMs learned interaction strengths quite different from mutual information. Some field pairs which have high mutual information with labels only get low interaction strengths, for example <tt>(AD_ID, SUBDOMAIN)</tt>, <tt>(CREATIVE_ID, PAGE_TLD)</tt>. While some field pairs with low mutual information have high interaction strengths, such as <tt>(LAYOUT_ID, GENDER)</tt>. The Pearson correlation coefficient between the learned field pair interaction strengths and mutual information is -0.1210. This is not surprising since FMs is not considering field information when modeling feature interaction strengths.</p>    <p>As shown in Figure&#x00A0;5c, FFMs are able to learn interaction strengths more similar to mutual information. This is confirmed by a higher Pearson correlation coefficient of 0.1544, which is much better than FMs.</p>    <p>For FwFMs, Figure&#x00A0;5d shows that the heat map of the learned interaction strengths are very similar to that of mutual information. For example, the field pairs that have highest mutual informations with labels, such as (<tt>AD_ID</tt>,<tt>SUBDOMAIN</tt>), (<tt>AD_ID</tt>,<tt>PAGE_TLD</tt>), and (<tt>CREATIVE_ID</tt>,<tt>PAGE_TLD</tt>), also have higher interaction strengths. For fields such as <tt>LAYOUT_ID</tt>, <tt>HOUR_OF_DAY</tt>, <tt>DAY_OF_WEEK</tt>, <tt>GENDER</tt> and <tt>AD_POSITION_ID</tt>, field pairs between any of them have low mutual information with labels as well as low interaction strengths. The Pearson correlation coefficient with mutual information is 0.4271, which is much better than FMs and FFMs. <figure id="fig6">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186040/images/www2018-49-fig6.jpg" class="img-responsive" alt="Figure 6"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 6:</span>      <span class="figure-title">Heat map of learned field interaction weight <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>       </span> , and heat map of learned interaction strengths between field pairs of FwFMs without <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>       </span>      </span>     </div>    </figure>    </p>    <p>To understand the importance of the weights <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span> in modeling the interaction strength of field pairs, we plot the heap map of <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span>, as well as learned field interaction strengths of FwFMs without <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span> in Figure&#x00A0;<a class="fig" href="#fig6">6</a>.</p>    <p>The heat map of <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span> (Figure 6a) is very similar to that of mutual information (Figure 5a) and learned field interaction strengths of FwFMs (Figure 5d). The corresponding Pearson correlation coefficient with mutual information is 0.5554. This implies that <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span> can indeed learn different interaction strengths of different field pairs. On the other hand, the learned field interaction strengths without <span class="inline-equation"><span class="tex">$r_{F_k, F_l}$</span>    </span> correlates poorly with mutual information, as depicted in Figure&#x00A0;6b. Its Pearson correlation coefficient with mutual information is only 0.0522.</p>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Related Work</h2>    </div>    </header>    <p>CTR prediction is an essential task in online display advertising. Many models have been proposed to resolve this problem such as Logistic Regression (LR) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>], Polynomial-2 (Poly2) &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>], tree-based models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>], tensor-based models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], Bayesian models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>], and Field-aware Factorization Machines (FFMs)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. Recently, deep learning for CTR prediction also attracted much research attention&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]. However, only FFMs explicitly model different feature interactions from different fields in the multi-field categorical data, which we have shown to be very effective for CTR prediction.</p>    <p>FFMs extended the ideas of Factorization Machines (FMs)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. The latter have been proved to be very successful in recommender systems. Recommendation is a different but very closely related topic with CTR prediction. Collaborative filtering (CF) techniques such as matrix factorization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] have been established as standard approaches for various recommendation tasks. However, FMs have been shown to outperform matrix factorization in many recommendation tasks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>] and they are also capable of effectively resolving cold-start problems&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>].</p>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">8</span> Conclusion</h2>    </div>    </header>    <p>In this paper, we propose Field-weighted Factorization Machines (FwFMs) for CTR prediction in online display advertising. We show that FwFMs are competitive to FFMs with significantly less parameters. When using the same number of parameters, FwFMs can achieve consistently better performance than FFMs. We also introduce a novel linear term representation to augment FwFMs so that their performance can be further improved. Finally, comprehensive analysis on real-world data sets also verifies that FwFMs can indeed learn different feature interaction strengths from different field pairs. There are many potential directions for future research. For example, it is interesting to see whether there are other alternatives to model different field interactions rather than learning one weight for each field pair. Another direction is to explore the possibility of combining deep learning with FwFMs since the combination of deep neural networks and FMs has shown promising results in CTR prediction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>].</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Michal Aharon, Natalie Aizenberg, Edward Bortnikov, Ronny Lempel, Roi Adadi, Tomer Benyamini, Liron Levin, Ran Roth, and Ohad Serfaty. 2013. OFF-set: one-pass factorization of feature sets for online recommendation in persistent cold start settings. In <em>      <em>Proceedings of the 7th ACM Conference on Recommender Systems</em>     </em>. ACM, 375&#x2013;378.</li>    <li id="BibPLXBIB0002" label="[2]">Interactive&#x00A0;Advertising Bureau. 2016. IAB internet advertising revenue report. (2016). <a class="link-inline force-break" target="_blank" href="https://www.iab.com/wp-content/uploads/2016/04/IAB_Internet_Advertising_Revenue_Report_FY_2016.pdf">https://www.iab.com/wp-content/uploads/2016/04/IAB_Internet_Advertising_Revenue_Report_FY_2016.pdf</a></li>    <li id="BibPLXBIB0003" label="[3]">Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. 2010. Training and testing low-degree polynomial data mappings via linear SVM. <em>      <em>Journal of Machine Learning Research</em>     </em>11, Apr (2010), 1471&#x2013;1490.</li>    <li id="BibPLXBIB0004" label="[4]">Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2015. Simple and scalable response prediction for display advertising. <em>      <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em>     </em>5, 4(2015), 61.</li>    <li id="BibPLXBIB0005" label="[5]">Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, <em>et al.</em> 2016. Wide &#x0026; deep learning for recommender systems. In <em>      <em>Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</em>     </em>. ACM, 7&#x2013;10.</li>    <li id="BibPLXBIB0006" label="[6]">Thomas&#x00A0;M Cover and Joy&#x00A0;A Thomas. 2012. <em>      <em>Elements of information theory</em>     </em>. John Wiley &#x0026; Sons.</li>    <li id="BibPLXBIB0007" label="[7]">Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. <em>      <em>Journal of machine learning research</em>     </em>9, Aug (2008), 1871&#x2013;1874.</li>    <li id="BibPLXBIB0008" label="[8]">Thore Graepel, Joaquin&#x00A0;Q Candela, Thomas Borchert, and Ralf Herbrich. 2010. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft&#x0027;s bing search engine. In <em>      <em>Proceedings of the 27th international conference on machine learning (ICML-10)</em>     </em>. 13&#x2013;20.</li>    <li id="BibPLXBIB0009" label="[9]">Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. <em>      <em>arXiv preprint arXiv:1703.04247</em>     </em>(2017).</li>    <li id="BibPLXBIB0010" label="[10]">Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse Predictive Analytics. (2017).</li>    <li id="BibPLXBIB0011" label="[11]">Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, <em>et al.</em> 2014. Practical lessons from predicting clicks on ads at facebook. In <em>      <em>Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</em>     </em>. ACM, 1&#x2013;9.</li>    <li id="BibPLXBIB0012" label="[12]">Yuchin Juan, Damien Lefortier, and Olivier Chapelle. 2017. Field-aware factorization machines in a real-world online advertising system. In <em>      <em>Proceedings of the 26th International Conference on World Wide Web Companion</em>     </em>. International World Wide Web Conferences Steering Committee, 680&#x2013;688.</li>    <li id="BibPLXBIB0013" label="[13]">Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware factorization machines for CTR prediction. In <em>      <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>     </em>. ACM, 43&#x2013;50.</li>    <li id="BibPLXBIB0014" label="[14]">Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. <em>      <em>Computer</em>     </em>42, 8 (2009).</li>    <li id="BibPLXBIB0015" label="[15]">Criteo Labs. 2014. Display Advertising Challenge. (2014). <a class="link-inline force-break" target="_blank"     href="https://www.kaggle.com/c/criteo-display-ad-challenge">https://www.kaggle.com/c/criteo-display-ad-challenge</a></li>    <li id="BibPLXBIB0016" label="[16]">H&#x00A0;Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, <em>et al.</em> 2013. Ad click prediction: a view from the trenches. In <em>      <em>Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em>     </em>. ACM, 1222&#x2013;1230.</li>    <li id="BibPLXBIB0017" label="[17]">Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In <em>      <em>Data Mining (ICDM), 2016 IEEE 16th International Conference on</em>     </em>. IEEE, 1149&#x2013;1154.</li>    <li id="BibPLXBIB0018" label="[18]">Steffen Rendle. 2010. Factorization machines. In <em>      <em>Data Mining (ICDM), 2010 IEEE 10th International Conference on</em>     </em>. IEEE, 995&#x2013;1000.</li>    <li id="BibPLXBIB0019" label="[19]">Steffen Rendle. 2012. Factorization machines with libfm. <em>      <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em>     </em>3, 3(2012), 57.</li>    <li id="BibPLXBIB0020" label="[20]">Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In <em>      <em>Proceedings of the third ACM international conference on Web search and data mining</em>     </em>. ACM, 81&#x2013;90.</li>    <li id="BibPLXBIB0021" label="[21]">Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In <em>      <em>Proceedings of the 16th international conference on World Wide Web</em>     </em>. ACM, 521&#x2013;530.</li>    <li id="BibPLXBIB0022" label="[22]">Ying Shan, T&#x00A0;Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, and JC Mao. 2016. Deep Crossing: Web-scale modeling without manually crafted combinatorial features. In <em>      <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 255&#x2013;262.</li>    <li id="BibPLXBIB0023" label="[23]">Nguyen Thai-Nghe, Lucas Drumond, Tom&#x00E1;s Horv&#x00E1;th, and Lars Schmidt-Thieme. 2012. Using factorization machines for student modeling.. In <em>      <em>UMAP Workshops</em>     </em>.</li>    <li id="BibPLXBIB0024" label="[24]">Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep &#x0026; Cross Network for Ad Click Predictions. <em>      <em>arXiv preprint arXiv:1708.05123</em>     </em>(2017).</li>    <li id="BibPLXBIB0025" label="[25]">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In <em>      <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>     </em>. ACM, 1113&#x2013;1120.</li>    <li id="BibPLXBIB0026" label="[26]">Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field categorical data. In <em>      <em>European conference on information retrieval</em>     </em>. Springer, 45&#x2013;57.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186040">https://doi.org/10.1145/3178876.3186040</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
