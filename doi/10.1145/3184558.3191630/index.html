<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Stochastic Multi-path Routing Problem with Non-stationary Rewards: Building PayU's Dynamic Routing</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191630'>https://doi.org/10.1145/3184558.3191630</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191630'>https://w3id.org/oa/10.1145/3184558.3191630</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Stochastic Multi-path Routing Problem with Non-stationary Rewards: Building PayU's Dynamic Routing</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Pankaj</span> <span class="surName">Trivedi</span> PayU Payments Pvt. Ltd., <a href="mailto:pankaj.trivedi@payu.in">pankaj.trivedi@payu.in</a>
        </div>
        <div class="author">
          <span class="givenName">Arvind</span> <span class="surName">Singh</span> PayU Payments Pvt. Ltd., <a href="mailto:arvind.singh@payu.in">arvind.singh@payu.in</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191630" target="_blank">https://doi.org/10.1145/3184558.3191630</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Payment transaction engine at PayU processes multimillion transactions every day through multiple payment gateways. Routing a transaction through an appropriate payment gateway is crucial to the engine for optimizing the availability and cost. The problem is that every transaction needs to choose one of K available payment gateways characterized by an unknown probability reward distribution. The reward for a gateway is a combination of its health and cost factors. The reward for a gateway is only realized when transaction is processed by the gateway i.e. by its success or failure. The objective of dynamic routing is to maximize the cumulative expected rewards over some given horizon of transactions’ life. To do this, the dynamic switching system needs to acquire information about gateways (exploration) while simultaneously optimizing immediate rewards by selecting the best gateway at the moment (exploitation); the price paid due to this trade off is referred to as the regret. The main objective is to minimize the regret and maximize the rewards. The basic idea is to choose a gateway according to its probability of being the best gateway.</small></p>
        <p><small>The routing problem is a direct formulation of reinforcement learning (RL) problem. In an RL problem, an agent interacts with a dynamic, stochastic, and incompletely known environment, with the goal of finding an action-selection strategy, or policy, that optimizes some long-term performance measure. Thompson Sampling algorithm has experimentally been shown to be close to optimal.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Machine Learning</small>,</span> <span class="keyword"><small>Reinforcement Learning</small>,</span> <span class="keyword"><small>Routing Problem</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Pankaj Trivedi and Arvind Singh. 2018. Stochastic Multi-path Routing Problem with Non-stationary Rewards: Building PayU's Dynamic Routing. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 6 Pages. <a href="https://doi.org/10.1145/3184558.3191630" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3191630</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>PayU is a payment gateway aggregator which integrates with multiple payment gateways capable of processing online card payments. PayU also integrates merchants who are selling their products and services online. Each payment gateway integrated with PayU platform has an unknown probability of making a successful transaction at a given point in time (Figure 1). It becomes very critical for the business to be able to route a transaction to appropriate payment gateway so that the probability of a successful transaction is maximized. This may be modeled as a routing problem wherein an appropriate payment gateway must be selected for each transaction so that overall success rate is maximized. Every payment gateway also has a known but different cost to process the successful transaction. The cost factor to each gateway adds another dimension of optimization.</p>
      <p>It is intuitive to apply reinforcement learning technique to learn from the feedback supplied in form of results of processed transactions. The learning is further used to recommend the appropriate gateway for each transaction.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work and Modification</h2>
        </div>
      </header>
      <p>The approach in this paper is a modification of multi-armed bandit problem. In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. Since their inception, MAB problems with various modifications have been studied extensively in Statistics, Economics, Operations Research, and Computer Science, and are used to model a plethora of dynamic optimization problems under uncertainty; examples include clinical trials ([3]), strategic pricing ([17]), investment in innovation ([18]), packet routing ([19]), on-line auctions ([7]), assortment selection ([20]), and online advertising ([21]), to name but a few.</p>
      <p>In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. The uncertainties in the rewards are introduced due to various reasons of a transaction failure. The other variation in the formulation is introduced in the decaying rewards because the exploitation is required to be biased towards recent history. The transaction routing is also required to be cost effective and hence another modification is required to consider cost variation of various gateways. The business requires various knobs to control the tradeoff between success rates and cost. The above mentioned requirements have been accomplished by significantly modified techniques.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Problem Formulation</h2>
        </div>
      </header>
      <p>Let <em>χ</em> = {1, ..., <em>K</em>} be a set of gateways. Let <em>Γ</em> = {1, 2, ..., <em>T</em>} denote the sequence of decision epochs faced by the recommendation system. At any epoch <em>t</em> ∈ <em>Γ</em>, a recommender selects one of the K gateways. When a transaction is processed through the gateway <em>k</em> ∈ <em>χ</em> at epoch <em>t</em> ∈ <em>Γ</em> , a reward <span class="inline-equation"><span class="tex">$X_k^t \in [0,1]$</span></span> is obtained depending on the result of transaction processing, where <span class="inline-equation"><span class="tex">$X_k^t$</span></span> is a random variable with expectation</p>
      <div class="table-responsive" id="Xeq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mu _k^t = E[ X_k^t] \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>We denote the best possible expected reward at decision epoch <em>t</em> by <span class="inline-equation"><span class="tex">$\mu _t^*$</span></span>
      <div class="table-responsive" id="Xeq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mu _t^* = \max _{k \in \chi }{ \mu _k^t} \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>
      <p></p>
      <p>The goal is to maximize the expected total reward in time T. It is more convenient to work with the equivalent measure of expected total regret: the amount we lose because of not playing optimal gateway in each step. Then the expected total regret in time T is given by</p>
      <div class="table-responsive" id="Xeq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} E[R(T)] = E\left[ \sum _{t=1}^T (\mu _{t}^* - \mu _k^t) \right] \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>
      <p></p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Instances of variation in the expected rewards of various gateways</span>
        </div>
      </figure>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Solution</h2>
        </div>
      </header>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Reinforcement Learning</h3>
          </div>
        </header>
        <p>Reinforcement learning is a class of methods whereby the problem to be solved by the control system is defined in terms of payoffs (which represent rewards or punishments). The aim of the system is to maximize the payoffs received over time. Therefore high payoffs are given for desirable behavior and low payoffs for undesirable behavior. The system is otherwise unconstrained in its sequence of actions referred to as its policy used to maximize the payoffs received. In effect, the system must find its own method of solving the given task.</p>
        <p>A block diagram of a reinforcement system is shown in Figure 2, which shows the basic interaction between a controller and its environment. The payoff function is fixed, as are the sensors and actuators which really form part of the environment as far as the control system is concerned. The control system is the adaptive part which learns to produce the control action a in response to the state input x based on maximizing the payoff r.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Reinforcement Learning System.</span>
          </div>
        </figure>
        <p></p>
        <p>The algorithm starts in an ignorant state, where it knows nothing, and begins to acquire data by testing the system. As it acquires data and results, it learns what the best and worst behaviours are (in this case, it learns which gateway is the best).</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3>Environment</h3>
          </div>
        </header>
        <p>In our transaction routing problem, the environment is the current probable approval rates of various gateways, which are unknown at any given point in time. The environment is hence non-stationary and non-stationary environment implies that the old information is potentially less relevant due to possible changes in the underlying rewards. For example, if a payment gateway was processing transactions at an approval rate of 80 percent two hours ago, the gateway may currently be down with zero approval rate.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3>Reward</h3>
          </div>
        </header>
        <p>Every transaction, once tried with one of the eligible gateways, results into a success or a failure which translates into the reward using the payoff function.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3>Recommender</h3>
          </div>
        </header>
        <p>The control system acts as a recommender for every transaction. The recommender takes a set of eligible gateways as input and responds with the best gateway to process the transaction depending on the state of environment it has learnt so far.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3>Feedback</h3>
          </div>
        </header>
        <p>The reward obtained after processing the transaction is fed into the control system to obtain the new state of the environment.</p>
        <p>Any transaction initiated in the system would ask for recommendation for payment gateway, and then the transaction is tried with the recommended gateway, the result of the trial is fed into the learning algorithm as reward, for example if the transaction was successful, it is considered 1 as reward for the gateway and if it is failed the reward is 0. This arrangement is called a Bernoulli arrangement as the rewards are only 0 and 1. The learning algorithm learns by the feedback loop and tries to maximize the successes.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Exploration vs. Exploitation : Beta Distribution</h3>
          </div>
        </header>
        <p>The underlying problem is to know when to explore and exploit while selecting the gateway. Exploring means that we try a new gateway in hopes that the probability of rewards is higher even if its current reward probability is not the best. Exploitation means that we continue selecting the best performing gateway. Exploitation is the right thing to do to maximize the expected reward in the given transaction, but exploration may produce the greater total reward in the long run. If a gateway is not producing successful transactions very often and we haven't sampled it too much then probability of success is low but our confidence in the estimate is low too, so let's give it a small chance in future. However, if a gateway has processed too many transactions already and doing good, our confidence in the estimate is also high, so let's give a higher chance of being recommended.</p>
        <p>To handle the problem of exploration and exploitation we used Beta Distribution. We used Beta Distribution for following convenient reasons.</p>
        <ol class="list-no-style">
          <li id="list1" label="(1)">Beta distribution can have values between 0 and 1, which is nice for our use case to represent probability of success and failure of the gateway. In general, it can present the transactional health of the gateway.<br /></li>
          <li id="list2" label="(2)">When the prior is a Beta distribution and the likelihood is a Bernoulli distribution, the posterior is also a Beta distribution.<br /></li>
          <li id="list3" label="(3)">The update formula for determining the posterior only involves addition and subtraction, whereas for other distributions it can be more complicated.<br /></li>
        </ol>
        <p>The Beta distribution has 2 parameters, <em>α</em> and <em>β</em>, which govern its shape. We refer to a specific distribution as Beta(<em>α</em>, <em>β</em>)</p>
        <p>Initially, we set <em>α</em> =1 and <em>β</em> =1. This results in a uniform distribution, i.e. we assume nothing about our chances of winning – all probabilities of winning are equally likely. This is called a minimally informative prior.</p>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Beta Distribution with increasing <em>β</em>. Notice that the graph is becoming sharper towards low value of health.</span>
          </div>
        </figure>
        <p></p>
        <p>A fat distribution means more “exploration”. A sharp distribution means more “exploitation” (if it has a relative high transaction health). Note that as the programmer, you don't choose whether or not to explore or exploit. You sample from the distributions, meaning it is randomly decided whether you should explore or exploit. Essentially the decision is random, with a bias toward gateways who have proven themselves to be more healthy.</p>
        <figure id="fig4">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span> <span class="figure-title">Beta Distribution with increasing <em>α</em>. Notice that the graph is becoming sharper towards high value of health.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Thompson Sampling</h3>
          </div>
        </header>
        <p>For simplicity of discussion, we first provide the details of Thompson Sampling algorithm for the Bernoulli problem, i.e. when the rewards are either 0 or 1, and for gateway <em>i</em> the probability of success (reward =1) is <em>μ<sub>i</sub></em> . Next, we propose a simple new extension of this algorithm to general reward distributions with support [0, 1].</p>
        <p>The algorithm for Bernoulli gateways maintains Bayesian priors on the Bernoulli means <span class="inline-equation"><span class="tex">$\mu _{i}^{\prime }s$</span></span> . Beta distribution turns out to be a very convenient choice of priors for Bernoulli rewards. Let us briefly recall that beta distributions form a family of continuous probability distributions on the interval (0, 1).</p>
        <figure id="fig5">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span> <span class="figure-title">Beta Distribution with increasing <em>α</em>. Notice that the graph is becoming sharper towards high value of health.</span>
          </div>
        </figure>
        <p></p>
        <p>The standard <em>Beta</em> distribution gives the probability density of a value <em>x</em> on the interval (0,1):</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} Beta(\alpha ,\beta):\,\, prob(x|\alpha ,\beta)=\frac{x^{\alpha -1}(1-x)^{\beta -1}}{B(\alpha ,\beta)} \end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>where <em>B</em> is the beta function
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ B(\alpha ,\beta)=\int _{0}^{1}t^{\alpha -1}(1-t)^{\beta -1}dt \]</span><br />
          </div>
        </div>. The mean of Beta(<em>α</em>, <em>β</em>) is <em>α</em>/(<em>α</em> + <em>β</em>); and as is apparent from the pdf, higher the <em>α</em>, <em>β</em>, tighter is the concentration of Beta(<em>α</em>, <em>β</em>) around the mean. Beta distribution is useful for Bernoulli rewards because if the prior is a Beta(<em>α</em>, <em>β</em>) distribution, then after observing a Bernoulli trial, the posterior distribution is simply Beta(<em>α</em>+1, <em>β</em>) or Beta(<em>α</em>, <em>β</em>+1), depending on whether the trial resulted in a success or failure, respectively. The Thompson Sampling algorithm initially assumes gateway <em>i</em> to have prior Beta(1, 1) on <em>μ<sub>i</sub></em> , which is natural because Beta(1, 1) is the uniform distribution on (0, 1). At time t, having observed <em>α<sub>t</sub></em> successes (reward = 1) and <em>β<sub>t</sub></em> failures (reward = 0) in <em>k<sub>i</sub></em> (<em>t</em>) = <em>α<sub>t</sub></em> + <em>β<sub>t</sub></em> transactions of gateway i, the algorithm updates the distribution on <em>μ<sub>i</sub></em> as Beta(<em>α<sub>t</sub></em> + 1, <em>β<sub>t</sub></em> + 1). The algorithm then samples from these posterior distributions of the <span class="inline-equation"><span class="tex">$\mu _{i}^{\prime }s$</span></span> , and processes a transaction through a gateway according to the probability of its mean being the largest. We summarize the Thompson Sampling algorithm below (Table 1).
        <p></p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Thompson Algorithm</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">Algorithm for Thompson Sampling</td>
              </tr>
              <tr>
                <td style="text-align:left;">1: <em>α<sub>i</sub></em> = 0, <em>β<sub>i</sub></em> = 0.</td>
              </tr>
              <tr>
                <td style="text-align:left;">2: for t = 1, 2, . . . , do</td>
              </tr>
              <tr>
                <td style="text-align:left;">3: For each gateway i = 1, . . . , N, sample <em>θ<sub>i</sub></em> (<em>t</em>)</td>
              </tr>
              <tr>
                <td style="text-align:left;">from the beta distribution with parameters (<em>α<sub>i</sub></em> +1, <em>β<sub>i</sub></em> + 1).</td>
              </tr>
              <tr>
                <td style="text-align:left;">4: Recommend for transaction<em>i</em>(<em>t</em>) := arg <em>max<sub>i</sub>θ<sub>i</sub></em> (<em>t</em>)</td>
              </tr>
              <tr>
                <td style="text-align:left;">and observe reward <em>r</em>.</td>
              </tr>
              <tr>
                <td style="text-align:left;">5: if r = 1, then <em>α<sub>i</sub></em> = <em>α<sub>i</sub></em> + 1, else <em>β<sub>i</sub></em> = <em>β<sub>i</sub></em> + 1.</td>
              </tr>
              <tr>
                <td style="text-align:left;">6: end for</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Non-stationary Rewards</h3>
          </div>
        </header>
        <p>The updating <em>α<sub>t</sub></em> and <em>β<sub>t</sub></em> can be appropriate for a stationary environment, where the probability distribution of rewards are stationary but the success rates of the gateways do not remain stationary over time, the health of a gateway keeps changing and hence the distribution is effectively non-stationary. In this case it makes sense to weight recent rewards more heavily than long-past ones. The way we have used for this to use a constant step-size parameter, and we call it as a history decay factor. The <em>α<sub>t</sub></em> and <em>β<sub>t</sub></em> are hence updated in the following way:</p>
        <div class="table-responsive" id="Xeq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \alpha _{t+1} = \lambda (\alpha _{t} + r_{t}) \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <div class="table-responsive" id="Xeq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \beta _{t+1} = \lambda (\beta _{t} + r_{t}) \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>Please note that <em>λ</em> is the decay factor and <em>r<sub>t</sub></em> is the reward at epoch <em>t</em>. The decaying the history also helps in setting up an upper bound to the values of <em>α<sub>t</sub></em> and <em>β<sub>t</sub></em> . The upper bounds of the these values ensure the possibility of exploration. We considered the rewards to be only 0 and 1 for simplicity of the discussion but these rewards can be any value between 0 and 1 to represent various rewards. For example, in our case, the transaction failures due to risk rules don't suggest that health degradation of the gateway so its reward could vary between 0 and 1. The Thompson Algorithm with the updates can be summarized as below (Table 2):
        <p></p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Thompson Algorithm</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:left;">Algorithm for Thompson Sampling with Decay</td>
              </tr>
              <tr>
                <td style="text-align:left;">1: <em>α<sub>i</sub></em> = 0, <em>β<sub>i</sub></em> = 0.</td>
              </tr>
              <tr>
                <td style="text-align:left;">2: for t = 1, 2, . . . , do</td>
              </tr>
              <tr>
                <td style="text-align:left;">3: For each gateway i = 1, . . . , N, sample <em>θ<sub>i</sub></em> (<em>t</em>)</td>
              </tr>
              <tr>
                <td style="text-align:left;">from the beta distribution with parameters (<em>α<sub>i</sub></em> +1, <em>β<sub>i</sub></em> + 1).</td>
              </tr>
              <tr>
                <td style="text-align:left;">4: Recommend for transaction<em>i</em>(<em>t</em>) := arg <em>max<sub>i</sub>θ<sub>i</sub></em> (<em>t</em>)</td>
              </tr>
              <tr>
                <td style="text-align:left;">and observe reward <span class="inline-equation"><span class="tex">$\lbrace r_{\alpha _{t}}, r_{\beta _{t}} \rbrace$</span></span> .</td>
              </tr>
              <tr>
                <td style="text-align:left;">5: <em>α<sub>i</sub></em> = <em>λ</em> (<em>α<sub>i</sub></em> + <span class="inline-equation"><span class="tex">$r_{\alpha _{t}}$</span></span> ), and <em>β<sub>i</sub></em> = <em>λ</em>(<em>β<sub>i</sub></em> + <span class="inline-equation"><span class="tex">$r_{\beta _{t}}$</span></span> ).</td>
              </tr>
              <tr>
                <td style="text-align:left;">6: end for</td>
              </tr>
            </tbody>
            <tfoot>
              <tr>
                <td></td>
              </tr>
            </tfoot>
          </table>
        </div>
      </section>
    </section>
    <section id="sec-15">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Results and discussion</h2>
        </div>
      </header>
      <p>We have compared the success rates of the recommended and existing dynamic switching algorithms in Figure 6. The current implementation looks at the monthly, weekly and hourly success rates for each gateway for making a recommendation for the transaction. The probability of making a successful transaction is calculated using Bays’ Theorem. The implementation takes the notion of discrete success rates for each gateway. The exploration for the new gateways or the gateways which were marked as down, is done on a regular interval.</p>
      <ul class="list-no-style">
        <li id="list4" label="•">The graphs (Figure 6 and Figure 7) represent the comparison of success rates for the overall transaction traffic and for top 20 merchants at PayU.<br /></li>
        <li id="list5" label="•">The data is on the basis of gateways allowed for specific merchants.<br /></li>
        <li id="list6" label="•">CURRENT_SR: Success rate, as realized by current implementation. This forces the eligible payment gateway for each transaction according to the current settings.<br /></li>
        <li id="list7" label="•">RECOMMENDED_SR: Success rate by the new learning algorithm.<br /></li>
        <li id="list8" label="•">BEST_SR: The best possible success rate if all the eligible gateways are enabled for the merchant.<br /></li>
        <li id="list9" label="•">Eligible Gateways: Every transaction has a set of eligible gateways which can process the transaction. This set depends on the payment method used by the customer and the gateways enabled on the merchant.<br /></li>
      </ul>
      <figure id="fig6">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig6.jpg" class="img-responsive" alt="Figure 6" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 6:</span> <span class="figure-title">Recommended vs Current Success Rates.</span>
        </div>
      </figure>
      <figure id="fig7">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191630/images/www18companion-369-fig7.jpg" class="img-responsive" alt="Figure 7" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 7:</span> <span class="figure-title">Recommended vs Current vs Best Success Rates.</span>
        </div>
      </figure>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Impact</h3>
          </div>
        </header>
        <ul class="list-no-style">
          <li id="list10" label="•">The proposed implementation results into the overall success rate improvement by 3.23% from the current algorithm. This improvement is obtained without changing any arrangement for eligibility of a gateway for a transaction depending on its merchant and bin.<br /></li>
          <li id="list11" label="•">The proposed implementation results into the overall success rate improvement by 8.21% if all the routable gateways are enabled for all the transactions. This considers only eligible gateways for a given transaction in terms of bin processing ability of a transaction.<br /></li>
          <li id="list12" label="•">The proposed implementation provides an ability to predict the success rate and the cost if a particular payment gateway is enabled for a merchant. This can be used to give merchant visibility about enabling a particular gateway. This will also help in enabling more gateways on a merchant and take benefit of having more options while routing a transaction.<br /></li>
          <li id="list13" label="•">The proposed solutions enables us to predict volume shift between various gateways in various arrangements.<br /></li>
          <li id="list14" label="•">The exploration is done by the transaction with smaller amounts which reduces the loss occured due to exploration process.<br /></li>
          <li id="list15" label="•">The proposed implementation is more sensitive to changes in the health of gateways, hence it behaves better in the situations when a gateway goes down or becomes healthy.<br /></li>
          <li id="list16" label="•">Cost based routing resulted into improvement of overall margins without impacting the success rates.<br /></li>
        </ul>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion</h2>
        </div>
      </header>
      <p>The application of reinforcement learning to the transaction routing problem has resulted into significant improvement in both success rates and margins. The reward tweaking has also contributed in further improvement of our understanding of impact of various failure reasons on overall success rates of various gateways. The growing interest for machine learning and reinforcement learning, in particular, arises from the large number of industrially relevant problems that can be modeled as a multi-path routing problems. A large number of problems in science and engineering, from robotics to game playing, tutoring systems, resource management, financial portfolio management, medical treatment design and beyond, can be characterized as sequential decision-making under uncertainty. Many interesting sequential decision-making tasks can be formulated as reinforcement learning problems. The techniques explained in this paper can be generalized to many such problems by simply modifying the model used for reward distribution.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Omar Besbes, Yonatan Gur, Assaf Zeevi. <em>Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-stationary Rewards.</em></li>
        <li id="BibPLXBIB0002" label="[2]">P. Auer, N. Cesa-Bianchi, and P. Fischer. <em>Finite-time analysis of the multiarmed bandit problem.</em> <em>Machine Learning</em>, 47(2-3):235–256, 2002.</li>
        <li id="BibPLXBIB0003" label="[3]">M. Zelen. <em>Play the winner rule and the controlled clinical trials.</em> <em>Journal of the American Statistical Association</em>, 64:131–146, 1969.</li>
        <li id="BibPLXBIB0004" label="[4]">D. A. Berry and B. Fristedt. <em>Bandit problems: sequential allocation of experiments.</em>Chapman and Hall, 1985.</li>
        <li id="BibPLXBIB0005" label="[5]">O. Chapelle and L. Li. <em>An empirical evaluation of thompson sampling.</em>In NIPS, 2011.</li>
        <li id="BibPLXBIB0006" label="[6]">P. Auer, N. Cesa-Bianchi, and P. Fischer. <em>Finite-time analysis of the multiarmed bandit problem.</em> <em>Machine Learning</em>, 2002.</li>
        <li id="BibPLXBIB0007" label="[7]">R. D. Kleinberg and T. Leighton. <em>The value of knowing a demand curve: Bounds on regret for online posted-price auctions.</em> In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 594–605, 2003.</li>
        <li id="BibPLXBIB0008" label="[8]">Sebastien Bubeck and Nicolo Cesa-Bianchi. <em>Regret analysis of stochastic and nonstochastic multi-armed bandit problems.</em> <em>Foundations and Trends in Machine Learning</em>, 5(1):1–122, 2012.</li>
        <li id="BibPLXBIB0009" label="[9]">J. C. Gittins. <em>Bandit processes and dynamic allocation indices (with discussion).</em> <em>Journal of the Royal Statistical Society, Series B</em>, 41:148–177, 1979.</li>
        <li id="BibPLXBIB0010" label="[10]">O. Besbes, Y. Gur, and A. Zeevi. <em>Non-stationary stochastic optimization.</em>Working paper, 2014.</li>
        <li id="BibPLXBIB0011" label="[11]">S. Agrawal and N. Goyal. <em>Analysis of thompson sampling for the multi-armed bandit problem.</em> <em>CoRR, abs/2011.1797</em>, 2011.</li>
        <li id="BibPLXBIB0012" label="[12]">M. Babaioff, Y. Sharma, and A. Slivkins. <em>Characterizing truthful multi-armed bandit mechanisms: extended abstract.</em> <em>In Tenth ACM Conference on Electronic Commerce</em>, pages 79–88. ACM, 2009.</li>
        <li id="BibPLXBIB0013" label="[13]">O. Chapelle and L. Li. <em>An empirical evaluation of thompson sampling.</em>In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, <em>editors, Advances in Neural Information Processing Systems 24</em>, pages 2249–2257. Curran Associates, Inc., 2011.</li>
        <li id="BibPLXBIB0014" label="[14]">N. Gatti, A. Lazaric, and F. Trovò. <em>A truthful learning mechanism for contextual multi-slot sponsored search auctions with externalities.</em> <em>In Thirteenth ACM Conference on Electronic Commerce</em>, pages 605–622, 2012.</li>
        <li id="BibPLXBIB0015" label="[15]">O.-C. Granmo. <em>Solving two-armed bernoulli bandit problems using a bayesian learning automaton.</em> <em>International Journal of Intelligent Computing and Cybernetics</em>, 3(2):207–234, 2010.</li>
        <li id="BibPLXBIB0016" label="[16]">S. Scott. <em>A modern bayesian look at the multi-armed bandit.</em> <em>Applied Stochastic Models in Business and Industry</em>, 26:639–658, 2010.</li>
        <li id="BibPLXBIB0017" label="[17]">D. Bergemann and J. Valimaki. <em>Learning and strategic pricing.</em> <em>Econometrica</em>, 64:1125–1149, 1996.</li>
        <li id="BibPLXBIB0018" label="[18]">D. Bergemann and U. Hege. <em>The financing of innovation: Learning and stopping.</em> <em>RAND Journal of Economics</em>, 36 (4):719–752, 2005.</li>
        <li id="BibPLXBIB0019" label="[19]">B. Awerbuch and R. D. Kleinberg. <em>Addaptive routing with end-to-end feedback: distributed learning and geometric approaches.</em> In Proceedings of the 36th ACM Symposiuim on Theory of Computing (STOC), pages 45–53, 2004.</li>
        <li id="BibPLXBIB0020" label="[20]">F. Caro and G. Gallien. <em>Dynamic assortment with demand learning for seasonal consumer goods.</em> <em>Management Science</em>, 53:276–292, 2007.</li>
        <li id="BibPLXBIB0021" label="[21]">S. Pandey, D. Agarwal, D. Charkrabarti, and V. Josifovski. 53:27<em>Bandits for taxonomies: A model-based approach.</em>In SIAM International Conference on Data Mining, 2007.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191630">https://doi.org/10.1145/3184558.3191630</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
