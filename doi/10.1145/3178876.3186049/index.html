<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>A Fast Deep Learning Model for Textual Relevance in Biomedical Information Retrieval</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186049"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186049'>https://doi.org/10.1145/3178876.3186049</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186049'>https://w3id.org/oa/10.1145/3178876.3186049</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">A Fast Deep Learning Model for Textual Relevance in Biomedical Information Retrieval</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Sunil</span> <span class="surName">Mohan<a class="fn" href="#fn1" id="foot-fn1"><sup>⁎</sup></a></span>, Chan Zuckerberg Initiative, Palo Alto, CA 94301USA, <a href="mailto:smohan@chanzuckerberg.com">smohan@chanzuckerberg.com</a>
        </div>
        <div class="author">
          <span class="givenName">Nicolas</span> <span class="surName">Fiorini</span>, National Center for Biotechnology Information, Bethesda, MD 20894USA, <a href="mailto:nicolas.fiorini@nih.gov">nicolas.fiorini@nih.gov</a>
        </div>
        <div class="author">
          <span class="givenName">Sun</span> <span class="surName">Kim</span>, National Center for Biotechnology Information, Bethesda, MD 20894USA, <a href="mailto:sun.kim@nih.gov">sun.kim@nih.gov</a>
        </div>
        <div class="author">
          <span class="givenName">Zhiyong</span> <span class="surName">Lu</span>, National Center for Biotechnology Information, Bethesda, MD 20894USA, <a href="mailto:zhiyong.lu@nih.gov">zhiyong.lu@nih.gov</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186049" target="_blank">https://doi.org/10.1145/3178876.3186049</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Publications in the life sciences are characterized by a large technical vocabulary, with many lexical and semantic variations for expressing the same concept. Towards addressing the problem of relevance in biomedical literature search, we introduce a deep learning model for the relevance of a document's text to a keyword style query. Limited by a relatively small amount of training data, the model uses pre-trained word embeddings. With these, the model first computes a variable-length Delta matrix between the query and document, representing a difference between the two texts, which is then passed through a deep convolution stage followed by a deep feed-forward network to compute a relevance score. This results in a fast model suitable for use in an online search engine. The model is robust and outperforms comparable state-of-the-art deep learning approaches.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Learning to rank;</strong> <em>Probabilistic retrieval models;</em> • <strong>Computing methodologies</strong> → <strong>Learning to rank;</strong> <strong>Supervised learning by classification;</strong> <strong>Neural networks;</strong> • <strong>Applied computing</strong> → <strong>Life and medical sciences;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Deep Learning; Biomedical Information Retrieval; Search; Learning to Rank</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Sunil Mohan, Nicolas Fiorini, Sun Kim, and Zhiyong Lu. 2018. A Fast Deep Learning Model for Textual Relevance in Biomedical Information Retrieval. In <em>WWW 2018: The 2018 Web Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186049" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3178876.3186049</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>PubMed<sup>®</sup><a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> is a free online search engine covering over 27 million articles from biomedical and life sciences journals and other texts, with about 1 million added each year. It is used worldwide by biomedical researchers, add healthcare professionals as well as lay people, serving about 3 million queries a day [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>]. While expert users either search for most recent articles by an author or construct elaborate query expressions, most queries are short keyword-ese, covering one or two biomedical concepts. Although the size of the corpus is much smaller than in general web search, biomedical literature uses a very large technical vocabulary (e.g. the UMLS<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a> metathesaurus [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] specifies over 3 million biomedical concepts, along with several lexical variations and synonymous phrases. This makes it much harder to identify concepts across documents (e.g. see [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>]). To improve the retrieval, PubMed expands a user's query by mapping it to related MeSH<sup>®</sup> terms [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>]. While this increases recall, it often decreases precision [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. Usage analysis [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] shows that PubMed users are persistent, often reformulating their query, issuing over 4 queries per session on average. As part of improving relevance for such keyword queries, we describe a deep learning model that addresses the relevance of a document's text to the query. The eventual goal is for this model to be incorporated as a factor into a reranker that also includes other document attributes and metadata (e.g. year, journal).</p>
      <p>To train our model, we collected data from PubMed click logs, restricting this to relevance search instead of the default sort order by date. Removing author searches and disjunctive boolean expressions resulted in a training set of about 20k queries. Given the small size of this data, we pre-trained word embeddings using <tt>word2vec</tt> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] on the entire PubMed corpus, producing a vocabulary of about 200k. This large gap beween training data and vocabulary sizes highlights a major challenge: how to make the model robust? Our Delta deep learning model begins by computing a variable-sized ‘Delta’ matrix between a document and a query, comprising the vector difference between document word embeddings and the closest matching query word, and three scalar similarity measures between the query and document. The document is truncated to control run-time cost. The Delta matrix is processed through a stacked convolutional network and pooled to a fixed length. This, together with a summary query match statistic, is processed by a feed-forward network to produce a relevance score. Pairwise loss is optimized for training. This approach produces a model that is both robust, and fast enough for use in a search engine.</p>
      <p>In addition to model robustness, we also wanted to address two common search engine problems: (i) the <em>under-specified query problem</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>], where even irrelevant documents have prominent presence of the query terms, and relevance requires analysis of the topics and semantics not directly specified in the query, and (ii) the <em>term mismatch problem</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], which requires detection of related alternative terms or phrases in the document when the actual query terms are not in the document. Our experiments show the Delta model outperforms traditional lexical match factors and some related state-of-the-art neural approaches.</p>
      <p>The next sections discuss some related work, followed by a description of the model, the experiments and evaluation of results, ending with some concluding remarks.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>Traditional lexical Information Retrieval (IR) factors, like Okapi BM25 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] and Query Likelihood [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>], measure the prominence of query terms occurring in documents treated as bags of words. Neural approaches to text relevance attempt to go beyond exact matches of query terms in documents, and model a degree of semantic match as a complex function in a continuous space (good reviews can be found in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>]). We will discuss some related approaches here.</p>
      <p>Most neural models begin by mapping words to points embedded in a real space. A popular approach (e.g. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>]), also used in our model, is to pre-train word embeddings, e.g. using <tt>word2vec</tt> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>]. The benefit of this approach is that a much larger unlabeled corpus can be used to train the embeddings, and our ‘Delta matrix’ takes advantage of the semantic relationships captured in the vector differences between words.</p>
      <p>The simplest embeddings based model is Word Mover's Distance (WMD) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>], a non-parameterized model for text similarity that does not require any training. We use this as one of our baselines. Pre-trained embeddings are not necessarily targeted for optimal relevance scores. Nalisnick et al. <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a> also use the ‘input’ vectors normally discarded by <tt>word2vec</tt> to overcome some of this limitation. The ‘DSSM’ models of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>] take a different approach by mapping each word to a bag of letter tri-grams and combining the corresponding one-hot vectors. Xiong et al. <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a> show that training word embeddings as part of the relevance model has a major impact on the performance of a relvance model. However this requires a large amount of training data. Diaz et al. <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a> show that ‘locally trained’ embeddings on pseudo-relevance feedback documents can provide better results, while admitting that this approach is not “computationally convenient”.</p>
      <p>Neural relevance models also differ in how they process document and query text. Some (e.g. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>]) process each document and query using separate ‘Siamese’ networks into independent semantic vectors. A second stage then scores the similarity between these vectors. This approach is very attractive for search engines, because the document vectors can be pre-processed and stored, and the query vector need be produced once before scoring the documents, significantly reducing the cost at query time. We use the recent model described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>] as a baseline.</p>
      <p>Another approach to text matching first develops ‘local interactions’ by comparing all possible combinations of words and word sequences between the document and query texts, often starting with a document-query word similarity matrix. Examples are described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>]. The authors in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] argue that the local interaction based approach is better at capturing detail, especially exact query term matches, and in their experiments their ‘DRMM’ model outperforms many previous approaches. This is a more computationally intensive architecture that does not allow any pre-computation. We take a similar approach by pairing each document word with a single query word, followed by deep convolutions to capture some related compositional semantics. Run-time cost in our approach is controlled by truncating the document. We show that our approach outperforms the DRMM model.</p>
      <p>The ‘PACRR-firstk’ model in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>] also truncates the document, then processes the resulting similarity matrix through several 2D convolutional layers aimed at capturing n-gram similarities, followed by a recurrent layer, resulting in a fairly complex model. The ‘DUET’ model described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>] combines a local interaction model with an independent semantic vector model, with the goal of combining the benefits of ‘exact match’ and embedding based word similairities. Our simpler approach explicitly targets run-time efficiency, and a variant of the Delta model combines some lexical factors (similar to [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]) to further improve ranking performance.</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> The Delta Model</h2>
        </div>
      </header>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186049/images/www2018-58-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">The Delta Relevance Model.</span>
        </div>
      </figure>
      <p>The components of the Delta Relevance Model (figure&nbsp;<a class="fig" href="#fig1">1</a>) are described below. The unshaded blocks represent inputs to the model: two vectors of word indices, one each for the Document <em>D</em> and the Query <em>Q</em>, and a vector of query-document Lexical Match factors <strong>L</strong> <sub><em>DQ</em></sub> = lexmatch(<em>D</em>, <em>Q</em>) for some chosen lexical match function.</p>
      <p>The small size of the training data (∼ 20, 000 queries) compared to the vocabulary size (∼ 200, 000) prevented us from training word embeddings as part of the model training. We had to adapt the word vectors pre-trained using word2vec's unsupervised approach, to the task of relevance prediction. The Delta model uses two techniques that help in this and thus learn a richer and more robust decision surface. Changing the input space from word embeddings to differences in word embeddings shifts the domain of the decision surface to coordinates relative to the query. In addition, the Delta model's use of a stack of convolution layers instead of a single layer adds more non-linearities to help capture a complex decision surface, a technique successful in image recognition [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>]. The convolution layers also extract relevance-match signals from text <em>n</em>-grams, and are much faster than a recurrent layer which has a similar goal.</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Word Embeddings</h3>
          </div>
        </header>
        <p>We leveraged the large PubMed corpus of over 27 million documents to pre-train the word vectors, using the SkipGram Hierarchical Softmax method of <tt>word2vec</tt> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], with a window size of ± 5, a minimum term-frequency of 101, and a word-vector size of <em>V</em> = 300 (see [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] for experiments with different parameter settings for biomedical text). This resulted in a vocabulary of 207,716 words. Rare words were replaced with the generic “UNK” token, which was initialized to ∼ <em>U</em>[ − 0.25, 0.25], as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>].</p>
        <p>Given a document word sequence <span class="inline-equation"><span class="tex">$D = \langle {w^{d}_1, \ldots , w^{d}_N} \rangle$</span></span> and query text <span class="inline-equation"><span class="tex">$Q = \langle {w^{q}_1, \ldots , w^{q}_M} \rangle$</span></span> , where <em>w<sub>i</sub></em> are indices into the vocabulary, the Embeddings layer replaces each word with its vector, giving us <em>D<sup>e</sup></em> = ⟨<em>d</em> <sub>1</sub>, …, <em>d<sub>N</sub></em> ⟩, <em>Q<sup>e</sup></em> = ⟨<em>q</em> <sub>1</sub>, …, <em>q<sub>M</sub></em> ⟩ where each <span class="inline-equation"><span class="tex">$d_i, q_i \in \operatorname{\mathbb {R}}^{V}$</span></span> , and <em>V</em> is the size of the word embedding. If a document has fewer than <em>N</em> words, or the query fewer than <em>M</em> words, they are padded on the right with zeros. Longer documents are truncated, and <em>M</em> is the longest query length in our data (see section <a class="sec" href="#sec-13">4.1</a>).</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> The Delta Stage</h3>
          </div>
        </header>
        <p>This is an unparameterized stage, responsible for computing the Delta Matrix between the Document and the Query as follows:</p>
        <ol class="list-no-style">
          <li id="list1" label="(1)">Compute the Euclidean distance between each pair <em>d<sub>i</sub></em> , <em>q<sub>j</sub></em> .<br /></li>
          <li id="list2" label="(2)">For each document word <em>d<sub>i</sub></em> , determine the closest query word <span class="inline-equation"><span class="tex">$q^{*}_i$</span></span> , using these distances.<br /></li>
          <li id="list3" label="(3)">Compute the vector differences <span class="inline-equation"><span class="tex">$(d_i - q^{*}_i)$</span></span> .<br /></li>
          <li id="list4" label="(4)">Compute the Delta features: cosine<span class="inline-equation"><span class="tex">$(d_i, q^{*}_i)$</span></span> , <span class="inline-equation"><span class="tex">$|d_i - q^{*}_i|$</span></span> and normalized proximity similarity metric <span class="inline-equation"><span class="tex">$1 - |d_i - q^{*}_i| / (|d_i| + |q^{*}_i|)$</span></span> .<br /></li>
        </ol>
        <p>The output of this stage is <em>Δ</em> = ⟨<em>δ</em> <sub>1</sub>, …, <em>δ<sub>N</sub></em> ⟩, a <em>N</em> × (<em>V</em> + 3) sized real matrix. All operations above are masked to ignore padding.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> The Network</h3>
          </div>
        </header>
        <p>The trainable portion of the Delta model consists of a Convolutional Stage followed by a Feed-Forward Stage. The Convolutional stage attempts to pick up significant contextual and <em>n</em>-gram similarity features. These are combined with the Lexical Match features <strong>L</strong> <sub><em>DQ</em></sub> , then processed by the Feed-Forward Stage to produce a final relevance score. The weights for the layers in these stages comprise the trainable parameters for the model.</p>
        <p>A convolution operation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>] has the parameters: border mode, number of filters or feature maps <em>n<sub>f</sub></em> , filter width <em>k</em> and stride <em>s</em>. We use 1-dimensional convolution along the text width with border mode ‘<em>same</em>’ which implicitly pads the input on either side before convolving, and a stride <em>s</em> = 1, resulting in an output of the same width as the input. Using <em>conv</em>(<em>X</em>; <em>n<sub>f</sub></em> , <em>k</em>) to represent such a convolution on input <em>X</em>, the Delta model's Convolutional Stage performs the following operations:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \mathbf {Y}^{(i)} &amp;= f_i(conv(\mathbf {Y}^{(i-1)}; n_f, k)) &amp; i = 1, \ldots , N_C\end{align*}</span><br />
          </div>
        </div>where <em>f<sub>i</sub></em> is an activation function, <strong>Y</strong> <sup>(0)</sup> = <em>Δ</em>, <em>N<sub>C</sub></em> is the number of convolution layers, and <span class="inline-equation"><span class="tex">$\mathbf {Y}^{(N_C)} \in \operatorname{\mathbb {R}}^{N \times n_f}$</span></span> . The number of filters and filter width are kept the same for each layer, and all operations are masked to ignore padding.
        <p></p>
        <p>The output of the final convolution layer is ‘max-pooled’ by taking the maximum of each of the <em>n<sub>f</sub></em> output features along the text width dimension, yielding a vector of size <em>n<sub>f</sub></em> . This is the output of the Convolution Stage:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathbf {Y} = \max (\mathbf {Y}^{(N_C)}; \text{axis}=0) \in \operatorname{\mathbb {R}}^{n_f} \]</span><br />
          </div>
        </div>This output is combined with the Lexical Match features <strong>L</strong> <sub><em>DQ</em></sub> and sent to the Feed-Forward stage which is a series of <em>N<sub>F</sub></em> layers:
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} \mathbf {Z}^{(i)} &amp;= g_i(\mathbf {Z}^{(i-1)} \cdot \mathbf {W}_i + \mathbf {b}_i) &amp; i = 1, \ldots , N_F\end{align*}</span><br />
          </div>
        </div>Here: <em>g<sub>i</sub></em> is an activation function, <strong>Z</strong> <sup>(0)</sup> = ⟨<strong>Y</strong>, <strong>L</strong> <sub><em>DQ</em></sub> ⟩,&nbsp; <strong>Z</strong> <sup>(<em>i</em> − 1)</sup> · <strong>W</strong> <sub><em>i</em></sub> is the matrix multiplication operation, <em>N<sub>F</sub></em> is the number of layers in the Feed-Forward stage, and <strong>W</strong> <sub><em>i</em></sub> , <strong>b</strong> <sub><em>i</em></sub> are sized so that the number of outputs of the <em>i</em>-th layer are <em>K<sub>i</sub></em> . The output of the final layer is a single number, i.e. <span class="inline-equation"><span class="tex">$K_{N_F} = 1, \mathbf {Z}^{(N_F)} \in \operatorname{\mathbb {R}}$</span></span> , representing the relevance score of the document <em>D</em> to the query <em>Q</em>.
        <p></p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Training The Model</h3>
          </div>
        </header>
        <p>The training data derived from PubMed's click logs provides relevance levels for query-document pairs based on the number of clicks they received (see next section for more details). The Delta relevance model is trained to give more relevant documents a higher score by tuning its parameters <em>Θ</em> to minimize the pairwise maximum margin loss. Given a query <em>Q</em> and two matching documents <em>D</em> <sup>+</sup>, <em>D</em> <sup>−</sup> where <em>D</em> <sup>+</sup> has higher relevance to the query than <em>D</em> <sup>−</sup>, the loss for this triple is expressed as:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \mathcal {L}(Q, D^+, D^-; \Theta) = \max (0, 1 - s(Q, D^+; \Theta) + s(Q, D^-; \Theta)) \]</span><br />
          </div>
        </div>where <em>s</em>(<em>Q</em>, <em>D</em>; <em>Θ</em>) is the relevance score produced by the Delta model for the query-document pair <em>Q</em>, <em>D</em>. The Adagrad [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] stochastic gradient descent method was used to train the model, using a mini-batch size of 256. In addition to early stopping, separate L2 regularization costs were added on the weights of the Convolutional and Feed-Forward stages, and a dropout layer was added before the max-pooling layer in the Convolutional stage. The regularization coefficients and dropout probability were tuned using the held out validation data. Adding dropout layers to the Feed-Forward stage was also tested but was not found to help.
        <p></p>
      </section>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experimental Setup</h2>
        </div>
      </header>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class="table-title">Test data and its subsets.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;"></th>
              <th style="text-align:right;">Full Test Data</th>
              <th style="text-align:right;">Neg20+</th>
              <th style="text-align:right;">OneNewWord</th>
              <th style="text-align:right;">AllNewWords</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;">Nbr. of Queries</td>
              <td style="text-align:right;">6,734</td>
              <td style="text-align:right;">2,600</td>
              <td style="text-align:right;">1,732</td>
              <td style="text-align:right;">933</td>
            </tr>
            <tr>
              <td style="text-align:left;">Nbr. of Samples</td>
              <td style="text-align:right;">413,971</td>
              <td style="text-align:right;">208,723</td>
              <td style="text-align:right;">86,438</td>
              <td style="text-align:right;">47,825</td>
            </tr>
            <tr>
              <td style="text-align:left;">Prop. of Samples +ive</td>
              <td style="text-align:right;">45.2%</td>
              <td style="text-align:right;">39.5%</td>
              <td style="text-align:right;">49.2%</td>
              <td style="text-align:right;">49.2%</td>
            </tr>
            <tr>
              <td style="text-align:left;">Prop. of Samples -ive</td>
              <td style="text-align:right;">54.8%</td>
              <td style="text-align:right;">60.5%</td>
              <td style="text-align:right;">50.8%</td>
              <td style="text-align:right;">50.8%</td>
            </tr>
            <tr>
              <td style="text-align:left;">+ives without all Query terms in Title</td>
              <td style="text-align:right;">38.7%</td>
              <td style="text-align:right;">13.9%</td>
              <td style="text-align:right;">32.7%</td>
              <td style="text-align:right;">22.9%</td>
            </tr>
            <tr>
              <td style="text-align:left;">-ives with all Query terms in Title</td>
              <td style="text-align:right;">59.5%</td>
              <td style="text-align:right;">83.6%</td>
              <td style="text-align:right;">68.2%</td>
              <td style="text-align:right;">78.0%</td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> The Data</h3>
          </div>
        </header>
        <p>We collected query-document pairs extracted from PubMed click logs over several months where users selected ‘Best Match’ (relevance) as the retrieval sort order and clicked on at least one document in the search results. We recorded the first page of results of up to 20 documents, supplemented with the clicked document if it was not on the first page. Since our primary goal was to improve relevance for simple keyword style queries, we discarded queries containing disjunctive expressions, faceted queries, and queries longer than 7 words. Log extracts were further restricted to queries with at least 21 documents, and at least 3 clicked documents. These filters reduced the logs to about 33,500 queries, which were randomly split to 60% training, and 20% each for validation and testing.</p>
        <p><em>Relevance Levels.</em> The relevance level assigned to each query-document pair extracted from click logs is a probability of relevance, scaled to the range [0, 100] so that the minimum possible non-zero relevance is 1. For each query-document pair <em>Q</em>, <em>D</em>, we accumulated over the collection period the number of click-throughs <em>c</em>(<em>D</em>, <em>Q</em>) from search results to the document summary page, whether the document's full-text was available in PubMed <em>I<sub>ft</sub></em> (<em>D</em>) ∈ {0, 1}, and the number of subsequent click-throughs <em>c<sub>ft</sub></em> (<em>D</em>, <em>Q</em>) to the document's full-text. These were used to derive a weighted click-count <em>c<sub>w</sub></em> (<em>Q</em>, <em>D</em>) that rewarded documents for which full-text was requested without penalizing those for which full-text was not available. From that a probability of relevance <em>rel</em>(<em>Q</em>, <em>D</em>) was calculated:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} c_w(Q,D) &amp;= \left(\mu + (1 - I_{ft}(D)).\lambda \right) \cdot c(D,Q) + (1 - \mu) \cdot c_{ft}(D,Q) \\rel(Q,D) &amp;= \frac{c_w(Q,D)}{\sum _{D^{^{\prime }}} c_w(Q,D^{^{\prime }})} \\srel(Q,D) &amp;= 1 + 99 \times rel(Q,D) \ldots \text{ if } rel(Q,D) {\gt} 0 \\ &amp;= 0 \hspace{68.00003pt} \ldots \text{ if } rel(Q,D) = 0\end{align*}</span><br />
          </div>
        </div>Finally, we scaled the non-zero relevance levels to the range (1, 100] to get <em>srel</em>(<em>Q</em>, <em>D</em>). This ensured a minimum margin between documents of low relevance and no relevance, and also put a high penalty in the NDCG metric for ranking high relevance documents below low relevance ones. The coefficients were tuned to match NCBI domain experts’ relevance judgments: <em>μ</em> = 0.333, <em>λ</em> = 0.067.
        <p></p>
        <p><em>Tokenization.</em> Each document in our data had a Title and an Abstract. For the neural models, we concatenated these to form the document's ‘Text’. All document and query text was tokenized by splitting on space and punctuation, while preserving abbreviations and numeric forms, followed by a conversion to lower-case. To further reduce the vocabulary size, all punctuation was discarded and numeric forms were collapsed into 7 classes: Integer, Fraction in (0, 1), Real number, year “19xx”, year “20xx”, Percentage (number followed by “%”), and dollar amount (number preceded by “<font style="normal">$</font>”). While <tt>word2vec</tt> processed the tokenzed documents in sentences, the document input to the neural models was a flat sequence of words without sentence breaks or markers. The distribution of document text widths (nbr. of words) in the data is shown in figure&nbsp;<a class="fig" href="#fig2">2</a> a. We experimented with stopword removal in the Query and the Document, but they did not help.</p>
        <p><em>Test Data Subsets.</em> The 20% held out test data comprised 6,734 queries and 413,971 samples (query-document pairs). Presence of query words in a document's Title is often a good indication of relevance. Among the relevant documents (“+ives”) for all the test queries, 38.7% did not contain all query terms in the title. Similarly 59.5% of all the non-relevant documents (“-ives”) actually contained all the query terms in their title (see table&nbsp;<a class="tbl" href="#tab1">1</a>).</p>
        <p>In addition to comparing ranking metrics of the different approaches on the test data, we wanted to explore model robustness, and model performance with <em>under-specified queries</em>. To help answer these questions, we also compared ranking metrics on the following subsets of the test data:</p>
        <ul class="list-no-style">
          <li id="uid18" label="Neg20+:">This consisted of all queries for which there were at least 20 non-relevant documents that contained all the query words in the title. This subset was used to evaluate performance on under-specified queries.<br /></li>
          <li id="uid19" label="OneNewWord:">The 1,732 test queries that contained at least one new word not occurring in any training or validation queries.<br /></li>
          <li id="uid20" label="AllNewWords:">A smaller subset of queries all of whose words were new: none of the training or validation queries included these words.<br /></li>
        </ul>
        <p>The last two subsets help evaluate model robustness. The statistics of the test data and its subsets are summarized in table&nbsp;<a class="tbl" href="#tab1">1</a>.</p>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Configuration Settings for the Delta Model</h3>
          </div>
        </header>
        <p>The Delta Model's hyper-parameters were tuned to optimize the ranking metric NDCG.20 on the validation data. We found truncating documents to the first <em>N</em> = 50 words provided a good compromise between ranking performance and the run time to score a query-document pair (discussed below), with larger values providing only marginal improvements. The maximum query size was <em>M</em> = 7 as described above. The Convolutional stage used <em>N<sub>C</sub></em> = 3 layers of convolutions, each with a filter width <em>k</em> = 3. We report metrics for various number of filters <em>n<sub>f</sub></em> below. The Feed-Forward stage used <em>N<sub>F</sub></em> = 3 layers. Finally, we found downsampling the training data so that there were an equal number of relevant and non-relevant documents for each query to produce the best model, resulting in 7,084,244 training samples of (<em>Q</em>, <em>D</em> <sup>+</sup>, <em>D</em> <sup>−</sup>) triples. This downsampling was also performed for the other neural models described below. The validation and test data were not downsampled.</p>
        <p>With the maximum-margin loss function, there was no reason to constrain the range of the final layer's activation function. We got best results using the Leaky Rectified Linear Unit (Leaky ReLU) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>] with the slope of the negative region fixed at <em>α</em> = 0.3. The Leaky ReLU was also used as the activation function for all the other layers of the Feed-Forward and Convolutional stages.</p>
        <p>An earlier version of the Delta model is described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]. The main changes since then are: a simpler Delta Matrix, changes to the activation functions used in all the stages, training to a pairwise loss function with different sample weighting, and comprehensive test on a number of lexical features. These changes resulted in a <span class="inline-equation"><span class="tex">$\sim 10\%$</span></span> improvement in the NDCG metrics. We only report metrics for the current version of the Delta model below, along with a comparison against some new baselines.</p>
        <section id="sec-15">
          <p><em>4.2.1 Relevance-based Sample Weighting.</em> Best results were obtained by adding a weight to each (<em>Q</em>, <em>D</em> <sup>+</sup>, <em>D</em> <sup>−</sup>) training sample in the loss function by taking the square-root of the difference in the scaled-relevance levels of the two documents:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \text{weight}(Q, D^+, D^-) = (srel(Q, D^+) - srel(Q, D^-))^{0.5} \]</span><br />
            </div>
          </div>
          <p></p>
        </section>
        <section id="sec-16">
          <p><em>4.2.2 Lexical Match Features.</em> As an extension to the “word overlap measures” used in the SevMos model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>], we tested 18 features for use as the ‘Lexical Match Factors’ input to the Delta model:</p>
          <ol class="list-no-style">
            <li id="list5" label="(1)">Proportion of unique Query words present in document Text.<br /></li>
            <li id="list6" label="(2)">Proportion of unique Query bigrams present in doc Text.<br /></li>
            <li id="list7" label="(3)">Jaccard Similarity between Query and document Text.<br /></li>
            <li id="list8" label="(4)">IDF weighted version of (1).<br /></li>
            <li id="list9" label="(5)">An IDF weighted version of Jaccard Similarity (3).<br /></li>
            <li id="list10" label="(6)">BM25 on Query, document Title.<br /></li>
            <li id="list11" label="(7)">BM25 on Query, document Abstract.<br /></li>
            <li id="list12" label="(8)">BM25 on Query, document Text.<br /></li>
            <li id="list13" label="(9)">Proportion of unique Query words present in document Title.<br /></li>
            <li id="list14" label="(10)">Proportion of unique Query bigrams present in doc Title.<br /></li>
            <li id="list15" label="(11)">Jaccard Similarity between Query and document Title.<br /></li>
            <li id="list16" label="(12)">IDF weighted version of (9).<br /></li>
            <li id="list17" label="(13)">An IDF weighted version of Jaccard Similarity (11).<br /></li>
            <li id="list18" label="(14)">Proportion of unique Query words present in doc Abstract.<br /></li>
            <li id="list19" label="(15)">Proportion of unique Query bigrams present in doc Abstract.<br /></li>
            <li id="list20" label="(16)">Jaccard Similarity between Query and document Abstract.<br /></li>
            <li id="list21" label="(17)">IDF weighted version of (14).<br /></li>
            <li id="list22" label="(18)">An IDF weighted version of Jaccard Similarity (16).<br /></li>
          </ol>
          <p>To compute these factors, Queries and Documents were tokenized as described above, without the rare word conflation needed for computing word embeddings. Document Text refers to the combined Title and Abstract, each of these (as well as the Query) treated as a sequence of words with no truncation. These factors were selected based on the speed of their computation in a search engine. Factors (3, 5, 11, 13, 16, 18) were also used in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>].</p>
        </section>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Baselines</h3>
          </div>
        </header>
        <p>We compared the performance of the Delta deep learning model against some traditional bag-of-words based textual relevance factors, a distance measurement based on distributional representations of words, and a couple of recent neural network models.</p>
        <section id="sec-18">
          <p><em>4.3.1 Lexical Factors.</em> We compared the performance of Okapi BM25 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] on the document Title, Abstract and Text (Title + Abstract), and found BM25 on Title to give the best ranking performance, with parameter settings at <em>k</em> <sub>1</sub> = 2.0 and <em>b</em> = 0.75.</p>
          <p>The second lexical factor we tested was Unigram Query Likelihood (UQLM), which estimates the probability with which the most likely random process that generated the bag-of-words representation of the document, would generate the query. It is based on a generative unigram language model that is a mixture of two multinomial models [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] based on the document and the corpus, combined using Dirichlet smoothing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>]. Just like in the case of BM25, we found UQLM applied to the document Title to perform the best, and quote only those metrics below.</p>
        </section>
        <section id="sec-19">
          <p><em>4.3.2 Word Mover's Distance.</em> Since all the neural models in our experiments started with pre-trained word embeddings, the <em>Word Mover's Distance</em> (WMD) model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>] for text dis-similarity (score decreases with increasing similarity) was an obvious baseline approach. Based on the Earth Mover's Distance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] applied to a bag-of-words representation for text, it is a non-parameterized approach to determine the minimum amount of total transportation cost (sum of product of inter-word cost and amount transported) needed to convert one document into the other. It uses the Euclidean distance between the word-vector representations of two words as the cost of moving from one word to another. We only report metrics for WMD applied to the document Title without removal of stop-words, as it performed better than the other alternatives tested.</p>
        </section>
        <section id="sec-20">
          <p><em>4.3.3 The Severyn-Moschitti Model.</em> As a recent example of the Independent Semantic Vector approach, we implemented the relevance classification model described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>], along with a few variations. The query and document are fed into separate Convolutional stages, each comprising a single convolution layer with 256 feature maps and a filter width of 5, followed by Dropout and Global Max-Pooling. A similarity measure is computed from these pooled outputs using a similarity weight matrix. The similarity measure, the pooled outpus, and some lexical match features (“overlap measures” in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]) are fed into a Classifier stage consisting of a series of feed-forward layers. In our experiments, we provided the SevMos models with all 18 lexical match features described in section&nbsp;<a class="sec" href="#sec-16">4.2.2</a>. Optimal values for the L2-regularization and Dropout probability hyper-parameters were determined by tuning on validation data, as described for the Delta model.</p>
          <p>We tested several variants of this model covering: replacing the single convolution layer with a 3-layer stack of convolutions of filter width 3, similar to the Delta model's Convolutional stage; training the model as a classifier v/s a relevance scorer to the pairwise max-margin loss; and various sample-weighting schemes. Best results were obtained with the classification model using a 3-layer convolution stack and square-root weighting of samples. We report the metrics for this approach as the “SevMos-C3” model below, and the corresponding single convolution layer based classifier as the “SevMos-C1” model.</p>
          <figure id="fig2">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186049/images/www2018-58-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span> <span class="figure-title">(a) Distribution of Document Text widths. (b)&nbsp;DRMM performance by max document width <em>N</em>.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-21">
          <p><em>4.3.4 The DRMM Model.</em> The Deep Relevance Matching Model (DRMM) is a recent example of the Local Interaction approach to text relevance, described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] to outperform several previous neural models on the Robust04 and ClueWeb-09-Cat-B datasets. While it is a simple model with only 162 trainable parameters, it begins by computing the cosine similarity between the embeddings of each document and query word pair, which dominates the model's computational cost. We implemented the DRMM model as described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>], using the Krovetz word stemmer during text tokenization, stopwords removed from queries, and the CBOW method of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] to compute word embeddings.</p>
          <p>We tested DRMM on increasing values of <em>N</em> (maximum document width) and found the ranking metrics stopped improving after a width of 200 words (figure&nbsp;<a class="fig" href="#fig2">2</a> b). DRMM uses the same pairwise loss function; we found different sample-weighting schemes to have an insignificant effect on the metrics. We report metrics for the version using square-root weighting and <em>N</em> = 200.</p>
        </section>
      </section>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Metrics</h3>
          </div>
        </header>
        <p>Each of the following metrics has values in the range [0, 1], with higher values for better rankings. Scoring ties in all the compared approaches were resolved by sorting on decreasing document-id.</p>
        <section id="sec-23">
          <p><em>4.4.1 NDCG.</em> Discounted Cumulative Gain (DCG) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] is a relevance and rank correlation metric that penalizes placement of relevant documents at lower ranks, computed as:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ DCG(n) = \sum _{i=1}^{n} \frac{2^{rel(i)} - 1}{\log _2(i + 1)} \]</span><br />
            </div>
          </div>where <em>n</em> is the rank to which DCG is accumulated, and <em>rel</em>(<em>i</em>) ≥ 0 is the relevance level of the document placed at rank <em>i</em>. Normalized Discounted Cumulative Gain (NDCG) then measures the relative DCG of a ranking compared to the best possible ranking for that data: <em>NDCG</em>(<em>n</em>) = <em>DCG</em>(<em>n</em>)/<em>IDCG</em>(<em>n</em>), where IDCG(n) is the DCG(n) for the ideal ranking. When there are multiple queries, NDCG refers to the mean value across queries. We use the scaled relevance levels (section&nbsp;<a class="sec" href="#sec-13">4.1</a>), and quote “NDCG.20” metrics for <em>n</em> = 20.
          <p></p>
        </section>
        <section id="sec-24">
          <p><em>4.4.2 Precision at Rank and MAP.</em> Average Precision [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] measures, for a single query, the precision observed in a ranking up to the rank of each relevant document, averaged over the number of relevant documents for that query. It is thus a ranking measure that factors out the size of the ranked list and the number of relevant documents, without any rank-based penalization or discounting. We quote the Mean Average Precision (MAP), which is the mean of the Average Precision across queries in our test dataset. We also quote some Precision at rank <em>n</em> metrics (“Prec.<em>n</em>” in the tables).</p>
        </section>
      </section>
    </section>
    <section id="sec-25">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Evaluation</h2>
        </div>
      </header>
      <div class="table-responsive" id="tab2">
        <div class="table-caption">
          <span class="table-number">Table 2:</span> <span class="table-title"><b>Ranking metrics on the Full test data</b>. The superscripts indicate statistical comparisons: ‘+’ for increase, ‘-’ for decrease, ‘=’ for equivalent, to a 99% confidence using a paired t-test. The comparison baselines are indicated with numbers 1 through 5, as marked in the first column. Highest values are in bold.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;"></th>
              <th style="text-align:left;">NDCG.20</th>
              <th style="text-align:left;">MAP</th>
              <th style="text-align:left;">Prec.5</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;">rev DocID</td>
              <td style="text-align:left;">0.141</td>
              <td style="text-align:left;">0.455</td>
              <td style="text-align:left;">0.344</td>
            </tr>
            <tr>
              <td style="text-align:left;">BM25-Title<sup>1</sup></td>
              <td style="text-align:left;">0.325</td>
              <td style="text-align:left;">0.567</td>
              <td style="text-align:left;">0.591</td>
            </tr>
            <tr>
              <td style="text-align:left;">UQLM-Title</td>
              <td style="text-align:left;">0.314</td>
              <td style="text-align:left;">0.560</td>
              <td style="text-align:left;">0.574</td>
            </tr>
            <tr>
              <td style="text-align:left;">WMD-Title<sup>2</sup></td>
              <td style="text-align:left;">0.329<sup>= 1</sup></td>
              <td style="text-align:left;">0.579<sup>+ 1</sup></td>
              <td style="text-align:left;">0.603<sup>+ 1</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">DRMM</td>
              <td style="text-align:left;">0.300<sup>− 1</sup></td>
              <td style="text-align:left;">0.545<sup>− 1</sup></td>
              <td style="text-align:left;">0.549<sup>− 1</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">SevMos-C1<sup>3</sup></td>
              <td style="text-align:left;">0.352<sup>+ 2</sup></td>
              <td style="text-align:left;">0.597<sup>+ 2</sup></td>
              <td style="text-align:left;">0.625<sup>+ 2</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">SevMos-C3<sup>4</sup></td>
              <td style="text-align:left;">0.373<sup>+ 3</sup></td>
              <td style="text-align:left;">0.594<sup>= 3, +2</sup></td>
              <td style="text-align:left;">0.626<sup>= 3, +2</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">Delta-32<sup>5</sup></td>
              <td style="text-align:left;">0.365<sup>+ 3, −4</sup></td>
              <td style="text-align:left;">0.601<sup>+ 3, 4</sup></td>
              <td style="text-align:left;">0.634<sup>+ 3, 4</sup></td>
            </tr>
            <tr>
              <td style="text-align:left;">Delta-32-Lex3</td>
              <td style="text-align:left;"><strong>0.394<sup>+ 4, 5</sup></strong></td>
              <td style="text-align:left;"><strong>0.609<sup>+ 4, 5</sup></strong></td>
              <td style="text-align:left;"><strong>0.646<sup>+ 4, 5</sup></strong></td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-26">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Test Metrics</h3>
          </div>
        </header>
        <p>We compare ranking performance of two versions of the Delta model against the other approaches. The ‘Delta-32’ model uses <em>n<sub>f</sub></em> = 32 feature maps in the Convolutional stage, and no Lexical Match features. The ‘Delta-32-Lex3’ version of the model adds the following three Lexical Match features: BM25 on the Document Abstract, IDF weighted Jaccard Similarity between the Query and the Document Title, and IDF-weighted proportion of unique Query words in the Document Title. These features were selected using greedy search on the list of 18 described earlier, with NDCG.20 on the validation data as the selection criterion. The feature selection was limited to three to control model run-time cost.</p>
        <p>We begin with ranking metrics for the various approaches on the full test data (table&nbsp;<a class="tbl" href="#tab2">2</a>). The first row in the table provides metrics for an uninformed ranker, where documents are ranked on decreasing document id, to provide a low threshold of performance. The table indicates whether there was a statistically significant change (to a 99% confidence, using a paired t-test) against a baseline. Among the non-trained relevance models, Word Mover's Distance (WMD) performed at least as well as BM25, with no change in NDCG.20, but an improvement in MAP and Prec.5 (Precision at rank 5), while the Query Language Model (UQLM) did not match BM25’s level of performance. Among the trained neural models, DRMM performed the worst, with lower metrics than even BM25. The SevMos-C1 model performed better overall than WMD, and SevMos-C3 further improved the NDCG.20 score.</p>
        <p>Among the Delta models, Delta-32 showed better performance overall than SevMos-C1. However compared to SevMos-C3, its NDCG.20 score was lower, while the MAP and Prec.5 scores were higher. The Delta-32-Lex3 model exhibited the best metrics overall, bettering both Delta-32 and SevMos-C3. These gains were observed not just in the relevance-weighted NDCG.20 metrics that use our derived scaled relevance levels, but also in the MAP and Prec.5 precision-based metrics that use a binary notion of relevance.</p>
        <p>The good performance of SevMos-C3 over SevMos-C1 demonstrates the benefits of using a convolutional stack. Combining these elements with the Delta matrix implementation of the Local Interaction architecture yields even better results, as depicted in the metrics for Delta-32-Lex3.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">Ranking metrics on the ‘Neg20+’ test data.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">NDCG.20</th>
                <th style="text-align:left;">MAP</th>
                <th style="text-align:left;">Prec.5</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">rev DocID</td>
                <td style="text-align:left;">0.081</td>
                <td style="text-align:left;">0.413</td>
                <td style="text-align:left;">0.310</td>
              </tr>
              <tr>
                <td style="text-align:left;">BM25-Title<sup>1</sup></td>
                <td style="text-align:left;">0.233</td>
                <td style="text-align:left;">0.474</td>
                <td style="text-align:left;">0.490</td>
              </tr>
              <tr>
                <td style="text-align:left;">WMD-Title<sup>2</sup></td>
                <td style="text-align:left;">0.243<sup>+ 1</sup></td>
                <td style="text-align:left;">0.483<sup>+ 1</sup></td>
                <td style="text-align:left;">0.496<sup>= 1</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">DRMM</td>
                <td style="text-align:left;">0.242<sup>+ 1, =2</sup></td>
                <td style="text-align:left;">0.461<sup>− 1, 2</sup></td>
                <td style="text-align:left;">0.462<sup>− 1, 2</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">SevMos-C1<sup>3</sup></td>
                <td style="text-align:left;">0.290<sup>+ 2</sup></td>
                <td style="text-align:left;">0.510<sup>+ 2</sup></td>
                <td style="text-align:left;">0.538<sup>+ 2</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">SevMos-C3<sup>4</sup></td>
                <td style="text-align:left;">0.304<sup>+ 2, 3</sup></td>
                <td style="text-align:left;">0.502<sup>+ 2, −3</sup></td>
                <td style="text-align:left;">0.535<sup>+ 2, =3</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32<sup>5</sup></td>
                <td style="text-align:left;">0.296<sup>+ 2, =3</sup></td>
                <td style="text-align:left;">0.513<sup>+ 2, =3</sup></td>
                <td style="text-align:left;">0.550<sup>+ 2, 3</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32-Lex3</td>
                <td style="text-align:left;"><strong>0.326<sup>+ 4, 5</sup></strong></td>
                <td style="text-align:left;"><strong>0.522<sup>+ 4, 5</sup></strong></td>
                <td style="text-align:left;"><strong>0.560<sup>+ 4, 5</sup></strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class="table-title">Ranking metrics on the ‘OneNewWord’ test data. The significance for DRMM's NDCG.20 comparison is to a confidence of 95%.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">NDCG.20</th>
                <th style="text-align:left;">MAP</th>
                <th style="text-align:left;">Prec.5</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">rev DocID</td>
                <td style="text-align:left;">0.191</td>
                <td style="text-align:left;">0.488</td>
                <td style="text-align:left;">0.364</td>
              </tr>
              <tr>
                <td style="text-align:left;">BM25-Title<sup>1</sup></td>
                <td style="text-align:left;">0.333</td>
                <td style="text-align:left;">0.593</td>
                <td style="text-align:left;">0.604</td>
              </tr>
              <tr>
                <td style="text-align:left;">WMD-Title<sup>2</sup></td>
                <td style="text-align:left;">0.330<sup>= 1</sup></td>
                <td style="text-align:left;">0.603<sup>+ 1</sup></td>
                <td style="text-align:left;">0.614<sup>+ 1</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">DRMM</td>
                <td style="text-align:left;">0.318<sup>− 1</sup></td>
                <td style="text-align:left;">0.580<sup>− 1</sup></td>
                <td style="text-align:left;">0.578<sup>− 1</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">SevMos-C1<sup>3</sup></td>
                <td style="text-align:left;">0.358<sup>+ 2</sup></td>
                <td style="text-align:left;">0.624<sup>+ 2</sup></td>
                <td style="text-align:left;">0.642<sup>+ 2</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">SevMos-C3<sup>4</sup></td>
                <td style="text-align:left;">0.375<sup>+ 2, 3</sup></td>
                <td style="text-align:left;">0.621<sup>+ 2, =3</sup></td>
                <td style="text-align:left;">0.640<sup>+ 2, =3</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32<sup>5</sup></td>
                <td style="text-align:left;">0.382<sup>+ 3</sup></td>
                <td style="text-align:left;">0.629<sup>= 3</sup></td>
                <td style="text-align:left;">0.648<sup>= 3</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32-Lex3</td>
                <td style="text-align:left;"><strong>0.413<sup>+ 4, 5</sup></strong></td>
                <td style="text-align:left;"><strong>0.638<sup>+ 4, 5</sup></strong></td>
                <td style="text-align:left;"><strong>0.666<sup>+ 4, 5</sup></strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>To evaluate performance on the <em>Under-Specified Query Problem</em>, we compared metrics on the ‘Neg20+’ subset of the test data (table&nbsp;<a class="tbl" href="#tab3">3</a>). These are harder queries to rank for, since many non-relevant documents contain all the query words. Not surprisingly, the scores for all the models dropped. WMD was still the benchmark among untrained models, and DRMM the lowest performing deep learning model, although it did have better NDCG.20 score than BM25. The Delta-32-Lex3 model again exhibited the best overall performance.</p>
        <p>To evaluate model robustness, we looked at performance on the ‘OneNewWord’ and ‘AllNewWords’ subsets of the test data (tables&nbsp;<a class="tbl" href="#tab4">4</a> and <a class="tbl" href="#tab5">5</a>). The general trend among models here was the same as for the full test data, with DRMM performing no better than BM25, and SevMos-C3 better than SevMos-C1 and WMD. The Delta-32-Lex3 model showed the best overall performance, demonstrating that it was the most robust approach among the models tested. As a final note, some of the models exhibited better metrics on these subsets than the overall data because they tend to do better on shorter queries, and these test subsets had a higher concentration of shorter queries.</p>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class="table-title">Ranking metrics on the ‘AllNewWords’ test data.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">NDCG.20</th>
                <th style="text-align:left;">MAP</th>
                <th style="text-align:left;">Prec.5</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">rev DocID</td>
                <td style="text-align:left;">0.195</td>
                <td style="text-align:left;">0.508</td>
                <td style="text-align:left;">0.389</td>
              </tr>
              <tr>
                <td style="text-align:left;">BM25-Title<sup>1</sup></td>
                <td style="text-align:left;">0.309</td>
                <td style="text-align:left;">0.581</td>
                <td style="text-align:left;">0.586</td>
              </tr>
              <tr>
                <td style="text-align:left;">WMD-Title<sup>2</sup></td>
                <td style="text-align:left;">0.306<sup>= 1</sup></td>
                <td style="text-align:left;">0.590<sup>+ 1</sup></td>
                <td style="text-align:left;">0.595<sup>= 1</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">DRMM</td>
                <td style="text-align:left;">0.311<sup>= 1, 2</sup></td>
                <td style="text-align:left;">0.578<sup>= 1, −2</sup></td>
                <td style="text-align:left;">0.570<sup>= 1, −2</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">SevMos-C1<sup>3</sup></td>
                <td style="text-align:left;">0.344<sup>+ 2</sup></td>
                <td style="text-align:left;">0.615<sup>+ 2</sup></td>
                <td style="text-align:left;">0.624<sup>+ 2</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">SevMos-C3<sup>4</sup></td>
                <td style="text-align:left;">0.355<sup>+ 2, =3</sup></td>
                <td style="text-align:left;">0.614<sup>+ 2, =3</sup></td>
                <td style="text-align:left;">0.632<sup>+ 2, =3</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32<sup>5</sup></td>
                <td style="text-align:left;">0.362<sup>+ 3, =4</sup></td>
                <td style="text-align:left;">0.622<sup>+ 3, 4</sup></td>
                <td style="text-align:left;">0.638<sup>= 3, 4</sup></td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32-Lex3</td>
                <td style="text-align:left;"><strong>0.400<sup>+ 4, 5</sup></strong></td>
                <td style="text-align:left;"><strong>0.633<sup>+ 4, 5</sup></strong></td>
                <td style="text-align:left;"><strong>0.661<sup>+ 4, 5</sup></strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-27">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Impact of Different Features</h3>
          </div>
        </header>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186049/images/www2018-58-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">Comparing the impact of number of filters <em>n<sub>f</sub></em> (feature maps) in the Convolutional Stage of the Delta Model.</span>
          </div>
        </figure>
        <p>Next we look at how different aspects of the Delta model affected its performance. The Convolutional stage extracts match-related features from word <em>n</em>-grams in the document. The parameter <em>n<sub>f</sub></em> controls the number of such features, and its effect on ranking is charted in figure&nbsp;<a class="fig" href="#fig3">3</a>. Both NDCG.20 and MAP improved as <em>n<sub>f</sub></em> increased till around 32 filters, and then performance leveled off and then dropped slightly. At larger number of filters, the model becomes more complex, but this increase in complexity does not continue to yield better performance. More complex models are more likely to overfit the training data, and perhaps learning rate decay techniques might help converge to a better solution, an area to be explored further. However since our goal was to construct a fast model for use in a search engine, we had a preference for smaller models, and <em>n<sub>f</sub></em> = 32 provided a good balance between speed and performance.</p>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class="table-title">Comparing the impact of different features on the ranking metrics for the Delta model.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">NDCG.20</th>
                <th style="text-align:left;">MAP</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Delta-32, no Difference vectors</td>
                <td style="text-align:left;">0.323</td>
                <td style="text-align:left;">0.574</td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32, no Delta features</td>
                <td style="text-align:left;">0.333</td>
                <td style="text-align:left;">0.584</td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32</td>
                <td style="text-align:left;">0.365</td>
                <td style="text-align:left;">0.601</td>
              </tr>
              <tr>
                <td style="text-align:left;">Delta-32-Lex3, (top 3 Lex features)</td>
                <td style="text-align:left;">0.394</td>
                <td style="text-align:left;">0.609</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Table&nbsp;<a class="tbl" href="#tab6">6</a> compares four different versions of the Delta model with <em>n<sub>f</sub></em> = 32. The Delta Stage of the model computes, for each document word, a difference vector against the closest query word, and three ‘Delta features’: the cosine similarity, euclidean distance, and normalized proximity. The first two rows of table&nbsp;<a class="tbl" href="#tab6">6</a> show the impact of removing the difference vectors and the Delta features from the Delta-32 model, on the ranking metrics for the test data. Both show a significant drop in performance compared to Delta-32. Finally, as reviewed above, adding the 3 lexical match features resulted in a significant improvement for the Delta-32-Lex3 model over the Delta-32 model.</p>
        <p>In our greedy search for selecting the most useful lexical match features, BM25 on Document Abstract showed the most impact because it compensated for the truncation of the abstract when the document was limited to 50 words. The other two lexical match features are useful in accounting for the matches of out-of-vocabulary query terms (as discussed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]), and also for providing for query term significance through IDF-weighting.</p>
      </section>
      <section id="sec-28">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Evaluation as a Reranker</h3>
          </div>
        </header>
        <p>Our goal for this research was to develop a good textual relevance model whose output could be used as a factor in a reranker (which would also use other factors like document meta-data) in a search engine like PubMed. We set a target of re-ranking the top 500 documents as produced by a fast naive ranker, and a run-time performance constraint on the relevance model of scoring the 500 documents in under 0.1 seconds on a GPU, yielding a throughput of at least 10 queries per second per GPU. While the complete design of a two-round ranker was outside the scope of this project, we compared the candidate relevance models on the (up to) top 500 results for the same test queries, as ranked by PubMed's implementation of BM25, which also incorporates query expansion terms.</p>
        <section id="sec-29">
          <p><em>5.3.1 Ranking Metrics.</em> Table <a class="tbl" href="#tab7">7</a> shows the ranking metrics for our candidate and baseline models on the top 500 results test data, with models sorted on the NDCG.20 metric. This data provided a particular challenge for relevance models, since on average less than 4% of the documents were relevant to the corresponding query. It also did not have the same selection bias present in the training data which was extracted from clicks on results sorted by PubMed's relevance ranker, a more complex expression that includes BM25 as one of the factors. As a result, all the metrics were lower than for the previous test data. However the general trends among the models was the same, with Delta-32-Lex3 the best model overall by a large margin, followed by SevMos-C3. An area worth exploring further is whether adding some randomly sampled documents from deep in the search results (e.g. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>]) could help overcome some of this selection bias.</p>
          <div class="table-responsive" id="tab7">
            <div class="table-caption">
              <span class="table-number">Table 7:</span> <span class="table-title">Ranking metrics for re-ranking the top 500 results as provided by PubMed's BM25, sorted on NDCG.20.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:left;"></th>
                  <th style="text-align:left;">NDCG.20</th>
                  <th style="text-align:left;">MAP</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:left;">Original sort order</td>
                  <td style="text-align:left;">0.025</td>
                  <td style="text-align:left;">0.109</td>
                </tr>
                <tr>
                  <td style="text-align:left;">BM25-Title</td>
                  <td style="text-align:left;">0.032</td>
                  <td style="text-align:left;">0.077</td>
                </tr>
                <tr>
                  <td style="text-align:left;">SevMos-C1</td>
                  <td style="text-align:left;">0.083</td>
                  <td style="text-align:left;">0.104</td>
                </tr>
                <tr>
                  <td style="text-align:left;">DRMM</td>
                  <td style="text-align:left;">0.106</td>
                  <td style="text-align:left;">0.135</td>
                </tr>
                <tr>
                  <td style="text-align:left;">WMD-Title</td>
                  <td style="text-align:left;">0.122</td>
                  <td style="text-align:left;">0.153</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Delta-32</td>
                  <td style="text-align:left;">0.141</td>
                  <td style="text-align:left;">0.154</td>
                </tr>
                <tr>
                  <td style="text-align:left;">SevMos-C3</td>
                  <td style="text-align:left;">0.160</td>
                  <td style="text-align:left;">0.151</td>
                </tr>
                <tr>
                  <td style="text-align:left;">Delta-32-Lex3</td>
                  <td style="text-align:left;"><strong>0.191</strong></td>
                  <td style="text-align:left;"><strong>0.188</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
        </section>
        <section id="sec-30">
          <p><em>5.3.2 Run-time Cost.</em> As discussed above, the Independent Semantic Vector approach like that used in the SevMos models [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>] is particularly attractive for use in search engines, because the document semantic vectors (e.g. pooled output from the convolution stage in SevMos) can be pre-computed and stored, the query vector needs to be computed just once per search, and the remaining computation (the similarity measure and classifier stage in SevMos) is fairly small and fast. This caching is not possible in the Local Interaction approaches like the DRMM and Delta models. In these two models, the computation is dominated by the cost to compare each pair of document and query words, so we look to reducing the size of the document by truncation to control the computation. In scientific literature, authors are strongly motivated to provide a highly informative and noise-free document Title and Abstract, making it particularly amenable to this approach.</p>
          <p>We measured the time each model took to score 500 documents for a single query on a NVIDIA GeForce GTX TITAN X GPU, including the time to transfer the data to the GPU in a single batch, but not including the time to load the model and its parameters (including embeddings). While best ranking metrics for DRMM were obtained for <em>N</em> = 200 (with NDCG.20 = 0.300), its run-time of 0.177 seconds, corresponding to a throughput of 5.6 queries per second per GPU, exceeded our criterion; DRMM for <em>N</em> = 100 came in at 0.079 seconds (throughput 12.7 qps per GPU), but a slightly lower overall NDCG.20 of 0.293 (figure&nbsp;<a class="fig" href="#fig2">2</a> b). The ‘Delta-32-Lex3’ model had a run-time of 0.049 secs, (20 qps per GPU). As a comparison the full ‘SevMos-C3’ model scored 500 documents in 0.040 seconds (25 qps per GPU).</p>
        </section>
      </section>
      <section id="sec-31">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.4</span> Example Queries</h3>
          </div>
        </header>
        <p>We compare the rankings of the ‘Delta-32-Lex3’, ‘SevMos-C3’ and ‘WMD’ relevance models on some interesting queries. The NDCG at 20 at that query is quoted for each model, along with Titles and relevance levels for the top 3 scoring documents, and the relevance leves for the next 4 documents. The examples demonstrate the Delta model's ability to detect relevance in documents without exact match of query terms (addressing the <em>term mismatch problem</em>), and where the context of the match is also important.</p>
        <section id="sec-32">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.4.1</span> Query: <tt>countermovement jump</tt></h4>
            </div>
          </header>
          <p>The word <em>countermovement</em> did not occur in training or validation queries. This is also an example where relevance depends on the other words in the document text besides those matching the query. Number of documents in the test dataset: relevant = 17, non-relevant = 16. Top relevance levels: 21.5, 18.1, 11.2, 7.8, 4.4.</p>
          <p>As ranked by <strong>Delta-32-Lex3</strong> (NDCG.20 = 0.98):</p>
          <ol class="list-no-style">
            <li id="list23" label="i.">(21.5) Determinants of countermovement jump performance: a kinetic and kinematic analysis.<br /></li>
            <li id="list24" label="ii.">(11.2) Which drop jump technique is most effective at enhancing countermovement jump ability, “countermovement” drop jump or “bounce” drop jump?<br /></li>
            <li id="list25" label="iii.">(4.4) The MARS for squat, countermovement, and standing long jump performance analyses: are measures reproducible?<br /></li>
            <li id="list26" label="iv.">Next four relevance levels = 4.4, 0, 18.1, 4.4.<br /></li>
          </ol>
          <p>As ranked by <strong>SevMos-C3</strong> (NDCG.20 = 0.32):</p>
          <ol class="list-no-style">
            <li id="list27" label="i.">(11.2) Which drop jump technique is most effective at enhancing countermovement jump ability, “countermovement” drop jump or “bounce” drop jump?<br /></li>
            <li id="list28" label="ii.">(0) A mechanics comparison between landing from a countermovement jump and landing from stepping off a box.<br /></li>
            <li id="list29" label="iii.">(4.4) Comparison of acute countermovement jump responses after functional isometric and dynamic half squats.<br /></li>
            <li id="list30" label="iv.">Next four relevance levels = 0, 4.4, 0, 21.5.<br /></li>
          </ol>
          <p>As ranked by <strong>WMD</strong> (NDCG.20 = 0.5):</p>
          <ol class="list-no-style">
            <li id="list31" label="i.">(11.2) Which drop jump technique is most effective at enhancing countermovement jump ability, “countermovement” drop jump or “bounce” drop jump?<br /></li>
            <li id="list32" label="ii.">(0) Reductions in Sprint Paddling Ability and Countermovement Jump Performance After Surfing Training.<br /></li>
            <li id="list33" label="iii.">(21.5) Determinants of countermovement jump performance: a kinetic and kinematic analysis.<br /></li>
            <li id="list34" label="iv.">Next four relevance levels = 4.4, 0, 0, 0.<br /></li>
          </ol>
        </section>
        <section id="sec-33">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.4.2</span> Query: <tt>oesophageal cancer review</tt></h4>
            </div>
          </header>
          <p>The word <em>oesopha-geal</em> did not occur in training or validation queries. The word <em>review</em> does not occur in the title of all relevant documents. The three models successfully located alternative spellings of the word. Number of documents in the test dataset: relevant = 22, non-relevant = 28. Top relevance levels: 11.7, and four at 7.1.</p>
          <p>As ranked by <strong>Delta-32-Lex3</strong> (NDCG.20 = 0.94):</p>
          <ol class="list-no-style">
            <li id="list35" label="i.">(11.7) Esophageal cancer: Recent advances in screening, targeted therapy, and management.<br /></li>
            <li id="list36" label="ii.">(0) Outcomes in the management of esophageal cancer.<br /></li>
            <li id="list37" label="iii.">(0) Current advances in esophageal cancer proteomics.<br /></li>
            <li id="list38" label="iv.">Next four relevance levels = 5.6, 5.6, 0, 7.1.<br /></li>
          </ol>
          <p>As ranked by <strong>SevMos-C3</strong> (NDCG.20 = 0.05):</p>
          <ol class="list-no-style">
            <li id="list39" label="i.">(5.6) Esophageal Cancer Staging.<br /></li>
            <li id="list40" label="ii.">(2.5) Esophageal cancer: staging system and guidelines for staging and treatment.<br /></li>
            <li id="list41" label="iii.">(0) Outcomes in the management of esophageal cancer.<br /></li>
            <li id="list42" label="iv.">Next four relevance levels = 0, 0, 5.6, 0.<br /></li>
          </ol>
          <p>As ranked by <strong>WMD</strong> (NDCG.20 = 0.05):</p>
          <ol class="list-no-style">
            <li id="list43" label="i.">(5.6) Esophageal Cancer Staging.<br /></li>
            <li id="list44" label="ii.">(7.1) Endoscopic Management of Early Esophageal Cancer.<br /></li>
            <li id="list45" label="iii.">(0) Endoscopic treatment of early esophageal cancer.<br /></li>
            <li id="list46" label="iv.">Next four relevance levels = 0, 0, 5.6, 0.<br /></li>
          </ol>
        </section>
        <section id="sec-34">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.4.3</span> Query: <tt>chronic headache and depression review</tt></h4>
            </div>
          </header>
          <p>All three models were able to leverage word vectors to relate headache to migraine. Delta-32-Lex3 placed the most relevant document (“Psychological Risk Factors in Headache”, rel. level = 10) at rank 5. It did not feature in the top 10 of any of the other neural and lexical models tested. This example demonstrates the need for deeper semantic modeling, where the Delta model has some limited success. Number of documents in the test dataset: relevant = 23, non-relevant = 18. Top relevance levels: a 10, and four at 5.5.</p>
          <p>As ranked by <strong>Delta-32-Lex3</strong> (NDCG.20 = 0.4):</p>
          <ol class="list-no-style">
            <li id="list47" label="i.">(5.5) Comprehensive management of headache and depression.<br /></li>
            <li id="list48" label="ii.">(0) Medication overuse headache.<br /></li>
            <li id="list49" label="iii.">(0) Clinical features and mechanisms of chronic migraine and medication-overuse headache.<br /></li>
            <li id="list50" label="iv.">Next four relevance levels = 0, 10, 5.5, 5.5.<br /></li>
          </ol>
          <p>As ranked by <strong>SevMos-C3</strong> (NDCG.20 = 0.3):</p>
          <ol class="list-no-style">
            <li id="list51" label="i.">(5.5) Chronic headaches and the neurobiology of somatization.<br /></li>
            <li id="list52" label="ii.">(2.7) Pathophysiology of migraine – from molecular to personalized medicine.<br /></li>
            <li id="list53" label="iii.">(0) Medication overuse headache.<br /></li>
            <li id="list54" label="iv.">Next four relevance levels = 5.5, 2.5, 5.5, 0.<br /></li>
          </ol>
          <p>As ranked by <strong>WMD</strong> (NDCG.20 = 0.35):</p>
          <ol class="list-no-style">
            <li id="list55" label="i.">(5.5) Comprehensive management of headache and depression.<br /></li>
            <li id="list56" label="ii.">(5.5) Migraine and depression: biological aspects.<br /></li>
            <li id="list57" label="iii.">(5.5) Migraine and depression.<br /></li>
            <li id="list58" label="iv.">Next four relevance levels = 5.5, 0, 0, 0.<br /></li>
          </ol>
        </section>
      </section>
    </section>
    <section id="sec-35">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion</h2>
        </div>
      </header>
      <p>While deep learning models for text understanding have made dramatic gains in recent years, they have tended to be large and slow. The challenge in information retrieval is still one of combining predictive power with low run-time overhead. This is also true when the corpus is scientific literature.</p>
      <p>We described the Delta Relevance model, a new deep learning model for text relevance, targeted for information retrieval in biomedical science literature. The main innovation in the model is to base the modeling function on differences and distances between word embeddings, as captured in the Delta features, rather than directly using the embeddings themselves like most NLP approaches. Other researchers have shown the benefit of training word embeddings as part of the model. That was not feasible here since the training data had only 20k queries compared to a vocabulary size of 200k. Using Delta features as the input to the model helped adapt the pre-trained word embeddings to our task.</p>
      <p>To achieve our goal of fast run-time, we used a convolutional network rather than the recurrent approaches popular in most neural NLP models. Using a stack of narrow convolutional layers instead of a single wide convolution gave the model more power.</p>
      <p>We showed that the Delta model outperformed comparable recent approaches in ranking metrics when trained and evaluated on data derived from the PubMed search engine click logs. We demonstrated that the model was robust, despite being trained on a relatively small amount of data, and the model was fast enough for use in an on-line search engine.</p>
      <p>The Delta model might be especially suited to scientific literature in taking advantage of the high quality of the document Title and first few sentences of the Abstract. We believe that the previously observed good performance of the DRMM approach was on documents that were quite noisy (they contain a lot of meta data in the document text). An area worth exploring further, for its potential in improving both prediction performance and run-time costs, is pre-processing a document to extract significant portions for evaluating relevance, thus reducing the size of the input at run-time. Another area to explore is the benefit from retaining sentence and grammatical structure in document text.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-36">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This research was supported by the Intramural Research Program of the NIH, National Library of Medicine.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Olivier Bodenreider. 2004. The Unified Medical Language System (UMLS): integrating biomedical terminology. 32 (Database issue) (2004), D267–D270.</li>
        <li id="BibPLXBIB0002" label="[2]">Chris Buckley and Ellen&nbsp;M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In <em><em>Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> (SIGIR ’00). ACM, New York, NY, USA, 33–40.</li>
        <li id="BibPLXBIB0003" label="[3]">Billy Chiu, Gamal Crichton, Anna Korhonen, and Sampo Pyysalo. 2016. How to Train good Word Embeddings for Biomedical NLP. In <em><em>Proceedings of the 15th Workshop on Biomedical Natural Language Processing</em></em> (BioNLP ’16). ACL, 166–174.</li>
        <li id="BibPLXBIB0004" label="[4]">Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query Expansion with Locally-Trained Word Embeddings. In <em><em>Proceedings of ACL 2016</em></em> . ACL, Berlin, Germany, 367–377.</li>
        <li id="BibPLXBIB0005" label="[5]">Rezarta&nbsp;Islamaj Dogan, G.&nbsp;Craig Murray, Aurélie Névéol, and Zhiyong Lu. 2009. Understanding PubMed<sup>®</sup> user search behavior through log analysis. <em><em>Database</em></em> 2009(2009), bap018.</li>
        <li id="BibPLXBIB0006" label="[6]">John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. <em><em>Journal of Machine Learning Research</em></em> 12 (Jul 2011), 2121–2159.</li>
        <li id="BibPLXBIB0007" label="[7]">Frederico Durao, Karunakar Bayyapu, Guandong Xu, Peter Dolog, and Ricardo Lage. 2013. <em><em>Medical Information Retrieval Enhanced with User's Query Expanded with Tag-Neighbors</em></em> . Springer New York, New York, NY, 17–40. <a class="link-inline force-break" href="https://doi.org/10.1007/978-1-4614-8495-0_2" target="_blank">https://doi.org/10.1007/978-1-4614-8495-0_2</a>
        </li>
        <li id="BibPLXBIB0008" label="[8]">G.&nbsp;W. Furnas, T.&nbsp;K. Landauer, L.&nbsp;M. Gomez, and S.&nbsp;T. Dumais. 1987. The Vocabulary Problem in Human system Communication. <em><em>CACM</em></em> 30, 11 (nov 1987), 964–971.</li>
        <li id="BibPLXBIB0009" label="[9]">Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, and Li Deng. 2014. Modeling Interestingness with Deep Neural Networks. In <em><em>Proceedings of EMNLP 2014</em></em> . ACL, Doha, Qatar, 2–13.</li>
        <li id="BibPLXBIB0010" label="[10]">Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. <em><em>Deep Learning</em></em> . MIT Press, Cambridge, MA. <a class="link-inline force-break" href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.
        </li>
        <li id="BibPLXBIB0011" label="[11]">Jiafeng Guo, Yixing Fan, Qingyao Ai, and W.&nbsp;Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In <em><em>Proceedings of CIKM 2016</em></em> . ACM, New York, NY, USA, 55–64.</li>
        <li id="BibPLXBIB0012" label="[12]">W. Hersh, S. Price, and L. Donohoe. 2000. Assessing Thesaurus-Based Query Expansion Using the UMLS Metathesaurus. In <em><em>Proceedings of the AMIA Symposium</em></em> . 344–348.</li>
        <li id="BibPLXBIB0013" label="[13]">Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional Neural Network Architectures for Matching Natural Language Sentences. In <em><em>Advances in NIPS 27</em></em> , Z.&nbsp;Ghahramani, M.&nbsp;Welling, C.&nbsp;Cortes, N.&nbsp;D. Lawrence, and K.&nbsp;Q. Weinberger (Eds.). 2042–2050.</li>
        <li id="BibPLXBIB0014" label="[14]">Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In <em><em>Proceedings of CIKM 2013</em></em> . ACM, New York, NY, USA, 2333–2338.</li>
        <li id="BibPLXBIB0015" label="[15]">Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. PACRR: A Position-Aware Neural IR Model for Relevance Matching. In <em><em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em></em> . Association for Computational Linguistics, 1060–1069. <a class="link-inline force-break" href="http://aclweb.org/anthology/D17-1111" target="_blank">http://aclweb.org/anthology/D17-1111</a>
        </li>
        <li id="BibPLXBIB0016" label="[16]">Kalervo Järvelin and Jaana Kekäläinen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In <em><em>Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> (SIGIR ’00). ACM, New York, NY, USA, 41–48.</li>
        <li id="BibPLXBIB0017" label="[17]">Thorsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In <em><em>Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></em> (KDD ’02). ACM, New York, NY, USA, 133–142.</li>
        <li id="BibPLXBIB0018" label="[18]">Sun Kim, Zhiyong Lu, and W.&nbsp;John Wilbur. 2015. Identifying named entities from PubMed® for enriching semantic categories. <em><em>BMC Bioinformatics</em></em> 16, 1 (21 Feb 2015), 57. <a class="link-inline force-break" href="https://doi.org/10.1186/s12859-015-0487-2" target="_blank">https://doi.org/10.1186/s12859-015-0487-2</a>
        </li>
        <li id="BibPLXBIB0019" label="[19]">Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From Word Embeddings To Document Distances. In <em><em>Proceedings of The 32nd International Conference on Machine Learning</em></em> (ICML 2015). JMLR, 957–966.</li>
        <li id="BibPLXBIB0020" label="[20]">John Lafferty and Chengxiang Zhai. 2001. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In <em><em>Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> (SIGIR ’01). ACM, New York, NY, USA, 111–119. <a class="link-inline force-break" href="https://doi.org/10.1145/383952.383970" target="_blank">https://doi.org/10.1145/383952.383970</a>
        </li>
        <li id="BibPLXBIB0021" label="[21]">Yann LeCun. 1989. Generalization and Network Design Strategies. In <em><em>Connectionism in Perspective</em></em> , R.&nbsp;Pfeifer, Z.&nbsp;Schreter, F.&nbsp;Fogelman, and L.&nbsp;Steels (Eds.). Elsevier, Zurich, Switzerland.</li>
        <li id="BibPLXBIB0022" label="[22]">Zhiyong Lu, Won Kim, and W.&nbsp;John Wilbur. 2009. Evaluation of query expansion using MeSH in PubMed. <em><em>Information retrieval</em></em> 12, 1 (May 2009), 69–80.</li>
        <li id="BibPLXBIB0023" label="[23]">Zhengdong Lu and Hang Li. 2013. A Deep Architecture for Matching Short Texts. In <em><em>Advances in NIPS 26</em></em> , C.&nbsp;Burges, L.&nbsp;Bottou, M.&nbsp;Welling, Z.&nbsp;Ghahramani, and K.&nbsp;Q. Weinberger (Eds.). 1367–1375.</li>
        <li id="BibPLXBIB0024" label="[24]">Andrew&nbsp;L. Maas, Awni&nbsp;Y. Hannun, and Andrew&nbsp;Y. Ng. 2013. Rectifier Nonlinearities Improve Neural Network Acoustic Models. In <em><em>Proceedings of the ICML Workshop on Deep Learning for Audio, Speech, and Language Processing</em></em> (WDLASL 2013).</li>
        <li id="BibPLXBIB0025" label="[25]">Tomas Mikolov, Kai Chen, Greg&nbsp;S. Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In <em><em>Proceedings of the Workshop at ICLR 2013</em></em> .</li>
        <li id="BibPLXBIB0026" label="[26]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&nbsp;S. Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In <em><em>Advances in NIPS 26</em></em> , C.&nbsp;J.&nbsp;C. Burges, L.&nbsp;Bottou, M.&nbsp;Welling, Z.&nbsp;Ghahramani, and K.&nbsp;Q. Weinberger (Eds.). 3111–3119.</li>
        <li id="BibPLXBIB0027" label="[27]">David R.&nbsp;H. Miller, Tim Leek, and Richard&nbsp;M. Schwartz. 1999. A Hidden Markov Model Information Retrieval System. In <em><em>Proceedings of SIGIR 1999</em></em> . ACM, New York, NY, USA, 214–221.</li>
        <li id="BibPLXBIB0028" label="[28]">Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. <em><em>CoRR</em></em> abs/1705.01509(2017). <a class="link-inline force-break" href="https://arxiv.org/abs/1705.01509" target="_blank">https://arxiv.org/abs/1705.01509</a>
        </li>
        <li id="BibPLXBIB0029" label="[29]">Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In <em><em>Proceedings of the 26th International Conference on World Wide Web</em></em> (WWW ’17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 1291–1299. <a class="link-inline force-break" href="https://doi.org/10.1145/3038912.3052579" target="_blank">https://doi.org/10.1145/3038912.3052579</a>
        </li>
        <li id="BibPLXBIB0030" label="[30]">Sunil Mohan, Nicolas Fiorini, Sun Kim, and Zhiyong Lu. 2017. Deep Learning for Biomedical Information Retrieval: Learning Textual Relevance from Click Logs. In <em><em>BioNLP 2017</em></em> . Association for Computational Linguistics, Vancouver, Canada,, 222–231. <a class="link-inline force-break" href="http://www.aclweb.org/anthology/W17-2328" target="_blank">http://www.aclweb.org/anthology/W17-2328</a>
        </li>
        <li id="BibPLXBIB0031" label="[31]">Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual Word Embeddings. In <em><em>Proceedings of the 25th International Conference Companion on World Wide Web</em></em> (WWW ’16 Companion). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 83–84. <a class="link-inline force-break" href="https://doi.org/10.1145/2872518.2889361" target="_blank">https://doi.org/10.1145/2872518.2889361</a>
        </li>
        <li id="BibPLXBIB0032" label="[32]">Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching as Image Recognition. (2016). <a class="link-inline force-break" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11895" target="_blank">https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11895</a>
        </li>
        <li id="BibPLXBIB0033" label="[33]">Stephen&nbsp;E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In <em><em>Proceedings of TREC 1994</em></em> . NIST, Dept. of Commerce, 109–126.</li>
        <li id="BibPLXBIB0034" label="[34]">Y. Rubner, C. Tomasi, and L.&nbsp;J. Guibas. 1998. A metric for distributions with applications to image databases. In <em><em>Proceedings of the Sixth International Conference on Computer Vision</em></em> . IEEE.</li>
        <li id="BibPLXBIB0035" label="[35]">Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In <em><em>Proceedings of SIGIR 2015</em></em> . ACM, New York, NY, USA, 373–382.</li>
        <li id="BibPLXBIB0036" label="[36]">Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014. A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. In <em><em>Proceedings of CIKM 2014</em></em> . ACM, New York, NY, USA, 101–110.</li>
        <li id="BibPLXBIB0037" label="[37]">Gregor Urban, Krzysztof&nbsp;J. Geras, Samira&nbsp;Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson, and Rich Caruana. 2017. Do Deep Convolutional Nets Really Need to be Deep and Convolutional?. In <em><em>Proceedings of the Fifth International Conference on Learning Representations</em></em> (ICLR 2017). Toulon, France.</li>
        <li id="BibPLXBIB0038" label="[38]">Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In <em><em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> (SIGIR ’17). ACM, New York, NY, USA, 55–64. <a class="link-inline force-break" href="https://doi.org/10.1145/3077136.3080809" target="_blank">https://doi.org/10.1145/3077136.3080809</a>
        </li>
        <li id="BibPLXBIB0039" label="[39]">Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. <em><em>ACM Trans. Inf. Syst.</em></em> 22, 2 (April 2004), 179–214.</li>
        <li id="BibPLXBIB0040" label="[40]">Y. Zhang, M.&nbsp;M. Rahman, A. Braylan, B. Dang, H. Chang, H. Kim, Q. McNamara, A. Angert, E. Banner, V. Khetan, T. McDonnell, A.&nbsp;T. Nguyen, D. Xu, B.&nbsp;C. Wallace, and M. Lease. 2016. Neural Information Retrieval: A Literature Review. <em><em>CoRR</em></em> abs/1611.06792(2016). <a class="link-inline force-break" href="http://arxiv.org/abs/1611.06792" target="_blank">http://arxiv.org/abs/1611.06792</a>
        </li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Corresponding author. This work was done while this author was at NCBI.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class="link-inline force-break" href="http://pubmed.gov">http://pubmed.gov</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="http://umlsks.nlm.nih.gov">http://umlsks.nlm.nih.gov</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3178876.3186049">https://doi.org/10.1145/3178876.3186049</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
