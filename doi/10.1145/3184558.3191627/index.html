<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Detection of Stress and Relaxation Magnitudes for Tweets</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
<link rel="cite-as" href="https://doi.org/10.1145/3184558.3191627"/></head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191627'>https://doi.org/10.1145/3184558.3191627</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191627'>https://w3id.org/oa/10.1145/3184558.3191627</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Detection of Stress and Relaxation Magnitudes for Tweets</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Reshmi Gopalakrishna</span> <span class="surName">Pillai</span>, Research Institute in Information and Language Processing, University of Wolverhampton, Wolverhampton, UK, <a href="mailto:reshmi.g85@gmail.com">reshmi.g85@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Mike</span> <span class="surName">Thelwall</span>, Research Institute in Information and Language Processing, University of Wolverhampton, Wolverhampton, UK, <a href="mailto:M.Thelwall@wlv.ac.uk">M.Thelwall@wlv.ac.uk</a>
        </div>
        <div class="author">
          <span class="givenName">Constantin</span> <span class="surName">Orasan</span>, Research Institute in Information and Language Processing, University of Wolverhampton, Wolverhampton, UK, <a href="mailto:C.Orasan@wlv.ac.uk">C.Orasan@wlv.ac.uk</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191627" target="_blank">https://doi.org/10.1145/3184558.3191627</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The ability to automatically detect human stress and relaxation is crucial for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress-measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of the social media can be leveraged to identify stress and relaxation, since many people habitually share their recent life experiences through social networking sites. This paper introduces an improved method to detect expressions of stress and relaxation in social media content. It uses word sense disambiguation by word sense vectors to improve the performance of the first and only lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that incorporating word sense disambiguation substantially improves the performance of the original TensiStrength. It performs better than state-of-the-art machine learning methods too in terms of Pearson correlation and percentage of exact matches. We also propose a novel framework for identifying the causal agents of stress and relaxation in tweets as future work.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information Systems</strong> → Information Retrieval → Retrieval tasks and goals → Sentiment Analysis • <strong>Human Centered Computing</strong> → Collaborative and Social Computing → Social Media • <strong>Computing Methodologies</strong> → Artificial Intelligence → Natural Language Processing</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Stress</small>,</span> <span class="keyword"><small>social media</small>,</span> <span class="keyword"><small>Twitter</small>,</span> <span class="keyword"><small>sentiment analysis</small>,</span> <span class="keyword"><small>word sense disambiguation</small>,</span> <span class="keyword"><small>word vectors</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Reshmi Gopalakrishna Pillai, Mike Thelwall and Constantin Orasan. 2018. Detection of Stress and Relaxation Magnitudes for Tweets. In <em>Proceedings of The 2018 Web Conference Companion (WWW '18 Companion). ACM, New York, NY, USA, 11 pages.</em> <a href="https://doi.org/10.1145/3184558.3191627" target="_blank">https://doi.org/10.1145/3184558.3191627</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-001">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> INTRODUCTION</h2>
        </div>
      </header>
      <p>The ability to identify stress and relaxation is important in the health domain as a diagnostic tool for health-related disorders, for customer focused business to detect unpleasant experiences (e.g., tourism), and for systems that manage humans on a large scale (e.g., transportation, crowd management). The early detection of stress can help action to be taken to avoid the situation escalating or going untreated. Depending on the context, the actions might include recommending a medical consultation for a stress-related disorder, changing hotel operating procedures to avoid customer stress hotspots, re-routing stressed customers to human agents rather than automatic phone agents, or triggering emergency traffic management measures. Similarly, it is important to measure and know the causes of relaxation, as an affective state opposite to stress and as an indicator of satisfaction with applications and services.</p>
      <p>Traditionally, stress and relaxation have been measured by monitors identifying variations in physiological parameters or questionnaires targeted at examining the onset of psychological manifestations. These methods, though widely accepted, have limitations. Sensors measuring physical parameters can be expensive and time consuming to use. On the other hand, diagnosis of stress and relaxation through psychological questionnaires is often influenced by manipulated answers given by the participants who might give the responses to project a better image of themselves rather than being honest about their mental state.</p>
      <p>Social Networking Sites are popular for both personal and broadcast communication. People habitually share updates from their life on social media platforms. Their online activities and interactions can be expected to mirror the offline incidents and states to some extent. This can be harnessed to evaluate the stress and relaxation levels of a user in a non-intrusive manner.</p>
      <p>The detection of stress and relaxation from social media content is a largely unexplored research field. The current study implements a vector-based word sense disambiguation (WSD) system to improve the performance of the first ever lexicon-based stress/relaxation detection method TensiStrength [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib1">1</a>] on Twitter. Results show that incorporating WSD as a preprocessing phase significantly improves the accuracy of TensiStrength and this modified system performs better than standard machine learning algorithms as well. The novel contributions are summarized as follows:</p>
      <ol class="list-no-style">
        <li label="1.">Incorporates WSD with stress/relaxation detection, for the first time in social media content research.<br /></li>
        <li label="2.">Analyzes stress/relaxation expressions in tweets belonging to a variety of domains such as traffic, politics, life events and sports.<br /></li>
        <li label="3.">Introduces an updated lexicon with stress/relaxation scores for affect words which accommodates polysemy.<br /></li>
      </ol>
      <p></p>
    </section>
    <section id="sec-002">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> LITERATURE REVIEW</h2>
        </div>
      </header>
      <section id="sec2Z1">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Traditional Methods of Stress/Relaxation measurement</h3>
          </div>
        </header>
        <p>The seminal work, ‘The Stress of Life’ [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib2">2</a>] defines stress as the non-specific response of the body to any demand for change. This definition puts forward, two distinct but related aspects of stress: The stimulus i.e., the demand for change and the response. The latter can manifest itself in different forms, mainly as variation in physiological parameters. Stress has been traditionally measured by monitoring indicative parameters such as heart rates [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib3">3</a>], galvanic skin response [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib4">4</a>] and pupil diameter [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib5">5</a>]. Another method is to use questionnaires to find stress-prone Type A personalities [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib6">6</a>] or stress-inducing life events [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib7">7</a>].</p>
        <p>These methods are mostly reactive in nature, requiring the researchers to continuously monitor expensive sensors or rely on users who might be reluctant to share direct observations on their own mental states or manipulate their responses to meet self-imposed psychological images. These limitations provide motivation to develop novel, unobtrusive sources of information to analyze stress and relaxation expressions.</p>
      </section>
      <section id="sec2Z2">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Identification of Stress and Relaxation from Social Media Content</h3>
          </div>
        </header>
        <p>The ubiquity of social media makes it a potential tool for behavioral and mental health evaluation. Analysis of internet usage pattern [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib8">8</a>] and Facebook status updates [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib9">9</a>] have been established as unconventional, innovative methods to identify people at risk of depression. There have been efforts to study the content of Twitter, for indicators of mental-health disorders such as depression [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib10">10</a>] or Post Traumatic Stress Disorder (PTSD) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>].</p>
        <p>The potential for mining social media postings for indicators of mental illness is covered in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib12">12</a>] including the multimedia opportunities and ethical challenges. In a related work, tweets are considered as inputs for predictive models about the postpartum emotional and behavioral changes in new mothers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib13">13</a>]. Using observations on prenatal behaviors, and specifically attributes such as engagement, emotion, ego network and linguistic styles, these models could classify mothers who would exhibit significant postpartum changes with an accuracy of 71% and 80-83% while additionally considering the initial postnatal data as well.</p>
        <p>Similarly, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib10">10</a>], crowd-sourced Twitter user data is utilized to build a classifier that predicts risk estimation before the onset. This study further analyzes the dataset characteristics to reach observations on diurnal patterns, social connectedness and volume of postings for both depression and non-depression classes. The model was able to predict the onset of depression with an accuracy of 72.384% with all features and reduced dimensions.</p>
        <p>[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>] uses an automated analysis followed by manual refinement to find positive and negative samples for PTSD. The tweets posted by these users were collected during a time window to create a corpus of positive and negative PTSD data, to train three different classifiers. These were used to identify and evaluate PTSD trends in tweets across military and civilian populations. This method was further enhanced [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib14">14</a>] for indicators of bipolar disorder, major depressive disorder and seasonal affective disorder.</p>
        <p>TensiStrength is the first published system to detect the strength of stress and relaxation expressed in tweets. It is primarily a lexical approach. The lexicon is partly derived from LIWC [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib15">15</a>], General Inquirer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib16">16</a>] and emotion terms from SentiStrength [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib17">17</a>] a similar lexicon-based sentiment analysis program. For the original evaluation, a corpus of 3066 English tweets were human-coded for stress and relaxation on five point scales: -1 denoting no stress and -5 denoting the highest stress; +1 for no relaxation and +5 for the highest relaxation. Its performance was similar to several machine learning algorithms, including Support Vector Machines.</p>
        <p>The hybrid system presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib18">18</a>] combines a factor graph model and convolutional neural network (CNN) and estimates the relationship between users’ psychological stress levels and their social network interactions. This method improves stress detection by 6-9% in terms of F1-score and was used to investigate the social interactions of stressed and non-stressed users.</p>
        <p>Moodee[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib19">19</a>] a practical mobile application addresses challenges of stress detection from social media content, including missing data, time-series modelling and data sparsity problems. It extracts the tweet-level linguistic, visual and social attributes defined in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib20">20</a>]. These attributes are fed into cross auto-encoders which are embedded in a CNN, integrating them to user level content attributes [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib21">21</a>]. This system recommends links to users to dissipate stress.</p>
        <p>The system described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib22">22</a>] identifies stress in social media texts. This uses stressor event categories and stressor subjects and a collection of words related to each category and subject on the basis of word embeddings. The stressor event and subjects are identified using a novel hybrid model which combines multi-task learning with CNN. Tweets are assigned a stress value based on the Social Readjustment Rating Scale [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib7">7</a>]. The stress scores obtained this way give results comparable to some state of the art machine learning models.</p>
        <p>As reviewed above, research so far primarily focuses on the health implications of stress, trying to identify and remedy the symptoms in a reactive manner. There has been very little analysis of the underlying causes of stress and relaxation. In contrast, one pioneering attempt to find the causes of stress and relaxation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib7">7</a>], limits its area of interest to personal events such as divorce, death and childbirth. Our study, on the other hand, analyzes tweets expressing short-term or long-term stress and relaxation in a variety of contexts such as traffic, politics, personal events and academia.</p>
      </section>
      <section id="sec2Z3">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Word Sense Disambiguation in Sentiment Analysis</h3>
          </div>
        </header>
        <p>An important challenge for detecting expressions of stress or any affective state is that social media text uses non-standard grammar and informal language. Affective words can be ambiguous, with their sense changing according to the context. For example, in “There was a chill in the air”, the affect word “chill” indicates stress whereas in “I am a pretty chill guy”, it indicates relaxation. A system to resolve the meaning of this word is essential to correctly identify stress. The natural language processing task of WSD identifying the meaning of words in context in a computational manner [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib23">23</a>], is the traditional response to this issue.</p>
        <p>A WSD system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib24">24</a>] using Babelfy and SentiWordNet illustrates how it can improve the accuracy of sentiment detection in Twitter and SMS test data. Babelfy is a multilingual graph-based method for disambiguating word senses. It is based on the semantic network BabelNet 3.0. The tweets were preprocessed using the tokenizer of Carnegie Mellon University (CMU) Twitter NLP tool and NLTK. The tweets were represented as vectors with three features – sums of corresponding positive, negative and neutral scores of words in SentiWordNet. A supervised Random Forest Decision Trees classifier was trained based on these representations. This system was shown to have better accuracy (58.55%) compared to the baseline method (45.26%). The paper establishes that WSD improves sentiment analysis accuracy of social media data, specifically tweets.</p>
        <p>A research work on sentiment analysis for figurative language [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib25">25</a>] applies WSD and assigns polarity to word sense through a graph-based method. Further, sentence-level polarity is detected based on two Hidden Markov Models each trained for positive and negative cases. This system has significantly better recall and precision compared to a polarity detection method without WSD and a baseline WSD method (which assigns the first sense entry in WordNet to all senses).</p>
        <p>Incorporating WSD based on the context of word-of-mouth documents to modify SentiWordNet lexicons [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib26">26</a>] is found to improve sentiment analysis performance. Similarly, a WSD system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib27">27</a>] uses a path based semantic relatedness to find the most appropriate sense and it is found to give a better classification f-score for sentiment analysis task.</p>
        <p>Yet another study [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib28">28</a>] integrates a WSD algorithm using extended gloss overlap described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib29">29</a>] with VoxPop [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib30">30</a>], a classifier of text polarity based on SentiWordNet and improved accuracy from 50.5% to 60.0%.</p>
        <p>While varying in methodology and lexical resources, these studies illustrate that sentiment analysis systems with WSD substantially outperform those without WSD. However, the implications of incorporating WSD in stress/relaxation analysis have never been studied before and our research treats it as a potential way to improve the accuracy of an existing lexical method.</p>
      </section>
      <section id="sec2Z4">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.4</span> Word Vector Representation and its Application in Word Sense Disambiguation</h3>
          </div>
        </header>
        <p>Standard natural language processing methods for WSD are predicated on standard grammar and spelling, which is uncommon in the social web. A promising method for WSD for informal text is to represent a word as a real-valued vector registering the frequency of its contextual words (e.g., in the same sentence). This can be compared to similar aggregate vectors for the different senses of the word and the closest matching vector used to select the most likely work sense. GloVe, an unsupervised learning algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib30">30</a>] and Word2Vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib31">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib32">32</a>] are the two most common architectures for finding vector representations for words. Word2Vec uses continuous vector representations of words in two models – Bag of words and Skip-gram. In bag of words, the target is predicting a central word in focus, given the input context words. The Skip-gram model, on the other hand, has the focus word as the single input vector and the target is to predict the context words. The GloVe model aims to combine the benefits of the skip-gram model with global statistical information about word co-occurrences using matrix factorization methods.</p>
        <p>Sense2Vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib34">34</a>] is a supervised sense embedding system which can distinguish between different senses of the same word based on the context. The system leverages supervised NLP labels to determine the sense in an instance. This reduces the computational overhead making it more suitable for computationally intensive NLP tasks. It is demonstrated as an efficient WSD method in the context of traditional NLP tasks such as Named Entity Resolution, Sentiment Disambiguation and Part of Speech tagging.</p>
        <p>Similarly, a sense vector scheme obtained from skip-gram based word vectors and WordNet glosses has been proposed for WSD [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib34">34</a>]. The algorithm presented in this paper works as two steps: initialization of word and sense vectors and sense disambiguation. In the first step, the word vectors are learned through skip-gram model and from this, sense vectors for each sense is constructed. For each word in the gloss, the cosine similarity with the original word is calculated. Those words with cosine similarity higher than a threshold are added to a candidate set. The average of the vectors of the candidate words is taken as the sense vector.</p>
        <p>In the second step of word disambiguation, given a sentence, an initial context vector is calculated by finding the average of content words’ vectors. The sense which has the highest cosine similarity to the context vector is taken as the disambiguation result. We implemented this method of sense disambiguation with word vectors in our current experiments.</p>
        <p>We chose this unified vector model or word sense representation and disambiguation in our study for the following reasons:</p>
        <ol class="list-no-style">
          <li label="1.">It uses a predefined inventory of senses. In our implementation, we pre-computed these vector representations, prior to actual tweets’ processing, thus minimizing the computational overhead.<br /></li>
          <li label="2.">This system comprehensively covers all senses in the standard lexical resource, WordNet, representing each by a corresponding vector. It makes use of the high-quality glosses in WordNet, thus forming the basis for word senses with reliable semantic contexts.<br /></li>
        </ol>
        <p></p>
      </section>
    </section>
    <section id="sec-003">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> METHODOLOGY</h2>
        </div>
      </header>
      <section id="sec3Z1">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Overview</h3>
          </div>
        </header>
        <p>The purpose of this research is to enhance the existing TensiStrength method to identify stress/relaxation strengths by incorporating a pre-processing phase to disambiguate ambiguous affect words in tweets. This is performed by implementing the WSD solution proposed in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib34">34</a>], by finding out which sense vector, among a set of candidate senses, has the highest cosine similarity to the vector representing the context/sentence.</p>
        <p>The performance of the improved version of TensiStrength with an incorporated WSD system is compared to that of classic TensiStrength and is found to be consistently better for stress/relaxation accuracy.</p>
      </section>
      <section id="sec3Z2">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Workflow</h3>
          </div>
        </header>
        <p>The basic approach, devised in the original TensiStrength system and followed in this modified version is to assign to each sentence two scores: those of the highest stress term and the highest relaxation term found in it, modifying these scores with a number of rules covering issues like spelling, negation and booster words. The dual scoring is based on the observation that the same sentence can possibly contain expressions of both stress and relaxation.</p>
        <p>The rules considered while assigning stress/relaxation values in original and modified versions of TensiStrength are the following:</p>
        <ol class="list-no-style">
          <li label="1.">Two or more repeated letters increase the stress/relaxation values by 1. (scarryy has a higher stress value than scary.)<br /></li>
          <li label="2.">Idioms are treated as single unit with assigned stress/relaxation value. The scores of individual words are ignored.<br /></li>
          <li label="3.">Negation of stress words neutralises them.<br /></li>
          <li label="4.">Negation of relaxing words turn them as stress words.<br /></li>
          <li label="5.">Emoticons are assigned appropriate stress/relaxation values.<br /></li>
          <li label="6.">Spelling correction to delete repeated letters to form words.<br /></li>
        </ol>
        <p></p>
        <p>In the current work, we modified TensiStrength, by adding a pre-processing step, which implements WSD (Figure <a class="fig" href="#fig1">1</a>). It first selects a sense out of a list of senses from WordNet using Word2Vec. This pre-processed tweet with the correctly disambiguated affect word is then given to the lexicon based stress/relaxation magnitude detection method with an extended lexicon to include different scores for different word senses rather than a single score for all word senses, as in the previous TensiStrength.</p>
        <figure id="fig1">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191627/images/image1.jpg" class="img-responsive" alt="Figure 1:" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span> <span class="figure-title">Workflow of tweets' processing by the modified TensiStrength algorithm (with WSD).</span>
          </div>
        </figure>
        <p>In the first example (Table <a class="tbl" href="#tb1">1</a>), there are two stress terms, uncertain and glum with stress strengths -3 and -2 respectively. The score of the highest stress term is assigned to the tweet. In the second tweet, there are two affect words, one each for stress and relaxation and the tweet is assigned stress and relaxation scores corresponding to these words.</p>
        <div class="table-responsive" id="tb1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Examples of tweets with stress/relaxation values assigned by TensiStrength with and without WSD (with affect (stress/relaxation) terms highlighted).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Tweet</th>
                <th style="text-align:left;">TensiStrength without WSD</th>
                <th style="text-align:left;">TensiStrength with WSD</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">The future looked <strong>uncertain</strong> and <strong>glum</strong></td>
                <td style="text-align:left;">(-3, +1)</td>
                <td style="text-align:left;">(-3, +1)</td>
              </tr>
              <tr>
                <td style="text-align:left;">Everyone in the valley has a <strong>heavy</strong> heart but the best thing to do was <strong>worship</strong></td>
                <td style="text-align:left;">(-3, +2)</td>
                <td style="text-align:left;">(-3, +2)</td>
              </tr>
              <tr>
                <td style="text-align:left;">She was wearing a <strong>cool</strong> pink dress</td>
                <td style="text-align:left;">(-1, +2)</td>
                <td style="text-align:left;">(-1, +2)</td>
              </tr>
              <tr>
                <td style="text-align:left;">The lady gave me <strong>cool</strong> stare</td>
                <td style="text-align:left;">(-1, +2)</td>
                <td style="text-align:left;">(-2, +1)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>TensiStrength without WSD assigns a relaxation strength of +2 and stress strength -1 to the last example. In the modified version, the WSD module identifies the right sense of the affect word ‘cold’ in the given context and assigns stress strength of -2 and relaxation strength of +1.</p>
      </section>
      <section id="sec3Z3">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Extending lexicon of TensiStrength to incorporate ambiguity of affect words</h3>
          </div>
        </header>
        <p>TensiStrength uses a lexical approach with a list of terms annotated with strengths of stress and relaxation. TensiStrength's lexicon is a combination of terms from LIWC [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib15">15</a>], General Inquirer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib16">16</a>] and the lexicon of the sentiment detection program, SensiStrength [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib17">17</a>] with a few manual additions. We identified 40 ambiguous affect words present in the existing TensiStrength lexicon and manually updated it to accommodate the stress/relaxation values for different senses of each of them. It is to be noted that while the language contains several more ambiguous words, we have, in this experiment, considered only those ambiguous words already present in the TensiStrength lexicon as affect words. A complete list is given in Table <a class="tbl" href="#tb2">2</a> .</p>
        <div class="table-responsive" id="tb2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">List of the ambiguous affect words whose stress/relaxation scores were updated in the modified TensiStrength lexicon.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">fine concern</th>
                <th style="text-align:left;">jam dark</th>
                <th style="text-align:left;">fair tight</th>
                <th style="text-align:left;">nervous fire</th>
                <th style="text-align:left;">block breakdown</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>chill</strong></td>
                <td style="text-align:left;">wreck</td>
                <td style="text-align:left;">heavy</td>
                <td style="text-align:left;">strain</td>
                <td style="text-align:left;">Fume</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Tax</strong></td>
                <td style="text-align:left;">cold</td>
                <td style="text-align:left;">cool</td>
                <td style="text-align:left;">choke</td>
                <td style="text-align:left;">Stick</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>late</strong></td>
                <td style="text-align:left;">turbulence</td>
                <td style="text-align:left;">fraught</td>
                <td style="text-align:left;">radical</td>
                <td style="text-align:left;">Noise</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>waste</strong></td>
                <td style="text-align:left;">critical</td>
                <td style="text-align:left;">trick</td>
                <td style="text-align:left;">protest</td>
                <td style="text-align:left;">Stress</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>scene</strong></td>
                <td style="text-align:left;">mad</td>
                <td style="text-align:left;">hard</td>
                <td style="text-align:left;">scheme</td>
                <td style="text-align:left;">Accident</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>artificial</strong></td>
                <td style="text-align:left;">brazen</td>
                <td style="text-align:left;">callous</td>
                <td style="text-align:left;">coarse</td>
                <td style="text-align:left;">Desert</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tb3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">Examples of additions made to the TensiStrength lexicon.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">Stress/relaxation scores</th>
                <th style="text-align:left;">Sense description from WordNet</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>fine_1</strong></td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">fine,&nbsp;mulct,&nbsp;amercement&nbsp;(money extracted as a penalty)</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>fine_2</strong></td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">issue a ticket or a fine to as a penalty</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>fine_3</strong></td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">all right,&nbsp;fine,&nbsp;o.k.,&nbsp;ok,&nbsp;okay,&nbsp;hunky-dory,&nbsp;cool&nbsp;(being satisfactory or in satisfactory condition)&nbsp;</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>cool_2</strong></td>
                <td style="text-align:left;">3</td>
                <td style="text-align:left;">aplomb,&nbsp;assuredness,&nbsp;cool,&nbsp;poise,&nbsp;sangfroid</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>cool_9</strong></td>
                <td style="text-align:left;">-2</td>
                <td style="text-align:left;">psychologically cool and unenthusiastic; unfriendly or unresponsive or showing dislike</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tb4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class="table-title">Inter-coder agreement for stress value annotation.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Method</th>
                <th style="text-align:left;">A and B</th>
                <th style="text-align:left;">B and C</th>
                <th style="text-align:left;">A and C</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Krippendorff's α</td>
                <td style="text-align:left;">0.774</td>
                <td style="text-align:left;">0.781</td>
                <td style="text-align:left;">0.771</td>
              </tr>
              <tr>
                <td style="text-align:left;">Pearson Correlation</td>
                <td style="text-align:left;">0.796</td>
                <td style="text-align:left;">0.814</td>
                <td style="text-align:left;">0.792</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>For example, the lexicon for TensiStrength without WSD had only one entry for the affect word ‘cool’, as +2 indicating moderate relaxation. But the modified lexicon, acknowledges the various senses of the same word, obtained from the resource WordNet. For example, the ninth sense of the word ‘cool’ in WordNet is ‘unfriendly or unresponsive or showing dislike’ which has an indication for stress, hence it is assigned the value of -2. Similarly, different senses of each of these ambiguous words are assigned appropriate stress/relaxation values too. The ordinal number of the sense is represented by a suffix to the original word. A few example additions to the lexicon, together with the assigned stress/relaxation strength and the WordNet gloss is given in the table.</p>
      </section>
      <section id="sec3Z4">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> <em>Dataset and Annotation</em></h3>
          </div>
        </header>
        <p>A set of one thousand tweets with ambiguous affect words (e.g. fine) was collected over the period of one month (from 1<sup>st</sup> February 2017 to 1<sup>st</sup> March 2017) as the dataset for the experiments. It was annotated individually and independently by a set of three human coders on a five point scale for stress and relaxation.</p>
        <p>The coders had participated in an identical annotation procedure as part of the TensiStrength experiment. Hence they had prior experience with the stress/relaxation strength annotation task. The tweets were annotated for stress and relaxation strengths with scores ranging from +1 to +5 (+1 denotes no relaxation and +5 very high relaxation) and -1 to -5 (-1 denotes no stress and -5 very high stress).</p>
        <p>The arithmetic mean of the three coders’ annotation scores were calculated for each tweet and rounded off to the nearest integer value. This was taken as the gold standard of stress and relaxation values. The inter-coder agreement between each pair of coders was calculated using Krippendorff's α [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib37">37</a>] and was found to have high positive values. Krippendorff suggests lowest conceivable limit of α &gt; 0.667. The overall agreement between the three coders for stress (0.778) and relaxation (0.781) was found to be high enough. It is compared with agreement between the coders in similar experiments in the following table.</p>
        <div class="table-responsive" id="Utb5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class="table-title">Inter-coder agreement for relaxation annotation.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Method</th>
                <th style="text-align:left;">A and B</th>
                <th style="text-align:left;">B and C</th>
                <th style="text-align:left;">A and C</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Krippendorff's α</td>
                <td style="text-align:left;">0.751</td>
                <td style="text-align:left;">0.802</td>
                <td style="text-align:left;">0.781</td>
              </tr>
              <tr>
                <td style="text-align:left;">Pearson Correlation</td>
                <td style="text-align:left;">0.796</td>
                <td style="text-align:left;">0.820</td>
                <td style="text-align:left;">0.797</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Higher agreement values in the subsequent experiments denote that the coders are consistently getting better with the annotation task. It also indicates that the problem statement and annotation instructions are well-defined.</p>
      </section>
      <section id="sec3Z5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span> Experimental Setup</h3>
          </div>
        </header>
        <p>A Twitter Word2Vec model trained on 400 million tweets, released as part of an ACL W-NUT task [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib19">19</a>] is used for training the word and sense vectors for this experiment. For each word in the ambiguous words’ list mentioned in the previous section, we calculate vectors of dimension 200, corresponding to each different sense, treating WordNet as the sense inventory. The WSD module based on the method presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>] was implemented using Anaconda Python (versions, Anaconda-4.4.0 and Python-3.6.1) packages, specifically Scipy, Numpy and nltk, as part of our research work to evaluate the improvement it would bring about in stress and relaxation strength detection.</p>
      </section>
    </section>
    <section id="sec4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> RESULTS</h2>
        </div>
      </header>
      <section id="sec4Z1">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Performance of TensiStrength with WSD</h3>
          </div>
        </header>
        <p>The accuracy of stress and relaxation detection in tweets using TensiStrength with WSD is compared with TensiStrength without the WSD preprocessing phase and a range of standard machine learning algorithms: Adaptive Boosting algorithm(AdaBoost), Naïve Bayes classifier(Bayes), Decision tree(J48 Tree), Logistic Regression(Logistic) and Support Vector Machines(SVM). Unigrams, Bigrams and Trigrams were used as features. Each classifier was implemented using its configuration in Weka 3.6. The performance of the machine learning algorithms was evaluated using 10-fold cross validation 30 times, with the average sores across the 30 iterations recorded.</p>
        <p>Based on Pearson's correlations and exact match percentages with the human-annotated data, it is found TensiStrength with a preprocessing phase to disambiguate the word senses performs considerably better than the state-of-the-art machine learning algorithms and TensiStrength without the preprocessing. The results are summarized in the tables <a class="tbl" href="#tb5">5</a>,<a class="tbl" href="#tb6">6</a>.</p>
        <div class="table-responsive" id="tb5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class="table-title">Stress detection results.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Method</th>
                <th style="text-align:left;">Pearson Correlation</th>
                <th style="text-align:left;">Exact match %</th>
                <th style="text-align:left;">Match %(within 1)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">AdaBoost</td>
                <td style="text-align:left;">0.3543</td>
                <td style="text-align:left;">30.1671</td>
                <td style="text-align:left;">83.1562</td>
              </tr>
              <tr>
                <td style="text-align:left;">Bayes</td>
                <td style="text-align:left;">0.3238</td>
                <td style="text-align:left;">35.3464</td>
                <td style="text-align:left;">90.1824</td>
              </tr>
              <tr>
                <td style="text-align:left;">J48 Tree</td>
                <td style="text-align:left;">0.4649</td>
                <td style="text-align:left;">47.4562</td>
                <td style="text-align:left;">88.3543</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logistic</td>
                <td style="text-align:left;">0.4948</td>
                <td style="text-align:left;">49.2679</td>
                <td style="text-align:left;">89.2167</td>
              </tr>
              <tr>
                <td style="text-align:left;">SVM</td>
                <td style="text-align:left;">0.5124</td>
                <td style="text-align:left;">51.2543</td>
                <td style="text-align:left;">91.5626</td>
              </tr>
              <tr>
                <td style="text-align:left;">TensiStrength<br />
                Without WSD</td>
                <td style="text-align:left;">0.4735</td>
                <td style="text-align:left;">48.8112</td>
                <td style="text-align:left;">83.2367</td>
              </tr>
              <tr>
                <td style="text-align:left;">TensiStrength with WSD</td>
                <td style="text-align:left;">0.5443</td>
                <td style="text-align:left;">53.1091</td>
                <td style="text-align:left;">92.4137</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tb6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class="table-title">Relaxation detection results.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Method</th>
                <th style="text-align:left;">Pearson Correlation</th>
                <th style="text-align:left;">Exact Match Percentage</th>
                <th style="text-align:left;">Match percentage (within 1)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">AdaBoost</td>
                <td style="text-align:left;">0.3254</td>
                <td style="text-align:left;">34.1335</td>
                <td style="text-align:left;">83.5451</td>
              </tr>
              <tr>
                <td style="text-align:left;">Bayes</td>
                <td style="text-align:left;">0.3577</td>
                <td style="text-align:left;">38.9221</td>
                <td style="text-align:left;">84.6735</td>
              </tr>
              <tr>
                <td style="text-align:left;">J48 Tree</td>
                <td style="text-align:left;">0.5224</td>
                <td style="text-align:left;">51.5634</td>
                <td style="text-align:left;">86.1734</td>
              </tr>
              <tr>
                <td style="text-align:left;">Logistic</td>
                <td style="text-align:left;">0.4987</td>
                <td style="text-align:left;">54.3267</td>
                <td style="text-align:left;">89.1798</td>
              </tr>
              <tr>
                <td style="text-align:left;">SVM</td>
                <td style="text-align:left;">0.5546</td>
                <td style="text-align:left;">58.7324</td>
                <td style="text-align:left;">91.6598</td>
              </tr>
              <tr>
                <td style="text-align:left;">TensiStrength<br />
                Without WSD</td>
                <td style="text-align:left;">0.5304</td>
                <td style="text-align:left;">56.3878</td>
                <td style="text-align:left;">85.8364</td>
              </tr>
              <tr>
                <td style="text-align:left;">TensiStrength with WSD</td>
                <td style="text-align:left;">0.56441</td>
                <td style="text-align:left;">60.6981</td>
                <td style="text-align:left;">93.1227</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tb7">
          <div class="table-caption">
            <span class="table-number">Table 7:</span> <span class="table-title">Examples of errors.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Tweet</th>
                <th style="text-align:left;">Correct score</th>
                <th style="text-align:left;">TensiStrength Score (without WSD)</th>
                <th style="text-align:left;">TensiStrengthScore (WSD)</th>
                <th style="text-align:left;">Affect Word</th>
                <th style="text-align:left;">Context Word</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;"><strong>I am a human train wreck</strong></td>
                <td style="text-align:left;">(-3, +1)</td>
                <td style="text-align:left;">(-3, +1)</td>
                <td style="text-align:left;">(-2, +1)</td>
                <td style="text-align:left;">Wreck</td>
                <td style="text-align:left;">train</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>I had a heavy heart to carry</strong></td>
                <td style="text-align:left;">(-4,+1)</td>
                <td style="text-align:left;">(-2, +1)</td>
                <td style="text-align:left;">(-2, +1)</td>
                <td style="text-align:left;">heavy</td>
                <td style="text-align:left;">carry</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Great night mad crowd, thanks everyone</strong></td>
                <td style="text-align:left;">(-1, +3)</td>
                <td style="text-align:left;">(-1, +2)</td>
                <td style="text-align:left;">(-3, +1)</td>
                <td style="text-align:left;">mad</td>
                <td style="text-align:left;">crowd</td>
              </tr>
              <tr>
                <td style="text-align:left;"><strong>Antioxidants eliminate destructive potential of free radicals</strong></td>
                <td style="text-align:left;">(-1, +2)</td>
                <td style="text-align:left;">(-2, +1)</td>
                <td style="text-align:left;">(-2, +1)</td>
                <td style="text-align:left;">radical</td>
                <td style="text-align:left;">destructive</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>It can be observed that the WSD phase substantially improves the stress/relaxation detection accuracy in terms of Pearson's correlation, exact match percentage and match percentage within 1. TensiStrength with WSD outperforms the machine learning methods as well. However, it should be noted that the small size of the annotated dataset (and the training data) could have been a major cause of the relatively poor performance of the machine learning methods. Further results with higher number of tweets could better establish the superior effectiveness of TensiStrength with WSD over standard machine learning methods as a stress and relaxation detection. Also, tweets in the current dataset contain the ambiguous words added to the TensiStrength lexicon. The performance on a randomly chosen set of tweets could be different.</p>
      </section>
      <section id="sec4Z2">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Error Analysis</h3>
          </div>
        </header>
        <p>The misinterpreted stress/relaxation expressions occur in tweets with no direct affect words or affect words with context words misleading the WSD module. In the first category, we can find tweets such as ‘I am a train wreck’ or ‘I had a heavy heart to carry’. The correct (human annotated) stress score for ‘I am a train wreck’ is -3 and for ‘I had heavy heart to carry’ it is -4. However, the WSD module incorrectly disambiguates the sense of the ambiguous affect word (wreck, heavy) because the context words train and carry, and assigns a lower stress score of -2 to both. TensiStrength without WSD, gives the scores solely dependent on the lexicon scores of the affect words present.</p>
        <p>Another source of error is indirect expressions of stress and relaxation. Examples in our dataset are ‘Sat by the fire; little things in life’ (correct stress relaxation score is (-1, +4), our system score is (-2, +1)). ‘Straining ears for a laughter clip; but hearing nothing’ (correct score (-2, +1) our system score (-1, +3). Such indirect expressions are challenging for all automated systems in finding the sentiment or stress strengths.</p>
      </section>
      <section id="sec4Z3">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span> Domain Analysis of Stress/relaxation for tweets</h3>
          </div>
        </header>
        <p>We manually classified the tweets into one of these domains: Academics, Business, Climate, Entertainment, Food, Health, Personal Events, Politics, Religion, Sports and Travel. The analysis of stress/relaxation values and performance of TensiStrength with and without WSD are summarized in the following tables, Table <a class="tbl" href="#tb8">8</a> and Table <a class="tbl" href="#tb9">9.</a></p>
        <div class="table-responsive" id="tb8">
          <div class="table-caption">
            <span class="table-number">Table 8:</span> <span class="table-title">Stress detection performance of TensiStrength with and without WSD in different domains.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Domain</th>
                <th style="text-align:left;">% of tweets</th>
                <th style="text-align:left;">Mean Stress value</th>
                <th style="text-align:left;">Pearson correlation forTensiStrength(Without WSD)</th>
                <th style="text-align:left;">Pearson correlation for TensiStrength (With WSD)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Academics</td>
                <td style="text-align:left;">10%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.356</td>
                <td style="text-align:left;">0.412</td>
              </tr>
              <tr>
                <td style="text-align:left;">Business</td>
                <td style="text-align:left;">7%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.422</td>
                <td style="text-align:left;">0.455</td>
              </tr>
              <tr>
                <td style="text-align:left;">Climate</td>
                <td style="text-align:left;">11%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.485</td>
                <td style="text-align:left;">0.512</td>
              </tr>
              <tr>
                <td style="text-align:left;">Enteratainment</td>
                <td style="text-align:left;">8%</td>
                <td style="text-align:left;">-2</td>
                <td style="text-align:left;">0.487</td>
                <td style="text-align:left;">0.523</td>
              </tr>
              <tr>
                <td style="text-align:left;">Food</td>
                <td style="text-align:left;">6%</td>
                <td style="text-align:left;">-1</td>
                <td style="text-align:left;">0.365</td>
                <td style="text-align:left;">0.376</td>
              </tr>
              <tr>
                <td style="text-align:left;">Health</td>
                <td style="text-align:left;">9%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.418</td>
                <td style="text-align:left;">0.452</td>
              </tr>
              <tr>
                <td style="text-align:left;">Personal</td>
                <td style="text-align:left;">18%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.501</td>
                <td style="text-align:left;">0.532</td>
              </tr>
              <tr>
                <td style="text-align:left;">Politics</td>
                <td style="text-align:left;">11%</td>
                <td style="text-align:left;">-4</td>
                <td style="text-align:left;">0.567</td>
                <td style="text-align:left;">0.603</td>
              </tr>
              <tr>
                <td style="text-align:left;">Religion</td>
                <td style="text-align:left;">5%</td>
                <td style="text-align:left;">-4</td>
                <td style="text-align:left;">0.574</td>
                <td style="text-align:left;">0.631</td>
              </tr>
              <tr>
                <td style="text-align:left;">Sports</td>
                <td style="text-align:left;">7%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.452</td>
                <td style="text-align:left;">0.597</td>
              </tr>
              <tr>
                <td style="text-align:left;">Travel</td>
                <td style="text-align:left;">8%</td>
                <td style="text-align:left;">-3</td>
                <td style="text-align:left;">0.423</td>
                <td style="text-align:left;">0.482</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tb9">
          <div class="table-caption">
            <span class="table-number">Table 9:</span> <span class="table-title">Relaxation detection performance of TensiStrength with and without WSD in different domains.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Domain</th>
                <th style="text-align:left;">% of tweets</th>
                <th style="text-align:left;">Mean Relax value</th>
                <th style="text-align:left;">Pearson correlation forTensiStrength(Without WSD)</th>
                <th style="text-align:left;">Pearson correlation for TensiStrength With WSD)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">Academics</td>
                <td style="text-align:left;">7%</td>
                <td style="text-align:left;">1</td>
                <td style="text-align:left;">0.378</td>
                <td style="text-align:left;">0.425</td>
              </tr>
              <tr>
                <td style="text-align:left;">Business</td>
                <td style="text-align:left;">7%</td>
                <td style="text-align:left;">1</td>
                <td style="text-align:left;">0.523</td>
                <td style="text-align:left;">0.527</td>
              </tr>
              <tr>
                <td style="text-align:left;">Climate</td>
                <td style="text-align:left;">11%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.514</td>
                <td style="text-align:left;">0.497</td>
              </tr>
              <tr>
                <td style="text-align:left;">Enteratainment</td>
                <td style="text-align:left;">8%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.390</td>
                <td style="text-align:left;">0.426</td>
              </tr>
              <tr>
                <td style="text-align:left;">Food</td>
                <td style="text-align:left;">6%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.590</td>
                <td style="text-align:left;">0.653</td>
              </tr>
              <tr>
                <td style="text-align:left;">Health</td>
                <td style="text-align:left;">9%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.623</td>
                <td style="text-align:left;">0.671</td>
              </tr>
              <tr>
                <td style="text-align:left;">Personal</td>
                <td style="text-align:left;">18%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.612</td>
                <td style="text-align:left;">0.634</td>
              </tr>
              <tr>
                <td style="text-align:left;">Politics</td>
                <td style="text-align:left;">14%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.633</td>
                <td style="text-align:left;">0.645</td>
              </tr>
              <tr>
                <td style="text-align:left;">Religion</td>
                <td style="text-align:left;">5%</td>
                <td style="text-align:left;">1</td>
                <td style="text-align:left;">0.542</td>
                <td style="text-align:left;">0.602</td>
              </tr>
              <tr>
                <td style="text-align:left;">Sports</td>
                <td style="text-align:left;">7%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.552</td>
                <td style="text-align:left;">0.574</td>
              </tr>
              <tr>
                <td style="text-align:left;">Travel</td>
                <td style="text-align:left;">8%</td>
                <td style="text-align:left;">2</td>
                <td style="text-align:left;">0.587</td>
                <td style="text-align:left;">0.694</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p></p>
        <ol class="list-no-style">
          <li label="1.">The largest categories of tweets are personal events, politics and climate categories. The rest of the tweets are almost uniformly distributed across the remaining 8 domains. It can be seen that WSD consistently improves the performance of TensiStrength algorithm in tweets belonging to all domains. The performance improvement is not restricted to any single domain.<br /></li>
          <li label="2.">The tweets have higher values of stress compared to relaxation. The mean value of stress expressed across all domains is -2.81 whereas the mean value of relaxation expressed is +1.72. This could be due to the fact that stress is an affective state with high arousal and relaxation has low arousal. In that sense, stress is a stronger affective state and thus more probable in eliciting responses like tweets.<br /></li>
          <li label="3.">Politics and religion has higher stress values for the tweets (-4) and food and entertainment are domains with lowest stress values (-1 and -2 respectively). This is in agreement with the intuitive expectation of stress values from the domains.<br /></li>
        </ol>
        <p></p>
      </section>
    </section>
    <section id="sec5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> CONCLUSIONS AND FUTURE WORK</h2>
        </div>
      </header>
      <p>In this research work, we implemented a WSD solution as a pre-processing stage to an existing lexicon-based stress/relaxation method. A dataset comprising of 1000 tweets with ambiguous affect words was collected and annotated with high inter-annotator agreement. Incorporating a WSD was found to significantly improve the performance of TensiStrength in terms of Pearson's correlation and exact match percentage, for both stress and relaxation. TensiStrength with WSD outperforms machine learning methods as well. Given the relatively small size of the test set, this has to be further studied using bigger datasets annotated with stress and relaxation strengths.</p>
      <p>Our planned future step is to analyze the causes of stress and relaxation (namely, stressors and relaxers) expressed in tweets. In the literature review, this was found to be a largely unexplored research field. We formulated a novel framework for finding the stressors and relaxers in a variety of domains such as politics, sports, traffic and personal events. This framework will primarily consist of two modules: Categorizer and Cause Finder. We plan to use a machine learning classifier in the first module to tag the tweets with a broad category of topic such as politics, sports, entertainment, personal events, health, climate and traffic, as listed in the results section. Each category has a list of associated potential stressors and relaxers, to be manually compiled as part of the research. Thus, the reason for expressed stress and relaxation is narrowed down to one among this list. The Cause Finder module finds the actual cause from this list using word vector representations. It eliminates redundant words such as prepositions, conjunctions, interjections and articles from the tweet and constructs a set of keywords from the remaining words. The cosine similarity of vectors representing each word in this set with each potential stressor/relaxer in the relevant category. The stressor/relaxer word represented by the vector having the highest cosine similarity to any of the keywords in the tweet is chosen as the cause of stress/relaxation in the tweet. We ran pilot experiments with a small dataset of tweets to verify the feasibility of this approach and are currently working on compiling the complete dataset and running further experiments.</p>
      <p>Thus this research work contributes to the automatic detection of stress and relaxation by both improving the accuracy and putting forward a novel framework for identifying its causal agents. In future, this can potentially replace or complement traditional stress/relaxation detection methods and can contribute to improved applications and services such as healthcare, traffic management or customer care. It is a promising step in towards the greater goal of leveraging the Web and specifically social media data.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ack-001">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGEMENTS</h2>
        </div>
      </header>
      <p>This research was supported by the European Union's Horizon 2020 research and innovation programme under grant agreement No 636160-2, the Optimum project <strong><a class="link-inline force-break" target="_blank" href="http://www.optimumproject.eu">www.optimumproject.eu</a></strong>.</p>
    </section>
    <section id="bib-sec-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="bib1" label="[1]">Mike Thelwall.&nbsp;2017. TensiStrength: stress and relaxation magnitude detection for social media texts. Journal of Information Processing and Management.&nbsp;(2017); 53:&nbsp;106–121</li>
        <li id="bib2" label="[2]">Hans Selye. 1956. The Stress of Life. New York, McGraw-Hill Book Company, Inc.</li>
        <li id="bib3" label="[3]">J. Choi and R. Gutierrez-Osuna. Using heart rate monitors to detect mental stress,&nbsp;<em>Proc. 6th Int. Workshop Wearable Implantable BSN</em>, pp. 219-223, 2009.</li>
        <li id="bib4" label="[4]">Y. Shi, N. Ruiz, R. Taib, E. Choi and F. Chen. 2007. "Galvanic skin response (GSR) as an index of cognitive load",&nbsp;<em>Proc. CHI Ext. Abstr. Human Factors Comput. Syst.</em>, pp. 2651-2656.</li>
        <li id="bib5" label="[5]">P.&nbsp;Ren,&nbsp;A.&nbsp;Barreto,&nbsp;J.&nbsp;Huang,&nbsp;Y.&nbsp;Gao,&nbsp;F.R.&nbsp;Ortega, and M.&nbsp;Adjouadi. 2014. Off-line and on-line stress detection through processing of the pupil diameter signal Ann. Biomed. Eng.,&nbsp;42&nbsp;(1) pp.&nbsp;162-176,&nbsp;<span style="text-decoration: underline;">10.1007/s10439-013-0880-9</span></li>
        <li id="bib6" label="[6]">F. Kittel, M. Kornitzer and M. Dramaix. 1986.. Evaluation of type A personality. Postgraduate Medical Journal 62(730):781-3.</li>
        <li id="bib7" label="[7]">T.H. Holmes and R. H. Rahe. 1967. The social readjustment rating scale. Journal of psychosomatic research,. Elsevier</li>
        <li id="bib8" label="[8]">R. Kotikalapudi, S. Chellappan, F. Montgomery. 2012. Associating Internet usage with depressive behavior among college students. IEEE Technol Soc Mag.;31:73–80.</li>
        <li id="bib9" label="[9]">M. A. Moreno, L. A. Jelenchick, K. G. Egan, E. Cox, H. Young, K. E. Gannon et. al. 2011. Feeling bad on Facebook: Depression disclosure by college students on a social networking site.&nbsp;<em>Depression and Anxiety,</em>&nbsp;<em>28</em>(6), 447–455.</li>
        <li id="bib10" label="[10]">M. De Choudhury, M. Gamon, S. Counts, and E. Horvitz. 2013. Predicting depression via social media. In <em>Proceedings of the International AAAI Conference on Weblogs and Social Media</em> (ICWSM).</li>
        <li id="bib11" label="[11]">G. A. Coppersmith, C. T. Harman, and M. Dredze. 2014. Measuring post traumatic stress disorder in Twitter. In <em>Proceedings of the International AAAI Conference on Weblogs and Social Media</em> (ICWSM).</li>
        <li id="bib12" label="[12]">M. De Choudhury, S. Counts and E. Horvitz. 2013. Social media as a measurement tool of depression in populations. In <em>Proceedings of the 5th Annual ACM Web Science Conference</em> (WebSci '13). ACM, New York, NY, USA, 47-56. DOI=<a class="link-inline force-break" href="http://dx.doi.org/10.1145/2464464">http://dx.doi.org/10.1145/2464464</a>.
        </li>
        <li id="bib13" label="[13]">M. De Choudhury, S. Counts, and E. Horvitz. 2013. Predicting postpartum changes in emotion and behavior via social media. In <em>Proceedings of the ACM Annual Conference on Human Factors in Computing Systems</em> (CHI), 3267–3276.</li>
        <li id="bib14" label="[14]">G. Coppersmith, M. Dredze, C. Harman. 2014. Quantifying Mental Health Signals in Twitter. In: Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality. ACL; 2014. p. 51–60.</li>
        <li id="bib15" label="[15]">Y. R. Tausczik and J. W. Pennebaker, 2010. The psychological meaning of words: LIWC and computerized text analysis methods<em>. Journal of language and social psychology</em>, 29(1), 24-54.</li>
        <li id="bib16" label="[16]">P. J. Stone, D. C. Dunphy, M. S. Smith and D. M. Ogilvie. 1966. The general inquirer: A computer approach to content analysis. Cambridge, MA: The MIT Press.</li>
        <li id="bib17" label="[17]">M. Thelwall, K. Buckley G. Paltoglou, D. Cai and A. Kappas. 2010. Sentiment strength detection in short informal text. <em>Journal of the American Society for Information Science and Technology</em>, 61(12), 2544–2558.</li>
        <li id="bib18" label="[18]">H. Lin, J. Jia, J. J. Qiu, <em>et al.</em> 2017. Detecting stress based on social interactions in social networks. <em>IEEE Transactions on Knowledge and Data Engineering</em>.</li>
        <li id="bib19" label="[19]">H. Lin, J. Jia, J. Huang. <em>et al.</em> 2016. Moodee: An Intelligent Mobile Companion for Sensing Your Stress from Your Social Media Postings. Thirtieth AAAI Conference on Artificial Intelligence.</li>
        <li id="bib20" label="[20]">H. Lin, J. Jia, J. Huang. <em>et al.</em> 2014. Psychological stress detection from cross-media microblog data using deep sparse neural network. In <em>proceedings of IEEE International Conference on Multimedia &amp; Expo</em>.</li>
        <li id="bib21" label="[21]">H. Lin, J. Jia, J. Huang. <em>et al.</em> 2014. User-level psychological stress detection from social media using deep neural network. In <em>Proceedings of ACM Int. Conference on Multimedia</em>.</li>
        <li id="bib22" label="[22]">H. Lin, J. Jia, L. Nie, G. Shen, and T. S. Chua, 2016. What does social media say about your stress? In <em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,</em> 3775–3781.</li>
        <li id="bib23" label="[23]">R. Navigli. 2009. Word Sense Disambiguation: A survey. <em>ACM Computing Surveys,</em>&nbsp;41&nbsp;(2009), 1-69.</li>
        <li id="bib24" label="[24]">C. Sumanth and D. Inkpen. 2015. How much does word sense disambiguation help in sentiment analysis of micropost data? In <em>Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</em>, 115–121, Lisbon.</li>
        <li id="bib25" label="[25]">V. Rentoumi, G. Giannakopoulos, V. Karkaletsis, and G. Vouros. 2009. Sentiment analysis of figurative language using a word sense disambiguation approach. In&nbsp;<em>Proc. of the International Conference RANLP'09</em>, pages 370–375, Borovets, Bulgaria.</li>
        <li id="bib26" label="[26]">C. Hung, S. J. CHEN, 2013: Word sense disambiguation based sentiment lexicons for sentiment classification. Decision Support Systems, vol. 55, no. 3, pp. 685–697.</li>
        <li id="bib27" label="[27]">I. Hulpus, N. Prangnawarat, C. Hayes. 2015. Path-based semantic relatedness on linked data and its use to word and entity disambiguation,&nbsp;<em>Proc. 14th Int. Semantic Web Conf.</em>, pp. 442-457.</li>
        <li id="bib28" label="[28]">B. Razon and C. Cheng. 2011. Word Sense Disambiguation of Opinionated Words Using Extended Gloss Overlap<em>.</em> 2011. In <em>Proceedings of the 8<sup>th</sup> National Natural Language Processing Research Symposium</em>, 1-5, Manilla.</li>
        <li id="bib29" label="[29]">S. Banerjee and T. Peterson 2003. Extended Gloss Overlaps as a Measure of Semantic Relatedness<em>.</em> In <em>Proceedings of the 18<sup>th</sup> International Joint Conference on Artificial Intelligence,</em> 802-810, Mexico.</li>
        <li id="bib30" label="[30]">G. Z. Bautista, M. A. Garcia and R. J. Tan. 2010. VoxPop: Automated Opinion Detection and Classification with Data Clustering. Manila.</li>
        <li id="bib31" label="[31]">J. Pennington, R. Socher and C. D. Manning. 2014. Glove: Global Vectors for Word Representation. In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> (October 2014), 1532-1543.</li>
        <li id="bib32" label="[32]">T. Mikolov, K. Chen, G. Corrado and J. Dean. 2013. Efficient Estimation of Word Representation in Vector Space. <em>arXiv preprint arXiv:1301.3781</em>,1-12.</li>
        <li id="bib33" label="[33]">T. Mikolov, I. Sutskever, K. Chen, G. Corrado and J. Dean. 2013. Distributed Representations of Words, Advances in neural information processing systems, 3111-3119.</li>
        <li id="bib34" label="[34]">A. Trask, P. Michalak, and J. Liu. 2015. Sense2vec-a fast and accurate method for word sense disambiguation in neural word embeddings<em>. arXiv preprint arXiv:</em>1511.06388.</li>
        <li id="bib35" label="[35]">X. Chen, Z. Liu and M. Sun. 2014. A unified model for word sense representation and disambiguation. In&nbsp;<em>EMNLP</em>. 1025–1035.</li>
        <li id="bib36" label="[36]">F. Godin, B. Vandersmissen, W. De Neve and R. Van de Walle. 2015. Named entity recognition for Twitter microposts using distributed word representations. In&nbsp;<em>Proceedings of the Workshop on Noisy User-generated Text</em>, pages 146–153, Beijing, China, July 2015. Association for Computational Linguistics.</li>
        <li id="bib37" label="[37]">K. Krippendorff. 2004. Content analysis: An introduction to its methodology. Thousand Oaks, CA: Sage.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18 Companion, April 23–27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.<br />
      ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191627">https://doi.org/10.1145/3184558.3191627</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
