<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-layer Perceptron and Recurrent Convolutional Neural Network Model</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
<link rel="cite-as" href="https://doi.org/10.1145/3184558.3186334"/></head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186334'>https://doi.org/10.1145/3184558.3186334</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186334'>https://w3id.org/oa/10.1145/3184558.3186334</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-layer Perceptron and Recurrent Convolutional Neural Network Model</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Iqra</span> <span class="surName">Safder</span>, Department of Computer Science, Information Technology University, Pakistan, <a href="mailto:iqra.safder@itu.edu.pk">iqra.safder@itu.edu.pk</a>
        </div>
        <div class="author">
          <span class="givenName">Saeed-Ul</span> <span class="surName">Hassan</span>, Department of Computer Science, Information Technology University, Pakistan, <a href="mailto:saeed-ul-hassan@itu.edu.pk">saeed-ul-hassan@itu.edu.pk</a>
        </div>
        <div class="author">
          <span class="givenName">Naif Radi</span> <span class="surName">Aljohani</span>, Faculty of Information Systems, King Abdulaziz Universitym, Saudi Arabia, <a href="mailto:nraljohani@kau.edu.sa">nraljohani@kau.edu.sa</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186334" target="_blank">https://doi.org/10.1145/3184558.3186334</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Although, over the years, information retrieval systems have shown tremendous improvements in searching for relevant scientific literature, human cognition is still required to search for specific document elements in full text publications. For instance, pseudocodes pertaining to algorithms published in scientific publications cannot be correctly matched against user queries, hence the process requires human involvement. AlgorithmSeer, a state-of-the-art technique, claims to replace humans in this task, but one of the limitations of such an algorithm search engine is that the metadata is simply a textual description of each pseudocode, without any algorithm-specific information. Hence, the search is performed merely by matching the user query to the textual metadata and ranking the results using conventional textual similarity techniques. The ability to automatically identify algorithm-specific metadata such as precision, recall, or f-measure would be useful when searching for algorithms. In this article, we propose a set of algorithms to extract further information pertaining to the performance of each algorithm. Specifically, sentences in an article that convey information about the efficiency of the corresponding algorithm are identified and extracted using a recurrent convolutional neural network (RCNN). Furthermore, we propose improving the efficacy of the pseudocode detection task by using a multi-layer perceptron (MLP) classification trained with 15 features, which improves the classification performance of the state-of-the-art pseudocode detection methods used in AlgorithmSeer by 27%. Finally, we show the advantages of the AI-enabled search engine (based on RCNN and MLP models) over conventional text-retrieval models.</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Iqra Safder, Saeed-Ul Hassan and Naif Radi Aljohani. 2018. AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-layer Perceptron and Recurrent Convolutional Neural Network Model. In <em>Proceedings of The 2018 Web Conference Companion (WWW'18 Companion)</em>. ACM, New York, NY, USA, 11 pages. <a href="https://doi.org/10.1145/3184558.3186334" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186334</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec1">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> INTRODUCTION</h2>
        </div>
      </header>
      <p>For decades, academics in the field of computer science have proposed algorithmic solutions to solve computational problems by creating, analyzing, and applying automated techniques such as clustering, classification, decoding, hashing, sorting, machine learning, and so on. Interestingly, a vast variety of algorithms that were originally proposed for solving a particular problem in the field of computer science have later been used to provide an efficient solution for important problems in various other fields. For instance, in bioinformatics, the alignment of nucleotide or amino acid bio sequences is modelled by using the string-matching Greedy String Tilling algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib1">1</a>] originally proposed for plagiarism detection through text matching [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib2">2</a>]. Similarly, Burrows-Wheeler's sequence alignment algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib3">3</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib4">4</a>] is used extensively for DNA sequence alignment. Such an algorithm was originally proposed to undertake compression on the basis of matching repeated strings. Likewise, algorithms such as FP-growth, Prefix Span and Apriori, heavily employed in solving problems related to detecting money laundering [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib5">5</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib6">6</a>] and in telecommunications to provide better services to customers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib7">7</a>], were originally proposed for finding frequent patterns in large databases Hence, with the exponential growth of scholarly big data, the task of searching for an appropriate algorithm is non-trivial.</p>
      <p>Scholarly data consists of hundreds of thousands of research articles, many of which contain algorithms. According to Bhatia et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib8">8</a>], around 900 algorithms were published in major computer science research conferences during the years 2005 to 2009, and the number of algorithms in research articles is increasing each year. Such a phenomenon clearly shows that researchers are working actively on inventing new algorithms for emerging problems and the efficient improvement of existing deployed algorithms. There is the possibility that a new improved algorithm can improve the performance of existing systems. Therefore, software developers must keep themselves conversant with new algorithmic research horizons relating to their technologies and problems.</p>
      <p>In the past, quite a few significant models have been proposed to search for algorithms in scholarly documents [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib9">9</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib10">10</a>]. These models have used algorithm metadata, for instance the captions, number of lines in the pseudocode, font size and so on [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib12">12</a>], in traditional search engine techniques. However, such algorithm metadata does not contain any information specific to algorithmic features, such as time complexity or the related performance metrics of precision, recall, and so on. Thus, the search is performed simply by textual matching between a user query and algorithm metadata.</p>
      <p>Algorithmic techniques usually operate on a structured set of data, with various computational costs, to provide efficient solutions and better evaluation results for the problem in hand. An algorithm that has less computational cost yet gives improved evaluation results is typically considered to be efficient. So far, only human experience-based heuristics are used to make a decision on the selection of a certain algorithm for a given problem, since there are no standard systematic methods in place for an appropriate algorithm search, such as on the basis of computational cost or effectiveness in terms of an algorithm's precision or recall for a given problem.</p>
      <p>Automatic searching for algorithms and their effectiveness in scholarly big data is not a trivial task. This automatic modelling is not performed on plain document text, but on scholarly documents that contain heterogeneous document elements such as pseudocode, algorithmic procedures, tables, and images such as plots, graphs, flowcharts, and so on. These document elements are the entities, separate from the running text of the document, that are used to support and summarize the information that is written in the running text. While documenting experimental results in running text, authors add other document elements on top, either to present an argument or to summarize their discussion relating to an algorithm. Therefore, it is a challenging task to extract the evaluation results about a particular algorithm. Often, the results are summarized both in the form of supportive text and document elements. The running text that contains a discussion on the results or, more specifically, on the effectiveness of an algorithm has a context that helps the reader to understand the text. For instance, text relating to the effectiveness of an algorithm may go as follows: <em>“We have evaluated the LDA-SVD multi-document summarization algorithm by considering both cases of removing stop-words and not removing stop-words from the computed and the model summaries.</em> Table <a class="tbl" href="#tb2">2</a> <em>tabulates the ROUGE-1 recall values and its 95% confidence interval…”.</em></p>
      <p>Furthermore, to model this problem comprehensively, traditional techniques such as bag-of-words, Latent Dirichlet Allocation [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib13">13</a>], or mutual information fail to handle the semantics and word order of the text. To extract the text related to a given algorithm that conveys information about its efficiency, the text's semantics and word order are more significant, since these features are necessary to understand the context. High-order n-grams (5-gram, 6-gram, and so on) representation can be deployed, notwithstanding, to understand the context and semantics, but they suffer from a data sparsity problem that severely affects classification accuracy. In this article, we present the following three key contributions.</p>
      <p>First, an improved machine learning-based approach was designed to enhance the accuracy of the existing baseline state-of-the-art algorithm detection approach [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>] by using an MLP, a feed-forward artificial neural network model. Using the same dataset as our baseline model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>], a dataset of 258 manually annotated scholarly documents, originally selected from the CiteseerX repository, was used to validate the efficacy of the techniques deployed. Our model achieves an overall f-measure of 96.5% compared to the 75.95% reported by our baseline model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>].</p>
      <p>Furthermore, to tap into advancements in deep learning, an algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib14">14</a>] was deployed to create a sentence representation using both word embedding and a recurrent convolutional neural network (RCNN) for the detection of the portion of a document containing the discussion of an algorithm in terms of its effectiveness, for instance its precision, recall, and f-measure. This representation was fed into the neural network performing classification, allowing us to find accurately the ‘<em>evaluation results related text lines</em>’ in full-text documents. Finally, we conducted experiments on the same set of 258 manually annotated scholarly documents that was earlier used for the MLP-based algorithm detection model originally obtained from CiteseerX repository. After 100 training epochs, our model achieved 76.06% accuracy.</p>
      <p>The third contribution of this article is the prototype development of an AI-enabled search engine that implements the functionality of our proposed set of algorithms to extract further information pertaining to an algorithm's features. The accuracy of our AI-enabled search engine was compared to existing state-of-the-art models. Finally, the system was evaluated empirically to compare the effectiveness our AI-enabled search engine to conventional search engines to fetch relevant scholarly documents without the need for human cognition.</p>
    </section>
    <section id="sec2">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> LITERATURE REVIEW</h2>
        </div>
      </header>
      <p>Although an extensive literature on scholarly information extraction is available, we discuss only important works that are closely related to ours. In addition, we discuss recent debates that augment the role of cognitive computing and tap into rigorous syntactic rules that enable a wide range of applications, not only in cognitive informatics but in web search engines, word processing, and cognitive systems in particular [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib15">15</a>].</p>
      <section id="sec2Z1">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Brief Review of Cognitive Computing for Search Space</h3>
          </div>
        </header>
        <p>Our review suggests that the term “cognitive computing” is gathering popularity for systems (e.g. Watson by IBM) that intelligently process online information, beyond search, by deploying AI models to amplify existing tools to identify relevant search results from the wider scientific community. Specifically, search engines are the most common and some of the most important tools used by the general scientific community [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib16">16</a>]. Another class of cognitive-enabled computing systems tapping into the expansion of neurologically simulated computation has recently shown potential in processing non-textual information, such as videos and online images, across disciplines [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib17">17</a>]. Wang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib18">18</a>] argue that computing similarity – used in many search applications, such as information retrieval, semantic web, data clustering, and natural language processing – plays a central role in many cognitive capabilities.</p>
        <p>Further, they emphasize that cognitive-enabled models provide more highly relevant results to users’ queries than the simple TF-IDF-based (term frequency–inverted document frequency) models used in many search engines. More recently, Analytis et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib19">19</a>] deployed multi-attribute utility models as cognitive search engines, including: (i) a linear multi-attribute model; (ii) equal weighting of attributes; and (iii) a single-attribute heuristic. Their experiments on 12 real-world problems, based on formal decision-making theory, show approaches that predict that how decision makers will choose, on the basis of the presented rank order, as alternatives to the one used by commercial recommendation systems and search engines.</p>
      </section>
      <section id="sec2Z2">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Brief Review of Document Element Detection</h3>
          </div>
        </header>
        <p>The extraction of document elements such as pseudocode, algorithmic procedures, tables, figures, and plots from digital articles has been studied and explored extensively [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib12">12</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib13">13</a>] [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib20">20</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib22">22</a>]. Scientists often utilize such document elements in multiple ways, such as in summarizing results, describing step-by-step instructions, and illustrating ideas. Hence, the ability to detect and extract such document elements automatically would not only enable them to be indexed and searched, but also give rise to many data-mining applications that rely on these fact- and knowledge-concentrated document entities. While many of document element extraction methods work on text-based documents, optical character recognition-based techniques have been designed for the automatic extraction of document elements [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib23">23</a>-<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib24">24</a>]. Among these, FigureSeer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib25">25</a>] is prominent. This automatically extracts results from figures, using computer vision approaches, leading to the idea of the automatic extraction of results from articles. This extracted information is made available for search by efficient indexing.</p>
        <p>A specialized table search system on Chem<sub>x</sub>Seer, has been designed for the automatic extraction of tables and figures from scholarly articles in the field of chemistry [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib26">26</a>]. Another important search engine, AckSeer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib27">27</a>], has been proposed for indexing and searching acknowledgements in the CiteSeer<sup>x</sup> digital library. Note that, while both PlosOne and CiteSeer<sup>x</sup> digital libraries support a search functionality for tables and figures, none of these systems supports text summarization of document elements. To fill this gap, Bhatia and Mitra [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib8">8</a>] propose a method to generate a summary or textual description for a document element automatically by utilizing machine learning techniques to extract and re-order the relevant sentences in the article in which the corresponding document element appears. This summarization approach helps end users to understand the relevance of a document element to their information needs.</p>
        <p>More recently, AlgorithmSeer [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>], an algorithm search engine, was designed to undertake the automatic extraction, indexing and searching of algorithms. AlgorithmSeer has also implemented a document element summarization approach to extract algorithm textual metadata from running text. A synopsis is generated from algorithms’ metadata to make them searchable on the basis of a user query. AlgorithmSeer assumes that an algorithm can be represented in pseudocode, and utilizes a machine learning-based approach to locate and extract it in a textual-represented document. While their pseudocode detection approach is sufficiently accurate to be deployed in real systems, it can be improved. Hence, we have used this approach as a baseline to propose an improved machine learning technique by using MLP neural networks.</p>
      </section>
      <section id="sec2Z3">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Brief Review of Deep Neural Networks for Text Mining</h3>
          </div>
        </header>
        <p>Recently, the evolution in deep neural networks and representation learning has prompted new ways of solving the data sparsity problem and learning word representations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib28">28</a>]. Word embedding helps to measure words’ semantic relatedness by using the distance between two embedding vectors. The pre-trained word embeddings and deep neural networks have proved remarkable in many Natural Language Processing (NLP) and text classification tasks. To find the semantic relatedness of phrase and sentences in scholarly documents, RCNN-based models are the most significant for text classification [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib20">20</a>], paraphrase detection [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib28">28</a>], semantic role labelling [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib29">29</a>], and recursive neural tensor networks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib30">30</a>]. In this article, we deploy RCNN to extract the lines of text in which algorithms are discussed in terms of their effectiveness, such as their precision, recall, f-measure, and so on.</p>
      </section>
    </section>
    <section id="sec3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> METHOD</h2>
        </div>
      </header>
      <p>In this section, we describe our proposed approach. <a class="fig" href="#fig1">Fig. 1</a> shows a high level of detail of our work to extract semantic metadata. Our approach has two sub-modules. The first presents the advancement that we have made to the existing state-of-the-art algorithm (pseudocode, or PC) detection approach [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>], referred to as the <em>baseline PC</em>_<em>ML</em>. We trained the neural network-based model on 15 features extracted from the document. Our improved PC_ML architecture has better accuracy and precision than existing algorithms. The second module, that is, the evaluation metric detection (EMD), extracts lines of text containing a discussion of algorithms and/or experiments based on evaluation metrics. It employs a document segmentation approach [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib33">33</a>] to extract relevant sections and then to identify the target lines by employing RCNN that has been trained on our manually tagged dataset.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186334/images/image1.jpg" class="img-responsive" alt="Figure 1:" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">High-level diagram of proposed system.</span>
        </div>
      </figure>
      <section id="sec3Z1">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Pseudocode Detection in Scholarly Articles</h3>
          </div>
        </header>
        <p>This section discusses our improved method for automatic detection of pseudocode in scholarly articles. <a class="fig" href="#fig2">Fig. 2</a> shows the detail of our improved pseudocode-detection approach. Our technique handles PDF documents, since a large subset of scholarly articles in digital libraries is in PDF format. First, we extracted the plain text from the PDF document by using PDFbox library (<a class="link-inline force-break" href="https://pdfbox.apache.org/">https://pdfbox.apache.org/</a>). We also extracted the object and location information of the fonts, using the work of Hassan [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib31">31</a>] and Tiedemann [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib32">32</a>]. Next, a feature vector was designed, using a set of handcrafted features (see Table <a class="tbl" href="#tb1">1</a>). Lastly, for classification purposes, an MLP was implemented to classify whether or not either a line of text is a pseudocode line.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186334/images/image2.jpg" class="img-responsive" alt="Figure 2:" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Improved PC_ML algorithm.</span>
          </div>
        </figure>
        <div class="table-responsive" id="tb1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">Feature set for pseudocode (PC) classification, categorized into: content-based (CN); structure-based (ST); font style-based (FS); and context-based (CX).</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;"><strong>Feature</strong></th>
                <th style="text-align:left;"><strong>Description</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;" rowspan="4"><br />
                CN</td>
                <td style="text-align:left;">PC keywords %</td>
                <td style="text-align:left;">Ratio of PC words and no. of words</td>
              </tr>
              <tr>
                <td style="text-align:left;">PC symbols %</td>
                <td style="text-align:left;">Ratio of PC symbols and no. of chars in line</td>
              </tr>
              <tr>
                <td style="text-align:left;">Word sparsity</td>
                <td style="text-align:left;">Ratio of no. of words and avg. no. of words in a line</td>
              </tr>
              <tr>
                <td style="text-align:left;">Char. sparsity</td>
                <td style="text-align:left;">Fraction of no. of chars in a line and avg. no. of chars</td>
              </tr>
              <tr>
                <td style="text-align:left;" rowspan="4"><br />
                ST</td>
                <td style="text-align:left;">Greek chars %</td>
                <td style="text-align:left;">Ratio of Greek symbols and no. of chars in line</td>
              </tr>
              <tr>
                <td style="text-align:left;">Comment sign %</td>
                <td style="text-align:left;">Ratio of comment signs and no. of lines</td>
              </tr>
              <tr>
                <td style="text-align:left;">Functions %</td>
                <td style="text-align:left;">Ratio of no. of functions and no. of lines</td>
              </tr>
              <tr>
                <td style="text-align:left;">Begins with a no.</td>
                <td style="text-align:left;">Whether or not line begins with a no.</td>
              </tr>
              <tr>
                <td style="text-align:left;" rowspan="6"><br />
                <br />
                <br />
                FS</td>
                <td style="text-align:left;">Mode font size</td>
                <td style="text-align:left;">Mode font size of text</td>
              </tr>
              <tr>
                <td style="text-align:left;">Variance font size</td>
                <td style="text-align:left;">Variance font size of text</td>
              </tr>
              <tr>
                <td style="text-align:left;">Font styles %</td>
                <td style="text-align:left;">No. of font styles (combinations of font style and font name)</td>
              </tr>
              <tr>
                <td style="text-align:left;">Font-style switches</td>
                <td style="text-align:left;">No. of font-style switches</td>
              </tr>
              <tr>
                <td style="text-align:left;">Indentation</td>
                <td style="text-align:left;">Whether or not the line is indented</td>
              </tr>
              <tr>
                <td style="text-align:left;">Avg. indentation of first 4 characters</td>
                <td style="text-align:left;">Avg. indentation of first 4 chars</td>
              </tr>
              <tr>
                <td style="text-align:left;">CX</td>
                <td style="text-align:left;">Is it a caption</td>
                <td style="text-align:left;">Whether or not the text line is a caption line</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Our proposed improved PC_ML method extends the baseline to improve overall accuracy and f-measure. Since machine learning-based method does not rely on the rule-based caption detection method [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib8">8</a>], it directly detects the presence of pseudocode in articles. Normally, pseudocode is written in a spare manner that creates a sparse region in a document. Such sparse regions are called <em>sparse boxes.</em> The proposed improved PC_ML method first detects and extracts the pseudocode boxes by identifying the pseudocode lines. Next, a feature set is extracted and finally, a neural network-based classification model is applied. The following subsections contain the details of our improved PC_ML method.</p>
        <p><em>3.1.1 Sparse Box Extraction.</em> A sparse box is a set of N consecutive sparse lines. A sparse line is one that fulfills the following rules: a) the ratio of non-space characters to average number of characters per line must be less than the threshold (0.8); b) no headers or footers; and c) encapsulated by a sparse line. This method works well with a threshold of 0.8 and N=4. Also, the method shows a high coverage of 92.99%, as per the baseline PC_ML method.</p>
        <p><em>3.1.2 Feature Set Selection for Pseudocode Box Classification</em>. A set of 15 features in four categories are extracted for each line of the sparse box. Table <a class="tbl" href="#tb1">1</a> shows the extracted features and their description: context-based (CX); content-based (CN); style-based (ST); and font style-based (FS). The CX feature captures the presence of pseudocode captions, while CN extracts the features relating to the presence of pseudocode by capturing the coding styles and pseudocode-specific keywords. ST-based features capture the sparsity of pseudocode boxes and representative symbols.</p>
        <p><em>3.1.3 Neural Network-based Classification Model.</em> Each pseudocode sparse box is classified either as a pseudocode line or not by a neural network trained on the hand-tagged data. Our neural network consists of single hidden layer; the output layer consists of single neuron with sigmoid as an activation function. The input layer is of size 15 and takes the features described in Table <a class="tbl" href="#tb1">1</a> as input. The MLP classification model is trained on 70% of actual data and was tested on 30% data.</p>
      </section>
      <section id="sec3Z2">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Target Text Lines Detection</h3>
          </div>
        </header>
        <p>This section contains details of our deployed deep learning-based algorithm that detects the target text lines relating to discussion of the evaluation of algorithms in scholarly articles. For simplicity, we refer to this method as evaluation metrics detection, or EMD. Using RCNN, the model can capture the semantics of text to classify whether or not a line is a target line. The EMD takes as input related sections of research article the sequence of words w<sub>1,</sub> w<sub>2</sub>, w<sub>3</sub>…w<sub>n</sub> and outputs the class of the text. The probability function p(k|D, <span class="inline-equation"><span class="tex">$\theta $</span></span> ) function is used to find the probability of text line belonging to a class containing target lines. The following pre-processing steps were taken:</p>
        <p><em>3.2.1 Hierarchical Section Extractor.</em> Generally, articles are organized into sections. To extract evaluation result lines from research articles, we need to segment an article into its standard sections (i.e. abstract, introduction, background, related work, and so on). The purpose of segmentation is to keep only those sections that have high chance of containing results-related discussion. We used a rule-based method to segment out the standard sections of an article; this technique helped us to keep only the related sections (i.e. methodology, results, experiments, abstract, etc.) and discard unrelated sections in which the chance of target lines is minimal or close to none (i.e. introduction, related work, references, acknowledgements, etc.). Afterwards, text cleaning was performed to remove header/footers, the article title, author affiliations, and so on. Lastly, the text of the cleaned and related sections was given as input to the RCNN model.</p>
        <p><em>3.2.2 Deep Learning-based Neural Network Model</em>. <a class="fig" href="#fig3">Fig. 3</a> shows the RCNN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib14">14</a>] structure. The model takes the words and the context as a representation of a word. We used a bidirectional RCNN to capture the context of words. For the left and right context of word wi, Vectors cr(wi) and cl(wi) are defined, cl(wi), computed using <a class="eqn" href="#eqn1">Eq. 1</a>. Here, W(l) is a matrix that is used to transform context between the hidden layers. W(sl) is also a matrix, and is used to combine the left word's context with the current word. While e(wi-1) is the word-embedding vector of word wi-1, with real value elements, cl(wi-1) is the left-side context of the previous word wi-1. Similarly, cr(wi ) is calculated in same manner, as shown in <a class="eqn" href="#eqn2">Eq. 2</a>.</p>
        <div class="table-responsive" id="eqn1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}\;\;\;\;\;\;{c_{l\;\;}}\left( {{w_i}} \right) = \;f\left( {\left( {{W^{\left( l \right)}}} \right){c_l}\left( {{w_{i - 1}}} \right) + \left( {{W^{\left( {sl} \right)}}} \right)e\left( {{w_{i - 1}}} \right)\;} \right)\end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <div class="table-responsive" id="eqn2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}\;\;\;\;\;\;\;{c_{r\;}}\left( {{w_i}} \right) = \;f\left( {\left( {{W^{\left( r \right)}}} \right){c_r}\left( {{w_{i - 1}}} \right) + \left( {{W^{\left( {sr} \right)}}} \right)e\left( {{w_{i - 1}}} \right)} \right)\;\end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <p></p>
        <figure id="fig3">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186334/images/image3.jpg" class="img-responsive" alt="Figure 3:" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span> <span class="figure-title">RCNN network structure with subscripts to denote the position of a word in a sentence.</span>
          </div>
        </figure>
        <p><a class="eqn" href="#eqn1">Eq. 1</a> and <a class="eqn" href="#eqn2">Eq. 2</a> are context vectors for left and right words. Referring to <a class="fig" href="#fig3">Fig. 3</a>, <em>c<sub>l</sub>(w<sub>7</sub>)</em> contains the context and semantics of all left-side words in the form of encodings. The left context of the network is “SVD algorithm shows high” with all previous words of sentence. Similarly, <em>c<sub>r</sub>(w<sub>7</sub>)</em> contains the right context. Next, the word <em>w<sub>i</sub></em> representation is learned by combining left and right contexts, as shown in <a class="eqn" href="#eqn3">Eq. 3</a>:</p>
        <div class="table-responsive" id="eqn3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} {x_i}\; = \;\left[ {{c_l}\left( {{w_i}} \right);e\left( {{w_i}} \right);{c_r}\left( {{w_i}} \right)} \right]\end{equation}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <p></p>
        <p>The model obtains the all c<sub>l</sub> and c<sub>r</sub> in forward and backward passes, respectively. After learning the <em>x<sub>i</sub></em> representation of a word, a linear transformation was applied, with a <em>tanh</em> activation function to add some nonlinearity:</p>
        <div class="table-responsive" id="eqn4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}y_i^{\left( 2 \right)} = \;tanh\left( {{W^{\left( 2 \right)}}{x_i} + \;{b^{\left( 2 \right)}}} \right)\end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>
        <p></p>
        <p>The <span class="inline-equation"><span class="tex">$y_i^{( 2 )}$</span></span> is a latent semantic vector that contains the most important and powerful factors for the text representation by analysing each and every semantic factor. The model also contains a max-pooling layer, as mentioned in the network architecture (see <a class="fig" href="#fig3">Fig. 3</a>). The pooling layer converts the different length text to a fixed length vector. It helps to find the most important information through the entire text. Therefore, after learning the all words’ representation, we applied a max-pooling layer:</p>
        <div class="table-responsive" id="eqn5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} {y^{\left( 3 \right)}} = \;max_{i = 2}^n\;y_{\left( i \right)}^{\left( 2 \right)}{\;^{\;\;\;}}\end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p></p>
        <p>The max function is applied element wise, and the maximum of the <em>k-th</em> element of <span class="inline-equation"><span class="tex">$y_{( i )}^{( 2 )}$</span></span> is in the <em>k-th</em> position of y<sup>(3)</sup>. Finally, the model contains a single fully connected hidden layer as an output layer, as shown in <a class="eqn" href="#eqn6">Eq. 6</a>:</p>
        <div class="table-responsive" id="eqn6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} {y^{\left( 4 \right)}} = {W^{\left( 4 \right)}}{y^{\left( 3 \right)}}\end{equation} ${b^{\left( 4 \right)}}\;$</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <p></p>
        <p>Finally, a sigmoid activation function was applied to <span class="inline-equation"><span class="tex">${y^{( 4 )}}\;$</span></span> to give us the probability number, as shown in <a class="eqn" href="#eqn7">Eq. 7</a>:</p>
        <div class="table-responsive" id="eqn7">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} {p_{\left( i \right)}} = \frac{{exp\left( {y_k^{\left( 4 \right)}} \right)}}{{1 + ex{p^{\left( {y_k^{\left( 4 \right)}} \right)}}}}\;\end{equation}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>
        <p></p>
        <p><em>Training of RCNN Model:</em> First, we defined all the model parameters for training <span class="inline-equation"><span class="tex">$\theta $</span></span> , as shown in <a class="eqn" href="#eqn8">Eq. 8</a>:</p>
        <div class="table-responsive" id="eqn8">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}\theta = \left\{ {E,\;{b^{\left( 2 \right)}},\;{b^{\left( 4 \right)}},\;{c_l}\left( {{w_1}} \right),\;{c_r}\left( {{w_n}} \right),\;{W^{\left( 2 \right)}},{W^{\left( 4 \right)}},\;\;{W^{\left( l \right)}},{W^{\left( r \right)}},\;{W^{\left( {sl} \right)}},{W^{\left( {sr} \right)}}} \right\}\end{equation}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>
        <p></p>
        <p>Here, the vectors <span class="inline-equation"><span class="tex">${b^{( 2 )}},\;{b^{( 4 )}}$</span></span> are real-valued vectors and E is real-valued word embeddings, whereas <span class="inline-equation"><span class="tex">${W^{( 2 )}},{W^{( 4 )}},{W^{( l )}},\;\;{W^{( r )}},$</span></span> and <span class="inline-equation"><span class="tex">${W^{( {sl} )}},{W^{( {sr} )}}$</span></span> are transformation matrices and<span class="inline-equation"><span class="tex">$\;{c_l}( {{w_1}} ),{c_r}( {{w_n}} )$</span></span> are initial left and right context real-valued vectors. The target was to maximize the probability (log likelihood) with respect to<span class="inline-equation"><span class="tex">$\;\theta .$</span></span></p>
        <div class="table-responsive" id="eqn9">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}\theta \to \mathop \sum \limits_{c \in D} log\;p(clas{s_c}|\;D,\theta \;)\end{equation}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <p>In <a class="eqn" href="#eqn9">Eq. 9</a>, D is the set of documents and <span class="inline-equation"><span class="tex">$clas{s_c}$</span></span> is the positive class of text data. We used the stochastic gradient descent (SGD) for training optimization.</p>
        <p>Further, we employed Skip-gram model-based word embeddings (commonly used in NLP tasks [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib33">33</a>]) for word representation. Since our dataset suffers from a class imbalance problem, to avoid bias in our model we included the following balancing techniques: a) <em>Random Over-sampling (ROver)</em>, in which minority class examples are randomly replicated until both classes become equal; and b) <em>Random Under-sampling (RUnder)</em>, in which majority class examples are randomly dropped until both classes become equal. Note that the same 70% data was used for training and 30% for testing, as for the PC_ML model. However, after applying the balancing techniques, we had 4337 positive samples; that is, the target lines and 4770 negative instances, namely text lines other than target text lines, containing no information about the efficiency of the corresponding algorithm. Finally, we adjusted the network hyper parameter settings, such as H (hidden layer size) to 100, learning rate to 0.001, V (vocabulary size) to 3000, and training epochs to 100.</p>
      </section>
      <section id="sec3Z3">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> AI-enabled Search Engine Using Improved PC_ML and EMD Models</h3>
          </div>
        </header>
        <p>Our AI-enabled search engine has the following steps: i) synopsis generation using improved PC_ML and EMD models for each document in the system; ii) standard indexing for both the dataset, that is, the synopsis, and full text documents; and iii) finally, deployment of a state-of-the-art searching model to rank the results against user queries for a comparative analysis of a simple full text-based corpus and a synopsis-based corpus.</p>
      </section>
    </section>
    <section id="sec4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> EXPERIMENTATION AND EVALUATION</h2>
        </div>
      </header>
      <p>The experiments comprised two modules: pseudocode detection using MLP; and target text line detection (conveying information about the efficiency of the corresponding algorithm) using RCNN. The experiments were performed on Ubuntu, Nvidia Titan 750 GPU with 2 GB memory. We used the Python Chainer Library (<a class="link-inline force-break" href="https://chainer.org/">https://chainer.org/</a>) for RCNN implementation and Weka (<a class="link-inline force-break" href="https:/cs.waikato.ac.nz/">https:/cs.waikato.ac.nz/</a>) for MLP implementation.</p>
      <p>The dataset consists of 258 scholarly articles selected from the CiteSeer<sup>x</sup> repository [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib11">11</a>]. Note that our baseline model PC_ML used the same dataset, making our improved PC_ML comparable with the baseline. This dataset consists of 275 pseudocodes and 282 unique algorithms. For target text line detection, we used same dataset of 258 scholarly articles. Note that, of the total, that is, 37,000 text lines in our dataset, only 6.3% contain target text that conveys information about the efficiency of the corresponding algorithm. Note that the annotation was undertaken by four human experts, who identified 2331 text lines as target lines.</p>
      <section id="sec4Z1">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Evaluation of Improved PC_ML Detection</h3>
          </div>
        </header>
        <p>Standard precision, recall, f-measure and accuracy indices are used for the evaluation of pseudocode detection and target text line detection.</p>
        <p>Table <a class="tbl" href="#tb2">2</a> shows the comparison of our baseline PC_ML and improved PC_ML for pseudocode detection. We found that the improved PC_ML (using MLP) outperformed PC_ML (baseline) across all evaluation metrics. We achieved a 98.8% f-measure with the baseline improved PC_ML model, compared to only 75.95% with our PC_ML model. This is a 27% improvement on the baseline. Similarly, we achieved a significant improvement in terms of recall (from 67.17% to 96.7%), precision from (87.37% to 97.4%), and overall f1 measure (from 75.95% to 96.5%), respectively.</p>
        <div class="table-responsive" id="tb2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Precision, recall, and f-measure for pseudocode-detection model.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"><strong>Method</strong></th>
                <th style="text-align:left;"><strong>Model</strong></th>
                <th style="text-align:left;"><strong>Re%</strong></th>
                <th style="text-align:left;"><strong>Pr%</strong></th>
                <th style="text-align:left;"><strong>F1%</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">PC_ML</td>
                <td style="text-align:left;">Baseline</td>
                <td style="text-align:left;">67.17</td>
                <td style="text-align:left;">87.37</td>
                <td style="text-align:left;">75.95</td>
              </tr>
              <tr>
                <td style="text-align:left;">PC_ML (improved)</td>
                <td style="text-align:left;">MLP</td>
                <td style="text-align:left;">96.7</td>
                <td style="text-align:left;">97.4</td>
                <td style="text-align:left;">96.5</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec4Z2">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Evaluation of the EMD Model</h3>
          </div>
        </header>
        <p>For the evaluation of EMD model on our dataset, we used the following experimental settings. The network hyper parameters were assigned as follows: hidden layer size (H) 1000; learning rate 0.01; vocabulary size (V) 3000; and training epochs 100. The training calculating the metrics for every class and taking the average accuracy of our EMD method for 100 epochs was to depict the behaviour of our model during training. In the first 20 epochs, the model started learning very quickly. Afterwards, it showed a gradual increase in accuracy and reach, up to 76.06% (see <a class="fig" href="#fig5">Fig. 5</a>). Next, we presented our testing results in terms of precision, recall, and f-measure. The RCNN-based EMD model achieves 0.77 precision, 0.73 recall, and 0.75 f-measure, which is a very reasonable way to detect target lines in this type of database. Note that we reported the weighted precision, recall, and f-measure scores by calculating the metrics for every class and taking the average.</p>
      </section>
    </section>
    <section id="sec5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> PROTOTYPE OF AI-ENABLED SEARCH ENGINE</h2>
        </div>
      </header>
      <p>In this section, a prototype of an AI-enabled search engine is presented. <a class="fig" href="#fig4">Fig. 4</a> illustrates the high-level architecture of our proposed system. The corpus of 30% testing data, that is, 74 scholarly articles – containing 82 pseudocodes and 95 unique algorithms – is used to evaluate empirically the AI-enabled search engine system results against a conventional search engine.</p>
      <figure id="fig4">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186334/images/image4.jpg" class="img-responsive" alt="Figure 4:" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class="figure-title">System architecture of proposed AI-enabled search engine.</span>
        </div>
      </figure>
      <p>First, we enriched synopses generated from documents by embedding target text lines, as detected by the EMD model. Next, these enriched synopses were tokenized and indexed to make them searchable. User queries were employed to obtain a ranked list of results. The top-matched algorithms, along with their synopses, were presented to the end user. Lastly, algorithms detected by PC_ML were embedded in the query search results to make these more comprehensive and elaborative. Using Okapi BM25 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#bib34">34</a>], similarity scores were computed to rank the query search results for both search engines. In order to evaluate empirically the performance of our proposed AI-enabled search engine against a state-of-the-art conventional search engine, we selected a set of queries and relevant documents that were identified by two independent human annotators.</p>
      <p><a class="fig" href="#fig5">Fig. 5</a> shows the top 15 documents for the search query <em>“Deterministic Majority Voting Algorithm with High Performance and Less Complexity”,</em> using a conventional search engine. Note that the deterministic majority voting algorithm is used for N-modular redundancy. A detailed analysis revealed that the top 10 documents on the list present a discussion of this algorithm along with a comparative analysis of different algorithmic techniques. We also found a few documents that discuss the voting techniques used in classification-related problems and have nothing to do with the deterministic majority voting algorithm. Interestingly, the document that originally presented the algorithm and provided a detailed discussion on its performance evaluation appears in 12<sup>th</sup> position.</p>
      <figure id="fig5">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186334/images/image5.jpg" class="img-responsive" alt="Figure 5:" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 5:</span> <span class="figure-title">Screenshot of search results from conventional search engine.</span>
        </div>
      </figure>
      <p>In contrast, the search results from our AI-enabled search engine appear to be very different. <a class="fig" href="#fig6">Fig. 6</a> shows the top 15 documents for the same query. A detailed analysis of these documents showed that the top five documents present either pseudocode or an algorithmic procedure. Interestingly, our desired document, which appeared 12<sup>th</sup> in the conventional system, now appeared first. Further, PC_ML embedding allows the user to click on the respective “Show Pseudo-Code” link to actually see the section of the article where the pseudocode(s) have been placed in the full-text document. Overall, this offers a great user experience and enables users to search for the relevant information quickly. Since the indexing and searching techniques are implemented to undertake a text-based document retrieval task, searching for algorithms is a non-trivial problem that demands specialized knowledge to narrow down to the specific required algorithm. Therefore, the main purpose of this section is to demonstrate a basic prototype search system that utilizes algorithmic specific metadata, including performance-related features such as precision, recall, f-measure, and so on, in order to improve the relevance of the search results.</p>
      <figure id="fig6">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186334/images/image6.jpg" class="img-responsive" alt="Figure 6:" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 6:</span> <span class="figure-title">Screenshot of search results from AI-enabled search engine.</span>
        </div>
      </figure>
    </section>
    <section id="sec6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> CONCLUDING REMARKS</h2>
        </div>
      </header>
      <p>Computer science is all about developing and applying appropriate algorithms to computational problems. Recently, a search engine for algorithms that are represented as pseudocode has been proposed and prototyped. However, such an algorithm search engine matches only the user query to the extracted generic textual metadata of each pseudocode. In this article, we proposed solutions to enhance the performance of the pseudocode-detection task by training a multi-layer perceptron classifier using 15 features that specifically characterize the composition of pseudocode in scholarly documents. The proposed algorithm improves on the state-of-the-art pseudocode-detection method by 27%. Furthermore, we deployed state-of-the-art word embedding and an RCNN model to discover and retrieve sentences from a document to convey the information about the efficiency (such as precision, recall, and f-measure) of the corresponding algorithm. Finally, using a prototype of our AI-enabled search engine, we showed that our deployed models can improve search results. As future work, we plan to overcome some of the limitations of this work, such as the deployed technique using the same embedding vector for numeric figures and English-language text.</p>
      <p>Therefore, for the following text pertaining to algorithmic feature: <em>“the given method outperforms base model by 20% with precision 81.5 and recall 81.5....”</em>, we could employ natural language processing and machine learning techniques to extract a numeric representation of algorithmic performance. This information will enable a direct comparison to other algorithms. Furthermore, we could investigate the possibility of extracting other algorithm-specific metadata, such as run-time complexity, input, output, and compatible data structures, in a large-scale data corpus.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="bib-sec-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">References</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="bib1" label="[1]">M. J. Wise, “Neweyes: a system for comparing biological sequences using the running Karp-Rabin Greedy String-Tiling algorithm.,” in <em>ISMB</em>, 1995, pp. 393–401.</li>
        <li id="bib2" label="[2]">Z. DJurić and D. Gašević, “A source code similarity system for plagiarism detection,” <em>Comput. J.</em>, vol. 56, no. 1, pp. 70–86, 2013.</li>
        <li id="bib3" label="[3]">H. Li, “Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM,” <em>ArXiv Prepr. ArXiv13033997</em>, 2013.</li>
        <li id="bib4" label="[4]">P. J. Ochieng, T. Djatna, and W. A. Kusuma, “Tandem repeats analysis in DNA sequences based on improved Burrows-Wheeler transform,” in <em>Advanced Computer Science and Information Systems (ICACSIS), 2015 International Conference on</em>, 2015, pp. 117–122.</li>
        <li id="bib5" label="[5]">Z. Chen, A. Nazir, E. N. Teoh, E. K. Karupiah, and others, “Exploration of the effectiveness of expectation maximization algorithm for suspicious transaction detection in anti-money laundering,” in <em>Open Systems (ICOS), 2014 IEEE Conference on</em>, 2014, pp. 145–149.</li>
        <li id="bib6" label="[6]">R. Dreżewski, G. Dziuban, Lukasz Hernik, and M. Pączek, “Comparison of data mining techniques for Money Laundering Detection System,” in <em>Science in Information Technology (ICSITech), 2015 International Conference on</em>, 2015, pp. 5–10.</li>
        <li id="bib7" label="[7]">W. Verbeke, K. Dejaeger, D. Martens, J. Hur, and B. Baesens, “New insights into churn prediction in the telecommunication sector: A profit driven data mining approach,” <em>Eur. J. Oper. Res.</em>, vol. 218, no. 1, pp. 211–229, 2012.</li>
        <li id="bib8" label="[8]">S. Bhatia and P. Mitra, “Summarizing figures, tables, and algorithms in scientific publications to augment search results,” <em>ACM Trans. Inf. Syst. TOIS</em>, vol. 30, no. 1, p. 3, 2012.</li>
        <li id="bib9" label="[9]">S. Bajracharya <em>et al.</em>, “Sourcerer: a search engine for open source code supporting structure-based search,” in <em>Companion to the 21st ACM SIGPLAN symposium on Object-oriented programming systems, languages, and applications</em>, 2006, pp. 681–682.</li>
        <li id="bib10" label="[10]">C. McMillan, M. Grechanik, D. Poshyvanyk, C. Fu, and Q. Xie, “Exemplar: A source code search engine for finding highly relevant applications,” <em>IEEE Trans. Softw. Eng.</em>, vol. 38, no. 5, pp. 1069–1087, 2012.</li>
        <li id="bib11" label="[11]">S. Tuarob, S. Bhatia, P. Mitra, and C. L. Giles, “AlgorithmSeer: A system for extracting and searching for algorithms in scholarly big data,” <em>IEEE Trans. Big Data</em>, vol. 2, no. 1, pp. 3–17, 2016.</li>
        <li id="bib12" label="[12]">S. Tuarob, S. Bhatia, P. Mitra, and C. L. Giles, “Automatic detection of pseudocodes in scholarly documents using machine learning,” in <em>Document Analysis and Recognition (ICDAR), 2013 12th International Conference on</em>, 2013, pp. 738–742.</li>
        <li id="bib13" label="[13]">S. Hingmire, S. Chougule, G. K. Palshikar, and S. Chakraborti, “Document classification by topic labeling,” in <em>Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</em>, 2013, pp. 877–880.</li>
        <li id="bib14" label="[14]">Safder, I; Sarfraz, J; Hassan, SU; Ali, M; Tuarob, S; Detecting Target Text related to Algorithmic Efficiency in Scholarly Big Data using Recurrent Convolutional Neural Network Model, 19th International Conference on Asia-Pacific Digital Libraries (ICADL), Bangkok, Thailand.</li>
        <li id="bib15" label="[15]">Wang, Yingxu, and Robert C. Berwick. "Formal relational rules of english syntax for cognitive linguistics, machine learning, and cognitive computing. "<em>Journal of Advanced Mathematics and Applications</em> 2, no. 2 (2013): 182-195.</li>
        <li id="bib16" label="[16]">Gil, Yolanda, <em>et al.</em> "Amplify scientific discovery with artificial intelligence." <em>Science</em> 346.6206 (2014): 171-172.</li>
        <li id="bib17" label="[17]">M. Martialay, “Citizen Scientist,” The Approach; <a class="link-inline force-break" href="http://approach.rpi.edu/2014/04/25/citizen-scientist-your-safari-photos-are-the-data/">http://approach.rpi.edu/2014/04/25/citizen-scientist-your-safari-photos-are-the-data/</a>.
        </li>
        <li id="bib18" label="[18]">Wang, Y., Rolls, E.T., Howard, N., Raskin, V., Kinsner, W., Murtagh, F., Bhavsar, V.C., Patel, S., Patel, D. and Shell, D.F., 2015. Cognitive Informatics and Computational Intelligence: From Information Revolution to Intelligence Revolution. <em>International Journal of Software Science and Computational Intelligence (IJSSCI)</em>, <em>7</em>(2), pp.50-69.</li>
        <li id="bib19" label="[19]">Analytis, P.P., Kothiyal, A. and Katsikopoulos, K.V., 2014. Multi-attribute utility models as cognitive search engines. <em>Judgment and Decision Making</em>, <em>9</em>(5), pp.403-419.</li>
        <li id="bib20" label="[20]">S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent Convolutional Neural Networks for Text Classification.,” in AAAI, 2015, vol. 333, pp. 2267–2273.</li>
        <li id="bib21" label="[21]">T. Mikolov, W. Yih, and G. Zweig, “Linguistic Regularities in Continuous Space Word Representations.,” in <em>Hlt-naacl</em>, 2013, vol. 13, pp. 746–751.</li>
        <li id="bib22" label="[22]">B. Coüasnon and A. Lemaitre, “Recognition of Tables and Forms,” in <em>Handbook of Document Image Processing and Recognition</em>, Springer, 2014, pp. 647–677.</li>
        <li id="bib23" label="[23]">S. Z. Chen, M. J. Cafarella, and E. Adar, “Searching for statistical diagrams,” <em>Front. Eng. Natl. Acad. Eng.</em>, pp. 69–78, 2011.</li>
        <li id="bib24" label="[24]">S. Kataria, W. Browuer, P. Mitra, and C. L. Giles, “Automatic Extraction of Data Points and Text Blocks from 2-Dimensional Plots in Digital Documents.,” in <em>AAAI</em>, 2008, vol. 8, pp. 1169–1174.</li>
        <li id="bib25" label="[25]">N. Siegel, Z. Horvitz, R. Levin, S. Divvala, and A. Farhadi, “FigureSeer: Parsing Result-Figures in Research Papers,” in <em>European Conference on Computer Vision</em>, 2016, pp. 664–680.</li>
        <li id="bib26" label="[26]">P. Mitra, C. L. Giles, B. Sun, Y. Liu, and A. R. Jaiswal, “Scientific Data and Document Processing in ChemxSeer.,” in <em>AAAI Spring Symposium: Semantic Scientific Knowledge Integration</em>, 2008, pp. 51–56.</li>
        <li id="bib27" label="[27]">M. Khabsa, P. Treeratpituk, and C. L. Giles, “Ackseer: a repository and search engine for automatically extracted acknowledgments from digital libraries,” in <em>Proceedings of the 12th ACM/IEEE-CS joint conference on Digital Libraries</em>, 2012, pp. 185–194.</li>
        <li id="bib28" label="[28]">R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and C. D. Manning, “Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.,” in <em>NIPS</em>, 2011, vol. 24, pp. 801–809.</li>
        <li id="bib29" label="[29]">R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” <em>J. Mach. Learn. Res.</em>, vol. 12, no. Aug, pp. 2493–2537, 2011.</li>
        <li id="bib30" label="[30]">R. Socher <em>et al.</em>, “Recursive deep models for semantic compositionality over a sentiment treebank,” in <em>Proceedings of the conference on empirical methods in natural language processing (EMNLP)</em>, 2013, vol. 1631, p. 1642.</li>
        <li id="bib31" label="[31]">T. Hassan, “Object-level document analysis of PDF files,” in <em>Proceedings of the 9th ACM symposium on Document engineering</em>, 2009, pp. 47–55.</li>
        <li id="bib32" label="[32]">J. Tiedemann, “Improved text extraction from PDF documents for large-scale natural language processing,” in <em>International Conference on Intelligent Text Processing and Computational Linguistics</em>, 2014, pp. 102–112.</li>
        <li id="bib33" label="[33]">S. Tuarob, P. Mitra, and C. L. Giles, “A hybrid approach to discover semantic hierarchical sections in scholarly documents,” in <em>Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</em>, 2015, pp. 1081–1085.</li>
        <li id="bib34" label="[34]">M. Baroni, G. Dinu, and G. Kruszewski, “Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.,” in <em>ACL (1)</em>, 2014, pp. 238–247.</li>
        <li id="bib35" label="[35]">Robertson, S.E., Walker, S., Jones, S., Hancock-Beaulieu, M.M. and Gatford, M., 1995. Okapi at TREC-3. <em>Nist Special Publication Sp</em>,</li>
        <li id="bib36" label="[36]">Rose, S., Engel, D., Cramer, N. and Cowley, W., 2010. Automatic keyword extraction from individual documents. <em>Text Mining: Applications and Theory</em>, pp.1-20.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186334">https://doi.org/10.1145/3184558.3186334</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
