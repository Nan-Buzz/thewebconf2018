<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Wen Hua</span>      <span class="surName">Lin</span>     National Taiwan University, Taipei, Taiwanq868686qq@gmail.com, ktchen@cmlab.csie.ntu.edu.tw, kenny5312012@gmail.com, whsu@ntu.edu.tw     </div>     <div class="author">     <span class="givenName">Kuan-Ting</span>      <span class="surName">Chen</span>     National Taiwan University, Taipei, Taiwanq868686qq@gmail.com, ktchen@cmlab.csie.ntu.edu.tw, kenny5312012@gmail.com, whsu@ntu.edu.tw     </div>     <div class="author">     <span class="givenName">Hung Yueh</span>      <span class="surName">Chiang</span>     National Taiwan University, Taipei, Taiwanq868686qq@gmail.com, ktchen@cmlab.csie.ntu.edu.tw, kenny5312012@gmail.com, whsu@ntu.edu.tw     </div>     <div class="author">     <span class="givenName">Winston</span>      <span class="surName">Hsu</span>     National Taiwan University, Taipei, Taiwanq868686qq@gmail.com, ktchen@cmlab.csie.ntu.edu.tw, kenny5312012@gmail.com, whsu@ntu.edu.tw     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186354" target="_blank">https://doi.org/10.1145/3184558.3186354</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Recently, deep neural network models have achieved promising results in image captioning task. Yet, &#x201C;vanilla&#x201D; sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose <em>Netizen Style Commenting (NSC)</em>, to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid &#x201C;netizen&#x201D; style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Document topic models;</strong> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Natural language generation;</strong> <strong>Image representations;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Fashion; Image Captioning; Commenting; Diversity; Deep Learning; Topic Model</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Wen Hua Lin, Kuan-Ting Chen, Hung Yueh Chiang, and Winston Hsu. 2018. Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3186354" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186354</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>In accordance with&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], fashion has a vital impact on our society because clothing typically reflects a person&#x0027;s social status. This is also expected in the growing online retail sales, reaching 529 billion dollars in the US, and 302 billion euros in Europe by 2018 [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. Still today, people either wear up their new clothes or upload their new clothing photo on social media to receive comments of new clothes. However, dressing inappropriately sometimes causes embarrassing. Therefore, people tend to know whether they dress properly beforehand. As the promising results achieved by image captioning, the problem could be solved by fashion image captioning works, which automatically describe the outfit with netizen-like comments. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186354/images/www18companion-116-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Five sentences for each image from distinct commenting (captioning) methods. (a) One of the users&#x2019; comments (i.e., ground truth) randomly picked from the post (photo) in our collected NetiLook dataset. (b) The sentences from Microsoft CaptionBot. (c) The results from neural image caption generation (NC)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0033">33</a>] (d) The results from neural image caption generation with visual attention (Attention)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"        href="#BibPLXBIB0035">35</a>]. (e) Our proposed NSC. It marries style-weight to achieve vivid netizen style results.</span>     </div>     </figure>    </p>    <p>Whereas, image captioning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] is still a challenging and under researching topic despite deep learning developing rapidly in recent years. To generate a human-like captioning, machines not only recognize objects in an image but express their relationships in natural language, such as English. Large corpora of paired images and descriptions, such as MS COCO&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] and Flickr30k&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] are proposed to address the problem. Several deep recurrent neural network models are devised to follow the datasets and reach promising results. However, modern methods only focus on optimizing metrics used in machine translation, which causes absence of diversity &#x2014; producing conservative sentences. These sentences can achieve good scores in machine translation metrics but are short of humanity. Compared with human comments as shown in Figure&#x00A0;<a class="fig" href="#fig1">1</a> (a), due to the limitation of training data, current methods (e.g., Figure&#x00A0;<a class="fig" href="#fig1">1</a> (b)) merely describe &#x201C;vanilla&#x201D; sentences with low utilities, which are merely describing the shallow and apparent appearances (e.g., color, types) in photos and generate meaningless bot tokens to users &#x2014; lacking engagement, contexts, and feedbacks for user intentions, especially in the circumstances of online social media.</p>    <p>In order to generate human-like online comments (e.g, clothing style) for fashion photos, we collect a large corpus of paired user-contributed fashion photos and comments, called NetiLook, from an online clothing style community. To the best of our knowledge, our collected NetiLook is the largest fashion comment dataset. In our experiment on NetiLook, we found that these methods overfit to a general pattern, which makes captioning results insipid and banal (e.g., &#x201C;love the ...&#x201D;) (cf., Figure&#x00A0;<a class="fig" href="#fig1">1</a> (c) and (d)). Therefore, to compensate for the deficiency, we propose integrating latent topic models with state-of-the-art methods and make the generated sentences vivacious (cf., Figure&#x00A0;<a class="fig" href="#fig1">1</a> (e)). Besides, for evaluating diversity, we propose three novel measures to quantize variety.</p>    <p>For richness and diversity in text content, we propose a novel method to automatically generate characteristic fashion photo comments for user-contributed fashion photos by marrying <em>style-weight</em> (cf., Section&#x00A0;<a class="sec" href="#sec-7">4.2</a>) from topic discovery models (i.e., latent Dirichlet allocation (LDA) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]) with neural networks to achieve diverse comments with vivid &#x201C;netizen&#x201D; style. We look forward the breakthrough will foster further applications in social media, online customer services, e-commerce, chatbot developments, etc. It will be more exciting, in the very near future; for example, if the solution can work as an agent (or expert) in a living room and can comment for a user as testing the outfit in front of the mirror. To sum up, our main contributions are as follows:</p>    <ul class="list-no-style">     <li id="list1" label="&#x2022;">To our best knowledge, this is the first work to address the diverse measures of photo captioning in a large-scale fashion commenting dataset (cf., Section <a class="sec" href="#sec-2">1</a>-<a class="sec" href="#sec-3">2</a>).<br/></li>     <li id="list2" label="&#x2022;">We collect a brand new large-scale clothing dataset, NetiLook, which contains 300K posts (photos) with 5M comments (cf., Section <a class="sec" href="#sec-4">3</a>).<br/></li>     <li id="list3" label="&#x2022;">We investigate the diversity of clothing captioning and propose three measures to estimate the diversity (cf., Section <a class="sec" href="#sec-8">5</a>).<br/></li>     <li id="list4" label="&#x2022;">We leverage and investigate the merit of latent topic models, which is able to make up the insufficiency of conventional image captioning works (cf., Section <a class="sec" href="#sec-5">4</a>).<br/></li>     <li id="list5" label="&#x2022;">We demonstrate that our proposed approach significantly benefits fashion photo commenting and improves image captioning task both in accuracy and diversity over Flickr30k and NetiLook datasets (cf., Section <a class="sec" href="#sec-9">6</a>).<br/></li>    </ul>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>Image captioning which automatically describes the content of an image with properly formed sentences enables many important applications such as helping visually impaired users and human-robot interaction. According to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>][<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>], a CNN-RNN framework, taking high-level features extracted from a deep convolution neural network (CNN) as an input for a recurrent neural network (RNN) to generate a complete sentence in natural language, has performed promisingly in image captioning tasks during the last few years. For example,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] is an end-to-end CNN model followed by language generation of RNN. It was able to produce a grammatically correct sentence in natural language from an input image.</p>    <p>Following CNN-RNN frameworks, attention-based models ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]) were proposed. As human beings put different attentions at distinct objects while watching a photograph, attention-based models allow machines to put diverse weights on salient features. Compared with taking high-level representations of a whole image as input, attention-based models are able to dynamically weight various parts of images. Especially, when a lot of objects appear in an image, attention-based models can give more thorough captions&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>].</p>    <p>Presently, state-of-the-art works are majorly attention-based models ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]) because they focus on correctness of descriptions. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] assigned different weights to different words for fixing misrecognition. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] focused on evaluating the correctness of attention in neural image captioning models.</p>    <p>While applying current methods to generate comments, the demand for diversity is unveiled. Compared with depicting images, giving comments is more challenging because it needs to not only understand images but take care of engagement with users. To generate vivid comments, diversity is necessary. Besides commenting, diversity is also important in other areas (e.g., information retrieval [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]). In [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>], to increase the utility of automatically generated response options of email, diversity is essential. Moreover, in building general-purpose conversation agents, which are required for intelligent agents&#x2019; interaction with humans in natural language, diversity is also requisite. Therefore, we blend topic models with conventional methods to complement the diversity part of them.</p>    <p>Meanwhile, there has been increasing interest in clothing product analysis from the computer vision and multimedia communities. Most existing fashion analysis works focused on the investigation of the clothing attributes, such as clothing parsing ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>]), fashion trend ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]) and clothing retrieval ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>], [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]). In contrast to other works, we develop a novel framework that can leverage the learned Netizen-style embedding for commenting on fashion photos. Moreover, to our best knowledge, this is the first work to address the diverse measures of photo captioning in an all-time large-scale fashion commenting dataset. We detail the dataset and our method in the following sections. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186354/images/www18companion-116-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Examples from Flickr30k and our NetiLook. (a) Most sentences are describing the shallow appearances (e.g., types, colors) and have similar sentence patterns (e.g., &#x201C;A little girl ...&#x201D;). (b) The sentences involve diverse user intentions with abundant styles. Furthermore, emojis and emoticons inside make it much more intimate with people.</span>     </div>     </figure>    </p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Dataset &#x2014; NetiLook</h2>     </div>    </header>    <p>[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] mentioned that current captioning datasets are relatively small compared with object recognition datasets, such as ImageNet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. Besides, the descriptions require costly manual annotation. With the growing of social media, such as Facebook and Instagram, people constantly share their life with the world. Consequently, these are all potentially valuable training data for image captioning (or commenting). Among social platforms, there are some specific websites just for clothing style. Lookbook<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, an example shown in Figure&#x00A0;<a class="fig" href="#fig3">3</a>, is an online clothing style community where members share their personal style and draw fashion inspiration from each other. Such a rich and engaging social medium is potential to benefit intelligent and human-like commenting applications. Hence, we collected a large corpus of paired user-contributed fashion photos and comments from Lookbook called <em>NetiLook</em>.</p>    <p>     <em>NetiLook</em><a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>: To the best of our knowledge, this is the first and the largest netizen-style commenting dataset. It contains 355,205 images from 11,034 users and 5 million associated comments collected from Lookbook. As the examples shown in&#x00A0;Figure&#x00A0;<a class="fig" href="#fig1">1</a>, most of the images are fashion photos in various angles of views, distinct filters and different styles of collage. As&#x00A0;Figure&#x00A0;<a class="fig" href="#fig2">2</a> (b) shows, each image is paired with (diverse) user comments. The maximum number of comments is 427 and the average number of comments is 14 per image in our dataset. Note that we observe that there are 7% of images with no comments and we remove these images in our training stage. Besides, each post has a title named by an author, a publishing date and the number of hearts given by other user. Moreover, some users add names, brands, pantone of the clothes, and stores where they bought the clothes. Furthermore, we collect the authors&#x2019; public information. Some of them contain age, gender, country and the number of fans (cf.,&#x00A0;Figure&#x00A0;<a class="fig" href="#fig3">3</a>). We believe all of these are valuable to boost the domain of fashion photo commenting. In this paper, we only use the comments and the photos from our dataset. Other attributes can be used to refine the system in future work. For comparing the results on Flickr30k, we also sampled 28,000 for training, 1,000 for validation and 1,000 for testing. Besides, we also sampled five comments for each image.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Comparison with other image captioning benchmarks (Flickr30k&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0028">28</a>] and MS COCO&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0017">17</a>]). Our collected dataset, Netilook, has the most diverse and realistic sentences in the social media (e.g., largest unique words)</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">Dataset</td>       <td style="text-align:center;">Images</td>       <td style="text-align:center;">Sentences</td>       <td style="text-align:center;">Average Length</td>       <td style="text-align:center;">Unique Words</td>      </tr>      <tr>       <td style="text-align:left;">Flickr30k</td>       <td style="text-align:center;">30K</td>       <td style="text-align:center;">150K</td>       <td style="text-align:center;">13.39</td>       <td style="text-align:center;">23,461</td>      </tr>      <tr>       <td style="text-align:left;">MS COCO</td>       <td style="text-align:center;">200K</td>       <td style="text-align:center;">1M</td>       <td style="text-align:center;">10.46</td>       <td style="text-align:center;">54,231</td>      </tr>      <tr>       <td style="text-align:left;">NetiLook</td>       <td style="text-align:center;">350K</td>       <td style="text-align:center;">5M</td>       <td style="text-align:center;">3.75</td>       <td style="text-align:center;">597,629</td>      </tr>     </tbody>     </table>    </div>    <p>Compared to general image captioning datasets such as Flickr30k [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>], the data from social media are quite noisy, full of emojis, emoticons, slang and much shorter (cf.,&#x00A0;Figure&#x00A0;<a class="fig" href="#fig2">2</a> (b) and&#x00A0;Table&#x00A0;<a class="tbl" href="#tab1">1</a>), which makes generating a vivid &#x201C;netizen&#x201D; style comment much more challenging. Moreover, plenty of photos are in different styles of collage (cf., photos in&#x00A0;Figure&#x00A0;<a class="fig" href="#fig1">1</a>). Therefore, it makes the image features much more noisy than single view photos. To completely generate comments that entirely reflect the culture in social media, we demonstrate our method in the following section. <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186354/images/www18companion-116-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">An example to show the attributes of a post in Lookbook. The post includes a title named by the author, country of the author, a publishing date, names, brands, and pantone of the clothes.</span>     </div>     </figure>     <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186354/images/www18companion-116-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">An illustration of our proposed framework. Our system consists of LSTM&#x00A0;(cf., Section&#x00A0;<a class="sec" href="#sec-6">4.1</a>), topic models&#x00A0;(cf., Section&#x00A0;<a class="sec" href="#sec-7">4.2</a>) and beam search to boost results&#x00A0;(cf., Section&#x00A0;<a class="sec" href="#sec-10">6.1</a>). Our proposed approach leverages the outputs of image captioning model based on CNN-RNN frameworks and style-weight from LDA to generate a diverse comment with vivid &#x201C;netizen&#x201D; style.</span>     </div>     </figure>    </p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Method &#x2013; Netizen Style Commenting</h2>     </div>    </header>    <p>In NetiLook, we observed that user comments are much more diverse while comparing them with the sentences in general image captioning datasets. In addition, there are some frequently used sentences along with posts (e.g., &#x201C;love this!&#x201D;, &#x201C;nice&#x201D;) which cause current models inclined to generate similar sentences. The output comments become meaningless and insipid. To immerse the model in vivid netizen style, we fuse style-weight from topic models to image captioning models in order to keep long-range dependencies and take different comments from distinct points of view as topics.</p>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Image Captioning</h3>     </div>     </header>     <p>We follow [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] to extract image features from an image I by a CNN and feed it into the image captioning model at <em>t</em> = &#x2212;1 to inform a LSTM (cf., CNN in&#x00A0;Figure&#x00A0;<a class="fig" href="#fig4">4</a>). We extract the FC7 (a fully-connected layer) features as high-level meaning of the image from and feed it into the LSTM. <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {x}_{-1} = \textrm {CNN}(\textrm {I})\textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> We represent a word as a one-hot vector <strong>s</strong> of dimension equal to the size of dictionary. <em>T</em> is the maximum length of output sentences. We represent word embeddings as <em>W<sub>e</sub>     </em>. <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {x}_t = W_e\mathbf {s},~t \in {0...T-1}\textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> With the CNN features, we can obtain probabilities of words in each generating step from the image captioning model. Sentences from general image captioning dataset basically depict common content of images. Therefore, conventional image captioning models are able to focus on accuracy. Nevertheless, to strike a balance between accuracy and diversity in current frameworks is arduous. To keep the merit of conventional models, we modify the generating processes of modern models with topic models and make outputs diverse while facing vivid netizen comments.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Style Captioning</h3>     </div>     </header>     <p>To consider vivid netizen style comments, we introduce style-weight <strong>w</strong>     <sub>      <em>style</em>     </sub> element-wised multiplied (&#x25CB;) with outputs at each step of LSTM to season generated sentences. <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {p}_{t+1} = \textrm {Softmax}(\textrm {LSTM}(\mathbf {x}_t)) \circ \mathbf {w}_{style},~t \in {0...T-1}\textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> Style-weight <strong>w</strong>     <sub>      <em>style</em>     </sub> represents the comment style, which teaches models to be acquainted with style in the corpus while generating captioning.</p>     <p>However, abstract concepts are hard for people to give a specific definition. To obtain the comment style in NetiLook, we apply LDA to discover latent topics and fuse with current captioning models.</p>     <p>Suppose, a corpus contains <em>M</em> comments. Comments are composed of a subset of <em>N</em> words. We specify <em>K</em> (<em>K</em> topics (<em>z</em>     <sub>1</sub>, <em>z</em>     <sub>2</sub>, ..., <em>z<sub>K</sub>     </em>)) for LDA. It gives <em>N</em> dimensional topic-word vectors and <em>K</em> dimensional comment-topic vectors.</p>     <p>Topic-word vectors: Each topic <em>z</em> has a probabilistic vector of <em>N</em> words in dictionary. The vector describes the word distribution of the topic. The topic-word vector <em>&#x03D5;<sub>z</sub>     </em> of topic <em>z</em> is <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {\phi _{z}}=\lbrace P(w_1|z),P(w_2|z),...,P(w_N|z)\rbrace \textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <em>w</em>     <sub>1</sub>, <em>w</em>     <sub>2</sub>, ..., <em>w<sub>N</sub>     </em> are N words in dictionary.</p>     <p>Comment-topic vectors: Each comment <em>m</em> is also associated with a probabilistic vector of topics, which means the topics probability of the comment. The comment-topic vectors <em>&#x03B8;<sub>m</sub>     </em> of comment <em>m</em> is <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {\theta _{m}} = \lbrace P(z_1|m),P(z_2|m),...,P(z_k|m)\rbrace \textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where <em>z</em>     <sub>1</sub>, <em>z</em>     <sub>2</sub>, ..., <em>z<sub>K</sub>     </em> are different K topics. To find the topic distribution in corpus, each comment votes the topic with highest probability by <span class="inline-equation"><span class="tex">$\arg \max (\theta _m)$</span>     </span>. <span class="inline-equation"><span class="tex">$\mathbf {t}_m^i$</span>     </span> is the i-th dimension of <strong>t</strong>     <sub>      <em>m</em>     </sub>. In our finding, the voting gives the most characteristic style in the corpus. The mathematical notation can be represented as follow: <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} Let~\mathbf {t}_m^i = \lbrace \begin{array}{l}1~\textrm {if } i = \arg \max (\mathbf {\theta _{m}})\\ 0~\textrm {otherswise} \end{array}\textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> The topic distribution of the corpus <strong>y</strong> now can be computed by normalizing the summation of the number of topics from <strong>t</strong>     <sub>      <em>m</em>     </sub> by the total number of comments. It means the proportion of various points of view of comments in the corpus: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {y} = \sum _{m=1}^{M}\mathbf {t}_m / M\textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> With the topic distribution of corpus <strong>y</strong> and topic-word vectors <strong>      <em>&#x03D5;</em>     </strong>, our style-weight <strong>w</strong>     <sub>      <em>style</em>     </sub> is now defined as: <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{eqnarray} \mathbf {w}_{style} = \sum _{k=1}^{K} \mathbf {y}^k \mathbf {\phi }_k\textrm {.} \end{eqnarray} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div> where <strong>y</strong>     <sup>      <em>k</em>     </sup> means the k-th dimension of <strong>y</strong>.</p>     <p>As we embed style-weight in&#x00A0;Equation&#x00A0;(<a class="eqn" href="#eq3">3</a>), which could guide the generating process to select words that are much closer to the netizen style learned in the social media (e.g., we observe that one style-wight highlights emoji style), LSTM is capable to generate the sentences with the style in corpus. (cf., Latent Topic in&#x00A0;Figure&#x00A0;<a class="fig" href="#fig4">4</a>).</p>    </section>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Diversity Measures</h2>     </div>    </header>    <p>Since BLEU and METEOR are not for diversity measure, diversity measures are being put importance on sentence generation models. Currently,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>] and&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] report the degree of diversity by calculating the number of distinct words in generated responses scaled by the total number of generated tokens. However, this is not enough for diverse comments from the Internet, since comments can be represented not only in natural language but in various sentence patterns, such as emojis, and emoticons. Therefore, to compensate the defects of BLEU and METEOR, we propose three novel measures to judge the diversity of comments generated from captioning models.</p>    <p>We observed that more diverse sentences are generated, more unique words are used. Thus we devise an intuitive and trivial unique words measure, called DicRate.</p>    <p>     <em>DicRate</em>: The dictionary rate we proposed in this paper is measured through counting number of unique words among generated sentences divided by unique words among ground truth sentences. The number of unique words in ground truth sentences is <em>N<sub>t</sub>     </em>. The number of unique words in generated sentences is <em>N<sub>g</sub>     </em>. The DicRate is computed as follow: <div class="table-responsive" id="eq9">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \textrm {DicRate}(N_t, N_g) = N_g / N_t\textrm {.} \end{eqnarray} </span>      <br/>      <span class="equation-number">(9)</span>     </div>     </div> DicRate reflects the abundance of vocabulary of a model, but it is still not incapable to measure sentence diversity. Inspired by the paper [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] for conversation response generation, two novel measures based on entropy are carried out to judge the diversity of comments on fashion photos. Descriptions of the measures are as follows:</p>    <p>     <em>WF-KL</em>: The Kullback-Leibler divergence (KL divergence) of word frequency distribution between ground truth sentences and generated sentences. It shows how well a model learned the tendency of choosing words in a dataset. The number of unique words in the dataset is <em>N</em>. The occurrence times of each word in ground truth sentences are <strong>w</strong>     <sub>     <em>t</em>     </sub>. The word frequency distribution of ground truth sentences is <strong>w</strong>     <sub>     <em>ft</em>     </sub>. The occurrence of each word in generated sentences are <strong>w</strong>     <sub>     <em>g</em>     </sub>. The word frequency distribution of generated sentences is <strong>w</strong>     <sub>     <em>fg</em>     </sub>. By referring to the formula of term frequency-inverse document frequency (tf-idf), to avoid division by zero, we add one to <strong>w</strong>     <sub>     <em>t</em>     </sub> and <strong>w</strong>     <sub>     <em>g</em>     </sub>. <strong>w</strong>     <sup>     <em>i</em>     </sup> is the i-th dimension of <strong>w</strong>. <div class="table-responsive" id="eq10">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \mathbf {w}_{ft}^i = (\mathbf {w}_{t}^i + 1)/ \sum _{i=1}^{N}(\mathbf {w}_{t}^i+1)\textrm {.}\end{eqnarray} </span>      <br/>      <span class="equation-number">(10)</span>     </div>     </div>     <div class="table-responsive" id="eq11">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \mathbf {w}_{fg}^i = (\mathbf {w}_{g}^i + 1) / \sum _{i=1}^{N}(\mathbf {w}_{g}^i+1)\textrm {.} \end{eqnarray} </span>      <br/>      <span class="equation-number">(11)</span>     </div>     </div> The WF-KL can be computed as follow: <div class="table-responsive" id="eq12">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \textrm {WF-KL}(\mathbf {w}_{ft}, \mathbf {w}_{fg}) = \sum _{i=1}^{N}\mathbf {w}_{ft}^i\log (\mathbf {w}_{ft}^i/\mathbf {w}_{fg}^i)\textrm {.} \end{eqnarray} </span>      <br/>      <span class="equation-number">(12)</span>     </div>     </div>     <em>POS-KL</em>: The KL divergence of part-of-speech (POS) tagging frequency distribution between ground truth sentences and generated sentences. POS is a classic natural language processing task. One of the applications is identifying which spans of text are products in user search queries&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. Besides word distribution, POS also demonstrates the interaction between words in a sentence. The number of unique tags in the dataset is <em>N</em>. The occurrence times of each tag in ground truth sentences are <strong>t</strong>     <sub>     <em>t</em>     </sub>. The tag frequency distribution of ground truth sentences is <strong>t</strong>     <sub>     <em>ft</em>     </sub>. The occurrence times of each tag in generated sentences are <strong>t</strong>     <sub>     <em>g</em>     </sub>. The tag frequency distribution of generated sentences is <strong>t</strong>     <sub>     <em>fg</em>     </sub>. To avoid division by zero, we also add one to <strong>t</strong>     <sub>     <em>t</em>     </sub> and <strong>t</strong>     <sub>     <em>g</em>     </sub>. <strong>t</strong>     <sup>     <em>i</em>     </sup> is the i-th dimension of <strong>t</strong>. <div class="table-responsive" id="eq13">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \mathbf {t}_{ft}^i = (\mathbf {t}_{t}^i + 1)/ \sum _{i=1}^{N}(\mathbf {t}_{t}^i+1)\textrm {.}\end{eqnarray} </span>      <br/>      <span class="equation-number">(13)</span>     </div>     </div>     <div class="table-responsive" id="eq14">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \mathbf {t}_{fg}^i = (\mathbf {t}_{g}^i + 1) / \sum _{i=1}^{N}(\mathbf {t}_{g}^i+1)\textrm {.} \end{eqnarray} </span>      <br/>      <span class="equation-number">(14)</span>     </div>     </div> The POS-KL can be computed as follow: <div class="table-responsive" id="eq15">     <div class="display-equation">      <span class="tex mytex">\begin{eqnarray} \textrm {POS-KL}(\mathbf {t}_{ft}, \mathbf {t}_{fg}) = \sum _{i=1}^{N}\mathbf {t}_{ft}^i\log (\mathbf {t}_{ft}^i/\mathbf {t}_{fg}^i)\textrm {.} \end{eqnarray} </span>      <br/>      <span class="equation-number">(15)</span>     </div>     </div>    </p>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Experiments</h2>     </div>    </header>    <section id="sec-10">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.1</span> Experiment Setting</h3>     </div>     </header>     <p>To our best knowledge, this is the first captioning method that focuses on corpus style and sentence diversity. Generally, current methods are devoted to optimizing machine translation scores. Therefore, we only choose two famous captioning methods for comparison rather than other state-of-the-art methods (e.g.,&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>],&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>]). To demonstrate the improvement of diversity, we apply our style-weight to our baselines.</p>     <p>Dataset: Note that we only adopt Flick30k for our experiments to compare with NetiLook because of the characteristic of Flick30k that mainly depicts humans, which is closer to NetiLook. Additionally, images in Flick30k and NetiLook are all collected from social media, which makes images in a similar domain.</p>     <p>Pre-processing: We argue that the learning process should be autonomous and leverage the freely and hugely available online social media. To avoid noise, we follow [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] to remove the sentences that contain a word frequency that is less than five times in training set. We also filter the sentences that are more than 20 words in the dataset to reduce advertisement and also make sentence more readable [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>]. Noted that in order to thoroughly convey users&#x2019; intention and comment style, we do not remove any punctuation in sentences.</p>     <p>Evaluation: BLEU and METEOR are conventional machine translation scores, which base on the matching of answers without considering diversity. The difference between BLEU and METEOR is that METEOR can handle stemming and synonymy matching. In BLEU scores, we report it in 4-grams because it has the highest correlation with humans&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>]. For BLEU and METEOR, the higher scores mean that sentences are much correct according to the matching with ground truth. In our diversity measures, the higher DicRate shows that the more abundance of vocabulary of a model. Moreover, the lower WF-KL and POS-KL mean that the generated corpus is closer to the ground truth word distribution and sentence patterns.</p>     <p>Baseline: We duplicate two famous captioning methods (NC&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] and Attention&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>]) in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a> and&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a>. NC is a CNN-RNN framework method that considers global features of images. Attention is an attention-based method, which puts distinct weights on salient features. By comparing NC with Attention in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>], BLEU and METEOR have similar relation like the result reported in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a>. Our proposed method, NSC, fuses style-weight in the decoding stage. Following [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], we adopt beam search, an approximate inference algorithm, which is widely used in image captioning to boost the results. Because the number of possible sequences grows exponentially with the sentence length, beam search can explore generating process by spreading the most promising node in a limited set. We compare various beam sizes in our experiments and these methods get the best performance at the beam size of 3. Note that the optimal beam size might vary due to the properties of a dataset [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]. In our experiments, for LDA, analysis of the performance sensitivity is made by varying K from 1 to 15. For the first experiment on Flickr 30k, we set the number of topics to be 3 (K = 3); for the experiment on NetiLook, we have K = 5 in <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> and K = 3 in <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{Attention}}$</span>     </span>. We observe that topic models can reflect some semantic &#x201C;style&#x201D; of comments (e.g. emoji style). Therefore, compared to Flickr 30k, more topic models are selected in NetiLook because user comments are much more diverse in this dataset. Interestingly, the proper number of topic models in <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> is higher than <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{Attention}}$</span>     </span>. We observe that more topic models would not benefit the attention-based approach for the reason that attention-based models are greatly restricted the word selection.</p>    </section>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.2</span> Quantitative Analysis &#x2013; Dataset</h3>     </div>     </header>     <p>Traditional captioning dataset such as Flickr30k [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>] and MS COCO [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>] only focus on image description and do not emphasize on style and comment-like sentences. Therefore, we address the problem in the paper and contribute the dataset for brand-new problem definition. For comparing models with human and characteristics of datasets, we not only evaluate the generated sentences but also evaluate human comments. Also, as we can see differences from human evaluation between&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a> and&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a>, the comparison does highlight the distinctions between Netilook and Flicr30k. For a comment given by a human or machine, it is difficult to be evaluated on conventional measures such as BLEU in NetiLook (e.g.&#x00A0;0.108 in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a> vs. 0.008 in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a> in BLEU-4). Thus, we propose our measures DicRate, WF-KL and POS-KL to evaluate comments.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Performance on Flickr30k testing splits.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;">Method</td>        <td style="text-align:center;">BLUE-4</td>        <td style="text-align:center;">METEOR</td>        <td style="text-align:center;">WF-KL</td>        <td style="text-align:center;">POS-KL</td>        <td style="text-align:center;">DicRate</td>       </tr>       <tr>        <td style="text-align:center;">Human</td>        <td style="text-align:center;">0.108</td>        <td style="text-align:center;">0.235</td>        <td style="text-align:center;">1.090</td>        <td style="text-align:center;">0.013</td>        <td style="text-align:center;">0.664</td>       </tr>       <tr>        <td style="text-align:center;">NC</td>        <td style="text-align:center;">0.094</td>        <td style="text-align:center;">0.147</td>        <td style="text-align:center;">1.215</td>        <td style="text-align:center;">0.083</td>        <td style="text-align:center;">0.216</td>       </tr>       <tr>        <td style="text-align:center;">Attention</td>        <td style="text-align:center;">        <strong>0.121</strong>        </td>        <td style="text-align:center;">        <strong>0.148</strong>        </td>        <td style="text-align:center;">1.203</td>        <td style="text-align:center;">0.302</td>        <td style="text-align:center;">0.053</td>       </tr>       <tr>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>        </span>        </td>        <td style="text-align:center;">0.089</td>        <td style="text-align:center;">0.146</td>        <td style="text-align:center;">1.217</td>        <td style="text-align:center;">        <strong>0.075</strong>        </td>        <td style="text-align:center;">        <strong>0.228</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{Attention}}$</span>        </span>        </td>        <td style="text-align:center;">0.119</td>        <td style="text-align:center;">        <strong>0.148</strong>        </td>        <td style="text-align:center;">        <strong>1.202</strong>        </td>        <td style="text-align:center;">0.319</td>        <td style="text-align:center;">0.055</td>       </tr>      </tbody>     </table>     </div>     <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Performance on NetiLook testing splits.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;">Method</td>        <td style="text-align:center;">BLEU-4</td>        <td style="text-align:center;">METEOR</td>        <td style="text-align:center;">WF-KL</td>        <td style="text-align:center;">POS-KL</td>        <td style="text-align:center;">DicRate</td>       </tr>       <tr>        <td style="text-align:center;">Human</td>        <td style="text-align:center;">0.008</td>        <td style="text-align:center;">0.172</td>        <td style="text-align:center;">0.551</td>        <td style="text-align:center;">0.004</td>        <td style="text-align:center;">0.381</td>       </tr>       <tr>        <td style="text-align:center;">NC</td>        <td style="text-align:center;">0.013</td>        <td style="text-align:center;">0.151</td>        <td style="text-align:center;">0.665</td>        <td style="text-align:center;">1.126</td>        <td style="text-align:center;">0.036</td>       </tr>       <tr>        <td style="text-align:center;">Attention</td>        <td style="text-align:center;">0.020</td>        <td style="text-align:center;">0.133</td>        <td style="text-align:center;">        <strong>0.639</strong>        </td>        <td style="text-align:center;">1.629</td>        <td style="text-align:center;">0.011</td>       </tr>       <tr>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>        </span>        </td>        <td style="text-align:center;">0.013</td>        <td style="text-align:center;">        <strong>0.172</strong>        </td>        <td style="text-align:center;">0.695</td>        <td style="text-align:center;">        <strong>0.376</strong>        </td>        <td style="text-align:center;">        <strong>0.072</strong>        </td>       </tr>       <tr>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{Attention}}$</span>        </span>        </td>        <td style="text-align:center;">        <strong>0.030</strong>        </td>        <td style="text-align:center;">0.139</td>        <td style="text-align:center;">0.659</td>        <td style="text-align:center;">1.892</td>        <td style="text-align:center;">0.012</td>       </tr>      </tbody>     </table>     </div>     <p>In the scenario of online social media, punctuation, slang, emoticons and emojis are important for conveying emotion in a sentence. Thus, Netilook has much more diversity and unique words than other datasets (0.664 in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a> vs. 0.381 in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a> in DicRate). NetiLook specializes in describing clothing style as examples shown in&#x00A0;Figure&#x00A0;<a class="fig" href="#fig5">5</a>. Still, there are some common words and general patterns to describe and comment on the clothing style in comparison with Flickr30k, which mixes all types of images in the dataset. Thus NetiLook has lower score on WF-KL and POS-KL (e.g.&#x00A0;1.090 in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a> vs. 0.551 in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a> in WF-KL).</p>     <p>For such a diverse and characteristic dataset, machines are required considering overall corpus distribution and mimic comment style in order to get high performance in our evaluations. Nonetheless, for learning human commenting style, it is still challenging for general captioning models to generate diverse words while there are some general comments can achieve universally low loss (e.g., &#x201C;nice&#x201D;, &#x201C;I love this!&#x201D;). However, our style-weight brings human style in machine generated sentences.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.3</span> Quantitative Analysis &#x2013; Model Evaluation</h3>     </div>     </header>     <p>Table&#x00A0;<a class="tbl" href="#tab2">2</a> summarizes performances for the Flickr30k dataset. Attention models put weights on salient features in images, thus the models easily describe objects inside pictures and reach a better BLEU and METEOR (e.g.&#x00A0;0.094 vs. 0.121 in BLEU-4). However, attention-based models are greatly restricted the word selection while decoding stage. In our experiments, POS-KL and DicRate are much worse (e.g.&#x00A0;0.053 vs. 0.216 in DicRate) comparing Attention with NC. With our style-weight, the model <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> expands the word diversity without sacrifice much on BLEU and METEOR. Style-weight encourages models choosing the words that are closer to the original distribution rather than the words that can generally get the lowest loss during the training phase. As we show in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab2">2</a>, DicRate and POS-KL are improved comparing <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> with NC (e.g.&#x00A0;0.216 vs. 0.228 in DicRate). The impact of style-weight is also shown in Attention model. However, we observed that embedding style-weight does not improve much in Flickr30k dataset in terms of diversity, because the sentences are objectively depicting humans performing various activities in Flickr30k.</p>     <p>In NetiLook, the experiment in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a> shows that our method can greatly improve the diversity. Comparing NC with Attention in&#x00A0;Table&#x00A0;<a class="tbl" href="#tab3">3</a>, NC performs better than Attention (e.g.&#x00A0;0.036 vs. 0.011 in DicRate) except for BLEU-4 and WF-KL (e.g.&#x00A0;0.665 vs. 0.639 in WF-KL) because the selection of words is affected by salient features in an image, which makes the model miss the intention of the corpus while the whole dataset has similar objects. However, with style-weight, our <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> outperforms other baselines in POS-KL and DicRate (e.g.&#x00A0;0.376 of <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> in POS-KL). This proves that style-weight can guide the generating process to the comment that is much closer to the users&#x2019; behaviour in the social media, making machine mimic online netizen comment style. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186354/images/www18companion-116-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Examples of comments generated by different methods. The examples show that our proposed approach <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>        </span> can help generate more diverse and vivid comments.</span>      </div>     </figure>     </p>    </section>    <section id="sec-13">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.4</span> Image Commenting Results</h3>     </div>     </header>     <p>We show some real examples of fashion commenting results on NetiLook with various methods. Though there are emojis generated from Microsoft CaptionBot, the comments are still lacking engagement and can not afford to process photos in collage. While training general captioning models (e.g., NC and Attention) on NetiLook, the comments are much shorter than Human&#x0027;s and fixed in some patterns, which lacks diversity.</p>     <p>     <em>Similar intention like human</em>: With the style-weight, <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> can generate the comment that is much closer to users&#x2019; intention (cf.,&#x00A0;Figure&#x00A0;<a class="fig" href="#fig5">5</a> (a)).</p>     <p>     <em>More vivid comments</em>: While conveying the same intention, <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> is able to use emoticons, punctuations and capitalizations to generate more netizen-style comments than other captioning models (cf.,&#x00A0;Figure&#x00A0;<a class="fig" href="#fig5">5</a> (b)).</p>     <p>     <em>Another point of view</em>: By considering the topic distribution of data, <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> generates comments that are different from general captioning models&#x2019; and much closer to human beings (cf.,&#x00A0;Figure&#x00A0;<a class="fig" href="#fig5">5</a> (c) - (e)).</p>     <p>     <em>Wrong objects</em>: However, there are still some drawbacks in our <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span>, such as describing wrong objects in the images. Because the <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> is still based on image captioning models, it will also generate wrong comments as other captioning models due to the similarity of images (cf.,&#x00A0;Figure&#x00A0;<a class="fig" href="#fig5">5</a> (f)). It can be improved by jointly training the topic model with attention-based models.</p>    </section>    <section id="sec-14">     <header>     <div class="title-info">      <h3>       <span class="section-number">6.5</span> User Study</h3>     </div>     </header>     <p>Motivated by the paper &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] which conducts a human evaluation of image captioning by presenting images to three workers, we conducted a user study from 23 users to demonstrate the effect of diverse comments. The users are about 25 year-old and familiar with netizen style community and social media. The sex ratio in our user study is 2.83 males/female. They are asked to rank comments for 35 fashion photos. Each photo has 4 comments &#x2014; from one randomly picked human comments, NC, Attention and our <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span>. Therefore, each of the users has to appraise 140 comments generated from different methods. Furthermore, we collect user feedback to understand user judgements on comments generated by different methods.</p>     <p>As&#x00A0;Table&#x00A0;<a class="tbl" href="#tab4">4</a> shows, 36.8% out of 805 votes (35 &#x00D7; 23) rank the sentences generated from <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> at the first place which outperforms NC and Attention. It means that our NSC defeats human comments in some images. Furthermore, The difference between humans and <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> in rank 1 is only 9.3%. In top two ranks, the performance of <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> reaches 76.6%. This also demonstrates that our <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> can generate sentences with human-like quality. In our user study, people generally regard our <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> sentences as human comments. According to our user study, the main concern of people&#x0027;s ranking is emoticons. Emoticons is an important component in the sentences to connect human emotions and also make sentences more vivid. For instance, Figure&#x00A0;<a class="fig" href="#fig5">5</a> (d) in the user study, the voting of <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> outperforms Human at the first rank (39.1% vs. 34.8%). Relevance between comments and images takes the second concern of people&#x0027;s ranking. Objects mentioned in sentences should not be trivial or mismatch in the photos. For example, Figure&#x00A0;<a class="fig" href="#fig5">5</a> (c), <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>     </span> captures the outfit (coat) and floating hair resulting in the same voting as human in rank 1 (39.1%) in the user study. To sum up, our style-weight makes captioning model mimic human style and generates human-like comments which most people agree with in our user study.</p>     <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">Result of user study. <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>       </span>&#x2019;s comments are more likely to be regarded as human than other methods.</span>     </div>     <table class="table">      <tbody>       <tr>        <td style="text-align:center;">Ranking</td>        <td style="text-align:center;">Human</td>        <td style="text-align:center;">NC</td>        <td style="text-align:center;">Attention</td>        <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\textrm {NSC}_\textrm {{NC}}$</span>        </span>        </td>       </tr>       <tr>        <td style="text-align:center;">Rank 1</td>        <td style="text-align:center;">46.1%</td>        <td style="text-align:center;">10.8%</td>        <td style="text-align:center;">6.3%</td>        <td style="text-align:center;">36.8%</td>       </tr>       <tr>        <td style="text-align:center;">Rank 2</td>        <td style="text-align:center;">24.5%</td>        <td style="text-align:center;">21.4%</td>        <td style="text-align:center;">14.4%</td>        <td style="text-align:center;">39.8%</td>       </tr>       <tr>        <td style="text-align:center;">Rank 3</td>        <td style="text-align:center;">18.1%</td>        <td style="text-align:center;">31.9%</td>        <td style="text-align:center;">34.3%</td>        <td style="text-align:center;">15.7%</td>       </tr>       <tr>        <td style="text-align:center;">Rank 4</td>        <td style="text-align:center;">11.3%</td>        <td style="text-align:center;">35.9%</td>        <td style="text-align:center;">45.0%</td>        <td style="text-align:center;">7.8%</td>       </tr>      </tbody>     </table>     </div>    </section>   </section>   <section id="sec-15">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Conclusions</h2>     </div>    </header>    <p>We present style-weight that greatly influences on current captioning models to immerse into human online society. Also, we contribute our dataset NetiLook, which is a brand new large-scale clothing dataset, to achieve netizen style commenting with style-weight. An image captioning model automatically generates characteristic comments for user-contributed fashion photos. NSC leverages the advantage of style-weight which can keep long-range dependencies to achieve vivid &#x201C;netizen&#x201D; style comments. Experiments on Flickr30k and NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning task both in accuracy (quantified by conventional measures of image captioning) and diversity (quantified by our proposed measures). A user study is carried showing that our proposed idea can generate sentences with human-like quality. It is worth noting that our proposed approach can be applied on other fields (e.g., conversation response generation or question-answering system) to help generate sentences with various styles by the idea of style-weight. Moreover, NetiLook contains abundant attributes, researchers are able to use those attributes to build a more comprehensive system. For example, comments from different genders in future work. We believe that the integration of image captioning models, style-weight and the dataset proposed in this paper will have a great impact on related research domains.</p>   </section>   <section id="sec-16">    <header>     <div class="title-info">     <h2>      <span class="section-number">8</span> Acknowledgement</h2>     </div>    </header>    <p>This work was supported in part by Microsoft Research Asia and the Ministry of Science and Technology, Taiwan, under Grant MOST 105-2218-E-002-032. We also benefit from the grants from NVIDIA and the NVIDIA DGX-1 AI Supercomputer and the discussions with Dr. Ruihua Song, Microsoft Research Asia.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Lisa Anne&#x00A0;Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, and Trevor Darrell. 2016. Deep compositional captioning: Describing novel object categories without paired training data. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 1&#x2013;10.</li>     <li id="BibPLXBIB0002" label="[2]">David&#x00A0;M Blei, Andrew&#x00A0;Y Ng, and Michael&#x00A0;I Jordan. 2003. Latent dirichlet allocation. <em>      <em>Journal of machine Learning research</em>     </em>3, Jan (2003), 993&#x2013;1022.</li>     <li id="BibPLXBIB0003" label="[3]">Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue. 2012. Large-scale validation and analysis of interleaved search evaluation. <em>      <em>ACM Transactions on Information Systems (TOIS)</em>     </em>30, 1 (2012), 6.</li>     <li id="BibPLXBIB0004" label="[4]">KuanTing Chen, Kezhen Chen, Peizhong Cong, Winston&#x00A0;H Hsu, and Jiebo Luo. 2015. Who are the devils wearing prada in new york city?. In <em>      <em>Proceedings of the 23rd ACM international conference on Multimedia</em>     </em>. ACM, 177&#x2013;180.</li>     <li id="BibPLXBIB0005" label="[5]">Minghai Chen, Guiguang Ding, Sicheng Zhao, Hui Chen, Qiang Liu, and Jungong Han. 2017. Reference Based LSTM for Image Captioning.. In <em>      <em>AAAI</em>     </em>. 3981&#x2013;3987.</li>     <li id="BibPLXBIB0006" label="[6]">Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010. What does classifying more than 10,000 image categories tell us?<em>      <em>ECCV 2010</em>     </em> (2010), 71&#x2013;84.</li>     <li id="BibPLXBIB0007" label="[7]">Jeffrey Donahue, Lisa Anne&#x00A0;Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recognition and description. In <em>      <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>     </em>. 2625&#x2013;2634.</li>     <li id="BibPLXBIB0008" label="[8]">Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh&#x00A0;K Srivastava, Li Deng, Piotr Doll&#x00E1;r, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John&#x00A0;C Platt, <em>et al.</em> 2015. From captions to visual concepts and back. In <em>      <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>     </em>. 1473&#x2013;1482.</li>     <li id="BibPLXBIB0009" label="[9]">Centre for Retail&#x00A0;Research. 2017. Online Retailing: Britain, Europe, US and Canada 2017. www.retailresearch.org/onlineretailing.php. (2017). Accessed: 2018-01-25.</li>     <li id="BibPLXBIB0010" label="[10]">M Hadi&#x00A0;Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander&#x00A0;C Berg, and Tamara&#x00A0;L Berg. 2015. Where to buy it: Matching street clothing photos in online shops. In <em>      <em>Proceedings of the IEEE International Conference on Computer Vision</em>     </em>. 3343&#x2013;3351.</li>     <li id="BibPLXBIB0011" label="[11]">Shintami&#x00A0;C Hidayati, Kai-Lung Hua, Wen-Huang Cheng, and Shih-Wei Sun. 2014. What are the fashion trends in new york?. In <em>      <em>Proceedings of the 22nd ACM international conference on Multimedia</em>     </em>. ACM, 197&#x2013;200.</li>     <li id="BibPLXBIB0012" label="[12]">Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. <em>      <em>arXiv preprint arXiv:1508.01991</em>     </em>(2015).</li>     <li id="BibPLXBIB0013" label="[13]">Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos, Greg Corrado, L&#x00E1;szl&#x00F3; Luk&#x00E1;cs, Marina Ganea, Peter Young, <em>et al.</em> 2016. Smart reply: Automated response suggestion for email. <em>      <em>arXiv preprint arXiv:1606.04870</em>     </em>(2016).</li>     <li id="BibPLXBIB0014" label="[14]">Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 3128&#x2013;3137.</li>     <li id="BibPLXBIB0015" label="[15]">Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A diversity-promoting objective function for neural conversation models. <em>      <em>arXiv preprint arXiv:1510.03055</em>     </em>(2015).</li>     <li id="BibPLXBIB0016" label="[16]">Linghui Li, Sheng Tang, Lixi Deng, Yongdong Zhang, and Qi Tian. 2017. Image Caption with Global-Local Attention.. In <em>      <em>AAAI</em>     </em>. 4133&#x2013;4139.</li>     <li id="BibPLXBIB0017" label="[17]">Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll&#x00E1;r, and C&#x00A0;Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In <em>      <em>European Conference on Computer Vision</em>     </em>. Springer, 740&#x2013;755.</li>     <li id="BibPLXBIB0018" label="[18]">Chenxi Liu, Junhua Mao, Fei Sha, and Alan&#x00A0;L Yuille. 2017. Attention Correctness in Neural Image Captioning.. In <em>      <em>AAAI</em>     </em>. 4176&#x2013;4182.</li>     <li id="BibPLXBIB0019" label="[19]">Si Liu, Jiashi Feng, Csaba Domokos, Hui Xu, Junshi Huang, Zhenzhen Hu, and Shuicheng Yan. 2014. Fashion parsing with weak color-category labels. <em>      <em>IEEE Transactions on Multimedia</em>     </em>16, 1 (2014), 253&#x2013;265.</li>     <li id="BibPLXBIB0020" label="[20]">Si Liu, Jiashi Feng, Zheng Song, Tianzhu Zhang, Hanqing Lu, Changsheng Xu, and Shuicheng Yan. 2012. Hi, magic closet, tell me what to wear!. In <em>      <em>Proceedings of the 20th ACM international conference on Multimedia</em>     </em>. ACM, 619&#x2013;628.</li>     <li id="BibPLXBIB0021" label="[21]">Si Liu, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan. 2012. Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set. In <em>      <em>Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</em>     </em>. IEEE, 3330&#x2013;3337.</li>     <li id="BibPLXBIB0022" label="[22]">Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 1096&#x2013;1104.</li>     <li id="BibPLXBIB0023" label="[23]">Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2016. Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning. <em>      <em>arXiv preprint arXiv:1612.01887</em>     </em>(2016).</li>     <li id="BibPLXBIB0024" label="[24]">Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. 2014. Deep captioning with multimodal recurrent neural networks (m-rnn). <em>      <em>arXiv preprint arXiv:1412.6632</em>     </em>(2014).</li>     <li id="BibPLXBIB0025" label="[25]">Jonghwan Mun, Minsu Cho, and Bohyung Han. 2017. Text-Guided Attention Model for Image Captioning.. In <em>      <em>AAAI</em>     </em>. 4233&#x2013;4239.</li>     <li id="BibPLXBIB0026" label="[26]">Nirmaldasan. 2008. The Average Sentence Length. https://strainindex.wordpress.com/2008/07/28/the-average-sentence-length/. (July 2008). Accessed: 2017-04-06.</li>     <li id="BibPLXBIB0027" label="[27]">Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In <em>      <em>Proceedings of the 40th annual meeting on association for computational linguistics</em>     </em>. Association for Computational Linguistics, 311&#x2013;318.</li>     <li id="BibPLXBIB0028" label="[28]">Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using Amazon&#x0027;s Mechanical Turk. In <em>      <em>Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&#x0027;s Mechanical Turk</em>     </em>. Association for Computational Linguistics, 139&#x2013;147.</li>     <li id="BibPLXBIB0029" label="[29]">Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. 2017. Generating Long and Diverse Responses with Neural Conversation Models. <em>      <em>arXiv preprint arXiv:1701.03185</em>     </em>(2017).</li>     <li id="BibPLXBIB0030" label="[30]">Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, and Raquel Urtasun. 2015. Neuroaesthetics in fashion: Modeling the perception of fashionability. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 869&#x2013;877.</li>     <li id="BibPLXBIB0031" label="[31]">Subhashini Venugopalan, Lisa&#x00A0;Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2017. Captioning images with diverse objects. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>.</li>     <li id="BibPLXBIB0032" label="[32]">Ashwin&#x00A0;K Vijayakumar, Michael Cogswell, Ramprasath&#x00A0;R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models. <em>      <em>arXiv preprint arXiv:1610.02424</em>     </em>(2016).</li>     <li id="BibPLXBIB0033" label="[33]">Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>. 3156&#x2013;3164.</li>     <li id="BibPLXBIB0034" label="[34]">Cheng Wang, Haojin Yang, Christian Bartz, and Christoph Meinel. 2016. Image captioning with deep bidirectional LSTMs. In <em>      <em>Proceedings of the 2016 ACM on Multimedia Conference</em>     </em>. ACM, 988&#x2013;997.</li>     <li id="BibPLXBIB0035" label="[35]">Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron&#x00A0;C Courville, Ruslan Salakhutdinov, Richard&#x00A0;S Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.. In <em>      <em>ICML</em>     </em>, Vol.&#x00A0;14. 77&#x2013;81.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://lookbook.nu">lookbook.nu</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break" href="https://mashyu.github.io/NSC">https://mashyu.github.io/NSC</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186354">https://doi.org/10.1145/3184558.3186354</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
