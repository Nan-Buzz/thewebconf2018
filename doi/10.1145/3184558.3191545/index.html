<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.A Speech Quality Assessment Case Study</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3184558.3191545"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191545'>https://doi.org/10.1145/3184558.3191545</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191545'>https://w3id.org/oa/10.1145/3184558.3191545</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.A Speech Quality Assessment Case Study</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Rafael Zequeira</span> <span class="surName">Jiménez</span> Quality and Usability Lab Technische Universität Berlin, Ernst-Reuter-Platz 7Berlin 10587Germany, <a href="mailto:rafael.zequeira@tu-berlin.de">rafael.zequeira@tu-berlin.de</a>
        </div>
        <div class="author">
          <span class="givenName">Laura Fernández</span> <span class="surName">Gallardo</span> Quality and Usability Lab Technische Universität Berlin, Ernst-Reuter-Platz 7Berlin 10587Germany, <a href="mailto:laura.fernandezgallardo@tu-berlin.de">laura.fernandezgallardo@tu-berlin.de</a>
        </div>
        <div class="author">
          <span class="givenName">Sebastian</span> <span class="surName">Möller</span> Quality and Usability Lab Technische Universität Berlin, Ernst-Reuter-Platz 7Berlin 10587Germany, <a href="mailto:sebastian.moeller@tu-berlin.de">sebastian.moeller@tu-berlin.de</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191545" target="_blank">https://doi.org/10.1145/3184558.3191545</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, “trapping question” or “outliers detection” assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results’ validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Human-centered computing</strong> → <strong>User studies;</strong> <em>Field studies;</em> <em>Web-based interaction;</em> <em>Empirical studies in HCI;</em> • <strong>General and reference</strong> → Reliability;</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>outliers detection; trapping questions; crowdsourcing; speech quality assessment; gold-strandard questions; users reliability; data validity</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Rafael Zequeira Jiménez, Laura Fernández Gallardo, and Sebastian Möller. 2018. Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.A Speech Quality Assessment Case Study. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018 (WWW ’18 Companion),</em> <em>Lyon, France. ACM, New York, NY, USA</em> 4 Pages. <a href="https://doi.org/10.1145/3184558.3191545" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3191545</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>The crowdsourcing (CS) paradigm offers small tasks to anonymous users on the Internet that normally require human intelligence for being resolved. The users (crowd-workers) can perform such tasks from their computer or mobile device and get rewarded after completion. Nowadays, CS has been adopted in multiple domains and researchers have found a fast, low cost, and scalable method to gather more labeled data than traditional approaches.</p>
      <p>Subjective speech quality assessment experiments have been traditionally conducted under controlled laboratory conditions with high-end equipment. This approach permits an optimal control over the study setup to the detriment of the number of participants. In contrast, CS provides the means to reach a wider and diverse audience to collect quality ratings at a fraction of the cost and time than traditional practices of in-Lab annotations. However, it remains the question of whether the collected ratings in an online platform are still valid and reliable. CS has been typically found to deliver noisier data, therefore, different quality control mechanisms has been proposed to ensure reliable results and overcome this challenge&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]. Accurate speech quality ratings are of a main importance for telecommunication systems’ providers, as such data is used to train models to predict the perceived quality of the different codecs applied to the speech signal. Thus, reliable ratings are essential for a proper evaluation of their systems.</p>
      <p>The rest of the paper is structured as follows: the next section reviews existing work that used CS to assess the quality of different multimedia contents, and the impact of the quality control mechanism employed. Section&nbsp;<a class="sec" href="#sec-6">3</a> presents the experiment setup as well as the database employed. The results of contrasting the laboratory (Lab) with the CS outcomes are exposed in Section&nbsp;<a class="sec" href="#sec-9">4</a> with our approach to ensure accurate results. Finally, Section&nbsp;<a class="sec" href="#sec-10">5</a> concludes and outlines our directions for future work.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related Work</h2>
        </div>
      </header>
      <p>Trapping questions (TQ) can be seen as gold standard questions that can be used to identify inattentive or willfully cheating workers. Work in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] used mobile-CS to collect quality ratings of speech samples. The authors evaluated different types of TQ and examined their influence on the gathered ratings. An increase in the correlation was found between the MOS ratings collected in the Lab and the CS results when employing TQ (<em>ρ</em> = 0.886 to <em>ρ</em> = 0.909). However, the authors did not apply any outliers detection mechanism, and quality control relied on just discarding the ratings from the workers deemed unreliable by the TQ setup.</p>
      <p>[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] investigates the viability of a web-CS platform for the subjective evaluation of audio with intermediate impairments. The authors used a screening task as control mechanism to account for workers’ hearing abilities and listening environments. Results in terms of overall audio quality were correlated (<em>ρ</em> = 0.78) to previous ratings collected in lab.</p>
      <p>Research in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] proposes a CS test methodology to assess the user perceived quality of Internet applications like YouTube. The authors employed gold standard questions to identify unreliable workers. Their approach lead them to improve significantly the intra- and inter- rater reliability by discarding the ratings from untrustworthy workers. Yet, such a filtering technique reduced the number of valid crowd-workers by approx. 25% and made them discard three fourth of the subjective ratings collected. As well authors of&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] reduced the number of workers by 34.3% to be considered for a CS study in image aesthetic appeal. They filtered out the outliers following a multi-fold technique, and removed users based on verification questions.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experiment Settings</h2>
        </div>
      </header>
      <p>This paper investigates whether outliers detection, trapping questions or a combination of both, improve the results accuracy in the context of a speech quality assessment task. We also determine whether it is possible to optimize such quality control mechanism without discarding a big number of users and/or ratings, which may lead to poor performance in terms of costs and time.</p>
      <p>The analysis is conducted on data from our prior work in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>], which analyses the influence of the number of presented speech stimuli on the reliability of listeners’ ratings. Thus, a CS study was conducted with 53 workers. They were asked to rate speech stimuli with respect to their overall quality on a 5-point scale in accordance with the ITU-T Rec. P.800&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. We examine results’ accuracy in terms of correlations to previous ratings collected in Lab.</p>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Speech Material</h3>
          </div>
        </header>
        <p>The stimuli employed in our study were taken from the database number 501 from the ITU-T Rec. P.863&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>] competition. This database contains different degradation conditions in accordance to the ITU-T Rec. P.863. Four native German speakers were recorded per condition uttering four different sentences in German. In total, 200 speech files (8s to 10s long) were arranged accounting for 50 degradation conditions, e.g. different audio bandwidths, temporal clipping, speech coding at various bitrates, diverse types of ambient background noise, frequency distortions and, combinations of these degradations.</p>
        <p>The database contains subjective quality assessments to the 200 stimuli made by 24 different native German listeners, in accordance with the ITU-T Rec. P.800&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>]. The Mean Opinion Scores (MOS) for each stimulus are taken as a reference for the analysis presented in this paper (from now on referred as ”Lab-MOS”). A Kendall's&nbsp;<em>ζ</em> coefficient&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>] calculated among the listeners, revealed a statistically significant agreement between the Lab participants when assessing all the speech stimuli <em>W</em> = 0.614 (<em>p</em> &lt; 0.001). This coefficient can only be calculated if all samples are presented to all listeners, which is not the case in CS.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Crowdsourcing Study</h3>
          </div>
        </header>
        <p>We used the clickworker<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> CS platform to conduct our experiment. Most of its users are from German speaking countries, which was a good fit for our study needs. However, clickworker does not support for audio playback (as of February 2018), we then created a HTML JavaScript based framework to administer the test to the workers. We used crowdcrafting&nbsp;<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> as interface to display the test and a Node.js server for the data collection.</p>
        <p>Inspired by work in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>], our CS experiment included a Qualification phase that permitted to adjust the device volume to a comfortable level, and regulated the use of headphones. A short math exercise with digits panning left to right in stereo controlled for two-eared usage. Six different audio files were prepared for this and presented randomly every time the worker executed the Qualification. This questions served as well as quality control to prevent inattentive workers from participating in our study during 12 hours.</p>
        <p>Upon successful execution of the Qualification phase, or after 12 hours in case of failure, workers were presented immediately with the speech quality assessment task (SQAT). The study architecture can be seen in Figure&nbsp;<a class="fig" href="#fig1">1</a>. The SQAT permitted the listeners to assess the overall quality of 20 speech samples on a 5-point scale, see Figure&nbsp;<a class="fig" href="#fig2">2</a>. Workers could not provide their opinion on the scale unless they listened first to the speech sample. They were not able to go forward until the audio was played completely, and they could listen to each speech sample as many times as they wished.</p>
        <p>To evaluate the entire dataset, the listeners could participate in the study up to 10 times (with the restriction of only one execution every 12 hours). In addition, two trapping questions (TQ) were inserted randomly within the first five stimuli from every ten speech samples. Like in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>], these TQ presented a stimulus taken from the database but interrupted after four seconds, listeners were then informed about the importance of their work and were asked to select an specific item on the scale. The TQs’ GUI was the same as the rest of presented stimuli, see Figure&nbsp;<a class="fig" href="#fig2">2</a>. When workers failed to answer correctly the TQ, the ratings from the set of those ten stimuli where considered unreliable and discarded. More details on the Qualification and the SQAT can be found in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>].</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Results</h2>
        </div>
      </header>
      <p>87 workers in total participated in the CS study. 8 of them answered wrong the Qualification phase and were granted with a 12 hours window that prevented them from conducting our experiment. From those 8 workers, 6 returned back to our study and provided reliable ratings. Overall, 53 crowd-workers (balanced age) yielded 4840 ratings, more details on their demographics is presented in Table&nbsp;<a class="tbl" href="#tab1">1</a>. Surprisingly, all the workers answered correctly the TQ in the SQAT, and all the 4840 ratings were deemed reliable. The rest of the listeners either started out the study and did not finish, or dropped off after reading the instructions (we don't have an explanation for this behavior).</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191545/images/www18companion-284-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">Study architecture, workers could participate only once every 12 hours.</span>
        </div>
      </figure>
      <figure id="fig2">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191545/images/www18companion-284-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class="figure-title">Graphical interface presented to the workers for the SQAT. The text translate from German: “Speech Quality” and “Rating”. The scale (in descending order): “Excellent”, “Good”, “Fair”, “Poor” and “Bad”.</span>
        </div>
      </figure>
      <p></p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class="table-title">Demographic information of the 53 workers that executed properly the SQAT. Values are expressed in percentages. “NP” stands for <em>“Not Provided”</em>, e.g. workers did not provide that information.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td colspan="2" style="text-align:center;">
                Language
                <hr />
              </td>
            </tr>
            <tr>
              <td style="text-align:center;">German</td>
              <td style="text-align:center;">96.2</td>
            </tr>
            <tr>
              <td style="text-align:center;">NP</td>
              <td style="text-align:center;">3.8</td>
            </tr>
          </tbody>
          <tbody>
            <tr>
              <td colspan="2" style="text-align:center;">
                Country
                <hr />
              </td>
            </tr>
            <tr>
              <td style="text-align:center;">Germany</td>
              <td style="text-align:center;">98.1</td>
            </tr>
            <tr>
              <td style="text-align:center;">Austria</td>
              <td style="text-align:center;">1.9</td>
            </tr>
          </tbody>
          <tbody>
            <tr>
              <td colspan="2" style="text-align:center;">
                Gender
                <hr />
              </td>
            </tr>
            <tr>
              <td style="text-align:center;">Male</td>
              <td style="text-align:center;">60.4</td>
            </tr>
            <tr>
              <td style="text-align:center;">Female</td>
              <td style="text-align:center;">39.6</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>The 4840 gathered ratings account for 24 to 26 assessments made by different listeners on each of the 200 speech stimuli. To determine the validity of this gathered data, a Spearman's rank-order correlation was run to assess the relationship between the Laboratory and the CS ratings. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatter-plot. Also, the Root Mean Square Error (RMSE) was calculated between the ratings in the Lab and in CS. Strong positive correlation was found between the Lab-MOS and the CS-MOS, <em>ρ</em> = 0.864~(<em>p</em> &lt; .001), as well as a low <em>RMSE</em> = 0.474. This outcome motivates the use of CS for collecting reliable annotations of speech quality, as an alternative to a more controlled environment like in Lab test.</p>
      <p>Next, we examine whether filtering out the contributions from “unreliable workers” might improve or not the accuracy of our results. Work in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>] recommends the use of TQ or other control mechanisms as a mean to identify untrustworthy crowd-workers and discard all of their answers. Such a technique resulted to be effective in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] and improved slightly the results in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>].</p>
      <p>In this work, we labeled a worker as unreliable or untrustworthy when s/he failed the TQ in the SQAT, or the Qualification phase more than once. Figure&nbsp;<a class="fig" href="#fig3">3</a> exposes such cases (workers are assigned with <em>W1</em> to <em>W8</em>). Thus, when discarding the contributions from <em>W4</em>, <em>W5</em> and <em>W7</em> (320 ratings in total) (<em>W6</em> did not executed the SQAT), we account for 4520 ratings in total, which represent 21 to 25 assessments per speech sample made by different listeners. We call this method: <em>“filtering by trapping question”</em> (F-TQ). Calculating the Spearman's correlation under these conditions throw a slightly decreased score of: <em>ρ</em> = 0.862~(<em>p</em> &lt; .001), and even worst if we discard all the workers that failed the Qualification (F-TQ’): <em>ρ</em> = 0.854~(<em>p</em> &lt; .001).</p>
      <p>Filtering our data based on the TQ from the Qualification phase did not improved the results. This outcome shows that this method is not always valid, and workers failing once a quality control mechanism might actually provide reliable ratings in further executions of the study.</p>
      <p>Moreover, we investigate whether applying outliers detection to our data improves the results. According to the labeling rule introduced in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>], we removed the ratings with a distance from the median higher than 2.2 · <em>IQR</em> (interquartile range). We executed this analysis for each speech stimuli of the dataset and 122 ratings were identified as extreme outliers and were discarded. We refer to this method as: <em>“filtering by outliers detection&nbsp;1”</em> (F-OD1). Boxplots in Figure&nbsp;<a class="fig" href="#fig4">4</a> represents ten speech stimuli chosen arbitrarily to showcase some of the outliers and extreme outliers in our study. The resulting Spearman's correlation after applying F-OD1 was then: <em>ρ</em> = 0.863~(<em>p</em> &lt; .001), still not better than the first coefficient calculated when no data was removed.</p>
      <p>In addition, we eliminated all the ratings (1480 in total) from the 12 workers that were identified as outliers according to&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] (e.g. their ratings were outliers three times or more). We call this method <em>“filtering by outliers detection 2”</em> (F-OD2). An improve in the correlation coefficient was seen this time: <em>ρ</em> = 0.867~(<em>p</em> &lt; .001).</p>
      <p>Finally, we examine whether combining F-TQ, F-OD1 and F-OD2 leads to even more accurate results. We refer to this approach as: F-TQ-OD. To this end, we applied F-OD1 and F-OD2 to our data and discarded 1529 ratings. In addition, we identified the outliers made by all workers that failed the TQ in the Qualification phase. This analysis led us to drop 17 data points more. The resulting 3294 ratings represent 13 to 21 assessments from different listeners to each of the 200 speech files. The Spearman's rank-order correlation to the Lab-MOS is now higher compared to those previously calculated: <em>ρ</em> = 0.868~(<em>p</em> &lt; .001). Table&nbsp;<a class="tbl" href="#tab2">2</a> presents a summary of the correlations achieved with each of the methods, and the ratings discarded in each case from the amount of 4840 initially collected.</p>
      <p>F-TQ-OD slightly outperforms the approach in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] and in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] in terms of the amount of workers and ratings discarded. Applying F-TQ-OD in our study lead to filter out 22% of the crowd-workers and 31.9% of the collected ratings: less than in&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>], where 25% of the workers and 75% of the ratings were discarded, and also less in comparison to&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>], where 34.3% of the workers were removed from the final analysis.</p>
      <p>While F-TQ-OD led to more accurate results, it was at the expenses of discarding a considerable amount of data points. This translates in an increase of the experiment cost in case a certain number of assessments on the dataset is needed. However, this outcome also suggests that valid results are also possible with less iterations on the data (between 13 and 21 in our study).</p>
      <figure id="fig3">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191545/images/www18companion-284-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 3:</span> <span class="figure-title">Amount of times a worker failed the Qualification phase.</span>
        </div>
      </figure>
      <figure id="fig4">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191545/images/www18companion-284-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class="figure-title">Boxplots of ratings for ten speech stimuli of the dataset. Lines extend to 1.5 times the interquartile range, circles indicate outliers, asterisks illustrate extreme outliers and notches depict 95% confidence intervals.</span>
        </div>
      </figure>
      <p></p>
      <div class="table-responsive" id="tab2">
        <div class="table-caption">
          <span class="table-number">Table 2:</span> <span class="table-title">Correlation (<em>ρ</em>) and Root Mean Square Error (RMSE) between the Lab-MOS and the CS-MOS when filtering by outliers according to&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] and&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] (F-OD1 and F-OD2, respectively), and when filtering by TQ (F-TQ).</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">Method</th>
              <th style="text-align:center;">Ratings discarded</th>
              <th style="text-align:center;"><em>ρ</em></th>
              <th style="text-align:center;"><em>RMSE</em></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">-</td>
              <td style="text-align:center;">0</td>
              <td style="text-align:center;">0.864*</td>
              <td style="text-align:center;">0.474</td>
            </tr>
            <tr>
              <td style="text-align:center;">F-TQ</td>
              <td style="text-align:center;">320</td>
              <td style="text-align:center;">0.862*</td>
              <td style="text-align:center;">0.476</td>
            </tr>
            <tr>
              <td style="text-align:center;">F-TQ’</td>
              <td style="text-align:center;">780</td>
              <td style="text-align:center;">0.854*</td>
              <td style="text-align:center;">0.480</td>
            </tr>
            <tr>
              <td style="text-align:center;">F-OD1</td>
              <td style="text-align:center;">122</td>
              <td style="text-align:center;">0.863*</td>
              <td style="text-align:center;">0.477</td>
            </tr>
            <tr>
              <td style="text-align:center;">F-OD2</td>
              <td style="text-align:center;">1480</td>
              <td style="text-align:center;">0.867*</td>
              <td style="text-align:center;">0.474</td>
            </tr>
            <tr>
              <td style="text-align:center;">F-TQ-OD</td>
              <td style="text-align:center;">1546</td>
              <td style="text-align:center;">0.868*</td>
              <td style="text-align:center;">0.479</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td colspan="4" style="text-align:left;">
                *<em>p</em> &lt; 0.001
                <hr />
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusions</h2>
        </div>
      </header>
      <p>This work, proposes a method to filter unreliable data gathered in Crowdsourcing studies. Our method combines outliers detection mechanisms&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] with trapping questions to identify invalid data points and untrustworthy workers. Our approach have been tested with 4840 ratings collected in a speech quality assessment study conducted in a crowdsourcing platform. While the proposed method outperforms outliers detection and trapping questions when applied independently, further testing would be required to understand to which extend this mechanism can be applied, and for which types of experiments.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Daniel Archambault, Helen&nbsp;C Purchase, and Tobias Hoßfeld. 2017. Evaluation in the Crowd: An Introduction. In <em><em>Evaluation in the Crowd. Crowdsourcing and Human-Centered Experiments: Dagstuhl Seminar 15481, Dagstuhl Castle, Germany, November 22 – 27, 2015, Revised Contributions</em></em> , Daniel Archambault, Helen Purchase, and Tobias Hoßfeld (Eds.). Springer International Publishing, Cham, 1–5. <a class="link-inline force-break" href="https://doi.org/10.1007/978-3-319-66435-4_1" target="_blank">https://doi.org/10.1007/978-3-319-66435-4_1</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">Mark Cartwright, Bryan Pardo, Gautham&nbsp;J. Mysore, and Matt Hoffman. 2016. Fast and Easy Crowdsourced Perceptual Audio Evaluation. In <em><em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em></em> . 619–623. <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2016.7471749" target="_blank">https://doi.org/10.1109/ICASSP.2016.7471749</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">David&nbsp;C Hoaglin and Boris Iglewicz. 1987. Fine-tuning some resistant rules for outlier labeling. <em><em>J. Amer. Statist. Assoc.</em></em> 82, 400 (1987), 1147–1149.</li>
        <li id="BibPLXBIB0004" label="[4]">Tobias Hoßfeld, Matthias Hirth, Judith Redi, Filippo Mazza, Pavel Korshunov, Babak Naderi, Michael Seufert, Bruno Gardlo, Sebastian Egger, and Christian Keimel. 2014. Best Practices and Recommendations for Crowdsourced QoE - Lessons learned from the Qualinet Task Force ”Crowdsourcing”. (oct 2014).</li>
        <li id="BibPLXBIB0005" label="[5]">Tobias Hoßfeld, Michael Seufert, Matthias Hirth, Thomas Zinner, Phuoc Tran-Gia, and Raimund Schatz. 2011. Quantification of YouTube QoE via Crowdsourcing. In <em><em>2011 IEEE International Symposium on Multimedia</em></em> . 494–499. <a class="link-inline force-break" href="https://doi.org/10.1109/ISM.2011.87" target="_blank">https://doi.org/10.1109/ISM.2011.87</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">ITU-T Recommandation P.800. 1996. <em><em>Methods for subjective determination of transmission quality</em></em> . International Telecommunication Union, Geneva.</li>
        <li id="BibPLXBIB0007" label="[7]">ITU-T Recommandation P.863. 2014. <em><em>Perceptual objective listening quality assessment</em></em> . International Telecommunication Union, Geneva.</li>
        <li id="BibPLXBIB0008" label="[8]">Maurice&nbsp;George Kendall. 1970. <em><em>Rank Correlation Methods</em>(4th ed.)</em>. Charles Griffin.</li>
        <li id="BibPLXBIB0009" label="[9]">Babak Naderi, Tim Polzehl, Ina Wechsung, Friedemann Köster, and Sebastian Möller. 2015. Effect of Trapping Questions on the Reliability of Speech Quality Judgments in a Crowdsourcing Paradigm. In <em><em>Interspeech</em></em> . ISCA, 2799–2803.</li>
        <li id="BibPLXBIB0010" label="[10]">Judith Redi and Isabel Povoa. 2014. Crowdsourcing for Rating Image Aesthetic Appeal: Better a Paid or a Volunteer Crowd?. In <em><em>International ACM Workshop on Crowdsourcing for Multimedia</em></em> (<em>CrowdMM ’14</em>). 25–30. <a class="link-inline force-break" href="https://doi.org/10.1145/2660114.2660118" target="_blank">https://doi.org/10.1145/2660114.2660118</a>
        </li>
        <li id="BibPLXBIB0011" label="[11]">Judith Redi, Ernestasia Siahaan, Pavel Korshunov, Julian Habigt, and Tobias Hoßfeld. 2015. When the Crowd Challenges the Lab: Lessons Learnt from Subjective Studies on Image Aesthetic Appeal. <em><em>Fourth International Workshop on Crowdsourcing for Multimedia</em></em> (2015), 33–38. <a class="link-inline force-break" href="https://doi.org/10.1145/2810188.2810194" target="_blank">https://doi.org/10.1145/2810188.2810194</a>
        </li>
        <li id="BibPLXBIB0012" label="[12]">Barbara&nbsp;G Tabachnick and Linda&nbsp;S Fidell. 2007. <em><em>Using Multivariate Statistics</em></em> . Allyn &amp; Bacon/Pearson Education.</li>
        <li id="BibPLXBIB0013" label="[13]">Rafael Zequeira Jiménez, Laura Fernández Gallardo, and Sebastian Möller. 2017. Scoring Voice Likability using Pair-Comparison: Laboratory vs. Crowdsourcing Approach. In <em><em>Ninth International Conference on Quality of Multimedia Experience (QoMEX)</em></em> . 1–3. <a class="link-inline force-break" href="https://doi.org/10.1109/QoMEX.2017.7965678" target="_blank">https://doi.org/10.1109/QoMEX.2017.7965678</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Rafael Zequeira Jiménez, Laura Fernández Gallardo, and Sebastian Möller. 2018. Influence of Number of Stimuli for Subjective Speech Quality Assessment in Crowdsourcing. In <em><em>accepted for: 10th International Conference on Quality of Multimedia Experience (QoMEX)</em></em> .</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="https://www.clickworker.com">https://www.clickworker.com</a> last accessed February 2018</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>https://crowdcrafting.org last accessed February 2018</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3191545">https://doi.org/10.1145/3184558.3191545</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
