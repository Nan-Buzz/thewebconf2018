<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Contextual Word Embedding: A Case Study in Clustering
  Tweets about Emergency Situations</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Contextual Word Embedding: A Case
          Study in Clustering Tweets about Emergency
          Situations</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Debasis</span> <span class=
          "surName">Ganguly</span> IBM Research Labs, Dublin,
          Ireland
        </div>
        <div class="author">
          <span class="givenName">Kripabandhu</span> <span class=
          "surName">Ghosh</span> IIT Kanpur, India
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186935"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186935</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Effective clustering of short documents, such as
        tweets, is difficult because of the lack of sufficient
        semantic context. Word embedding is a technique that is
        effective in addressing this lack of semantic context.
        However, the process of word vector embedding, in turn,
        relies on the availability of sufficient contexts to learn
        the word associations. To get around this problem, we
        propose a novel word vector training approach that
        leverages topically similar tweets to better learn the word
        associations. We test our proposed word embedding approach
        by clustering a collection of tweets on disasters. We
        observe that the proposed method improves clustering
        effectiveness by up to 14%.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Clustering and classification;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Transformed Word Embedding;
          Tweets Clustering</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Debasis Ganguly and Kripabandhu Ghosh. 2018. Contextual
          Word Embedding: A Case Study in Clustering Tweets about
          Emergency Situations. In <em>WWW '18 Companion: The 2018
          Web Conference Companion,</em> <em>April 23–27,
          2018,</em> <em>Lyon, France. ACM, New York, NY, USA</em>
          2 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186935" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186935</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Automated analysis of social media posts about requirement
      and availability of resources (e.g. food and medicine) has
      been used to detect and continuously monitor disaster events
      such as earthquakes and floods [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>]. Clustering a stream of large volumes
      of tweets can potentially be helpful to analyze tweets
      pertaining to different emergency situations. Clustering may
      further help to identify tweets related to different aspects
      or topics of information requested or supplied during an
      emergency situation, e.g., requesting or declaring
      availability for food, medicine etc. Conventional text-based
      approaches for tweet clustering is likely to suffer from the
      lack of sufficient context and can lead to vocabulary
      mismatch problems due to the short length of the text and
      informal nature of the content. A semantically driven tweet
      clustering method, e.g. one which leverages word vector
      embedding, is likely to reduce this vocabulary mismatch
      problem. Generally speaking, embedding approaches, e.g.
      word2vec [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>],
      learn semantic associations between words or n-grams from the
      contexts around them. However, for the case of short and
      informal documents, such as tweets, this context is likely to
      be insufficient and noisy for effectively learning the word
      associations. This, in turn, may degrade the quality of
      clustering.</p>
      <p>To alleviate the problem of short and noisy contexts of
      tweets for the purpose of word vector learning, we propose a
      linear transformation based approach, similar in principle to
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>], that aims
      to improve the word vector representations by extending this
      context from other topically similar tweets. More precisely,
      we learn a transformation matrix that aims to maximize the
      cosine similarity between a current word of a tweet and other
      constituent words from similar tweets.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span>
          Methodology</h2>
        </div>
      </header>
      <p>In word2vec, a sliding window comprised of the current
      word and its context is used to maximize an objective
      function that aims to make the current word vector similar to
      its context and dissimilar to other randomly chosen words
      from outside this context [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>]. For short documents, such as tweets,
      the context vectors being small in size are unable to provide
      sufficient evidence for effective estimation of word-context
      semantic relations. To alleviate the problems of short
      contexts of tweets, we propose to learn a transformation
      matrix that transforms a word vector <strong>w</strong> close
      to a set of, generally speaking, topically similar words.
      Formally, this set of words that are topically similar to the
      word <em>w</em> is represented by the set</p>
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \Phi (w)=\lbrace
          v: (w,v) \in S\rbrace , \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>where <em>S</em> denotes the semantic relation between
      a pair of words. Assuming the existence of a pre-defined
      semantic relation <em>S</em> between word pairs (as per
      Equation <a class="eqn" href="#eq1">1</a>), Equation
      <a class="eqn" href="#eq2">2</a> defines the loss function
      for a word vector <strong>w</strong>.
      <div class="table-responsive" id="eq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} l(\mathbf
          {w};\theta)= \sum _{\mathbf {v}: v \in \Phi (w)} \sum
          _{\mathbf {u}: u \not\in \Phi (w)} \max \big (0,
          1-((\theta \mathbf {w})^T\mathbf {v} - (\theta \mathbf
          {w})^T\mathbf {u})\big) \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>The hinge loss function of Equation <a class="eqn"
      href="#eq2">2</a> is parameterized by the transformation
      matrix <span class="inline-equation"><span class=
      "tex">$\theta \in \mathbb {R}^{d\times d}$</span></span> ,
      and is learned by iterating with stochastic gradient descent.
      The word vectors used in learning the parameter matrix
      <em>θ</em> are obtained by the word2vec skip-gram algorithm.
      After training, each word vector <strong>w</strong> is
      transformed to <strong>w′</strong> = <em>θ</em> ·
      <strong>w</strong>. Informally speaking, the objective
      function aims to maximize the similarity between two word
      vectors <strong>w</strong> and <strong>v</strong> that are
      members of the same semantic context. On the other hand, it
      minimizes the similarity between the word vector
      <strong>w</strong> and a word vector <strong>u</strong>
      randomly sampled from outside its context (as defined by the
      semantic relation <em>S</em>).
      <p></p>
      <p>In the particular context of tweets, lexical similarity is
      likely to play an important role in topically grouping
      tweets. We propose to partition a collection of tweets into a
      set of clusters. The semantic relationship <em>S</em>
      (Equation <a class="eqn" href="#eq2">2</a>) then considers
      terms <em>u</em> and <em>v</em> from the same cluster to be
      semantically related. More specifically, each constituent
      word pair from the tweets belonging to the same cluster are
      considered to be members of the set <em>S</em>. These act as
      positive examples to learn the transformation matrix
      <em>θ</em> of Equation <a class="eqn" href="#eq2">2</a>.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experimental
          Setup</h2>
        </div>
      </header>
      <p><strong>Dataset</strong>. We perform experiments on the
      FIRE’16 Microblog Track data<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a>, comprising 60,685 tweets and 7
      topics along with relevance judgments for each topic. We use
      the relevance assessments of the FIRE-2016 Microblog dataset
      to construct the ground-truth for clustering evaluation. Each
      pair of documents judged relevant for the same query are
      considered to belong to the same cluster in the ground truth.
      The ground-truth thus considers a clustering output
      favourably if two documents that are relevant to the same
      query are predicted to be a part of the same cluster.</p>
      <p><strong>Baselines</strong>. The first baseline, denoted as
      ‘BoW’, represents documents by a tf-idf bag-of-words vector.
      The second baseline, denoted as ‘Word2vec-sum’, represents
      each document as a sum of the word vectors (dimension set to
      200) of the constituent words of a tweet obtained by the
      baseline skipgram model [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>]. A third baseline employs the doc2vec
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>] embedding
      of each tweet for clustering. Starting with the baseline
      skipgram vectors, we learn the transformation matrix by
      applying Equation <a class="eqn" href="#eq2">2</a>. We employ
      K-means clustering on BoW representation of documents to
      learn the transformation matrix. Similar to the
      ‘Word2vec-sum’ baseline, we represent documents by summing
      the transformed word vectors.</p>
      <p><strong>Parameters</strong>. The first parameter in the
      word vector transformation method is the number of clusters
      in which to partition the document collection for the purpose
      of defining the semantic contexts. We vary the number of
      clusters, <em>K</em>, within a range of 5 to 1000 for
      obtaining the transformed word vectors. The second parameter,
      which is applicable to the ‘Wordvec-sum’ baseline, is the
      length of the window (context vector) in word2vec. We vary
      this parameter, denoted as <em>wlen</em>, within a range of 3
      to 20. We then learn the transformation matrix on the word
      vectors obtained with the skipgram model with different
      values of these window lengths.</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Results</h2>
        </div>
      </header>
      <p>Table <a class="tbl" href="#tab1">1</a> shows the
      clustering results with the optimal parameter settings for
      each method. It can be seen that the text-only approach
      produces more effective clusters than the skipgram word
      vector based method, i.e., ‘Word2vec-sum’. Results are seen
      to considerably improve with the use of transformed word
      vectors. It can be seen that the ‘Semantic-wvec’ (proposed)
      yields better RI values than the BoW for values of <em>K</em>
      ≥ 20, which suggests that the context derived from similar
      tweets from the same cluster helps to learn improved word
      representations. Results worse than the text-only baseline,
      for too small values of <em>K</em>, suggests that the context
      needs to be large enough for effective learning of the word
      vectors. The proposed method shows better performance than
      doc2vec for relatively higher values of <em>wlen</em>.
      Further, too large values of <em>K</em> decreases clustering
      effectiveness.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Clustering effectiveness with different
          approaches.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:left;"></th>
              <th style="text-align:center;" colspan="2">
                Parameters
                <hr />
              </th>
              <th style="text-align:center;"></th>
            </tr>
            <tr>
              <th style="text-align:left;">(r)2-3 Document
              Representation</th>
              <th style="text-align:center;"><em>wlen</em></th>
              <th style="text-align:center;"><em>K</em></th>
              <th style="text-align:center;">R-Index</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:left;">BoW (Bag of words)</td>
              <td style="text-align:center;">-</td>
              <td style="text-align:center;">-</td>
              <td style="text-align:center;">0.6349</td>
            </tr>
            <tr>
              <td style="text-align:left;">Word2vec-sum (Sum of
              skipgram wvecs)</td>
              <td style="text-align:center;">16</td>
              <td style="text-align:center;">-</td>
              <td style="text-align:center;">0.5646</td>
            </tr>
            <tr>
              <td style="text-align:left;">Doc2vec</td>
              <td style="text-align:center;">16</td>
              <td style="text-align:center;">-</td>
              <td style="text-align:center;">0.6361</td>
            </tr>
            <tr>
              <td style="text-align:left;">Semantic-wvec (sum of
              transformed wvecs)</td>
              <td style="text-align:center;">6</td>
              <td style="text-align:center;">5</td>
              <td style="text-align:center;">0.6231</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;">3</td>
              <td style="text-align:center;">10</td>
              <td style="text-align:center;">0.6269</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;">11</td>
              <td style="text-align:center;">20</td>
              <td style="text-align:center;">0.6452</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;">18</td>
              <td style="text-align:center;">50</td>
              <td style="text-align:center;">
              <strong>0.6533</strong></td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;">12</td>
              <td style="text-align:center;">100</td>
              <td style="text-align:center;">0.6480</td>
            </tr>
            <tr>
              <td style="text-align:left;"></td>
              <td style="text-align:center;">5</td>
              <td style="text-align:center;">1000</td>
              <td style="text-align:center;">0.6068</td>
            </tr>
          </tbody>
        </table>
      </div>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186935/images/www18companion-175-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Sensitivity of clustering RI with
          variations in the <em>wlen</em> parameter (<em>K</em> was
          set to 50).</span>
        </div>
      </figure>
      <p>Figure <a class="fig" href="#fig1">1</a> shows the effect
      of varying <em>wlen</em> on RI for <em>K</em> = 50 (optimal
      value as observed in Table <a class="tbl" href=
      "#tab1">1</a>). The ‘Semantic-wvec’ method consistently
      outperforms the ‘Wordvec-sum’ and the text-only baseline
      (shown as a constant with respect to <em>wlen</em>) for a
      wide range of <em>wlen</em> variation. We observed that our
      method was able to group the following pair of tweets in the
      same cluster unlike the BoW, Word2vec-sum and Doc2vec
      baselines. Despite Tweet-2 not containing the term ‘food’, it
      is grouped in the same cluster as Tweet-1.</p>
      <p><img src=
      "../../../data/deliveryimages.acm.org/10.1145/3190000/3186935/images/www18companion-175-img1.svg"
      class="img-responsive" alt="" longdesc="" /></p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion and
          Future work</h2>
        </div>
      </header>
      <p>In this paper, we proposed a linear transformation based
      approach to improve the the word2vec embedding by leveraging
      semantic contexts. In our study, the semantic context refers
      to a set of topically similar documents obtained by
      clustering a document collection. Our experiments on the
      FIRE-2016 disaster dataset shows that representing documents
      as sum of transformed word vectors produces more effective
      clusters than BoW and sum of non-transformed word2vec
      representation baselines. In future, we would like to
      investigate the usefulness of other contexts, e.g. time and
      location for transformation of constituent word vectors of
      tweets.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Andrea Frome,
        Gregory&nbsp;S. Corrado, Jonathon Shlens, Samy Bengio,
        Jeffrey Dean, Marc'Aurelio Ranzato, and Tomas Mikolov.
        2013. DeViSE: A Deep Visual-Semantic Embedding Model. In
        <em><em>Proc. of NIPS’13</em></em> . 2121–2129.</li>
        <li id="BibPLXBIB0002" label="[2]">Muhammad Imran, Carlos
        Castillo, Ji Lucas, Patrick Meier, and Sarah Vieweg. 2014.
        AIDR: Artificial Intelligence for Disaster Response. In
        <em><em>Proc. of WWW’14 companion</em></em> . 159–162.</li>
        <li id="BibPLXBIB0003" label="[3]">Quoc Le and Tomas
        Mikolov. 2014. Distributed Representations of Sentences and
        Documents. In <em><em>Proc. of ICML’14</em></em> .
        II–1188–II–1196.</li>
        <li id="BibPLXBIB0004" label="[4]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Gregory&nbsp;S. Corrado, and Jeffrey
        Dean. 2013. Distributed Representations of Words and
        Phrases and their Compositionality. In <em><em>Proc. NIPS
        ’13</em></em> . 3111–3119.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "http://www.isical.ac.in/~fire/data/2016/FIRE2016-microblogs-track-data.tar.gz">http://www.isical.ac.in/~fire/data/2016/FIRE2016-microblogs-track-data.tar.gz</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186935">https://doi.org/10.1145/3184558.3186935</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
