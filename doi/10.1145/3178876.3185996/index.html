<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Haowen</span>     <span class="surName">Xu</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Wenxiao</span>     <span class="surName">Chen</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Nengwen</span>     <span class="surName">Zhao</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Zeyan</span>     <span class="surName">Li</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Jiahao</span>     <span class="surName">Bu</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Zhihan</span>     <span class="surName">Li</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Ying</span>     <span class="surName">Liu</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Youjian</span>     <span class="surName">Zhao</span>,     Tsinghua University    </div>    <div class="author">     <span class="givenName">Dan</span>     <span class="surName">Pei<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a></span>,     Tsinghua University</div>    <div class="author">     <span class="givenName">Yang</span>     <span class="surName">Feng</span>,     Alibaba Group    </div>    <div class="author">     <span class="givenName">Jie</span>     <span class="surName">Chen</span>,     Alibaba Group    </div>    <div class="author">     <span class="givenName">Zhaogang</span>     <span class="surName">Wang</span>,     Alibaba Group    </div>    <div class="author">     <span class="givenName">Honglin</span>     <span class="surName">Qiao</span>,     Alibaba Group    </div>            </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3185996" target="_blank">https://doi.org/10.1145/3178876.3185996</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (<em>e.g.</em>, Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed <em>Donut</em>, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, <em>Donut</em> greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for <em>Donut</em>, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Anomaly detection;</strong> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Traffic analysis;</strong></small> </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, and Honglin Qiao. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3185996" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3185996</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-3">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (key performance indicators) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. KPIs are time series data, measuring metrics such as Page Views, number of online users, and number of orders. Among all KPIs, the most ones are business-related KPIs (the focus of this paper), which are heavily influenced by user behavior and schedule, thus roughly have seasonal patterns occurring at regular intervals (<em>e.g.</em>, daily and/or weekly). However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels.</p>    <p>A rich body of literature exist on detecting KPI anomalies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>]. As discussed in&#x00A0;&#x00A7;<a class="sec" href="#sec-6">2.2</a>, existing anomaly detection algorithms suffer from the hassle of algorithm picking/parameter tuning, heavy reliance on labels, unsatisfying performance, and/or lack of theoretical foundations.</p>    <p>In this paper, we propose <em>Donut</em>, an unsupervised anomaly detection algorithm based on Variational Auto-Encoder (a representative deep generative model) with solid theoretical explanation, and this algorithm can work when there are no labels at all, and can take advantage of the occasional labels when available.</p>    <p>The contributions of this paper can be summarized as follows.</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">The three techniques in <em>Donut</em>, Modified ELBO and Missing Data Injection for training, and MCMC Imputation for detection, enable it to greatly outperform state-of-art supervised and VAE-based anomaly detection algorithms. The best F-scores of unsupervised <em>Donut</em> range from 0.75 to 0.9 for the studied KPIs from a top global Internet company.<br/></li>    <li id="list2" label="&#x2022;">For the first time in the literature, we discover that adopting VAE (or generative models in general) for anomaly detection requires training on both normal data <em>and abnormal data</em>, contrary to common intuition.<br/></li>    <li id="list3" label="&#x2022;">We propose a novel KDE interpretation in z-space for <em>Donut</em>, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation unlike&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. This interpretation may benefit the design of other deep generative models in anomaly detection. We discover a <em>time gradient effect</em> in latent z-space, which nicely explain <em>Donut</em>&#x2019;s excellent performance for detecting anomalies in seasonal KPIs.<br/></li>    </ul>   </section>   <section id="sec-4">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Background and Problem</h2>    </div>    </header>    <section id="sec-5">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.1</span> Context and Anomaly Detection in General</h3>     </div>    </header>    <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">2.5-day-long fragments of the seasonal KPI datasets in our paper, with anomalies in red color and missing points (filled with zeros) in orange. Within each dataset, there are variations for the same time slot in different days.</span>     </div>    </figure>    <p>In this paper, we focus on business-related KPIs. These time series are heavily influenced by user behavior and schedule, thus roughly have <em>seasonal</em> patterns occurring at regular intervals (<em>e.g.</em>, daily and/or weekly). On the other hand, the <em>shapes</em> of the KPI curves at each repetitive cycle are <em>not</em> exactly the same, since user behavior can vary across days. We hereby name the KPIs we study <strong>&#x201C;seasonal KPIs with local variations&#x201D;</strong>. Examples of such KPIs are shown in Fig <a class="fig" href="#fig1">1</a>. Another type of local variation is the increasing trend over days, as can be identified by Holt-Winters&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>] and Time Series Decomposition&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]. An anomaly detection algorithm may not work well unless these local variations are properly handled.</p>    <p>In addition to the seasonal patterns and local variations of the KPI <em>shapes</em>, there are also noises on these KPIs, which we assume to be independent, zero-mean Gaussian at every point. The exact values of the Gaussian noises are meaningless, thus we only focus on the statistics of these noises, <em>i.e.</em>, the variances of the noises.</p>    <p>We can now formalize the &#x201C;normal patterns&#x201D; of seasonal KPIs as a combination of two components: (1) the seasonal patterns with local variations, and (2) the statistics of the Gaussian noises.</p>    <p>We use &#x201C;anomalies&#x201D; to denote the recorded points which do not follow normal patterns (<em>e.g.</em>, sudden spikes and dips) , while using &#x201C;abnormal&#x201D; to denote both anomalies and missing points. See Fig <a class="fig" href="#fig1">1</a> for examples of both anomalies and missing points. Because the KPIs are monitored periodically (<em>e.g.</em>, every minute), missing points are recorded as &#x201C;null&#x201D; (when the monitoring system does not receive the data) and thus are straightforward to identify. We thus focus on detecting anomalies for the KPIs.</p>    <p>Because operators need to deal with the anomalies for troubleshooting/mitigation, some of the anomalies are anecdotally labeled. Note that such occasional labels&#x2019; coverage of anomalies are far from what&#x0027;s needed for typical supervised learning algorithms.</p>    <p>     <strong>Anomaly detection</strong> on KPIs can be formulated as follows: for any time <em>t</em>, given historical observations <em>x</em>     <sub>      <em>t</em> &#x2212; <em>T</em> + 1</sub>, &#x2026;, <em>x<sub>t</sub>     </em>, determine whether an anomaly occurs (denoted by <em>y<sub>t</sub>     </em> = 1). An anomaly detection algorithm typically computes a real-valued score indicating the certainty of having <em>y<sub>t</sub>     </em> = 1, <em>e.g.</em>, <em>p</em>(<em>y<sub>t</sub>     </em> = 1|<em>x</em>     <sub>      <em>t</em> &#x2212; <em>T</em> + 1</sub>, &#x2026;, <em>x<sub>t</sub>     </em>), instead of directly computing <em>y<sub>t</sub>     </em>. Human operators can then affect whether to declare an anomaly by choosing a threshold, where a data point with a score exceeding this threshold indicates an anomaly.</p>    </section>    <section id="sec-6">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.2</span> Previous Work</h3>     </div>    </header>    <p>     <strong>Traditional statistical models.</strong> Over the years, quite a few anomaly detectors based on traditional statistical models (<em>e.g.</em>, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>], mostly time series models) have been proposed to compute anomaly scores. Because these algorithms typically have simple assumptions for applicable KPIs, expert&#x0027;s efforts need to be involved to pick a suitable detector for a given KPI, and then fine-tune the detector&#x0027;s parameters based on the training data. Simple ensemble of these detectors, such as majority vote&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] and normalization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>], do not help much either according to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>]. As a result, these detectors see only limited use in the practice.</p>    <p>     <strong>Supervised ensemble approaches.</strong> To circumvent the hassle of algorithm/parameter tuning for traditional statistical anomaly detectors, supervised ensemble approaches, EGADS&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] and Opprentice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>], were proposed. They train anomaly classifiers using the user feedbacks as labels and using anomaly scores output by traditional detectors as features. Both EGADS and Opprentice showed promising results, but they heavily rely on good labels (much more than the anecdotal labels accumulated in our context), which is generally not feasible in large scale applications. Furthermore, running multiple traditional detectors to extract features during detection introduces lots of computational cost, which is a practical concern.</p>    <p>     <strong>Unsupervised approaches and deep generative models.</strong> Recently, there is a rising trend of adopting unsupervised machine learning algorithms for anomaly detection, <em>e.g.</em>, one-class SVM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>], clustering based methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>] like K-Means&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>] and GMM&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], KDE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>], and VAE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] and VRNN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. The philosophy is to focus on normal patterns instead of anomalies: since the KPIs are typically composed mostly of normal data, models can be readily trained even without labels. Roughly speaking, they all first recognize &#x201C;normal&#x201D; regions in the original or some latent feature space, and then compute the anomaly score by measuring &#x201C;how far&#x201D; an observation is from the normal regions.</p>    <p>Along this direction, we are interested in deep generative models for the following reasons. First, learning normal patterns can be seen as learning the distribution of training data, which is a topic of generative models. Second, great advances have been achieved recently to train generative models with deep learning techniques, <em>e.g.</em>, GAN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] and deep Bayesian network&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>]. The latter is family of deep generative models, which adopts the graphical&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0028">28</a>] model framework and variational techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>], with the VAE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] as a representative work. Third, despite deep generative model&#x0027;s great promise in anomaly detection, existing VAE-based anomaly detection method&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] was not designed for KPIs (time series), and does not perform well in our settings (see &#x00A7;<a class="sec" href="#sec-13">4</a>), and there is no theoretical foundation to back up its designs of deep generative models for anomaly detection (see &#x00A7;<a class="sec" href="#sec-20">5</a>). Fourth, simply adopting the more complex models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] based on VRNN shows long training time and poor performance in our experiments. Fifth, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] assumes training only on clean data, which is infeasible in our context, while [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] does not discuss this problem.</p>    </section>    <section id="sec-7">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.3</span> Problem Statement</h3>     </div>    </header>    <p>In summary, existing anomaly detection algorithms suffer from the hassle of algorithm picking/parameter tuning, heavy reliance on labels, unsatisfying performance, and/or lack of theoretical foundations. Existing approaches are either unsupervised, or supervised but depending heavily on labels. However, in our context, labels are occasionally available although far from complete, which should be somehow taken advantage of.</p>    <p>The problem statement of this paper is as follows. <strong>We aim at an unsupervised anomaly detection algorithm based on deep generative models with solid theoretical explanation, and this algorithm can take advantage of the occasionally available labels.</strong> Because VAE is a basic building block of deep Bayesian network, we chose to start our work with VAE.</p>    </section>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">2.4</span> Background of Variational Auto-Encoder</h3>     </div>    </header>    <p>Deep Bayesian networks use neural networks to express the relationships between variables, such that they are no longer restricted to simple distribution families, thus can be easily applied to complicated data. Variational inference techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0012">12</a>] are often adopted in training and prediction, which are efficient methods to solve posteriors of the distributions derived by neural networks. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Architecture of VAE. The prior of z is regarded as part of the generative model (solid lines), thus the whole generative model is denoted as <em>p<sub>&#x03B8;</sub>       </em>(x, z) = <em>p<sub>&#x03B8;</sub>       </em>(x|z)&#x2009;<em>p<sub>&#x03B8;</sub>       </em>(z). The approximated posterior (dashed lines) is denoted as <em>q<sub>&#x03D5;</sub>       </em>(z|x).</span>      </div>     </figure>    </p>    <p>VAE is a deep Bayesian network. It models the relationship between two random variables, latent variable z and visible variable x. A prior is chosen for z, which is usually multivariate unit Gaussian <span class="inline-equation"><span class="tex">$\mathcal {N}({\mathbf{{0}}},{\mathbf{{I}}})$</span>     </span>. After that, x is sampled from <em>p<sub>&#x03B8;</sub>     </em>(x|z), which is derived from a neural network with parameter <em>&#x03B8;</em>. The exact form of <em>p<sub>&#x03B8;</sub>     </em>(x|z) is chosen according to the demand of task. The true posterior <em>p<sub>&#x03B8;</sub>     </em>(z|x) is intractable by analytic methods, but is necessary for training and often useful in prediction, thus the variational inference techniques are used to fit another neural network as the approximation posterior <em>q<sub>&#x03D5;</sub>     </em>(z|x). This posterior is usually assumed to be <span class="inline-equation"><span class="tex">$\mathcal {N}({\mathbf{{\mu }}}_{\phi }({\mathbf{{x}}}),{\mathbf{{\sigma }}}^2_{\phi }({\mathbf{{x}}}))$</span>     </span>, where <em>&#x03BC;</em>     <sub>      <em>&#x03D5;</em>     </sub>(x) and <em>&#x03C3;</em>     <sub>      <em>&#x03D5;</em>     </sub>(x) are derived by neural networks. The architecture of VAE is shown as Fig <a class="fig" href="#fig2">2</a>.</p>    <p>SGVB&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] is a variational inference algorithm that is often used along with VAE, where the approximated posterior and the generative model are jointly trained by maximizing the evidence lower bound (ELBO, Eqn (1)). We did not adopt more advanced variational inference algorithms, since SGVB already works. <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \log p_{\theta }({\mathbf{{{\mathbf{{x}}}}}}) &#x0026;\ge \log p_{\theta }({\mathbf{{x}}}) - \operatorname{KL}\left[{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}\,\big \Vert \,{p_{\theta }({\mathbf{{z}}}|{\mathbf{{x}}})}\right] \\ &#x0026;= \mathcal {L}({\mathbf{{x}}}) \qquad \mathrm{(2)} \\ &#x0026;= \operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{x}}}) + \log p_{\theta }({\mathbf{{z}}}|{\mathbf{{x}}}) - \log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}\right] \\ &#x0026;= \operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{x}}},{\mathbf{{z}}}) - \log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}\right] \\ &#x0026;= \operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}}) + \log p_{\theta }({\mathbf{{z}}}) - \log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}\right]\end{align*} </span>       <br/>      </div>     </div> Monte Carlo integration&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] is often adopted to approximate the expectation in Eqn (1), as Eqn <a class="eqn" href="#eq1">2</a>, where z<sup>(<em>l</em>)</sup>, <em>l</em> = 1&#x2026;<em>L</em> are samples from <em>q<sub>&#x03D5;</sub>     </em>(z|x). We stick to this method throughout this paper. <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{f({\mathbf{{z}}})}\right] \approx \frac{1}{L} \sum _{l=1}^L f({\mathbf{{z}}}^{(l)}) \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div>    </p>    </section>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Architecture</h2>    </div>    </header>    <figure id="fig3">    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 3:</span>     <span class="figure-title">Overall architecture of <em>Donut</em>.</span>    </div>    </figure>    <p>The overall architecture of our algorithm <em>Donut</em> is illustrated as Fig <a class="fig" href="#fig3">3</a>. The three key techniques are <em>Modified ELBO</em> and <em>Missing Data Injection</em> during training, and <em>MCMC Imputation</em> in detection.</p>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Network Structure</h3>     </div>    </header>    <p>As aforementioned in &#x00A0;&#x00A7;<a class="sec" href="#sec-5">2.1</a>, the KPIs studied in this paper are assumed to be time sequences with Gaussian noises. However, VAE is not a sequential model, thus we apply sliding windows&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>] of length <em>W</em> over the KPIs: for each point <em>x<sub>t</sub>     </em>, we use <em>x</em>     <sub>      <em>t</em> &#x2212; <em>W</em> + 1</sub>, &#x2026;, <em>x<sub>t</sub>     </em> as the x vector of VAE. This sliding window was first adopted because of its simplicity, but it turns out to actually bring an important and beneficial consequence, which will be discussed in &#x00A7;<a class="sec" href="#sec-21">5.1</a>. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Network structure of <em>Donut</em>. Gray nodes are random variables, and white nodes are layers. The double lines highlight our special designs upon a general VAE.</span>      </div>     </figure>    </p>    <p>The overall network structure of <em>Donut</em> is illustrated in Fig <a class="fig" href="#fig4">4</a>, where the components with double-lined outlines (<em>e.g.</em>, Sliding Window x, W Dimensional at bottom left) are our new designs and the remaining components are from standard VAEs. The prior <em>p<sub>&#x03B8;</sub>     </em>(z) is chosen to be <span class="inline-equation"><span class="tex">$\mathcal {N}({\mathbf{{0}}},{\mathbf{{I}}})$</span>     </span>. Both x and z posterior are chosen to be diagonal Gaussian: <span class="inline-equation"><span class="tex">$p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}}) = \mathcal {N}({\mathbf{{\mu _x}}},{\mathbf{{\sigma _x}}}^2{\mathbf{{I}}})$</span>     </span>, and <span class="inline-equation"><span class="tex">$q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}}) = \mathcal {N}({\mathbf{{\mu _z}}},{\mathbf{{\sigma _z}}}^2{\mathbf{{I}}})$</span>     </span>, where <em>&#x03BC;</em>     <sub>x</sub>, <em>&#x03BC;</em>     <sub>z</sub> and <em>&#x03C3;</em>     <sub>x</sub>, <em>&#x03C3;</em>     <sub>z</sub> are the means and standard deviations of each independent Gaussian component. z is chosen to be <em>K</em> dimensional. Hidden features are extracted from x and z, by separated hidden layers <em>f<sub>&#x03D5;</sub>     </em>(x) and <em>f<sub>&#x03B8;</sub>     </em>(z). Gaussian parameters of x and z are then derived from the hidden features. The means are derived from linear layers: <span class="inline-equation"><span class="tex">${\mathbf{{\mu _x}}} = {\mathbf{{W}}}^{\top }_{{\mathbf{{\mu _x}}}}f_{\theta }({\mathbf{{z}}})+{\mathbf{{b_{\mu _x}}}}$</span>     </span> and <span class="inline-equation"><span class="tex">${\mathbf{{\mu _z}}} = {\mathbf{{W}}}^{\top }_{{\mathbf{{\mu _z}}}}f_{\phi }({\mathbf{{x}}})+{\mathbf{{b_{\mu _z}}}}$</span>     </span>. The standard deviations are derived from soft-plus layers, plus a non-negative small number &#x03F5;: <span class="inline-equation"><span class="tex">${\mathbf{{\sigma _x}}} = \operatorname{SoftPlus}[{\mathbf{{W}}}^{\top }_{{\mathbf{{\sigma _x}}}}f_{\theta }({\mathbf{{z}}})+{\mathbf{{b_{\sigma _x}}}}] + {\mathbf{{\epsilon }}}$</span>     </span> and <span class="inline-equation"><span class="tex">${\mathbf{{\sigma _z}}} = \operatorname{SoftPlus}[{\mathbf{{W}}}^{\top }_{{\mathbf{{\sigma _z}}}}f_{\phi }({\mathbf{{x}}})+{\mathbf{{b_{\sigma _z}}}}] + {\mathbf{{\epsilon }}}$</span>     </span>, where <span class="inline-equation"><span class="tex">$\operatorname{SoftPlus}[a] = \log [\exp (a) + 1]$</span>     </span>. All the W-s and b-s presented here are parameters of corresponding layers. Note when scalar function <em>f</em>(<em>x</em>) is applied on vector x, it means to apply on every component.</p>    <p>We choose to derive <em>&#x03C3;</em>     <sub>x</sub> and <em>&#x03C3;</em>     <sub>z</sub> in such a way, instead of deriving log&#x2009;<em>&#x03C3;</em>     <sub>x</sub> and log&#x2009;<em>&#x03C3;</em>     <sub>z</sub> using linear layers as others do, for the following reason. The local variations in the KPIs of our interest are so small that <em>&#x03C3;</em>     <sub>x</sub> and <em>&#x03C3;</em>     <sub>z</sub> would probably get extremely close to zero, making log&#x2009;<em>&#x03C3;</em>     <sub>x</sub> and log&#x2009;<em>&#x03C3;</em>     <sub>z</sub> unbounded. This would cause severe numerical problems when computing the likelihoods of Gaussian variables. We thus use the soft-plus and the &#x03F5; trick to prevent such problems.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Training</h3>     </div>    </header>    <p>Training is straightforward by optimizing the ELBO (Eqn (1)) with SGVB&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] algorithm. Since it is reported by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>] that one sample is sufficient for computing the ELBO when training VAE with the SGVB algorithm, we let sampling number <em>L</em> = 1 during training. The windows of x are randomly shuffled before every epoch, which is beneficial for stochastic gradient descent. A sufficiently large number of x are taken in every mini-batch, which is critical for stabilizing the training, since sampling introduces extra randomness.</p>    <p>As discussed in &#x00A7;<a class="sec" href="#sec-6">2.2</a>, the VAE based anomaly detection works by learning normal patterns, thus we need to avoid learning abnormal patterns whenever possible. Note that the &#x201C;anomalies&#x201D; in training are labeled anomalies, and there can be no labels for a given KPI, in which case the anomaly detection becomes an unsupervised one.</p>    <p>One might be tempted to replace labeled anomalies (if any) and missing points (known) in training data with synthetic values. Some previous work has proposed methods to impute missing data, <em>e.g.</em>, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>], but it is hard to produce data that follow the &#x201C;normal patterns&#x201D; well enough. More importantly, training a generative model with data generated by another algorithm is quite absurd, since one major application of generative models is exactly to generate data. Using data imputed by any algorithm weaker than VAE would potentially downgrade the performance. Thus we do not adopt missing data imputation before training VAE, instead we choose to simply fill the missing points as zeros (in the <em>Data Preparation</em> step in Fig <a class="fig" href="#fig3">3</a>), and then modify the ELBO to exclude the contribution of anomalies and missing points (shown as Modified ELBO (<strong>M-ELBO</strong> for short hereafter) in the <em>Training</em> step in Fig <a class="fig" href="#fig3">3</a>).</p>    <p>More specifically, we modify the standard ELBO in Eqn (1) to our version Eqn <a class="eqn" href="#eq2">3</a>. <em>&#x03B1;<sub>w</sub>     </em> is defined as an indicator, where <em>a<sub>w</sub>     </em> = 1 indicates <em>x<sub>w</sub>     </em> being not anomaly or missing, and <em>a<sub>w</sub>     </em> = 0 otherwise. <em>&#x03B2;</em> is defined as <span class="inline-equation"><span class="tex">$(\sum _{w=1}^W \alpha _w) / W$</span>     </span>. Note that Eqn <a class="eqn" href="#eq2">3</a> still holds when there is no labeled anomalies in the training data. The contribution of <em>p<sub>&#x03B8;</sub>     </em>(<em>x<sub>w</sub>     </em>|z) from labeled anomalies and missing points are directly excluded by <em>&#x03B1;<sub>w</sub>     </em>, while the scaling factor <em>&#x03B2;</em> shrinks the contribution of <em>p<sub>&#x03B8;</sub>     </em>(z) according to the ratio of normal points in x. This modification trains <em>Donut</em> to correctly reconstruct the normal points within x, even if some points in x are abnormal. We do not shrink <em>q<sub>&#x03D5;</sub>     </em>(z|x), because of the following two considerations. Unlike <em>p<sub>&#x03B8;</sub>     </em>(z), which is part of the generative network (<em>i.e.</em>, model of the &#x201C;normal patterns&#x201D;), <em>q<sub>&#x03D5;</sub>     </em>(z|x) just describes the mapping from x to z, without considering&#x201C;normal patterns&#x201D;. Thus, discounting the contribution of <em>q<sub>&#x03D5;</sub>     </em>(z|x) seems not necessary. Another reason is that <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}[-\log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})]$</span>     </span> is exactly the entropy of <em>q<sub>&#x03D5;</sub>     </em>(z|x). This entropy term actually has some other roles in training (which will be discussed in &#x00A7;<a class="sec" href="#sec-23">5.3</a>), thus might be better kept untouched. <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} { \widetilde{\mathcal {L}}({\mathbf{{x}}}) = \operatorname{\mathbb {E}}_{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}\bigg [\sum _{w=1}^W \alpha _w \log p_{\theta }(x_w|{\mathbf{{z}}}) + \beta \log p_{\theta }({\mathbf{{z}}}) - \log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})\bigg ] } \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> Besides Eqn <a class="eqn" href="#eq2">3</a>, another way to deal with anomalies and missing points is to exclude all windows containing these points from training data. This approach turns out to be inferior to M-ELBO. We will demonstrate the performance of both approaches in &#x00A7;<a class="sec" href="#sec-18">4.5</a>.</p>    <p>Furthermore, we also introduce missing data injection in training: we randomly set <em>&#x03BB;</em> ratio of normal points to be zero, as if they are missing points. With more missing points, <em>Donut</em> is trained more often to reconstruct normal points when given abnormal x, thus the effect of M-ELBO is amplified. This injection is done before every epoch, and the points are recovered once the epoch is finished. This missing data injection is shown in the <em>Training</em> step in Fig <a class="fig" href="#fig3">3</a>.</p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Detection</h3>     </div>    </header>    <p>Generative models like VAE can derive various outputs. In the scope of anomaly detection, the likelihood of observation window x, <em>i.e.</em>, <em>p<sub>&#x03B8;</sub>     </em>(x) in VAE, is an important output, since we want to see how well a given x follows the normal patterns. Monte Carlo methods can be adopted to compute the probability density of x, by <span class="inline-equation"><span class="tex">$p_{\theta }({\mathbf{{x}}}) = \operatorname{\mathbb {E}}_{{p_{\theta }({\mathbf{{z}}})}}\left[{p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>. Despite the theoretically nice interpretation, sampling on the prior actually does not work well enough in practice, as will be shown in &#x00A7;<a class="sec" href="#sec-13">4</a>.</p>    <p>Instead of sampling on the prior, one may seek to derive useful outputs with the variational posterior <em>q<sub>&#x03D5;</sub>     </em>(z|x). One choice is to compute <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>. Although similar to <em>p<sub>&#x03B8;</sub>     </em>(x), it is not a well-defined probability density. Another choice is to compute <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>, which is adopted in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>], named as &#x201C;reconstruction probability&#x201D;. These two choices are very similar. Since only the ordering rather than the exact values of anomaly scores are concerned in anomaly detection, we follow [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] and use the latter one. As an alternative, the ELBO (Eqn (1)) may also be used for approximating log&#x2009;<em>p<sub>&#x03B8;</sub>     </em>(x), as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>]. However, the extra term <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{z}}}) - \log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}\right]$</span>     </span> in ELBO makes its internal mechanism hard to understand. Since the experiments in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] does not support this alternative&#x0027;s superiority, we choose not to use it.</p>    <p>During detection, the anomalies and missing points in a testing window x can bring bias to the mapped z, and further make the reconstruction probability inaccurate, which would be discussed in &#x00A7;<a class="sec" href="#sec-22">5.2</a>. Since the missing points are always known (as &#x201C;null&#x201D;), we have the chance to eliminate the biases introduced by missing points. We choose to adopt the MCMC-based missing data imputation technique with the trained VAE, which is proposed by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>]. Meanwhile, we do not know the exact positions of anomalies before detection, thus MCMC cannot be adopted on anomalies.</p>    <p>More specifically, the testing x is divided into observed and missing parts, <em>i.e.</em>, (x<sub>      <em>o</em>     </sub>, x<sub>      <em>m</em>     </sub>). A z sample is obtained from <em>q<sub>&#x03D5;</sub>     </em>(z|x<sub>      <em>o</em>     </sub>, x<sub>      <em>m</em>     </sub>), then a reconstruction sample <span class="inline-equation"><span class="tex">$({\mathbf{{x}}}^{\prime }_o, {\mathbf{{x}}}^{\prime }_m)$</span>     </span> is obtained from <em>p<sub>&#x03B8;</sub>     </em>(x<sub>      <em>o</em>     </sub>, x<sub>      <em>m</em>     </sub>|z). (x<sub>      <em>o</em>     </sub>, x<sub>      <em>m</em>     </sub>) is then replaced by <span class="inline-equation"><span class="tex">$({\mathbf{{x}}}_o, {\mathbf{{x}}}^{\prime }_m)$</span>     </span>, <em>i.e.</em>, the observed points are fixed and the missing points are set to new values. This process is iterated for <em>M</em> times, then the final <span class="inline-equation"><span class="tex">$({\mathbf{{x}}}_o, {\mathbf{{x}}}^{\prime }_m)$</span>     </span> is used for computing the reconstruction probability. The intermediate <span class="inline-equation"><span class="tex">${\mathbf{{x}}}^{\prime }_m$</span>     </span> will keep getting closer to normal values during the whole procedure. Given sufficiently large <em>M</em>, the biases can be reduced, and we can get a more accurate reconstruction probability. The MCMC method is illustrated in Fig <a class="fig" href="#fig5">5</a> and is shown in the <em>Detection</em> step in Fig <a class="fig" href="#fig3">3</a>. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Illustration of one iteration in MCMC. x is decomposed as (x<sub>        <em>o</em>       </sub>, x<sub>        <em>m</em>       </sub>), then x<sub>        <em>o</em>       </sub> is fixed and x<sub>        <em>m</em>       </sub> is replaced by <span class="inline-equation"><span class="tex">${\mathbf{{x}}}^{\prime }_m$</span>       </span> from the reconstruction sample, in order to get the new x&#x2032;.</span>      </div>     </figure>    </p>    <p>After MCMC, we take <em>L</em> samples of z to compute the reconstruction probability by Monte Carlo integration. Although we may compute the reconstruction probability for each point in every window of x, we only use the score for the last point (<em>i.e.</em>, <em>x<sub>t</sub>     </em> in <em>x</em>     <sub>      <em>t</em> &#x2212; <em>T</em> + 1</sub>, &#x2026;, <em>x<sub>t</sub>     </em>), since we want to respond to anomalies as soon as possible during detection. We will still use vector notations in later texts, corresponding to the architecture of VAE.</p>    </section>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Evaluation</h2>    </div>    </header>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Datasets</h3>     </div>    </header>    <p>We obtain 18 well-maintained business KPIs (where the time span is long enough for training and evaluation) from a large Internet company. All KPIs have an interval of 1 minute between two observations. We choose 3 datasets, denoted as <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, which have relatively small, medium and large noises among the 18 datasets, so we can evaluate <em>Donut</em> for noises at different levels. We divide each dataset into training, validation and testing sets, whose ratios are 49%, 21%, 30% respectively. Figures of datasets <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span> are shown in Fig <a class="fig" href="#fig1">1</a>, while statistics are shown in Table <a class="tbl" href="#tab1">1</a>. The operators of the Internet company labeled all the anomalies in these three datasets. For evaluation purpose, we can consider we have the ground truth of all anomalies in these three datasets.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Statistics of <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>       </span>, <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>       </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>       </span>.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:center;">DataSet</th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>        </span>       </th>       <th style="text-align:center;">        <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>        </span>       </th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">Total points</td>       <td style="text-align:center;">296460</td>       <td style="text-align:center;">317522</td>       <td style="text-align:center;">285120</td>       </tr>       <tr>       <td style="text-align:center;">Missing points</td>       <td style="text-align:center;">1222/0.41%</td>       <td style="text-align:center;">1117/0.35%</td>       <td style="text-align:center;">304/0.11%</td>       </tr>       <tr>       <td style="text-align:center;">Anomaly points</td>       <td style="text-align:center;">1213/0.41%</td>       <td style="text-align:center;">1883/0.59%</td>       <td style="text-align:center;">4394/1.54%</td>       </tr>       <tr>       <td style="text-align:center;">Total windows<sup>*</sup>       </td>       <td style="text-align:center;">296341</td>       <td style="text-align:center;">317403</td>       <td style="text-align:center;">285001</td>       </tr>       <tr>       <td style="text-align:center;">Abnormal windows<sup>**</sup>       </td>       <td style="text-align:center;">20460/6.90%</td>       <td style="text-align:center;">20747/6.54%</td>       <td style="text-align:center;">17288/6.07%</td>       </tr>      </tbody>      <tfoot>       <tr>       <td>        <sup>*</sup> Each sliding window has a length <em>W</em> = 120.</td>       <td/>       <td/>       <td/>       </tr>       <tr>       <td>        <sup>**</sup> Each abnormal window contains at least one anomaly or missing point.</td>       <td/>       <td/>       <td/>       </tr>      </tfoot>     </table>    </div>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Performance Metrics</h3>     </div>    </header>    <p>In our evaluation, we totally ignore outputs of all algorithms at missing points (&#x201C;null&#x201D;) since they are straightforward to identify.</p>    <p>All the algorithms evaluated in this paper compute one anomaly score for each point. A threshold can be chosen to do the decision: if the score for a point is greater than the threshold, an alert should be triggered. In this way, anomaly detection is similar to a classification problem, and we may compute the precision and recall corresponding to each threshold. We may further compute the AUC, which is the average precision over recalls, given all possible thresholds; or the F-score, which is the harmonic mean of precision and recall, given one particular threshold. We may also enumerate all thresholds, obtaining all F-scores, and use the <em>best F-score</em> as the metric. The best F-score indicates the best possible performance of a model on a particular testing set, given an optimal global threshold. In practice, the best F-score is mostly consistent with AUC, except for slight differences (see Fig <a class="fig" href="#fig7">7</a>). We prefer the best F-score to AUC, since it should be more important to have an excellent F-score at a certain threshold than to have just high but not so excellent F-scores on most thresholds. <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Illustration of the strategy for modified metrics. The first row is the truth with 10 contiguous points and two anomaly segments highlighted in the shaded squares. The detector scores are shown in the second row. The third row shows the point-wise detector results with a threshold of 0.5. The forth row shows the detector results after adjustment. We shall get precision 0.6, and recall 0.5. From the third row, the alert delay for the first segment is 1 interval (1 minute).</span>      </div>     </figure>    </p>    <p>In real applications, the human operators generally do not care about the point-wise metrics. It is acceptable for an algorithm to trigger an alert for any point in a contiguous anomaly segment, if the delay is not too long. Some metrics for anomaly detection have been proposed to accommodate this preference, <em>e.g.</em>, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>], but most are not widely accepted, likely because they are too complicated. We instead use a simple strategy: if any point in an anomaly segment in the ground truth can be detected by a chosen threshold, we say this segment is detected correctly, and all points in this segment are treated as if they can be detected by this threshold. Meanwhile, the points outside the anomaly segments are treated as usual. The precision, recall, AUC, F-score and best F-score are then computed accordingly. This approach is illustrated in Fig <a class="fig" href="#fig6">6</a>.</p>    <p>In addition to the accuracy metric, we compute the alert delay for each detected segment, which is also important to the operators. For a true positive segment, the alert delay is the time difference between the first point and the first detected point in the segment.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Experiment Setup</h3>     </div>    </header>    <figure id="fig7">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig7.jpg" class="img-responsive" alt="Figure 7"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 7:</span>      <span class="figure-title">AUC, the best F-Score, and the average alert delay corresponding to the best F-score. <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>       </span>, <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>       </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>       </span> are the three datasets. &#x201C;0%&#x201D;, &#x201C;10%&#x201D; and &#x201C;100%&#x201D; are the ratio of the labels preserved in training. Note there is no result for Opprentice when there are 0% of anomaly labels. The black stick on top of each bar is the deviation of 10 repeated experiments.</span>     </div>    </figure>    <p>We set the window size <em>W</em> to be 120, which spans 2 hours in our datasets. The choice of <em>W</em> is restricted by two factors. On the one hand, too small a <em>W</em> will cause the model to be unable to capture the patterns, since the model is expected to recognize what the normal pattern is with the information only from the window (see &#x00A7;<a class="sec" href="#sec-21">5.1</a>). On the other hand, too large a <em>W</em> will increase the risk of over-fitting, since we stick to fully-connected layers without weight sharing, thus the number of model parameters is proportional to <em>W</em>. We set the latent dimension <em>K</em> to be 3 for <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, since the 3-d dimensional space can be easily visualized for analysis and luckily <em>K</em> = 3 works well empirically for for <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>. As for <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>, we found 3 is too small, so we empirically increase <em>K</em> to 8. These empirical choices of <em>K</em> are proven to be quite good on testing set, as will be shown in Fig <a class="fig" href="#fig9">9</a>. The hidden layers of <em>q<sub>&#x03D5;</sub>     </em>(z|x) and <em>p<sub>&#x03B8;</sub>     </em>(x|z) are both chosen as two ReLU layers, each with 100 units, which makes the variational and generative network have equal size. We did not carry out exhaustive search on the structure of hidden networks.</p>    <p>Other hyper-parameters are also chosen empirically. We use 10<sup>&#x2212; 4</sup> as &#x03F5; of the std layer. We use 0.01 as the injection ratio <em>&#x03BB;</em>. We use 10 as the MCMC iteration count <em>M</em>, and use 1024 as the sampling number <em>L</em> of Monte Carlo integration. We use 256 as the batch size for training, and run for 250 epochs. We use Adam optimizer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], with an initial learning rate of 10<sup>&#x2212; 3</sup>. We discount the learning rate by 0.75 after every 10 epochs. We apply L2 regularization to the hidden layers, with a coefficient of 10<sup>&#x2212; 3</sup>. We clip the gradients by norm, with a limit of 10.0.</p>    <p>In order to evaluate <em>Donut</em> with no labels, we ignore all the labels. For the case of occasional labels, we down-sample the anomaly labels of training and validation set to make it contain 10% of labeled anomalies. Note that missing points are not down-sampled. We keep throwing away anomaly segments randomly, with a probability that is proportional to the length of each segment, until the desired down-sampling rate is reached. We use this approach instead of randomly throwing away individual anomaly points, because KPIs are time sequences and each anomaly point could leak information about its neighboring points, resulting in over-estimated performance. Such downsampling are done 10 times, which enables us to do 10 independent, repeated experiments. Overall for each dataset, we have three versions: 0% labels, 10% labels, and 100% labels.</p>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.4</span> Overall Performance</h3>     </div>    </header>    <p>We measure the AUC, the best F-Score, and the average alert delay corresponding to the best F-score in Fig <a class="fig" href="#fig7">7</a> of <em>Donut</em>, and compared with three selected algorithms.</p>    <p>     <strong>Opprentice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0023">23</a>]</strong> is an ensemble supervised framework using Random Forest classifier. On datasets similar to ours, Opprentice is reported to consistently and significantly outperform 14 anomaly detectors based on traditional statistical models (<em>e.g.</em>, [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0038">38</a>]), with in total 133 enumerated configurations of hyper-parameters for these detectors. Thus, in our evaluation of <em>Donut</em>, Opprentice not only serves as a state-of-art competitor algorithm from the non deep learning areas, but also serves as a proxy to compare with the empirical performance &#x201C;upper bound&#x201D; of these traditional anomaly detectors.</p>    <p>     <strong>VAE baseline.</strong> The VAE-based anomaly detection in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] does not deal with time sequences, thus we set up the VAE baseline as follows. First, the VAE baseline has the same network structure as <em>Donut</em>, as shown in Fig <a class="fig" href="#fig4">4</a>. Second, among all the techniques in Fig <a class="fig" href="#fig3">3</a>, only those techniques in the Data Preparation step are used. Third, as suggested by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>], we exclude <strong>all windows</strong> containing either labeled anomalies or missing points from training data.</p>    <p>     <strong>Donut-Prior</strong>. Given that a generative model learns <em>p</em>(x) by nature, while in VAE <em>p</em>(x) is defined as <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{p_{\theta }({\mathbf{{z}}})}}\left[{p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>, we also evaluate the prior counterpart of reconstruction probability, <em>i.e.</em>, <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{p_{\theta }({\mathbf{{z}}})}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>. We just need a baseline of the prior, so we compute the prior expectation by plain Monte Carlo integration, without advanced techniques to improve the result.</p>    <p>The best F-score of <em>Donut</em> is quite satisfactory in totally unsupervised case, ranges from 0.75 to 0.9, better than the supervised Opprentice in all cases. In fact, when labels are incomplete, the best F-score of the Opprentice drops heavily in <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span>, only remaining acceptable in <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>. The number of anomalies are much larger in <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span> than <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span>, while having 10% of labels are likely to be just enough for training. <em>Donut</em> has an outstanding performance in the unsupervised scenario, and we see that feeding anomaly labels into <em>Donut</em> would in general make it work even better. There is, however, an unusual behavior of <em>Donut</em>, where the best F-score in <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, as well as the AUC in <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, are slightly worse with 100% labels than 10%. This is likely an optimization problem, where the unlabeled anomalies might cause training to be unstable, accidentally pulling the model out of a sub-optimal equilibrium (&#x00A7;<a class="sec" href="#sec-24">5.4</a>). Such phenomenon seems to diminish when <em>K</em> increases from 3 (<span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>) to 8 (<span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>). Fortunately, it does not matter too much, so we would suggest to use labels in <em>Donut</em> whenever possible.</p>    <p>     <em>Donut</em> outperforms the VAE baseline by a large margin in <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span>, while it does not show such great advantage in <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>. In fact, the relative advantage of <em>Donut</em> is the largest in <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>, medium in <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span>, and the smallest in <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>. This is caused by the following reasons. Naturally, VAE models normal x. As a result, the reconstruction probability actually expects x to be mostly normal (see &#x00A7;<a class="sec" href="#sec-21">5.1</a>). However, since x are sliding windows of KPIs and we are required to produce one anomaly score for every point, it is sometimes inevitable to have abnormal points in x. This causes the VAE baseline to suffer a lot. In contrast, the techniques developed in this paper enhances the ability of <em>Donut</em> to produce reliable outputs even when anomalies present in earlier points in the same window. Meanwhile, abnormal points with similar abnormal magnitude would appear relatively &#x201C;more abnormal&#x201D; when the KPI is smoother. Given that <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span> is the smoothest, <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> is medium, and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span> is the least smoothest, above observation in the relative advantage is not surprising.</p>    <p>Finally, the best F-score of the <em>Donut-Prior</em> is much worse than the reconstruction probability, especially when the dimension of z is larger. However, it is worth mentioning that the posterior expectation in reconstruction probability only works under certain conditions (&#x00A7;<a class="sec" href="#sec-22">5.2</a>). Fortunately, this problem does not matter too much to <em>Donut</em> (see &#x00A7;<a class="sec" href="#sec-22">5.2</a>). As such, the reconstruction probability can be used without too much concern.</p>    <p>The average alert delays of <em>Donut</em>, Opprentice and VAE Baseline are acceptable over all datasets, whereas Donut-Prior is not. Meanwhile, the best F-score of <em>Donut</em> is much better than others. In conclusion, <em>Donut</em> could achieve the best performance without increasing the alert delay, thus <em>Donut</em> is practical for operators.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.5</span> Effects of <em>Donut</em> Techniques</h3>     </div>    </header>    <figure id="fig8">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig8.jpg" class="img-responsive" alt="Figure 8"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 8:</span>      <span class="figure-title">Best F-score of (1) VAE baseline, (2) <em>Donut</em> with M-ELBO, (3) M-ELBO + missing data injection, (4) M-ELBO + MCMC, and (5) M-ELBO + both MCMC and injection. The M-ELBO alone contributes most of the improvement.</span>     </div>    </figure>    <p>We have proposed three techniques in this paper: (1) M-ELBO (Eqn <a class="eqn" href="#eq2">3</a>), (2) missing data injection, and (3) MCMC imputation. In Fig <a class="fig" href="#fig9">9</a>, we present the best F-score of <em>Donut</em> with four possible combinations of these techniques, plus the VAE baseline for comparison. These techniques are closely related to the KDE interpretation, which will be discussed further in &#x00A7;<a class="sec" href="#sec-22">5.2</a>.</p>    <p>     <strong>M-ELBO</strong> alone contributes most of the improvement over the VAE baseline. It works by training <em>Donut</em> to get used to possible abnormal points in x, and to produce desired outputs in such cases. Although we expected M-ELBO to work, we did not expect it to work such well. In conclusion, <strong>it would not be a good practice to train a VAE for anomaly detection using only normal data, although it seems natural for a generative model (&#x00A7;<a class="sec" href="#sec-22">5.2</a>).</strong> To the best of our knowledge, M-ELBO and its importance have never been stated in previous work, thus is a major contribution of ours.</p>    <p>     <strong>Missing data injection</strong> is designed for amplifying the effect of M-ELBO, and can actually be seen as a data augmentation method. In fact, it would be better if we inject not only missing points, but also synthetically generated anomalies during training. However, it is difficult to generate anomalies similar enough to the real ones, which should be a large topic and is out of the scope of this paper. We thus only inject the missing points. The improvement of best F-score introduced by missing data injection is not very significant, and in the case of 0% labels on <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, it is slightly worse than M-ELBO only. This is likely because the injection introduces extra randomness to training, such that it demands larger training epochs, compared to the case of M-ELBO only. We are not sure how many number of epochs to run when the injection is adopted, in order to get an objective comparison, thus we just use the same epochs in all cases, leaving the result as it is. We still recommend to use missing data injection, even with a cost of larger training epochs, as it is expected to work with a large chance.</p>    <p>     <strong>MCMC imputation</strong> is also designed to help <em>Donut</em> deal with abnormal points. Although <em>Donut</em> obtains significant improvement of best F-score with MCMC in only some cases, it never harms the performance. According to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>], this should be an expected result. We thus recommend to always adopt MCMC in detection.</p>    <p>In conclusion, we recommend to use all the three techniques of <em>Donut</em>. The result of such configuration is also presented in Fig <a class="fig" href="#fig8">8</a>.</p>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.6</span> Impact of K</h3>     </div>    </header>    <figure id="fig9">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig9.jpg" class="img-responsive" alt="Figure 9"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 9:</span>      <span class="figure-title">The best F-score of unsupervised <em>Donut</em> with different <em>K</em>, averaged over 10 repeated experiments.</span>     </div>    </figure>    <p>The number of z dimensions, <em>i.e.</em>, <em>K</em>, plays an important role. Too small a <em>K</em> would potentially cause under-fitting, or sub-optimal equilibrium (see &#x00A7;<a class="sec" href="#sec-24">5.4</a>). On the other hand, too large a <em>K</em> would probably cause the reconstruction probability unable to find a good posterior (see &#x00A7;<a class="sec" href="#sec-21">5.1</a>). It is difficult to choose a good <em>K</em> in totally unsupervised scenario, thus we leave it as a future work.</p>    <p>In Fig <a class="fig" href="#fig9">9</a>, we present the average best F-score with different <em>K</em> on testing set for unsupervised <em>Donut</em>. This does not help us choose the best <em>K</em> (since we cannot use testing test to pick <em>K</em>), but can show our empirical choice of 8,3,3 is quite good. The best F-score reaches maximum at 5 for <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>, 4 for <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> and 3 for <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>. In other words, the best F-score could be achieved with fairly small <em>K</em>. On the other hand, the best F-score does not drop too heavily for <em>K</em> up to 21. This gives us a large room to empirically choose <em>K</em>. Finally, we notice that smoother KPIs seem to demand larger <em>K</em>. Such phenomenon is not fully studied in this paper, and we leave it as a future work. Based on the observations in Fig <a class="fig" href="#fig9">9</a>, for KPIs similar to <span class="inline-equation"><span class="tex">$\mathcal {A}$</span>     </span>, <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span> or <span class="inline-equation"><span class="tex">$\mathcal {C}$</span>     </span>, we suggest an empirical choice of <em>K</em> within the range from 5 to 10.</p>    </section>   </section>   <section id="sec-20">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Analysis</h2>    </div>    </header>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> KDE Interpretation</h3>     </div>    </header>    <figure id="fig10">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig10.jpg" class="img-responsive" alt="Figure 10"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 10:</span>      <span class="figure-title">The z layout of dataset <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>       </span> with (a) <em>Donut</em>, (b) untrained VAE, (c) VAE trained using <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}\left[{\log p_{\theta }({\mathbf{{z}}})}\right] + \operatorname{H}\left[{\mathbf{{z}}}|{\mathbf{{x}}}\right]$</span>       </span> as loss. Figures are plotted by sampling z from <em>q<sub>&#x03D5;</sub>       </em>(z|x), corresponding to normal x randomly chosen from the testing set. <em>K</em> is chosen as 2, so the x- and y-axis are the two dimensions of z samples. We plot z samples instead of <em>&#x03BC;</em>       <sub>z</sub> of <em>q<sub>&#x03D5;</sub>       </em>(z|x), since we want to take into account the effects of <em>&#x03C3;</em>       <sub>z</sub> in the figures. The color of z a sample denotes its time of the day.</span>     </div>    </figure>    <figure id="fig11">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig11.jpg" class="img-responsive" alt="Figure 11"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 11:</span>      <span class="figure-title">Illustration of the KDE interpretation. For a given x potentially with anomalies, <em>Donut</em> tries to recognize what normal pattern it follows, encoded as <em>q<sub>&#x03D5;</sub>       </em>(z|x). The black ellipse in the middle figure denotes the 3-<em>&#x03C3;</em>       <sub>z</sub> region of <em>q<sub>&#x03D5;</sub>       </em>(z|x). <em>L</em> samples of z are then taken from <em>q<sub>&#x03D5;</sub>       </em>(z|x), denoted as the crosses in the middle figure. Each z is associated with a density estimator kernel log&#x2009;<em>p<sub>&#x03B8;</sub>       </em>(x|z). The blue curves in the right two figures are <em>&#x03BC;</em>       <sub>x</sub> of each kernel, while the surrounding stripes are <em>&#x03C3;</em>       <sub>x</sub>. Finally, the values of log&#x2009;<em>p<sub>&#x03B8;</sub>       </em>(x|z) are computed from each kernel, and further averaged together as the reconstruction probability.</span>     </div>    </figure>    <figure id="fig12">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig12.jpg" class="img-responsive" alt="Figure 12"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 12:</span>      <span class="figure-title">3-d latent space of all three datasets.</span>     </div>    </figure>    <p>Although the reconstruction probability <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span> has been adopted in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>], how it actually works has not yet been made clear. Some may see it as a variant of <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>, but <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right] = \int p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}}) q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}}) \mathrm{d}{\mathbf{{z}}}$</span>     </span>, which is definitely not a well-defined probability<a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>. Thus neither of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] can be explained by the probabilistic framework. We hereby propose the KDE (kernel density estimation) interpretation for the reconstruction probability, and for the entire <em>Donut</em> algorithm.</p>    <p>The posterior <em>q<sub>&#x03D5;</sub>     </em>(z|x) for normal x exhibits time gradient, as Fig 10a shows. The windows of x at contiguous time (<strong>contiguous x</strong> for short hereafter) are mapped to nearby <em>q<sub>&#x03D5;</sub>     </em>(z|x), mostly with small variance <em>&#x03C3;</em>     <sub>z</sub> (see Fig <a class="fig" href="#fig11">11</a>). The <em>q<sub>&#x03D5;</sub>     </em>(z|x) are thus organized in smooth transition, causing z samples to exhibit color gradient in the figure. We name this structure &#x201C;time gradient&#x201D;. The KPIs in this paper are smooth in general, so contiguous x are highly similar. The root cause of time gradient is the transition of <em>q<sub>&#x03D5;</sub>     </em>(z|x) in the shape of x (rather than the one in time), because <em>Donut</em> consumes only the shape of x and no time information. Time gradient benefits the generalization of <em>Donut</em> on unseen data: if we have a posterior <em>q<sub>&#x03D5;</sub>     </em>(z|x) somewhere between two training posteriors, it would be well-defined, avoiding absurd detection output.</p>    <p>For a partially abnormal x<a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>, the dimension reduction would allow <em>Donut</em> to recognize its normal pattern <span class="inline-equation"><span class="tex">$\tilde{{\mathbf{{x}}}}$</span>     </span>, and cause <em>q<sub>&#x03D5;</sub>     </em>(z|x) to be approximately <span class="inline-equation"><span class="tex">$q_{\phi }({\mathbf{{z}}}|\tilde{{\mathbf{{x}}}})$</span>     </span>. This effect is caused by the following reasons. <em>Donut</em> is trained to reconstruct normal points in training samples with best efforts, while the dimension reduction causes <em>Donut</em> to be only able to capture a small amount of information from x. As a result, only the overall shape is encoded in <em>q<sub>&#x03D5;</sub>     </em>(z|x). The abnormal information is likely to be dropped in this procedure. However, if a x is too abnormal, <em>Donut</em> might fail to recognize any normal <span class="inline-equation"><span class="tex">$\tilde{{\mathbf{{x}}}}$</span>     </span>, such that <em>q<sub>&#x03D5;</sub>     </em>(z|x) would become ill-defined.</p>    <p>The fact that <em>q<sub>&#x03D5;</sub>     </em>(z|x) for a partially abnormal x would be similar to <span class="inline-equation"><span class="tex">$q_{\phi }({\mathbf{{z}}}|\tilde{{\mathbf{{x}}}})$</span>     </span> brings special meanings to the reconstruction probability in <em>Donut</em>. Since M-ELBO is maximized with regard to normal patterns during training, log&#x2009;<em>p<sub>&#x03B8;</sub>     </em>(x|z) for <span class="inline-equation"><span class="tex">${\mathbf{{z}}} \sim q_{\phi }({\mathbf{{z}}}|\tilde{{\mathbf{{x}}}})$</span>     </span> should produce high scores for x similar to <span class="inline-equation"><span class="tex">$\tilde{{\mathbf{{x}}}}$</span>     </span>, and vise versa. That is to say, each log&#x2009;<em>p<sub>&#x03B8;</sub>     </em>(x|z) can be used as a density estimator, indicating how well x follows the normal pattern <span class="inline-equation"><span class="tex">$\tilde{{\mathbf{{x}}}}$</span>     </span>. The posterior expectation then sums up the scores from all log&#x2009;<em>p<sub>&#x03B8;</sub>     </em>(x|z), with the weight <em>q<sub>&#x03D5;</sub>     </em>(z|x) for each z. This procedure is very similar to weighted kernel density estimation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>]. We thus carry out the KDE interpretation: <strong>the reconstruction probability </strong>     <span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>     <strong> in</strong>     <em>Donut</em>     <strong>can be seen as weighted kernel density estimation, with <em>q<sub>&#x03D5;</sub>      </em>(z|x) as weights and log&#x2009;<em>p<sub>&#x03B8;</sub>      </em>(x|z) as kernels</strong><a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>.</p>    <p>Fig <a class="fig" href="#fig11">11</a> is an illustration of the KDE interpretation. We also visualize the 3-d latent spaces of all datasets in Fig <a class="fig" href="#fig12">12</a>. From the KDE interpretation, we suspect the prior expectation would not work well, whatever technique is adopted to improve the result: sampling on the prior should obtain kernels for all patterns of x, potentially confusing the density estimation for a particular x.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Find Good Posteriors for Abnormal x</h3>     </div>    </header>    <p>     <em>Donut</em> can recognize the normal pattern of a partially abnormal x, and find a good posterior for estimating how well x follows the normal pattern. We now analyze how the techniques in <em>Donut</em> can enhance such ability of finding good posteriors. <figure id="fig13">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig13.jpg" class="img-responsive" alt="Figure 13"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 13:</span>       <span class="figure-title">MCMC visualization. A normal x is chosen, whose posterior <em>q<sub>&#x03D5;</sub>       </em>(z|x) is plotted at right: the cross denotes <em>&#x03BC;</em>       <sub>x</sub> and the ellipse denotes its 3-<em>&#x03C3;</em>       <sub>x</sub> region. We randomly set 15% x points as missing, to obtain the abnormal x&#x2032;. We run MCMC over x&#x2032; with 10 iterations. At first, the z sample is far from <em>q<sub>&#x03D5;</sub>       </em>(z|x). After that, z samples quickly approach <em>q<sub>&#x03D5;</sub>       </em>(z|x), and begin to move around <em>q<sub>&#x03D5;</sub>       </em>(z|x) after only 3 iterations.</span>      </div>     </figure>    </p>    <p>     <em>Donut</em> is forced to reconstruct normal points within abnormal windows correctly during training, by M-ELBO. It is thus explicitly trained to find good posteriors. This is the main reason why M-ELBO plays a vital role in Fig <a class="fig" href="#fig8">8</a>. Missing data injection amplifies the effect of M-ELBO, with synthetically generated missing points. On the other hand, MCMC imputation does not change the training process. Instead, it improves the detection, by iteratively approaching better posteriors, as illustrated in Fig <a class="fig" href="#fig13">13</a>.</p>    <p>Despite these techniques, <em>Donut</em> may still fail to find a good posterior, if there are too many anomalies in x. In our scenario, the KPIs are time sequences, with one point per minute. For long-lasting anomalies, having the correct detection scores and raise alerts at first few minutes are sufficient in our context<a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a>. The operators can take action once any score reaches the threshold, and simply ignore the following inaccurate scores. <strong>Nevertheless, the KDE interpretation can help us know the limitations of reconstruction probability, in order to use it properly.</strong>    </p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Causes of Time Gradient</h3>     </div>    </header>    <p>In this section we discuss the causes of the time gradient effect. To simplify the discussion, let us assume training x are all normal, thus M-ELBO is now equivalent to the original ELBO. M-ELBO can then be decomposed into three terms as in Eqn (4) (we leave out some subscripts for shorter notation). <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \mathcal {L}({\mathbf{{x}}}) &#x0026;= \operatorname{\mathbb {E}}_{{q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}})}}\left[{ \log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}}) + \log p_{\theta }({\mathbf{{z}}}) - \log q_{\phi }({\mathbf{{z}}}|{\mathbf{{x}}}) }\right] \\ &#x0026;= \operatorname{\mathbb {E}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right] + \operatorname{\mathbb {E}}\left[{\log p_{\theta }({\mathbf{{z}}})}\right] + \operatorname{H}\left[{\mathbf{{z}}}|{\mathbf{{x}}}\right] \qquad \mathrm{(5)}\end{align*} </span>       <br/>      </div>     </div> The 1st term requires z samples from <em>q<sub>&#x03D5;</sub>     </em>(z|x) to have a high likelihood of reconstructing x. As a result, <em>q<sub>&#x03D5;</sub>     </em>(z|x) for x with dissimilar shapes are separated. The 2nd term causes <em>q<sub>&#x03D5;</sub>     </em>(z|x) to concentrate on <span class="inline-equation"><span class="tex">$\mathcal {N}({\mathbf{{0}}},{\mathbf{{I}}})$</span>     </span>. The 3rd term, the entropy of <em>q<sub>&#x03D5;</sub>     </em>(z|x), causes <em>q<sub>&#x03D5;</sub>     </em>(z|x) to expand wherever possible. Recall the 2nd term sets a restricted area for <em>q<sub>&#x03D5;</sub>     </em>(z|x) to expand (see Fig 10c for the combination effect of the 2nd and 3rd term). Taking the 1st term into account, this expansion would also stop if <em>q<sub>&#x03D5;</sub>     </em>(z|x) for two dissimilar x reach each other. In order for every <em>q<sub>&#x03D5;</sub>     </em>(z|x) to have a maximal territory when training converges (<em>i.e.</em>, these three terms reach an equilibrium), similar x would have to get close to each other, allowing <em>q<sub>&#x03D5;</sub>     </em>(z|x) to grow larger with overlapping boundaries. Since contiguous x are similar in seasonal KPIs (and vise versa), the time gradient would be a natural consequence, if such equilibrium could be achieved.</p>    <p>Next we discuss how the equilibrium could be achieved. The SGVB algorithm keeps pushing <em>q<sub>&#x03D5;</sub>     </em>(z|x) for dissimilar x away during training, as illustrated in Fig <a class="fig" href="#fig14">14</a>. The more dissimilar two <em>q<sub>&#x03D5;</sub>     </em>(z|x) are, the further they are pushed away. Since we initialize the variational network randomly, <em>q<sub>&#x03D5;</sub>     </em>(z|x) are mixed everywhere when training just begins, as Fig 10b shows. At this time, every <em>q<sub>&#x03D5;</sub>     </em>(z|x) are pushed away by all other <em>q<sub>&#x03D5;</sub>     </em>(z|x). Since x are sliding windows of KPIs, any pair of x far away in time will be generally more dissimilar, thus get pushed away further from each other. This gives <em>q<sub>&#x03D5;</sub>     </em>(z|x) an initial layout. As training goes on, the time gradient is fine-tuned and gradually established, as Fig 15a shows. The training dynamics also suggest that the learning rate annealing technique is very important, since it can gradually stabilize the layout. <figure id="fig14">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig14.jpg" class="img-responsive" alt="Figure 14"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 14:</span>       <span class="figure-title">Suppose <span class="inline-equation"><span class="tex">${\mathbf{{\mu }}}_{{\mathbf{{z}}}^{(1)}}$</span>       </span> and <span class="inline-equation"><span class="tex">${\mathbf{{\mu }}}_{{\mathbf{{z}}}^{(2)}}$</span>       </span> are the mean of <em>q<sub>&#x03D5;</sub>       </em>(z|x) corresponding to training data x<sup>(1)</sup> and x<sup>(2)</sup>, with the surrounding circles represent <span class="inline-equation"><span class="tex">${\mathbf{{\sigma }}}_{{\mathbf{{z}}}^{(1)}}$</span>       </span> and <span class="inline-equation"><span class="tex">${\mathbf{{\sigma }}}_{{\mathbf{{z}}}^{(2)}}$</span>       </span>. When these two distributions accidentally &#x201C;overlaps&#x201D; during training, the sample z<sup>(1)</sup> from <em>q<sub>&#x03D5;</sub>       </em>(z|x<sup>(1)</sup>) may get too close to <span class="inline-equation"><span class="tex">${\mathbf{{\mu }}}_{{\mathbf{{z}}}^{(2)}}$</span>       </span>, such that the reconstructed distribution will be close to <em>p<sub>&#x03B8;</sub>       </em>(x|z<sup>(2)</sup>) with some z<sup>(2)</sup> for x<sup>(2)</sup>. If x<sup>(1)</sup> and x<sup>(2)</sup> are dissimilar, log&#x2009;<em>p<sub>&#x03B8;</sub>       </em>(x<sup>(1)</sup>|z<sup>(2)</sup>) in the loss will then effectively push <span class="inline-equation"><span class="tex">${\mathbf{{\mu }}}_{{\mathbf{{z}}}^{(1)}}$</span>       </span> away from <span class="inline-equation"><span class="tex">${\mathbf{{\mu }}}_{{\mathbf{{z}}}^{(2)}}$</span>       </span>.</span>      </div>     </figure>     <figure id="fig15">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3185996/images/www2018-5-fig15.jpg" class="img-responsive" alt="Figure 15"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 15:</span>       <span class="figure-title">Evolution of the z space of dataset <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>       </span> during training. We sample normal x from validation set, and plot z samples accordingly. (a) converges to a good equilibrium, with a final F-score 0.871, while (b) converges to a sub-optimal one, with a final F-score 0.826. We plot step 4300 in (a), because it is a very important turning point, where the green points just begin to get away from the purple points.</span>      </div>     </figure>    </p>    <p>Surprisingly, we cannot find any term in M-ELBO that directly pulls <em>q<sub>&#x03D5;</sub>     </em>(z|x) for similar x together. The time gradient is likely to be caused mainly by expansion (<span class="inline-equation"><span class="tex">$\operatorname{H}\left[{\mathbf{{z}}}|{\mathbf{{x}}}\right]$</span>     </span>), squeezing (<span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}\left[{\log p_{\theta }({\mathbf{{z}}})}\right]$</span>     </span>), pushing (<span class="inline-equation"><span class="tex">$\operatorname{\mathbb {E}}\left[{\log p_{\theta }({\mathbf{{x}}}|{\mathbf{{z}}})}\right]$</span>     </span>), and the training dynamics (random initialization and SGVB). This could sometimes cause trouble, and result in sub-optimal layouts, as we shall see in &#x00A7;<a class="sec" href="#sec-24">5.4</a>.</p>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Sub-Optimal Equilibrium</h3>     </div>    </header>    <p>     <em>q<sub>&#x03D5;</sub>     </em>(z|x) may sometimes converge to a sub-optimal equilibrium. Fig 15b demonstrates such a problem, where the purple points accidentally get through the green points after the first 100 steps. The purple points push the green points away towards both sides, causing the green points to be totally cut off at around 5000 steps. As training goes on, the green points will be pushed even further, such that the model is locked to this sub-optimal equilibrium and never escapes. Such bad layout of z breaks the time gradient, where a testing x following green patterns might accidentally be mapped to somewhere between the green two halves and get recognized as purple. This would certainly downgrade the detection performance, according to the KDE interpretation.</p>    <p>When there are unlabeled anomalies, the training would become unstable so that the model might be accidentally brought out of a sub-optimal equilibrium and achieve a better equilibrium afterwards. With the help of early-stopping during training, the best encountered equilibrium is chosen eventually. This explains why sometimes having complete labels would not benefit the performance. This effect is likely to be less obvious with larger <em>K</em>, since having more dimensions gives <em>q<sub>&#x03D5;</sub>     </em>(z|x) extra freedom to grow, reducing the chance of bad layouts. When sub-optimal equilibrium is not a vital problem, the convergence of training then becomes more important, while having more labels definitely helps stabilize the training. In conclusion, using anomaly labels in <em>Donut</em> is likely to benefit the performance, as long as <em>K</em> is adequately large.</p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>    </div>    </header>    <p>In this paper, we proposed an unsupervised anomaly detection algorithm <em>Donut</em> based on VAE for seasonal KPIs with local variations. The new techniques enabled <em>Donut</em> to greatly outperform state-of-art supervised and VAE-based anomaly detection algorithms. The best F-scores of <em>Donut</em> range from 0.75 to 0.90 for the studied KPIs.</p>    <p>    <em>Donut</em>&#x2019;s excellent performance are explained by our theoretical analysis with KDE interpretation and the new discovery of the time gradient effect. Our experimental and theoretical analyses imply broader impacts: anomaly detection based on dimension reduction needs to use reconstruction; anomaly detection with generative models needs to train with both normal and abnormal data.</p>   </section>   <section id="sec-26">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> Acknowledgements</h2>    </div>    </header>    <p>The work was supported by National Natural Science Foundation of China (NSFC) under grant No. 61472214 and No. 61472210, and Alibaba Innovative Research (AIR). We also thank Prof. Jun Zhu and his PhD. student Jiaxin Shi for helpful and constructive discussions.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Mennatallah Amer, Markus Goldstein, and Slim Abdennadher. 2013. Enhancing one-class support vector machines for unsupervised anomaly detection. In <em>      <em>Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description</em>     </em>. ACM, 8&#x2013;15.</li>    <li id="BibPLXBIB0002" label="[2]">Jinwon An and Sungzoon Cho. 2015. <em>      <em>Variational Autoencoder based Anomaly Detection using Reconstruction Probability</em>     </em>. Technical Report. SNU Data Mining Center. 1&#x2013;18 pages.</li>    <li id="BibPLXBIB0003" label="[3]">Matthew&#x00A0;James Beal. 2003. <em>      <em>Variational algorithms for approximate Bayesian inference</em>     </em>. University of London London.</li>    <li id="BibPLXBIB0004" label="[4]">Christopher&#x00A0;M Bishop. 2006. <em>      <em>Pattern recognition and machine learning</em>     </em>. springer.</li>    <li id="BibPLXBIB0005" label="[5]">Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. <em>      <em>ACM computing surveys (CSUR)</em>     </em>41, 3 (2009), 15.</li>    <li id="BibPLXBIB0006" label="[6]">Yingying Chen, Ratul Mahajan, Baskar Sridharan, and Zhi-Li Zhang. 2013. A Provider-side View of Web Search Response Time. In <em>      <em>Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM</em>     </em>(SIGCOMM &#x2019;13). ACM, New York, NY, USA, 243&#x2013;254. 978-1-4503-2056-6https://doi.org/10.1145/2486001.2486035</li>    <li id="BibPLXBIB0007" label="[7]">Sarah&#x00A0;M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie. 2016. High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning. <em>      <em>Pattern Recognition</em>     </em>58(2016), 121&#x2013;134.</li>    <li id="BibPLXBIB0008" label="[8]">Romain Fontugne, Pierre Borgnat, Patrice Abry, and Kensuke Fukuda. 2010. MAWILab: Combining Diverse Anomaly Detectors for Automated Anomaly Labeling and Performance Benchmarking. In <em>      <em>Proceedings of the 6th International COnference</em>     </em>(Co-NEXT &#x2019;10). ACM, Article 8, 12&#x00A0;pages. 978-1-4503-0448-1https://doi.org/10.1145/1921168.1921179</li>    <li id="BibPLXBIB0009" label="[9]">Zhouyu Fu, Weiming Hu, and Tieniu Tan. 2005. Similarity based vehicle trajectory clustering and anomaly detection. In <em>      <em>Image Processing, 2005. ICIP 2005. IEEE International Conference on</em>     </em>, Vol.&#x00A0;2. IEEE, II&#x2013;602.</li>    <li id="BibPLXBIB0010" label="[10]">John Geweke. 1989. Bayesian inference in econometric models using Monte Carlo integration. <em>      <em>Econometrica: Journal of the Econometric Society</em>     </em> (1989), 1317&#x2013;1339.</li>    <li id="BibPLXBIB0011" label="[11]">Francisco J&#x00A0;Goerlich Gisbert. 2003. Weighted samples, kernel density estimators and convergence. <em>      <em>Empirical Economics</em>     </em>28, 2 (2003), 335&#x2013;351.</li>    <li id="BibPLXBIB0012" label="[12]">Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. <em>      <em>Deep Learning</em>     </em>. MIT Press.</li>    <li id="BibPLXBIB0013" label="[13]">Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In <em>      <em>Advances in neural information processing systems</em>     </em>. 2672&#x2013;2680.</li>    <li id="BibPLXBIB0014" label="[14]">Wolfgang H&#x00E4;rdle, Axel Werwatz, Marlene M&#x00FC;ller, and Stefan Sperlich. 2004. Nonparametric density estimation. <em>      <em>Nonparametric and Semiparametric Models</em>     </em>(2004), 39&#x2013;83.</li>    <li id="BibPLXBIB0015" label="[15]">Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <em>      <em>arXiv preprint arXiv:1412.6980</em>     </em>(2014).</li>    <li id="BibPLXBIB0016" label="[16]">Diederik&#x00A0;P Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In <em>      <em>Proceedings of the International Conference on Learning Representations</em>     </em>.</li>    <li id="BibPLXBIB0017" label="[17]">Florian Knorn and Douglas&#x00A0;J Leith. 2008. Adaptive kalman filtering for anomaly detection in software appliances. In <em>      <em>INFOCOM Workshops 2008, IEEE</em>     </em>. IEEE, 1&#x2013;6.</li>    <li id="BibPLXBIB0018" label="[18]">Balachander Krishnamurthy, Subhabrata Sen, Yin Zhang, and Yan Chen. 2003. Sketch-based change detection: methods, evaluation, and applications. In <em>      <em>Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement</em>     </em>. ACM, 234&#x2013;247.</li>    <li id="BibPLXBIB0019" label="[19]">Nikolay Laptev, Saeed Amizadeh, and Ian Flint. 2015. Generic and scalable framework for automated time-series anomaly detection. In <em>      <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>     </em>. ACM, 1939&#x2013;1947.</li>    <li id="BibPLXBIB0020" label="[20]">Alexander Lavin and Subutai Ahmad. 2015. Evaluating Real-Time Anomaly Detection Algorithms&#x2013;The Numenta Anomaly Benchmark. In <em>      <em>Machine Learning and Applications (ICMLA), 2015 IEEE 14th International Conference on</em>     </em>. IEEE, 38&#x2013;44.</li>    <li id="BibPLXBIB0021" label="[21]">Rikard Laxhammar, Goran Falkman, and Egils Sviestins. 2009. Anomaly detection in sea traffic-a comparison of the gaussian mixture model and the kernel density estimator. In <em>      <em>Information Fusion, 2009. FUSION&#x2019;09. 12th International Conference on</em>     </em>. IEEE, 756&#x2013;763.</li>    <li id="BibPLXBIB0022" label="[22]">Suk-Bok Lee, Dan Pei, MohammadTaghi Hajiaghayi, Ioannis Pefkianakis, Songwu Lu, He Yan, Zihui Ge, Jennifer Yates, and Mario Kosseifi. 2012. Threshold compression for 3g scalable monitoring. In <em>      <em>INFOCOM, 2012 Proceedings IEEE</em>     </em>. IEEE, 1350&#x2013;1358.</li>    <li id="BibPLXBIB0023" label="[23]">Dapeng Liu, Youjian Zhao, Haowen Xu, Yongqian Sun, Dan Pei, Jiao Luo, Xiaowei Jing, and Mei Feng. 2015. Opprentice: Towards Practical and Automatic Anomaly Detection Through Machine Learning. In <em>      <em>Proceedings of the 2015 ACM Conference on Internet Measurement Conference</em>     </em>(IMC &#x2019;15). ACM, New York, NY, USA, 211&#x2013;224. 978-1-4503-3848-6https://doi.org/10.1145/2815675.2815679</li>    <li id="BibPLXBIB0024" label="[24]">Wei Lu and Ali&#x00A0;A Ghorbani. 2009. Network anomaly detection based on wavelet analysis. <em>      <em>EURASIP Journal on Advances in Signal Processing</em>     </em>2009 (2009), 4.</li>    <li id="BibPLXBIB0025" label="[25]">Ajay Mahimkar, Zihui Ge, Jia Wang, Jennifer Yates, Yin Zhang, Joanne Emmons, Brian Huntley, and Mark Stockert. 2011. Rapid detection of maintenance induced changes in service performance. In <em>      <em>Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies</em>     </em>. ACM, 13.</li>    <li id="BibPLXBIB0026" label="[26]">Gerhard M&#x00FC;nz, Sa Li, and Georg Carle. 2007. Traffic anomaly detection using k-means clustering. In <em>      <em>GI/ITG Workshop MMBnet</em>     </em>.</li>    <li id="BibPLXBIB0027" label="[27]">Miguel Nicolau, James McDermott, <em>et al.</em> 2016. One-Class Classification for Anomaly Detection with Kernel Density Estimation and Genetic Programming. In <em>      <em>European Conference on Genetic Programming</em>     </em>. Springer, 3&#x2013;18.</li>    <li id="BibPLXBIB0028" label="[28]">Thomas&#x00A0;Dyhre Nielsen and Finn&#x00A0;Verner Jensen. 2009. <em>      <em>Bayesian networks and decision graphs</em>     </em>. Springer Science &#x0026; Business Media.</li>    <li id="BibPLXBIB0029" label="[29]">Brandon Pincombe. 2005. Anomaly detection in time series of graphs using arma processes. <em>      <em>Asor Bulletin</em>     </em>24, 4 (2005), 2.</li>    <li id="BibPLXBIB0030" label="[30]">Danilo&#x00A0;Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In <em>      <em>Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32</em>     </em>(ICML&#x2019;14). JMLR.org, Beijing, China, II&#x2013;1278&#x2013;II&#x2013;1286.</li>    <li id="BibPLXBIB0031" label="[31]">Terrence&#x00A0;J Sejnowski and Charles&#x00A0;R Rosenberg. 1987. Parallel networks that learn to pronounce English text. <em>      <em>Complex systems</em>     </em>1, 1 (1987), 145&#x2013;168.</li>    <li id="BibPLXBIB0032" label="[32]">Shashank Shanbhag and Tilman Wolf. 2009. Accurate anomaly detection through parallelism. <em>      <em>Network, IEEE</em>     </em>23, 1 (2009), 22&#x2013;28.</li>    <li id="BibPLXBIB0033" label="[33]">Maximilian S&#x00F6;lch, Justin Bayer, Marvin Ludersdorfer, and Patrick van&#x00A0;der Smagt. 2016. Variational inference for on-line anomaly detection in high-dimensional time series. <em>      <em>International Conference on Machine Laerning Anomaly detection Workshop</em>     </em> (2016).</li>    <li id="BibPLXBIB0034" label="[34]">Jonathan&#x00A0;AC Sterne, Ian&#x00A0;R White, John&#x00A0;B Carlin, Michael Spratt, Patrick Royston, Michael&#x00A0;G Kenward, Angela&#x00A0;M Wood, and James&#x00A0;R Carpenter. 2009. Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls. <em>      <em>Bmj</em>     </em>338(2009), b2393.</li>    <li id="BibPLXBIB0035" label="[35]">Hao Wang and Dit-Yan Yeung. 2016. Towards Bayesian deep learning: A survey. <em>      <em>arXiv preprint arXiv:1604.01662</em>     </em>(2016).</li>    <li id="BibPLXBIB0036" label="[36]">H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei, Y. Feng, J. Chen, Z. Wang, and H. Qiao. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications. <em>      <em>ArXiv e-prints</em>     </em> (Feb. 2018). arxiv:cs.LG/1802.03903</li>    <li id="BibPLXBIB0037" label="[37]">Asrul&#x00A0;H Yaacob, Ian&#x00A0;KT Tan, Su&#x00A0;Fong Chien, and Hon&#x00A0;Khi Tan. 2010. Arima based network anomaly detection. In <em>      <em>Communication Software and Networks, 2010. ICCSN&#x2019;10. Second International Conference on</em>     </em>. IEEE, 205&#x2013;209.</li>    <li id="BibPLXBIB0038" label="[38]">He Yan, Ashley Flavel, Zihui Ge, Alexandre Gerber, Dan Massey, Christos Papadopoulos, Hiren Shah, and Jennifer Yates. 2012. Argus: End-to-end service anomaly detection and localization from an ISP&#x0027;s point of view. In <em>      <em>INFOCOM, 2012 Proceedings IEEE</em>     </em>. IEEE, 2756&#x2013;2760.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>Dan Pei is the corresponding author.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>In general it should give no useful information by computing the expectation of log&#x2009;<em>p<sub>&#x03B8;</sub>    </em>(x|z) upon the posterior <em>q<sub>&#x03D5;</sub>    </em>(z|x), using a <strong>potentially abnormal</strong> x.</p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a>We call a x partially abnormal if only a small portion of points within x are abnormal, such that we can easily tell what normal pattern x should follow.</p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a>The weights <em>q<sub>&#x03D5;</sub>    </em>(z|x) are implicitly applied by sampling in Monte Carlo integration.</p>   <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a>In practice, ensuing and continuous alerts are typically filtered out anyway.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3185996">https://doi.org/10.1145/3178876.3185996</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
