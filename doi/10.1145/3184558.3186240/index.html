<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Uniform Random Sampling Not Recommended</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186240'>https://doi.org/10.1145/3184558.3186240</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186240'>https://w3id.org/oa/10.1145/3184558.3186240</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Uniform Random Sampling Not
          Recommended</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Jianguo</span> <span class=
          "surName">Lu</span> School of Computer Science University
          of Windsor, <a href=
          "mailto:jlu@uwindsor.ca">jlu@uwindsor.ca</a>
        </div>
        <div class="author">
          <span class="givenName">Hao</span> <span class=
          "surName">Wang</span> School of Computer Science
          University of Windsor, <a href=
          "mailto:wang115o@uwindsor.ca">wang115o@uwindsor.ca</a>
        </div>
        <div class="author">
          <span class="givenName">Dingding</span> <span class=
          "surName">Li</span> Department of Economics University of
          Windsor, <a href=
          "mailto:dli@uwindsor.ca">dli@uwindsor.ca</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186240"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186240</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We show that uniform random sampling is not as
        effective as PPS (probability proportional to size)
        sampling in many estimation tasks. In the setting of
        (graph) size estimation, this paper demonstrates that
        random edge sampling outperforms random node sampling, with
        a performance ratio proportional to the normalized graph
        degree variance. This result is particularly important in
        the era of big data, when data are typically large and
        scale-free, resulting in large degree variance. We derive
        the result by first giving the variances of random node and
        random edge estimators. A simpler and more intuitive result
        is obtained by assuming that the data is large and degree
        distribution follows a power law.</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Jianguo Lu, Hao Wang, and Dingding Li. 2018. Uniform
          Random Sampling Not Recommended. In <em>WWW '18
          Companion: The 2018 Web Conference Companion,</em>
          <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New
          York, NY, USA</em> 5 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186240" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186240</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Size estimation is a classic problem that has many
      applications, ranging from the war time problem of finding
      out the number of German tanks [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>], to the more recent challenge of
      gauging the size of the Web and search engines [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0012">12</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>] and online
      social networks [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>]. The direct calculation of data size
      is often not possible or desirable for several reasons. Quite
      often, data are hidden behind some searchable interfaces and
      programmable web APIs, such as online social networks and
      deep web data sources. The access is limited, and the data in
      its entirety are not available [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>]. The data can be distributed, and
      there is no central data repository such as in the case of
      peer-to-peer networks [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>] or the Web [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>]. Even when the data are
      available in one place, there are requirements for fast
      just-in-time analysis of the data [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0010">10</a>]. Regardless of a large
      variety of application scenarios, a common approach to
      solving these problems is to use samples to have a fast
      estimation of the data size, instead of slow and direct
      counting of the data.</p>
      <p>Many datasets can be viewed as graphs, especially the ones
      extracted from the Web and online social networks such as
      Twitter and Facebook. These graphs are large, often
      distributed and hidden behind searchable interfaces. The
      sampling process requires sending queries that occupy network
      traffic. In addition, most data sources impose daily quotas.
      In such cases, the sample size has to be far less than the
      data size, and it is paramount to choose an efficient
      sampling and estimation method.</p>
      <p>For ease of discussion, sampling is modelled in the
      context of a graph, where uniform sampling corresponds to
      uniform random node (RN) sampling, PPS (probability
      proportional to size) sampling corresponds to random edge
      (RE) sampling. In this setting, we define the size as the
      number of nodes in the graph. Random walk (RW) sampling
      approximates PPS sampling in that the sampling probability is
      proportional to its degree asymptotically.</p>
      <p>The norm of size estimation is to use uniform random
      samples whenever possible. Real data sources seldom provide
      uniform random samples directly. Therefore, there have been
      tremendous efforts to obtain uniform random samples from the
      Web [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>],
      search engine indexes [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>], and online social networks
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>], to name a
      few. These uniform random samples are costly, in that each
      valid sample may be accompanied by many invalid ones that are
      thrown away. Recently, it was empirically observed that,
      instead of obtaining those costly uniform random samples, RW
      sampling is actually better than RN sampling for size
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>] and
      average degree estimation [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>][<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>] on <em>some</em> datasets.</p>
      <p>This paper shows that the sampling methods for very large
      graphs should be different from the ones traditionally
      preferred. Instead of RW, we show that it is RE that is
      better than RN when the graph is very large. We demonstrate
      our conclusion not only empirically on 18 datasets and
      simulated data, but also analytically by showing that its
      variance is smaller in our setting. In addition, we
      delineated the details as for</p>
      <ul class="list-no-style">
        <li id="list1" label="•">When is RE better than RN? RE is
        better than RN only when the graph is very large, and
        consequently, the sample size <em>n</em> has to be much
        smaller than the data size <em>N</em>. This is the scenario
        we assume, with application background such as estimating
        online social networks with a limited number of web-based
        queries.<br /></li>
        <li id="list2" label="•">How much better is RE over RN? We
        demonstrate that there is an upper bound for the
        performance improvement, which is quantified by <em>γ</em>
        <sup>2</sup> + 1. Here <em>γ</em> is the coefficient of
        variation of node degrees. The upper bound is derived
        analytically, and confirmed empirically on 18 large data
        sets. The derivation uses the assumption that the data is
        very large.<br /></li>
        <li id="list3" label="•">What can approximate RE sampling?
        When RE sampling is not available in practice, we need to
        resort to other methods to approximate RE (or PPS)
        sampling. RW is an option, but the performance varies
        widely from data to data. We find that RW can approximate
        the performance of RE for online social networks, but not
        for Web graphs.<br /></li>
      </ul>
      <p>This result is particularly important in the age of big
      data when large and scale-free networks are ubiquitous
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0002">2</a>] [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>]. These
      networks can have very large degree variance. In theory,
      <em>γ</em> <sup>2</sup> can be infinitely large when the
      slope of the scale-free network falls under certain range. In
      practice, we observe <em>γ</em> <sup>2</sup> as large as 1300
      for the Twitter network in 2009&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>], meaning that potentially
      RE sampling can be better in three orders of magnitude in
      terms of variance. Such huge difference between the sampling
      methods will not only change the landscape of sampling
      practice, but also shift the research focus. In the past,
      people strive for uniform random samples [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>]. Nowadays for very large
      data, we should take PPS samples, or develop sampling methods
      that can approximate PPS sampling.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Sampling
          Methods and Their Estimators</h2>
        </div>
      </header>
      <p>Given an undirected graph <em>G</em>(<em>V</em>,
      <em>E</em>), where <em>V</em> is the set of nodes, and
      <em>E</em> the set of edges. Let <em>N</em> = |<em>V</em>|,
      the parameter we want to estimate. Nodes are labeled as 1, 2,
      …, <em>N</em>, and their corresponding degrees are <em>d</em>
      <sub>1</sub>, <em>d</em> <sub>2</sub>, …,
      <em>d<sub>N</sub></em> . The volume of the graph is
      <span class="inline-equation"><span class="tex">$\tau =\sum
      _{i=1}^{N}d_i$</span></span> , the average degree is
      <span class="inline-equation"><span class="tex">$\langle d
      \rangle =\frac{1}{N}\sum _{i=1}^{N}d_i=\tau /N$</span></span>
      . The variance <em>σ</em> <sup>2</sup> of the degrees in the
      graph is defined as <em>σ</em> <sup>2</sup> = ⟨<em>d</em>
      <sup>2</sup>⟩ − ⟨<em>d</em>⟩<sup>2</sup>, where <span class=
      "inline-equation"><span class="tex">$\langle d^2 \rangle
      =\sum _{i=1}^{N}d_i^2/N$</span></span> is the second moment,
      i.e., the arithmetic mean of the square of the degrees. The
      coefficient of variation (denoted as <em>γ</em>) is defined
      as the standard deviation, or the square root of the
      variance, normalized by the mean of the degrees:</p>
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \gamma
          ^2=\frac{\sigma ^2}{\langle d \rangle ^2}=\frac{\langle
          d^2 \rangle }{\langle d \rangle ^2}-1.
          \end{align}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>Let <em>Γ</em> = <em>γ</em> <sup>2</sup> + 1.
      <p></p>
      <p>Suppose that a sample of <em>n</em> nodes <span class=
      "inline-equation"><span class="tex">$(d_{x_1}, \dots ,
      d_{x_n})$</span></span> is taken from the graph, where
      <em>x<sub>i</sub></em> ∈ {1, 2, …, <em>N</em>} for <em>i</em>
      = 1, 2, …, <em>n</em>. Among them, there are
      <em>f<sub>j</sub></em> nodes that are sampled exactly
      <em>j</em> times. Then, sample size <em>n</em> =
      ∑<em>jf<sub>j</sub></em> . Let <em>C</em> denote the number
      of collisions in the sample, i.e., <span class=
      "inline-equation"><span class="tex">$C=\sum {j \choose 2}
      f_j$</span></span> . Note that <em>C</em> is larger than the
      number of duplicates that is often used in capture-recapture
      methods [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0004">4</a>].
      Our task is to estimate <em>N</em> using the sample. Table
      summarizes the notations used in this paper.</p>
      <p>This paper focuses on three basic sampling methods, i.e.,
      RN (random node), RE (random edge), and RW (random walk). In
      RN sampling, each node is sampled uniformly at random with
      replacement. In RE sampling, edges are selected with equal
      probability and two nodes incident to a random edge are
      collected. Thus, RE sampling is a kind of PPS (probability
      proportional to size) sampling in that each node is sampled
      with probability proportional to its degree. RW sampling
      selects the next node in the current neighbourhood uniformly
      at random. Its node selection probability is proportional to
      the degree asymptotically.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> RN
            Sampling</h3>
          </div>
        </header>
        <p>Different sampling methods require different estimators.
        When nodes are sampled uniformly at random, each node is
        sampled with equal probability, i.e.,</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} p_i= \frac{1}{N},
            \text{ for } i = 1, 2, \dots , N.
            \end{align}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>When two nodes are chosen, the probability that a
        collision (the same node being selected twice) happens is
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} p=\sum
            _{i=1}^{N}p_i^2= \frac{1}{N^2} \sum _{i=1}^{N}1
            =\frac{1}{N} . \end{align}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <p></p>
        <p>Since there are <span class=
        "inline-equation"><span class="tex">${n \choose
        2}$</span></span> pairs, the expected number of collisions
        is</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathbb {E}(C)=
            {n \choose 2} \sum _{i=1}^{N}p_i^2 ={n \choose 2}
            \frac{1}{N}. \end{align}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>
        <p></p>
        <p>Thus, the RN estimator for <em>N</em> is</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            \widehat{N}_N={n\choose 2}\frac{1}{C}.
            \end{align}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> RE
            Sampling</h3>
          </div>
        </header>
        <p>When nodes are chosen with probability proportional to
        their sizes, the probability of choosing node <em>i</em> is
        <em>p<sub>i</sub></em> = <em>d<sub>i</sub></em>
        /<em>τ</em>, where ∑<em>p<sub>i</sub></em> = 1. When two
        nodes are chosen independently at random with probability
        proportional to size <em>d<sub>i</sub></em> , the
        probability that a collision happens is</p>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} p=\sum
            _{i=1}^{N}p_i^2= \frac{1}{\tau ^2} \sum _{i=1}^{N}d_i^2
            =\frac{\Gamma }{N} . \end{align}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <p></p>
        <p>The expected number of collisions <em>C</em> is</p>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathbb {E}(C)=
            {n \choose 2} \sum _{i=1}^{N}p_i^2 ={n \choose 2}
            \frac{\Gamma }{N}. \end{align}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>
        <p></p>
        <p>Thus, the RE estimator for <em>N</em> is</p>
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            \widehat{N}_E={n\choose 2}\frac{\Gamma }{C}.
            \end{align}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>
        <p></p>
        <p>Thereby, we derived the RE estimator using <em>Γ</em>.
        The introduction of <em>Γ</em> in the estimator is
        important–it reveals the difference between the RE and RN
        estimators, consequently we can compare them. The same
        estimator in very different forms are used in [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0004">4</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0011">11</a>]. Our
        derivation is different, so that we can compare these two
        estimators for uniform and PPS samples. Comparing the
        estimators in equations <a class="eqn" href="#eq5">5</a>
        and <a class="eqn" href="#eq8">8</a>, the only difference
        is that RE sampling produces <em>Γ</em> times more
        collisions using the same sample size. Consequently, the
        estimate is adjusted by a factor of <em>Γ</em>. When more
        collisions are observed, the accuracy of the estimation is
        also improved. Intuitively, RE method can outperform RN
        sampling by a factor of <em>Γ</em>. In reality, the
        performance improvement is upper-bounded by <em>Γ</em> as
        we will show in this paper.</p>
        <p>The second issue is whether <em>Γ</em> is large enough
        to result in significant performance improvement for RE
        sampling. Our first observation is that when the graph
        being studied is regular, <em>Γ</em> = 1 and the RE
        estimator is reduced to the RN estimator. However, many
        networks are large and scale-free, inducing very large
        <em>Γ</em>. For instance, <em>Γ</em> ≈ 1300 for the Twitter
        user network in the year of 2009 [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0015">15</a>]. This large <em>Γ</em>
        makes the RE sampling the obvious choice.</p>
        <p>The third issue is that <em>Γ</em> itself needs to be
        estimated. <em>Γ</em> is the ratio of the average degree of
        the sampled nodes and the average degree of the original
        graph, and can be estimated using the following formula
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0015">15</a>]:</p>
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \widehat{\Gamma
            }=\frac{\widehat{\langle d_x\rangle }}{\widehat{\langle
            d \rangle }}=\frac{\sum _{i=1}^{n}d_{x_i}}{n}
            \frac{1}{\widehat{\langle d \rangle }}.
            \end{align}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>
        <p></p>
        <p>In turn, the average degree can be estimated by the
        harmonic mean with high accuracy [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0016">16</a>]:</p>
        <div class="table-responsive" id="eq10">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \widehat{\langle
            d \rangle }=\frac{n}{\sum _{i=1}^{n}1/d_{x_i}}.
            \end{align}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>
        <p></p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Variances Of
          The Estimators</h2>
        </div>
      </header>
      <p>Estimators are normally evaluated in terms of bias,
      variance (<span class="inline-equation"><span class=
      "tex">$var(\widehat{N})$</span></span> ), and the combination
      of them, i.e., mean squared error (MSE). In [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>], we discussed the bias
      problem, which is rather small in general. This paper focuses
      on the variances of the two estimators. We do not use
      Chebyshev's inequality for evaluation as some other papers
      do, because Chebyshev's inequality gives an upper bound that
      is valid for any data distribution. Consequently,
      experimental results can not be explained well using
      Chebyshev's inequality. We observed that the estimates are of
      normal distribution [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>], thus there is a much tighter
      bounds. For instance, when relative standard error
      RSE<span class="inline-equation"><span class="tex">$
      \left(=\sqrt {var(\widehat{N})}/N \right)$</span></span> is
      0.1, the 95% confidence interval is roughly <span class=
      "inline-equation"><span class="tex">$\widehat{N}\pm 0.2
      \widehat{N}$</span></span> . This is the why in our
      experiments the RSE values are around 0.1.</p>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Variance of
            RN Sampling</h3>
          </div>
        </header>
        <p>We derive the variances using the classic Delta method.
        The key difference is the approximations we make due to the
        big data assumption. Otherwise, the Taylor expansion has a
        sequence of long terms, and loses the intuitive
        understanding. Let <em>C</em>, the number of collisions, be
        the random variable. The Taylor expansion of 1/<em>C</em>
        around <span class="inline-equation"><span class=
        "tex">$\mathbb {E}(C)$</span></span> is:</p>
        <div class="table-responsive" id="eq11">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            \frac{1}{C}=\frac{1}{\mathbb
            {E}(C)}&amp;-\frac{C-\mathbb {E}(C)}{\mathbb {E}(C)^2}
            +\frac{2}{\mathbb {E}(C)^3}\frac{(C-\mathbb
            {E}(C))^2}{2!} \dots \end{align}</span><br />
            <span class="equation-number">(11)</span>
          </div>
        </div>
        <p></p>
        <p>By applying <em>var</em> on Eq. <a class="eqn" href=
        "#eq5">5</a>, and taking the first two terms in the Taylor
        expansion, we have</p>
        <div class="table-responsive" id="eq12">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            var(\widehat{N}_N) \approx
            \frac{n^4}{4}var\left(\frac{1}{C}\right) =\frac{n^4}{4
            \mathbb {E}(C)^4}var(C). \end{align}</span><br />
            <span class="equation-number">(12)</span>
          </div>
        </div>
        <p></p>
        <p>When selecting two nodes randomly from a set of
        <em>N</em> nodes, the probability of having a collision is
        <em>p</em> = 1/<em>N</em>. When <em>n</em> number of sample
        nodes are selected, there are <span class=
        "inline-equation"><span class="tex">${n \choose
        2}$</span></span> pairs. The number of collisions follows
        the binomial distribution <em>B</em>(<em>n</em>(<em>n</em>
        − 1)/2, 1/<em>N</em>) whose variance is</p>
        <div class="table-responsive" id="eq13">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} var(C)={n \choose
            2}p(1-p) = \mathbb {E}(C) (1-1/N)
            \end{align}</span><br />
            <span class="equation-number">(13)</span>
          </div>
        </div>When <em>N</em> is large, <span class=
        "inline-equation"><span class="tex">$var(C) \approx \mathbb
        {E}(C)$</span></span> . Substitute this into Eq. <a class=
        "eqn" href="#eq12">12</a>, and note that <span class=
        "inline-equation"><span class="tex">$n^2/(2\mathbb
        {E}(C))=N$</span></span> , we derive the following:
        <p></p>
        <div class="lemma" id="enc1">
          <label>Lemma 1.</label>
          <p>(Variance of <span class=
          "inline-equation"><span class="tex">$\widehat{N}_N$</span></span>
          ) The estimated variance of RN estimator <span class=
          "inline-equation"><span class=
          "tex">$\widehat{N}_N$</span></span> is</p>
          <div class="table-responsive" id="eq14">
            <div class="display-equation">
              <span class="tex mytex">\begin{align}
              \widehat{var}(\widehat{N}_N) \approx
              \frac{N^2}{\mathbb {E}(C)} \approx \frac{2N^3}{n^2}.
              \end{align}</span><br />
              <span class="equation-number">(14)</span>
            </div>
          </div>
          <p></p>
        </div>
        <p>Reformulating the above result using RSE, we see that
        the accuracy of the estimation depends solely on the
        expected number of collisions:</p>
        <div class="table-responsive" id="eq15">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            RSE(\widehat{N}_N) =\frac{\sqrt
            {\widehat{var}(\widehat{N}_N)}}{N} \approx
            \frac{1}{\sqrt {\mathbb {E}(C)}}.
            \end{align}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>
        <p></p>
        <p>Since the derivation employs several approximations, we
        conduct a simulation study to verify our result and
        understand its limitation. The simulation study is depicted
        in Fig. . The data size <em>N</em> = 10<sup>6</sup>. Sample
        sizes range between 4472 and 14142, so that the expected
        collisions range between 10 and 100. For each sample size,
        estimations are repeated 1000 times to obtain the observed
        collisions and RSEs.</p>
        <p>First, the simulation study shows that random variable
        <em>C</em> does follow the binomial distribution
        <em>B</em>(<em>n</em>(<em>n</em> − 1)/2, <em>p</em>) as
        depicted in panels (A) and (B) of Fig. . Both plots are
        histograms of the collisions, along with the corresponding
        binomial distributions. Panel (A) plots the histogram when
        <span class="inline-equation"><span class="tex">$\mathbb
        {E}(C)=10$</span></span> , panel (B) is when <span class=
        "inline-equation"><span class="tex">$\mathbb
        {E}(C)=100$</span></span> .</p>
        <p>Second, the observed variance of <em>C</em> fits the
        estimated variance very well over various sample sizes, as
        illustrated by panel (C). I.e., <span class=
        "inline-equation"><span class="tex">$\widehat{var}(C)
        \approx \mathbb {E}(C)$</span></span> . Third, the observed
        RSE (or equivalently variance) fits the estimated RSE when
        sample size is not very small. From panel (D) we can see
        that RSE of <span class="inline-equation"><span class=
        "tex">$\widehat{N}_N$</span></span> is about <span class=
        "inline-equation"><span class="tex">$1/\sqrt {\mathbb
        {E}(C)}$</span></span> when <span class=
        "inline-equation"><span class="tex">$\mathbb
        {E}(C){\gt}20$</span></span> . When <span class=
        "inline-equation"><span class="tex">$\mathbb
        {E}(C)=10$</span></span> , there is a gap between the
        estimated and observed RSEs, introduced by the Taylor
        expansion approximation. When <span class=
        "inline-equation"><span class="tex">$\mathbb
        {E}(C)$</span></span> is as small as 10, the third term in
        Eq.<a class="eqn" href="#eq11">11</a> can be no longer
        omitted.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Variance of
            RE Sampling</h3>
          </div>
        </header>
        <p>The variance of RE estimator involves three variables,
        the collisions <em>C</em>, the estimated average degree
        <span class="inline-equation"><span class=
        "tex">$\widehat{\langle d \rangle }$</span></span> of the
        original graph, and the average degree of the sampled nodes
        <span class="inline-equation"><span class=
        "tex">$\widehat{\langle d_x\rangle }$</span></span> . The
        variance of <span class="inline-equation"><span class=
        "tex">$\widehat{N}_E$</span></span> is too complicated to
        compare with that of <span class=
        "inline-equation"><span class=
        "tex">$\widehat{N}_N$</span></span> without some
        assumptions. We assume that <em>N</em> is very large, and
        <em>C</em> ≈ 100. Consequently <span class=
        "inline-equation"><span class="tex">$n=\sqrt {2NC/\Gamma
        }$</span></span> . We can see that <em>C</em> ≪ <em>n</em>
        ≪ <em>N</em>. We restrict the collisions around 100 so that
        the corresponding <span class=
        "inline-equation"><span class="tex">$\widehat{N}_N$</span></span>
        estimator has RSE 0.1, or, the 95% confidence interval is
        <em>N</em> ± 0.2<em>N</em>. Under such assumption, we can
        approximate the variance of <span class=
        "inline-equation"><span class=
        "tex">$\widehat{N}_E$</span></span> as follows:</p>
        <div class="lemma" id="enc2">
          <label>Lemma 2.</label>
          <p>The variance of <span class=
          "inline-equation"><span class=
          "tex">$\widehat{N}_E$</span></span> is</p>
          <div class="table-responsive" id="eq16">
            <div class="display-equation">
              <span class="tex mytex">\begin{align}
              var(\widehat{N}_E)&amp;\approx \frac{N^2}{\mathbb
              {E}(C)} \left(1+\frac{2n\langle d^3 \rangle }{N\Gamma
              \langle d \rangle ^3} \right)
              \end{align}</span><br />
              <span class="equation-number">(16)</span>
            </div>
          </div>
          <p></p>
        </div>
        <p>Comparing the variances for RN and RE samplings in Lemma
        1 and Lemma 2, we have the following:</p>
        <div class="theorem" id="enc3">
          <label>Theorem 1.</label>
          <p>Given the same sample size n. The variance ratio
          between RN and RE sampling is:</p>
          <div class="table-responsive" id="eq17">
            <div class="display-equation">
              <span class="tex mytex">\begin{align}
              \frac{var(\widehat{N}_N)}{var(\widehat{N}_E)} \approx
              \Gamma \left(1+\frac{2n\langle d^3 \rangle }{N\Gamma
              \langle d \rangle ^3} \right) ^{-1}
              \end{align}</span><br />
              <span class="equation-number">(17)</span>
            </div>
          </div>
          <p></p>
        </div>
        <p>We highlight two points regarding this result. First,
        when sample size <em>n</em> ≪ <em>N</em>, the second term
        in Eq. <a class="eqn" href="#eq17">17</a> is small enough
        to be negligible. In this case, RE sampling outperforms RN
        sampling up to <em>Γ</em> folds in terms of variance, and
        <span class="inline-equation"><span class="tex">$\sqrt
        \Gamma$</span></span> in terms of sample size.</p>
        <p>Second, the second term grows with sample size
        <em>n</em>, indicating eventually RN will become better.
        The tipping point is</p>
        <div class="table-responsive" id="eq18">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} n= N\Gamma
            ^2\langle d \rangle ^3/(2\langle d^3 \rangle).
            \end{align}</span><br />
            <span class="equation-number">(18)</span>
          </div>
        </div>When sampling large graphs, in general RE is better
        than RN, or <em>n</em> &lt; <em>NΓ</em>
        <sup>2</sup>⟨<em>d</em>⟩<sup>3</sup>/(2⟨<em>d</em>
        <sup>3</sup>⟩), as we will show in our simulation studies
        and in 18 real networks. This is due to two reasons: 1)
        <em>n</em> is in the order of <span class=
        "inline-equation"><span class="tex">$\sqrt {2N/\Gamma
        }$</span></span> to generate enough collisions, or gain
        sufficient estimation precision. The ratio
        <em>n</em>/<em>N</em> is in the order of <span class=
        "inline-equation"><span class="tex">$O(1/\sqrt {N\Gamma
        })$</span></span> . 2) Although in theory we can let
        <em>n</em> approach or even surpass <em>N</em>, the essence
        of sampling is to use a very small portion of the data to
        predicate the properties.
        <p></p>
      </section>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experiments on
          Real Networks</h2>
        </div>
      </header>
      <p>We demonstrate our results on 18 datasets. Most of them
      are from the Stanford SNAP graph collection [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>]. Due to space limitation,
      for some network categories only one graph is reported if
      they have similar behaviour. For instance, citation graphs
      have similar degree distribution, similar coefficient of
      variation, and similar error ratios between RN, RE, and RW
      sampling. For these networks, we choose only one
      representative network for each category. In the category of
      the Web graph datasets, RW sampling deviates greatly from RE
      sampling. So we include several Web graphs, including the Web
      graph on the domains of Notre Dame, Stanford, and
      Berkley-Stanford, to investigate the cause for such
      deviation. Complete data description and programs can be
      found at <a class="link-inline force-break" href=
      "http://cs.uwindsor.ca/~jlu/size">http://cs.uwindsor.ca/~jlu/size</a>
      ,</p>
      <p>We compare the sample sizes needed to obtain the same RSE
      for all the datasets. We show that there is a strong
      correlation between <span class=
      "inline-equation"><span class="tex">$\sqrt
      \Gamma$</span></span> and <em>RN</em>/<em>RE</em> ratio. Fig.
      <a class="fig" href="#fig1">1</a> plots the sample size ratio
      against <span class="inline-equation"><span class=
      "tex">$\sqrt \Gamma$</span></span> for the 18 datasets when
      <em>RSE</em> = 0.2 (panel A) and <em>RSE</em> = 0.1 (panel
      B).</p>
      <p>The plot shows that 1) RE is better than RN consistently
      for all the datasets, as all the RN/RE ratio values are
      greater than one; 2) The ratio has a strong linear
      correlation with <span class="inline-equation"><span class=
      "tex">$\sqrt \Gamma$</span></span> as can be seen visually
      from the plot, and from the Pearson's correlation coefficient
      (0.98 when RSE=0.2 and 0.95 when RSE =0.1); 3) The
      improvement ratio is bounded from above by <span class=
      "inline-equation"><span class="tex">$\sqrt
      \Gamma$</span></span> , as all the ratio values are below the
      line.</p>
      <p>To summarize, albeit the great varieties of the datasets,
      RE sampling always outperforms RN sampling, and the ratio has
      a strong positive relation to <span class=
      "inline-equation"><span class="tex">$\sqrt {\Gamma
      }$</span></span> with very high correlation coefficient.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186240/images/www18companion-93-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">RN/RE ratio of sample sizes is bounded
          from above by <span class="inline-equation"><span class=
          "tex">$\sqrt {\Gamma }$</span></span> for 18 networks.
          Panel (A) displays the ratio of sample sizes needed to
          achieve 0.2 RSE; panel (B) the ratios to achieve 0.1 RSE.
          RSE is obtained over 500 repetitions.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Discussions and
          Conclusions</h2>
        </div>
      </header>
      <p>The state of art in size estimation is to use uniform
      random samples whenever possible. We show that on the
      contrary to this common practice, PPS sampling outperforms
      uniform random sampling by a factor up to <span class=
      "inline-equation"><span class="tex">$\sqrt {\Gamma
      }$</span></span> for large data in terms of sample size.</p>
      <p>In retrospect, this phenomenon was not observed in the
      past probably due to several reasons: 1) In traditional size
      estimation studies, <em>Γ</em> is typically small (between
      one and two), thus the difference is hardly discernible. Our
      result shows that the improvement ratio is up-bounded by
      <em>Γ</em>. Thus, when <em>Γ</em> is small, RE could be worse
      than RN. Even in scale-free networks, <em>Γ</em> in real
      networks may not be large due to the cut-off for the maximal
      values. For instance, Facebook has an up-limit of the number
      of followers, resulting small <em>Γ</em> value around two.
      Only recently we see large scale-free networks whose
      <em>Γ</em> value can be as high as 1000, such as Twitter and
      WikiTalk; 2) RE sampling is hardly studied in the past.
      Random walk sampling is often used, but it is only an
      approximation to PPS sampling. The comparison between RW and
      RN samplings often has a mixed results, failing to reveal a
      definite answer. In particular, RW on the Web graph is always
      worse than RN; 3) The result is true only for big data. In
      the synthetic data that assumes a power law distribution, we
      show that the improvement ratio grows almost linearly with
      the data size. When the data size is very small, RN can be
      better than RE even if the network is scale-free.</p>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span>
          Acknowledgements</h2>
        </div>
      </header>
      <p>The research is supported by NSERC discovery grant
      (RGPIN-2014-04463).</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Z. Bar-Yossef and M.
        Gurevich. 2008. Random sampling from a search engine's
        index. <em><em>J. ACM</em></em> 55, 5 (2008), 1–74.</li>
        <li id="BibPLXBIB0002" label="[2]">A.L. Barabási and R.
        Albert. 1999. Emergence of scaling in random networks.
        <em><em>Science</em></em> 286, 5439 (1999), 509–512.</li>
        <li id="BibPLXBIB0003" label="[3]">Andrei Broder and et
        al.2006. Estimating corpus size via queries. In
        <em><em>CIKM</em></em> . ACM, 594–603. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/1183614.1183699" target="_blank">
          https://doi.org/10.1145/1183614.1183699</a>
        </li>
        <li id="BibPLXBIB0004" label="[4]">A. Chao, SM Lee, and SL
        Jeng. 1992. Estimating population size for
        capture-recapture data when capture probabilities vary by
        time and individual animal. <em><em>Biometrics</em></em>
        48, 1 (1992), 201–216.</li>
        <li id="BibPLXBIB0005" label="[5]">Anirban Dasgupta, Ravi
        Kumar, and Tamas Sarlos. 2014. On estimating the average
        degree. In <em><em>Proceedings of the 23rd international
        conference on World wide web</em></em> . International
        World Wide Web Conferences Steering Committee,
        795–806.</li>
        <li id="BibPLXBIB0006" label="[6]">M. Gjoka, M. Kurant,
        C.T. Butts, and A. Markopoulou. 2009. A walk in Facebook:
        Uniform sampling of users in online social networks.
        <em><em>Arxiv preprint arXiv:0906.0060</em></em>
        (2009).</li>
        <li id="BibPLXBIB0007" label="[7]">Leo&nbsp;A Goodman.
        1954. Some Practical Techniques in Serial Number Analysis.
        <em><em>J. Amer. Statist. Assoc.</em></em> 49, 265 (1954),
        97–112.</li>
        <li id="BibPLXBIB0008" label="[8]">Stephen&nbsp;J Hardiman
        and Liran Katzir. 2013. Estimating clustering coefficients
        and size of social networks via random walk. In
        <em><em>Proceedings of the 22nd international conference on
        World Wide Web</em></em> . ACM, 539–550.</li>
        <li id="BibPLXBIB0009" label="[9]">M.R. Henzinger, A.
        Heydon, M. Mitzenmacher, and M. Najork. 2000. On
        near-uniform URL sampling. <em><em>Computer
        Networks</em></em> 33, 1-6 (2000), 295–308.</li>
        <li id="BibPLXBIB0010" label="[10]">Howie Huang, Nan Zhang,
        Wei Wang, Gautam Das, and A Szalay. 2012. Just-in-time
        analytics on large file systems. <em><em>Computer</em></em>
        61, 11 (2012), 1651–1664.</li>
        <li id="BibPLXBIB0011" label="[11]">L. Katzir, E. Liberty,
        and O. Somekh. 2011. Estimating sizes of social networks
        via biased sampling. In <em><em>WWW</em></em> . ACM,
        597–606.</li>
        <li id="BibPLXBIB0012" label="[12]">S. Lawrence and C.L.
        Giles. 1998. Searching the world wide web.
        <em><em>Science</em></em> 280, 5360 (1998), 98–100.</li>
        <li id="BibPLXBIB0013" label="[13]">J. Leskovec and C.
        Faloutsos. 2006. Sampling from large graphs. In
        <em><em>SIGKDD</em></em> . ACM, 631–636.</li>
        <li id="BibPLXBIB0014" label="[14]">J. Lu and D. Li. 2012.
        Sampling Online Social Networks by Random Walk. In
        <em><em>ACM SIGKDD Workshop on Hot Topics in Online Social
        Networks</em></em> . ACM, 33–40.</li>
        <li id="BibPLXBIB0015" label="[15]">Jianguo Lu and Dingding
        Li. 2013. Bias correction in small sample from big data.
        <em><em>TKDE, IEEE Transactions on Knowledge and Data
        Engineering</em></em> 25, 11(2013), 2658–2663.</li>
        <li id="BibPLXBIB0016" label="[16]">Jianguo Lu and Hao
        Wang. 2014. Variance Reduction in Large Graph Sampling.
        <em><em>Information Processing and Management</em></em> 50,
        3 (2014), 476–491.</li>
        <li id="BibPLXBIB0017" label="[17]">Sandeep Mane, Sandeep
        Mopuru, Kriti Mehra, and Jaideep Srivastava. 2005. Network
        size estimation in a peer-to-peer network.
        <em><em>University of Minnesota, MN, Tech. Rep</em></em>
        (2005), 05–030.</li>
        <li id="BibPLXBIB0018" label="[18]">M. Newman. 2010.
        <em><em>Networks: an introduction</em></em> . Oxford
        University Press, Inc.</li>
        <li id="BibPLXBIB0019" label="[19]">B. Ribeiro and D.
        Towsley. 2010. Estimating and sampling graphs with
        multidimensional random walks. In <em><em>Annual conference
        on Internet measurement</em></em> . ACM, 390–403.</li>
        <li id="BibPLXBIB0020" label="[20]">Milad Shokouhi, Justin
        Zobel, Falk Scholer, and S.&nbsp;M.&nbsp;M. Tahaghoghi.
        2006. Capturing collection size for distributed
        non-cooperative retrieval. In <em><em>SIGIR</em></em> .
        ACM, 316–323. <a class="link-inline force-break" href=
        "https://doi.org/10.1145/1148170.1148227" target="_blank">
          https://doi.org/10.1145/1148170.1148227</a>
        </li>
        <li id="BibPLXBIB0021" label="[21]">Yan Wang, Jie Liang,
        and Jianguo Lu. 2014. Discover hidden web properties by
        random walk on bipartite graph. <em><em>Information
        Retrieval</em></em> 17, 3 (2014), 203–228.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186240">https://doi.org/10.1145/3184558.3186240</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
