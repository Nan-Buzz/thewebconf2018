<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Stop the KillFies! Using Deep Learning Models to Identify Dangerous Selfies</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.TeX-AMS_CHTML.js"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Stop the KillFies! Using Deep Learning Models to Identify Dangerous Selfies</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Vedant</span>      <span class="surName">Nanda</span>     IIIT-Delhi, Carnegie Mellon University     </div>     <div class="author">     <span class="givenName">Hemank</span>      <span class="surName">Lamba</span>     IIIT-Delhi, Carnegie Mellon University     </div>     <div class="author">     <span class="givenName">Divyansh</span>      <span class="surName">Agarwal</span>     IIIT-Delhi, Carnegie Mellon University     </div>     <div class="author">     <span class="givenName">Megha</span>      <span class="surName">Arora</span>     IIIT-Delhi, Carnegie Mellon University     </div>     <div class="author">     <span class="givenName">Niharika</span>      <span class="surName">Sachdeva</span>     IIIT-Delhi, Carnegie Mellon University     </div>     <div class="author">     <span class="givenName">Ponnurangam</span>      <span class="surName">Kumaraguru</span>     IIIT-Delhi, Carnegie Mellon University, <a href="mailto:vedant15114, divyansha, niharikas, pk@iiitd.ac.in, hlamba@cs.cmu.edu, marora@andrew.cmu.edu">vedant15114, divyansha, niharikas, pk@iiitd.ac.in, hlamba@cs.cmu.edu, marora@andrew.cmu.edu</a>     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3191575" target="_blank">https://doi.org/10.1145/3184558.3191575</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>Selfies have become a prominent medium for self-portrayal on social media. Unfortunately, certain social media users go to extreme lengths to click selfies, which puts their lives at risk. Two hundred and sixteen individuals have died since March 2014 until January 2018 while trying to click selfies. It is imperative to be able to identify dangerous selfies posted on social media platforms to be able to build an intervention for users going to extreme lengths for clicking such selfies. In this work, we propose a convolutional neural network based classifier to identify dangerous selfies posted on social media using only the image (no metadata). We show that our proposed approach gives an accuracy of 98% and performs better than previous methods.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Vedant Nanda, Hemank Lamba, Divyansh Agarwal, Megha Arora, Niharika Sachdeva, and Ponnurangam Kumaraguru. 2018. Stop the KillFies! Using Deep Learning Models to Identify Dangerous Selfies. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 5 Pages. <a href="https://doi.org/10.1145/3184558.3191575" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3191575</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>A selfie is defined as <em>a photograph that one has taken of oneself, typically taken with a smartphone or a webcam and shared via social media</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. The popularity of selfie culture can be estimated from the fact that in 2015, 24 billion selfies were uploaded to Google Photos&#x00A0;<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. Pew research reported that around 55% of millennials have posted a selfie on a social media platform&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>]. Selfie nowadays has become a ubiquitous tool for self-presentation on social media.</p>    <p>Previous research has extensively focussed on the psychological and social variables of the people who post selfies. These works show that people posting a lot of selfies have personality traits such as narcissism, lack of self-esteem, self-embellishment and social alienation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. Self-embellishment has been reported as one of the primary reasons for clicking a selfie; most selfies are clicked to be posted on a social platform&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]. In extreme cases, users may often engage in dangerous activities and situations to click selfies which might make them popular on social media&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>]. Users often engage in such situations to portray themselves as adventurous and enhance their appearance to others while risking their own physical well-being&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. Continuing the statistic in &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], we found that as many as 216 individuals have died while attempting to take selfies.</p>    <p>We define a dangerous selfie as <em>a selfie which potentially might cause harm to an individual or a group that may occur while the individual(s) attempts to take a selfie</em>. To be able to detect the users who post such dangerous selfies, and to make an intervention, it is essential to find and identify dangerous selfies. By identifying such selfies being posted on the social media platform by a user, combined with the frequency at which the user is posting them, the social networking platform can decide if a particular user is overindulging in risk-taking behavior, which could potentially be harmful to their health. In this work, we propose a deep-learning based framework to identify dangerous selfies posted on Twitter. We use existing deep neural networks such as VGG16 and VGG19&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], Inception v3&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>], ResNet50&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>] etc. and adapt it to perform well on the task of detecting dangerous selfies. We discover that our model outperforms the previously proposed models by a factor of 1.34 in terms of accuracy on the test set. We believe that this work will help researchers understand a user&#x0027;s propensity to post such selfies on online social media in a much better way, thus resulting in effective intervention technologies.</p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>     </div>    </header>    <p>Numerous research works have investigated the effect of selfie culture on the mental well-being of selfie-er. Researchers discovered that the people who post more selfies have shallow relationships with people&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] or decreased intimacy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>], ultimately leading to feelings of loneliness and worry. These users were also found to have the dark triad personality (narcissism, psychopathy, and machiavellianism)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. Previous research has also tried to view the number of likes, comments, and shares an individual gets for their selfies as the social currency for the youth, and this desire of gaining more of such currency prompts youth to extreme lengths&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>].</p>    <p>     <strong>Selfies and physical harm</strong>: Subrahmanyam et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>] discuss how selfie can cause physical harm to the selfie-ers in different situations. Lamba et al. was the first work in the area of dangerous selfies to characterize the number of selfie deaths in the past years, and analyze their victims and causes&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. They also proposed a multi-modal classifier which takes into account posts&#x2019; text, image, and location to identify if a particular user is in a dangerous situation or not. In this work, we show how our method outperforms the previously proposed approaches.</p>    <p>     <strong>Deep-Learning and Image Recognition</strong>: In the recent years, a lot of work has been done in the field of large-scale visual recognition and image classification. Many methods are available, including Alexnet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], which was the first model to popularize the use of convolutional neural networks (CNNs) for object recognition. Following AlexNet, many different architectures were proposed and improvement was noted with GoogLeNet/Inception&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>], VGG 16 and VGG 19&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>], and Inception v3&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. All of these architectures have obtained high accuracies of classifying images in the Imagenet dataset. However, training and testing them on social media images has been a challenge since getting annotations isn&#x0027;t easy.</p>    <p>The past work on classifying dangerous selfies is heavily dependent on using image captions, text and location features of a post. Leetaru et al. &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>] find that only 1.6% of tweets have the exact location, which makes it hard to infer using the previously proposed model if a given selfie is dangerous or not. Moreover, getting image captions can also be challenging in some cases (say on a smartphone), thus rendering the previously proposed approach non-tractable. Our method leverages the high learning capabilities of these very deep neural networks to build a classifier which detects if a given image is a dangerous selfie or not. We also try SVMs and fine-tune the model to perform a large-scale analysis of how the models perform for our task.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Dataset</h2>     </div>    </header>    <p>For the data-collection process, we chose Twitter as it is a popular social media observing selfie culture. We collected tweets related to selfies by searching words like <em>selfie</em> or its immediate variants (#selfie, #dangerousselfie, #extremeselfie, #letmetakeaselfie, #selfieoftheday, and #drivingselfie). The data collection was done between August 1, 2016, and September 27, 2016. Through this method, we obtained 138K unique tweets posted by 78K individual users. The dataset was filtered for only images and geo-location. Following this, we were left with 9,444 geocoded tweets. To validate which of the images contained in tweets were selfies, we trained a classifier (explained below).</p>    <p>     <strong>Pre-processing:</strong> We used the same preprocessing methodology as proposed in &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. We use a classifier to distinguish selfie images from non-selfie images based on the CNN model architecture InceptionV3&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]. After curating, we manually annotate a dataset of 2.1K images into 1.3K selfies and 800 non-selfies. We used a transfer learning framework (DeCAF&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]) to retrain the Inception model for our dataset. We found that this model gave 88.48% accuracy with 10-fold cross-validation using which the labels (selfie or not a selfie) were obtained for all the 9,444 geocoded images. This process yielded a candidate set of 6,842 tweets which were potential tweets containing selfies, rest being flagged by the model as non-selfie tweets.</p>    <p>     <strong>Manual Annotation:</strong> The final step for identifying dangerous selfies involved human annotations on the obtained selfie candidate set of 6,842 tweets. For the purpose of annotation, we developed a web interface and provided each annotator with an authenticating login and password. We recruited annotators via posting a request for participation on the mailing list of different universities. The annotation session started with a 15 minute introduction about the annotation procedure. All annotators used the &#x201C;dangerous selfie definition&#x201D; provided by the authors in Section &#x00A0;<a class="sec" href="#sec-2">1</a>. Following the introduction, each annotator marked whether they would consider the shown image as a selfie and if so, whether it is a dangerous selfie or not. We also asked annotators to note the possible reason for it being dangerous such as &#x201C;selfie was taken on a mountain&#x201D;. Each selfie was annotated by 3 distinct annotators. The inter-annotator agreement rate, using the Fleiss Kappa metric&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] was 0.58, thus indicating moderate agreement between the annotators&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. We used majority voting to decide the final label for a given selfie, and ties were resolved randomly. We found that from the selfie candidate set of 6,842 tweets, our annotators agreed that 6,460 tweets contained selfies. Among these, 623 were marked as dangerous selfie containing tweets and remaining 5,837 as non-dangerous. We conduct all our future analysis on this set of 6,460 annotated tweets. It should be noted that this dataset was curated by the authors in a previous work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>] and this work uses the same dataset.&#x00A0;<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>    </p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Proposed Classifier</h2>     </div>    </header>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Results for fine-tuned models. ResNet50 with 128 nodes in the densely connected layer outperforms other models.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">Model Name (optimal nodes in the dense layer)</td>       <td style="text-align:left;">Train set accuracy</td>       <td style="text-align:left;">Test set accuracy</td>       <td style="text-align:left;">Precision</td>       <td style="text-align:left;">Recall</td>       <td style="text-align:left;">F1 score</td>      </tr>      <tr>       <td style="text-align:left;">VGG 16 (512)</td>       <td style="text-align:left;">0.973</td>       <td style="text-align:left;">0.979</td>       <td style="text-align:left;">0.971</td>       <td style="text-align:left;">0.989</td>       <td style="text-align:left;">0.980</td>      </tr>      <tr>       <td style="text-align:left;">VGG 19 (256)</td>       <td style="text-align:left;">0.969</td>       <td style="text-align:left;">0.976</td>       <td style="text-align:left;">0.963</td>       <td style="text-align:left;">0.992</td>       <td style="text-align:left;">0.977</td>      </tr>      <tr>       <td style="text-align:left;">InceptionV3 (1024)</td>       <td style="text-align:left;">0.965</td>       <td style="text-align:left;">0.962</td>       <td style="text-align:left;">0.945</td>       <td style="text-align:left;">0.985</td>       <td style="text-align:left;">0.965</td>      </tr>      <tr>       <td style="text-align:left;">Xception (2048)</td>       <td style="text-align:left;">0.970</td>       <td style="text-align:left;">0.977</td>       <td style="text-align:left;">0.975</td>       <td style="text-align:left;">0.980</td>       <td style="text-align:left;">0.978</td>      </tr>      <tr>       <td style="text-align:left;">        <strong>ResNet50 (128)</strong>       </td>       <td style="text-align:left;">        <strong>0.979</strong>       </td>       <td style="text-align:left;">        <strong>0.981</strong>       </td>       <td style="text-align:left;">        <strong>0.982</strong>       </td>       <td style="text-align:left;">        <strong>0.982</strong>       </td>       <td style="text-align:left;">        <strong>0.982</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">InceptionResNetV2 (512)</td>       <td style="text-align:left;">0.967</td>       <td style="text-align:left;">0.974</td>       <td style="text-align:left;">0.964</td>       <td style="text-align:left;">0.986</td>       <td style="text-align:left;">0.975</td>      </tr>     </tbody>     </table>    </div>    <p>Previous research showed that multimodal features can be useful for identifying posts containing dangerous selfies on Twitter&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. Authors showed that the image features (dense captions created by text from the images) gave the best accuracy among all the modes of features. It was also noted that combination of all the three features performed the best, and gave 73% accuracy. In this work, we propose a CNN-based architecture that works only on image-based features. In our work, we leverage the existing deep-learning models that have performed well in identifying images on large-scale benchmark datasets such as Imagenet&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>]. These state of the art models are pre-trained on Imagenet dataset used for ILSVRC (Imagenet Large Scale Visual Recognition Challenge), containing 1.2 million images labeled with 1,000 class labels. Applying the same architecture to our dataset, and re-training the network is a challenge, as for successful training of large architecture, a huge number of samples is required, which in our case isn&#x0027;t available. Therefore, we use pre-trained architectures and apply transfer learning for solving our task. We use the weights of models that do well on the Imagenet dataset to fine tune them for our problem statement. The intuition behind doing this is that the image features a model trained on Imagenet is using should be similar to the features we require. We model the problem as a two-class classification problem with the positive class consisting of dangerous selfies (623 samples) and negative class of non-dangerous selfies (5,837 samples).</p>    <p>     <strong>Handling Skewness</strong>: The number of positive samples (623 dangerous selfies) in our dataset is much less than the number of negative samples (5,837 non-dangerous selfies). This can be viewed as a rare class classification problem. To have more representative class balance in our dataset, we use data augmentation operations - shift (shifting the image pixels linearly in a range of 20% of width and height of image), flip (flipping the image pixels horizontally and vertically), rotate (rotating the image by a certain degree, randomly chosen in a range of 0-180), and shear (with a random zoom range and shear intensity of 0.2). Following which, we further downsample our dataset to give us a balanced dataset of 3,115 images in each class. Downsampling is a well-known method to handle class imbalance challenge in classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>].</p>    <p>     <strong>Data pre-processing</strong>: Once we obtained 3,115 images in both classes (total 6,230 images), we scaled each image to a size of 224 by 224 pixels and all these images were shuffled to ensure there&#x0027;s no bias in training data. A random 80:20 train test split was then done to get 4,984 and 1,246 images in the train and test respectively. Finally, all images were normalized using the mean and standard deviation of the dataset it was pre-trained on (Imagenet).</p>    <p>     <strong>Feature Extraction</strong>: Since our dataset is relatively different from the Imagenet dataset (which is a more generic dataset, and does not limit itself to just selfies), we also explore using the pre-trained models as feature extractors and then fitting a non-neural network based classifier (like SVM) on those features. For feature extraction, we took a pre-trained model and removed the softmax layer. The vector obtained on a forward propagation of an image (the layer just before softmax output) was treated as the feature vector for that image. So if, for example, we use VGG 16 or VGG 19 to extract image features, we get 4,096 features corresponding to each image. ReLu (Rectified Linear unit) activation was applied to these features which were then used to train SVMs with linear and RBF kernels.</p>    <p>     <strong>Training SVMs</strong>: We use the primal formulation of SVM and train both soft and hard margin classifiers by tuning the hyperparameter C where a larger value of C corresponds to more penalty for misclassification and a smaller C in lesser penalty thus leading to hard and soft margin classifiers respectively. We find the best value of C by doing grid search for C &#x2208; [0.01,1.28] where step size of the interval was increased exponentially with each iteration. Accuracy was used as the metric to evaluate the best value of C using 3 fold cross-validation on the training set. We also explore the application of Principal Component Analysis (PCA)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] to the extracted features using which we reduce the dimension of each feature vector to 100. Further both linear and Radial Basis Function(RBF) were used as kernel functions. For each of the architectures used for feature extraction and corresponding to each kernel function, we get a separate SVM model.</p>    <p>     <strong>Architecture Customization</strong>: The Imagenet dataset has 1,000 classes, and all architectures use the softmax layer as the final layer to make predictions. We modify the architecture by removing the softmax layer from each of the pre-trained models. Further, we apply the global average pooling operation on the output layer. We append this architecture by adding two dense layers - the first one is with ReLu activation function, followed by a layer consisting of 2 nodes with softmax activation, and this becomes our output layer. The number of nodes in the dense ReLu activation layer is treated as a hyperparameter for each model and the best number was decided by applying a grid search using 3-fold Cross-validation. For training, all the pre-trained layers were frozen, and weights of densely connected layers - which were initialized randomly - were trained for 300 epochs with a batch gradient descent, keeping batch size to be 50. After training the densely connected layers, last two layers of the pre-trained model were unfrozen and fine-tuned along with the densely connected layers for another 300 epochs. We experimented with the following architectures - VGG-16, VGG-19, InceptionV3, Xception, ResNet50, and InceptionResNetV2&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>]. The results for these models, along with the best hyperparameter - which in this case is the number of nodes in the densely connected layer - are presented in Table <a class="tbl" href="#tab1">1</a> and are discussed in Section <a class="sec" href="#sec-6">5</a>. The proposed architecture is shown in Figure &#x00A0;<a class="fig" href="#fig1">1</a>. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191575/images/www18companion-314-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Proposed Model. We used 3-fold Cross-validation and found that 128 nodes in the dense layer gives best Cross-validation accuracy.</span>     </div>     </figure>    </p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Results</h2>     </div>    </header>    <div class="table-responsive" id="tab2">     <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">Results for SVM with and without PCA. Features extracted using ResNet50 perform best for both linear and RBF kernels, both with and without PCA. Overall, PCA along with RBF kernel performs best.</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;"/>       <td colspan="5" style="text-align:left;">        <strong>Linear Kernel (with PCA)</strong>        <hr/>       </td>       <td colspan="5" style="text-align:left;">        <strong>RBF Kernel (with PCA)</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">        <strong>Feature Extractor</strong>       </td>       <td style="text-align:left;">        <strong>Train set acc</strong>       </td>       <td style="text-align:left;">        <strong>Test set acc</strong>       </td>       <td style="text-align:left;">        <strong>Precision</strong>       </td>       <td style="text-align:left;">        <strong>Recall</strong>       </td>       <td style="text-align:left;">        <strong>F1 score</strong>       </td>       <td style="text-align:left;">        <strong>Train set acc</strong>       </td>       <td style="text-align:left;">        <strong>Test set acc</strong>       </td>       <td style="text-align:left;">        <strong>Precision</strong>       </td>       <td style="text-align:left;">        <strong>Recall</strong>       </td>       <td style="text-align:left;">        <strong>F1 score</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">VGG 16</td>       <td style="text-align:left;">0.876</td>       <td style="text-align:left;">0.860</td>       <td style="text-align:left;">0.872</td>       <td style="text-align:left;">0.858</td>       <td style="text-align:left;">0.865</td>       <td style="text-align:left;">0.991</td>       <td style="text-align:left;">0.601</td>       <td style="text-align:left;">1.000</td>       <td style="text-align:left;">0.234</td>       <td style="text-align:left;">0.380</td>      </tr>      <tr>       <td style="text-align:left;">VGG 19</td>       <td style="text-align:left;">0.888</td>       <td style="text-align:left;">0.870</td>       <td style="text-align:left;">0.900</td>       <td style="text-align:left;">0.844</td>       <td style="text-align:left;">0.871</td>       <td style="text-align:left;">0.980</td>       <td style="text-align:left;">0.600</td>       <td style="text-align:left;">1.000</td>       <td style="text-align:left;">0.233</td>       <td style="text-align:left;">0.378</td>      </tr>      <tr>       <td style="text-align:left;">Inception V3</td>       <td style="text-align:left;">0.903</td>       <td style="text-align:left;">0.888</td>       <td style="text-align:left;">0.915</td>       <td style="text-align:left;">0.866</td>       <td style="text-align:left;">0.890</td>       <td style="text-align:left;">0.997</td>       <td style="text-align:left;">0.921</td>       <td style="text-align:left;">0.951</td>       <td style="text-align:left;">0.894</td>       <td style="text-align:left;">0.921</td>      </tr>      <tr>       <td style="text-align:left;">Xception</td>       <td style="text-align:left;">0.915</td>       <td style="text-align:left;">0.908</td>       <td style="text-align:left;">0.929</td>       <td style="text-align:left;">0.891</td>       <td style="text-align:left;">0.910</td>       <td style="text-align:left;">0.966</td>       <td style="text-align:left;">0.932</td>       <td style="text-align:left;">0.958</td>       <td style="text-align:left;">0.909</td>       <td style="text-align:left;">0.933</td>      </tr>      <tr>       <td style="text-align:left;">        <strong>ResNet50</strong>       </td>       <td style="text-align:left;">        <strong>0.938</strong>       </td>       <td style="text-align:left;">        <strong>0.937</strong>       </td>       <td style="text-align:left;">        <strong>0.961</strong>       </td>       <td style="text-align:left;">        <strong>0.917</strong>       </td>       <td style="text-align:left;">        <strong>0.938</strong>       </td>       <td style="text-align:left;">        <strong>0.999</strong>       </td>       <td style="text-align:left;">        <strong>0.952</strong>       </td>       <td style="text-align:left;">        <strong>0.990</strong>       </td>       <td style="text-align:left;">        <strong>0.917</strong>       </td>       <td style="text-align:left;">        <strong>0.952</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">InceptionResNetV2</td>       <td style="text-align:left;">0.916</td>       <td style="text-align:left;">0.903</td>       <td style="text-align:left;">0.918</td>       <td style="text-align:left;">0.894</td>       <td style="text-align:left;">0.906</td>       <td style="text-align:left;">0.994</td>       <td style="text-align:left;">0.941</td>       <td style="text-align:left;">0.944</td>       <td style="text-align:left;">0.941</td>       <td style="text-align:left;">0.943</td>      </tr>      <tr>       <td style="text-align:left;"/>       <td colspan="5" style="text-align:left;">        <strong>Linear Kernel (without PCA)</strong>        <hr/>       </td>       <td colspan="5" style="text-align:left;">        <strong>RBF Kernel (without PCA)</strong>        <hr/>       </td>      </tr>      <tr>       <td style="text-align:left;">        <strong>Feature Extractor</strong>       </td>       <td style="text-align:left;">        <strong>Train set acc</strong>       </td>       <td style="text-align:left;">        <strong>Test set acc</strong>       </td>       <td style="text-align:left;">        <strong>Precision</strong>       </td>       <td style="text-align:left;">        <strong>Recall</strong>       </td>       <td style="text-align:left;">        <strong>F1 score</strong>       </td>       <td style="text-align:left;">        <strong>Train set acc</strong>       </td>       <td style="text-align:left;">        <strong>Test set acc</strong>       </td>       <td style="text-align:left;">        <strong>Precision</strong>       </td>       <td style="text-align:left;">        <strong>Recall</strong>       </td>       <td style="text-align:left;">        <strong>F1 score</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">VGG 16</td>       <td style="text-align:left;">0.999</td>       <td style="text-align:left;">0.914</td>       <td style="text-align:left;">0.903</td>       <td style="text-align:left;">0.935</td>       <td style="text-align:left;">0.919</td>       <td style="text-align:left;">0.987</td>       <td style="text-align:left;">0.927</td>       <td style="text-align:left;">0.927</td>       <td style="text-align:left;">0.934</td>       <td style="text-align:left;">0.930</td>      </tr>      <tr>       <td style="text-align:left;">VGG 19</td>       <td style="text-align:left;">0.999</td>       <td style="text-align:left;">0.898</td>       <td style="text-align:left;">0.894</td>       <td style="text-align:left;">0.912</td>       <td style="text-align:left;">0.903</td>       <td style="text-align:left;">0.981</td>       <td style="text-align:left;">0.919</td>       <td style="text-align:left;">0.924</td>       <td style="text-align:left;">0.920</td>       <td style="text-align:left;">0.922</td>      </tr>      <tr>       <td style="text-align:left;">Inception V3</td>       <td style="text-align:left;">0.967</td>       <td style="text-align:left;">0.911</td>       <td style="text-align:left;">0.931</td>       <td style="text-align:left;">0.895</td>       <td style="text-align:left;">0.913</td>       <td style="text-align:left;">0.950</td>       <td style="text-align:left;">0.914</td>       <td style="text-align:left;">0.943</td>       <td style="text-align:left;">0.889</td>       <td style="text-align:left;">0.915</td>      </tr>      <tr>       <td style="text-align:left;">Xception</td>       <td style="text-align:left;">0.960</td>       <td style="text-align:left;">0.934</td>       <td style="text-align:left;">0.949</td>       <td style="text-align:left;">0.923</td>       <td style="text-align:left;">0.936</td>       <td style="text-align:left;">0.926</td>       <td style="text-align:left;">0.911</td>       <td style="text-align:left;">0.941</td>       <td style="text-align:left;">0.884</td>       <td style="text-align:left;">0.912</td>      </tr>      <tr>       <td style="text-align:left;">        <strong>ResNet50</strong>       </td>       <td style="text-align:left;">        <strong>0.982</strong>       </td>       <td style="text-align:left;">        <strong>0.941</strong>       </td>       <td style="text-align:left;">        <strong>0.960</strong>       </td>       <td style="text-align:left;">        <strong>0.924</strong>       </td>       <td style="text-align:left;">        <strong>0.942</strong>       </td>       <td style="text-align:left;">        <strong>0.961</strong>       </td>       <td style="text-align:left;">        <strong>0.941</strong>       </td>       <td style="text-align:left;">        <strong>0.974</strong>       </td>       <td style="text-align:left;">        <strong>0.911</strong>       </td>       <td style="text-align:left;">        <strong>0.941</strong>       </td>      </tr>      <tr>       <td style="text-align:left;">InceptionResNetV2</td>       <td style="text-align:left;">0.958</td>       <td style="text-align:left;">0.922</td>       <td style="text-align:left;">0.939</td>       <td style="text-align:left;">0.909</td>       <td style="text-align:left;">0.924</td>       <td style="text-align:left;">0.949</td>       <td style="text-align:left;">0.924</td>       <td style="text-align:left;">0.941</td>       <td style="text-align:left;">0.911</td>       <td style="text-align:left;">0.926</td>      </tr>     </tbody>     </table>    </div>    <p>To ensure that these results are not a false indication of the performance of the models, we make sure at the time of train-test split that data is properly shuffled. This results in a fairly balanced test set containing 649 and 597 samples in the positive and negative class respectively. Other than test set accuracy, we also make sure we look at other factors such as training accuracy, precision, recall and F1 score to make sure the model hasn&#x0027;t overfitted and the insights obtained are correct.</p>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Feature Transformation</h3>     </div>     </header>     <p>From Table &#x00A0;<a class="tbl" href="#tab2">2</a>, we see that features extracted from ResNet50 work better than other models giving a test set accuracy of 95% with PCA and RBF kernel. High precision, recall, and F1 scores further validate the model&#x0027;s performance. Another interesting thing to note is that ResNet50 works better regardless of the kernel or feature selection using PCA. This indicates that ResNet50 is a better feature extractor than other architectures when it comes to the task of detecting dangerous selfies. We posit that ResNet&#x0027;s ability to classify objects in the Imagenet dataset with higher accuracy (top-5 validation error of 6.71% compared to VGGnet&#x0027;s 8%) than other models is a contributing factor as Imagenet contains images having objects like vehicles, animals/insects, scenes of a cliff, water body etc. presence of which can possibly result in a dangerous selfie.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> CNN-based Results</h3>     </div>     </header>     <p>We see from Table &#x00A0;<a class="tbl" href="#tab1">1</a> that even for a fine-tuned models, ResNet50 gives the best performance. It achieves a test set accuracy of 98% getting high (0.98) precision and recall values. The training set accuracy (97.9%) is slightly lower than test set accuracy thus showing that the model hasn&#x0027;t overfitted and generalizes well. Overall fine-tuned models perform best in terms of all the evaluation metrics (precision, recall, accuracy). We further show that deep-learning approaches perform really well obtaining high accuracy, precision, and recall over the test set. It also performs better than the previous classifier proposed in the literature (Fig &#x00A0;<a class="fig" href="#fig2">2</a>). <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3191575/images/www18companion-314-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">ROC curve for fine-tuned models on the left compared to previous models. Fine-tuned models show high area under the curve thus showing robust classification capabilities.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusion</h2>     </div>    </header>    <p>In this work, we conduct a large scale analysis of machine learning models to classify an image as a dangerous or a non-dangerous selfie. Our work shows that using only image features can identify dangerous selfies more accurately than using the multimodal features, as proposed in existing literature. We achieve an accuracy of 98% with high precision and recall values, as compared to the 73% accuracy achieved previously. This is a major improvement not only in terms of accuracy but also in terms of usability. Using only image features does not require image captions, posts&#x2019; location or text to classify an image &#x2013; some of which might not be available in certain cases. It should also be noted that using only the image features for the classification task allows real-time detetection and can be used to build effective intervention technologies. Thus, we provide a more usable, robust and an accurate solution.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> Discussion</h2>     </div>    </header>    <p>While this work explores and proposes a classification framework for an important problem of identifying dangerous selfies, there is definitely an immediate need to apply this classifier in real world scenarios. Further tools need to be developed over the classifier, which can exactly identify individuals at risk and also build efficient intervention tools which can potentially prevent deaths and injuries. We have already built a crowdsourcing tool called &#x201C;Saftie&#x201C;, which is available on app store (<a class="link-inline force-break" href="http://goo.gl/2sIdYT">goo.gl/2sIdYT</a>) and as a Facebook chatbot (<a class="link-inline force-break" href="http://fb.me/saftiebot">fb.me/saftiebot</a>). This tool is currently live, and has collated about 1500+ dangerous locations, where a person should not click selfies. Another tool, currently in development, is to integrate the classifier into the camera app, which can nudge a user in an effective way before they try to click a dangerous selfie. We hope this work can result in more such technologies, that can prevent further harm.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Shivani Arora. 2016. Social Networking-A Double-Edged Sword. <em>      <em>International Conference On Recent Trends In Engineering Science And Management</em>     </em> (2016).</li>     <li id="BibPLXBIB0002" label="[2]">L. Blaine. 2013. How Selfies Are Ruining Your Relationships. (2013).</li>     <li id="BibPLXBIB0003" label="[3]">Pew&#x00A0;Research Center. 2014. More than half of Millennials have shared a &#x2019;Selfie&#x2019;. (2014).</li>     <li id="BibPLXBIB0004" label="[4]">Fran&#x00E7;ois Chollet. 2016. Xception: Deep Learning with Depthwise Separable Convolutions. <em>      <em>CoRR</em>     </em>abs/1610.02357(2016).</li>     <li id="BibPLXBIB0005" label="[5]">J. Donahue <em>et al.</em> 2014. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.. In <em>      <em>ICML</em>     </em>.</li>     <li id="BibPLXBIB0006" label="[6]">Kalev&#x00A0;Leetaru et al. 2013. Mapping the global Twitter heartbeat: The geography of Twitter. <em>      <em>First Monday</em>     </em>18, 5 (2013). <a class="link-inline force-break" href="https://doi.org/10.5210/fm.v18i5.4366"      target="_blank">https://doi.org/10.5210/fm.v18i5.4366</a></li>     <li id="BibPLXBIB0007" label="[7]">Joseph&#x00A0;L Fleiss. 1971. Measuring nominal scale agreement among many raters.<em>      <em>Psychological bulletin</em>     </em>(1971).</li>     <li id="BibPLXBIB0008" label="[8]">Jesse Fox and Margaret&#x00A0;C Rooney. 2015. The Dark Triad and trait self-objectification as predictors of men&#x0027;s use and self-presentation behaviors on social networking sites. <em>      <em>Personality and Individual Differences</em>     </em>(2015).</li>     <li id="BibPLXBIB0009" label="[9]">Jennifer Guay. 2014. Most dangerous selfies. (2014).</li>     <li id="BibPLXBIB0010" label="[10]">Haibo He and Edwardo&#x00A0;A. Garcia. 2009. Learning from Imbalanced Data. <em>      <em>IEEE Trans. on Knowl. and Data Eng.</em>     </em>21, 9 (Sept. 2009), 22.</li>     <li id="BibPLXBIB0011" label="[11]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. <em>      <em>CoRR</em>     </em>abs/1512.03385(2015).</li>     <li id="BibPLXBIB0012" label="[12]">David Houghton&#x00A0;et al. 2013. Tagger&#x0027;s delight? Disclosure and liking in Facebook: the effects of sharing photographs amongst multiple known social circles. (2013).</li>     <li id="BibPLXBIB0013" label="[13]">Peter&#x00A0;K. Jonason and Laura Krause. 2013. The emotional deficits associated with the Dark Triad traits: Cognitive empathy, affective empathy, and alexithymia. <em>      <em>Personality and Individual Differences</em>     </em>55, 5 (2013), 532 &#x2013; 537.</li>     <li id="BibPLXBIB0014" label="[14]">Alex Krizhevsky, Ilya Sutskever, and Geoffrey&#x00A0;E Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In <em>      <em>NIPS</em>     </em>. 1097&#x2013;1105.</li>     <li id="BibPLXBIB0015" label="[15]">AK Lakshmi. 2015. The Selfie Culture: Narcissism or Counter Hegemony?<em>      <em>Journal of Communication and media Studies</em>     </em>(2015). Issue 1.</li>     <li id="BibPLXBIB0016" label="[16]">Hemank Lamba <em>et al.</em> 2017. From Camera to Deathbed: Understanding Dangerous Selfies on Social Media. (2017).</li>     <li id="BibPLXBIB0017" label="[17]">J.&#x00A0;R. Landis and G.&#x00A0;G. Koch. 1977. The measurement of observer agreement for categorical data. <em>      <em>Biometrics</em>     </em> (1977).</li>     <li id="BibPLXBIB0018" label="[18]">Mark&#x00A0;R Leary&#x00A0;et al. 1994. Self-presentation can be hazardous to your health: impression management and health risk.<em>      <em>Health Psychology</em>     </em> (1994).</li>     <li id="BibPLXBIB0019" label="[19]">Olga Russakovsky&#x00A0;et al.2015. ImageNet Large Scale Visual Recognition Challenge. <em>      <em>Int. J. Comput. Vision</em>     </em>115, 3 (2015).</li>     <li id="BibPLXBIB0020" label="[20]">Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>      <em>CoRR</em>     </em>abs/1409.1556(2014).</li>     <li id="BibPLXBIB0021" label="[21]">BV Subrahmanyam and et al. 2016. Selfie Related Deaths Perils of Newer Technologies. <em>      <em>Narayana Medical Journal</em>     </em>(2016).</li>     <li id="BibPLXBIB0022" label="[22]">Christian Szegedy <em>et al.</em> 2014. Going Deeper with Convolutions. <em>      <em>CoRR</em>     </em>abs/1409.4842(2014).</li>     <li id="BibPLXBIB0023" label="[23]">Christian Szegedy <em>et al.</em> 2015. Rethinking the Inception Architecture for Computer Vision. <em>      <em>CoRR</em>     </em>abs/1512.00567(2015).</li>     <li id="BibPLXBIB0024" label="[24]">Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. 2016. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. <em>      <em>CoRR</em>     </em>abs/1602.07261(2016).</li>     <li id="BibPLXBIB0025" label="[25]">I Taslim and Md&#x00A0;Z Rezwan. 2013. Selfie Re-de-fined: Self-(more/less). <em>      <em>Wizcraft Journal of Language and Literature</em>     </em>(2013).</li>     <li id="BibPLXBIB0026" label="[26]">Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. <em>      <em>Chemometrics and Intelligent Laboratory Systems</em>     </em>2, 1 (1987), 37 &#x2013; 52. Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break"     href="https://googleblog.blogspot.in/2016/05/google-photos-one-year-200-million.html">https://googleblog.blogspot.in/2016/05/google-photos-one-year-200-million.html</a>   </p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class="link-inline force-break"     href="http://precog.iiitd.edu.in/requester.php?dataset=killfie2018">http://precog.iiitd.edu.in/requester.php?dataset=killfie2018</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3191575">https://doi.org/10.1145/3184558.3191575</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
