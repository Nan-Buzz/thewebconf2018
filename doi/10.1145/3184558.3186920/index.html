<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Retrieving Information from Multiple Sources</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3184558.3186920'>https://doi.org/10.1145/3184558.3186920</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186920'>https://w3id.org/oa/10.1145/3184558.3186920</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Retrieving Information from
          Multiple Sources</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Anurag</span> <span class=
          "surName">Roy</span> IIEST Shibpur, India
        </div>
        <div class="author">
          <span class="givenName">Kripabandhu</span> <span class=
          "surName">Ghosh</span> IIT Kanpur, India
        </div>
        <div class="author">
          <span class="givenName">Moumita</span> <span class=
          "surName">Basu</span> UEM Kolkata; IIEST Shibpur, India
        </div>
        <div class="author">
          <span class="givenName">Parth</span> <span class=
          "surName">Gupta</span> Amazon, India
        </div>
        <div class="author">
          <span class="givenName">Saptarshi</span> <span class=
          "surName">Ghosh</span> IIT Kharagpur; IIEST Shibpur,
          India
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186920"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186920</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The Web has several information sources on which
        an ongoing event is discussed. To get a complete picture of
        the event, it is important to retrieve information from
        multiple sources. We propose a novel neural network based
        model which integrates the embeddings from multiple
        sources, and thus retrieves information from them jointly,
        as opposed to combining multiple retrieval results. The
        importance of the proposed model is that no
        document-aligned comparable data is needed. Experiments on
        posts related to a particular event from three different
        sources - Facebook, Twitter and WhatsApp - exhibit the
        efficacy of the proposed model.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Information retrieval;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Multi-view retrieval; word
          embedding; deep learning</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Anurag Roy, Kripabandhu Ghosh, Moumita Basu, Parth Gupta,
          and Saptarshi Ghosh. 2018. Retrieving Information from
          Multiple Sources. In <em>WWW '18 Companion: The 2018 Web
          Conference Companion,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 2 Pages.
          <a href="https://doi.org/10.1145/3184558.3186920" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186920</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction
          and Motivation</h2>
        </div>
      </header>
      <p>The Web contains several information sources, including
      social media (e.g., Facebook, Twitter, WhatsApp), news sites,
      personal blogs, and so on. An ongoing event is discussed on
      all these channels; however, there are usually qualitative
      differences in the information obtained from different
      sources. As a result, to get a complete picture of an ongoing
      topic or event, it is necessary to retrieve information from
      multiple information sources. Further, given the real-time
      nature of online sources, the retrieval model for the
      multiple sources needs to be learned quickly.</p>
      <p>The existing methodologies for retrieval from multiple
      sources depend upon a central issue – <em>availability of
      document-aligned comparable data</em>. If such data is
      available, then common topic models or word embeddings can be
      learned; e.g., this approach was used by Vulic <em>et
      al.</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>] for bilingual retrieval. However,
      preparing document-aligned comparable data requires a lot of
      human involvement and time. Hence such approaches are not
      suitable when information about an ongoing event has to be
      retrieved quickly. If document-aligned comparable training
      data is <em>not</em> available, then the two most intuitive
      approaches are –</p>
      <p><strong>(Approach 1) Learning a single topic model or word
      embedding across all sources taken together:</strong>This
      approach ignores the fact that different information sources
      have their own inherent characteristics which vary from once
      source to another.</p>
      <p><strong>(Approach 2) Retrieving separately from different
      sources and then combining:</strong>Retrieval models are used
      to retrieve results separately for each source, and then the
      results are combined, e.g., using data fusion
      techniques&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>].</p>
      <p><strong>Proposed approach:</strong>In this work, we
      investigate the modeling of text across different information
      sources (or views) in a unified framework. We propose a novel
      deep learning-based multi-view retrieval model which attempts
      to learn document embeddings on a common space, where
      differences among the various data sources would not exist.
      Importantly, the proposed model does not require
      document-aligned training data.</p>
      <p>We analyse the performance of our proposed model over
      posts related to a common event, from three distinct online
      sources – Facebook, Twitter and WhatsApp. We demonstrate that
      the proposed model enables significantly better retrieval
      compared to the two approaches described above (in absence of
      document-aligned comparable data).</p>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Proposed
          Approach</h2>
        </div>
      </header>
      <p>The proposed multi-view model is structurally a
      feed-forward neural network that maps an input document
      vector to a multi-view space that would have the contextual
      information of multiple domains. We assume that for a small
      set of queries, the relevant documents from the different
      sources (views) are known. Note that this training data is
      much simpler to build, as compared to document-level
      comparable data. The proposed model, trained over this
      training data, allows efficient retrieval for many other
      queries.</p>
      <p>Let <span class="inline-equation"><span class=
      "tex">$e_i\in \mathbb {R}^n$</span></span> be the embedding
      of a document from source <em>i</em> relevant to a query
      <em>q</em>. Let <em>e<sub>j</sub></em> be the embedding of
      another document, which is also relevant to query <em>q</em>,
      from any other source <em>j</em>. We wish to learn a generic
      space <span class="inline-equation"><span class="tex">$e_o
      \in \mathbb {R}^n$</span></span> such that
      <em>e<sub>o</sub></em> normalises the differences between the
      information sources and helps better retrieval. The idea is
      to obtain <em>e<sub>o</sub></em> such that it exhibits the
      characteristics of both <em>e<sub>i</sub></em> and
      <em>e<sub>j</sub></em> . To this end, we use a feed-forward
      neural network with a single hidden layer that takes
      <em>e<sub>i</sub></em> and <em>e<sub>j</sub></em> as input
      and gives <em>e<sub>o</sub></em> as output. The
      transformation of <em>e<sub>i</sub></em> into
      <em>e<sub>o</sub></em> can be explained as: <em>h</em> =
      <em>f</em>(<em>W</em> <sub>1</sub>*<em>e<sub>i</sub></em> +
      <em>b</em> <sub>1</sub>)  and   <em>e<sub>o</sub></em> =
      <em>f</em>(<em>W</em> <sub>2</sub>*<em>h</em> + <em>b</em>
      <sub>2</sub>)</p>
      <p>where <em>W<sub>i</sub></em> and <em>b<sub>i</sub></em>
      represent the <em>i<sup>th</sup></em> layer weights and
      biases respectively; <em>h</em> represents the hidden layer
      and <em>f</em> is the non-linear activation function. The
      model is trained to minimize the objective function
      <span class="inline-equation"><span class="tex">$J(\theta) =
      \displaystyle \Vert e_o - e_i\circ e_j \Vert$</span></span>
      where ○ denotes the Hadamard Product, i.e., element-wise
      product between two vectors, and ‖<em>X</em>‖ denotes the
      <em>L</em>2 norm.</p>
      <p>Once the multi-view model is trained, we generate a
      generalized document embedding for each document. For
      retrieval, the query will also be passed through this
      multi-view network and matching will take place in the same
      space.</p>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Experiments and
          Results</h2>
        </div>
      </header>
      <p>This section shows the efficacy of our proposed multi-view
      model, by comparing its performance with those of baseline
      models.</p>
      <p><strong>Dataset:</strong>For the present work, we
      considered a specific event – the Nepal earthquake in April
      2015 – and the messages posted after the event on three
      social media: Twitter, Facebook and WhatsApp. The Twitter
      posts were collected using the Twitter Search API, and the
      Facebook posts were collected using the Radian6 tool
      (<a class="link-inline force-break" href=
      "https://www.marketingcloud.com/products/social-media-marketing/radian6">https://www.marketingcloud.com/products/social-media-marketing/radian6</a>),
      both using the search keywords ‘nepal’ and ‘quake’, and for
      the duration of two weeks following the earthquake. We also
      collected WhatsApp chat-logs of members of a medical NGO
      (Doctors For You) who were engaged in relief operations after
      the earthquake. After de-duplication, we obtained
      (i)&nbsp;50,018 tweets, (ii)&nbsp;85,483 Facebook posts, and
      (iii)&nbsp;3,438 WhatsApp messages. All experiments reported
      here were carried out on these three datasets.</p>
      <p><strong>Queries and gold standard relevance
      judgements:</strong>Based on the feedback of NGOs, we
      identified a set of <em>30 queries</em> or information needs
      specific to the event. Next, we employed human annotators to
      develop the gold standard relevance judgements for the
      queries. Table&nbsp;<a class="tbl" href="#tab1">1</a> shows
      some of the queries, and the number of relevant documents
      found for each query from the three datasets.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title"><strong>Examples of some queries and the
          number of relevant documents from each
          dataset.</strong></span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;">
              <strong>Query</strong></td>
              <td style="text-align:right;">
              <strong>#Twitter</strong></td>
              <td style="text-align:right;">
              <strong>#Facebook</strong></td>
              <td style="text-align:right;">
              <strong>#WhatsApp</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;">money donation</td>
              <td style="text-align:right;">1076</td>
              <td style="text-align:right;">39</td>
              <td style="text-align:right;">1072</td>
            </tr>
            <tr>
              <td style="text-align:center;">available
              hospitals</td>
              <td style="text-align:right;">50</td>
              <td style="text-align:right;">15</td>
              <td style="text-align:right;">280</td>
            </tr>
            <tr>
              <td style="text-align:center;">blood donation</td>
              <td style="text-align:right;">130</td>
              <td style="text-align:right;">0</td>
              <td style="text-align:right;">138</td>
            </tr>
            <tr>
              <td style="text-align:center;">medicine medical
              equipment need</td>
              <td style="text-align:right;">41</td>
              <td style="text-align:right;">49</td>
              <td style="text-align:right;">101</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p><strong>Train-test setup:</strong></p>
      <p>We performed a 3-fold cross-validation on the set of 30
      queries. Query numbers 1-10, 11-20 and 21-30 were used as
      training sets in turn for the three folds, while the rest of
      the queries served as the test sets.</p>
      <p><strong>Baseline retrieval models:</strong>We compare the
      retrieval of our proposed multi-view model with that of the
      following baselines:</p>
      <p><em>(1) BM25&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>]:</em>We combined all documents from
      all three sources into a single corpus, and applied
      Okapi-BM25 ranking (with <em>k</em> = 0.5).</p>
      <p><em>(2) Language model with Dirichlet smoothing
      (LM)&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>]:</em>For this baseline also, we rank
      the documents from all the three different sources taken
      together (parameter <em>μ</em> taken as 2000).</p>
      <p><em>(3) Single view embeddings:</em>For this baseline, a
      single word embedding is learned over all the documents (in
      the training sets) from all the sources. Subsequently,
      retrieval was done on the test set of queries using the
      learned embedding.</p>
      <p><em>(4) Data fusion:</em>Three different word embeddings
      were learned over the training sets for the three sources,
      and were used for retrieval on the test sets (from the same
      source). The results were fused using a standard data fusion
      algorithm <em>CombSUM</em>&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>].</p>
      <div class="table-responsive" id="tab2">
        <div class="table-caption">
          <span class="table-number">Table 2:</span> <span class=
          "table-title">Comparison of retrieval performance,
          averaged over 3-fold cross-validation. Bold font shows
          the best value, which the proposed method always
          achieves. Super-scripts S, D, B and L indicate that the
          proposed method is statistically significantly better at
          95% confidence interval (<em>p</em> &lt; 0.05) than
          Single view, Data fusion, BM25 and LM
          respectively.</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;">
              <strong>Algorithm</strong></td>
              <td style="text-align:center;">
              <strong>MAP</strong></td>
              <td style="text-align:center;">
              <strong>Precision@20</strong></td>
              <td style="text-align:center;">
              <strong>Recall@100</strong></td>
              <td style="text-align:center;">
              <strong>Bpref</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;"><strong>Single
              view</strong></td>
              <td style="text-align:center;">0.0087</td>
              <td style="text-align:center;">0.0450</td>
              <td style="text-align:center;">0.0044</td>
              <td style="text-align:center;">0.0469</td>
            </tr>
            <tr>
              <td style="text-align:center;"><strong>Data
              Fusion</strong></td>
              <td style="text-align:center;">0.0169</td>
              <td style="text-align:center;">0.0683</td>
              <td style="text-align:center;">0.0206</td>
              <td style="text-align:center;">0.0835</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>BM25</strong></td>
              <td style="text-align:center;">0.0026</td>
              <td style="text-align:center;">0.0367</td>
              <td style="text-align:center;">0.0059</td>
              <td style="text-align:center;">0.0128</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>LM</strong></td>
              <td style="text-align:center;">0.0051</td>
              <td style="text-align:center;">0.0350</td>
              <td style="text-align:center;">0.0193</td>
              <td style="text-align:center;">0.0189</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>Proposed</strong></td>
              <td style="text-align:center;">
              <strong>0.0280</strong> <sup><em>SDBL</em></sup></td>
              <td style="text-align:center;">
              <strong>0.1367</strong> <sup><em>SDBL</em></sup></td>
              <td style="text-align:center;">
              <strong>0.0287</strong> <sup><em>SDBL</em></sup></td>
              <td style="text-align:center;">
              <strong>0.0942</strong> <sup><em>SBL</em></sup></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p><strong>Embeddings and retrieval setup:</strong>All
      experiments utilize word embeddings learned using
      Word2vec&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>] with the following parameters –
      skip-gram model, vector size: 400, context size: 3, learning
      rate: 0.01, negative sample size for negative subsampling =
      5. For both a query and a document, we construct a vector by
      averaging the vectors of the constituent words generated by
      the underlying embedding model. The posts in the
      corresponding datasets are arranged in the decreasing order
      of the <em>cosine similarity</em> score of each
      document-vector with the associated query-vector.</p>
      <p><strong>Evaluation measures:</strong>We report the
      retrieval performance of all models in terms of Precision@20,
      Recall@100, Mean Average Precision (MAP) and Bpref. For all
      models, the retrieval is performed and evaluated once for
      each test query (via 3-fold cross validation), and the
      average across all queries is reported.</p>
      <p><strong>Retrieval results:</strong></p>
      <p>Table&nbsp;<a class="tbl" href="#tab2">2</a> reports the
      retrieval results of the proposed methodology and the
      baselines, averaged over all the queries in the test set. We
      see that retrieval using the proposed methodology numerically
      outperforms all the baseline methodologies. The proposed
      approach gives statistically significant performance
      improvements computed at 95% confidence interval (<em>p</em>
      &lt; 0.05) by Wilcoxon signed-rank test&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0004">4</a>] over Single view, BM25 and
      LM in all the measures and over Data Fusion in all the
      measures except Bpref. It should also be noted that Data
      Fusion performs better than single view approach, which
      highlights the importance of handling different sources
      separately.</p>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Conclusion and
          future directions</h2>
        </div>
      </header>
      <p>We proposed a novel neural network architecture for
      retrieval from multiple sources. The proposed architecture
      does not need expensive document-aligned training data, which
      makes the proposed model attractive for quick retrieval
      across multiple online sources.</p>
      <p>The low performance scores achieved by all the methods
      indicate that the problem is challenging and necessitates
      better methods. We also look to use the proposed architecture
      in retrieval across data sources varied in length, languages,
      scripts and modalities.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Edward&nbsp;A. Fox and
        Joseph&nbsp;A. Shaw. 1993. Combination of Multiple
        Searches. In <em><em>Proceedings of TREC 1993</em></em> .
        http://trec.nist.gov/pubs/trec2/papers/txt/23.txt.</li>
        <li id="BibPLXBIB0002" label="[2]">T. Mikolov, W.T. Yih,
        and G. Zweig. 2013. Linguistic Regularities in Continuous
        Space Word Representations. In <em><em>NAACL HLT
        2013</em></em> .</li>
        <li id="BibPLXBIB0003" label="[3]">Stephen&nbsp;E.
        Robertson and Hugo Zaragoza. 2009. The Probabilistic
        Relevance Framework: BM25 and Beyond. <em><em>Foundations
        and Trends in Information Retrieval</em></em> 3, 4(2009),
        333–389.</li>
        <li id="BibPLXBIB0004" label="[4]">S. Siegel. 1956.
        <em><em>Nonparametric Statistics for the Behavioral
        Sciences</em></em> . McGraw-Hill.</li>
        <li id="BibPLXBIB0005" label="[5]">Ivan Vulić and
        Marie-Francine Moens. 2015. Monolingual and Cross-Lingual
        Information Retrieval Models Based on (Bilingual) Word
        Embeddings. In <em><em>Proc. ACM SIGIR</em></em> .
        363–372.</li>
        <li id="BibPLXBIB0006" label="[6]">Chengxiang Zhai and John
        Lafferty. 2001. A Study of Smoothing Methods for Language
        Models Applied to Ad Hoc Information Retrieval. In
        <em><em>Proc. ACM SIGIR</em></em> .</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186920">https://doi.org/10.1145/3184558.3186920</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
