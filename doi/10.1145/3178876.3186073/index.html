<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Subgraph-augmented Path Embedding for Semantic User Search
  on Heterogeneous Social Network</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Subgraph-augmented Path Embedding
          for Semantic User Search on Heterogeneous Social
          Network</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Zemin</span> <span class=
          "surName">Liu</span>, Zhejiang University, China
        </div>
        <div class="author">
          <span class="givenName">Vincent W.</span> <span class=
          "surName">Zheng<a class="fn" href="#fn1" id=
          "foot-fn1"><sup>⁎</sup></a></span>, Advanced Digital
          Sciences Center, Singapore,
        </div>
        <div class="author">
          <span class="givenName">Zhou</span> <span class=
          "surName">Zhao</span>, Zhejiang University, China
        </div>
        <div class="author">
          <span class="givenName">Hongxia</span> <span class=
          "surName">Yang</span>, Alibaba Group, China
        </div>
        <div class="author">
          <span class="givenName">Kevin Chen-Chuan</span>
          <span class="surName">Chang</span>, University of
          Illinois at Urbana-Champaign, USA
        </div>
        <div class="author">
          <span class="givenName">Minghui</span> <span class=
          "surName">Wu</span>, Zhejiang University City College,
          China
        </div>
        <div class="author">
          <span class="givenName">Jing</span> <span class=
          "surName">Ying</span>, Zhejiang University, China
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186073"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186073</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Semantic user search is an important task on
        heterogeneous social networks. Its core problem is to
        measure the proximity between two user objects in the
        network w.r.t. certain semantic user relation.
        State-of-the-art solutions often take a path-based
        approach, which uses the sequences of objects connecting a
        query user and a target user to measure their proximity.
        Despite their success, we assert that path as a low-order
        structure is insufficient to capture the rich semantics
        between two users. Therefore, in this paper we introduce a
        new concept of <em>subgraph-augmented path</em> for
        semantic user search. Specifically, we consider sampling a
        set of object paths from a query user to a target user;
        then in each object path, we replace the linear object
        sequence between its every two neighboring users with their
        shared subgraph instances. Such subgraph-augmented paths
        are expected to leverage both path's distance awareness and
        subgraph's high-order structure. As it is non-trivial to
        model such subgraph-augmented paths, we develop a
        Subgraph-augmented Path Embedding (SPE) framework to
        accomplish the task. We evaluate our solution on six
        semantic user relations in three real-world public data
        sets, and show that it outperforms the
        baselines.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Computing methodologies</strong>
        → <strong>Statistical relational
        learning;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Heterogeneous Network;
          Subgraph-augmented Path Embedding</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Zemin Liu, Vincent W. Zheng, Zhou Zhao, Hongxia Yang,
          Kevin Chen-Chuan Chang, Minghui Wu, and Jing Ying. 2018.
          Subgraph-augmented Path Embedding for Semantic User
          Search on Heterogeneous Social Network. In <em>WWW 2018:
          The 2018 Web Conference,</em> <em>April 23–27, 2018 (WWW
          2018),</em> <em>Lyon, France. ACM, New York, NY, USA</em>
          10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186073" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186073</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Heterogeneous social networks are prevalent nowadays
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0037">37</a>]. Because
      social networks are human-centric, it is common to observe
      that users interact with many other types of objects. For
      example, as shown in Fig. <a class="fig" href="#fig1">1</a>,
      on a social network, user interact with not only other users,
      but also college, location and employer. These different
      types of interactions suggest different semantics of the
      user-user relationships. For example, Alice and Bob both
      attend UCLA, thus they are <em>schoolmates</em>; whereas
      Chris and Donna both work for Facebook, thus they are
      <em>colleagues</em>. Therefore, it gives us a unique
      opportunity to do <em>semantic user search</em>. In general,
      semantic user search is a task that given a query user
      (<em>e.g.</em>, Alice) on a heterogeneous social network and
      a semantic relation (<em>e.g.</em>, <em>schoolmates</em>), we
      want to find the other users (<em>e.g.</em>, Bob) that meet
      that relation with the query user. Such semantic user search
      is very useful [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>]. For example, we can use it to find
      colleagues, schoolmates and families on social networks such
      as Facebook and LinkedIn, or find advisors and advisees on
      academic networks such as DBLP.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186073/images/www2018-82-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Semantic user search on a heterogeneous
          social network with rich user interactions with different
          objects.</span>
        </div>
      </figure>
      <p></p>
      <p>Traditionally, <em>path-based</em> approach is used for
      solving semantic user search. This is because in semantic
      user search, the target user is often not immediately linked
      to the query user. A plausible choice is then to consider
      paths from the query user to the target user, and see whether
      they match the desired semantic relation. For example,
      Meta-Path Proximity (MPP) [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0030">30</a>] first relies on domain experts to
      specify a few path patterns (<em>i.e.</em>, metapaths) that
      indicate the desired semantic relation, and then enumerates
      the number of metapath instances between the query user and a
      target user. Path Ranking Algorithm (PRA) [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>] first enumerates
      bounded-length (relation) path patterns; then it recursively
      defines a score for each path pattern; finally, for a target
      node, its proximity to the query node is computed as a linear
      combination of its corresponding path instances. Recently
      deep learning starts to exploit learning representations for
      the paths between a query node and a target node, and then
      using them for proximity estimation. For example, ProxEmbed
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>] first
      samples a number of paths from the query node to the target
      node, and then uses a recurrent neural network to embed each
      path as a vector; finally it aggregates multiple path
      embedding vectors for proximity estimation.</p>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186073/images/www2018-82-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">Combining path's distance awareness and
          subgraph's higher-order structure for semantic user
          search.</span>
        </div>
      </figure>
      <p></p>
      <p>Despite the success of such a path-based approach, we
      assert that path as a low-order structure is insufficient to
      capture the rich semantics between two users. Consider an
      object path connecting a query user Alice and a target user
      Donna in Fig.&nbsp;<a class="fig" href="#fig2">2</a>(a) . In
      fact, Alice and Bob not only attended the same college UCLA,
      but also live in the same city L.A.. Such information is
      missing in the path, but it is possible to be captured by
      some higher-order subgraph structure. We are inspired by the
      state-of-the-art work on exploting subgraph patterns to
      organize complex networks [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>]. Suppose we already have some offline
      mined subgraph patterns in Fig.&nbsp;<a class="fig" href=
      "#fig2">2</a>(b), such as user-user (<em>m</em>
      <sub>1</sub>), user-college-user (<em>m</em> <sub>2</sub>),
      user-college &amp; location-user (<em>m</em> <sub>3</sub>)
      and so on. Then we can replace the linear object sequence
      between Alice and Bob with richer subgraph instances for
      <em>m</em> <sub>1</sub>, <em>m</em> <sub>2</sub> and
      <em>m</em> <sub>3</sub>. In this way, we have a more complete
      picture of the semantic relation between Alice and Bob. We
      envision that, once we better understand the semantic
      relation between every two neighboring users in a path, we
      can better estimate the proximity between the query user and
      the target user. Note that, we focus on augmenting the
      neighoring user objects only. There are two reasons of
      avoiding augmenting any two neighboring objects regardless of
      their types. Firstly, in semantic user search, we wish to
      directly model the semantic relation between users. Secondly,
      by constraining the subgraph patterns to involve two users,
      we can significantly reduce the number of subgraph patterns
      and thus greatly improve the efficiency in offline subgraph
      indexing, as suggested in [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>].</p>
      <p>In this paper, we introduce a new concept of
      <em>subgraph-augmented path</em> for semantic user search.
      Specifically, we consider sampling a set of object paths from
      a query user to a target user; then in each object path, we
      replace the linear object sequence between its every two
      neighboring users with their shared subgraph instances. Such
      subgraph-augmented paths are expected to leverage both path's
      <em>distance awareness</em> (<em>i.e.</em>, able to model
      multi-hop connections between a query user and a target user)
      and subgraph's <em>high-order structure</em> (<em>i.e.</em>,
      able to use more complex structures than linear sequences).
      Given these subgraph-augmented paths as new inputs, we aim to
      embed them into low-dimensional vectors and then aggregate
      them for proximity estimation. In this work, we assume the
      subgraph patterns and subgraph instances are given as inputs.
      Such an assumption is mild in practice, because frequent
      subgraphs are useful, and often offline mined as basic graph
      indexing to support many useful applications [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0010">10</a>]. For example, frequent
      subgraphs are used for fraud detection in Alibaba<a class=
      "fn" href="#fn2" id="foot-fn2"><sup>1</sup></a>, and
      user/content recommendation in Twitter [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>]. There also exist
      efficient algorithms to mine frequent subgraph patterns and
      match subgraph instances [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>].</p>
      <p>However, embedding subgraph-augmented path (or,
      <em>s-path</em> for abbreviation) is not trivial. A
      straightforward approach is to apply ProxEmbed [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>]. For each
      s-path, we represent each of its node as a key-value pair
      (Def. <a class="enc" href="#enc5">3.5</a>), where the key is
      the end user pair, and the value denotes the number of each
      subgraph instances shared by these two users. Then we apply a
      recurrent neural network to encode each node in the s-path,
      and finally we pool all the output vectors of the nodes as
      one. For multiple s-paths, we use distance discounted pooling
      to de-emphasize those long paths. Yet, such a straightforward
      approach overlooks two challenges. First of all, subgraphs
      are structural and noisy. To represent a node in an s-path,
      we have to take into account the structure of each subgraph,
      as well as the fact that not all the subgraphs are useful for
      a particular semantic user relation (<em>e.g.</em>,
      <em>m</em> <sub>5</sub> is less indicative than <em>m</em>
      <sub>2</sub> for <em>schoolmates</em>). Secondly, s-paths are
      noisy in and among themselves. In each s-path, its nodes are
      not equally useful for a semantic relation; <em>e.g.</em>, if
      Alice and Donna are truly <em>schoolmates</em>, then node
      (Alice, Bob) in the s-path, which implies a
      <em>schoolmates</em> relation, is more important than the
      other nodes in the same s-path. Similarly, not all the
      s-paths are equally useful either; <em>e.g.</em>, an s-path
      constructed from Alice–Emily–Frances–Donna in
      Fig.&nbsp;<a class="fig" href="#fig1">1</a> is less
      indicative than the one in Fig.&nbsp;<a class="fig" href=
      "#fig2">2</a>(c) for the <em>schoolmates</em> relation, since
      it has no clear signal for that relation.</p>
      <p>To model s-paths for semantic user search, we develop a
      novel <em>Subgraph-augmented Path Embedding</em> (SPE)
      framework. In SPE, we first represent an object in an s-path
      with an aggregation of its subgraphs’ embedding vectors.
      Specifically, we construct a structural similarity matrix
      among the subgraphs, and based on this matrix we learn an
      embedding vector for each subgraph to preserve the structural
      similarity. Then, we introduce the <em>attention</em>
      mechanism [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0040">40</a>]
      to automatically weigh the subgraphs in aggregation to
      represent each s-path's node. To deal with the noise in and
      among s-paths, we also introduce the attention mechanism to
      automatically weigh each node in an s-path and each s-path
      between two users. In all, we have a three-layer attention
      architecture on subgraphs, s-path's nodes and s-paths.
      Finally, we embed all the s-paths together into a vector,
      based on which we compute the proximity score and thus the
      ranking loss for model training.</p>
      <p>We summarize our contributions as follows.</p>
      <p>• We introduce a new concept of subgraph-augmented path,
      which for the first time systematically combine path's
      distance awareness and subgraph's high-order structure to
      solve semantic user search.</p>
      <p>• We develop a novel SPE framework to embed these
      subgraph-augmented paths for user proximity estimation.</p>
      <p>• We evaluate SPE on six semantic user relations in three
      public data sets, and show it outperforms the
      state-of-the-art baselines.</p>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <p>Earlier graph semantic search work such as Personalized
      PageRank [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0017">17</a>]
      and SimRank [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0016">16</a>]
      often consider homogeneous networks as input, and they do not
      differentiate semantic classes. Recent work starts to
      consider the rich network structure in heterogeneous
      networks. For example, Supervised Random Walk (SRW)
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0001">1</a>] tries to
      bias a random walk on the network, so as to ensure the
      resulting ranking result on the network to be consistent with
      the ground truth. MPP [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0030">30</a>] and PRA [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>] try to match the paths
      between a query node and a target node with some supervision
      (<em>i.e.</em>, either metapath patterns or ground truth
      labels) to see whether certain semantic relation holds.
      Meta-Graph Proximity (MGP) [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0011">11</a>] considers more general subgraph
      patterns than metapaths. It first identifies a few frequent
      subgraph patterns as metagraphs; then it leverages the
      supervision to automatically learn which metagraph is
      indicative for a desired semantic relation; finally it counts
      the number of indicative metagraphs between two users to
      measure their proximity. Thanks to the higher-order structure
      of subgraph, MGP improves the path-based methods such as SRW
      and MPP. But MGP lacks distance awareness; <em>i.e.</em>, if
      a query user and a target user are multi-hop away and no
      metagraph is shared by them, then their proximity becomes
      (close to) zero. Both MPP and MGP can be seen as exploiting
      explicit graph features for proximity estimation.</p>
      <p>With the development of neural networks, some recent
      studies start to consider learning “implicit” graph features
      for proximity estimation. For example, in graph embedding,
      DeepWalk [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0027">27</a>],
      LINE [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0032">32</a>],
      node2vec [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0012">12</a>],
      and many more [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>] all try to learn an embedding vector
      for each node in the graph, which can preserve the graph
      structure. In particular, metapath2vec [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0008">8</a>] extends DeepWalk by using
      metapath to guide the random walk for better node embedding.
      Struc2vec [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0028">28</a>]
      exploits additional structural equivalence of two nodes for
      node embedding. A comprehensive survey of graph embedding is
      recently available [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>]. One possible approach to make use of
      such a node-level embedding for semantic search is to
      aggregate two nodes’ embedding vectors (<em>e.g.</em>, first
      applying a Hadamard product and then multiplying it with a
      parameter vector) for estimating their proximity. However,
      such an approach is considered as “indirect”, as suggested by
      ProxEmbed [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0020">20</a>],
      since it does not directly encode the network structure
      between two possibly distant nodes. In contrast, ProxEmbed
      expresses the network structure between two objects by a set
      of paths connecting them, and directly encodes these paths
      into a proximity embedding vector. However, since ProxEmbed
      takes object paths as input, it is unable to leverage the
      readily available subgraphs’ high-order structure. Similarly,
      although D2AGE [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>] manages to model multiple object
      paths as one directed acyclic graph for proximity embedding,
      it is also unable to leverage the offline mined
      subgraphs.</p>
      <p>In the line of graph embedding, there exist several
      related, yet different concepts. First of all, some recent
      work exploits the concept of “high-order proximity” in graph
      embedding [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0006">6</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0038">38</a>]. They use
      higher-order reachability to construct adjacency matrix of a
      graph, and then run node embedding on this adjacency matrix.
      There are two major differences with our method: 1) they do
      not exploit the high-order structure of subgraph patterns; 2)
      they consider node embedding instead of path embedding.
      Secondly, some other work models high-order structure by
      graph convolution [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>]. These methods are powerful to
      capture local graph patterns, but it is not clear how to
      incorporate the path's distance awareness for the task of
      semantic user search. Besides, they cannot leverage the
      readily available subgraph patterns. Thirdly, graph kernel
      methods [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0007">7</a>],
      especially Weisfeiler-Lehman graph kernels [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0029">29</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0041">41</a>], try to measure the
      similarity between two (small) graphs w.r.t. their
      structures. They often exploit some predefined subgraph
      structures, such as edges, subtrees and shortest paths. But
      their goal of measuring similarity between graphs is very
      different from ours of measuring proximity between nodes.
      Besides, it is also not clear how to adapt their methods with
      path distance awareness for our task. Finally, in the field
      of knowledge base, recent work such as TransE [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0004">4</a>], TransH [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0039">39</a>] and TransNet [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0034">34</a>] has
      greatly advanced the study of knowledge embedding. However,
      these methods are not directly applicable to our task, due to
      the different problem settings. They often require the edges
      to have explicit descriptions, and aim to generate node/edge
      embedding instead of path embedding. Besides, it is also not
      clear how to extend these methods with the subgraph's
      high-order structure and the path's distance awareness for
      our task.</p>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Problem
          Formulation</h2>
        </div>
      </header>
      <p>We first introduce terminologies and notations (listed in
      Table <a class="tbl" href="#tab1">1</a>).</p>
      <p></p>
      <div class="definition" id="enc1">
        <label>Definition 3.1.</label>
        <p>A <strong>heterogeneous network</strong> is <em>G</em> =
        (<em>V</em>, <em>E</em>, <em>C</em>, <em>τ</em>), where
        <em>V</em> is a set of objects, <em>E</em> is a set of
        edges between the objects in <em>V</em>, <em>C</em> =
        {<em>c</em> <sub>1</sub>, ..., <em>c<sub>K</sub></em> } is
        a set of distinct object types, and <em>τ</em>: <em>V</em>
        → <em>C</em> is an object type mapping function.</p>
      </div>
      <p></p>
      <p>For example, in Fig.&nbsp;<a class="fig" href=
      "#fig1">1</a>, we have <span class=
      "inline-equation"><span class="tex">$C = \lbrace \textsf
      {user}, \textsf {college}, \textsf {location}, \textsf
      {employer}\rbrace$</span></span> . For an object of Alice,
      <em>τ</em>(Alice) = user.</p>
      <p></p>
      <div class="definition" id="enc2">
        <label>Definition 3.2.</label>
        <p>A <strong>subgraph pattern</strong> is <em>m</em> =
        (<em>C<sub>m</sub></em> , <em>E<sub>m</sub></em> ), where
        <em>C<sub>m</sub></em> is a set of object types,
        <em>E<sub>m</sub></em> is a set of edges between object
        types in <em>C<sub>m</sub></em> .</p>
      </div>
      <p></p>
      <p>For example, Fig.&nbsp;<a class="fig" href=
      "#fig2">2</a>(b) lists five subgraph patterns <em>m</em>
      <sub>1</sub>, ..., <em>m</em> <sub>5</sub>. We denote the set
      of possible subgraph patterns on <em>G</em> as <em>M</em>. As
      discussed in Sect.&nbsp;<a class="sec" href="#sec-8">1</a>,
      we consider <span class="inline-equation"><span class=
      "tex">$\mathcal {M}$</span></span> as the frequent subgraph
      patterns offline mined from <em>G</em>, and readily available
      as the input. Note that unlike <em>C</em> in <em>G</em>, the
      object types in <em>C<sub>m</sub></em> may be nondistinct.
      E.g., for subgraph pattern <em>m</em> <sub>1</sub> in
      Fig.&nbsp;<a class="fig" href="#fig2">2</a>(b), <span class=
      "inline-equation"><span class="tex">$C_{m_1} = \lbrace
      \textsf {user}, \textsf {user}\rbrace$</span></span> .</p>
      <p></p>
      <div class="definition" id="enc3">
        <label>Definition 3.3.</label>
        <p>An object subgraph <em>g</em> = (<em>V<sub>g</sub></em>
        , <em>E<sub>g</sub></em> ) is a <strong>subgraph
        instance</strong> of <em>m</em> = (<em>C<sub>m</sub></em> ,
        <em>E<sub>m</sub></em> ), if there exists a bijection
        between the node set of <em>g</em> and <em>m</em>,
        <em>ϕ</em>: <em>V<sub>g</sub></em> → <em>C<sub>m</sub></em>
        , such that</p>
        <p>• ∀<em>v</em> ∈ <em>V<sub>g</sub></em> , we have
        <em>τ</em>(<em>v</em>) = <em>ϕ</em>(<em>v</em>);</p>
        <p>• ∀<em>v</em>, <em>u</em> ∈ <em>V<sub>g</sub></em> , we
        have (<em>v</em>, <em>u</em>) ∈ <em>E<sub>g</sub></em> iff
        (<em>ϕ</em>(<em>v</em>), <em>ϕ</em>(<em>u</em>)) ∈
        <em>E<sub>m</sub></em> .</p>
      </div>
      <p></p>
      <p>For example, Alice-UCLA-Bob is an instance of <em>m</em>
      <sub>2</sub> in Fig.&nbsp;<a class="fig" href=
      "#fig2">2</a>(b) . We denote the set of possible subgraph
      instances for <span class="inline-equation"><span class=
      "tex">$\mathcal {M}$</span></span> on <em>G</em> as
      <span class="inline-equation"><span class="tex">$\mathcal
      {I}$</span></span> . For each <span class=
      "inline-equation"><span class="tex">$m \in \mathcal
      {M}$</span></span> , there may be multiple subgraph instances
      on <em>G</em>.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Notations used in this paper.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">
              <strong>Notation</strong></th>
              <th><strong>Description</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;"><em>G</em>,
              <em>V</em>, <em>E</em>, <em>C</em></td>
              <td>Network <em>G</em>, objects <em>V</em>, edges
              <em>E</em>, object types <em>C</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {M}$</span></span></td>
              <td>Set of frequent subgraph patterns from
              <em>G</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {I}$</span></span></td>
              <td>Set of subgraph instances for <span class=
              "inline-equation"><span class="tex">$\mathcal
              {M}$</span></span> on <em>G</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {D}$</span></span></td>
              <td>Set of training tuples</td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathcal
              {P}$</span></span></td>
              <td>Set of sampled object paths on <em>G</em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\hat{\mathcal
              {P}}$</span></span></td>
              <td>Set of subgraph-augmented paths constructed from
              <span class="inline-equation"><span class=
              "tex">$\mathcal {P}$</span></span></td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>r</em></td>
              <td>A subgraph-augmented node</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>x</strong></td>
              <td>Embedding vector for a subgraph</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>z</strong></td>
              <td>Embedding vector for a subgraph-augmented
              path</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>f</strong></td>
              <td>Proximity embedding vector between two users</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>π</em></td>
              <td>Proximity score</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>S</em></td>
              <td>Structural similarity matrix of subgraph
              patterns</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>γ</em>, ℓ</td>
              <td>Number of paths per object <em>γ</em>, walk
              length ℓ</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>d</em>,
              <em>d</em>′</td>
              <td>Embedding dimensions</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>ζ</em></td>
              <td>Average number of subgraph instances for a
              user</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p></p>
      <div class="definition" id="enc4">
        <label>Definition 3.4.</label>
        <p>An <strong>object path</strong> on <em>G</em> is a
        sequence of objects <em>v</em> <sub>1</sub> → <em>v</em>
        <sub>2</sub> → ... → <em>v<sub>t</sub></em> , where each
        <em>v<sub>i</sub></em> ∈ <em>V</em>, and <em>t</em> is the
        path length.</p>
      </div>
      <p></p>
      <p>For example, the sequence in Fig.&nbsp;<a class="fig"
      href="#fig2">2</a>(a) is an object path.</p>
      <p></p>
      <div class="definition" id="enc5">
        <label>Definition 3.5.</label>
        <p>A <strong>subgraph-augmented node</strong> (or “s-node”
        for abbreviation) <em>r</em> for two user objects
        <em>u</em>, <em>v</em> ∈ <em>V</em> is a key-value pair,
        whose key is (<em>u</em>, <em>v</em>) and value is a set of
        tuples {<em>m</em> <sub>1</sub>: <em>e</em> <sub>1</sub>,
        ..., <em>m<sub>l</sub></em> : <em>e<sub>l</sub></em> }.
        ∀<em>m<sub>i</sub></em> ∈ <em>M</em>,
        <em>e<sub>i</sub></em> is the number of
        <em>m<sub>i</sub></em> ’s instances between <em>u</em> and
        <em>v</em>.</p>
      </div>
      <p></p>
      <p>For example, in Fig.&nbsp;<a class="fig" href=
      "#fig2">2</a>(b), the s-node for Alice and Bob is defined as
      <em>r</em>.<em>key</em> = (Alice, Bob),
      <em>r</em>.<em>value</em> = {<em>m</em> <sub>1</sub>: 1,
      <em>m</em> <sub>2</sub>: 1, <em>m</em> <sub>3</sub>: 1}. We
      skip the subgraphs with zero instance in
      <em>r</em>.<em>value</em>. As discussed in
      Sect.&nbsp;<a class="sec" href="#sec-8">1</a>, we choose only
      augmenting two user objects with their shared subgraphs to
      both focus on user-user semantic relation and improve
      subgraph indexing efficiency. We leave as future work
      augmenting any two objects regardless of their types for
      semantic search.</p>
      <p></p>
      <div class="definition" id="enc6">
        <label>Definition 3.6.</label>
        <p>An <strong>subgraph-augmented path</strong> (or “s-path”
        for abbreviation) is a sequence of s-nodes <em>r</em>
        <sub>1</sub> → <em>r</em> <sub>2</sub> → ... →
        <em>r<sub>t</sub></em> .</p>
      </div>
      <p></p>
      <p>For example, the sequence in Fig.&nbsp;<a class="fig"
      href="#fig2">2</a>(c) is an s-path.</p>
      <p><strong>Problem inputs and outputs</strong>. For
      <em>inputs</em> of our model, we have a heterogeneous network
      <em>G</em>, a set of readily available frequent subgraph
      patterns <em>M</em> and their subgraph instances <span class=
      "inline-equation"><span class="tex">$\mathcal
      {I}$</span></span> on <em>G</em>, and finally a set of
      training tuples <span class="inline-equation"><span class=
      "tex">$\mathcal {D} = \lbrace (q_i,v_i,u_i): i = 1, ...,
      n\rbrace$</span></span> , where for each query user object
      <em>q<sub>i</sub></em> , user <em>v<sub>i</sub></em> is
      closer to <em>q<sub>i</sub></em> than user
      <em>u<sub>i</sub></em> . Besides, we also offline sample some
      object paths from <em>G</em> as inputs. We take a similar
      approach as DeepWalk [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>] for path sampling. Specifically,
      starting from each object in <em>G</em>, we randomly sample
      <em>γ</em> object paths, each of length ℓ. As a result, we
      obtain a set of object paths, denoted as <span class=
      "inline-equation"><span class="tex">$\mathcal
      {P}$</span></span> . These object paths are indexed to
      support efficient training and testing. For each query object
      <em>q</em> ∈ {<em>q</em> <sub>1</sub>, ...,
      <em>q<sub>n</sub></em> } and a corresponding target object
      <em>v</em> ∈ {<em>v</em> <sub>1</sub>, ...,
      <em>v<sub>n</sub></em> , <em>u</em> <sub>1</sub>, ...,
      <em>u<sub>n</sub></em> }, we extract multiple subpaths from
      <span class="inline-equation"><span class="tex">$\mathcal
      {P}$</span></span> . We denote all the subpaths starting from
      <em>q</em> and ending at <em>v</em> in <span class=
      "inline-equation"><span class="tex">$\mathcal
      {P}$</span></span> as <span class=
      "inline-equation"><span class="tex">$\mathcal
      {P}(q,v)$</span></span> , and those from <em>v</em> to
      <em>q</em> as <span class="inline-equation"><span class=
      "tex">$\mathcal {P}(v,q)$</span></span> . For each object
      path from <em>q</em> to <em>v</em>, we use the subgraph
      patterns <span class="inline-equation"><span class=
      "tex">$\mathcal {M}$</span></span> and their subgraph
      instances <span class="inline-equation"><span class=
      "tex">$\mathcal {I}$</span></span> to construct a
      subgraph-augmented path. We will introduce the details of
      s-path construction in Sect.&nbsp;<a class="sec" href=
      "#sec-11">4</a>.</p>
      <p>For <em>outputs</em> of our model, we generate a
      subgraph-augmented path embedding vector <span class=
      "inline-equation"><span class="tex">$\mathbf {z}(q,v) \in
      \mathbb {R}^d$</span></span> for each s-path between
      <em>q</em> and <em>v</em>, where <em>d</em> &gt; 0 is the
      embedding dimension. Since there are multiple s-paths between
      <em>q</em> and <em>v</em>, we will reasonably aggregate
      multiple <strong>z</strong>(<em>q</em>, <em>v</em>)’s into a
      proximity embedding vector <span class=
      "inline-equation"><span class="tex">$\mathbf {f}(q,v) \in
      \mathbb {R}^d$</span></span> . In this work, we consider both
      symmetric and asymmetric relations, where for symmetric
      relations <strong>f</strong>(<em>q</em>, <em>v</em>) =
      <strong>f</strong>(<em>v</em>, <em>q</em>) and for asymmetric
      ones <strong>f</strong>(<em>q</em>, <em>v</em>) ≠
      <strong>f</strong>(<em>v</em>, <em>q</em>). We will discuss
      how to compute <strong>z</strong>(<em>q</em>, <em>v</em>) and
      <strong>f</strong>(<em>q</em>, <em>v</em>) by some
      hierarchical neural network model in Sect.&nbsp;<a class=
      "sec" href="#sec-12">5</a>. Finally, we use
      <strong>f</strong>(<em>q</em>, <em>v</em>) to estimate a
      proximity score between <em>q</em> and <em>v</em> as</p>
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \pi (q, v) =
          \textstyle \theta ^T \mathbf {f}(q, v),
          \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\theta \in \mathbb {R}^d$</span></span> is a parameter
      vector.
      <p></p>
      <p>Our model has two types of parameters: 1) the hierarchical
      neural network parameters for getting
      <strong>z</strong>(<em>q</em>, <em>v</em>) and
      <strong>f</strong>(<em>q</em>, <em>v</em>); 2) the proximity
      estimation parameter <em>θ</em>. In training, we aim to learn
      these model parameters, such that
      <em>π</em>(<em>q<sub>i</sub></em> , <em>v<sub>i</sub></em> )
      ≥ <em>π</em>(<em>q<sub>i</sub></em> , <em>u<sub>i</sub></em>
      ) for each <span class="inline-equation"><span class=
      "tex">$(q_i,v_i,u_i) \in \mathcal {D}$</span></span> . We
      will introduce the details of training algorithm in
      Sect.&nbsp;<a class="sec" href="#sec-13">6</a>. Note that in
      offline training, we only need to compute subgraph-augmented
      path embedding for those (<em>q<sub>i</sub></em> ,
      <em>v<sub>i</sub></em> ) and (<em>q<sub>i</sub></em> ,
      <em>u<sub>i</sub></em> ) for <em>i</em> = 1, ..., <em>n</em>,
      instead of all the possible object pairs in <em>G</em>. In
      online testing, given a random query user <em>q</em> in
      <em>G</em>, we will quickly extract from <span class=
      "inline-equation"><span class="tex">$\mathcal
      {P}$</span></span> a set of sample object paths from
      <em>q</em> to each possible target user <em>v</em> in
      <em>G</em>. Then we construct the s-paths with <em>M</em>,
      and apply our model to compute the <em>π</em>(<em>q</em>,
      <em>v</em>) for each target <em>v</em> for ranking.</p>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> S-Path
          Construction</h2>
        </div>
      </header>
      <p>We introduce how to construct subgraph-augmented paths for
      a query user <em>q</em> and a target user <em>v</em>, based
      on: 1) a set of already sampled object paths from <em>q</em>
      to <em>v</em> on <em>G</em>; 2) a set of readily available
      frequent subgraph patterns <span class=
      "inline-equation"><span class="tex">$\mathcal
      {M}$</span></span> and their subgraph instances <span class=
      "inline-equation"><span class="tex">$\mathcal
      {I}$</span></span> on <em>G</em>.</p>
      <p><strong>Running example</strong>: Take Fig.&nbsp;<a class=
      "fig" href="#fig2">2</a>(b) as an example. For an object path
      of Alice–UCLA–Bob–Chris–Facebook–Donna, we first extract
      every pair of neighboring users by collapsing the non-user
      objects in the path. As a result, we get (Alice, Bob), (Bob,
      Chris) and (Chris, Donna). For each pair of neighboring
      users, we try to get each user's involved subgraph instances.
      In Fig.&nbsp;<a class="fig" href="#fig2">2</a>(b), we have
      listed five possible subgraph patterns <em>m</em>
      <sub>1</sub>, ..., <em>m</em> <sub>5</sub>; due to space
      limit, we skip listing their subgraph instances on the
      heterogeneous network in Fig.&nbsp;<a class="fig" href=
      "#fig1">1</a>. Take (Alice, Bob) as an example. For Alice,
      she has involved four subgraph instances w.r.t. <em>m</em>
      <sub>1</sub>, <em>m</em> <sub>2</sub> and <em>m</em>
      <sub>3</sub>. Specificaly, for <em>m</em> <sub>1</sub>, Alice
      has two subgraph instances in Fig.&nbsp;<a class="fig" href=
      "#fig1">1</a>: Alice–Bob and Alice–Emily. For <em>m</em>
      <sub>2</sub>, Alice has one subgraph instances in
      Fig.&nbsp;<a class="fig" href="#fig1">1</a>: Alice–UCLA–Bob.
      For <em>m</em> <sub>3</sub>, Alice has one subgraph instance
      in Fig.&nbsp;<a class="fig" href="#fig1">1</a>: Alice–UCLA
      &amp; L.A.–Bob. To replace the linear object path
      Alice–UCLA–Bob with subgraphs, we want to find all the
      subgraph instances shared by Alice and Bob. An easy way to
      find such shared subgraph instances is to scan all the
      subgraph instances of Alice and see whether they contain Bob.
      As we can see, Alice and Bob share one instance of <em>m</em>
      <sub>1</sub>, one instance of <em>m</em> <sub>2</sub> and one
      instance of <em>m</em> <sub>3</sub>. Then we construct a
      subgraph-augmented node (s-node) <em>r</em> <sub>1</sub>,
      with <em>r</em> <sub>1</sub>.<em>key</em> ← (Alice, Bob) and
      <em>r</em> <sub>1</sub>.<em>value</em> ← [<em>m</em>
      <sub>1</sub>: 1, <em>m</em> <sub>2</sub>: 1, <em>m</em>
      <sub>3</sub>: 1]. Similarly, we can construct an s-node
      <em>r</em> <sub>2</sub> for (Bob, Chris) and an s-node
      <em>r</em> <sub>3</sub> for (Chris, Donna). In the end, we
      obtain an subgraph-augmented path (s-path) of <em>p</em>:
      <em>r</em> <sub>1</sub> → <em>r</em> <sub>2</sub> →
      <em>r</em> <sub>3</sub>.</p>
      <p><img src=
      "../../../data/deliveryimages.acm.org/10.1145/3190000/3186073/images/www2018-82-img1.svg"
      class="img-responsive" alt="" longdesc="" /></p>
      <p>We abstract the above running example, and summarize the
      s-path construction algorithm in Alg.&nbsp;1 . In line 1, we
      first extract all the object subpaths from <em>q</em> to
      <em>v</em> from <span class="inline-equation"><span class=
      "tex">$\mathcal {P}$</span></span> . Here we overload the
      function “GetSubpaths” to get different subpaths for
      symmetric and asymmetric semantic relations. In line 4, for
      each resulting object path, we get all the neighboring user
      pairs. In line 6, for each pair of neighboring users, we
      identify their shared subgraph instances from the pre-indexed
      subgraph instance set <span class=
      "inline-equation"><span class="tex">$\mathcal
      {I}$</span></span> . After that, we construct an s-node in
      lines 7 and 8, and further append it to the previous s-node
      sequence to form an s-path in line 9.</p>
      <p><strong>Complexity analysis</strong>: as each object
      path's length is bounded by ℓ, getting the neighboring user
      pairs in line 4 takes <em>O</em>(ℓ). The straightforward
      approach to get shared subgraph instances between two users
      has to scan through all the subgraph instances of one user.
      Denote the average number of subgraph instance for a user on
      <em>G</em> as <em>ζ</em>. Because in practice the subgraph
      patterns have limited sizes (<em>e.g.</em>, less than six in
      our experiments), the above straightforward approach to run
      line 6 takes <em>O</em>(<em>ζ</em>). This complexity can be
      further reduced; as suggested by [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>], if in the subgraph
      indexing stage only those subgraph patterns that involved at
      least two users were considered, then both the number of
      subgraph patterns and the number of subgraph instances can be
      significantly reduced. By a sophisticated indexing of which
      subgraph instance matching which two users, we may reduce
      line 6’s complexity to a constant. In all, constructing an
      s-path from an object path takes <em>O</em>(ℓ +
      <em>ζ</em>).</p>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> S-Path
          Embedding</h2>
        </div>
      </header>
      <p>We first introduce how to embed each subgraph-augmented
      path to a vector <strong>z</strong>(<em>q</em>, <em>v</em>),
      and later aggregate multiple such vectors into a single one
      <strong>f</strong>(<em>q</em>, <em>v</em>). We design a
      hierarchical neural network for subgraph-augmented path
      embedding (SPE) as shown in Fig.&nbsp;<a class="fig" href=
      "#fig3">3</a>. As motivated in Sect.&nbsp;<a class="sec"
      href="#sec-8">1</a>, we will take multiple factors into the
      design. First of all, we embed each subgraph with structural
      information. Then, we aggregate the subgraph embedding in
      each subgraph-augmented node (s-node) with attention to
      obtain an s-node embedding. To model the sequential
      information of each s-path, we also employ a recurrent neural
      network architecture to learn an s-path's embedding. Finally,
      we aggregate all the s-paths’ embedding with attention to
      obtain an overall proximity embedding vector for (<em>q</em>,
      <em>v</em>). Next we introduce each step of embedding in
      Fig.&nbsp;<a class="fig" href="#fig3">3</a>.</p>
      <figure id="fig3">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186073/images/www2018-82-fig3.jpg"
        class="img-responsive" alt="Figure 3" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 3:</span> <span class=
          "figure-title">Subgraph-augmented path embedding.</span>
        </div>
      </figure><strong>Subgraph Embedding.</strong>To take the
      subgraph structure into account, we are inspired by the
      structural deep network embedding [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0038">38</a>] to consider embedding the
      subgraphs from a <em>subgraph structural similarity
      matrix</em>. In general, two subgraphs are similar if they
      share some common structures. Therefore, we adopt the widely
      used Maximum Common Subgraph (MCS) approach [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0035">35</a>] to measure the similarity
      between two subgraphs. Given two subgraphs
      <em>m<sub>i</sub></em> and <em>m<sub>j</sub></em> , we denote
      <em>m</em> <sup>*</sup> as their MCS. Then the structural
      similarity between two subgraphs is defined as
      <div class="table-responsive" id="Xeq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} S(m_i, m_j) =
          \frac{(|C_{m^*}| + |E_{m^*}|)^2}{(|C_{m_i}| + |E_{m_i}|)
          \times (|C_{m_j}| + |E_{m_j}|)}.
          \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>If <em>m</em> <sup>*</sup> is bigger,
      <em>S</em>(<em>m<sub>i</sub></em> , <em>m<sub>j</sub></em> )
      is bigger. We use stacked AutoEncoder [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>] to learn an embedding
      <span class="inline-equation"><span class="tex">$\mathbf
      {x}_i \in \mathbb {R}^d$</span></span> for each subgraph
      <em>m<sub>i</sub></em> . Denote <strong>s</strong>
      <sub><em>i</em></sub> = [<em>S</em>(<em>m<sub>i</sub></em> ,
      <em>m</em> <sub>1</sub>), ...,
      <em>S</em>(<em>m<sub>i</sub></em> , <em>m</em>
      <sub>|<em>M</em>|</sub>)] <sup><em>T</em></sup> . For
      simplicity, we use a three-layer AutoEncoder to illustrate
      how we construct <strong>x</strong> <sub><em>i</em></sub>
      from its <strong>s</strong> <sub><em>i</em></sub> . In
      particular, we define the subgraph embedding vector
      <strong>x</strong> <sub><em>i</em></sub> for
      <em>m<sub>i</sub></em> as
      <div class="table-responsive" id="eq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mathbf {x}_i =
          \sigma (W^{(a)}\mathbf {s}_i + \mathbf {b}^{(a)}),
          \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$W^{(a)}\in \mathbb {R}^{d \times |\mathcal
      {M}|}$</span></span> and <span class=
      "inline-equation"><span class="tex">$\mathbf {b}^{(a)}\in
      \mathbb {R}^{d}$</span></span> are parameters; <em>σ</em>(·)
      is a sigmoid function. We reconstruct <strong>s</strong>
      <sub><em>i</em></sub> from <strong>x</strong>
      <sub><em>i</em></sub> by
      <div class="table-responsive" id="eq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \hat{\mathbf
          {s}}_i = \sigma (W^{(b)}\mathbf {x}_i + \mathbf
          {b}^{(b)}), \end{equation}</span><br />
          <span class="equation-number">(4)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$W^{(b)}\in \mathbb {R}^{|\mathcal {M}| \times
      d}$</span></span> and <span class=
      "inline-equation"><span class="tex">$\mathbf {b}^{(b)}\in
      \mathbb {R}^{|\mathcal {M}|}$</span></span> are also
      parameters. Finally, we minimize the reconstruction error:
      <div class="table-responsive" id="eq4">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \textstyle \sum
          _{i=1}^{|M|} \Vert \hat{\mathbf {s}}_i - \mathbf {s}_i
          \Vert _2. \end{equation}</span><br />
          <span class="equation-number">(5)</span>
        </div>
      </div>Although it is possible to optimize <strong>x</strong>
      <sub><em>i</em></sub> ’s together with the proximity
      embedding later, in this paper we choose to optimize
      <strong>x</strong> <sub><em>i</em></sub> ’s from <em>S</em>
      separately, so as to keep the model simple.
      <p></p>
      <p><strong>S-Node Embedding.</strong>In general, there are
      multiple subgraphs involved in a subgraph-augmented node
      (s-node). To differentiate their contributions, we introduce
      an attention mechanism to automatically learn the weight for
      each subgraph in s-node embedding. For an s-node
      <em>r<sub>j</sub></em> .<em>value</em> = {<em>m</em>
      <sub>1</sub>: <em>e</em> <sub>1</sub>, ...,
      <em>m<sub>l</sub></em> : <em>e<sub>l</sub></em> }, we compute
      the attention score <em>α<sub>i</sub></em> for each subgraph
      <em>m<sub>i</sub></em> in <em>r<sub>j</sub></em> as</p>
      <div class="table-responsive" id="eq5">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \alpha _i &amp; =
          \frac{\exp (\beta _i)}{\sum _{j=1}^{l} \exp (\beta _j)},
          \end{align}</span><br />
          <span class="equation-number">(6)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq6">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \beta _i &amp; =
          e_i \left[ \eta \cdot \sigma (Q \mathbf {x}_i + \mathbf
          {b}) \right], \end{align}</span><br />
          <span class="equation-number">(7)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\eta \in \mathbb {R}^{d^{\prime }}$</span></span> ,
      <span class="inline-equation"><span class="tex">$Q \in
      \mathbb {R}^{d^{\prime } \times d}$</span></span> and
      <span class="inline-equation"><span class="tex">$\mathbf
      {b}\in \mathbb {R}^{d^{\prime }}$</span></span> are
      parameters. As a result, we compute the s-node embedding for
      <em>r<sub>j</sub></em> as
      <div class="table-responsive" id="Xeq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mathbf {y}_j
          \textstyle = \sum _{i=1}^l \alpha _i \mathbf {x}_i.
          \end{equation}</span><br />
          <span class="equation-number">(8)</span>
        </div>
      </div><strong>S-Path Embedding.</strong>Once having the
      s-node embeddings in an s-path <em>p<sub>k</sub></em> :
      <em>r</em> <sub>1</sub> → ... → <em>r<sub>t</sub></em> , we
      learn the s-path embedding by LSTM (Long Short Term Memory)
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0015">15</a>]. Formally,
      for each input s-node embedding <strong>y</strong>
      <sub><em>j</em></sub> , we output a vector <span class=
      "inline-equation"><span class="tex">$\mathbf {h}_j \in
      \mathbb {R}^{d^{\prime }}$</span></span> , by computing a
      series of neuron activations for an input gate <span class=
      "inline-equation"><span class="tex">$\mathbf
      {g}^{(i)}_j$</span></span> , an forget gate <span class=
      "inline-equation"><span class="tex">$\mathbf
      {g}^{(f)}_j$</span></span> , a memory cell state <span class=
      "inline-equation"><span class="tex">$\mathbf
      {g}^{(c)}_j$</span></span> and an output gate <span class=
      "inline-equation"><span class="tex">$\mathbf
      {g}^{(o)}_j$</span></span> :
      <div class="table-responsive" id="eq7">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathbf {g}^{(i)}_j
          &amp;= \textstyle \sigma (W^{(i)}\mathbf {y}_j +
          U^{(i)}\mathbf {h}_{j-1} + \mathbf {b}^{(i)}),
          \end{align}</span><br />
          <span class="equation-number">(9)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq8">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathbf {g}^{(f)}_j
          &amp;= \textstyle \sigma (W^{(f)}\mathbf {y}_j +
          U^{(f)}\mathbf {h}_{j-1} + \mathbf {b}^{(f)}),
          \end{align}</span><br />
          <span class="equation-number">(10)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq9">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathbf {g}^{(c)}_j
          &amp;= \textstyle \mathbf {g}^{(i)}_j \odot
          \tilde{\mathbf {g}}^{(c)}_{j} + \mathbf {g}^{(f)}_{j}
          \odot \mathbf {g}^{(c)}_{j-1}, \end{align}</span><br />
          <span class="equation-number">(11)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq10">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathbf {g}^{(o)}_j
          &amp;= \textstyle \sigma (W^{(o)}\mathbf {y}_j +
          U^{(o)}\mathbf {h}_{j-1} + \mathbf {b}^{(o)}),
          \end{align}</span><br />
          <span class="equation-number">(12)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq11">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \mathbf {h}_j
          &amp;= \mathbf {g}^{(o)}_j \odot tanh(\mathbf
          {g}^{(c)}_j), \end{align}</span><br />
          <span class="equation-number">(13)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\tilde{\mathbf {g}}^{(c)}_{t,j} = tanh(W^{(c)}\mathbf
      {y}_j + U^{(c)}\mathbf {h}_{j-1} + \mathbf
      {b}^{(c)})$</span></span> and ⊙ is an element-wise product.
      We denote the set of LSTM parameters as <span class=
      "inline-equation"><span class="tex">$\Theta ^{(lstm)} =
      \lbrace W^{(i)}\in \mathbb {R}^{d^{\prime } \times d},
      U^{(i)}\in \mathbb {R}^{d^{\prime } \times d^{\prime }},
      \mathbf {b}^{(i)}\in \mathbb {R}^{d^{\prime }}, W^{(f)}\in
      \mathbb {R}^{d^{\prime } \times d}, U^{(f)}\in \mathbb
      {R}^{d^{\prime } \times d^{\prime }}, \mathbf {b}^{(f)}\in
      \mathbb {R}^{d^{\prime }},$ $W^{(c)}\in \mathbb
      {R}^{d^{\prime } \times d}, U^{(c)}\in \mathbb {R}^{d^{\prime
      } \times d^{\prime }}, \mathbf {b}^{(c)}\in \mathbb
      {R}^{d^{\prime }}, W^{(o)}\in \mathbb {R}^{d^{\prime } \times
      d}, U^{(o)}\in \mathbb {R}^{d^{\prime } \times d^{\prime }},
      \mathbf {b}^{(o)}\in \mathbb {R}^{d^{\prime }}
      \rbrace$</span></span> .
      <p></p>
      <p>To differentiate the contributions of s-nodes, we also
      compute the attention score <span class=
      "inline-equation"><span class="tex">$\alpha _j^{\prime
      }$</span></span> for each s-node <em>r<sub>j</sub></em>
      as</p>
      <div class="table-responsive" id="eq12">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \alpha _j^{\prime }
          &amp; = \frac{\exp (\beta _j^{\prime })}{\sum _{k=1}^{t}
          \exp (\beta _k^{\prime })}, \end{align}</span><br />
          <span class="equation-number">(14)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq13">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \beta _j^{\prime }
          &amp; = \eta ^{\prime } \cdot \sigma (Q^{\prime } \mathbf
          {h}_j + \mathbf {b}^{\prime }), \end{align}</span><br />
          <span class="equation-number">(15)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\eta ^{\prime } \in \mathbb {R}^{d^{\prime
      }}$</span></span> , <span class=
      "inline-equation"><span class="tex">$Q^{\prime } \in \mathbb
      {R}^{d^{\prime } \times d^{\prime }}$</span></span> and
      <span class="inline-equation"><span class="tex">$\mathbf
      {b}^{\prime } \in \mathbb {R}^{d^{\prime }}$</span></span>
      are parameters. As a result, we compute the s-path embedding
      for <em>p<sub>k</sub></em> as
      <div class="table-responsive" id="Xeq3">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mathbf {z}_k
          \textstyle = \sum _{j=1}^{t} \alpha _j^{\prime } \mathbf
          {h}_j. \end{equation}</span><br />
          <span class="equation-number">(16)</span>
        </div>
      </div><strong>Proximity Embedding.</strong>Once having the
      s-path embedding for each of the s-paths between <em>q</em>
      and <em>v</em>, we can compute their proximity embedding. For
      a unified notation, we introduce <span class=
      "inline-equation"><span class="tex">$\hat{\mathcal
      {P}}(q,v)$</span></span> as the s-path set between <em>q</em>
      and <em>v</em>. For asymmetric relations, we define
      <span class="inline-equation"><span class=
      "tex">$\hat{\mathcal {P}}(q,v)$</span></span> as the s-paths
      from <em>q</em> to <em>v</em>. For symmetric relations, we
      define <span class="inline-equation"><span class=
      "tex">$\hat{\mathcal {P}}(q,v)$</span></span> as the s-paths
      both from <em>q</em> to <em>v</em> and from <em>v</em> to
      <em>q</em>. To differentiate the contributions of s-paths, we
      compute the attention score <span class=
      "inline-equation"><span class="tex">$\alpha _k^{\prime \prime
      }$</span></span> for each s-path <em>p<sub>k</sub></em> as
      <div class="table-responsive" id="eq14">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \alpha _k^{\prime
          \prime } &amp; = \frac{\exp (\beta _k^{\prime \prime
          })}{\sum _{p_i \in \hat{\mathcal {P}}(q,v)} \exp (\beta
          _i^{\prime \prime })}, \end{align}</span><br />
          <span class="equation-number">(17)</span>
        </div>
      </div>
      <div class="table-responsive" id="eq15">
        <div class="display-equation">
          <span class="tex mytex">\begin{align} \beta _k^{\prime
          \prime } &amp; = \eta ^{\prime \prime } \cdot \sigma
          (Q^{\prime \prime } \cdot \mathbf {z}_k + \mathbf
          {b}^{\prime \prime }), \end{align}</span><br />
          <span class="equation-number">(18)</span>
        </div>
      </div>where <span class="inline-equation"><span class=
      "tex">$\eta ^{\prime \prime } \in \mathbb {R}^{d^{\prime
      }}$</span></span> , <span class=
      "inline-equation"><span class="tex">$Q^{\prime \prime } \in
      \mathbb {R}^{d^{\prime } \times d^{\prime }}$</span></span>
      and <span class="inline-equation"><span class="tex">$\mathbf
      {b}^{\prime \prime } \in \mathbb {R}^{d^{\prime
      }}$</span></span> are parameters. Finally, we compute the
      proximity embedding for (<em>q</em>, <em>v</em>) as
      <div class="table-responsive" id="eq16">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \mathbf {f}(q,v)
          \textstyle = \sum _{p_k \in \hat{\mathcal {P}}(q,v)}
          \alpha _k^{\prime \prime } \mathbf {z}_k.
          \end{equation}</span><br />
          <span class="equation-number">(19)</span>
        </div>
      </div>This <strong>f</strong>(<em>q</em>, <em>v</em>) encodes
      the information of all the subgraph-augmented paths between
      <em>q</em> and <em>v</em>. It will be later used to estimate
      the proximity score of <em>q</em> and <em>v</em> by
      Eq.&nbsp;<a class="eqn" href="#eq1">1</a>.
      <p></p>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> End-to-End
          Training</h2>
        </div>
      </header>
      <p>In training, for each tuple (<em>q<sub>i</sub></em> ,
      <em>v<sub>i</sub></em> , <em>u<sub>i</sub></em> ),
      ∀<em>i</em> = 1, ..., <em>n</em>, we define a ranking loss
      based on the proximity scores
      <em>π</em>(<em>q<sub>i</sub></em> , <em>v<sub>i</sub></em> )
      and <em>π</em>(<em>q<sub>i</sub></em> ,
      <em>u<sub>i</sub></em> ). We define the ranking loss function
      as</p>
      <div class="table-responsive" id="eq17">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \ell (\pi
          (q_i,v_i), \pi (q_i,u_i)) = -\log \sigma _{\lambda }(\pi
          (q_i,v_i) - \pi (q_i,u_i)), \end{equation}</span><br />
          <span class="equation-number">(20)</span>
        </div>
      </div>where <em>σ<sub>λ</sub></em> (<em>x</em>) = 1/(1 +
      <em>e</em> <sup>− <em>λx</em></sup> ) and <em>λ</em> &gt; 0
      is a parameter. We denote the parameter set of our
      three-layer attentions for subgraphs, s-nodes and s-paths as
      <em>Θ</em> <sup>(<em>att</em>)</sup> = {<em>η</em>,
      <em>Q</em>, <strong>b</strong>, <em>η</em>′, <em>Q</em>′,
      <strong>b</strong>′, <em>η</em>′′, <em>Q</em>′′,
      <strong>b</strong>′′}. In total, our model parameters are
      <em>Θ</em> = {<em>Θ</em> <sup>(<em>lstm</em>)</sup>,
      <em>Θ</em> <sup>(<em>att</em>)</sup>, <em>θ</em>}. Our
      ultimate goal in training is to minimize
      <div class="table-responsive" id="eq18">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} L(\Theta) = \sum
          \nolimits _{i=1}^n \ell (\pi (q_i,v_i), \pi (q_i,u_i)) +
          \mu ~ \Omega (\Theta), \end{equation}</span><br />
          <span class="equation-number">(21)</span>
        </div>
      </div>where <em>μ</em> &gt; 0 is a trade-off parameter,
      <em>Ω</em>(·) is a regularization function (<em>e.g.</em>,
      the sum of <em>l</em> <sub>2</sub>-norm for each parameter in
      <em>Θ</em>).
      <p></p>
      <p><img src=
      "../../../data/deliveryimages.acm.org/10.1145/3190000/3186073/images/www2018-82-img2.svg"
      class="img-responsive" alt="" longdesc="" /></p>
      <p><strong>Training algorithm</strong>: we summarize the SPE
      training algorithm in Alg.&nbsp;2 . In lines 2–4, we sample
      object paths on <em>G</em>. In lines 6 and 7, we construct
      subgraph-augmented paths based on the object paths for each
      (<em>q</em>, <em>v</em>) and (<em>q</em>, <em>u</em>) in the
      training tuples. In line 8, we split the training tuples into
      batches, and then do batch stochastic gradient descent. In
      lines 12 and 13, we compute the proximity embedding vectors
      <strong>f</strong>(<em>q</em>, <em>v</em>) and
      <strong>f</strong>(<em>q</em>, <em>u</em>). In line 14, we
      compute the ranking loss ℓ(<em>π</em>(<em>q</em>,
      <em>v</em>), <em>π</em>(<em>q</em>, <em>u</em>)). In lines 15
      and 16, we accumulate the loss for batch <em>b</em> and do
      stochastic gradient descent.</p>
      <p><strong>Complexity analysis</strong>: we analyze the time
      complexity for Alg.&nbsp;2 . In lines 2–4, we sample
      <em>γ</em> object paths of length ℓ starting from each object
      in <em>G</em>, hence it takes
      <em>O</em>(|<em>V</em>|<em>γ</em>ℓ). In lines 5–7, we in
      total construct <span class="inline-equation"><span class=
      "tex">$|\hat{\mathcal {P}}|$</span></span> s-paths. According
      to Alg.&nbsp;1, contructing an s-path takes <em>O</em>(ℓ +
      <em>ζ</em>). Thus the complexity of lines 7-10 is
      <span class="inline-equation"><span class=
      "tex">$O(|\hat{\mathcal {P}}| (\ell +\zeta))$</span></span> .
      In line 8, generating the batches takes
      <em>O</em>(<em>n</em>). In line 12, computing proximity
      embedding <strong>f</strong>(<em>q</em>, <em>v</em>) requires
      three steps: 1) embedding an s-node, which takes <span class=
      "inline-equation"><span class="tex">$O(|\mathcal {M}|
      (d^{\prime } d + d^{\prime }))$</span></span> given at most
      <span class="inline-equation"><span class="tex">$|\mathcal
      {M}|$</span></span> subgraphs; 2) embedding an s-path, which
      takes <em>O</em>(ℓ(<em>d</em>′<em>d</em> +
      <em>d</em>′<sup>2</sup> + <em>d</em>′)) given an s-path's
      length of at most ℓ; 3) proximity embedding, which takes
      <span class="inline-equation"><span class=
      "tex">$O(|\hat{\mathcal {P}}(q,v)| (d^{\prime 2} + d^{\prime
      }))$</span></span> given <span class=
      "inline-equation"><span class="tex">$|\hat{\mathcal
      {P}}(q,v)|$</span></span> s-paths between <em>q</em> and
      <em>v</em>. In lines 9–16, we essentially compute
      <strong>f</strong>(<em>q</em>, <em>v</em>)’s and
      <strong>f</strong>(<em>q</em>, <em>u</em>)’s for all the
      <span class="inline-equation"><span class="tex">$(q,v,u) \in
      \mathcal {D}$</span></span> , which in total takes
      <span class="inline-equation"><span class=
      "tex">$O(|\hat{\mathcal {P}}| ((d^{\prime 2} + d^{\prime }) +
      \ell (d^{\prime } d + d^{\prime 2} + d^{\prime }) + \ell
      (|\mathcal {M}| (d^{\prime } d + d^{\prime })))) =
      O(|\hat{\mathcal {P}}| (\ell d^{\prime 2} + \ell |\mathcal
      {M}| d^{\prime }d + \ell d^{\prime }))$</span></span> .
      Computing the loss in line 14 over all the training tuples
      takes <em>O</em>(<em>nd</em>′). Updating <em>Θ</em> in line
      16 for <span class="inline-equation"><span class=
      "tex">$|\mathcal {B}|$</span></span> batches takes
      <span class="inline-equation"><span class="tex">$O(|\mathcal
      {B}| (d^{\prime } d + d^{\prime 2}))$</span></span> . Note
      that we compute the subgraph embedding offline once. It takes
      <span class="inline-equation"><span class="tex">$O(|\mathcal
      {M}|^2)$</span></span> to construct the structural similarity
      matrix, and <span class="inline-equation"><span class=
      "tex">$O(|\mathcal {M}| d + |\mathcal {M}|)$</span></span> to
      learn the subgraph embedding, which in total is <span class=
      "inline-equation"><span class="tex">$O(|\mathcal {M}|^2 +
      |\mathcal {M}| d)$</span></span> . In summary, the total
      complexity of Alg.&nbsp;2 is <span class=
      "inline-equation"><span class="tex">$O(|\hat{\mathcal {P}}|
      \zeta + |\mathcal {M}|^2 + |\mathcal {M}| d + |\hat{\mathcal
      {P}}| \ell (d^{\prime 2} + |\mathcal {M}| d^{\prime }d +
      d^{\prime }))$</span></span> . Since <span class=
      "inline-equation"><span class="tex">$|\hat{\mathcal {P}}| \le
      |V| \gamma \ell$</span></span> , the complexity of
      Alg.&nbsp;2 becomes <span class=
      "inline-equation"><span class="tex">$O(|V| \gamma \ell (\zeta
      + \ell (d^{\prime 2} + |\mathcal {M}| d^{\prime }d +
      d^{\prime })) + |\mathcal {M}|^2)$</span></span> .</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Experiments</h2>
        </div>
      </header>
      <p><strong>Heterogeneous social networks.</strong> We
      conducted extensive experiments on three real-world data sets
      collected by previous studies, namely LinkedIn [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0019">19</a>], Facebook
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>] and DBLP
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0036">36</a>]. Each data
      set contains objects of various types. In particular,
      LinkedIn includes the types of user, employer, location and
      college; Facebook includes user, concentration, degree,
      school, hometown, last-name, location, employer,
      work-location and work-project (other types are ignored due
      to their sparsity or irrelevance); DBLP includes paper,
      author, year, conference and keyword. We organized them into
      heterogeneous social networks, as summarized in
      Table&nbsp;<a class="tbl" href="#tab2">2</a>.</p>
      <div class="table-responsive" id="tab2">
        <div class="table-caption">
          <span class="table-number">Table 2:</span> <span class=
          "table-title">Summary of data sets and subgraphs.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">
              <strong>Network</strong></th>
              <th style="text-align:right;">
              <strong>Objects</strong></th>
              <th style="text-align:center;">
              <strong>Edges</strong></th>
              <th style="text-align:center;">
              <strong>Types</strong></th>
              <th style="text-align:center;">
              <strong>Average</strong></th>
              <th style="text-align:right;">
              <strong>Subgraph</strong></th>
              <th><strong>Subgraph</strong></th>
            </tr>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:right;"></th>
              <th style="text-align:center;"></th>
              <th style="text-align:center;"></th>
              <th style="text-align:center;">
              <strong>degree</strong></th>
              <th style="text-align:right;">
              <strong>patterns</strong></th>
              <th><strong>instances</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">LinkedIn</td>
              <td style="text-align:right;">65,925</td>
              <td style="text-align:center;">220,812</td>
              <td style="text-align:center;">4</td>
              <td style="text-align:center;">6.7</td>
              <td style="text-align:right;">173</td>
              <td>604,848,383</td>
            </tr>
            <tr>
              <td style="text-align:center;">Facebook</td>
              <td style="text-align:right;">5,025</td>
              <td style="text-align:center;">100,356</td>
              <td style="text-align:center;">10</td>
              <td style="text-align:center;">39.9</td>
              <td style="text-align:right;">981</td>
              <td>2,398,306,414</td>
            </tr>
            <tr>
              <td style="text-align:center;">DBLP</td>
              <td style="text-align:right;">165,728</td>
              <td style="text-align:center;">928,513</td>
              <td style="text-align:center;">5</td>
              <td style="text-align:center;">11.2</td>
              <td style="text-align:right;">88</td>
              <td>740,682,735</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p><strong>Ground truth.</strong> On LinkedIn, the user
      relationships are already labeled into different semantic
      classes. We tested two major classes: <em>schoolmate</em> and
      <em>colleague</em>. On Facebook, the user relationships are
      defined by [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0011">11</a>]
      with two classes: <em>family</em> and <em>classmate</em>. On
      DBLP, the <em>adivsor</em> and <em>advisee</em> in a
      co-author pair are identified based on the website of some
      faculty members as well as the Mathematics Genealogy and AI
      Genealogy projects [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>]. All unidentified co-author pairs
      are assumed to be negative. As summarized in
      Table&nbsp;<a class="tbl" href="#tab3">3</a>, the
      <em>advisor</em> and <em>advisee</em> classes are asymmetric,
      whereas the others are all symmetric.</p>
      <div class="table-responsive" id="tab3">
        <div class="table-caption">
          <span class="table-number">Table 3:</span> <span class=
          "table-title">Summary of semantic user relations and
          queries.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">
              <strong>Network</strong></th>
              <th style="text-align:center;">
              <strong>Semantic</strong></th>
              <th style="text-align:center;">
              <strong>Symmetry</strong></th>
              <th style="text-align:center;">
              <strong>Queries</strong></th>
              <th><strong>Results</strong></th>
            </tr>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:center;">
              <strong>relations</strong></th>
              <th style="text-align:center;"></th>
              <th style="text-align:center;"></th>
              <th><strong>per query</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">LinkedIn</td>
              <td style="text-align:center;">Schoolmate</td>
              <td style="text-align:center;">Yes</td>
              <td style="text-align:center;">172</td>
              <td>16.2</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">Colleague</td>
              <td style="text-align:center;">Yes</td>
              <td style="text-align:center;">173</td>
              <td>12.8</td>
            </tr>
            <tr>
              <td style="text-align:center;">Facebook</td>
              <td style="text-align:center;">Family</td>
              <td style="text-align:center;">Yes</td>
              <td style="text-align:center;">340</td>
              <td>4.0</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">Classmate</td>
              <td style="text-align:center;">Yes</td>
              <td style="text-align:center;">904</td>
              <td>6.5</td>
            </tr>
            <tr>
              <td style="text-align:center;">DBLP</td>
              <td style="text-align:center;">Advisor</td>
              <td style="text-align:center;">No</td>
              <td style="text-align:center;">2,439</td>
              <td>1.3</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">Advisee</td>
              <td style="text-align:center;">No</td>
              <td style="text-align:center;">1,204</td>
              <td>2.6</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p><strong>Training and testing.</strong>On each graph, a
      user <em>q</em> can be used as a query node, if there exists
      another user <em>v</em> such that <em>q</em> and <em>v</em>
      have the desired semantic relation in our ground truth. The
      number of query users for each network and each semantic
      relation, and the average number of results per query are
      shown in Table&nbsp;<a class="tbl" href="#tab3">3</a>. We
      randomly split these queries into two subsets: 20% reserved
      as training and the rest as testing. We repeated such
      splitting for 10 times, and averaged any result over these 10
      splits. In each split, based on the training queries, we
      further generated training examples (<em>q</em>, <em>v</em>,
      <em>u</em>) such that <em>q</em> and <em>v</em> belong to the
      desired semantic relation whereas <em>q</em> and <em>u</em>
      do not. For testing, we constructed an ideal ranking for each
      test query user and each desired semantic relation. We
      compared this ideal ranking against the ranking generated by
      various semantic user search algorithms. We adopted NDCG and
      MAP [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0011">11</a>]
      to evaluate the quality of the algorithmic rankings at the
      top 10 results.</p>
      <p><strong>Subgraphs.</strong>We repeated the subgraph
      pattern mining and subgraph instance matching algorithms in
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>] to obtain
      the set of frequent subgraph patterns <span class=
      "inline-equation"><span class="tex">$\mathcal
      {M}$</span></span> and its instances <span class=
      "inline-equation"><span class="tex">$\mathcal
      {I}$</span></span> on each network. Specifically, we first
      applied GRAMI [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>] on each graph to mine the set of
      frequent subgraph patterns. Then we filtered out those
      clearly non-viable subgraphs: 1) since our ground truth is
      designed for semantic user search, a viable subgraph must
      have at least two user objects; 2) a subgraph must contain at
      least two different types for capturing richer semantics; 3)
      to further constrain the number of subgraphs, we restricted
      them to have at most five nodes on LinkedIn and Facebook or
      six nodes on DBLP, which are found to be adequate in
      expressing the interactions between two users. The resulting
      number of subgraph patterns are shown in Table&nbsp;<a class=
      "tbl" href="#tab2">2</a>.</p>
      <div class="table-responsive" id="tab4">
        <div class="table-caption">
          <span class="table-number">Table 4:</span> <span class=
          "table-title">Running time of offline subgraph
          indexing.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:center;">LinkedIn</th>
              <th style="text-align:center;">Facebook</th>
              <th>DBLP</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">Time (hour)</td>
              <td style="text-align:center;">1.77</td>
              <td style="text-align:center;">3.34</td>
              <td>4.43</td>
            </tr>
          </tbody>
        </table>
      </div>
      <div class="table-responsive" id="tab5">
        <div class="table-caption">
          <span class="table-number">Table 5:</span> <span class=
          "table-title">Result comparison under different amounts
          of labels (<em>i.e.</em>, 10, 100, and 1000).</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:center;">Methods</th>
              <th colspan="6" style="text-align:center;">
                LinkedIn
                <hr />
              </th>
              <th colspan="6" style="text-align:center;">
                Facebook
                <hr />
              </th>
              <th colspan="6" style="text-align:center;">
                DBLP
                <hr />
              </th>
            </tr>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:center;"></th>
              <th colspan="3" style="text-align:center;">
                <em>Schoolmate</em>
                <hr />
              </th>
              <th colspan="3" style="text-align:center;">
                <em>Colleague</em>
                <hr />
              </th>
              <th colspan="3" style="text-align:center;">
                <em>Classmate</em>
                <hr />
              </th>
              <th colspan="3" style="text-align:center;">
                <em>Family</em>
                <hr />
              </th>
              <th colspan="3" style="text-align:center;">
                <em>Advisor</em>
                <hr />
              </th>
              <th colspan="3" style="text-align:center;">
                <em>Advisee</em>
                <hr />
              </th>
            </tr>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:center;"></th>
              <th style="text-align:center;">10</th>
              <th style="text-align:center;">100</th>
              <th style="text-align:center;">1000</th>
              <th style="text-align:center;">10</th>
              <th style="text-align:center;">100</th>
              <th style="text-align:center;">1000</th>
              <th style="text-align:center;">10</th>
              <th style="text-align:center;">100</th>
              <th style="text-align:center;">1000</th>
              <th style="text-align:center;">10</th>
              <th style="text-align:center;">100</th>
              <th style="text-align:center;">1000</th>
              <th style="text-align:center;">10</th>
              <th style="text-align:center;">100</th>
              <th style="text-align:center;">1000</th>
              <th style="text-align:center;">10</th>
              <th style="text-align:center;">100</th>
              <th>1000</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">
              <strong>NDCG@10</strong></td>
              <td style="text-align:center;">ProxEmbed</td>
              <td style="text-align:center;">0.646</td>
              <td style="text-align:center;">0.652</td>
              <td style="text-align:center;">0.670</td>
              <td style="text-align:center;">0.561</td>
              <td style="text-align:center;">0.606</td>
              <td style="text-align:center;">0.616</td>
              <td style="text-align:center;">0.796</td>
              <td style="text-align:center;">0.851</td>
              <td style="text-align:center;">0.852</td>
              <td style="text-align:center;">0.584</td>
              <td style="text-align:center;">0.743</td>
              <td style="text-align:center;">0.761</td>
              <td style="text-align:center;">0.753</td>
              <td style="text-align:center;">0.765</td>
              <td style="text-align:center;">0.771</td>
              <td style="text-align:center;">0.374</td>
              <td style="text-align:center;">0.405</td>
              <td>0.411</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">MGP</td>
              <td style="text-align:center;">0.546</td>
              <td style="text-align:center;">0.568</td>
              <td style="text-align:center;">0.574</td>
              <td style="text-align:center;">0.527</td>
              <td style="text-align:center;">0.546</td>
              <td style="text-align:center;">0.552</td>
              <td style="text-align:center;">
              <strong>0.797</strong></td>
              <td style="text-align:center;">0.843</td>
              <td style="text-align:center;">0.849</td>
              <td style="text-align:center;">
              <strong>0.627</strong></td>
              <td style="text-align:center;">0.732</td>
              <td style="text-align:center;">0.753</td>
              <td style="text-align:center;">0.618</td>
              <td style="text-align:center;">0.718</td>
              <td style="text-align:center;">0.745</td>
              <td style="text-align:center;">0.385</td>
              <td style="text-align:center;">0.403</td>
              <td>0.405</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">MPP</td>
              <td style="text-align:center;">0.503</td>
              <td style="text-align:center;">0.504</td>
              <td style="text-align:center;">0.504</td>
              <td style="text-align:center;">0.497</td>
              <td style="text-align:center;">0.510</td>
              <td style="text-align:center;">0.508</td>
              <td style="text-align:center;">0.707</td>
              <td style="text-align:center;">0.803</td>
              <td style="text-align:center;">0.796</td>
              <td style="text-align:center;">0.575</td>
              <td style="text-align:center;">0.647</td>
              <td style="text-align:center;">0.640</td>
              <td style="text-align:center;">0.581</td>
              <td style="text-align:center;">0.662</td>
              <td style="text-align:center;">0.674</td>
              <td style="text-align:center;">0.345</td>
              <td style="text-align:center;">0.380</td>
              <td>0.390</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">SRW</td>
              <td style="text-align:center;">0.520</td>
              <td style="text-align:center;">0.515</td>
              <td style="text-align:center;">0.515</td>
              <td style="text-align:center;">0.513</td>
              <td style="text-align:center;">0.502</td>
              <td style="text-align:center;">0.503</td>
              <td style="text-align:center;">0.396</td>
              <td style="text-align:center;">0.389</td>
              <td style="text-align:center;">0.394</td>
              <td style="text-align:center;">0.396</td>
              <td style="text-align:center;">0.389</td>
              <td style="text-align:center;">0.394</td>
              <td style="text-align:center;">0.689</td>
              <td style="text-align:center;">0.689</td>
              <td style="text-align:center;">0.689</td>
              <td style="text-align:center;">
              <strong>0.402</strong></td>
              <td style="text-align:center;">0.402</td>
              <td>0.402</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">DWR</td>
              <td style="text-align:center;">0.515</td>
              <td style="text-align:center;">0.518</td>
              <td style="text-align:center;">0.530</td>
              <td style="text-align:center;">0.493</td>
              <td style="text-align:center;">0.506</td>
              <td style="text-align:center;">0.504</td>
              <td style="text-align:center;">0.692</td>
              <td style="text-align:center;">0.689</td>
              <td style="text-align:center;">0.699</td>
              <td style="text-align:center;">0.585</td>
              <td style="text-align:center;">0.575</td>
              <td style="text-align:center;">0.583</td>
              <td style="text-align:center;">
              <strong>0.764</strong></td>
              <td style="text-align:center;">0.758</td>
              <td style="text-align:center;">0.661</td>
              <td style="text-align:center;">0.401</td>
              <td style="text-align:center;">0.395</td>
              <td>0.392</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">PES</td>
              <td style="text-align:center;">0.650</td>
              <td style="text-align:center;">0.676</td>
              <td style="text-align:center;">0.687</td>
              <td style="text-align:center;">0.552</td>
              <td style="text-align:center;">0.647</td>
              <td style="text-align:center;">0.668</td>
              <td style="text-align:center;">0.720</td>
              <td style="text-align:center;">0.851</td>
              <td style="text-align:center;">0.862</td>
              <td style="text-align:center;">0.482</td>
              <td style="text-align:center;">0.748</td>
              <td style="text-align:center;">0.752</td>
              <td style="text-align:center;">0.741</td>
              <td style="text-align:center;">0.768</td>
              <td style="text-align:center;">0.773</td>
              <td style="text-align:center;">0.374</td>
              <td style="text-align:center;">0.406</td>
              <td>0.416</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">SPE-A (ours)</td>
              <td style="text-align:center;">0.541</td>
              <td style="text-align:center;">0.603</td>
              <td style="text-align:center;">0.640</td>
              <td style="text-align:center;">0.545</td>
              <td style="text-align:center;">0.597</td>
              <td style="text-align:center;">0.630</td>
              <td style="text-align:center;">0.587</td>
              <td style="text-align:center;">0.565</td>
              <td style="text-align:center;">0.547</td>
              <td style="text-align:center;">0.424</td>
              <td style="text-align:center;">0.451</td>
              <td style="text-align:center;">0.411</td>
              <td style="text-align:center;">0.436</td>
              <td style="text-align:center;">0.451</td>
              <td style="text-align:center;">0.449</td>
              <td style="text-align:center;">0.325</td>
              <td style="text-align:center;">0.353</td>
              <td>0.356</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">SPE (ours)</td>
              <td style="text-align:center;">
              <strong>0.660</strong></td>
              <td style="text-align:center;">
              <strong>0.690</strong></td>
              <td style="text-align:center;">
              <strong>0.694</strong></td>
              <td style="text-align:center;">
              <strong>0.644</strong></td>
              <td style="text-align:center;">
              <strong>0.686</strong></td>
              <td style="text-align:center;">
              <strong>0.704</strong></td>
              <td style="text-align:center;">0.764</td>
              <td style="text-align:center;">
              <strong>0.866</strong></td>
              <td style="text-align:center;">
              <strong>0.869</strong></td>
              <td style="text-align:center;">0.467</td>
              <td style="text-align:center;">
              <strong>0.761</strong></td>
              <td style="text-align:center;">
              <strong>0.774</strong></td>
              <td style="text-align:center;">0.763</td>
              <td style="text-align:center;">
              <strong>0.782</strong></td>
              <td style="text-align:center;">
              <strong>0.789</strong></td>
              <td style="text-align:center;">0.373</td>
              <td style="text-align:center;">
              <strong>0.413</strong></td>
              <td><strong>0.417</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>MAP@10</strong></td>
              <td style="text-align:center;">ProxEmbed</td>
              <td style="text-align:center;">0.535</td>
              <td style="text-align:center;">0.541</td>
              <td style="text-align:center;">0.562</td>
              <td style="text-align:center;">0.443</td>
              <td style="text-align:center;">0.492</td>
              <td style="text-align:center;">0.503</td>
              <td style="text-align:center;">
              <strong>0.735</strong></td>
              <td style="text-align:center;">0.801</td>
              <td style="text-align:center;">0.803</td>
              <td style="text-align:center;">0.498</td>
              <td style="text-align:center;">0.678</td>
              <td style="text-align:center;">0.711</td>
              <td style="text-align:center;">0.672</td>
              <td style="text-align:center;">0.690</td>
              <td style="text-align:center;">0.701</td>
              <td style="text-align:center;">0.251</td>
              <td style="text-align:center;">0.283</td>
              <td>0.297</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">MGP</td>
              <td style="text-align:center;">0.288</td>
              <td style="text-align:center;">0.305</td>
              <td style="text-align:center;">0.313</td>
              <td style="text-align:center;">0.317</td>
              <td style="text-align:center;">0.333</td>
              <td style="text-align:center;">0.338</td>
              <td style="text-align:center;">0.673</td>
              <td style="text-align:center;">0.729</td>
              <td style="text-align:center;">0.738</td>
              <td style="text-align:center;">
              <strong>0.520</strong></td>
              <td style="text-align:center;">0.651</td>
              <td style="text-align:center;">0.681</td>
              <td style="text-align:center;">0.548</td>
              <td style="text-align:center;">0.663</td>
              <td style="text-align:center;">0.696</td>
              <td style="text-align:center;">0.278</td>
              <td style="text-align:center;">0.280</td>
              <td>0.295</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">MPP</td>
              <td style="text-align:center;">0.260</td>
              <td style="text-align:center;">0.260</td>
              <td style="text-align:center;">0.259</td>
              <td style="text-align:center;">0.294</td>
              <td style="text-align:center;">0.305</td>
              <td style="text-align:center;">0.304</td>
              <td style="text-align:center;">0.576</td>
              <td style="text-align:center;">0.681</td>
              <td style="text-align:center;">0.675</td>
              <td style="text-align:center;">0.467</td>
              <td style="text-align:center;">0.543</td>
              <td style="text-align:center;">0.534</td>
              <td style="text-align:center;">0.499</td>
              <td style="text-align:center;">0.597</td>
              <td style="text-align:center;">0.611</td>
              <td style="text-align:center;">0.245</td>
              <td style="text-align:center;">0.273</td>
              <td>0.281</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">SRW</td>
              <td style="text-align:center;">0.275</td>
              <td style="text-align:center;">0.272</td>
              <td style="text-align:center;">0.272</td>
              <td style="text-align:center;">0.294</td>
              <td style="text-align:center;">0.289</td>
              <td style="text-align:center;">0.291</td>
              <td style="text-align:center;">0.312</td>
              <td style="text-align:center;">0.324</td>
              <td style="text-align:center;">0.384</td>
              <td style="text-align:center;">0.259</td>
              <td style="text-align:center;">0.255</td>
              <td style="text-align:center;">0.259</td>
              <td style="text-align:center;">0.630</td>
              <td style="text-align:center;">0.630</td>
              <td style="text-align:center;">0.630</td>
              <td style="text-align:center;">0.294</td>
              <td style="text-align:center;">0.294</td>
              <td>0.294</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">DWR</td>
              <td style="text-align:center;">0.393</td>
              <td style="text-align:center;">0.391</td>
              <td style="text-align:center;">0.398</td>
              <td style="text-align:center;">0.363</td>
              <td style="text-align:center;">0.369</td>
              <td style="text-align:center;">0.368</td>
              <td style="text-align:center;">0.578</td>
              <td style="text-align:center;">0.575</td>
              <td style="text-align:center;">0.587</td>
              <td style="text-align:center;">0.467</td>
              <td style="text-align:center;">0.455</td>
              <td style="text-align:center;">0.463</td>
              <td style="text-align:center;">
              <strong>0.683</strong></td>
              <td style="text-align:center;">0.675</td>
              <td style="text-align:center;">0.541</td>
              <td style="text-align:center;">
              <strong>0.295</strong></td>
              <td style="text-align:center;">0.281</td>
              <td>0.276</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">PES</td>
              <td style="text-align:center;">
              <strong>0.547</strong></td>
              <td style="text-align:center;">0.567</td>
              <td style="text-align:center;">0.581</td>
              <td style="text-align:center;">0.432</td>
              <td style="text-align:center;">0.535</td>
              <td style="text-align:center;">0.559</td>
              <td style="text-align:center;">0.617</td>
              <td style="text-align:center;">0.797</td>
              <td style="text-align:center;">0.815</td>
              <td style="text-align:center;">0.354</td>
              <td style="text-align:center;">0.685</td>
              <td style="text-align:center;">0.691</td>
              <td style="text-align:center;">0.667</td>
              <td style="text-align:center;">0.693</td>
              <td style="text-align:center;">0.699</td>
              <td style="text-align:center;">0.267</td>
              <td style="text-align:center;">0.282</td>
              <td>0.293</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">SPE-A (ours)</td>
              <td style="text-align:center;">0.447</td>
              <td style="text-align:center;">0.508</td>
              <td style="text-align:center;">0.545</td>
              <td style="text-align:center;">0.439</td>
              <td style="text-align:center;">0.490</td>
              <td style="text-align:center;">0.525</td>
              <td style="text-align:center;">0.457</td>
              <td style="text-align:center;">0.436</td>
              <td style="text-align:center;">0.418</td>
              <td style="text-align:center;">0.294</td>
              <td style="text-align:center;">0.326</td>
              <td style="text-align:center;">0.277</td>
              <td style="text-align:center;">0.287</td>
              <td style="text-align:center;">0.297</td>
              <td style="text-align:center;">0.305</td>
              <td style="text-align:center;">0.231</td>
              <td style="text-align:center;">0.254</td>
              <td>0.243</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">SPE (ours)</td>
              <td style="text-align:center;">
              <strong>0.547</strong></td>
              <td style="text-align:center;">
              <strong>0.581</strong></td>
              <td style="text-align:center;">
              <strong>0.587</strong></td>
              <td style="text-align:center;">
              <strong>0.529</strong></td>
              <td style="text-align:center;">
              <strong>0.575</strong></td>
              <td style="text-align:center;">
              <strong>0.602</strong></td>
              <td style="text-align:center;">0.682</td>
              <td style="text-align:center;">
              <strong>0.822</strong></td>
              <td style="text-align:center;">
              <strong>0.826</strong></td>
              <td style="text-align:center;">0.368</td>
              <td style="text-align:center;">
              <strong>0.704</strong></td>
              <td style="text-align:center;">
              <strong>0.723</strong></td>
              <td style="text-align:center;">0.679</td>
              <td style="text-align:center;">
              <strong>0.700</strong></td>
              <td style="text-align:center;">
              <strong>0.703</strong></td>
              <td style="text-align:center;">0.249</td>
              <td style="text-align:center;">
              <strong>0.298</strong></td>
              <td><strong>0.299</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>As shown in Table <a class="tbl" href="#tab4">4</a>, the
      subgraph indexing (including mining the frequent set of
      subgraph patterns, and matching each subgraph pattern with
      its possible instances on the graph) can be done in a
      reasonable time (<em>i.e.</em>, a few hours). Since subgraph
      indexing is not the focus of this paper, we consider subgraph
      indexing as already done and the resulting subgraph
      patterns/instances are readily available for our
      subgraph-augmented path embedding algorithm.</p>
      <p><strong>Parameters and environment</strong>. For the fair
      comparison, we use the same object path sampling design and
      parameters as ProxEmbed [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>]. Specifically, on LinkedIn, for both
      <em>schoolmate</em> and <em>colleague</em> we set <em>γ</em>
      = 20, ℓ = 20. On Facebook, we set <em>γ</em> = 40, ℓ = 80 for
      <em>classmate</em> and <em>γ</em> = 20, ℓ = 80 for
      <em>family</em>. On DBLP, we set <em>γ</em> = 20, ℓ = 80 for
      <em>advisor</em> and <em>γ</em> = 20, ℓ = 40 for
      <em>advisee</em>. By default, we set the number of dimension
      <em>d</em>′ = 12 for the parameters in <em>Θ</em>
      <sup>(<em>att</em>)</sup>, and <em>μ</em> = 10<sup>− 4</sup>
      in Eq.&nbsp;<a class="eqn" href="#eq18">21</a>. We tune
      different dimensions <em>d</em> of subgraph embedding and
      <em>λ</em> for different semantic relations. We run these
      experiments on Linux servers with 32GB memory, and use Theano
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0033">33</a>] for SPE
      implementation and Java jdk-1.8 for path sampling and s-path
      construction.</p>
      <p><strong>Baselines</strong>. We compare our SPE with the
      following state-of-the-art semantic search baselines.</p>
      <p>• ProxEmbed [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>]: ProxEmbed uses object paths to
      describe the relations between two objects and measure their
      proximity.</p>
      <p>• MGP [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0011">11</a>]:
      Meta-Graph Proximity uses the number of meta-graph instances
      between two objects as features to measure proximity.</p>
      <p>• MPP [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0030">30</a>]:
      Meta-Path Proximity uses the number of meta-path instances
      between two objects as features to measure proximity.</p>
      <p>• SRW [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0001">1</a>]:
      Supervised Random Walk learns edge weights to bias a random
      walk for generating consistent ranking results with the
      ground truth. We define each edge's feature as a binary
      vector based on the types of its two objects.</p>
      <p>• DWR: DeepWalk Ranking was introduced in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>]. It first learns object
      embedding by DeepWalk [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>], then outputs a Hadamard product
      over two objects’ embedding as the proximity embedding.</p>
      <p>• PES: ProxEmbed with s-paths is a straightforward
      solution to model s-paths. It directly feeds s-paths into
      ProxEmbed without subgraph embedding, s-node embedding and
      s-path embedding.</p>
      <p>• SPE-A: SPE without Attention is a baseline for
      validating the need to modeling attention. It replaces the
      attention mechanisms for subgraph embedding, s-node embedding
      and s-path embedding in SPE with mean pooling, max pooling
      and max pooling respectively.</p>
      <p>For MGP and MPP, we use the same parameter setting as
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0011">11</a>]. For SRW,
      we set its regularization parameter <em>λ</em> = 10, random
      walk teleportation parameter <em>α</em> = 0.2 and loss
      parameter <em>b</em> = 0.1. We set the dimension of DWR as
      128, the same as [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>]. For SPE-A, we use the same
      parameter values as our SPE. We input the same object paths
      to ProxEmbed, DWR, PES, SPE-A and SPE.</p>
      <p><strong>Data and code availability.</strong>All the three
      data sets are publicly available online from their
      corresponding references, as mentioned earlier. We have made
      our code available online<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>2</sup></a>.</p>
      <div class="table-responsive" id="tab6">
        <div class="table-caption">
          <span class="table-number">Table 6:</span> <span class=
          "table-title">Relative improvement of SPE over the best
          baselines in each relation when using 100 training
          tuples.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;"></th>
              <th style="text-align:center;">NDCG</th>
              <th style="text-align:center;">MAP</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;">
              LinkedIn-<em>schoolmate</em></td>
              <td style="text-align:center;">5.8% (<em>p</em> &lt;
              0.01)</td>
              <td>7.4% (<em>p</em> &lt; 0.01)</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              LinkedIn-<em>colleague</em></td>
              <td style="text-align:center;">13.2% (<em>p</em> &lt;
              0.01)</td>
              <td>16.9% (<em>p</em> &lt; 0.01)</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              Facebook-<em>family</em></td>
              <td style="text-align:center;">2.4% (<em>p</em> &lt;
              0.05)</td>
              <td>3.8% (<em>p</em> &lt; 0.01)</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              Facebook-<em>classmate</em></td>
              <td style="text-align:center;">1.8% (<em>p</em> &lt;
              0.1)</td>
              <td>2.6% (<em>p</em> &lt; 0.01)</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              DBLP-<em>advisor</em></td>
              <td style="text-align:center;">2.2% (<em>p</em> &lt;
              0.05)</td>
              <td>1.4% (<em>p</em> &lt; 0.1)</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              DBLP-<em>advisee</em></td>
              <td style="text-align:center;">2.0% (<em>p</em> &lt;
              0.05)</td>
              <td>1.4% (<em>p</em> &lt; 0.1)</td>
            </tr>
          </tbody>
        </table>
      </div>
      <figure id="fig4">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186073/images/www2018-82-fig4.jpg"
        class="img-responsive" alt="Figure 4" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 4:</span> <span class=
          "figure-title">Impact of parameters: subgraph embedding
          dimension <em>d</em>, attention parameter dimension
          <em>d</em>′, ranking loss discount <em>λ</em>.</span>
        </div>
      </figure>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.1</span> Comparison
            with Baselines</h3>
          </div>
        </header>
        <p>We compare our proposed SPE with the seven
        state-of-the-art semantic search baselines introduced
        above. We test all the methods on the six semantic
        relations under different amount of training tuples,
        <em>i.e.</em>, 10, 100 and 1000. From the results reported
        in Table <a class="tbl" href="#tab5">5</a>, we make the
        following observations.</p>
        <p>First of all, our SPE generally outperforms the
        baselines across all the six semantic relations in terms of
        both NDCG and MAP. The only exception is when training with
        10 tuples, SPE does not generate the best performance among
        all the baselines. This is because SPE has more parameters
        to learn; when the number of training tuples is small, it
        does not perform well. As the number of training tuples
        increases, SPE consistently outperforms others.</p>
        <p>Secondly, SPE is better than ProxEmbed since s-paths
        carry more semantics than simple o-paths used in ProxEmbed.
        Moreover, SPE is also better than directly applying
        ProxEmbed on s-paths (<em>i.e.</em>, PES). This is because
        PES is unable to deal with the subgraph structures and
        noises. Such results validate that, modeling s-paths for
        proximity embedding is not trivial.</p>
        <p>Thirdly, SPE is better than MGP and MPP, showing that
        feature learning is more effective than feature engineering
        in proximity learning. Although SPE uses the same subgraph
        inputs as MGP, SPE further learns subgraph embedding,
        s-node embedding and s-path embedding from the sampled
        o-paths. SPE clearly benefits from the constructed
        subgraph-augmented paths, which leverage both path's
        distance awareness and subgraph's high-order structure.</p>
        <p>Fourthly, SPE outperforms SRW. which uses the biased
        random walk to guide semantic ranking. SRW seems
        insensitive to the number of training tuples. Besides, SPE
        is better than DWR, which uses the Hadamard product over
        two objects’ embedding as the proximity embedding. This
        observation shows that, the node embedding method, as
        solving the semantic search in an indirect manner, is less
        effective for proximity search.</p>
        <p>Finally, SPE outperforms SPE-A, which validates the need
        of modeling attentions. As can be seen in Table <a class=
        "tbl" href="#tab5">5</a>, SPE-A consistently generates
        inferior performance than SPE. This implies that well
        handling the noise in subgraph and s-paths is
        importantant.</p>
        <p>We summarize the performance improvement of SPE over the
        best baselines with paired t-test in Table <a class="tbl"
        href="#tab6">6</a>. The largest improvement is observed in
        LinkedIn-<em>colleague</em>, where SPE improves the best
        baseline (PES) by relatively 13.2% in terms of NDGG and
        16.9% in terms of MAP, with t-test <em>p</em>-values less
        than 0.01.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">7.2</span> Parameter
            Sensitivity</h3>
          </div>
        </header>
        <p>We also test the parameter sensitivity of SPE using 100
        training tuples. We vary the subgraph embedding's dimension
        <em>d</em> (Eq.&nbsp;<a class="eqn" href="#eq2">3</a>), the
        attention parameters’ dimension <em>d</em>′
        (Eq.&nbsp;<a class="eqn" href="#eq6">7</a>,
        Eq.&nbsp;<a class="eqn" href="#eq13">15</a> and
        Eq.&nbsp;<a class="eqn" href="#eq15">18</a>) and the loss
        discount <em>λ</em> (Eq.&nbsp;<a class="eqn" href=
        "#eq17">20</a>).</p>
        <p>As shown in Fig.&nbsp;<a class="fig" href="#fig4">4</a>,
        Facebook data set is much more sensitive to the parameters
        setting than the other two data sets (especially LinkedIn).
        The reason is that the total number of object types in
        Facebook is much larger than in LinkedIn and DBLP as shown
        in Table <a class="tbl" href="#tab2">2</a>. When searching
        for users that meet a particular semantic relation type,
        the irrelevant types may bring in noises to the semantic
        user search process. The more types there are, the more
        noises may exist. Therefore, for the data sets with more
        types, the parameters need to be carefully tuned so that
        the embedded subgraph, the learned attentions and loss
        discount help to filter useful information out of the
        noises. On the other hand, for the data sets with less
        types like LinkedIn, the performance is relatively robust
        because the noises introduced by irrelevant object types
        are limited.</p>
        <p>Based on both NDCG and MAP over all the six semantic
        relations on three data sets, SPE tends to generate the
        best performance at <em>d</em> = 16. Especially, for
        <em>classmate</em> and <em>family</em>, when <em>d</em> is
        too small, the resulting embeddings are unable to capture
        the rich semantics. When <em>d</em> is too big, it may
        bring in more noises and increase the number of parameters
        to learn. The attention dimension <em>d</em>′ also tends to
        suffer from the similar performance decrease when
        <em>d</em>′ is too small or too big. <em>d</em>′ = 16 is
        the best setting. For the ranking loss discount parameter
        <em>λ</em>, we see that <em>λ</em> = 0.1 usually gives the
        best results, suggesting the need to discount the ranking
        loss in Eq.&nbsp;<a class="eqn" href="#eq17">20</a>.</p>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we study the problem of semantic user
      search in heterogeneous social networks. We exploit the
      opportunity of integrating the path's distance awareness and
      the subgraph's high-order structure for learning a better
      representation of the proximity between two users. We propose
      a novel Subgraph-augmented Path Embedding (SPE) model. It
      takes object paths as input, and enriches them into
      subgraph-augmented paths. Then it addresses the challenges of
      incorporating the subgraph structure, the subgraph noise and
      the subgraph-augmented path noise. Finally, it embeds the
      subgraph-augmented paths between two users into a proximity
      embedding vector. With such a proximity embedding vector, we
      can easily measure the proximity between two users for
      semantic user search. We test SPE with six semantic relations
      in three public data sets and it improves the state of the
      art by at least 1.8%–13.2% (NDCG) and 1.4%–16.9% (MAP) with
      100 training samples.</p>
      <p>In the future, we would like to explore the heterogeneous
      social networks with rich edge features and graph
      dynamics.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>We thank the support from: Zhejiang Science and Technology
      Plan Project (No. 2015C01027), National Natural Science
      Foundation of China (No. 61602405), National Research
      Foundation, Prime Minister's Office, Singapore under its
      Campus for Research Excellence and Technological Enterprise
      (CREATE) programme, and National Science Foundation under
      Grant No. IIS 16-19302. Any opinions, findings, and
      conclusions or recommendations expressed in this publication
      are those of the author(s) and do not necessarily reflect the
      views of the funding agencies.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Lars Backstrom and Jure
        Leskovec. 2011. Supervised Random Walks: Predicting and
        Recommending Links in Social Networks. In <em>WSDM</em>.
        635–644.</li>
        <li id="BibPLXBIB0002" label="[2]">Yoshua Bengio. 2009.
        Learning Deep Architectures for AI. <em>Foundations and
        Trends in Machine Learning</em> 2, 1 (2009), 1–127.</li>
        <li id="BibPLXBIB0003" label="[3]">Austin&nbsp;R. Benson,
        David&nbsp;F. Gleich, and Jure Leskovec. 2016. Higher-order
        Organization of Complex Networks. <em>Science</em> 353,
        6295 (2016), 163–166.</li>
        <li id="BibPLXBIB0004" label="[4]">Antoine Bordes, Nicolas
        Usunier, Alberto García-Durán, Jason Weston, and Oksana
        Yakhnenko. 2013. Translating Embeddings for Modeling
        Multi-relational Data. In <em>NIPS</em>. 2787–2795.</li>
        <li id="BibPLXBIB0005" label="[5]">Hongyun Cai,
        Vincent&nbsp;W. Zheng, and Kevin Chen-Chuan Chang. 2018. A
        Comprehensive Survey of Graph Embedding: Problems,
        Techniques and Applications. <em>TKDE</em> (2018).</li>
        <li id="BibPLXBIB0006" label="[6]">Shaosheng Cao, Wei Lu,
        and Qiongkai Xu. 2016. Deep Neural Networks for Learning
        Graph Representations. In <em>AAAI</em>. 1145–1152.</li>
        <li id="BibPLXBIB0007" label="[7]">Hanjun Dai, Bo Dai, and
        Le Song. 2016. Discriminative Embeddings of Latent Variable
        Models for Structured Data. In <em>ICML</em>.
        2702–2711.</li>
        <li id="BibPLXBIB0008" label="[8]">Yuxiao Dong,
        Nitesh&nbsp;V Chawla, and Ananthram Swami. 2017.
        metapath2vec: Scalable Representation Learning for
        Heterogeneous Networks. In <em>KDD</em>. 135–144.</li>
        <li id="BibPLXBIB0009" label="[9]">Mohammed Elseidy, Ehab
        Abdelhamid, Spiros Skiadopoulos, and Panos Kalnis. 2014.
        GRAMI: Frequent Subgraph and Pattern Mining in a Single
        Large Graph. <em>PVLDB</em> 7, 7 (2014), 517–528.</li>
        <li id="BibPLXBIB0010" label="[10]">Alessandro Epasto,
        Silvio Lattanzi, and Mauro Sozio. 2015. Efficient Densest
        Subgraph Computation in Evolving Graphs. In <em>WWW</em>.
        300–310.</li>
        <li id="BibPLXBIB0011" label="[11]">Yuan Fang, Wenqing Lin,
        Vincent&nbsp;W. Zheng, Min Wu, Kevin&nbsp;Chen-Chuan Chang,
        and Xiaoli Li. 2016. Semantic proximity search on graphs
        with metagraph-based learning. In <em>ICDE</em>.
        277–288.</li>
        <li id="BibPLXBIB0012" label="[12]">Aditya Grover and Jure
        Leskovec. 2016. node2vec: Scalable Feature Learning for
        Networks. In <em>KDD</em>.</li>
        <li id="BibPLXBIB0013" label="[13]">Pankaj Gupta, Venu
        Satuluri, Ajeet Grewal, Siva Gurumurthy, Volodymyr Zhabiuk,
        Quannan Li, and Jimmy&nbsp;J. Lin. 2014. Real-Time Twitter
        Recommendation: Online Motif Detection in Large Dynamic
        Graphs. <em>PVLDB</em> 7, 13 (2014), 1379–1380.</li>
        <li id="BibPLXBIB0014" label="[14]">William&nbsp;L.
        Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive
        Representation Learning on Large Graphs. In
        <em>NIPS</em>.</li>
        <li id="BibPLXBIB0015" label="[15]">Sepp Hochreiter and
        Jürgen Schmidhuber. 1997. Long Short-Term Memory.
        <em>Neural Comput.</em> 9, 8 (Nov. 1997), 1735–1780.</li>
        <li id="BibPLXBIB0016" label="[16]">Glen Jeh and Jennifer
        Widom. 2002. SimRank: A Measure of Structural-context
        Similarity. In <em>KDD</em>. 538–543.</li>
        <li id="BibPLXBIB0017" label="[17]">Glen Jeh and Jennifer
        Widom. 2003. Scaling Personalized Web Search. In
        <em>WWW</em>. 271–279.</li>
        <li id="BibPLXBIB0018" label="[18]">Ni Lao and
        William&nbsp;W. Cohen. 2010. Relational retrieval using a
        combination of path-constrained random walks. <em>Machine
        Learning</em> 81, 1 (2010), 53–67.</li>
        <li id="BibPLXBIB0019" label="[19]">Rui Li, Chi Wang, and
        Kevin&nbsp;Chen-Chuan Chang. 2014. User profiling in an ego
        network: co-profiling attributes and relationships. In
        <em>WWW</em>. 819–830.</li>
        <li id="BibPLXBIB0020" label="[20]">Zemin Liu,
        Vincent&nbsp;W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin
        Chen-Chuan Chang, Minghui Wu, and Jing Ying. 2017. Semantic
        Proximity Search on Heterogeneous Graph by Proximity
        Embedding. In <em>AAAI</em>.</li>
        <li id="BibPLXBIB0021" label="[21]">Zemin Liu,
        Vincent&nbsp;W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin
        Chen-Chuan Chang, Minghui Wu, and Jing Ying. 2018.
        Distance-aware DAG Embedding for Proximity Search on
        Heterogeneous Graphs. In <em>AAAI</em>.</li>
        <li id="BibPLXBIB0022" label="[22]">Julian&nbsp;J. McAuley
        and Jure Leskovec. 2012. Learning to Discover Social
        Circles in Ego Networks. In <em>NIPS</em>. 548–556.</li>
        <li id="BibPLXBIB0023" label="[23]">Feiping Nie, Wei Zhu,
        and Xuelong Li. 2017. Unsupervised Large Graph Embedding.
        In <em>AAAI</em>.</li>
        <li id="BibPLXBIB0024" label="[24]">Mathias Niepert,
        Mohamed Ahmed, and Konstantin Kutzkov. 2016. Learning
        Convolutional Neural Networks for Graphs. In <em>ICML</em>.
        2014–2023.</li>
        <li id="BibPLXBIB0025" label="[25]">Giannis Nikolentzos,
        Polykarpos Meladianos, and Michalis Vazirgiannis. 2017.
        Matching Node Embeddings for Graph Similarity. In
        <em>AAAI</em>.</li>
        <li id="BibPLXBIB0026" label="[26]">Mingdong Ou, Peng Cui,
        Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric
        Transitivity Preserving Graph Embedding. In <em>KDD</em>.
        1105–1114.</li>
        <li id="BibPLXBIB0027" label="[27]">Bryan Perozzi, Rami
        Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning
        of Social Representations. In <em>KDD</em>. 701–710.</li>
        <li id="BibPLXBIB0028" label="[28]">Leonardo&nbsp;F.R.
        Ribeiro, Pedro&nbsp;H.P. Saverese, and Daniel&nbsp;R.
        Figueiredo. 2017. Struc2Vec: Learning Node Representations
        from Structural Identity. In <em>KDD</em>. 385–394.</li>
        <li id="BibPLXBIB0029" label="[29]">Nino Shervashidze,
        Pascal Schweitzer, Erik&nbsp;Jan van Leeuwen, Kurt
        Mehlhorn, and Karsten&nbsp;M. Borgwardt. 2011.
        Weisfeiler-Lehman Graph Kernels. <em>Journal of Machine
        Learning Research</em> 12 (2011), 2539–2561.</li>
        <li id="BibPLXBIB0030" label="[30]">Yizhou Sun, Jiawei Han,
        Xifeng Yan, Philip&nbsp;S Yu, and Tianyi Wu. 2011. PathSim:
        Meta Path-Based Top-K Similarity Search in Heterogeneous
        Information Networks. <em>PVLDB</em> 4, 11 (2011).</li>
        <li id="BibPLXBIB0031" label="[31]">Zhao Sun, Hongzhi Wang,
        Haixun Wang, Bin Shao, and Jianzhong Li. 2012. Efficient
        Subgraph Matching on Billion Node Graphs. <em>PVLDB</em> 5,
        9 (2012), 788–799.</li>
        <li id="BibPLXBIB0032" label="[32]">Jian Tang, Meng Qu,
        Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015.
        LINE: Large-scale Information Network Embedding. In
        <em>WWW</em>. 1067–1077.</li>
        <li id="BibPLXBIB0033" label="[33]">Theano&nbsp;Development
        Team. 2016. Theano: A Python framework for fast computation
        of mathematical expressions. <em>CoRR</em> abs/1605.02688
        (may 2016).</li>
        <li id="BibPLXBIB0034" label="[34]">Cunchao Tu, Zhengyan
        Zhang, Zhiyuan Liu, and Maosong Sun. 2017. TransNet:
        Translation-Based Network Representation Learning for
        Social Relation Extraction. In <em>IJCAI</em>.
        2864–2870.</li>
        <li id="BibPLXBIB0035" label="[35]">Rogier J.&nbsp;P. van
        Berlo, Wynand Winterbach, Marco J.&nbsp;L. de Groot,
        Andreas Bender, Peter J.&nbsp;T. Verheijen, Marcel
        J.&nbsp;T. Reinders, and Dick de Ridder. 2013. Efficient
        calculation of compound similarity based on maximum common
        subgraphs and its application to prediction of gene
        transcript levels. <em>IJBRA</em> 9, 4 (2013),
        407–432.</li>
        <li id="BibPLXBIB0036" label="[36]">Chi Wang, Jiawei Han,
        Yuntao Jia, Jie Tang, Duo Zhang, Yintao Yu, and Jingyi Guo.
        2010. Mining advisor-advisee relationships from research
        publication networks. In <em>KDD</em>. ACM, 203–212.</li>
        <li id="BibPLXBIB0037" label="[37]">Chi Wang, Rajat Raina,
        David Fong, Ding Zhou, Jiawei Han, and Greg Badros. 2011.
        Learning Relevance from Heterogeneous Social Network and
        Its Application in Online Targeting. In <em>SIGIR</em>.
        655–664.</li>
        <li id="BibPLXBIB0038" label="[38]">Daixin Wang, Peng Cui,
        and Wenwu Zhu. 2016. Structural Deep Network Embedding. In
        <em>KDD</em>. 1225–1234.</li>
        <li id="BibPLXBIB0039" label="[39]">Zhen Wang, Jianwen
        Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph
        Embedding by Translating on Hyperplanes. In <em>AAAI</em>.
        1112–1119.</li>
        <li id="BibPLXBIB0040" label="[40]">Kelvin Xu, Jimmy Ba,
        Ryan Kiros, Kyunghyun Cho, Aaron&nbsp;C. Courville, Ruslan
        Salakhutdinov, Richard&nbsp;S. Zemel, and Yoshua Bengio.
        2015. Show, Attend and Tell: Neural Image Caption
        Generation with Visual Attention. In <em>ICML</em>.
        2048–2057.</li>
        <li id="BibPLXBIB0041" label="[41]">Muhan Zhang and Yixin
        Chen. 2017. Weisfeiler-Lehman Neural Machine for Link
        Prediction. In <em>KDD</em>. 575–583.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>Corresponding
    author.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.alibabacloud.com/forum/read-492">https://www.alibabacloud.com/forum/read-492</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://github.com/vwz/SPE">https://github.com/vwz/SPE</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186073">https://doi.org/10.1145/3178876.3186073</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
