<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Learning from Multi-View Multi-Way Data via Structural Factorization Machines</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>   <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186071"/>
</head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186071'>https://doi.org/10.1145/3178876.3186071</a>.
 Published in WWW2018 Proceedings Â© 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186071'>https://w3id.org/oa/10.1145/3178876.3186071</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Learning from Multi-View Multi-Way Data via Structural Factorization Machines</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Chun-Ta</span>     <span class="surName">Lu</span>,     University of Illinois at Chicago, <a href="mailto:clu29@uic.edu">clu29@uic.edu</a>    </div>    <div class="author">     <span class="givenName">Lifang</span>     <span class="surName">He<a class="fn" href="#fn1" id="foot-fn1"><sup>&#x204E;</sup></a></span>,     Cornell University, <a href="mailto:lifanghescut@gmail.com">lifanghescut@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Hao</span>     <span class="surName">Ding</span>,     Purdue University, <a href="mailto:haoding.tourist@gmail.com">haoding.tourist@gmail.com</a>    </div>    <div class="author">     <span class="givenName">Bokai</span>     <span class="surName">Cao</span>,     University of Illinois at Chicago, <a href="mailto:caobokai@uic.edu">caobokai@uic.edu</a>    </div>    <div class="author">     <span class="givenName">Philip S.</span>     <span class="surName">Yu</span>,     University of Illinois at Chicago, <a href="mailto:psyu@cs.uic.edu">psyu@cs.uic.edu</a>    </div>                        <Affiliation id="aff6">Tsinghua University </Affiliation>    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186071" target="_blank">https://doi.org/10.1145/3178876.3186071</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Real-world relations among entities can often be observed and determined by different perspectives/views. For example, the decision made by a user on whether to adopt an item relies on multiple aspects such as the contextual information of the decision, the item&#x0027;s attributes, the user&#x0027;s profile and the reviews given by other users. Different views may exhibit multi-way interactions among entities and provide complementary information. In this paper, we introduce a multi-tensor-based approach that can preserve the underlying structure of multi-view data in a generic predictive model. Specifically, we propose structural factorization machines (SFMs) that learn the common latent spaces shared by multi-view tensors and automatically adjust the importance of each view in the predictive model. Furthermore, the complexity of SFMs is linear in the number of parameters, which make SFMs suitable to large-scale problems. Extensive experiments on real-world datasets demonstrate that the proposed SFMs outperform several state-of-the-art methods in terms of prediction accuracy and computational cost.</small>    </p>    </div>    <div class="CCSconcepts">    <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Machine learning;</strong> <strong>Factorization methods;</strong> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Data mining;</strong></small> </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Tensor Factorization; Multi-Way Interaction; Multi-View Learning</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Chun-Ta Lu, Lifang He, Hao Ding, Bokai Cao, and Philip S. Yu. 2018. Learning from Multi-View Multi-Way Data via Structural Factorization Machines. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 10 Pages. <a href="https://doi.org/10.1145/3178876.3186071" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186071</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>With the ability to access massive amounts of heterogeneous data from multiple sources, multi-view data have become prevalent in many real-world applications. For instance, in recommender systems, online review sites (like Amazon and Yelp) have access to contextual information of shopping histories of users, the reviews written by the users, the categorizations of the items, as well as the friends of the users. Each view may exhibit pairwise interactions (e.g., the friendships between users) or even higher-order interactions (e.g., a customer write a review for a product) among entities (such as customers, products, and reviews), and can be represented in a multi-way data structure, i.e., tensor. Since different views usually provide complementary information&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>], how to effectively incorporate information from multiple structural views is critical to good prediction performance for various machine learning tasks.</p>    <p>Typically, a predictive model is defined as a function of predictor variables (e.g., the customer id, the product id, and the categories of the product) to some target (e.g., the rating). The most common approach in predictive modeling for multi-view multi-way data is to describe samples with feature vectors that are flattened and concatenated from structural views, and apply a classical vector-based method, such as linear regression (LR) and support vector machines (SVMs), to learn the target function from observed samples. Recent works have shown that linear models fail for tasks with very sparse data&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>]. A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernels, such as the ANOVA kernels used in FMs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] and polynominal kernels used in polynominal networks&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>]. However, the disadvantages of this approach are that (1) the important structural information of each view will be discarded which may lead to the degraded prediction performance and (2) the feature vectors can grow very large which can make learning and prediction very slow or even infeasible, especially if each view involves relations of high cardinality. For example, including the relation &#x201C;friends of a user&#x201D; in the feature vector (represented by their IDs) can result in a very long feature vector. Further, it will repeatedly appear in many samples that involve the given user.</p>    <p>Matrix/tensor factorization models have been a topic of interest in the areas of multi-way data analysis, e.g., community detection&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>], collaborative filtering&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], knowledge graph completion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>], and neuroimage analysis&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. Assuming multi-view data have the same underlying low-rank structure (at least in one mode), coupled data analysis such as collective matrix factorization (CMF)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] and coupled matrix and tensor factorization (CMTF)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>] that jointly factorize multiple matrices (or tensors) has been applied to applications such as clustering and missing data recovery. However, they are only applicable to categorical variables. Moreover, since existing coupled factorization models are unsupervised, the importance of each structural view in modeling the target value cannot be automatically learned. Furthermore, when applying these models to data with rich meta information (e.g., friendships) but extremely sparse target values (e.g., ratings), it is very likely the learning process will be dominated by the meta information without manual tuning some hyperparameters, e.g., the weights of the fitting error of each matrix/tensor in the objective function&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], the weights of different types of latent factors in the predictive models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>], or the regularization hyperparamters of latent factor alignment&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>].</p>    <p>In this paper, we propose a general and flexible framework for learning the predictive structure from the complex relationships within the multi-view multi-way data. Each view of an instance in this framework is represented by a tensor that describes the multi-way interactions of subsets of entities, and different views have some entities in common. Constructing the tensors for each instance may not be realistic for real-world applications in terms of space and computational complexity, and the model parameters can have exponential growth and tend to be overfitting. In order to preserve the structural information of multi-view data without physically constructing the tensors, we introduce structural factorization machines (SFMs) that can learn the consistent representations in the latent feature spaces shared in the multi-view tensors while automatically adjust the contribution of each view in the predictive model. Furthermore, we provide an efficient method to avoid redundant computing on repeating patterns stemming from the relational structure of the data, such that SFMs can make the same predictions but with largely speed up computation.</p>    <p>The contributions of this paper are summarized as follows:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">We introduce a novel multi-tensor framework for mining data from heterogeneous domains, which can explore the high order correlations underlying multi-view multi-way data in a generic predictive model.<br/></li>    <li id="list2" label="&#x2022;">We develop structural factorization machines (SFMs) tailored for learning the common latent spaces shared in multi-view tensors and automatically adjusting the importance of each view in the predictive model. The complexity of SFMs is linear in the number of features, which makes SFMs suitable to large-scale problems.<br/></li>    <li id="list3" label="&#x2022;">Extensive experiments on eight real-world datasets are performed along with comparisons to existing state-of-the-art factorization models to demonstrate its advantages.<br/></li>    </ul>    <p>The rest of this paper is organized as follows. In Section&#x00A0;<a class="sec" href="#sec-8">2</a>, we briefly review related work on factorization models and multi-view learning. We introduce the preliminary concepts and problem definition in Section&#x00A0;<a class="sec" href="#sec-9">3</a>. We then propose the framework for learning multi-view multi-way data, and develop the structural factorization machines (SFMs), and provide an efficient computing method in Section&#x00A0;<a class="sec" href="#sec-12">4</a>. The experimental results and parameter analysis are reported in Section&#x00A0;<a class="sec" href="#sec-16">5</a>. Section&#x00A0;<a class="sec" href="#sec-24">6</a> concludes this paper.</p>   </section>   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>    <strong>Feature Interactions.</strong> Rendle pioneered the concept of feature interactions in Factorization Machines (FM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>]. Juan et al. presented Field-aware Factorization Machines (FFM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] to allow each feature to interact differently with another feature depending on its field. Novikov et al. proposed Exponential Machines (ExM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>] where the weight tensor is represented in a factorized format called Tensor Train. Zhang et al. used FM to initialize the embedding layer in a deep model [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>]. Qu et al. added a product layer on the top of the embedding layer to increase the model capacity [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. Other extensions of FM to deep architectures include Neural Factorization Machines (NFM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] and Attentional Factorization Machines (AFM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>]. In order to effectively model feature interactions, a variety of models has been developed in the industry as well. Microsoft studied feature interactions in deep models, including Deep Semantic Similarity Model (DSSM) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>], Deep Crossing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] and Deep Embedding Forest [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>]. They use features as raw as possible without manually crafted combinatorial features, and let deep neural networks take care of the rest. Alibaba proposed a Deep Interest Network (DIN) [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>] to learn user embeddings as a function of ad embeddings. Google used deep neural networks to learn from heterogeneous signals for YouTube recommendations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>]. In addition, Wide &#x0026; Deep Models [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>] were developed for app recommender systems in Google Play where the wide component includes cross features that are good at memorization and the deep component includes embedding layers for generalization. Guo et al. proposed to use FM as the wide component in Wide &#x0026; Deep with shared embeddings in the deep component [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>]. Wang et al. developed the Deep &#x0026; Cross Network (DCN) to learn explicit cross features of bounded degree [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>].</p>    <p>    <strong>Multi-View Learning.</strong> Multi-view learning (MVL) is concerned with predicting unknown values by taking multiple views into account. The traditional MVL refers to using relational features to construct a set of disjoint views, and these uncorrelated views are then used to model a target function to approximate the target concept to be learned&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. There are currently a plethora of studies available for MVL. Interested readers are referred to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] for a comprehensive survey of these techniques and applications. The most related works to ours are&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>] that introduced and explored the tensor product operator to integrate different views together in a tensor. Lu et al. further studied the multi-view feature interactions in the context of multi-task learning [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>]. However, this approach will introduce unexpected noise from the irrelevant feature interactions that can even be exaggerated after combinations, thereby degrading performance as demonstrated in the experiments. Different from conventional MVL approaches, the proposed algorithm can learn the common latent spaces shared in multi-view tensors and automatically adjusting the importance of each view in the predictive model.</p>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> Preliminaries</h2>    </div>    </header>    <p>In this section, we begin with a brief introduction to some related concepts and notation in tensor algebra, and then proceed to formulate the problem we are concerned with multi-view learning. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Example of multiple structural views, where <span class="inline-equation"><span class="tex">$\tilde{\mathcal {X}}^{(1)}= \tilde{\mathbf {x}}^{(1)}\circ \tilde{\mathbf {x}}^{(2)} \circ \tilde{\mathbf {x}}^{(3)}$</span>       </span> and <span class="inline-equation"><span class="tex">$\tilde{\mathbf {X}}^{(2)}= \tilde{\mathbf {x}}^{(3)}\circ \tilde{\mathbf {x}}^{(4)}$</span>       </span>.</span>     </div>    </figure>    </p>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Tensor Basics and Notation</h3>     </div>    </header>    <p>Tensor is a mathematical representation of a multi-way array. The order of a tensor is the number of modes (or ways). A zero-order tensor is a scalar, a first-order tensor is a vector, a second-order tensor is a matrix and a tensor of order three or higher is called a higher-order tensor. An element of a vector <strong>x</strong>, a matrix <strong>X</strong>, or a tensor <span class="inline-equation"><span class="tex">$\mathcal {X}$</span>     </span> is denoted by <em>x<sub>i</sub>     </em>, <em>x</em>     <sub>      <em>i</em>, <em>j</em>     </sub>, <em>x</em>     <sub>      <em>i</em>, <em>j</em>, <em>k</em>     </sub>, etc., depending on the number of modes. All vectors are column vectors unless otherwise specified. For an arbitrary matrix <span class="inline-equation"><span class="tex">$\mathbf {X} \in \mathbb {R}^{I \times J}$</span>     </span>, its <em>i</em>-th row and <em>j</em>-th column vector are denoted by <strong>x</strong>     <sup>      <em>i</em>     </sup> and <strong>x</strong>     <sub>      <em>j</em>     </sub>, respectively. Given two matrices <span class="inline-equation"><span class="tex">$\mathbf {X}, \mathbf {Y} \in \mathbb {R}^{I \times J}$</span>     </span>, <strong>X</strong>*<strong>Y</strong> denotes the element-wise (Hadamard) product between <strong>X</strong> and <strong>Y</strong>, defined as the matrix in <span class="inline-equation"><span class="tex">$\mathbb {R}^{I \times J}$</span>     </span>. An overview of the basic symbols used in this paper can be found in Table&#x00A0;<a class="tbl" href="#tab1">1</a>.</p>    <div class="definition" id="enc1">     <Label>Definition 3.1 (Inner product).</Label>     <p> The inner product of two same-sized tensors <span class="inline-equation"><span class="tex">$\mathcal {X}, \mathcal {Y} \in \mathbb {R}^{I_{1} \times I_{2} \times \cdots \times I_{M}}$</span>      </span> is defined as the sum of the products of their entries: <div class="table-responsive" id="eq1">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \left\langle \mathcal {X}, \mathcal {Y}\right\rangle =\sum _{i_{1}=1}^{I_1}\sum _{i_{2}=1}^{I_2}\cdots \sum _{i_{M}=1}^{I_M} x_{i_1,i_2,\ldots ,i_M} y_{i_1,i_2,\ldots ,i_M}. \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>       </div>      </div>     </p>    </div>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">List of basic symbols.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Symbol</th>       <th style="text-align:left;">Definition and description</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">        <em>x</em>       </td>       <td style="text-align:left;">each lowercase letter represents a scalar</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>x</strong>       </td>       <td style="text-align:left;">each boldface lowercase letter represents a vector</td>       </tr>       <tr>       <td style="text-align:left;">        <strong>X</strong>       </td>       <td style="text-align:left;">each boldface uppercase letter represents a matrix</td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\mathcal {X}$</span>        </span>       </td>       <td style="text-align:left;">each calligraphic letter represents a tensor</td>       </tr>       <tr>       <td style="text-align:left;">        <span class="inline-equation"><span class="tex">$\mathfrak {X}$</span>        </span>       </td>       <td style="text-align:left;">each gothic letter represent a general set or space</td>       </tr>       <tr>       <td style="text-align:left;">[1: <em>N</em>]</td>       <td style="text-align:left;">a set of integers in the range of 1 to <em>N</em> inclusively.</td>       </tr>       <tr>       <td style="text-align:left;">&#x27E8; &#x00B7;, &#x00B7;&#x27E9;</td>       <td style="text-align:left;">denotes inner product</td>       </tr>       <tr>       <td style="text-align:left;">&#x25CB;</td>       <td style="text-align:left;">denotes tensor product (outer product)</td>       </tr>       <tr>       <td style="text-align:left;">*</td>       <td style="text-align:left;">denotes Hadamard (element-wise) product</td>       </tr>      </tbody>     </table>    </div>    <div class="definition" id="enc2">     <Label>Definition 3.2 (Outer product).</Label>     <p> The outer product of two tensors <span class="inline-equation"><span class="tex">$\mathcal {X} \in \mathbb {R}^{I_{1} \times I_{2} \times \cdots \times I_{N}}$</span>      </span> and <span class="inline-equation"><span class="tex">$\mathcal {Y}\in \mathbb {R}^{I_{1}^{\prime } \times I_{2}^{\prime } \times \cdots \times I_{M}^{\prime }}$</span>      </span> is a (<em>N</em> + <em>M</em>)th-order tensor denoted by <span class="inline-equation"><span class="tex">$\mathcal {X} \circ \mathcal {Y}$</span>      </span>, and the elements are defined by <div class="table-responsive" id="eq2">       <div class="display-equation">       <span class="tex mytex">\begin{equation} \left(\mathcal {X} \circ \mathcal {Y}\right)_{i_1,i_2,\ldots ,i_N, i_1^{\prime },i_2^{\prime },\ldots ,i_M^{\prime }}\ =\ x_{i_1,i_2,\cdots ,i_N} y_{i_1^{\prime },i_2^{\prime },\cdots ,i_M^{\prime }} \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>       </div>      </div> for all values of the indices.</p>    </div>    <p>Notice that for rank-one tensors <span class="inline-equation"><span class="tex">$\mathcal {X}=\mathbf {x}^{(1)} \circ \mathbf {x}^{(2)} \circ \cdots \circ \mathbf {x}^{(M)}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathcal {Y}=\mathbf {y}^{(1)} \circ \mathbf {y}^{(2)} \circ \cdots \circ \mathbf {y}^{(M)}$</span>     </span>, it holds that <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \left\langle \mathcal {X}, \mathcal {Y}\right\rangle =\left\langle \mathbf {x}^{(1)}, \mathbf {y}^{(1)}\right\rangle \left\langle \mathbf {x}^{(2)}, \mathbf {y}^{(2)}\right\rangle \cdots \left\langle \mathbf {x}^{(M)}, \mathbf {y}^{(M)}\right\rangle . \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div>    </p>    <div class="definition" id="enc3">     <Label>Definition 3.3 (CP factorization&#x00A0;[22]).</Label>     <p> Given a tensor <span class="inline-equation"><span class="tex">$\mathcal {X} \in \mathbb {R}^{I_{1} \times I_{2} \times \cdots \times I_{M}}$</span>      </span> and an integer <em>R</em>, the CP factorization is defined by factor matrices <span class="inline-equation"><span class="tex">$\mathbf {X}^{(m)} \in \mathbb {R}^{I_m \times R}$</span>      </span> for <em>m</em> &#x2208; [1: <em>M</em>], respectively, such that <div class="table-responsive" id="eq4">       <div class="display-equation">       <span class="tex mytex">\begin{align} \mathcal {X} = \sum _{r=1}^{R} \mathbf {x}_{r}^{(1)} \circ \mathbf {x}_{r}^{(2)} \circ \cdots \circ \mathbf {x}_{r}^{(M)} = [[ \mathbf {X}^{(1)}, \mathbf {X}^{(2)}, \cdots , \mathbf {X}^{(M)} ]] ~, \end{align} </span>       <br/>       <span class="equation-number">(4)</span>       </div>      </div> where <span class="inline-equation"><span class="tex">$\mathbf {x}_{r}^{(m)} \in \mathbb {R}^{I_m}$</span>      </span> is the <em>r</em>-th column of the factor matrix <strong>X</strong>      <sup>(<em>m</em>)</sup>, and <span class="inline-equation"><span class="tex">$[[ \cdot ]]$</span>      </span> is used for shorthand notation of the sum of rank-one tensors.</p>    </div>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Problem Formulation</h3>     </div>    </header>    <p>Our problem is different from conventional multi-view learning approaches where multiple views of data are assumed independent and disjoint, and each view is described by a vector. We formulate the multi-view learning problem using coupled analysis of multi-view features in the form of multiple tensors.</p>    <p>Suppose that the problem includes <em>V</em> views where each view consists of a collection of subsets of entities (such as person, company, location, product) and different views have some entities in common. We denote a view as a tuple (<strong>x</strong>     <sup>(1)</sup>, <strong>x</strong>     <sup>(2)</sup>, &#x22C5;&#x22C5;&#x22C5;, <strong>x</strong>     <sup>(<em>M</em>)</sup>), <em>M</em> &#x2265; 2, where <span class="inline-equation"><span class="tex">$\mathbf {x}^{(m)} \in \mathbb {R}^{I_m}$</span>     </span> is a feature vector associated with the entity <em>m</em>. Inspired by [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>], we construct tensor representation for each view over its entities by <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \tilde{\mathcal {X}} = \tilde{\mathbf {x}}^{(1)} \circ \tilde{\mathbf {x}}^{(2)} \circ \cdots \circ \tilde{\mathbf {x}}^{(M)} \in \mathbb {R}^{(1+I_1) \times \cdots \times (1+I_M)}, \] </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$\tilde{\mathbf {x}}^{(m)} = [1; \mathbf {x}^{(m)}] \in \mathbb {R}^{1+ I_m}$</span>     </span> and &#x25CB; is the outer product operator. In this manner, the full-order interactions <a class="fn" href="#fn2" id="foot-fn2"><sup>1</sup></a> between entities are embedded within the tensor structure, which not only provides a unified and compact representation for each view, but also facilitate efficient design methods. Fig.&#x00A0;<a class="fig" href="#fig1">1</a> shows an example of two structural views, where the first view consists of the full-order interactions among the first three modes (e.g., review text, item ID, and user ID), and the second view consists of the full-order interactions among the last two modes (e.g., user ID and friend IDs).</p>    <p>After generating the tensor representation for each view, we define the multi-view learning problem as follows. Given a training set <span class="inline-equation"><span class="tex">$\mathfrak {D} = \big \lbrace \big (\big \lbrace \tilde{\mathcal {X}}^{(1)}_{n}, \tilde{\mathcal {X}}^{(2)}_{n}, \cdots , \tilde{\mathcal {X}}^{(V)}_{n} \big \rbrace , ~ y_{n} \big)~ |~ n \in [1 : N] \big \rbrace$</span>     </span>, where <span class="inline-equation"><span class="tex">$\tilde{\mathcal {X}}^{(v)}_{n} \in \mathbb {R}^{(1+I_1) \times \cdots \times (1+I_{M_v})}$</span>     </span> is the tensor representation in the <em>v</em>-th view for the <em>n</em>-th instance, <em>y<sub>n</sub>     </em> is the response of the <em>n</em>-th instance, <em>M<sub>v</sub>     </em> is the number of the constitutive modes in the <em>v</em>-th view, and <em>N</em> is the number of labeled instances. We assume different views have common entities, thus the resulting tensors will share common modes, e.g., the third mode in Fig&#x00A0;<a class="fig" href="#fig1">1</a>. As we are concerned with predicting unknown values of multiple coupled tensors, our goal is to leverage the relational information from all the views to help predict the unlabeled instances, as well as to use the complementary information among different views to improve the performance. Specifically, we are interested in finding a predictive function <span class="inline-equation"><span class="tex">$f : \mathfrak {X}^{(1)} \times \mathfrak {X}^{(2)} \cdots \times \mathfrak {X}^{(V)} \rightarrow \mathfrak {Y}$</span>     </span> that minimizes the expected loss, where <span class="inline-equation"><span class="tex">$\mathfrak {X}^{(v)}, v \in [1 : V]$</span>     </span> is the input space in the <em>v</em>-th view and <span class="inline-equation"><span class="tex">$\mathfrak {Y}$</span>     </span> is the output space.</p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> Methodology</h2>    </div>    </header>    <p>In this section, we first discuss how to design the predictive models for learning from multiple coupled tensors. We then derive structural factorization machines (SFMs) that can learn the common latent spaces shared in multi-view coupled tensors and automatically adjust the importance of each view in the predictive model.</p>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Predictive Models</h3>     </div>    </header>    <p>Without loss of generality, we take two views as an example to introduce our basic design of the predictive models. Specifically, we consider coupled analysis of a third-order tensor and a matrix with one mode in common, as shown in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>. Given an input instance <span class="inline-equation"><span class="tex">$\big (\big \lbrace \tilde{\mathcal {X}}^{(1)}, \tilde{\mathbf {X}}^{(2)} \big \rbrace , ~y \big)$</span>     </span>, where <span class="inline-equation"><span class="tex">$\tilde{\mathcal {X}}^{(1)} = \tilde{\mathbf {x}}^{(1)} \circ \tilde{\mathbf {x}}^{(2)} \circ \tilde{\mathbf {x}}^{(3)} \in \mathbb {R}^{(1+I) \times (1+J) \times (1+K)}$</span>     </span> and <span class="inline-equation"><span class="tex">$\tilde{\mathbf {X}}^{(2)} = \tilde{\mathbf {x}}^{(3)} \circ \tilde{\mathbf {x}}^{(4)} \in \mathbb {R}^{(1+K) \times (1+L)}$</span>     </span>. An intuitive solution is to build the following multiple linear model: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} f \left(\left\lbrace \tilde{\mathcal {X}}^{(1)}, \tilde{\mathbf {X}}^{(2)} \right\rbrace \right) = \left\langle \tilde{\mathcal {W}}^{(1)}, \tilde{\mathcal {X}}^{(1)} \right\rangle + \left\langle \tilde{\mathbf {W}}^{(2)}, \tilde{\mathbf {X}}^{(2)} \right\rangle \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\tilde{\mathcal {W}}^{(1)} \in \mathbb {R}^{(1+I) \times (1+J) \times (1+K)}$</span>     </span> and <span class="inline-equation"><span class="tex">$\tilde{\mathbf {W}}^{(2)} \in \mathbb {R}^{ (1+K) \times (1+L)}$</span>     </span> are the weights for each view to be learned. <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Example of the computational graph in a structural factorization machine, given the input <span class="inline-equation"><span class="tex">$\tilde{\mathcal {X}}^{(1)}$</span>       </span> and <span class="inline-equation"><span class="tex">$\tilde{\mathbf {X}}^{(2)}$</span>       </span>. By jointly factorizing weight tensors, the <strong>h</strong>       <sup>(<em>m</em>)</sup> can be regarded as the latent representation of the feature <strong>x</strong>       <sup>(<em>m</em>)</sup> in <em>m</em>-th mode, and &#x03C0;<sup>(<em>v</em>)</sup> can be regarded as the joint representation of all the modes in the <em>v</em>-th view, which can be easily computed through the Hadamard product. The contribution of &#x03C0;<sup>(<em>v</em>)</sup> to the final prediction score is automatically adjusted by the weight vector &#x03D5;<sup>        <em>v</em>       </sup>.</span>      </div>     </figure>    </p>    <p>However, in this case it does not take into account the relations and differences between two views. In order to incorporate the relations between two views and also discriminate the importance of each view, we introduce an indicator vector <span class="inline-equation"><span class="tex">$\mathbf {e}_v \in \mathbb {R}^{V}$</span>     </span> for each view <em>v</em> as <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\[ \mathbf {e}_v = [ \underbrace{0, \cdots , 0}_\text{v-1}, 1, 0, \cdots , 0 ]^\mathrm{T}, \] </span>       <br/>      </div>     </div> and transform the predictive model in Eq.&#x00A0;(<a class="eqn" href="#eq5">5</a>) into <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} f \left(\left\lbrace \tilde{\mathcal {X}}^{(1)}, \tilde{\mathbf {X}}^{(2)} \right\rbrace \right) = \left\langle \hat{\mathcal {W}}^{(1)}, \tilde{\mathcal {X}}^{(1)} \circ \mathbf {e}_1 \right\rangle + \left\langle \hat{\mathcal {W}}^{(2)}, \tilde{\mathbf {X}}^{(2)} \circ \mathbf {e}_2 \right\rangle , \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\hat{\mathcal {W}}^{(1)} \in \mathbb {R}^{(1+I) \times (1+J) \times (1+K) \times 2}$</span>     </span> and <span class="inline-equation"><span class="tex">$\hat{\mathcal {W}}^{(2)} \in \mathbb {R}^{(1+K) \times (1+L) \times 2}$</span>     </span>.</p>    <p>Directly learning the weight tensors <span class="inline-equation"><span class="tex">$\hat{\mathcal {W}}$</span>     </span>s leads to two drawbacks. First, the weight parameters are learned independently for different modes and different views. When the feature interactions rarely (or even never) appear during training, it is unlikely to learn the associated parameters appropriately. Second, the number of parameters in Eq.&#x00A0;(<a class="eqn" href="#eq6">6</a>) is exponential to the number of features, which can make the model prone to overfitting and ineffective on sparse data. Here, we assume that each weight tensor has a low-rank approximation, and <span class="inline-equation"><span class="tex">$\hat{\mathcal {W}}^{(1)}$</span>     </span> and <span class="inline-equation"><span class="tex">$\hat{\mathcal {W}}^{(2)}$</span>     </span> can be decomposed by CP factorization as <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \hat{\mathcal {W}}^{(1)} &#x0026;= [[ \hat{\mathbf {\Theta }}^{(1,1)}, \hat{\mathbf {\Theta }}^{(1, 2)}, \hat{\mathbf {\Theta }}^{(1,3)}, \mathbf {\Phi } ]] \nonumber \\ &#x0026;= [[ [\mathbf {b}^{(1,1)}; \mathbf {\Theta }^{(1)}], [\mathbf {b}^{(1,2)}; \mathbf {\Theta }^{(2)}], [\mathbf {b}^{(1,3)}; \mathbf {\Theta }^{(3)}], \mathbf {\Phi } ]] ,\end{align*} </span>       <br/>      </div>     </div> and <div class="table-responsive">      <div class="display-equation">       <span class="tex mytex">\begin{align*} \hat{\mathcal {W}}^{(2)} = [[ \hat{\mathbf {\Theta }}^{(2, 3)}, \hat{\mathbf {\Theta }}^{(2, 4)}, \mathbf {\Phi } ]] = [[ [\mathbf {b}^{(2,3)}; \mathbf {\Theta }^{(3)}], [\mathbf {b}^{(2,4)}; \mathbf {\Theta }^{(4)}], \mathbf {\Phi } ]] ,\end{align*} </span>       <br/>      </div>     </div> where <span class="inline-equation"><span class="tex">$ {\Theta }^{(m)} \in \mathbb {R}^{I_m \times R}$</span>     </span> is the factor matrix for the features in the <em>m</em>-th mode. It is worth noting that <strong>      <em>&#x0398;</em>     </strong>     <sup>(3)</sup> is shared in the two views. <span class="inline-equation"><span class="tex">$ {\Phi } \in \mathbb {R}^{2 \times R}$</span>     </span> is the factor matrix for the view indicator, and <span class="inline-equation"><span class="tex">$\mathbf {b}^{(v,m)} \in \mathbb {R}^{1 \times R}$</span>     </span>, which is always associated with the constant one in <span class="inline-equation"><span class="tex">$\tilde{\mathbf {x}}^{(m)} = [1;\mathbf {x}^{(m)}]$</span>     </span>, represents the bias factors of the <em>m</em>-th mode in the <em>v</em>-th view. Through <strong>b</strong>     <sup>(<em>v</em>, <em>m</em>)</sup>, the lower-order interactions (the interactions excluding the features from the <em>m</em>-th mode) in the <em>v</em>-th view are explored in the predictive function.</p>    <p>Then we can transform Eq.&#x00A0;(<a class="eqn" href="#eq6">6</a>) into <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation}\displaystyle \begin{aligned} &#x0026;\left\langle \hat{\mathcal {W}}^{(1)}, \tilde{\mathcal {X}}^{(1)} \circ \mathbf {e}_1 \right\rangle + \left\langle \hat{\mathcal {W}}^{(2)}, \tilde{\mathbf {X}}^{(2)} \circ \mathbf {e}_2 \right\rangle \\ = &#x0026; \sum _{r=1}^{R} \left\langle \hat{ {\theta }}_r^{(1,1)} \circ \hat{ {\theta }}_r^{(1,2)} \circ \hat{ {\theta }}_r^{(1,3)} \circ {\phi }_{r} ~,~ \tilde{\mathbf {x}}^{(1)} \circ \tilde{\mathbf {x}}^{(2)} \circ \tilde{\mathbf {x}}^{(3)} \circ \mathbf {e}_1 \right\rangle \\ &#x0026; + \sum _{r=1}^{R} \left\langle \hat{ {\theta }}_r^{(2,3)} \circ \hat{ {\theta }}_r^{(2,4)} \circ {\phi }_{r} ~,~ \tilde{\mathbf {x}}^{(3)} \circ \tilde{\mathbf {x}}^{(4)} \circ \mathbf {e}_2 \right\rangle \\ = &#x0026; {\phi }^1 \left(\prod _{m=1}^{3} \ast \left(\tilde{\mathbf {x}}^{(m)^\mathrm{T}} \hat{ {\Theta }}^{(1,m)} \right) \right)^\mathrm{T} + {\phi }^2 \left(\prod _{m=3}^{4} \ast \left(\tilde{\mathbf {x}}^{(m)^\mathrm{T}} \hat{ {\Theta }}^{(2,m)} \right) \right)^\mathrm{T} \\ = &#x0026; {\phi }^1 \left(\prod _{m=1}^{3} \ast \left(\mathbf {x}^{(m)^\mathrm{T}} {\Theta }^{(m)} + \mathbf {b}^{(1,m)} \right) \right)^\mathrm{T} + {\phi }^2 \left(\prod _{m=3}^{4} \ast \left(\mathbf {x}^{(m)^\mathrm{T}} {\Theta }^{(m)} + \mathbf {b}^{(2,m)} \right) \right)^\mathrm{T} \end{aligned}\end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div>    </p>    <p>where * is the Hadamard (elementwise) product and <span class="inline-equation"><span class="tex">$ {\phi }^v \in \mathbb {R}^{1 \times R}$</span>     </span> is the <em>v</em>-th row of the factor matrix &#x03A6;.</p>    <p>For convenience, we let <span class="inline-equation"><span class="tex">$\mathbf {h}^{(m)} = {\Theta }^{(m)^\mathrm{T}}\mathbf {x}^{(m)}$</span>     </span>, <em>S<sub>M</sub>     </em>(<em>v</em>) denote the set of modes in the <em>v</em>-th views, <span class="inline-equation"><span class="tex">$ {\pi }^{(v)} = \prod \limits _{m \in S_M(v)} \ast \left(\mathbf {h}^{(m)} + \mathbf {b}^{(v,m)^\mathrm{T}} \right)$</span>     </span>, and <span class="inline-equation"><span class="tex">$ {\pi }^{(v,-m)} = \prod \limits _{m^{\prime } \in S_M(v), m^{\prime } \ne m}$</span>     </span>     <span class="inline-equation"><span class="tex">$\ast \left(\mathbf {h}^{(m^{\prime })} + \mathbf {b}^{(v,m^{\prime })^\mathrm{T}} \right)$</span>     </span>. The predictive model for the general cases is given as follows <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation}\displaystyle \begin{aligned} f(\lbrace \tilde{\mathcal {X}}^{(v)}\rbrace) &#x0026;= \sum _{v=1}^{V} \left\langle \hat{\mathcal {W}}^{(v)},\tilde{\mathcal {X}}^{(v)} \circ \mathbf {e}_v \right\rangle \\ &#x0026; = \sum _{v=1}^{V} {\phi }^v \prod _{m \in S_M(v)} \ast \left(\mathbf {x}^{(m)^\mathrm{T}} {\Theta }^{(m)} + \mathbf {b}^{(v,m)} \right)^\mathrm{T} \\ &#x0026; = \sum _{v=1}^{V} {\phi }^v \prod _{m \in S_M(v)} \ast \left(\mathbf {h}^{(m)} + \mathbf {b}^{(v,m)^\mathrm{T}} \right) \end{aligned}\end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>    </p>    <p>A graphical illustration of the proposed model is shown in Fig.&#x00A0;<a class="fig" href="#fig2">2</a>. We name this model as structural factorization machines (SFMs). Clearly, the parameters are jointly factorized, which benefits parameter estimation under sparsity since dependencies exist when the interactions share the same features. Therefore, the model parameters can be effectively learned without direct observations of such interactions especially in highly sparse data. More importantly, after factorizing the weight tensor <span class="inline-equation"><span class="tex">$\hat{\mathcal {W}}$</span>     </span>s, there is no need to construct the input tensor physically. Furthermore, the model complexity is linear in the number of original features. In particular, the model complexity is <em>O</em>(<em>R</em>(<em>V</em> + <em>I</em> + &#x2211;<sub>      <em>v</em>     </sub>     <em>M<sub>v</sub>     </em>)), where <em>M<sub>v</sub>     </em> is the number of modes in the <em>v</em>-th view.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Learning Structural Factorization Machines</h3>     </div>    </header>    <p>Following the traditional supervised learning framework, we propose to learn the model parameters by minimizing the following regularized empirical risk: <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \mathcal {R}= \frac{1}{N} \sum _{n=1}^{N} \ell \left(f(\lbrace \mathcal {X}_{n}^{(v)}\rbrace), y_{n} \right) + \lambda \Omega (\mathbf {\Phi },\lbrace \mathbf {\Theta }^{(m)}\rbrace , \lbrace \mathbf {b}^{(v,m)}\rbrace) \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> where &#x2113; is a prescribed loss function, <em>&#x03A9;</em> is the regularizer encoding the prior knowledge of {<strong>      <em>&#x0398;</em>     </strong>     <sup>(<em>m</em>)</sup>} and <strong>      <em>&#x03A6;</em>     </strong>, and <em>&#x03BB;</em> &#x2265; 0 is the regularization parameter that controls the trade-off between the empirical loss and the prior knowledge.</p>    <p>The partial derivative of <span class="inline-equation"><span class="tex">$\mathcal {R}$</span>     </span> w.r.t. <strong>      <em>&#x0398;</em>     </strong>     <sup>(<em>m</em>)</sup> is given by <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{align} \frac{\partial \mathcal {R}}{\partial \mathbf {\Theta }^{(m)}} = \frac{\partial \mathcal {L}}{\partial f} \frac{\partial f}{\partial \mathbf {\Theta }^{(m)}} + \lambda \frac{\partial \Omega _{\lambda }(\mathbf {\Theta }^{(m)}) }{\partial \mathbf {\Theta }^{(m)}} \end{align} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> where <span class="inline-equation"><span class="tex">$\frac{\partial \mathcal {L}}{\partial f} = \frac{1}{N} \left[\begin{array}{c}\frac{\partial \ell _{1} }{\partial f}, \cdots , \frac{\partial \ell _{N} }{\partial f} \end{array}\right]^\mathrm{T} \in \mathbb {R}^{N}$</span>     </span>.</p>    <p>For convenience, we let <em>S<sub>V</sub>     </em>(<em>m</em>) denote the set of views that contains the <em>m</em>-th mode, <span class="inline-equation"><span class="tex">$\mathbf {X}^{(m)} = [\mathbf {x}_{1}^{(m)}, \cdots , \mathbf {x}_{N}^{(m)}]$</span>     </span>, <span class="inline-equation"><span class="tex">$ {\Pi }^{(v)} = [ {\pi }_{1}^{(v)}, \cdots , {\pi }_{N}^{(v)}]^\mathrm{T}$</span>     </span> and <span class="inline-equation"><span class="tex">$ {\Pi }^{(v,-m)} = [ {\pi }_{1}^{(v,-m)}, \cdots , {\pi }_{N}^{(v,-m)}]^\mathrm{T}$</span>     </span>. We then have that <div class="table-responsive" id="eq11">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \frac{\partial \mathcal {L}}{\partial f} \frac{\partial f}{\partial \mathbf {\Theta }^{(m)}} = \mathbf {X}^{(m)}\left(\sum _{v \in S_V(m)}\left(\left(\frac{\partial \mathcal {L}}{\partial f} {\phi }^{v} \right) \ast {\Pi }^{(v,-m)} \right) \right) \end{equation} </span>       <br/>       <span class="equation-number">(11)</span>      </div>     </div>    </p>    <p>Similarly, the partial derivative of <span class="inline-equation"><span class="tex">$\mathcal {R}$</span>     </span> w.r.t. <strong>b</strong>     <sup>(<em>v</em>, <em>m</em>)</sup> is given by <div class="table-responsive" id="eq12">      <div class="display-equation">       <span class="tex mytex">\begin{align} \frac{\partial \mathcal {R}}{\partial \mathbf {b}^{(v,m)}} &#x0026;= \frac{\partial \mathcal {L}}{\partial f} \frac{\partial f}{\partial \mathbf {b}^{(v,m)}} + \lambda \frac{\partial \Omega _{\lambda }(\mathbf {b}^{(v,m)}) }{\partial \mathbf {b}^{(v,m)}}\nonumber \\ &#x0026;= \mathbf {1}^\mathrm{T} \left(\left(\frac{\partial \mathcal {L}}{\partial f} {\phi }^{v} \right) \ast {\Pi }^{(v,-m)} \right)+ \lambda \frac{\partial \Omega _{\lambda }(\mathbf {b}^{(v,m)}) }{\partial \mathbf {b}^{(v,m)}} \end{align} </span>       <br/>       <span class="equation-number">(12)</span>      </div>     </div>    </p>    <p>The partial derivative of <span class="inline-equation"><span class="tex">$\mathcal {R}$</span>     </span> w.r.t. <strong>      <em>&#x03A6;</em>     </strong> is given by <div class="table-responsive" id="eq13">      <div class="display-equation">       <span class="tex mytex">\begin{align} \frac{\partial \mathcal {R}}{\partial \mathbf {\Phi }} &#x0026;= \left[\begin{array}{c}\left(\frac{\partial \mathcal {L}}{\partial f} \right)^{\mathrm{T}} {\Pi }^{(1)}~; ~\cdots ;~\left(\frac{\partial \mathcal {L}}{\partial f} \right)^{\mathrm{T}} {\Pi }^{(V)} \end{array}\right] + \lambda \frac{\partial \Omega _{\lambda }(\mathbf {\Phi })}{\partial \mathbf {\Phi }} \end{align} </span>       <br/>       <span class="equation-number">(13)</span>      </div>     </div>    </p>    <p>Finally, the gradient of <span class="inline-equation"><span class="tex">$\mathcal {R}$</span>     </span> can be formed by vectorizing the partial derivatives with respect to each factor matrix and concatenating them all, i.e., <div class="table-responsive" id="eq14">      <div class="display-equation">       <span class="tex mytex">\begin{align} \nabla \mathcal {R} = \left[ \begin{array}{c}\text{vec}(\frac{\partial \mathcal {R}}{\partial {\Theta }^{(1)}})\\\vdots \\\text{vec}(\frac{\partial \mathcal {R}}{\partial {\Theta }^{(M)}})\\\text{vec}(\frac{\partial \mathcal {R}}{\partial \mathbf {b}^{(1,1)}})\\\vdots \\\text{vec}(\frac{\partial \mathcal {R}}{\partial \mathbf {b}^{(V,M)}})\\\text{vec}(\frac{\partial \mathcal {R}}{\partial {\Phi }}) \end{array} \right] \end{align} </span>       <br/>       <span class="equation-number">(14)</span>      </div>     </div>    </p>    <p>Once we have the function, <span class="inline-equation"><span class="tex">$\mathcal {R}$</span>     </span> and gradient, <span class="inline-equation"><span class="tex">$\nabla \mathcal {R}$</span>     </span>, we can use any gradient-based optimization algorithm to compute the factor matrices. For the results presented in this paper, we use the Adaptive Moment Estimation (Adam) optimization algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] for parameter updates. Adam is an adaptive version of gradient descent that controls individual adaptive learning rates for different parameters from estimates of first and second moments of the gradient. It combines the best properties of the AdaGrad&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>], which works well with sparse gradients, and RMSProp&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>], which works well in on-line and non-stationary settings. Readers can refer to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>] for details of the Adam optimization algorithm.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.3</span> Efficient Computing with Relational Structures</h3>     </div>    </header>    <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">(a) Feature vectors of the same entity repeatedly appear in the plain formatted feature matrix <strong>X</strong>. (b) Repeating patterns in <strong>X</strong> can be formalized by the relational structure <strong>B</strong> of each mode. For example, the forth column of the feature matrix <strong>X</strong> can be represented as <span class="inline-equation"><span class="tex">$\mathbf {x}_4 = [\mathbf {x}_{\psi (4)}^{(1)}; \mathbf {x}_{\psi (4)}^{(2)}; \mathbf {x}_{\psi (4)}^{(3)}; \mathbf {x}_{\psi (4)}^{(4)}]$</span>       </span>       <span class="inline-equation"><span class="tex">$=[\mathbf {x}_2^{B^{(1)}}; \mathbf {x}_1^{B^{(2)}}; \mathbf {x}_4^{B^{(3)}}; \mathbf {x}_2^{B^{(4)}}]$</span>       </span>.</span>     </div>    </figure>    <p>In relational domains, we can often observe that feature vectors of the same entity repeatedly appear in the plain formatted feature matrix <strong>X</strong>, where <span class="inline-equation"><span class="tex">$\mathbf {X} = [\mathbf {X}^{(1)};\cdots ;\mathbf {X}^{(M)}] \in \mathbb {R}^{I \times N}$</span>     </span> and <span class="inline-equation"><span class="tex">$\mathbf {X}^{(m)} \in \mathbb {R}^{I_m \times N}$</span>     </span> is the feature matrix in the <em>m</em>-th mode. Consider Fig.&#x00A0;<a class="fig" href="#fig3">3</a>(a) as an example, where the parts highlighted in yellow in the forth mode (which represents the friends of the user) are repeatedly appear in the first three columns. Clearly, these repeating patterns stem from the relational structure of the same entity.</p>    <p>In the following, we show how the proposed SFM method can make use of relational structure of each mode, such that the learning and prediction can be scaled to predictor variables generated from relational data involving relations of high cardinality. We adopt the idea from&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0035">35</a>] to avoid redundant computing on repeating patterns over a set of feature vectors.</p>    <p>Let <span class="inline-equation"><span class="tex">$\mathcal {B} = \lbrace (\mathbf {X}^{B^{(m)}}, \psi ^{B^{(m)}})\rbrace _{m=1}^{M}$</span>     </span> be the set of relational structures, where <span class="inline-equation"><span class="tex">$\mathbf {X}^{B^{(m)}} \in \mathbb {R}^{I_m \times N_m}$</span>     </span> denotes the relational matrix of <em>m</em>-th mode, <span class="inline-equation"><span class="tex">$\psi ^{B^{(m)}}: \lbrace 1, \cdots , N\rbrace \rightarrow \lbrace 1,\cdots , N_{m}\rbrace$</span>     </span> denotes the mapping from columns in the feature matrix <strong>X</strong> to columns within <span class="inline-equation"><span class="tex">$\mathbf {X}^{B^{(m)}}$</span>     </span>. To shorten notation, the index <em>B</em> is dropped from the mapping <em>&#x03C8;<sup>B</sup>     </em> whenever it is clear which block the mapping belongs to. From <span class="inline-equation"><span class="tex">$\mathcal {B}$</span>     </span>, one can reconstruct <strong>X</strong> by concatenating the corresponding columns of the relational matrices using the mappings. For instance, the feature vector <strong>x</strong>     <sub>      <em>n</em>     </sub> of the <em>n</em>-th case in the plain feature matrix <strong>X</strong> is represented as <span class="inline-equation"><span class="tex">$\mathbf {x}_n = [ \mathbf {x}_{\psi (n)}^{(1)}; \cdots ; \mathbf {x}_{\psi (n)}^{(M)}]$</span>     </span>. Fig.&#x00A0;<a class="fig" href="#fig3">3</a>(b) shows an example how the feature matrix can be represented in relational structures. Let <em>N<sub>z</sub>     </em>(<strong>A</strong>) denote the number of non-zeros in a matrix <strong>A</strong>. The space required for using relational structures to represent the input data is <span class="inline-equation"><span class="tex">$|\mathcal {B}| = NM + \sum _m N_z(\mathbf {X}^{B^{(m)}})$</span>     </span>, which is much smaller than <em>N<sub>z</sub>     </em>(<strong>X</strong>) if there are repeating patterns in the feature matrix <strong>X</strong>.</p>    <p>Now we can rewrite the predictive model in Eq.&#x00A0;(<a class="eqn" href="#eq8">8</a>) as follows <div class="table-responsive" id="eq15">      <div class="display-equation">       <span class="tex mytex">\begin{equation} f(\lbrace \mathcal {X}_{n}^{(v)}\rbrace = \sum _{v=1}^{V} {\phi }^v \prod _{m \in S_M(v)} \ast \left(\mathbf {h}_{\psi (n)}^{B^{(m)}} + \mathbf {b}^{(v,m)^\mathrm{T}} \right), \end{equation} </span>       <br/>       <span class="equation-number">(15)</span>      </div>     </div> with the caches <span class="inline-equation"><span class="tex">$\mathbf {H}^{B^{(m)}} = [\mathbf {h}^{B^{(m)}}_1, \cdots , \mathbf {h}^{B^{(m)}}_{N_m}]$</span>     </span> for each mode, where <span class="inline-equation"><span class="tex">$\mathbf {h}^{B^{(m)}}_j = {\Theta }^{(m)^\mathrm{T}} \mathbf {x}_j^{B^{(m)}}, ~\forall j \in [1:N_m]$</span>     </span>.</p>    <p>This directly shows how <em>N</em> samples can be efficiently predicted: (i) compute <span class="inline-equation"><span class="tex">$\mathbf {H}^{B^{(m)}}$</span>     </span> in <span class="inline-equation"><span class="tex">$O(R N_z(\mathbf {X}^{B^{(m)}}))$</span>     </span> for each mode, (ii) compute <em>N</em> predictions with Eq.&#x00A0;(<a class="eqn" href="#eq15">15</a>) using caches in <em>O</em>(<em>RN</em>(<em>V</em> + &#x2211;<sub>      <em>v</em>     </sub>     <em>M<sub>v</sub>     </em>)). With the help of relational structures, SFMs can learn the same parameters and make the same predictions but with a much lower runtime complexity.</p>    </section>   </section>   <section id="sec-16">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experiments</h2>    </div>    </header>    <div class="table-responsive" id="tab2">    <div class="table-caption">     <span class="table-number">Table 2:</span>     <span class="table-title">The statistics for each dataset. <em>N<sub>z</sub>      </em>(<em>X</em>) and <span class="inline-equation"><span class="tex">$N_z(\mathcal {B})$</span>      </span> are the number of non-zeros in plain formatted feature matrix and in relational structures, respectively. Game: Video Games, Cloth: Clothing, Shoes and Jewelry, Sport: Sports and Outdoors, Health: Health and Personal Care, Home: Home and Kitchen, Elec: Electronics.</span>    </div>    <table class="table">     <thead>      <tr>       <th style="text-align:left;">Dataset</th>       <th style="text-align:center;">#Samples</th>       <th colspan="5" style="text-align:center;">Mode<hr/>       </th>       <th style="text-align:center;">Density</th>       <th style="text-align:center;">       <em>N<sub>z</sub>       </em>(<em>X</em>)</th>       <th style="text-align:center;">       <span class="inline-equation"><span class="tex">$N_z(\mathcal {B})$</span>       </span>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:left;">Amazon</td>       <td style="text-align:center;"/>       <td style="text-align:center;">#Users</td>       <td style="text-align:center;">#Items</td>       <td style="text-align:center;">#Words</td>       <td style="text-align:center;">#Categories</td>       <td style="text-align:center;">#Links</td>       <td style="text-align:center;"/>       <td/>       <td/>      </tr>      <tr>       <td style="text-align:left;">Game</td>       <td style="text-align:center;">231,780</td>       <td style="text-align:center;">24,303</td>       <td style="text-align:center;">10,672</td>       <td style="text-align:center;">7,500</td>       <td style="text-align:center;">193</td>       <td style="text-align:center;">17,974</td>       <td style="text-align:center;">0.089%</td>       <td style="text-align:center;">32.9M</td>       <td style="text-align:center;">15.2M</td>      </tr>      <tr>       <td style="text-align:left;">Cloth</td>       <td style="text-align:center;">278,677</td>       <td style="text-align:center;">39,387</td>       <td style="text-align:center;">23,033</td>       <td style="text-align:center;">3,493</td>       <td style="text-align:center;">1,175</td>       <td style="text-align:center;">107,139</td>       <td style="text-align:center;">0.031%</td>       <td style="text-align:center;">25.6M</td>       <td style="text-align:center;">7.3M</td>      </tr>      <tr>       <td style="text-align:left;">Sport</td>       <td style="text-align:center;">296,337</td>       <td style="text-align:center;">35,598</td>       <td style="text-align:center;">18,357</td>       <td style="text-align:center;">5,202</td>       <td style="text-align:center;">1,432</td>       <td style="text-align:center;">73,040</td>       <td style="text-align:center;">0.045%</td>       <td style="text-align:center;">34.2M</td>       <td style="text-align:center;">10.2M</td>      </tr>      <tr>       <td style="text-align:left;">Health</td>       <td style="text-align:center;">346,355</td>       <td style="text-align:center;">38,609</td>       <td style="text-align:center;">18,534</td>       <td style="text-align:center;">5,889</td>       <td style="text-align:center;">849</td>       <td style="text-align:center;">80,379</td>       <td style="text-align:center;">0.048%</td>       <td style="text-align:center;">33.6M</td>       <td style="text-align:center;">12.1M</td>      </tr>      <tr>       <td style="text-align:left;">Home</td>       <td style="text-align:center;">551,682</td>       <td style="text-align:center;">66,569</td>       <td style="text-align:center;">28,237</td>       <td style="text-align:center;">6,455</td>       <td style="text-align:center;">970</td>       <td style="text-align:center;">99,090</td>       <td style="text-align:center;">0.029%</td>       <td style="text-align:center;">46.8M</td>       <td style="text-align:center;">19.4M</td>      </tr>      <tr>       <td style="text-align:left;">Elec</td>       <td style="text-align:center;">1,689,188</td>       <td style="text-align:center;">192,403</td>       <td style="text-align:center;">63,001</td>       <td style="text-align:center;">12,805</td>       <td style="text-align:center;">967</td>       <td style="text-align:center;">89,259</td>       <td style="text-align:center;">0.014%</td>       <td style="text-align:center;">161.5M</td>       <td style="text-align:center;">69M</td>      </tr>      <tr>       <td style="text-align:left;"/>       <td style="text-align:center;"/>       <td style="text-align:center;">#Users</td>       <td style="text-align:center;">#Venues</td>       <td style="text-align:center;">#Friends</td>       <td style="text-align:center;">#Categories</td>       <td style="text-align:center;">#Cities</td>       <td style="text-align:center;"/>       <td/>       <td/>      </tr>      <tr>       <td style="text-align:left;">Yelp</td>       <td style="text-align:center;">1,319,870</td>       <td style="text-align:center;">88,009</td>       <td style="text-align:center;">40,520</td>       <td style="text-align:center;">88,009</td>       <td style="text-align:center;">892</td>       <td style="text-align:center;">412</td>       <td style="text-align:center;">0.037%</td>       <td style="text-align:center;">70.5M</td>       <td style="text-align:center;">1.4M</td>      </tr>      <tr>       <td style="text-align:left;"/>       <td style="text-align:center;"/>       <td style="text-align:center;">#Users</td>       <td style="text-align:center;">#Books</td>       <td style="text-align:center;">#Countries</td>       <td style="text-align:center;">#Ages</td>       <td style="text-align:center;">#Authors</td>       <td style="text-align:center;"/>       <td/>       <td/>      </tr>      <tr>       <td style="text-align:left;">BX</td>       <td style="text-align:center;">244,848</td>       <td style="text-align:center;">24,325</td>       <td style="text-align:center;">45,074</td>       <td style="text-align:center;">57</td>       <td style="text-align:center;">8</td>       <td style="text-align:center;">17,178</td>       <td style="text-align:center;">0.022%</td>       <td style="text-align:center;">1.2M</td>       <td style="text-align:center;">163K</td>      </tr>     </tbody>    </table>    </div>    <figure id="fig4">    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>    <div class="figure-caption">     <span class="figure-number">Figure 4:</span>     <span class="figure-title">Schema of the structural views in each dataset.</span>    </div>    </figure>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Datasets</h3>     </div>    </header>    <p>To evaluate the ability and applicability of the proposed SFMs, we include a spectrum of large datasets from different domains. The statistics for each dataset is summarized in Table <a class="tbl" href="#tab2">2</a>, the schema of the structural views in each dataset is presented in Fig.&#x00A0;<a class="fig" href="#fig4">4</a>, and the details are as follows:</p>    <p>     <strong>Amazon</strong><a class="fn" href="#fn3" id="foot-fn3"><sup>2</sup></a>: The first group of datasets are from Amazon.com recently introduced by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]. This is among the largest datasets available that include review texts and metadata of items. Each top-level category of products on Amazon.com has been constructed as an independent dataset in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0031">31</a>]. In this paper, we take a variety of large categories as listed in Tabel&#x00A0;<a class="tbl" href="#tab2">2</a>.</p>    <p>Each sample in these datasets has five modes, <em>i.e.</em>, users, items, review texts, categories, and linkage. The user mode and item mode are represented by one-hot encoding. The &#x2113;<sub>2</sub>-normalized TF-IDF vector representation of review text&#x00A0;<a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a> of the item given by the user is used as the text mode. The category mode and linkage mode consists of all the categories and all the co-purchasing items of the item, which might be from other categories. The last two modes are &#x2113;<sub>1</sub>-normalized.</p>    <p>     <strong>Yelp</strong><a class="fn" href="#fn5" id="foot-fn5"><sup>4</sup></a>: It is a large-scale dataset consisting of venue reviews. Each sample in this dataset contains five modes, <em>i.e.</em>, users, venues, friends, categories and cities. The user mode and venue mode are represented by one-hot encoding. The friend mode consists of the friends&#x2019; ids of users. The category mode and city mode consists of all the categories and the city of the venue. The last three modes are &#x2113;<sub>1</sub>-normalized.</p>    <p>     <strong>BookCrossing (BX)</strong><a class="fn" href="#fn6" id="foot-fn6"><sup>5</sup></a>: It is a book review dataset collected from the Book-Crossing community. Each sample in this dataset contains five modes, <em>i.e.</em>, users, books, countries, ages and authors. The ages are split in eight bins as in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>]. The country mode and age mode consist of the corresponding meta information of the user. The author modes represents the authors of the book. All the modes are represented by one-hot encoding.</p>    <p>The values of samples range within [1:5] in Amazon and Yelp datasets, and range within [1:10] in BX dataset.</p>    </section>    <section id="sec-18">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Comparison Methods</h3>     </div>    </header>    <p>In order to demonstrate the effectiveness of the proposed SFMs, we compare a series of state-of-the-art methods.</p>    <p>     <strong>Matrix Factorization (MF)</strong> is used to validate that meta information is helpful for improving prediction performance. We use the LIBMF implementation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] for comparison in the experiment.</p>    <p>     <strong>Factorization Machine (FM)</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] is the state-of-the-art method in recommender systems. We compare with its higher-order extension&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>] with up to second-order, and third-order feature interactions, and denote them as FM-2 and FM-3.</p>    <p>     <strong>Polynomial Network (PolyNet)</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0027">27</a>] is a recently proposed method that utilizes polynomial kernel on all features. We compare the augmented PolyNet (which adds a constant one to the feature vector&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]) with up to the second-order, and third-order kernel and denote them as PolyNet-2 and PolyNet-3.</p>    <p>     <strong>Multi-View Machine (MVM)</strong>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>] is a tensor factorization based method that explores the latent representation embedded in the full-order interactions among all the modes.</p>    <p>     <strong>Structural Factorization Machine (SFM)</strong> is the proposed model that learns the common latent spaces shared in multi-way data.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">MSE comparison on all the datasets. The best results are listed in bold.</span>     </div>     <table class="table">      <thead>       <tr>       <th style="text-align:left;">Dataset</th>       <th style="text-align:center;">(a)</th>       <th style="text-align:center;">(b)</th>       <th style="text-align:center;">(c)</th>       <th style="text-align:center;">(d)</th>       <th style="text-align:center;">(e)</th>       <th style="text-align:center;">(f)</th>       <th style="text-align:center;">(g)</th>       <th colspan="3" style="text-align:center;">Improvement of SFM verus<hr/>       </th>       </tr>       <tr>       <th style="text-align:left;"/>       <th style="text-align:center;">        <strong>MF</strong>       </th>       <th style="text-align:center;">        <strong>MVM</strong>       </th>       <th style="text-align:center;">        <strong>FM-2</strong>       </th>       <th style="text-align:center;">        <strong>FM-3</strong>       </th>       <th style="text-align:center;">        <strong>PolyNet-2</strong>       </th>       <th style="text-align:center;">        <strong>PolyNet-3</strong>       </th>       <th style="text-align:center;">        <strong>SFM</strong>       </th>       <th style="text-align:center;">b</th>       <th style="text-align:center;">min(c,d)</th>       <th>min(e,f)</th>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:left;">Game</td>       <td style="text-align:center;">1.569 &#x00B1; 0.005</td>       <td style="text-align:center;">0.753 &#x00B1; 0.007</td>       <td style="text-align:center;">0.764 &#x00B1; 0.006</td>       <td style="text-align:center;">0.749 &#x00B1; 0.007</td>       <td style="text-align:center;">0.749 &#x00B1; 0.004</td>       <td style="text-align:center;">0.748 &#x00B1; 0.006</td>       <td style="text-align:center;">        <strong>0.723 &#x00B1; 0.006</strong>       </td>       <td style="text-align:center;">4.06%</td>       <td style="text-align:center;">3.52%</td>       <td>3.35%</td>       </tr>       <tr>       <td style="text-align:left;">Cloth</td>       <td style="text-align:center;">1.624 &#x00B1; 0.009</td>       <td style="text-align:center;">0.725 &#x00B1; 0.046</td>       <td style="text-align:center;">0.678 &#x00B1; 0.004</td>       <td style="text-align:center;">0.679 &#x00B1; 0.004</td>       <td style="text-align:center;">0.678 &#x00B1; 0.007</td>       <td style="text-align:center;">0.680 &#x00B1; 0.005</td>       <td style="text-align:center;">        <strong>0.659 &#x00B1; 0.013</strong>       </td>       <td style="text-align:center;">9.03%</td>       <td style="text-align:center;">2.82%</td>       <td>2.84%</td>       </tr>       <tr>       <td style="text-align:left;">Sport</td>       <td style="text-align:center;">1.290 &#x00B1; 0.004</td>       <td style="text-align:center;">0.646 &#x00B1; 0.019</td>       <td style="text-align:center;">0.638 &#x00B1; 0.003</td>       <td style="text-align:center;">0.632 &#x00B1; 0.007</td>       <td style="text-align:center;">0.631 &#x00B1; 0.005</td>       <td style="text-align:center;">0.632 &#x00B1; 0.005</td>       <td style="text-align:center;">        <strong>0.614 &#x00B1; 0.011</strong>       </td>       <td style="text-align:center;">5.00%</td>       <td style="text-align:center;">2.91%</td>       <td>2.79%</td>       </tr>       <tr>       <td style="text-align:left;">Health</td>       <td style="text-align:center;">1.568 &#x00B1; 0.007</td>       <td style="text-align:center;">0.807 &#x00B1; 0.012</td>       <td style="text-align:center;">0.779 &#x00B1; 0.004</td>       <td style="text-align:center;">0.778 &#x00B1; 0.004</td>       <td style="text-align:center;">0.779 &#x00B1; 0.005</td>       <td style="text-align:center;">0.776 &#x00B1; 0.005</td>       <td style="text-align:center;">        <strong>0.763 &#x00B1; 0.019</strong>       </td>       <td style="text-align:center;">5.47%</td>       <td style="text-align:center;">2.02%</td>       <td>1.77%</td>       </tr>       <tr>       <td style="text-align:left;">Home</td>       <td style="text-align:center;">1.591 &#x00B1; 0.004</td>       <td style="text-align:center;">0.729 &#x00B1; 0.067</td>       <td style="text-align:center;">0.714 &#x00B1; 0.002</td>       <td style="text-align:center;">0.714 &#x00B1; 0.004</td>       <td style="text-align:center;">0.690 &#x00B1; 0.003</td>       <td style="text-align:center;">0.692 &#x00B1; 0.005</td>       <td style="text-align:center;">        <strong>0.678 &#x00B1; 0.008</strong>       </td>       <td style="text-align:center;">6.93%</td>       <td style="text-align:center;">5.00%</td>       <td>1.72%</td>       </tr>       <tr>       <td style="text-align:left;">Elec</td>       <td style="text-align:center;">1.756 &#x00B1; 0.002</td>       <td style="text-align:center;">0.792 &#x00B1; 0.042</td>       <td style="text-align:center;">0.776 &#x00B1; 0.006</td>       <td style="text-align:center;">0.749 &#x00B1; 0.007</td>       <td style="text-align:center;">0.760 &#x00B1; 0.004</td>       <td style="text-align:center;">0.757 &#x00B1; 0.001</td>       <td style="text-align:center;">        <strong>0.747 &#x00B1; 0.006</strong>       </td>       <td style="text-align:center;">5.69%</td>       <td style="text-align:center;">0.27%</td>       <td>1.33%</td>       </tr>       <tr>       <td style="text-align:left;">Yelp</td>       <td style="text-align:center;">1.713 &#x00B1; 0.003</td>       <td style="text-align:center;">1.2575 &#x00B1; 0.013</td>       <td style="text-align:center;">1.277 &#x00B1; 0.002</td>       <td style="text-align:center;">1.277 &#x00B1; 0.002</td>       <td style="text-align:center;">1.272 &#x00B1; 0.002</td>       <td style="text-align:center;">1.272 &#x00B1; 0.002</td>       <td style="text-align:center;">        <strong>1.256 &#x00B1; 0.010</strong>       </td>       <td style="text-align:center;">0.09%</td>       <td style="text-align:center;">1.58%</td>       <td>1.19%</td>       </tr>       <tr>       <td style="text-align:left;">BX</td>       <td style="text-align:center;">4.094 &#x00B1; 0.025</td>       <td style="text-align:center;">2.844 &#x00B1; 0.024</td>       <td style="text-align:center;">2.766 &#x00B1; 0.012</td>       <td style="text-align:center;">2.767 &#x00B1; 0.014</td>       <td style="text-align:center;">2.654 &#x00B1; 0.013</td>       <td style="text-align:center;">2.658 &#x00B1; 0.013</td>       <td style="text-align:center;">        <strong>2.541 &#x00B1; 0.025</strong>       </td>       <td style="text-align:center;">10.66%</td>       <td style="text-align:center;">8.16%</td>       <td>4.27%</td>       </tr>       <tr>       <td colspan="3" style="text-align:left;">Average on all datasets<hr/>       </td>       <td style="text-align:center;"/>       <td style="text-align:center;"/>       <td style="text-align:center;"/>       <td style="text-align:center;"/>       <td style="text-align:center;"/>       <td style="text-align:center;">5.87%</td>       <td style="text-align:center;">3.29%</td>       <td>2.41%</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Experimental Settings</h3>     </div>    </header>    <p>For each dataset, we randomly split 50%, 10%, and 40% of labeled samples as training set, validation set, and testing set, respectively. Validation sets are used for hyper-parameter tuning for each model. Each of the validation and testing sets does not overlap with any other set so as to ensure the sanity of the experiment. For simplicity and fair comparison, in all the comparison methods, the dimension of latent factors <em>R</em> = 20 and the maximum number of epochs is set as 400 and we use early stop to obtain the best results for each method. Forbenius norm regularizers are used to avoid overfitting. The regularization hyper-parameter is tuned from {10<sup>&#x2212; 5</sup>, ~10<sup>&#x2212; 4</sup>, ~&#x22C5;&#x22C5;&#x22C5;, ~10<sup>0</sup>}.</p>    <p>All the methods except MF are implemented in TensorFlow, and the parameters are initialized using scaling variance initializer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0014">14</a>]. We tune the scaling factor of initializer <em>&#x03C3;</em> from {1, 2, 5, 10, 100} and the learning rate <em>&#x03B7;</em> from {0.01, 0.1, 1} using the validation sets. In the experiment, we set <em>&#x03C3;</em> = 2 (default setting in TensorFlow) and <em>&#x03B7;</em> = 0.01 for these methods except MVM. We found that MVM is more sensitive to the configuration, because MVM will element-wisely multiply the latent factors of all the modes which leads to an extremely small value approaching zero. <em>&#x03C3;</em> = 10 and <em>&#x03B7;</em> = 0.1 yielded the best performance for MVM.</p>    <p>To investigate the performance of comparison methods, we adopt mean squared error (MSE) on the test data as the evaluation metrics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0045">45</a>]. The smaller value of the metric indicates the better performance. Each experiment was repeated for 10 times, and the mean and standard deviation of each metric in each data set were reported. All experiments are conducted on a single machine with Intel Xeon 6-Core CPUs of 2.4 GHz and equipped with a Maxwell Titan X GPU. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Training Time (Seconds/Epoch) Comparison.</span>      </div>     </figure>     <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Performance gain in MSE compared with MF for users with limited training samples. <em>G</em>       <sub>1</sub>, <em>G</em>       <sub>2</sub>, and <em>G</em>       <sub>3</sub> are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.</span>      </div>     </figure>     <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/images/www2018-80-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Sensitivity analysis of the latent dimension <em>R</em>.</span>      </div>     </figure>    </p>    </section>    <section id="sec-20">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.4</span> Performance Analysis</h3>     </div>    </header>    <p>The experimental results are shown in Table&#x00A0;<a class="tbl" href="#tab3">3</a>. The best method of each dataset is in bold. For clarity, on the right of the tables we show the percentage improvement of the proposed SFM method over a variety of methods. From these results, we can observe that SFM consistently outperforms all the comparison methods. We also make a few comparisons and summarize our findings as follows.</p>    <p>Compared with MF, SFM performs better with an average improvement of nearly 50%. MF usually performs well in practice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>], while in datasets which are extremely sparse, as is shown in our case, MF is unable to learn an accurate representation of users/items. Thus MF under-performs other methods which takes the meta information into consideration.</p>    <p>In both FM and PolyNet methods, the feature vectors from all the modes are concatenated as a single input feature vector. The major difference between these two methods is the choice of kernel applied&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0002">2</a>]. The polynomial kernel used in PolyNet considers all monomials (the products of features), i.e., all combinations of features <em>with</em> replacement. The ANOVA kernel used in FM considers only monomials composed of distinct features, i.e., feature combinations <em>without</em> replacement. Compared with the best results obtained from FM methods and from PolyNet methods, SFM leads to an average improvement of 3.3% and 2.4% in MSE, respectively.</p>    <p>The primary reason behind the results is how the latent factors of each feature are learned. For any factorization based method, the latent factors of a feature are essentially learned from its interactions with other features observed in the data, as can be observed from its update rule. In FM and PolyNet, all the feature interactions are taken into consideration without distinguishing the features from different modes. As a result, important feature interactions (e.g., the interactions between the given user and her friend) would be easily buried in irrelevant feature interactions from the same modes (e.g., the interactions between the friends of the same user). Hence, the learned latent factors are less representative in FM and PolyNet, compared with the proposed SFM. Besides, we can find that including higher-order interactions in FM and PolyNet (i.e., FM-3 and PolyNet-3) does not always improve the performance. Instead, it may even degrade the performance, as shown in Cloth, Yelp, and BX datasets. This is probably due to overfitting, as they need to include more parameters to model the interactions in higher orders while the datasets are extremely sparse such that the parameters cannot be properly learned.</p>    <p>Compared to the MVM method, which models the full-order interactions among all the modes, our proposed SFM leads to an average improvement of 5.87%. This is because not all the modes are relevant, and some irrelevant feature interactions may introduce unexpected noise to the learning task. The irrelevant information can even be exaggerated after combinations, thereby degrading performance. This suggests that preserving the nature of relational structure is important in building predictive models.</p>    </section>    <section id="sec-21">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.5</span> Computational Cost Analysis</h3>     </div>    </header>    <p>Next, we investigate the computational cost for comparison methods. The averaged training time (seconds per epoch) required for each dataset is shown in Fig.&#x00A0;<a class="fig" href="#fig5">5</a>. We can easily find that the proposed SFM requires much less computational cost on all the datasets, especially for the Yelp dataset (roughly 11% of computational cost required for training FM-3). The efficiency comes from the use of relational structure representation. As shown in Table&#x00A0;<a class="tbl" href="#tab2">2</a>, the number of non-zeros of the feature matrix <em>N<sub>z</sub>     </em>(<strong>X</strong>) is much larger than the number of non-zeros of the relational structure representation <span class="inline-equation"><span class="tex">$N_z(\mathcal {B})$</span>     </span>. The amount of repeating patterns is much higher for the Yelp dataset than for the other dataset, because adding all the friends of a user significantly increases results in large repeating blocks in the plain feature matrix. Standard ML algorithms like the compared methods have typically at best a linear complexity in <em>N<sub>z</sub>     </em>(<strong>X</strong>), while using the relational structure representation for SFM have a linear complexity in <span class="inline-equation"><span class="tex">$N_z(\mathcal {B})$</span>     </span>. This experiment substantiates the efficiency of the proposed SFM for large datasets.</p>    </section>    <section id="sec-22">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.6</span> Analysis of the Impact of Data Sparsity</h3>     </div>    </header>    <p>We proceed by further studying the impact of data sparsity on different methods. As can be found in the experimental results, the improvement of SFM over the traditional collaborative filtering methods (e.g., MF) is significant for datasets that are sparse, mainly because the number of samples is too scarce to model the items and users adequately. We verify this finding by comparing the performance of comparison methods with MF on users with limited training data. Shown in Fig.&#x00A0;<a class="fig" href="#fig6">6</a> is the gain of each method compared with MF for users with limited training samples, where <em>G</em>     <sub>1</sub>, <em>G</em>     <sub>2</sub>, and <em>G</em>     <sub>3</sub> are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set. Due to space limit, we only report the results from two Amazon datasets (Sport and Health) while the observations still hold for the rest datasets. It can be seen that the proposed SFM gains the most in group <em>G</em>     <sub>1</sub>, in which the users have extremely few training items. The performance gain starts to decrease with the number of training items available for each user. The results indicate that including meta information can be valuable information especially when limited information available.</p>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.7</span> Sensitivity analysis</h3>     </div>    </header>    <p>The number of latent factors <em>R</em> is an important hyperparameter for the factorization models. We analyze different values of <em>R</em> and report the averaged results in Fig.&#x00A0;<a class="fig" href="#fig7">7</a>. The results again show that SFM consistently outperforms other methods with various values of <em>R</em>. In contrast to findings in other related factorization models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>] where prediction error can steadily get reduced with larger <em>R</em>, we observe that the performance of each method is rather stable even with the increasing of <em>R</em>. It is reasonable in a general sense, as the expressiveness of the model is enough to describe the information embedded in data. Although larger <em>R</em> renders the model with greater expressiveness, when the available observations regarding the target values are too sparse but the meta information is rich, only a few number of factors are required to fit the data well.</p>    </section>   </section>   <section id="sec-24">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions</h2>    </div>    </header>    <p>In this paper, we introduce a generic framework for learning structural data from heterogeneous domains, which can explore the high order correlations underlying multi-view multi-way data. We develop structural factorization machines (SFMs) that learn the common latent spaces shared in the multi-view tensors while automatically adjust the contribution of each view in the predictive model. With the help of relational structure representation, we further provide an efficient approach to avoid unnecessary computation costs on repeating patterns of the multi-view data. It was shown that the proposed SFMs outperform state-of-the-art factorization models on eight large-scale datasets in terms of prediction accuracy and computational cost.</p>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>Acknowledgments</h2>    </div>    </header>    <p>This work is supported in part by NSF through grants IIS-1526499, and CNS-1626432, and NSFC 61672313, 61503253 and NSF of Guangdong Province (2017A030313339). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Evrim Acar, Tamara&#x00A0;G Kolda, and Daniel&#x00A0;M Dunlavy. 2011. All-at-once optimization for coupled matrix and tensor factorizations. <em>arXiv preprint <a href="../../../data/deliveryimages.acm.org/10.1145/3190000/3186071/arXiv:1105.3422" target="_blank">arXiv:1105.3422</a></em> (2011).</li>    <li id="BibPLXBIB0002" label="[2]">Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-Order Factorization Machines. In <em>Advances in Neural Information Processing Systems</em>. 3351&#x2013;3359.</li>    <li id="BibPLXBIB0003" label="[3]">Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, and Naonori Ueda. 2016. Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms. In <em>Proceedings of the 33nd International Conference on Machine Learning</em>. 850&#x2013;858.</li>    <li id="BibPLXBIB0004" label="[4]">Bokai Cao, Lifang He, Xiangnan Kong, Philip&#x00A0;S. Yu, Zhifeng Hao, and Ann&#x00A0;B. Ragin. 2014. Tensor-Based Multi-view Feature Selection with Applications to Brain Diseases. In <em>IEEE International Conference on Data Mining</em>. 40&#x2013;49.</li>    <li id="BibPLXBIB0005" label="[5]">Bokai Cao, Lei Zheng, Chenwei Zhang, Philip&#x00A0;S Yu, Andrea Piscitello, John Zulueta, Olu Ajilore, Kelly Ryan, and Alex&#x00A0;D Leow. 2017. DeepMood: Modeling Mobile Phone Typing Dynamics for Mood Detection. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 747&#x2013;755.</li>    <li id="BibPLXBIB0006" label="[6]">Bokai Cao, Hucheng Zhou, Guoqiang Li, and Philip&#x00A0;S Yu. 2016. Multi-view Machines. In <em>ACM International Conference on Web Search and Data Mining</em>. 427&#x2013;436.</li>    <li id="BibPLXBIB0007" label="[7]">Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, <em>et al.</em> 2016. Wide &#x0026; deep learning for recommender systems. In <em>DLRS</em>. ACM, 7&#x2013;10.</li>    <li id="BibPLXBIB0008" label="[8]">Wei-Sheng Chin, Bo-Wen Yuan, Meng-Yuan Yang, Yong Zhuang, Yu-Chin Juan, and Chih-Jen Lin. 2016. LIBMF: A library for parallel matrix factorization in shared-memory systems. <em>The Journal of Machine Learning Research</em> 17, 1 (2016), 2971&#x2013;2975.</li>    <li id="BibPLXBIB0009" label="[9]">Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In <em>ACM Recommender Systems Conference (RecSys)</em>. ACM, 191&#x2013;198.</li>    <li id="BibPLXBIB0010" label="[10]">John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. <em>The Journal of Machine Learning Research</em> 12 (2011), 2121&#x2013;2159.</li>    <li id="BibPLXBIB0011" label="[11]">Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. <em>arXiv preprint arXiv:1703.04247</em> (2017).</li>    <li id="BibPLXBIB0012" label="[12]">Hongyu Guo and Herna&#x00A0;L Viktor. 2006. Mining relational data through correlation-based multiple view validation. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 567&#x2013;573.</li>    <li id="BibPLXBIB0013" label="[13]">F&#x00A0;Maxwell Harper and Joseph&#x00A0;A Konstan. 2016. The movielens datasets: History and context. <em>ACM Transactions on Interactive Intelligent Systems (TiiS)</em> 5, 4(2016), 19.</li>    <li id="BibPLXBIB0014" label="[14]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In <em>Proceedings of the IEEE international conference on computer vision</em>. 1026&#x2013;1034.</li>    <li id="BibPLXBIB0015" label="[15]">Lifang He, Xiangnan Kong, S&#x00A0;Yu Philip, Ann&#x00A0;B Ragin, Zhifeng Hao, and Xiaowei Yang. 2014. Dusk: A dual structure-preserving kernel for supervised tensor learning with applications to neuroimages. <em>matrix</em> 3, 1 (2014), 2.</li>    <li id="BibPLXBIB0016" label="[16]">Lifang He, Chun-Ta Lu, Jiaqi Ma, Jianping Cao, Linlin Shen, and Philip&#x00A0;S Yu. 2016. Joint community and structural hole spanner detection via harmonic modularity. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. 875&#x2013;884.</li>    <li id="BibPLXBIB0017" label="[17]">Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse Predictive Analytics. In <em>Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.</li>    <li id="BibPLXBIB0018" label="[18]">G Hinton, N Srivastava, and K Swersky. 2012. RMSProp: Divide the gradient by a running average of its recent magnitude. <em>Neural networks for machine learning, Coursera lecture 6e</em> (2012).</li>    <li id="BibPLXBIB0019" label="[19]">Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In <em>ACM International Conference on Information and Knowledge Management</em>. ACM, 2333&#x2013;2338.</li>    <li id="BibPLXBIB0020" label="[20]">Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware factorization machines for CTR prediction. In <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>. ACM, 43&#x2013;50.</li>    <li id="BibPLXBIB0021" label="[21]">Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em> (2014).</li>    <li id="BibPLXBIB0022" label="[22]">Tamara&#x00A0;G Kolda and Brett&#x00A0;W Bader. 2009. Tensor decompositions and applications. <em>SIAM review</em> 51, 3 (2009), 455&#x2013;500.</li>    <li id="BibPLXBIB0023" label="[23]">Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 426&#x2013;434.</li>    <li id="BibPLXBIB0024" label="[24]">Yehuda Koren. 2010. Factor in the neighbors: Scalable and accurate collaborative filtering. <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em> 4, 1(2010), 1.</li>    <li id="BibPLXBIB0025" label="[25]">Tingting Liang, Lifang He, Chun-Ta Lu, Liang Chen, Philip&#x00A0;S. Yu, and Jian Wu. 2017. A Broad Learning Approach for Context-Aware Mobile Application Recommendation. In <em>2017 IEEE International Conference on Data Mining (ICDM)</em>. 955&#x2013;960.</li>    <li id="BibPLXBIB0026" label="[26]">Guang Ling, Michael&#x00A0;R Lyu, and Irwin King. 2014. Ratings meet reviews, a combined approach to recommend. In <em>Proceedings of the 8th ACM Conference on Recommender systems</em>. 105&#x2013;112.</li>    <li id="BibPLXBIB0027" label="[27]">Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. 2014. On the computational efficiency of training neural networks. In <em>Advances in Neural Information Processing Systems</em>. 855&#x2013;863.</li>    <li id="BibPLXBIB0028" label="[28]">Chun-Ta Lu, Lifang He, Weixiang Shao, Bokai Cao, and Philip&#x00A0;S. Yu. 2017. Multilinear Factorization Machines for Multi-Task Multi-View Learning. In <em>Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</em>. 701&#x2013;709.</li>    <li id="BibPLXBIB0029" label="[29]">Chun-Ta Lu, Sihong Xie, Weixiang Shao, Lifang He, and Philip&#x00A0;S Yu. 2016. Item recommendation for emerging online businesses. In <em>Proceedings of International Joint Conference Artificial Intelligence</em>. 3797&#x2013;3803.</li>    <li id="BibPLXBIB0030" label="[30]">Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In <em>Proceedings of the 7th ACM conference on Recommender systems</em>. 165&#x2013;172.</li>    <li id="BibPLXBIB0031" label="[31]">Julian McAuley, Rahul Pandey, and Jure Leskovec. 2015. Inferring networks of substitutable and complementary products. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 785&#x2013;794.</li>    <li id="BibPLXBIB0032" label="[32]">Alexander Novikov, Mikhail Trofimov, and Ivan Oseledets. 2017. Exponential machines. In <em>International Conference on Learning Representations</em>.</li>    <li id="BibPLXBIB0033" label="[33]">Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In <em>Data Mining (ICDM), 2016 IEEE 16th International Conference on</em>. IEEE, 1149&#x2013;1154.</li>    <li id="BibPLXBIB0034" label="[34]">Steffen Rendle. 2012. Factorization machines with libFM. <em>Intelligent Systems and Technology</em> 3, 3 (2012), 57.</li>    <li id="BibPLXBIB0035" label="[35]">Steffen Rendle. 2013. Scaling factorization machines to relational data. In <em>Proceedings of the VLDB Endowment</em>, Vol.&#x00A0;6. VLDB Endowment, 337&#x2013;348.</li>    <li id="BibPLXBIB0036" label="[36]">Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In <em>Proceedings of the third ACM international conference on Web search and data mining</em>. ACM, 81&#x2013;90.</li>    <li id="BibPLXBIB0037" label="[37]">Ying Shan, T&#x00A0;Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, and JC Mao. 2016. Deep Crossing: Web-scale modeling without manually crafted combinatorial features. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>. ACM, 255&#x2013;262.</li>    <li id="BibPLXBIB0038" label="[38]">Ajit&#x00A0;P Singh and Geoffrey&#x00A0;J Gordon. 2008. Relational learning via collective matrix factorization. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 650&#x2013;658.</li>    <li id="BibPLXBIB0039" label="[39]">Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep &#x0026; Cross Network for Ad Click Predictions. <em>arXiv preprint arXiv:1708.05123</em> (2017).</li>    <li id="BibPLXBIB0040" label="[40]">Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. In <em>International Joint Conference on Artificial Intelligence</em>.</li>    <li id="BibPLXBIB0041" label="[41]">Chang Xu, Dacheng Tao, and Chao Xu. 2013. A survey on multi-view learning. <em>arXiv:1304.5634</em> (2013).</li>    <li id="BibPLXBIB0042" label="[42]">Ling Yan, Wu-jun Li, Gui-Rong Xue, and Dingyi Han. 2014. Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising. In <em>International Conference on Machine Learning</em>. 802&#x2013;810.</li>    <li id="BibPLXBIB0043" label="[43]">Jingyuan Zhang, Chun-Ta Lu, Bokai Cao, Yi Chang, and Philip&#x00A0;S. Yu. 2017. Connecting Emerging Relationships from News via Tensor Factorization. In <em>Proceedings of IEEE International Conference on Big Data</em>. IEEE.</li>    <li id="BibPLXBIB0044" label="[44]">Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field categorical data. In <em>European conference on information retrieval</em>. Springer, 45&#x2013;57.</li>    <li id="BibPLXBIB0045" label="[45]">Lei Zheng, Vahid Noroozi, and Philip&#x00A0;S Yu. 2017. Joint Deep Modeling of Users and Items Using Reviews for Recommendation. In <em>Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</em>. 425&#x2013;434.</li>    <li id="BibPLXBIB0046" label="[46]">Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Xingya Dai, Han Zhu, Junqi Jin, Han Li, and Kun Gai. 2017. Deep Interest Network for Click-Through Rate Prediction. <em>arXiv preprint arXiv:1706.06978</em> (2017).</li>    <li id="BibPLXBIB0047" label="[47]">Jie Zhu, Ying Shan, JC Mao, Dong Yu, Holakou Rahmanian, and Yi Zhang. 2017. Deep Embedding Forest: Forest-based Serving with Deep Embedding Features. In <em>Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining</em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>&#x204E;</sup></a>Corresponding author.</p>   <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>Full-order interactions range from the first-order interactions (i.e., contributions of single entity features) to the highest-order interactions (i.e., contributions of the outer product of features from all entities).</p>   <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a><a class="link-inline force-break" href="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a>Stemming, lemmatization, removing stop-words and words with frequency less than 100 times, etc., are handled beforehand.</p>   <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a><a class="link-inline force-break" href="https://www.yelp.com/dataset-challenge">https://www.yelp.com/dataset-challenge</a>   </p>   <p id="fn6"><a href="#foot-fn6"><sup>5</sup></a><a class="link-inline force-break" href="http://www2.informatik.uni-freiburg.de/">http://www2.informatik.uni-freiburg.de/&#x223C;cziegler/BX/</a></p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186071">https://doi.org/10.1145/3178876.3186071</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
