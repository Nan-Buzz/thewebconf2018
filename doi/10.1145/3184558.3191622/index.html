<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Gold Standard Creation for Microblog Retrieval: Challenges
  of Completeness in IRMiDis 2017</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191622'>https://doi.org/10.1145/3184558.3191622</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191622'>https://w3id.org/oa/10.1145/3184558.3191622</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Gold Standard Creation for
          Microblog Retrieval: Challenges of Completeness in
          IRMiDis 2017</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Ribhav</span> <span class=
          "surName">Soni</span> Department of Computer Science and
          EngineeringIndian Institute of Technology (Banaras Hindu
          University), Varanasi, India<a class="fn" href="#fn1" id=
          "foot-fn1"><sup>⁎</sup></a>, <a href=
          "mailto:ribhav.soni.cse13@iitbhu.ac.in">ribhav.soni.cse13@iitbhu.ac.in</a>
        </div>
        <div class="author">
          <span class="givenName">Sukomal</span> <span class=
          "surName">Pal</span> Department of Computer Science and
          EngineeringIndian Institute of Technology (Banaras Hindu
          University), Varanasi, India, <a href=
          "mailto:spal.cse@iitbhu.ac.in">spal.cse@iitbhu.ac.in</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191622"
        target=
        "_blank">https://doi.org/10.1145/3184558.3191622</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Microblogging sites like Twitter, Facebook, etc.,
        are important sources of first-hand accounts during
        disaster situations, and have the potential to
        significantly aid disaster relief efforts. The IRMiDis
        track at FIRE 2017 focused on developing and comparing IR
        approaches to automatically identify and match tweets that
        indicate the need or availability of a resource, leading to
        the creation of a benchmark dataset for future improvements
        in this task. However, based on our experiments, we argue
        that the gold standard data obtained in the track is
        substantially incomplete. We also discuss some reasons why
        it may have been so, and provide some suggestions for
        making more robust ground truth data in such
        tasks.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Information retrieval;</strong> Information systems
        applications; World Wide Web;</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Crisis
          Informatics</small>,</span> <span class=
          "keyword"><small>Disaster</small>,</span> <span class=
          "keyword"><small>Microblog Retrieval</small>,</span>
          <span class="keyword"><small>Social Media</small>,</span>
          <span class="keyword"><small>Gold
          Standard</small>,</span> <span class=
          "keyword"><small>Language Models</small>,</span>
          <span class="keyword"><small>Word
          Embeddings</small>,</span> <span class=
          "keyword"><small>word2vec</small>,</span> <span class=
          "keyword"><small>GloVe</small>,</span> <span class=
          "keyword"><small>WordNet</small>,</span> <span class=
          "keyword"><small>Query Expansion</small>,</span>
          <span class="keyword"><small>Relevance
          Feedback</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Ribhav Soni and Sukomal Pal. 2018. Gold Standard Creation
          for Microblog Retrieval: Challenges of Completeness in
          IRMiDis 2017. In <em>WWW '18 Companion: The 2018 Web
          Conference Companion,</em> <em>April 23–27, 2018,</em>
          <em>Lyon, France. ACM, New York, NY, USA</em> 4 Pages.
          <a href="https://doi.org/10.1145/3184558.3191622" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3191622</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>During times of disasters, a huge volume of
      disaster-related information is posted by users on
      microblogging sites (like Twitter, Facebook, etc.), which
      includes first-hand accounts of the situation that can help
      immensely in knowing about the on-ground situation, and thus
      in aiding disaster relief efforts.</p>
      <p>The Information Retrieval from Microblogs during Disasters
      (IRMiDis) track [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>] in FIRE 2017 <a class="fn" href=
      "#fn2" id="foot-fn2"><sup>1</sup></a> in particular focused
      on developing and comparing automated IR approaches to
      identify and match ”need tweets” and ”availability tweets”
      among a collection of about 46,000 tweets posted during the
      Nepal earthquake in 2015,<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>2</sup></a> where they were defined as:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">Need-tweet: A tweet that indicates
        the scarcity or requirement of some specific resource, such
        as food, water, medical aid, shelter, etc.<br /></li>
        <li id="list2" label="•">Availability-tweet: A tweet that
        indicates the future or actual availability of some
        specific resource.<br /></li>
      </ul>
      <p>The track consisted of two sub-tasks, with sub-task 1
      focused on identifying need tweets and availability tweets
      separately, from the given collection. Sub-task 2 was to
      match need-tweets with corresponding availability tweets that
      could satisfy the need of at least one resource mentioned in
      the need-tweet.</p>
      <p>The track led to the creation of a benchmark dataset for
      research in IR approaches to identify and match need and
      availability tweets posted during a disaster situation.
      However, based on our experiments, we argue that the gold
      standard creation for sub-task 2 (i.e., the list of correct
      pairs of need-availability tweets such that at least one
      resource mentioned in the need tweet is satisfied by the
      availability tweet, as identified by the human annotators) is
      substantially incomplete.</p>
      <p>The remainder of this paper is organised as follows. We
      describe the dataset used in the IRMiDis track in Section 2,
      the gold standard creation method used in Section 3, our
      experiments and observations in Section 4, some discussion in
      Section 5, and conclusion, along with some future directions
      of work, in Section 6.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Tweets
          dataset</h2>
        </div>
      </header>
      <p>The organizers had collected a set of about 66k tweets
      posted during the Nepal earthquake in 2015, which included
      tweets in English, Nepali, Hindi, etc., as well as code-mixed
      tweets (i.e., a single tweet containing two or more languages
      or scripts). A set of 20k tweets was made available as
      training data, while the remaining 46k tweets were used as
      test data for the track.</p>
      <p>Among the training tweets, the gold standard data (i.e.,
      the list of all need-tweets and availability-tweets for
      sub-task 1, and the list of all matching need-availability
      tweet pairs for sub-task 2) were also provided to the
      participants.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Gold Standard
          creation process adopted in IRMiDis 2017</h2>
        </div>
      </header>
      <p>The track organizers employed three annotators for
      creating the gold standard. The gold standard creation for
      sub-task 1 involved the following three stages.</p>
      <ol class="list-no-style">
        <li id="list3" label="(1)">First, each annotator
        independently searched for relevant need or availability
        tweets using manual runs, after the tweets were
        indexed.<br /></li>
        <li id="list4" label="(2)">Then, the annotators mutually
        discussed and finalized the relevance of the tweets that at
        least one of them had found in stage 1.<br /></li>
        <li id="list5" label="(3)">Finally, the top-100 results
        from each of the submitted runs were pooled, and judged by
        the annotators.<br /></li>
      </ol>
      <p>For sub-task 2 (i.e., matching the need and availability
      tweets that were separately listed in the gold standard for
      sub-task 1), the annotators were asked to manually match each
      need-tweet with the availability-tweets that could satisfy
      the need of at least one resource that was mentioned as
      lacking in the need-tweet. Additionally, pooling was applied
      over the submitted runs to judge the matching pairs that may
      have been missed by the annotators. As per the task
      instructions, only the top 5 matching availability-tweets
      were to be output for each need-tweet by a run submission,
      all of which by each run were taken in the pool for manual
      assessment by the annotators.</p>
      <p>There were only 10 runs submitted for sub-task 2, and they
      included only 4 different kinds of models (others differing
      only in the parameters of the model).</p>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experiments and
          Observations</h2>
        </div>
      </header>
      <p>As the sub-task 1 gold standard, 211 need-tweets and 718
      availability-tweets were identified by the annotators in the
      training data of 20k tweets. In the test data of about 46k
      tweets, 427 need-tweets and 980 availability-tweets were
      identified.</p>
      <p>For sub-task 2, the annotators identified a total of 3091
      need-availability tweet pairs from the training tweets, for
      200 need tweets, i.e., an average of 15.46 availability
      tweets identified for each need tweet. For the test data,
      they found 4117 need-availability tweet pairs, with 427 need
      tweets, i.e., an average of only 9.64 availability tweets for
      each need tweet.</p>
      <p>While there was no need-tweet in the training data for
      sub-task 2 for which no matching availability-tweet could be
      identified by the annotators, 126 out of 427 need-tweets in
      the test data were such that no matching availability-tweet
      was identified by the annotators. On manual inspection, we
      were easily able to identify about 10 matching
      availability-tweets for each of at least 10 of those 126
      tweets. In addition, even for many of the remaining
      need-tweets for which some availability-tweets were
      identified by the annotators, we could easily find at least
      10 more matching availability-tweets that were missed by the
      annotators.</p>
      <p>Some examples of need-tweets for which the annotators
      failed to find a single availability-tweet, along with some
      availability-tweets that should have been found, are shown in
      Table 1.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Examples of need-tweets for which no
          matching availability-tweets were found by the
          annotators, along with some availability-tweets that
          should have been found (the text has been translated to
          English wherever needed)</span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:left;">Need-tweets</td>
              <td style="text-align:left;">Matching
              Availability-tweets</td>
            </tr>
            <tr>
              <td style="text-align:left;">There is need for 4 lakh
              tents, food for 3.5 million people in
              Nepal.http://hindi.news-roompost.com/45248/nepal-needs-more-help/
              … (id:594732773539237888)</td>
              <td style="text-align:left;">Gud job
              by—&gt;@RaviNepal, has put together a list of places
              to getfood, water and shelter in Kathmandu.
              http://www.bit.ly/nepalrelief15#Earthquake
              (id:592751043164938240)</td>
            </tr>
            <tr>
              <td style="text-align:left;">Nepal AFTER QUAKE: Today
              the need is tent, food and medicine.
              (id:593199294527643648)</td>
              <td style="text-align:left;">Today our volunteers
              distributed food to more than 1500 earthquake
              affected people at #ArtofLiving center at
              Raxaul#NepalEarthquakeRelief
              (id:592757889535737860)</td>
            </tr>
            <tr>
              <td style="text-align:left;">In the coming days,
              people of Nepal need basic essentials such as food,
              clean water and shelter. #earthquake
              (id:596366795931406336)</td>
              <td style="text-align:left;">at bharatpur hospital
              ...really all the earthquake victims are well treated
              with food, clothes, mats, medicines, etc #Earthquake
              #Nepalquake (id:593783416333717504)</td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Discovering
            relevant pairs from i runs</h3>
          </div>
        </header>
        <p>We randomly selected 20 need-tweets from the 427
        need-tweets identified in the Gold Standard for the test
        data in sub-task 1.</p>
        <p>We also applied a total of 8 IR methods: (1) Lucene
        <a class="fn" href="#fn4" id="foot-fn4"><sup>3</sup></a>
        default model (which uses a variant of Tf-idf for scoring),
        (2) Word2vec [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>] vectors pre-trained using a Google
        News dataset, (3) GloVe [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>] vectors trained on a Twitter
        dataset, (4) GloVe vectors trained on a Wikipedia dataset,
        (5) a unigram language model, (6) a bigram language model,
        (7) searching using Lucene after Query Expansion using
        WordNet [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0005">5</a>], (8) searching using Lucene after
        manual relevance feedback.</p>
        <p>For each of the 8 models, for each of the randomly
        selected 20 need-tweets, we output the top five matching
        availability tweets among the availability tweets in the
        test data identified in the gold standard for sub-task 1.
        We manually checked the relevance of each of the resulting
        8*20*5=800 pairs of tweets, and found 327 need-availability
        tweet pairs to be relevant (i.e., the availability-tweet
        mentions the availability of at least one resource
        mentioned as lacking in the corresponding need-tweet,
        according to our judgment), and only 49 of them (i.e., only
        about 15%) were present in the gold standard for sub-task 2
        (as identified by the task's annotators). The number of
        relevant pairs found by each of these 8 methods, along with
        how many of them were present in the gold standard, are
        shown in Table 2.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Number of relevant tweet pairs found by
            each method</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">S. No.</td>
                <td style="text-align:left;">Method</td>
                <td style="text-align:center;">No. of relevant
                pairs found (out of 100)</td>
                <td style="text-align:center;">No. of pairs also in
                GS</td>
                <td style="text-align:center;">Percentage</td>
              </tr>
              <tr>
                <td style="text-align:center;">1</td>
                <td style="text-align:left;">Lucene default
                model</td>
                <td style="text-align:center;">49</td>
                <td style="text-align:center;">10</td>
                <td style="text-align:center;">20.41%</td>
              </tr>
              <tr>
                <td style="text-align:center;">2</td>
                <td style="text-align:left;">Word2vec on Google
                News data</td>
                <td style="text-align:center;">41</td>
                <td style="text-align:center;">7</td>
                <td style="text-align:center;">17.07 %</td>
              </tr>
              <tr>
                <td style="text-align:center;">3</td>
                <td style="text-align:left;">GloVe on Twitter
                data</td>
                <td style="text-align:center;">39</td>
                <td style="text-align:center;">3</td>
                <td style="text-align:center;">7.69 %</td>
              </tr>
              <tr>
                <td style="text-align:center;">4</td>
                <td style="text-align:left;">GloVe on Wikipedia
                data</td>
                <td style="text-align:center;">44</td>
                <td style="text-align:center;">4</td>
                <td style="text-align:center;">9.09 %</td>
              </tr>
              <tr>
                <td style="text-align:center;">5</td>
                <td style="text-align:left;">Unigram LM</td>
                <td style="text-align:center;">29</td>
                <td style="text-align:center;">6</td>
                <td style="text-align:center;">20.69 %</td>
              </tr>
              <tr>
                <td style="text-align:center;">6</td>
                <td style="text-align:left;">Bigram LM</td>
                <td style="text-align:center;">16</td>
                <td style="text-align:center;">1</td>
                <td style="text-align:center;">6.25 %</td>
              </tr>
              <tr>
                <td style="text-align:center;">7</td>
                <td style="text-align:left;">Lucene, with QE using
                WordNet</td>
                <td style="text-align:center;">36</td>
                <td style="text-align:center;">5</td>
                <td style="text-align:center;">13.89 %</td>
              </tr>
              <tr>
                <td style="text-align:center;">8</td>
                <td style="text-align:left;">Lucene, with QE using
                relevance feedback</td>
                <td style="text-align:center;">73</td>
                <td style="text-align:center;">13</td>
                <td style="text-align:center;">17.81 %</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>To analyze how the number of relevant need-availability
        tweet pairs discovered increases with the number of
        participating systems, we performed experiments by taking
        all combinations of the 8 methods, first 1 method at a
        time, then 2 at a time, and so on. The average, minimum,
        and maximum number of distinct pairs discovered by the
        methods when taking <em>i</em>, 1 ≤ <em>i</em> ≤ 8 ,
        methods at a time, are shown in Table 3, and plotted in
        Figure 1.</p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Number of relevant tweet pairs discovered
            by <em>i</em> systems, 1 ≤ <em>i</em> ≤ 8</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">Number of
                methods</td>
                <td style="text-align:center;">Number of</td>
                <td colspan="3" style="text-align:center;">
                  Number of relevant tweet pairs found
                  <hr />
                </td>
              </tr>
              <tr>
                <td style="text-align:left;">taken at a time</td>
                <td style="text-align:left;">combinations</td>
                <td style="text-align:left;">Average</td>
                <td style="text-align:left;">Minimum</td>
                <td style="text-align:left;">Maximum</td>
              </tr>
              <tr>
                <td style="text-align:center;">1</td>
                <td style="text-align:center;">8</td>
                <td style="text-align:center;">41</td>
                <td style="text-align:center;">17</td>
                <td style="text-align:center;">73</td>
              </tr>
              <tr>
                <td style="text-align:center;">2</td>
                <td style="text-align:center;">28</td>
                <td style="text-align:center;">76</td>
                <td style="text-align:center;">46</td>
                <td style="text-align:center;">113</td>
              </tr>
              <tr>
                <td style="text-align:center;">3</td>
                <td style="text-align:center;">56</td>
                <td style="text-align:center;">107</td>
                <td style="text-align:center;">77</td>
                <td style="text-align:center;">140</td>
              </tr>
              <tr>
                <td style="text-align:center;">4</td>
                <td style="text-align:center;">70</td>
                <td style="text-align:center;">134</td>
                <td style="text-align:center;">99</td>
                <td style="text-align:center;">164</td>
              </tr>
              <tr>
                <td style="text-align:center;">5</td>
                <td style="text-align:center;">56</td>
                <td style="text-align:center;">158</td>
                <td style="text-align:center;">126</td>
                <td style="text-align:center;">183</td>
              </tr>
              <tr>
                <td style="text-align:center;">6</td>
                <td style="text-align:center;">28</td>
                <td style="text-align:center;">180</td>
                <td style="text-align:center;">157</td>
                <td style="text-align:center;">199</td>
              </tr>
              <tr>
                <td style="text-align:center;">7</td>
                <td style="text-align:center;">8</td>
                <td style="text-align:center;">199</td>
                <td style="text-align:center;">192</td>
                <td style="text-align:center;">214</td>
              </tr>
              <tr>
                <td style="text-align:center;">8</td>
                <td style="text-align:center;">1</td>
                <td style="text-align:center;">216</td>
                <td style="text-align:center;">216</td>
                <td style="text-align:center;">216</td>
              </tr>
            </tbody>
          </table>
        </div>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3200000/3191622/images/www18companion-361-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">Variation in the no. of
            relevant tweet pairs found by considering different
            number of systems</span>
          </div>
        </figure>
        <p>Since the graph has not saturated for 8 runs,
        extrapolating for 10 run submissions (which was the actual
        number of run submissions in sub-task 2), we can expect a
        significant increase in the number of relevant pairs
        discovered if more runs are added to the pool.</p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Discussion</h2>
        </div>
      </header>
      <p>The final gold standard for sub-task 2 obtained in the
      track does not intuitively seem to be complete enough for a
      reliable ranking of systems other than those which
      contributed to the pool.</p>
      <p>The pooling method failed to find many of the relevant
      need-availability tweet pairs. Some reasons why this happened
      may be:</p>
      <ul class="list-no-style">
        <li id="list6" label="•">There were only 10 runs submitted
        for sub-task 2, with only 4 different kinds of models
        applied (others only differed from these in terms of the
        parameters of the model). Clearly, a pool made from such a
        collection is neither diverse nor large enough to include
        most relevant need-availability tweet pairs. This problem
        also occurred in the corresponding track last year
        [<a class="bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0001">1</a>], as
          reported in [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0008">8</a>].<br />
        </li>
        <li id="list7" label="•">The organizers had set a limit of
        outputting 5 matching availability-tweets for each
        need-tweet by a run submission. In the Microblog Track at
        TREC,<a class="fn" href="#fn5" id=
        "foot-fn5"><sup>4</sup></a> there used to be more than 100
        run submissions, and generally the pooling depth was set as
        100 [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0002">2</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0007">7</a>]. Setting
        a pooling depth of only 5 here, with only 10 run
        submissions, caused the pool to not be reliable. In fact,
        there were more than 100 tweets in the gold standard for
        availability-tweets in sub-task 1 that included the keyword
        ”food”, indicating that there may have been about 100 or
        more availability-tweets that would match any need-tweet
        that mentions the need of food. Furthermore, the P@5 value
        for even the best submissions was close to 0.2, which means
        that, on average, it could retrieve only 1 matching
        availability-tweet for each need-tweet. With only 10
        submissions, we can only expect 10 matching
        availability-tweets for each need-tweet from this pooling
        approach, which falls drastically short of the number of
        potential matches. So, the allowance for the number of
        matching availability-tweets for each need-tweet in a run
        should have been much more liberal, perhaps up to 100
        tweets.<br />
        </li>
      </ul>
      <p>Such a gold standard cannot reliably judge the performance
      of new systems, which may use a model different from the
      participating systems that made up the pool, as evident from
      the results for our methods, for which only about 15% (49 out
      of 327) of the correct need-availability tweet pairs were
      present in the gold standard.</p>
      <p>One way to make pools more robust even with a low number
      of participating systems is for the organizers themselves to
      implement about 10-15 different approaches, and add their
      results to the pool. Another option is to employ continuous
      evaluations [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0009">9</a>],
      rather than working with a static collection of relevance
      judgments. This way, a new system that is able to output many
      relevant tweet pairs that are not in the gold standard would
      not be unfairly ranked low.</p>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion and
          future work</h2>
        </div>
      </header>
      <p>The IRMiDis track at FIRE 2017 involved the development
      and comparison of many IR approaches to the important task of
      identifying and matching need and availability tweets during
      disaster situations. However, based on our experiments, we
      found that the gold standard data obtained in the task was
      substantially incomplete. We discussed some reasons for it,
      as well as some ways to make a more robust gold standard for
      similar tasks in the future.</p>
      <p>Future directions of work include exploring the
      feasibility of different approaches for pooling (like
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>]) to make
      more robust gold standards for future tasks in this
      domain.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Saptarshi Ghosh and
        Kripabandhu Ghosh. 2016. Overview of the FIRE 2016
        Microblog track: Information Extraction from Microblogs
        Posted during Disasters.. In <em><em>FIRE (Working
        Notes)</em></em> . 56–61.</li>
        <li id="BibPLXBIB0002" label="[2]">Jimmy Lin, Miles Efron,
        Yulu Wang, and Garrick Sherman. 2014. <em><em>Overview of
        the trec-2014 microblog track</em></em> . Technical Report.
        MARYLAND UNIV COLLEGE PARK.</li>
        <li id="BibPLXBIB0003" label="[3]">David&nbsp;E Losada,
        Javier Parapar, and Alvaro Barreiro. 2017. Multi-armed
        bandits for adjudicating documents in pooling-based
        evaluation of information retrieval systems.
        <em><em>Information Processing &amp; Management</em></em>
        53, 5 (2017), 1005–1025.</li>
        <li id="BibPLXBIB0004" label="[4]">Tomas Mikolov, Kai Chen,
        Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation
        of word representations in vector space. <em><em>arXiv
        preprint arXiv:1301.3781</em></em> (2013).</li>
        <li id="BibPLXBIB0005" label="[5]">George&nbsp;A Miller.
        1995. WordNet: a lexical database for English.
        <em><em>Commun. ACM</em></em> 38, 11 (1995), 39–41.</li>
        <li id="BibPLXBIB0006" label="[6]">Jeffrey Pennington,
        Richard Socher, and Christopher&nbsp;D. Manning. 2014.
        GloVe: Global Vectors for Word Representation. In
        <em><em>Empirical Methods in Natural Language Processing
        (EMNLP)</em></em> . 1532–1543.
        http://www.aclweb.org/anthology/D14-1162</li>
        <li id="BibPLXBIB0007" label="[7]">Ian Soboroff, Iadh
        Ounis, Craig Macdonald, and Jimmy&nbsp;J Lin. 2012.
        Overview of the TREC-2012 Microblog Track.. In
        <em><em>TREC</em></em> , Vol.&nbsp;2012. 20.</li>
        <li id="BibPLXBIB0008" label="[8]">Ribhav Soni and Sukomal
        Pal. 2017. Microblog Retrieval for Disaster Relief: How To
        Create Ground Truths?. In <em><em>SMERP@ ECIR</em></em> .
        42–51.</li>
        <li id="BibPLXBIB0009" label="[9]">Alberto Tonon, Gianluca
        Demartini, and Philippe Cudré-Mauroux. 2015. Pooling-based
        continuous evaluation of information retrieval systems.
        <em><em>Information Retrieval Journal</em></em> 18, 5
        (2015), 445–472.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>This is the
    corresponding author</p>
    <p id="fn2"><a href="#foot-fn2"><sup>1</sup></a>
    <tt>http://fire.irsi.res.in/fire/2017/home</tt></p>
    <p id="fn3"><a href="#foot-fn3"><sup>2</sup></a>
    <tt>https://en.wikipedia.org/wiki/April_2015_Nepal_earthquake</tt></p>
    <p id="fn4"><a href="#foot-fn4"><sup>3</sup></a><a class=
    "link-inline force-break" href=
    "https://lucene.apache.org/">https://lucene.apache.org/</a></p>
    <p id="fn5"><a href="#foot-fn5"><sup>4</sup></a>
    <tt>http://trec.nist.gov/data/microblog.html</tt></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3191622">https://doi.org/10.1145/3184558.3191622</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
