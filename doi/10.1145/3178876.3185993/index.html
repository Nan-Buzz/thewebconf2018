<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>ROSC: Robust Spectral Clustering on Multi-scale
  Data</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">ROSC: Robust Spectral Clustering
          on Multi-scale Data</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Xiang</span> <span class=
          "surName">Li</span>, The University of Hong Kong,
          Pokfulam Road, Hong Kong, <a href=
          "mailto:xli2@cs.hku.hk">xli2@cs.hku.hk</a>
        </div>
        <div class="author">
          <span class="givenName">Ben</span> <span class=
          "surName">Kao</span>, The University of Hong Kong,
          Pokfulam Road, Hong Kong, <a href=
          "mailto:kao@cs.hku.hk">kao@cs.hku.hk</a>
        </div>
        <div class="author">
          <span class="givenName">Siqiang</span> <span class=
          "surName">Luo</span>, The University of Hong Kong,
          Pokfulam Road, Hong Kong, <a href=
          "mailto:sqluo@cs.hku.hk">sqluo@cs.hku.hk</a>
        </div>
        <div class="author">
          <span class="givenName">Martin</span> <span class=
          "surName">Ester</span>, Simon Fraser University, Burnaby,
          BC, Canada, <a href=
          "mailto:ester@sfu.ca">ester@sfu.ca</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3185993"
        target=
        "_blank">https://doi.org/10.1145/3178876.3185993</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We investigate the effectiveness of spectral
        methods in clustering multi-scale data, which is data whose
        clusters are of various sizes and densities. We review
        existing spectral methods that are designed to handle
        multi-scale data and propose an alternative approach that
        is orthogonal to existing methods. We put forward the
        algorithm ROSC, which computes an affinity matrix that
        takes into account both objects’ feature similarity and
        reachability similarity. We perform extensive experiments
        comparing ROSC against 9 other methods on both real and
        synthetic datasets. Our results show that ROSC performs
        very well against the competitors. In particular, it is
        very robust in that it consistently performs well over all
        the datasets tested. Also, it outperforms others by wide
        margins for datasets that are highly
        multi-scale.</small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Spectral clustering;
          robustness; multi-scale data</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Xiang Li<sup>*</sup>, Ben Kao<sup>*</sup>, Siqiang
          Luo<sup>*</sup>, and Martin Ester<sup>†</sup>. 2018.
          ROSC: Robust Spectral Clustering on Multi-scale Data. In
          <em>WWW 2018: The 2018 Web Conference,</em> <em>April
          23–27, 2018, Lyon, France.</em> ACM, New York, NY, USA,
          10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3185993" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3185993</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-2">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Cluster analysis is a core technique in data mining and
      machine learning. Given a set of objects, the general idea of
      clustering is to group objects that are <em>similar</em> into
      the same clusters and to separate dissimilar objects into
      different clusters. Among existing techniques, <em>spectral
      clustering</em> has been shown to be very effective,
      particularly in the fields of image
      segmentation&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0036">36</a>] and text mining&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0008">8</a>].</p>
      <p>Based on spectral graph theory, spectral clustering
      methods transform the clustering problem into a graph
      partitioning problem. Given a set of <em>n</em> objects
      <span class="inline-equation"><span class="tex">$\mathcal {X}
      = \lbrace x_1, \ldots , x_n\rbrace$</span></span> , and a
      similarity matrix <em>S</em>, such that the matrix entry
      <em>S<sub>ij</sub></em> captures the affinity of objects
      <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> , spectral
      clustering first constructs a weighted graph <span class=
      "inline-equation"><span class="tex">$G=(\mathcal
      {X},S)$</span></span> , where <span class=
      "inline-equation"><span class="tex">$\mathcal
      {X}$</span></span> gives the set of vertices and
      <em>S<sub>ij</sub></em> gives the weight of the edge
      connecting <em>x<sub>i</sub></em> to <em>x<sub>j</sub></em> .
      The graph <em>G</em> is then partitioned with the objective
      of optimizing a criterion that measures the quality of the
      partitioning such as the <em>normalized
      cut</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>].</p>
      <p>Figure&nbsp;<a class="fig" href="#fig1">1</a>(a)
      illustrates the key steps of basic spectral
      clustering<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a>. Given a similarity matrix
      <em>S</em>, we first compute a normalized Laplacian matrix
      <em>L</em> from <em>S</em>. Then, we apply
      eigen-decomposition on <em>L</em> to obtain the <em>k</em>
      smallest eigenvectors<a class="fn" href="#fn2" id=
      "foot-fn2"><sup>2</sup></a>, e<sub>1</sub>, …, e
      <sub><em>k</em></sub> . Let <em>M</em> be an <em>k</em> ×
      <em>n</em> matrix whose <em>i</em>-th row is e
      <sub><em>i</em></sub> . We take the <em>j</em>-th column of
      <em>M</em> as the feature vector of object
      <em>x<sub>j</sub></em> and perform <em>k</em>-means
      clustering on the objects. In a nutshell, spectral clustering
      methods map objects into low dimensional embeddings using the
      <em>k</em> smallest eigenvectors.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The key steps of (a) basic spectral
          clustering; (b) with local scaling and PI; (c)
          ROSC.</span>
        </div>
      </figure>
      <p></p>
      <p>Despite the successes of spectral clustering, previous
      works have pointed out that spectral methods can be adversely
      affected by the presence of <em>multi-scale
      data</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0037">37</a>], which is defined as data whose
      object clusters are of various sizes and densities. As an
      illustrative example, Figure&nbsp;<a class="fig" href=
      "#fig2">2</a>(a) shows a dataset of three clusters: two dense
      rectangular clusters on top of a narrow sparse stripe
      cluster. Figure&nbsp;<a class="fig" href="#fig2">2</a>(b)
      shows the result of applying the standard spectral clustering
      method NJW to the dataset. We see that the stripe cluster is
      segmented into three parts, two of which are incorrectly
      merged with the rectangular clusters. The objective of this
      paper is to address the multi-scale data issue in spectral
      clustering. In particular, we review existing methods for
      handling multi-scale data, provide insight into how the issue
      can be addressed, and put forward our algorithm called ROSC
      which outperforms existing methods in clustering multi-scale
      data.</p>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">(a) A multi-scale dataset, (b) clustering
          by NJW.</span>
        </div>
      </figure>
      <p></p>
      <p>There are two general approaches to address the
      multi-scale data problem: one on scaling the similarity
      matrix <em>S</em> and another on applying the power iteration
      technique to obtain pseudo-eigenvectors that contain rich
      cluster separation information.</p>
      <p>Recall that spectral clustering methods model data objects
      as a graph and perform clustering by graph partitioning. The
      similarity matrix <em>S</em> should therefore capture
      objects’ local neighborhood information. A common choice of
      such a similarity function is the Gaussian kernel:
      <span class="inline-equation"><span class="tex">$S_{ij}= \exp
      \left(-\frac{||_i-_j||^2}{2\sigma ^2}\right)$</span></span> ,
      where x (boldface) denotes a feature vector of an object
      <em>x</em>, and <em>σ</em> is a global scaling parameter. A
      major difficulty in using the Gaussian function to cluster
      multi-scale data lies in the choice of <em>σ</em>. If
      <em>σ</em> is set small, then <em>S<sub>ij</sub></em> will
      become small. Objects in a sparse cluster (which are
      relatively distant among themselves compared with objects in
      a dense cluster) will likely be judged as dissimilar leading
      to partitioning of the cluster. On the other hand, if
      <em>σ</em> is set large, <em>S<sub>ij</sub></em> will be
      large. Hence, nearby dense clusters could be judged similar
      to each other and are inadvertently merged.</p>
      <p>To tackle this problem, the ZP method&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>] applies <em>local
      scaling</em> and modifies the Gaussian similarity to
      <span class="inline-equation"><span class="tex">$S_{ij}= \exp
      \left(-\frac{||_i-_j||^2}{\sigma _i\sigma
      _j}\right)$</span></span> . Here, <em>σ<sub>i</sub></em> (and
      likewise for <em>σ<sub>j</sub></em> ) is a local scaling
      parameter for object <em>x<sub>i</sub></em> . It is defined
      as the distance between <em>x<sub>i</sub></em> and its
      <em>l</em>-th nearest neighbor for some empirically
      determined value <em>l</em>. If <em>x<sub>i</sub></em> is
      located in a sparse cluster, then <em>σ<sub>i</sub></em> will
      be large. This boosts the similarity of
      <em>x<sub>i</sub></em> and its neighboring objects and thus
      avoids the splitting of sparse clusters. Also, if
      <em>x<sub>i</sub></em> is located in a dense cluster,
      <em>σ<sub>i</sub></em> will be small. Objects will have to be
      very close to be considered neighbors. This avoids the
      merging of nearby dense clusters.</p>
      <p>Previous studies&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>] have suggested that employing more
      eigenvectors beyond the <em>k</em> smallest ones can help
      capture more cluster separation information and thus improve
      spectral clustering in handling multi-scale data. A <em>power
      iteration</em> (PI) method&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>] was put forward as an efficient
      method for computing the dominant eigenvector of a matrix. It
      is observed in&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] that one can <em>truncate</em> the
      iteration process to obtain an intermediate
      pseudo-eigenvector v <sub><em>t</em></sub> . It is shown that
      v <sub><em>t</em></sub> represents a weighted linear
      combination of all the eigenvectors and is thus a very
      effective replacement of the <em>k</em> smallest eigenvectors
      typically used in a standard spectral clustering process.
      Figure &nbsp;<a class="fig" href="#fig1">1</a>(b) shows how
      the local-scaling method (green box) and the power iteration
      method (yellow box) are integrated into the basic spectral
      clustering method.</p>
      <p>In this paper we take a different approach to tackle the
      multi-scale data problem. The core idea is to construct an
      <em>n</em> × <em>n</em> coefficient matrix <em>Z</em> such
      that the entry <em>Z<sub>ij</sub></em> <a class="fn" href=
      "#fn3" id="foot-fn3"><sup>3</sup></a> reflects how well an
      object <em>x<sub>i</sub></em> characterizes another object
      <em>x<sub>j</sub></em> . Our objective is to derive such a
      <em>Z</em> with “grouping effect”: if two objects
      <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> are highly
      correlated (and thus should be put into the same cluster),
      then their corresponding coefficient vectors z
      <sub><em>i</em></sub> and z <sub><em>j</em></sub> given in
      <em>Z</em> are similar. The interesting question we address
      is how to find such a <em>Z</em>.</p>
      <p>The main feature of our algorithm ROSC is illustrated by
      the red box shown in Figure &nbsp;<a class="fig" href=
      "#fig1">1</a>(c). Instead of using PI to obtain low
      dimensional embeddings of the objects as input to
      <em>k</em>-means (yellow box in Figure &nbsp;<a class="fig"
      href="#fig1">1</a>(b)), ROSC uses the embeddings to construct
      the matrix <em>Z</em> (upper path in the red box). We note
      that two objects that belong to the same cluster could be
      located at distant far ends of a cluster, their high
      correlation is therefore not expressed properly by a
      distance-based similarity matrix <em>S</em>. To capture the
      high correlation between distant objects in the same cluster,
      we propose to use a transitive <em>K</em> nearest neighbor
      (TKNN) graph (lower path in the red box). Two objects
      <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> are
      connected by an edge in the TKNN graph if there is a sequence
      of objects &lt; <em>x<sub>i</sub></em> , …,
      <em>x<sub>j</sub></em> &gt; such that adjacent objects in the
      sequence are mutual <em>K</em> nearest neighbors of each
      other. We use the TKNN graph to regularize the matrix
      <em>Z</em> so that it possesses the desired grouping effect.
      The matrix <em>Z</em> is then fed to the pipeline of spectral
      clustering, taking the role of <em>S</em>.</p>
      <p>Our main contributions are:</p>
      <p>• We address the multi-scale data problem in spectral
      clustering and propose the ROSC algorithm. ROSC uses PI to
      derive a coefficient matrix <em>Z</em>, which is regularized
      by a TKNN graph. The regularized <em>Z</em> replaces the
      similarity matrix <em>S</em> in the spectral clustering
      process.</p>
      <p>• We mathematically prove that the regularized <em>Z</em>
      possesses the grouping effect. Hence, it is effective in
      improving clustering results.</p>
      <p>• We conduct extensive experiments to evaluate the
      performance of ROSC against 9 other clustering methods. Our
      results show that ROSC performs very well against the
      competitors. In particular, it is very robust in that it
      consistently performs well over all the datasets tested.
      Also, it outperforms others by wide margins for datasets that
      are highly multi-scale.</p>
      <p>The rest of the paper is organized as follows.
      Section&nbsp;<a class="sec" href="#sec-3">2</a> mentions
      related works and describes a number of key spectral
      clustering algorithms. Section&nbsp;<a class="sec" href=
      "#sec-4">3</a> presents the ROSC algorithm.
      Section&nbsp;<a class="sec" href="#sec-10">4</a> describes
      the experiments and presents experimental results. Finally,
      Section&nbsp;<a class="sec" href="#sec-13">5</a> concludes
      the paper.</p>
    </section>
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Works</h2>
        </div>
      </header>
      <p>Spectral clustering is a well-studied topic. For an
      introduction and an analysis of the method,
      see&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0014">14</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>]. There are
      a number of variants of the basic method, some of which
      differ in the way they normalize the graph Laplacian,
      <em>D</em> − <em>S</em>, where <em>D</em> is the diagonal
      matrix with <em>D<sub>ii</sub></em> = ∑ <sub><em>j</em></sub>
      <em>S<sub>ij</sub></em> . For example, the <em>normalized
      cuts</em> (NCuts) method&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>] employs random-walk-based
      normalization <em>D</em> <sup>− 1</sup>(<em>D</em> −
      <em>S</em>) while the <em>Ng-Jordan-Weiss</em> (NJW)
      method&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0026">26</a>]
      uses symmetric normalization <span class=
      "inline-equation"><span class=
      "tex">$D^{-\frac{1}{2}}(D-S)D^{-\frac{1}{2}}$</span></span>
      .</p>
      <p>There are many previous works studying the various aspects
      of spectral clustering techniques. For example, there are
      studies that focus on the performance of spectral clustering
      under different data characteristics&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0032">32</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0038">38</a>], on computational
      efficiency&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>], and on the probabilistic theory of
      the method&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>].</p>
      <p>There are also works that point out the degradation of
      spectral clustering's effectiveness when data is
      multi-scale&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>] or noisy&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0017">17</a>]. To address these
      problems, a number of methods have been
      proposed&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0037">37</a>]. For example, the <em>self-tuning
      spectral clustering</em> method ZP&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>] considers the local
      statistics information of each object and constructs a
      locally scaled similarity matrix as we have described in
      Section&nbsp;<a class="sec" href="#sec-2">1</a>. Moreover, ZP
      estimates the number of clusters by rotating the eigenvectors
      to best align them with a canonical coordinate system. The
      number of clusters that gives the minimal alignment cost is
      then selected.</p>
      <p>Standard spectral clustering uses the so-called “most
      informative” eigenvectors. Heuristically, the <em>k</em>
      smallest eigenvectors are usually taken as the most
      informative ones. However, it is pointed out
      in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0019">19</a>]
      that it is possible that some of these smallest eigenvectors
      correspond to some particularly salient noise in the data,
      while other non-top-<em>k</em>-smallest vectors contain good
      cluster separation information. This observation leads to
      studies of how eigenvectors should be selected in spectral
      clustering. For example, Xiang et al.&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0033">33</a>] enhance spectral
      clustering by measuring the relevance of an eigenvector
      according to how well it can separate data into different
      clusters, particularly in the presence of noise.</p>
      <p>As we mentioned in Section&nbsp;<a class="sec" href=
      "#sec-2">1</a>, power iteration (PI) can be used to find
      pseudo-eigenvectors in spectral clustering. Given a matrix
      <em>W</em>, PI starts with a random vector v<sub>0</sub> ≠ 0
      and iterates:</p>
      <div class="table-responsive" id="Xeq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} {v}_{t+1} =
          \frac{W {v}_t}{||W {v}_t||_1}, \;\;\; t \ge 0.
          \end{equation}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>Suppose <em>W</em> has eigenvalues <em>λ</em>
      <sub>1</sub> &gt; <em>λ</em> <sub>2</sub> &gt; ... &gt;
      <em>λ<sub>n</sub></em> with associated eigenvectors
      e<sub>1</sub>, e<sub>2</sub>, ..., e <sub><em>n</em></sub> .
      We express v<sub>0</sub> as
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} {v}_{0} = c_1
          {e}_1 + c_2 {e}_2 + ... + c_n {e}_n,
          \end{equation}</span><br />
          <span class="equation-number">(2)</span>
        </div>
      </div>for some constants <em>c</em> <sub>1</sub>…,
      <em>c<sub>n</sub></em> . Let <span class=
      "inline-equation"><span class="tex">$R = \prod _{i=0}^{t-1}
      \Vert W{ v}_i \Vert _1$</span></span> , we have,
      <div class="table-responsive" id="eq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} \begin{split} {
          v}_t &amp; = W^t {v}_0 / R\\ &amp; = (c_1 W^t {e}_1 + c_2
          W^t {e}_2 + ... + c_n W^t {e}_n)/R\\ &amp; = (c_1 \lambda
          _1^t {e}_1 + c_2 \lambda _2^t {e}_2 + ... + c_n \lambda
          _n^t {e}_n)/R \\ &amp; = \frac{c_1\lambda _1^t}{R}\left[
          {e}_1 + \frac{c_2}{c_1}\left(\frac{\lambda _2}{\lambda
          _1}\right)^t {e}_2+...+
          \frac{c_n}{c_1}\left(\frac{\lambda _n}{\lambda
          _1}\right)^t {e}_n\right]. \end{split}
          \end{equation}</span><br />
          <span class="equation-number">(3)</span>
        </div>
      </div>v <sub><em>t</em></sub> is thus a linear combination of
      the eigenvectors. Moreover, if v<sub>0</sub> has a nonzero
      component in the direction of e<sub>1</sub> (i.e., <em>c</em>
      <sub>1</sub> ≠ 0), v <sub><em>t</em></sub> converges to a
      scaled version of the dominant eigenvector e<sub>1</sub>.
      <p></p>
      <p>In the context of spectral clustering, we set <em>W</em> =
      <em>D</em> <sup>− 1</sup> <em>S</em>. It is easy to see that
      the <em>k</em> smallest eigenvectors of <em>D</em> <sup>−
      1</sup>(<em>D</em> − <em>S</em>) computed in NCuts are
      equivalent to the <em>k</em> largest eigenvectors of
      <em>W</em>. Lin et al.&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] propose the <em>Power Iteration
      Clustering</em> (PIC) method. PIC employs truncated power
      iteration to obtain an intermediate pseudo-eigenvector v
      <sub><em>t</em></sub> , which is shown to be a weighted
      linear combination of all the eigenvectors. The <em>j</em>-th
      component of v <sub><em>t</em></sub> , v
      <sub><em>t</em></sub> [<em>j</em>], is taken as the feature
      of object <em>x<sub>j</sub></em> . Based on these feature
      values, <em>k</em>-means clustering is applied to cluster
      objects.</p>
      <p>PIC uses PI to obtain one single pseudo-eigenvector from
      which objects’ feature values are extracted. A single
      pseudo-eigenvector, however, is generally not enough when the
      number of clusters is large due to the <em>cluster collision
      problem</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>]. In&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>], the PIC-<em>k</em>
      method is proposed which runs PI multiple times to obtain
      multiple pseudo-eigenvectors. These generated
      pseudo-eigenvectors provide more dimensions of object
      features that are used in the <em>k</em>-means clustering
      step of spectral clustering. These pseudo-eigenvectors,
      however, could be similar to each other and are thus
      redundant. In &nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0029">29</a>], a <em>Deflation-based Power
      Iteration Clustering</em> (DPIC) method is proposed. DPIC
      applies Schur complement to generate multiple orthogonal
      pseudo-eigenvectors to address the redundancy issue. Another
      issue of the PIC method is that the more dominant
      eigenvectors contribute higher weights in the
      pseudo-eigenvector (see Equation&nbsp;<a class="eqn" href=
      "#eq2">3</a>). They thus overshadow the other minor but
      indispensable eigenvectors, especially in the case of
      multi-scale and noisy data. To deal with this problem, Huang
      et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0012">12</a>]
      put forward a <em>Diverse Power Iteration Embedding</em>
      (DPIE) method. In DPIE, when a new pseudo-eigenvector is
      generated, information of other previously obtained
      pseudo-eigenvectors is removed from the new one. Also,
      certain threshold values are wisely set to prevent minor
      eigenvectors from being overshadowed. Recently, a <em>Full
      Spectral Clustering</em> method FUSE&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0035">35</a>] is proposed. It first
      generates <em>p</em> &gt; <em>k</em> pseudo-eigenvectors and
      then uses independent component analysis (ICA) to rotate
      pseudo-eigenvectors so that they become pairwise
      statistically independent. The <em>k</em> rotated
      pseudo-eigenvectors with the most cluster-separation
      information are selected for clustering.</p>
      <p>Our algorithm ROSC differs from these previous works in
      two aspects. First, ROSC uses PI to obtain
      pseudo-eigenvectors not as input to the <em>k</em>-means
      clustering step, but to construct a coefficient matrix
      <em>Z</em> which expresses how one object is characterized by
      other objects. Second, we propose the TKNN graph, which is
      derived from a locally-scaled similarity matrix <em>S</em>.
      The TKNN graph is used to regularize the matrix <em>Z</em> so
      that <em>Z</em> possesses the <em>grouping effect</em>. The
      matrix <em>Z</em> serves as input to the spectral clustering
      pipeline in place of the original similarity matrix
      <em>S</em>. ROSC is thus orthogonal to other existing
      techniques of spectral clustering.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Algorithm</h2>
        </div>
      </header>
      <p>In this section we describe our ROSC algorithm.
      Figure&nbsp;<a class="fig" href="#fig1">1</a>(c) shows a flow
      diagram of ROSC. ROSC follows the basic pipeline of spectral
      clustering except that it generates a coefficient matrix
      <em>Z</em> and feeds it into the clustering pipeline in place
      of the similarity matrix <em>S</em>. The objective is to find
      a <em>Z</em> that possesses grouping effect. To achieve that,
      ROSC uses PI to obtain pseudo-eigenvectors from which a basic
      <em>Z</em> is derived. Next, it generates the TKNN graph with
      which <em>Z</em> is rectified. In the following, we discuss
      how <em>Z</em> is derived from the pseudo-eigenvectors,
      define the TKNN graph, describe the rectification process,
      and prove that the resulting <em>Z</em> has the desired
      grouping effect.</p>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3>Pseudo-Eigenvectors</h3>
          </div>
        </header>
        <p>Given a similarity matrix <em>S</em>, we normalize it by
        <em>D</em> <sup>− 1</sup> <em>S</em> and apply PI to obtain
        pseudo-eigenvectors. Similar to&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>], we run PI multiple
        times with different random initial vectors to generate a
        set of pseudo-eigenvectors, which maps each object into a
        low dimensional embedding. Note that small eigenvectors are
        <em>shrunk</em> by PI&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0019">19</a>]. To alleviate the shrinkage of
        small eigenvectors, we follow the approach
        of&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>] and gradually decrease the number
        of iterations executed in PI as more pseudo-eigenvectors
        are obtained.</p>
        <p>Since the pseudo-eigenvectors approximate the most
        dominant eigenvector, they could be similar. To reduce this
        redundancy, whitening&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>] is used to make the
        pseudo-eigenvectors uncorrelated. Moreover, noise in the
        pseudo-eigenvectors are reduced by a rectification process,
        which will be discussed later.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3>Transitive <em>K</em> Nearest Neighbor (TKNN)
            Graph</h3>
          </div>
        </header>
        <p>Our objective is to capture the high correlations
        between objects that belong to the same cluster even though
        the objects could be located at distant far ends of a
        cluster. These correlations are expressed via a TKNN graph,
        which is used to regularize the coefficient matrix
        <em>Z</em>.</p>
        <div class="definition" id="enc1">
          <label>Definition 3.1.</label>
          <p><strong>(Mutual <em>K</em>-nearest neighbors)</strong>
          Let <em>N<sub>K</sub></em> (<em>x</em>) be the set of
          <em>K</em> nearest neighbors of an object <em>x</em>. Two
          objects <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em>
          are said to be mutual <em>K</em>-nearest neighbors of
          each other, denoted by <em>x<sub>i</sub></em> ∼
          <em>x<sub>j</sub></em> , iff <em>x<sub>i</sub></em> ∈
          <em>N<sub>K</sub></em> (<em>x<sub>j</sub></em> ) and
          <em>x<sub>j</sub></em> ∈ <em>N<sub>K</sub></em>
          (<em>x<sub>i</sub></em> ). <span class=
          "inline-equation"><span class=
          "tex">$\Box$</span></span></p>
        </div>
        <div class="definition" id="enc2">
          <label>Definition 3.2.</label>
          <p><strong>(Reachability)</strong> Two objects
          <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> are
          said to be reachable from each other if there exists a
          sequence of <em>h</em> ≥ 2 objects <span class=
          "inline-equation"><span class="tex">$\lbrace x_i =
          x_{a_1}, \ldots , x_{a_h} = x_j\rbrace$</span></span>
          such that <span class="inline-equation"><span class=
          "tex">$x_{a_r} \sim x_{a_{r+1}}$</span></span> for 1 ≤
          <em>r</em> &lt; <em>h</em>. <span class=
          "inline-equation"><span class=
          "tex">$\Box$</span></span></p>
        </div>
        <div class="definition" id="enc3">
          <label>Definition 3.3.</label>
          <p><strong>(Transitive <em>K</em>-nearest neighbor (TKNN)
          graph)</strong> Given a set of objects <span class=
          "inline-equation"><span class="tex">$\mathcal {X} =
          \lbrace x_1, x_2,..., x_n\rbrace$</span></span> , the
          TKNN graph <span class="inline-equation"><span class=
          "tex">$\mathcal {G}_K = (\mathcal {X},\mathcal
          {E})$</span></span> is an undirected graph where
          <span class="inline-equation"><span class="tex">$\mathcal
          {X}$</span></span> is the set of vertices and
          <span class="inline-equation"><span class="tex">$\mathcal
          {E}$</span></span> is the set of edges. Specifically, the
          edge (<em>x<sub>i</sub></em> , <em>x<sub>j</sub></em> )
          <span class="inline-equation"><span class="tex">$\in
          \mathcal {E}$</span></span> iff <em>x<sub>i</sub></em>
          and <em>x<sub>j</sub></em> are reachable from each other.
          We represent the TKNN graph by an <em>n</em> × <em>n
          reachability matrix</em> <span class=
          "inline-equation"><span class="tex">$\mathcal
          {W}$</span></span> whose (<em>i</em>, <em>j</em>)-entry
          <span class="inline-equation"><span class="tex">$\mathcal
          {W}_{ij} = 1$</span></span> if (<em>x<sub>i</sub></em> ,
          <em>x<sub>j</sub></em> ) <span class=
          "inline-equation"><span class="tex">$\in \mathcal
          {E}$</span></span> ; 0 otherwise. <span class=
          "inline-equation"><span class=
          "tex">$\Box$</span></span></p>
        </div>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3>Coefficient Matrix</h3>
          </div>
        </header>
        <p>The similarity between two objects describes the degree
        to which they share common characteristics. The more
        similar they are, the more likely that one object can be
        represented by the other. Therefore, to depict the
        relationships between objects in multi-scale data, each
        object is linearly characterized by other objects, assuming
        the well-known linear subspace model&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0020">20</a>].</p>
        <p>We generate <em>p</em> pseudo-eigenvectors using PI. Let
        <span class="inline-equation"><span class="tex">$X\in
        \mathcal {R}^{p\times n}$</span></span> be a matrix whose
        rows are the pseudo-eigenvectors. The <em>q</em>-th column
        of <em>X</em> can be taken as a feature vector
        <sub><em>q</em></sub> of an object <em>x<sub>q</sub></em> .
        We normalize the column vectors of <em>X</em> such that
        <span class="inline-equation"><span class="tex">$x_q^T x_q
        = 1 \; \forall 1 \le q \le n$</span></span> . We determine
        a coefficient matrix <span class=
        "inline-equation"><span class="tex">$Z \in \mathcal {R}^{n
        \times n}$</span></span> by<a class="fn" href="#fn4" id=
        "foot-fn4"><sup>4</sup></a></p>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} X = XZ.
            \end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>One can interpret <em>Z<sub>ij</sub></em> as a value
        that reflects how well object <em>x<sub>i</sub></em>
        characterizes object <em>x<sub>j</sub></em> .
        <p></p>
        <p>As indicated by previous works&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>], the generated
        pseudo-eigenvectors are likely to be noise-corrupted. We
        thus extend Equation&nbsp;<a class="eqn" href="#eq3">4</a>
        by introducing a noise matrix <em>O</em>, giving:</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} X = XZ + O.
            \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p></p>
        <p>As we have explained, the TKNN graph conveys useful
        clustering information, bringing highly correlated objects
        that are located at distant far ends of a cluster together.
        We thus use the TKNN graph to regularize matrix <em>Z</em>.
        We derive the following objective function:</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \min _Z
            ||X-XZ||_F^2 + \alpha _1 ||Z||_F^2 + \alpha _2
            ||Z-\mathcal {W}||_F^2, \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>where <em>α</em> <sub>1</sub> &gt; 0, <em>α</em>
        <sub>2</sub> ≥ 0 are two weighting factors that adjust the
        relative weights of the three components that constitute
        the objective function. The objective function consists of
        three terms. The first term aims to reduce the noise matrix
        <em>O</em> (see Equation&nbsp;<a class="eqn" href=
        "#eq4">5</a>), the second term is the Frobenius norm on
        <em>Z</em>, and the third term regularizes <em>Z</em> by
        the TKNN graph. A closed-form solution, <em>Z</em>
        <sup>*</sup>, to the optimization problem is
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} Z^* = (X^TX +
            \alpha _1I + \alpha _2I)^{-1}(X^TX+\alpha _2\mathcal
            {W}). \end{equation}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3>Grouping Effect</h3>
          </div>
        </header>
        <p>For an object <em>x<sub>p</sub></em> , let z
        <sub><em>p</em></sub> be the <em>p</em>-th column of the
        coefficient matrix <em>Z</em>. We interpret the entries of
        z <sub><em>p</em></sub> as the coefficients that express
        <em>x<sub>p</sub></em> as a linear combinations of other
        objects. Previous works&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0011">11</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>] have shown that if <em>Z</em> has
        <em>grouping effect</em>, then performing spectral
        clustering based on <em>Z</em> would be effective.
        Intuitively, <em>Z</em> has grouping effect if, given two
        <em>highly correlated</em> objects <em>x<sub>i</sub></em>
        and <em>x<sub>j</sub></em> , their characterizations of
        other objects are similar. Existing works mostly consider
        <em>high correlation</em> between objects as <em>high
        similarity</em> of their feature vectors. With ROSC, we
        consider object similarity in terms of both feature
        similarity and reachability similarity. Feature similarity
        is measured by the objects’ feature vectors as given by the
        columns of matrix <em>X</em>. Reachability similarity is
        measured by the columns of matrix <span class=
        "inline-equation"><span class="tex">$\mathcal
        {W}$</span></span> , each of which shows the reachability
        of an object to all others. Formally,</p>
        <div class="definition" id="enc4">
          <label>Definition 3.4.</label>
          <p><strong>(Grouping effect)</strong>. Given a set of
          objects <span class="inline-equation"><span class=
          "tex">$\mathcal {X} = \lbrace x_1, x_2,...,
          x_n\rbrace$</span></span> , let w <sub><em>q</em></sub>
          be the <em>q</em>-th column of <span class=
          "inline-equation"><span class="tex">$\mathcal
          {W}$</span></span> . Further, let <em>x<sub>i</sub></em>
          → <em>x<sub>j</sub></em> denote the condition: (1)
          <span class="inline-equation"><span class="tex">$x_i^T
          x_j \rightarrow 1$</span></span> and (2) ‖w
          <sub><em>i</em></sub> − w <sub><em>j</em></sub>
          ‖<sub>2</sub> → 0. A matrix <em>Z</em> is said to have
          grouping effect if</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ x_{i} \rightarrow x_{j}
              \Rightarrow |Z_{ip} - Z_{jp}| \rightarrow 0\; \forall
              1 \le p \le n. \]</span><br />
            </div>
          </div>
          <p></p>
        </div>
        <p>Our next task is to prove that the optimal solution
        <em>Z</em> <sup>*</sup> (as given in
        Equation&nbsp;<a class="eqn" href="#eq6">7</a>) has the
        grouping effect. In the following discussion, we use
        <span class="inline-equation"><span class="tex">${
        z}_q^*$</span></span> to denote the <em>q</em>-th column
        vector of <em>Z</em> <sup>*</sup>.</p>
        <div class="lemma" id="enc5">
          <label>Lemma 3.5.</label>
          <p>Given a set of objects <span class=
          "inline-equation"><span class="tex">$\mathcal
          {X}$</span></span> , the matrix <span class=
          "inline-equation"><span class="tex">$X\in \mathcal
          {R}^{p\times n}$</span></span> that is composed of the
          pseudo-eigenvectors as rows, the reachability matrix
          <span class="inline-equation"><span class="tex">$\mathcal
          {W}$</span></span> , and the optimal soution <em>Z</em>
          <sup>*</sup> of Equation&nbsp;<a class="eqn" href=
          "#eq5">6</a>,</p>
          <div class="table-responsive" id="eq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} Z_{ip}^* =
              \frac{ {x}_i^T({x}_p-X {z}_p^*) + \alpha _2 \mathcal
              {W}_{ip}}{\alpha _1+\alpha _2}, \;\;\; \forall 1 \le
              i, p \le n. \end{equation}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>
          <p></p>
        </div>
        <div class="proof" id="proof1">
          <label>Proof.</label>
          <p>For 1 ≤ <em>p</em> ≤ <em>n</em>, let <span class=
          "inline-equation"><span class="tex">$J({z}_p) = ||
          {x}_p-X {z}_p||_2^2 + \alpha _1 || {z}_p||_2^2 + \alpha
          _2 || {z}_p- {w}_p||_2^2$</span></span> . Since
          <em>Z</em> <sup>*</sup> is the optimal solution of
          Equation&nbsp;<a class="eqn" href="#eq5">6</a>, we have
          <span class="inline-equation"><span class=
          "tex">$\frac{\partial {J}}{\partial {Z}_{ip}}|_{ {z}_p =
          {z}_p^*} = 0\; \forall 1\le i \le n$</span></span> .
          Hence, <span class="inline-equation"><span class=
          "tex">$-2 {x}_i^T({x}_p-X {z}_p^*)+2\alpha
          _1Z_{ip}^*+2\alpha _2(Z_{ip}^*-\mathcal {W}_{ip}) =
          0$</span></span> , which induces Equation&nbsp;<a class=
          "eqn" href="#eq7">8</a>.</p>
        </div>
        <div class="lemma" id="enc6">
          <label>Lemma 3.6.</label>
          <p>∀1 ≤ <em>i</em>, <em>j</em>, <em>p</em> ≤
          <em>n</em>,</p>
          <div class="table-responsive" id="eq8">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              |Z_{ip}^*-Z_{jp}^*| \le \frac{c\sqrt {2(1-r)} +
              \alpha _2|\mathcal {W}_{ip}-\mathcal
              {W}_{jp}|}{\alpha _1+\alpha _2},
              \end{equation}</span><br />
              <span class="equation-number">(9)</span>
            </div>
          </div>where <span class="inline-equation"><span class=
          "tex">$c = \sqrt {1+\alpha _2||
          {w}_p||_2^2}$</span></span> and <span class=
          "inline-equation"><span class="tex">$r = {x}_i^T
          {x}_j$</span></span> .
          <p></p>
        </div>
        <div class="proof" id="proof2">
          <label>Proof.</label>
          <p>From Equation&nbsp;<a class="eqn" href="#eq7">8</a>,
          we have</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation*} \nonumber
              Z_{ip}^*-Z_{jp}^* = \frac{({x}_i^T - {x}_j^T)({x}_p-X
              {z}_p^*) + \alpha _2 (\mathcal {W}_{ip}-\mathcal
              {W}_{jp})}{\alpha _1+\alpha
              _2}.\end{equation*}</span><br />
            </div>
          </div>That implies
          <div class="table-responsive" id="eq9">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \begin{split} |Z_{ip}^*-Z_{jp}^*| &amp; \le
              \frac{|({x}_i^T - {x}_j^T)({x}_p-X {z}_p^*)| + \alpha
              _2 |\mathcal {W}_{ip}-\mathcal {W}_{jp}|}{\alpha
              _1+\alpha _2}\\ &amp; \le \frac{|| {x}_i -
              {x}_j||_2|| {x}_p-X {z}_p^*||_2 + \alpha _2 |\mathcal
              {W}_{ip}-\mathcal {W}_{jp}|}{\alpha _1+\alpha _2}\\
              \end{split} \end{equation}</span><br />
              <span class="equation-number">(10)</span>
            </div>
          </div>
          <p></p>
          <p>Since the column vectors of <em>X</em> are normalized
          (i.e., <span class="inline-equation"><span class=
          "tex">$x_q^T x_q = 1 \; \forall 1 \le q \le
          n$</span></span> ) , we have <span class=
          "inline-equation"><span class="tex">$|| {x}_i - {x}_j||_2
          = \sqrt {2(1-r)}$</span></span> , where <span class=
          "inline-equation"><span class="tex">$r = {x}_i^T
          {x}_j$</span></span> . As <em>Z</em> <sup>*</sup> is the
          optimal solution of Equation&nbsp;<a class="eqn" href=
          "#eq5">6</a>, we have</p>
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation}
              \begin{split} J({z}_p^*) &amp; = || {x}_p-X
              {z}_p^*||_2^2 + \alpha _1 || {z}_p^*||_2^2 + \alpha
              _2 || {z}_p^*- {w}_p||_2^2 \le \\ J({0}) &amp; = ||
              {x}_p||_2^2 + \alpha _2 || {w}_p||_2^2 = 1 + \alpha
              _2 || {w}_p||_2^2. \end{split}
              \end{equation}</span><br />
              <span class="equation-number">(11)</span>
            </div>
          </div>Hence, <span class="inline-equation"><span class=
          "tex">$|| {x}_p-X {z}_p^*||_2 \le \sqrt {1 + \alpha _2 ||
          {w}_p||_2^2} = c$</span></span> . Equation&nbsp;<a class=
          "eqn" href="#eq9">10</a> can be further simplified as
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation*} \nonumber
              |Z_{ip}^*-Z_{jp}^*| \le \frac{c\sqrt {2(1-r)}+ \alpha
              _2 |\mathcal {W}_{ip}-\mathcal {W}_{jp}|}{\alpha
              _1+\alpha _2}.\end{equation*}</span><br />
            </div>
          </div>
          <p></p>
        </div>
        <div class="lemma" id="enc7">
          <label>Lemma 3.7.</label>
          <p>Matrix <em>Z</em> <sup>*</sup> has grouping
          effect.</p>
        </div>
        <div class="proof" id="proof3">
          <label>Proof.</label>
          <p>Given two objects <em>x<sub>i</sub></em> and
          <em>x<sub>j</sub></em> such that <em>x<sub>i</sub></em> →
          <em>x<sub>j</sub></em> , we have, (1) <span class=
          "inline-equation"><span class="tex">$x_i^T x_j
          \rightarrow 1$</span></span> and (2) ||w
          <sub><em>i</em></sub> − w <sub><em>j</em></sub>
          ||<sub>2</sub> → 0. These imply <span class=
          "inline-equation"><span class="tex">$r = x_i^T x_j
          \rightarrow 1$</span></span> and <span class=
          "inline-equation"><span class="tex">$|\mathcal
          {W}_{ip}-\mathcal {W}_{jp}| \rightarrow 0$</span></span>
          . Hence, the two terms of the numerator of the R.H.S of
          Equation&nbsp;<a class="eqn" href="#eq8">9</a> are close
          to 0. Therefore, <span class=
          "inline-equation"><span class="tex">$|Z_{ip}^*-Z_{jp}^*|
          \rightarrow 0$</span></span> and thus <em>Z</em>
          <sup>*</sup> has grouping effect.</p>
        </div>
        <p>Indeed, Equation&nbsp;<a class="eqn" href="#eq8">9</a>
        shows how our algorithm ROSC enhances the effectiveness of
        spectral clustering on multi-scale data. Comparing with
        traditional approaches, which focus on feature similarity,
        ROSC uses <em>Z</em> <sup>*</sup> to integrate feature
        similarity (<em>r</em>) with reachability similarity
        (<span class="inline-equation"><span class="tex">$|\mathcal
        {W}_{ip}-\mathcal {W}_{jp}|$</span></span> ). In
        particular, two distant objects <em>x<sub>i</sub></em> and
        <em>x<sub>j</sub></em> of a cluster may not share a strong
        feature similarity. This leads to a small <em>r</em> and
        traditional approaches will likely put them into separate
        clusters. On the contrary, ROSC considers the strong
        reachability of the objects to derive a small value of
        <span class="inline-equation"><span class="tex">$|\mathcal
        {W}_{ip}-\mathcal {W}_{jp}|$</span></span> , and thus
        keeping them in the same cluster. Moreover, for
        <em>x<sub>i</sub></em> and <em>x<sub>j</sub></em> that
        belong to two different dense clusters but happen to be
        close in space (i.e., <em>x<sub>i</sub></em> and
        <em>x<sub>j</sub></em> have strong feature similarity),
        traditional approaches may inadvertently merge them into
        the same cluster. ROSC, however, would discover their low
        reachability (via the mutual-KNN relation) and derive a
        large value of <span class="inline-equation"><span class=
        "tex">$|\mathcal {W}_{ip}-\mathcal {W}_{jp}|$</span></span>
        . This regulates matrix <em>Z</em> <sup>*</sup> and avoids
        the incorrect merging. As we will see in the next section,
        ROSC's approach greatly improves clustering quality and is
        more robust than other algorithms in handling multi-scale
        data.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3>ROSC: Robust Spectral Clustering</h3>
          </div>
        </header>
        <p>We note that the matrix <em>Z</em> <sup>*</sup> obtained
        may be asymmetric and it may contain negative values. To
        construct a matrix of object similarity, a common
        fix&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>] is to compute <span class=
        "inline-equation"><span class="tex">$\tilde{Z} =
        (|Z^*|+|(Z^*)^T|)/2$</span></span> . After <span class=
        "inline-equation"><span class=
        "tex">$\tilde{Z}$</span></span> is computed, ROSC executes
        a standard spectral clustering method (e.g., NCuts) using
        <span class="inline-equation"><span class=
        "tex">$\tilde{Z}$</span></span> as the similarity matrix in
        place of <em>S</em>. It can be proved that |<em>Z</em>
        <sup>*</sup>|, |(<em>Z</em> <sup>*</sup>)
        <sup><em>T</em></sup> |, and hence <span class=
        "inline-equation"><span class=
        "tex">$\tilde{Z}$</span></span> all have grouping effect.
        Due to space limitations, readers are referred
        to&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0002">2</a>]
        for the proofs. Finally, ROSC is summarized in
        Algorithm&nbsp;1 .</p>
        <p><img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-img1.svg"
        class="img-responsive" alt="" longdesc="" /></p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Experiment</h2>
        </div>
      </header>
      <p>We conducted extensive experiments to evaluate the
      performance of ROSC. This section summarizes our results. We
      compare the various methods using three popular measures,
      namely, <em>purity</em>, <em>adjusted mutual information
      (AMI)</em>, and <em>rand index (RI)</em>. These measures
      evaluate clustering quality and their values range from 0 to
      1, with a larger value indicating a better clustering
      quality. Readers are referred to&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0019">19</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0030">30</a>] for definitions of the
      three measures.</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Algorithms
            for comparison</h3>
          </div>
        </header>
        <p>We evaluate ROSC and 9 other methods. These methods are
        grouped into the following four categories.</p>
        <p>•<strong>(Standard spectral clustering
        methods)</strong>: <strong>NCuts</strong> and
        <strong>NJW</strong> are two standard methods we evaluate
        against ROSC.</p>
        <p>•<strong>(Power iteration (PI) based methods)</strong>:
        This group includes <strong>PIC</strong>,
        <strong>PIC-<em>k</em></strong> , <strong>DPIC</strong>,
        and <strong>DPIE</strong>. They were described in
        Section&nbsp;<a class="sec" href="#sec-3">2</a>.</p>
        <p>•<strong>(Multi-scale-data-oriented methods)</strong>:
        This group includes <strong>ZP</strong> and
        <strong>FUSE</strong>, which are designed to specifically
        handle multi-scale data. They were also discussed in
        Section&nbsp;<a class="sec" href="#sec-3">2</a>. Note that
        ZP automatically estimates the number of clusters. For
        fairness, we modify ZP so that it returns <em>k</em> (the
        number of true) clusters.</p>
        <p>•<strong>(ROSC variants)</strong>: <strong>ROSC</strong>
        regularizes matrix <em>Z</em> using the reachability
        matrix. This is achieved via the term <span class=
        "inline-equation"><span class="tex">$||Z-\mathcal
        {W}||_F^2$</span></span> in Equation&nbsp;<a class="eqn"
        href="#eq5">6</a>. To study the regularization effect, we
        modify ROSC with this term removed. Specifically,
        <strong>ROSC-R</strong> (reads “ROSC minus reachability”)
        is ROSC without the reachability component.</p>
        <p><strong>[Experiment settings]</strong> The parameters of
        all the methods are set according to their original papers.
        For all the datasets, we use Euclidean distance of objects’
        attributes to derive <em>S</em>, which are locally scaled
        based on ZP's local scaling procedure. For ROSC, we set
        <em>α</em> <sub>1</sub> = 1.0, <em>α</em> <sub>2</sub> =
        0.01, and we set <em>K</em> = 4 in constructing TKNN
        graphs. All methods adopt <em>k</em>-means as the last step
        to return clustering results. For this step, we run
        <em>k</em>-means 100 times with random starting centroids
        and the most frequent cluster assignment is
        used&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0019">19</a>]. For ROSC, we generate <em>k</em>
        pseudo-eigenvectors with random starting vectors as is done
        in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0029">29</a>]. For each method and dataset, we
        run the experiment 50 times and report average results.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Performance
            results</h3>
          </div>
        </header>
        <p>We first use two synthetic datasets to visually
        illustrate the performance of the various methods. After
        that, we use 7 real datasets to perform an in-depth
        analysis of the algorithms.</p>
        <p><strong>[Synthetic datasets]</strong>The synthetic
        datasets are designed to represent very difficult cases of
        clustering, with a highlight on multi-scale data.
        Figure&nbsp;<a class="fig" href="#fig3">3</a>(a) shows the
        synthetic dataset <font style=
        "font-variant: small-caps">Syn1</font>, which consists of
        three clusters of different sizes and densities.
        Specifically, there is a medium-sized sparse rectangular
        cluster (blue) sandwiched by a small dense circular cluster
        (magenta) and a large dense rectangular one (yellow). These
        clusters are physically very close to each other. In
        particular, the distance between two objects of different
        clusters could be smaller than the distance of two objects
        that belong to the same cluster.</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Clustering results on
            <font style=
            "font-variant: small-caps">Syn1</font>.</span>
          </div>
        </figure>
        <p></p>
        <p>We apply all 10 methods on <font style=
        "font-variant: small-caps">Syn1</font>. Due to space
        limitations, for each category of methods, we only show the
        best performing one. They are, namely, NJW, PIC-<em>k</em>,
        FUSE, and ROSC. Their clustering results are shown in
        Figures&nbsp;<a class="fig" href="#fig3">3</a>(b)-(e),
        respectively. From the figures, we see that both NJW and
        PIC-<em>k</em> can identify the small circular magenta
        cluster. However, the blue and the yellow rectangular
        clusters are chopped into halves, and these halves are
        incorrectly merged. FUSE fails for this dataset as well. In
        particular, about 1/3 of the yellow cluster is merged with
        the blue cluster, while about half of the blue cluster is
        merged with the magenta cluster. Among the four methods,
        ROSC is the only one that can recover the yellow cluster.
        This indicates the effectiveness of the TKNN graph in
        correlating objects that are at the far ends of the big
        elongated cluster. Moreover, although ROSC does not recover
        the complete blue cluster, a majority fraction of the blue
        objects are clustered by ROSC as a single group. In
        contrast, for the other three methods, all blue objects are
        either merged with those of the magenta cluster or those of
        the yellow cluster.</p>
        <p>Figure&nbsp;<a class="fig" href="#fig4">4</a>(a) shows
        another synthetic dataset, <font style=
        "font-variant: small-caps">Syn2</font>, which consists of a
        sparse ring cluster (green) and two dense circular clusters
        (red and blue). The best performing methods of the 4
        categories are NJW, PIC-<em>k</em>, ZP, and ROSC. Their
        clustering results are shown in Figures&nbsp;<a class="fig"
        href="#fig4">4</a>(b)-(e), respectively. From the figures,
        we see that the dataset is a very difficult case for
        existing methods. For example, with NJW and ZP, the green
        ring cluster is partitioned into three segments, two of
        which are merged incorrectly with the circular clusters. A
        similar situation is also seen for PIC-<em>k</em>. In
        contrast, ROSC is the only method that can recover almost
        the entire green cluster. It is also able to correctly
        identify the two circular clusters except for a small
        number of objects on the green ring.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Clustering results on
            <font style=
            "font-variant: small-caps">Syn2</font>.</span>
          </div>
        </figure>
        <p></p>
        <p>Tables&nbsp;<a class="tbl" href="#tab1">1</a> and
        <a class="tbl" href="#tab2">2</a> show the purity, AMI and
        RI scores of all 10 methods for the datasets <font style=
        "font-variant: small-caps">Syn1</font> and <font style=
        "font-variant: small-caps">Syn2</font>, respectively. From
        the tables, we see that the scores of ROSC are all much
        larger than those of the other methods. It thus
        significantly outperforms the others for these two
        difficult cases. The two tables also show the scores of
        ROSC-R, which is ROSC without using the TKNN graph or the
        reachability matrix for regularization. Considering the
        wide gaps between the scores of ROSC and ROSC-R, we see a
        strong positive effect of the reachability regularization.
        The use of TKNN graph and reachability has a significant
        effect in the two synthetic datasets because both datasets
        consist of large elongated clusters (e.g., the green ring
        in <font style="font-variant: small-caps">syn2</font>).
        Objects in these clusters are at far distances from each
        other and their correlations are effectively captured by
        the reachability matrix that ROSC employs.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Purity, AMI, and RI scores of methods for
            dataset <font style=
            "font-variant: small-caps">Syn1</font>.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Measure</th>
                <th style="text-align:center;">NJW</th>
                <th style="text-align:center;">NCuts</th>
                <th style="text-align:center;">PIC</th>
                <th style="text-align:center;">PIC-<em>k</em></th>
                <th style="text-align:center;">DPIC</th>
                <th style="text-align:center;">DPIE</th>
                <th style="text-align:center;">ZP</th>
                <th style="text-align:center;">FUSE</th>
                <th style="text-align:center;">ROSC-R</th>
                <th style="text-align:center;">ROSC</th>
              </tr>
            </thead>
            <thead></thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Purity</td>
                <td style="text-align:center;">0.8000</td>
                <td style="text-align:center;">0.8000</td>
                <td style="text-align:center;">0.7262</td>
                <td style="text-align:center;">0.7772</td>
                <td style="text-align:center;">0.6663</td>
                <td style="text-align:center;">0.6348</td>
                <td style="text-align:center;">0.8000</td>
                <td style="text-align:center;">0.7688</td>
                <td style="text-align:center;">0.7594</td>
                <td style="text-align:center;">0.8538</td>
              </tr>
              <tr>
                <td style="text-align:center;">AMI</td>
                <td style="text-align:center;">0.4338</td>
                <td style="text-align:center;">0.4256</td>
                <td style="text-align:center;">0.3941</td>
                <td style="text-align:center;">0.4686</td>
                <td style="text-align:center;">0.2502</td>
                <td style="text-align:center;">0.1381</td>
                <td style="text-align:center;">0.4232</td>
                <td style="text-align:center;">0.4873</td>
                <td style="text-align:center;">0.4331</td>
                <td style="text-align:center;">0.6255</td>
              </tr>
              <tr>
                <td style="text-align:center;">RI</td>
                <td style="text-align:center;">0.6811</td>
                <td style="text-align:center;">0.6817</td>
                <td style="text-align:center;">0.6513</td>
                <td style="text-align:center;">0.6901</td>
                <td style="text-align:center;">0.5786</td>
                <td style="text-align:center;">0.4707</td>
                <td style="text-align:center;">0.6812</td>
                <td style="text-align:center;">0.6837</td>
                <td style="text-align:center;">0.6762</td>
                <td style="text-align:center;">0.8354</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Purity, AMI, and RI scores of methods for
            dataset <font style=
            "font-variant: small-caps">Syn2</font>.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Measure</th>
                <th style="text-align:center;">NJW</th>
                <th style="text-align:center;">NCuts</th>
                <th style="text-align:center;">PIC</th>
                <th style="text-align:center;">PIC-<em>k</em></th>
                <th style="text-align:center;">DPIC</th>
                <th style="text-align:center;">DPIE</th>
                <th style="text-align:center;">ZP</th>
                <th style="text-align:center;">FUSE</th>
                <th style="text-align:center;">ROSC-R</th>
                <th style="text-align:center;">ROSC</th>
              </tr>
            </thead>
            <thead></thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Purity</td>
                <td style="text-align:center;">0.6775</td>
                <td style="text-align:center;">0.6775</td>
                <td style="text-align:center;">0.6541</td>
                <td style="text-align:center;">0.6849</td>
                <td style="text-align:center;">0.5196</td>
                <td style="text-align:center;">0.5171</td>
                <td style="text-align:center;">0.6875</td>
                <td style="text-align:center;">0.6773</td>
                <td style="text-align:center;">0.7002</td>
                <td style="text-align:center;">0.8359</td>
              </tr>
              <tr>
                <td style="text-align:center;">AMI</td>
                <td style="text-align:center;">0.4681</td>
                <td style="text-align:center;">0.4679</td>
                <td style="text-align:center;">0.4157</td>
                <td style="text-align:center;">0.4740</td>
                <td style="text-align:center;">0.2136</td>
                <td style="text-align:center;">0.1320</td>
                <td style="text-align:center;">0.4746</td>
                <td style="text-align:center;">0.4554</td>
                <td style="text-align:center;">0.4602</td>
                <td style="text-align:center;">0.6178</td>
              </tr>
              <tr>
                <td style="text-align:center;">RI</td>
                <td style="text-align:center;">0.6725</td>
                <td style="text-align:center;">0.6724</td>
                <td style="text-align:center;">0.6477</td>
                <td style="text-align:center;">0.6789</td>
                <td style="text-align:center;">0.4866</td>
                <td style="text-align:center;">0.4392</td>
                <td style="text-align:center;">0.6780</td>
                <td style="text-align:center;">0.6683</td>
                <td style="text-align:center;">0.6909</td>
                <td style="text-align:center;">0.8056</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>We further investigate how the various methods deal with
        multi-scale data by varying the sizes and densities of some
        clusters in the synthetic datasets. Here, we show some
        representative results. Specifically, we consider the
        middle sparse blue cluster in <font style=
        "font-variant: small-caps">Syn1</font>
        (Figure&nbsp;<a class="fig" href="#fig3">3</a>(a)) and make
        two changes: (1) increase its density while keeping its
        size unchanged, and (2) increase its size while maintaining
        its density unchanged. We use <em>Δd</em> to denote the
        density change (e.g., <em>Δd</em> = 100% means that the
        density of the cluster is doubled). We change the size of
        the cluster by changing its length (enlarging the cluster
        sideway), while the height is kept unchanged. We use
        <em>Δs</em> to denote the size change (e.g., <em>Δs</em> =
        50% means that the cluster is 1.5 times wider than the one
        shown in Figure&nbsp;<a class="fig" href="#fig3">3</a>(a)).
        We make similar changes to <font style=
        "font-variant: small-caps">Syn2</font> by modifying the
        density and size of the ring cluster. Specifically, we
        gradually reduce the size of the ring from a whole ring (◯,
        <em>Δs</em> = 0%) to a lower half ring (⌣, <em>Δs</em> =
        -50%).</p>
        <p>We show the performance scores of the 10 methods as we
        apply the changes in density and size to the clusters in
        <font style="font-variant: small-caps">Syn1</font>
        (Figures&nbsp;<a class="fig" href="#fig5">5</a> and
        <a class="fig" href="#fig6">6</a>) and <font style=
        "font-variant: small-caps">Syn2</font>
        (Figures&nbsp;<a class="fig" href="#fig7">7</a> and
        <a class="fig" href="#fig4">8</a>). From the figures, we
        see that ROSC gives the best and the most stable
        performance among all the methods over the whole spectrum
        of test cases. The performance gaps between ROSC and other
        competitors are also sizable. This shows that ROSC is very
        robust in dealing with multi-scale data of various sizes
        and densities.</p>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Results vs. varying blue
            cluster's density in <font style=
            "font-variant: small-caps">Syn1</font>.</span>
          </div>
        </figure>
        <figure id="fig6">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig6.jpg"
          class="img-responsive" alt="Figure 6" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 6:</span>
            <span class="figure-title">Results vs. varying blue
            cluster's size in <font style=
            "font-variant: small-caps">Syn1</font>.</span>
          </div>
        </figure>
        <figure id="fig7">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig7.jpg"
          class="img-responsive" alt="Figure 7" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 7:</span>
            <span class="figure-title">Results vs. varying green
            cluster's in <font style=
            "font-variant: small-caps">Syn2</font>.</span>
          </div>
        </figure>
        <figure id="fig8">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig8.jpg"
          class="img-responsive" alt="Figure 8" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 8:</span>
            <span class="figure-title">Results vs. varying green
            cluster's size in <font style=
            "font-variant: small-caps">Syn2</font>.</span>
          </div>
        </figure><strong>[Real datasets]</strong>We further
        evaluate the methods using 7 real datasets. They are:
        <em>MNIST0127</em> (hand-written digit images),
        <em>COIL20</em> (images), <em>Yale_5</em> <font style=
        "font-variant: small-caps">class</font> (facial images),
        <em>isolet_5</em> <font style=
        "font-variant: small-caps">class</font> (speech, UCI
        repository), <em>seg_7</em> <font style=
        "font-variant: small-caps">class</font> (images, UCI
        repository), <em>Yeast_4</em> <font style=
        "font-variant: small-caps">class</font> (biological data,
        UCI repository), <em>glass</em> (UCI repository).
        Table&nbsp;<a class="tbl" href="#tab3">3</a> summarizes
        these datasets. For each dataset, we show the number of
        objects, the number of dimensions, and the number of
        clusters. Also, to show whether a dataset is multi-scale or
        not, we measure the <em>size</em> and <em>density</em> of
        each gold-standard cluster in each dataset. Specifically,
        for each cluster, we find the largest distance of any
        object-pair of the cluster. This distance is taken as the
        <em>diameter</em> of cluster, reflecting how big in size
        the cluster is. Moreover, for each cluster, we find the
        average distance, <em>ρ</em>, of all object-pairs of the
        cluster. Then, we use 1/<em>ρ</em> as a measure of density.
        These cluster sizes and densities are shown in bar graphs
        in Table&nbsp;<a class="tbl" href="#tab3">3</a>. The sizes
        (densities) shown are all normalized by the size (density)
        of the biggest (densest) cluster of the corresponding
        dataset to the range [0, 1]. Intuitively, the more
        variations in the bars of a graph indicate the more
        multi-scale the corresponding dataset is.
        <p></p>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Statistics of 7 real datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">#objects</th>
                <th style="text-align:center;">#dimensions</th>
                <th style="text-align:center;">#clusters</th>
                <th style="text-align:center;">size</th>
                <th style="text-align:center;">density</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">COIL20</td>
                <td style="text-align:center;">1440</td>
                <td style="text-align:center;">1024</td>
                <td style="text-align:center;">20</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig16.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/COIL20_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
              <tr>
                <td style="text-align:center;">seg_7<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">210</td>
                <td style="text-align:center;">19</td>
                <td style="text-align:center;">7</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig17.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/seg_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
              <tr>
                <td style="text-align:center;">glass</td>
                <td style="text-align:center;">214</td>
                <td style="text-align:center;">9</td>
                <td style="text-align:center;">6</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig18.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/glass_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
              <tr>
                <td style="text-align:center;">MNIST0127</td>
                <td style="text-align:center;">1666</td>
                <td style="text-align:center;">784</td>
                <td style="text-align:center;">4</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig19.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/mnist_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
              <tr>
                <td style="text-align:center;">isolet_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">300</td>
                <td style="text-align:center;">617</td>
                <td style="text-align:center;">5</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig20.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/isolet_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
              <tr>
                <td style="text-align:center;">Yeast_4<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">1299</td>
                <td style="text-align:center;">8</td>
                <td style="text-align:center;">4</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig21.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/yeast_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
              <tr>
                <td style="text-align:center;">Yale_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">55</td>
                <td style="text-align:center;">1024</td>
                <td style="text-align:center;">5</td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig22.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
                <td style="text-align:center;"><img src=
                "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/yale_den.jpg"
                class="img-responsive" alt="" longdesc="" /></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>From Table&nbsp;<a class="tbl" href="#tab3">3</a>, we
        see the dataset <em>COIL20</em> is highly multi-scale. For
        example, cluster 2 (the largest cluster) is 5.6 times
        larger than cluster 15 (the smallest cluster). However,
        cluster 15 is 5.9 times denser than cluster 2. The dataset
        <em>glass</em> is moderately multi-scale, and
        <em>Yale_5</em> <font style=
        "font-variant: small-caps">class</font> is relatively
        uniform. The dataset <em>seg_7</em> <font style=
        "font-variant: small-caps">class</font> is interesting
        because it has one very big and sparse cluster (cluster 3);
        the other 6 clusters are quite uniform.</p>
        <p>Tables&nbsp;<a class="tbl" href=
        "#tab4">4</a>,&nbsp;<a class="tbl" href="#tab5">5</a>
        and&nbsp;<a class="tbl" href="#tab6">6</a> show the purity,
        AMI, and RI scores of the 10 methods when they are applied
        to the 7 real datasets. Each row in the table corresponds
        to a (measure, dataset) combination — or a contest among
        the 10 methods. There are thus 21 (3 measures × 7 datasets)
        contests. For each contest, the winner's score is shown in
        bold type. For ROSC, its ranking in each contest is given
        in the bracket next to its score. The performance of a
        method can be judged by the column under the method
        spanning the three tables. From the tables, we make several
        observations:</p>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Purity scores, real datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">NJW</th>
                <th style="text-align:center;">NCuts</th>
                <th style="text-align:center;">PIC</th>
                <th style="text-align:center;">PIC-<em>k</em></th>
                <th style="text-align:center;">DPIC</th>
                <th style="text-align:center;">DPIE</th>
                <th style="text-align:center;">ZP</th>
                <th style="text-align:center;">FUSE</th>
                <th style="text-align:center;">ROSC-R</th>
                <th style="text-align:center;">ROSC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">COIL20</td>
                <td style="text-align:center;">0.4115</td>
                <td style="text-align:center;">0.3926</td>
                <td style="text-align:center;">0.2801</td>
                <td style="text-align:center;">0.2801</td>
                <td style="text-align:center;">0.2361</td>
                <td style="text-align:center;">0.3496</td>
                <td style="text-align:center;">0.5028</td>
                <td style="text-align:center;">0.4177</td>
                <td style="text-align:center;">0.4715</td>
                <td style="text-align:center;">0.9398 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">seg_7<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.5608</td>
                <td style="text-align:center;">0.5403</td>
                <td style="text-align:center;">0.3483</td>
                <td style="text-align:center;">0.3566</td>
                <td style="text-align:center;">0.3000</td>
                <td style="text-align:center;">0.4756</td>
                <td style="text-align:center;">0.5143</td>
                <td style="text-align:center;">0.5912</td>
                <td style="text-align:center;">0.6209</td>
                <td style="text-align:center;">0.6636 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">glass</td>
                <td style="text-align:center;">0.5234</td>
                <td style="text-align:center;">0.5187</td>
                <td style="text-align:center;">0.4976</td>
                <td style="text-align:center;">0.5029</td>
                <td style="text-align:center;">0.5245</td>
                <td style="text-align:center;">0.5158</td>
                <td style="text-align:center;">0.5374</td>
                <td style="text-align:center;">0.5390</td>
                <td style="text-align:center;">0.5748</td>
                <td style="text-align:center;">0.5760 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">MNIST0127</td>
                <td style="text-align:center;">0.5066</td>
                <td style="text-align:center;">0.4970</td>
                <td style="text-align:center;">0.4975</td>
                <td style="text-align:center;">0.4924</td>
                <td style="text-align:center;">0.5898</td>
                <td style="text-align:center;">0.4395</td>
                <td style="text-align:center;">0.5066</td>
                <td style="text-align:center;">0.6436</td>
                <td style="text-align:center;">0.6649</td>
                <td style="text-align:center;">0.6666 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">isolet_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.8120</td>
                <td style="text-align:center;">0.7967</td>
                <td style="text-align:center;">0.5863</td>
                <td style="text-align:center;">0.5867</td>
                <td style="text-align:center;">0.3033</td>
                <td style="text-align:center;">0.8572</td>
                <td style="text-align:center;">0.7767</td>
                <td style="text-align:center;">0.7825</td>
                <td style="text-align:center;">0.8495</td>
                <td style="text-align:center;">0.8179 (3)</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yeast_4<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.4819</td>
                <td style="text-align:center;">0.4665</td>
                <td style="text-align:center;">0.4428</td>
                <td style="text-align:center;">0.4557</td>
                <td style="text-align:center;">0.3831</td>
                <td style="text-align:center;">0.4671</td>
                <td style="text-align:center;">0.4819</td>
                <td style="text-align:center;">0.4999</td>
                <td style="text-align:center;">0.4877</td>
                <td style="text-align:center;">0.4933 (2)</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yale_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.5273</td>
                <td style="text-align:center;">0.5091</td>
                <td style="text-align:center;">0.4516</td>
                <td style="text-align:center;">0.4596</td>
                <td style="text-align:center;">0.4000</td>
                <td style="text-align:center;">0.5225</td>
                <td style="text-align:center;">0.5091</td>
                <td style="text-align:center;">0.5458</td>
                <td style="text-align:center;">0.5295</td>
                <td style="text-align:center;">0.5455 (2)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">AMI scores, real datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">NJW</th>
                <th style="text-align:center;">NCuts</th>
                <th style="text-align:center;">PIC</th>
                <th style="text-align:center;">PIC-<em>k</em></th>
                <th style="text-align:center;">DPIC</th>
                <th style="text-align:center;">DPIE</th>
                <th style="text-align:center;">ZP</th>
                <th style="text-align:center;">FUSE</th>
                <th style="text-align:center;">ROSC-R</th>
                <th style="text-align:center;">ROSC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">COIL20</td>
                <td style="text-align:center;">0.4718</td>
                <td style="text-align:center;">0.4258</td>
                <td style="text-align:center;">0.2989</td>
                <td style="text-align:center;">0.2781</td>
                <td style="text-align:center;">0.2507</td>
                <td style="text-align:center;">0.3642</td>
                <td style="text-align:center;">0.5702</td>
                <td style="text-align:center;">0.4448</td>
                <td style="text-align:center;">0.5291</td>
                <td style="text-align:center;">0.9682 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">seg_7<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.5043</td>
                <td style="text-align:center;">0.4603</td>
                <td style="text-align:center;">0.2339</td>
                <td style="text-align:center;">0.2385</td>
                <td style="text-align:center;">0.0915</td>
                <td style="text-align:center;">0.3954</td>
                <td style="text-align:center;">0.4298</td>
                <td style="text-align:center;">0.5049</td>
                <td style="text-align:center;">0.5255</td>
                <td style="text-align:center;">0.5730 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">glass</td>
                <td style="text-align:center;">0.3469</td>
                <td style="text-align:center;">0.3465</td>
                <td style="text-align:center;">0.3162</td>
                <td style="text-align:center;">0.3193</td>
                <td style="text-align:center;">0.2807</td>
                <td style="text-align:center;">0.2683</td>
                <td style="text-align:center;">0.3426</td>
                <td style="text-align:center;">0.2589</td>
                <td style="text-align:center;">0.3137</td>
                <td style="text-align:center;">0.3204 (4)</td>
              </tr>
              <tr>
                <td style="text-align:center;">MNIST0127</td>
                <td style="text-align:center;">0.4353</td>
                <td style="text-align:center;">0.4241</td>
                <td style="text-align:center;">0.3623</td>
                <td style="text-align:center;">0.3822</td>
                <td style="text-align:center;">0.3714</td>
                <td style="text-align:center;">0.2059</td>
                <td style="text-align:center;">0.4219</td>
                <td style="text-align:center;">0.4125</td>
                <td style="text-align:center;">0.4920</td>
                <td style="text-align:center;">0.4826 (2)</td>
              </tr>
              <tr>
                <td style="text-align:center;">isolet_<font style=
                "font-variant: small-caps">5class</font></td>
                <td style="text-align:center;">0.7595</td>
                <td style="text-align:center;">0.7204</td>
                <td style="text-align:center;">0.5280</td>
                <td style="text-align:center;">0.5292</td>
                <td style="text-align:center;">0.0489</td>
                <td style="text-align:center;">0.7481</td>
                <td style="text-align:center;">0.7379</td>
                <td style="text-align:center;">0.6516</td>
                <td style="text-align:center;">0.7356</td>
                <td style="text-align:center;">0.7524 (2)</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yeast_4<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.1173</td>
                <td style="text-align:center;">0.1052</td>
                <td style="text-align:center;">0.1081</td>
                <td style="text-align:center;">0.1165</td>
                <td style="text-align:center;">0.0214</td>
                <td style="text-align:center;">0.1318</td>
                <td style="text-align:center;">0.1138</td>
                <td style="text-align:center;">0.1816</td>
                <td style="text-align:center;">0.1503</td>
                <td style="text-align:center;">0.1582 (2)</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yale_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.3121</td>
                <td style="text-align:center;">0.3321</td>
                <td style="text-align:center;">0.2357</td>
                <td style="text-align:center;">0.2320</td>
                <td style="text-align:center;">0.1468</td>
                <td style="text-align:center;">0.3305</td>
                <td style="text-align:center;">0.2788</td>
                <td style="text-align:center;">0.3495</td>
                <td style="text-align:center;">0.3201</td>
                <td style="text-align:center;">0.3448 (2)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab6">
          <div class="table-caption">
            <span class="table-number">Table 6:</span> <span class=
            "table-title">Rand index scores, real datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Dataset</th>
                <th style="text-align:center;">NJW</th>
                <th style="text-align:center;">NCuts</th>
                <th style="text-align:center;">PIC</th>
                <th style="text-align:center;">PIC-<em>k</em></th>
                <th style="text-align:center;">DPIC</th>
                <th style="text-align:center;">DPIE</th>
                <th style="text-align:center;">ZP</th>
                <th style="text-align:center;">FUSE</th>
                <th style="text-align:center;">ROSC-R</th>
                <th style="text-align:center;">ROSC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">COIL20</td>
                <td style="text-align:center;">0.7303</td>
                <td style="text-align:center;">0.6245</td>
                <td style="text-align:center;">0.4940</td>
                <td style="text-align:center;">0.4481</td>
                <td style="text-align:center;">0.7737</td>
                <td style="text-align:center;">0.6114</td>
                <td style="text-align:center;">0.8534</td>
                <td style="text-align:center;">0.7424</td>
                <td style="text-align:center;">0.8264</td>
                <td style="text-align:center;">0.9923 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">seg_7<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.8242</td>
                <td style="text-align:center;">0.7962</td>
                <td style="text-align:center;">0.4830</td>
                <td style="text-align:center;">0.5000</td>
                <td style="text-align:center;">0.7212</td>
                <td style="text-align:center;">0.7162</td>
                <td style="text-align:center;">0.8208</td>
                <td style="text-align:center;">0.8210</td>
                <td style="text-align:center;">0.8357</td>
                <td style="text-align:center;">0.8549 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">glass</td>
                <td style="text-align:center;">0.6890</td>
                <td style="text-align:center;">0.6880</td>
                <td style="text-align:center;">0.6808</td>
                <td style="text-align:center;">0.6851</td>
                <td style="text-align:center;">0.6556</td>
                <td style="text-align:center;">0.6281</td>
                <td style="text-align:center;">0.6949</td>
                <td style="text-align:center;">0.6693</td>
                <td style="text-align:center;">0.7036</td>
                <td style="text-align:center;">0.7054 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">MNIST0127</td>
                <td style="text-align:center;">0.5683</td>
                <td style="text-align:center;">0.5459</td>
                <td style="text-align:center;">0.5941</td>
                <td style="text-align:center;">0.5887</td>
                <td style="text-align:center;">0.6598</td>
                <td style="text-align:center;">0.4648</td>
                <td style="text-align:center;">0.6018</td>
                <td style="text-align:center;">0.7022</td>
                <td style="text-align:center;">0.7382</td>
                <td style="text-align:center;">0.7388 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">isolet_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.9058</td>
                <td style="text-align:center;">0.8942</td>
                <td style="text-align:center;">0.7288</td>
                <td style="text-align:center;">0.7296</td>
                <td style="text-align:center;">0.6792</td>
                <td style="text-align:center;">0.9123</td>
                <td style="text-align:center;">0.8993</td>
                <td style="text-align:center;">0.8695</td>
                <td style="text-align:center;">0.9016</td>
                <td style="text-align:center;">0.9001 (4)</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yeast_4<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.6046</td>
                <td style="text-align:center;">0.5929</td>
                <td style="text-align:center;">0.5643</td>
                <td style="text-align:center;">0.5733</td>
                <td style="text-align:center;">0.5770</td>
                <td style="text-align:center;">0.5037</td>
                <td style="text-align:center;">0.6201</td>
                <td style="text-align:center;">0.6346</td>
                <td style="text-align:center;">0.6405</td>
                <td style="text-align:center;">0.6405 (1)</td>
              </tr>
              <tr>
                <td style="text-align:center;">Yale_5<font style=
                "font-variant: small-caps">class</font></td>
                <td style="text-align:center;">0.7626</td>
                <td style="text-align:center;">0.7519</td>
                <td style="text-align:center;">0.6772</td>
                <td style="text-align:center;">0.6843</td>
                <td style="text-align:center;">0.6846</td>
                <td style="text-align:center;">0.7542</td>
                <td style="text-align:center;">0.7600</td>
                <td style="text-align:center;">0.7363</td>
                <td style="text-align:center;">0.7578</td>
                <td style="text-align:center;">0.7667 (1)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>(O1): Among the 21 contests, standard spectral
        clustering methods (NJW, NCuts) win only 2 times
        (AMI-<em>glass</em>-NJW and AMI-<em>isolet_5</em>
        <font style="font-variant: small-caps">class</font>-NJW).
        PI-based methods (PIC, PIC-<em>k</em>, DPIC, DPIE) also win
        only 2 times (purity-<em>isolet_5</em> <font style=
        "font-variant: small-caps">class</font>-DPIE and
        RI-<em>isolet_5</em> <font style=
        "font-variant: small-caps">class</font>-DPIE). These two
        categories of methods, which are not designed to
        specifically handle multi-scale data, are generally not
        outstanding.</p>
        <p>(O2): The multi-scale-data-oriented methods (ZP, FUSE)
        win 4 times. In fact, if we take away ROSC, then (ZP, FUSE)
        win 14 times of the 21 contests. They are thus fairly
        effective in clustering multi-scale data. However, with a
        head-to-head comparison between ZP and FUSE, we see that
        among the 21 contests, ZP wins 9 times while FUSE wins 12
        times. In some cases, there are big gaps in their
        performance scores. For example, for the contest
        (AMI-<em>COIL20</em>), ZP beats FUSE 0.5702 to 0.4448; for
        (AMI-<em>Yale_5</em> <font style=
        "font-variant: small-caps">class</font>), FUSE outscores ZP
        0.3495 to 0.2788. There is also a case
        (AMI-<em>isolet_5</em> <font style=
        "font-variant: small-caps">class</font>) in which FUSE
        (0.6516) loses significantly to the basic NJW method
        (0.7595). We thus see that although ZP and FUSE generally
        perform well for multi-scale data, they are not very robust
        in providing consistently good performances across the
        datasets.</p>
        <p>(O3): ROSC is winner 12 times and is first runner-up 6
        times. Moreover, for those cases that ROSC does not win,
        its score is very close to that of the winner. For example,
        ROSC ranks 4th in (RI-<em>isolet_5</em> <font style=
        "font-variant: small-caps">class</font>). Its score
        (0.9001), however, is quite close to that of the winner
        DPIE (0.9123). We thus see that ROSC is generally the best
        performer among all the 10 methods and it is very robust
        across the 7 datasets tested.</p>
        <p>(O4): For <em>COIL20</em>, ROSC outperforms other
        methods by very wide margins. For example, in terms of
        purity, ROSC scores 0.9398, which is much better than NJW
        (0.4115), DPIE (0.3496), and FUSE (0.4177). From
        Table&nbsp;<a class="tbl" href="#tab3">3</a>, we see that
        <em>COIL20</em> is highly multi-scale. The results thus
        highlight the ability of ROSC in clustering multi-scale
        data. From the cluster size distribution, we see that there
        are clusters in <em>COIL20</em> that are much bigger than
        others. Objects in these big clusters can thus be very
        distant from each other and hence their feature
        similarities are small. Their correlations are
        alternatively captured by ROSC using the TKNN graph and the
        derived reachability matrix. To see the effectiveness of
        this approach, we compare the scores of ROSC against those
        of ROSC-R, which does not use the reachability matrix to
        regularize <em>Z</em>. From Tables&nbsp;<a class="tbl"
        href="#tab4">4</a>, <a class="tbl" href="#tab5">5</a>,
        <a class="tbl" href="#tab6">6</a>, we see big differences
        in the scores of ROSC and ROSC-R for the rows of
        <em>COIL20</em>. This verifies the importance of the TKNN
        graph in regularizing <em>Z</em>.</p>
        <p><strong>[Grouping effect]</strong>We showed in
        Lemma&nbsp;<a class="enc" href="#enc7">7</a> that the
        rectified matrix <em>Z</em> <sup>*</sup> and hence
        <span class="inline-equation"><span class=
        "tex">$\tilde{Z}$</span></span> have the desired grouping
        effect. Figure&nbsp;<a class="fig" href="#fig9">9</a>
        visually compares the original similarity matrix <em>S</em>
        and <span class="inline-equation"><span class=
        "tex">$\tilde{Z}$</span></span> for <em>COIL20</em>. Each
        figure displays values in a matrix by pixel brightness.
        Rows and columns in the matrix are reordered by
        gold-standard clusters. From Figure&nbsp;<a class="fig"
        href="#fig9">9</a>(b), we see a clear well-defined block
        diagonal matrix with 20 sub-blocks. Compared with the fuzzy
        display of <em>S</em> in Figure&nbsp;<a class="fig" href=
        "#fig9">9</a>(a), we see that <span class=
        "inline-equation"><span class=
        "tex">$\tilde{Z}$</span></span> is much more effective in
        spectral clustering. This explains the excellent
        performance of ROSC in clustering <em>COIL20</em>.
        <strong>[TKNN graph]</strong>We end this section with a
        further discussion on the TKNN graph regularizing matrix
        <em>Z</em> (via the reachability matrix <span class=
        "inline-equation"><span class="tex">$\mathcal
        {W}$</span></span> ). From Equation&nbsp;<a class="eqn"
        href="#eq5">6</a>, we see that the degree of such
        regularization is controlled by the parameter <em>α</em>
        <sub>2</sub>. The larger <em>α</em> <sub>2</sub> is, the
        more dominant is the TKNN graph regularization in the
        objective function. Figures&nbsp;<a class="fig" href=
        "#fig10">10</a>(a) and (b) show the performance scores of
        ROSC as <em>α</em> <sub>2</sub> varies for the dataset
        <em>COIL20</em> and <em>glass</em>, respectively. Note that
        the x-axes are shown in log scale. We note that trends of
        the curves in the two figures are quite different: For
        <em>COIL20</em>, the performance drops when <em>α</em>
        <sub>2</sub> is very small, while for <em>glass</em>, it is
        the other way round. The reason is that <em>COIL20</em> is
        highly multi-scale (see Table&nbsp;<a class="tbl" href=
        "#tab3">3</a>). In particular, there are very big clusters
        whose objects could be at large distances from each other.
        The TKNN graph has the effect of capturing the reachability
        similarities of these objects despite their weak feature
        similarities, and hence maintain their strong correlations.
        At very small <em>α</em> <sub>2</sub>, the regularization
        effect is given too small a weight for it to be effective,
        leading to smaller performance scores. For <em>glass</em>,
        the dataset is much less multi-scale. The performance of
        ROSC stays very stable over a range of <em>α</em>
        <sub>2</sub> values until <em>α</em> <sub>2</sub> becomes
        very large, in which case, reachability similarity
        over-dominates feature similarity, resulting in a mild dip
        in performance.</p>
        <figure id="fig9">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig9.jpg"
          class="img-responsive" alt="Figure 9" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 9:</span>
            <span class="figure-title">A visual comparison of
            <em>S</em> and <span class=
            "inline-equation"><span class=
            "tex">$\tilde{Z}$</span>.</span> for
            <em>COIL20</em></span>
          </div>
        </figure>
        <figure id="fig10">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3185993/images/www2018-2-fig10.jpg"
          class="img-responsive" alt="Figure 10" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 10:</span>
            <span class="figure-title">ROSC's performance scores
            vs. <em>α</em> <sub>2</sub></span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span>
          Conclusions</h2>
        </div>
      </header>
      <p>In this paper we studied the effectiveness of spectral
      clustering methods in handling multi-scale data. We discussed
      the traditional approaches of locally scaling the similarity
      matrix and power-iteration-based techniques. We described the
      methods ZP and FUSE, which were previously proposed to
      cluster multi-scale data. We pointed out that these existing
      approaches focus on measuring the correlations of objects via
      feature similarity. However, for data with various sizes and
      densities, objects that belong to the same big cluster could
      be at substantial distances from each other. Feature
      similarity could fail in this case. In view of this, we
      proposed ROSC, which computes an affinity matrix <span class=
      "inline-equation"><span class="tex">$\tilde{Z}$</span></span>
      that takes into account both feature similarity and
      reachability similarity. In particular, <span class=
      "inline-equation"><span class="tex">$\tilde{Z}$</span></span>
      is obtained by regularizing a primitive affinity matrix with
      a TKNN graph. We mathematically proved that <span class=
      "inline-equation"><span class="tex">$\tilde{Z}$</span></span>
      has the desired grouping effect, which makes it a very
      effective replacement of the similarity matrix <em>S</em>
      used in spectral clustering. We conducted extensive
      experiments comparing the performance of ROSC against 9 other
      methods. Our results show that ROSC provides very good
      performances across all the datasets, both real and
      synthetic, we tested. In particular, for cases where the
      datasets are highly multi-scale, ROSC substantially
      outperforms other competitors. ROSC is therefore a very
      robust solution for clustering multi-scale data.</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span>
          Acknowledgments</h2>
        </div>
      </header>
      <p>This research is supported by Hong Kong Research Grants
      Council GRF HKU 17254016.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Charles&nbsp;J Alpert
        and So-Zen Yao. 1995. Spectral partitioning: the more
        eigenvectors, the better. In <em><em>Proceedings of the
        32nd annual ACM/IEEE Design Automation Conference</em></em>
        . ACM, 195–200.</li>
        <li id="BibPLXBIB0002" label="[2]">Anonymous Author(s).
        2017. Technical report. (2017). <a href=
        "https://www.dropbox.com/s/i5nmwzbnuehupw8/report.pdf?dl=0"
          target=
          "_blank">https://www.dropbox.com/s/i5nmwzbnuehupw8/report.pdf?dl=0</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Aleksandar Bojchevski,
        Yves Matkovic, and Stephan Günnemann. 2017. Robust Spectral
        Clustering for Noisy Data: Modeling Sparse Corruptions
        Improves Latent Embeddings. In <em><em>KDD</em></em> .
        737–746.</li>
        <li id="BibPLXBIB0004" label="[4]">Xinlei Chen and Deng
        Cai. 2011. Large Scale Spectral Clustering with
        Landmark-Based Representation. In <em><em>AAAI</em></em> .
        313–318.</li>
        <li id="BibPLXBIB0005" label="[5]">David Cheng, Ravi
        Kannan, Santosh Vempala, and Grant Wang. 2003. <em><em>On a
        recursive spectral algorithm for clustering from pairwise
        similarities</em></em> . Technical Report.</li>
        <li id="BibPLXBIB0006" label="[6]">Carlos&nbsp;D Correa and
        Peter Lindstrom. 2012. Locally-scaled spectral clustering
        using empty region graphs. In <em><em>KDD</em></em> .
        1330–1338.</li>
        <li id="BibPLXBIB0007" label="[7]">Jane&nbsp;K Cullum and
        Ralph&nbsp;A Willoughby. 2002. <em><em>Lanczos algorithms
        for large symmetric eigenvalue computations: Vol. I:
        Theory</em></em> . SIAM.</li>
        <li id="BibPLXBIB0008" label="[8]">Inderjit&nbsp;S Dhillon.
        2001. Co-clustering documents and words using bipartite
        spectral graph partitioning. In <em><em>KDD</em></em> .
        269–274.</li>
        <li id="BibPLXBIB0009" label="[9]">Inderjit&nbsp;S Dhillon,
        Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cuts
        without eigenvectors a multilevel approach.
        <em><em>TPAMI</em></em> 29, 11 (2007).</li>
        <li id="BibPLXBIB0010" label="[10]">Alex Gittens,
        Prabhanjan Kambadur, and Christos Boutsidis. 2013.
        Approximate spectral clustering via randomized sketching.
        <em><em>Ebay/IBM Research Technical Report</em></em>
        (2013).</li>
        <li id="BibPLXBIB0011" label="[11]">Han Hu, Zhouchen Lin,
        Jianjiang Feng, and Jie Zhou. 2014. Smooth representation
        clustering. In <em><em>CVPR</em></em> . 3834–3841.</li>
        <li id="BibPLXBIB0012" label="[12]">Hao Huang, Shinjae Yoo,
        Dantong Yu, and Hong Qin. 2014. Diverse power iteration
        embeddings and its applications. In <em><em>ICDM</em></em>
        . 200–209.</li>
        <li id="BibPLXBIB0013" label="[13]">Ling Huang, Donghui
        Yan, Nina Taft, and Michael&nbsp;I Jordan. 2009. Spectral
        clustering with perturbed data. In <em><em>NIPS</em></em> .
        705–712.</li>
        <li id="BibPLXBIB0014" label="[14]">Ravi Kannan, Santosh
        Vempala, and Adrian Vetta. 2004. On clusterings: Good, bad
        and spectral. <em><em>JACM</em></em> 51, 3 (2004),
        497–515.</li>
        <li id="BibPLXBIB0015" label="[15]">Agnan Kessy, Alex
        Lewin, and Korbinian Strimmer. 2017. Optimal whitening and
        decorrelation. <em><em>The American Statistician</em></em>
        (2017).</li>
        <li id="BibPLXBIB0016" label="[16]">Stephane Lafon and
        Ann&nbsp;B Lee. 2006. Diffusion maps and coarse-graining: A
        unified framework for dimensionality reduction, graph
        partitioning, and data set parameterization.
        <em><em>TPAMI</em></em> 28, 9 (2006), 1393–1403.</li>
        <li id="BibPLXBIB0017" label="[17]">Zhenguo Li, Jianzhuang
        Liu, Shifeng Chen, and Xiaoou Tang. 2007. Noise robust
        spectral clustering. In <em><em>ICCV</em></em> . 1–8.</li>
        <li id="BibPLXBIB0018" label="[18]">Frank Lin. 2012.
        <em>Scalable methods for graph-based unsupervised and
        semi-supervised learning</em>. Ph.D. Dissertation. Carnegie
        Mellon University.</li>
        <li id="BibPLXBIB0019" label="[19]">Frank Lin and
        William&nbsp;W Cohen. 2010. Power iteration clustering. In
        <em><em>ICML</em></em> . 655–662.</li>
        <li id="BibPLXBIB0020" label="[20]">Guangcan Liu, Zhouchen
        Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. 2013.
        Robust recovery of subspace structures by low-rank
        representation. <em><em>TPAMI</em></em> 35, 1 (2013),
        171–184.</li>
        <li id="BibPLXBIB0021" label="[21]">Canyi Lu, Jiashi Feng,
        Zhouchen Lin, and Shuicheng Yan. 2013. Correlation adaptive
        subspace segmentation by trace lasso. In
        <em><em>ICCV</em></em> . 1345–1352.</li>
        <li id="BibPLXBIB0022" label="[22]">Can-Yi Lu, Hai Min,
        Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, and Shuicheng
        Yan. 2012. Robust and efficient subspace segmentation via
        least squares regression. In <em><em>ECCV</em></em> .
        347–360.</li>
        <li id="BibPLXBIB0023" label="[23]">Marina Meila and Jianbo
        Shi. 2001. A random walks view of spectral segmentation.
        (2001).</li>
        <li id="BibPLXBIB0024" label="[24]">Boaz Nadler and Meirav
        Galun. 2006. Fundamental limitations of spectral
        clustering. In <em><em>NIPS</em></em> . 1017–1024.</li>
        <li id="BibPLXBIB0025" label="[25]">Boaz Nadler, Stephane
        Lafon, Ronald Coifman, and Ioannis Kevrekidis. 2005.
        Diffusion maps, spectral clustering and eigenfunctions of
        Fokker-Planck operators. In <em><em>NIPS</em></em> .
        955–962.</li>
        <li id="BibPLXBIB0026" label="[26]">Andrew&nbsp;Y Ng,
        Michael&nbsp;I Jordan, Yair Weiss, <em>et al.</em> 2001. On
        spectral clustering: Analysis and an algorithm. In
        <em><em>NIPS</em></em> . 849–856.</li>
        <li id="BibPLXBIB0027" label="[27]">Yousef Saad. 2011.
        <em><em>Numerical Methods for Large Eigenvalue Problems:
        Revised Edition</em></em> . SIAM.</li>
        <li id="BibPLXBIB0028" label="[28]">Jianbo Shi and Jitendra
        Malik. 2000. Normalized cuts and image segmentation.
        <em><em>TPAMI</em></em> 22, 8 (2000), 888–905.</li>
        <li id="BibPLXBIB0029" label="[29]">Anh&nbsp;Pham The,
        Nguyen&nbsp;Duc Thang, La&nbsp;The Vinh, Young-Koo Lee, and
        Sungyoung Lee. 2013. Deflation-based power iteration
        clustering. <em><em>Applied Intelligence</em></em> 39, 2
        (2013), 367–385.</li>
        <li id="BibPLXBIB0030" label="[30]">Nguyen&nbsp;Xuan Vinh,
        Julien Epps, and James Bailey. 2010. Information theoretic
        measures for clusterings comparison: Variants, properties,
        normalization and correction for chance.
        <em><em>JMLR</em></em> 11(2010), 2837–2854.</li>
        <li id="BibPLXBIB0031" label="[31]">Ulrike
        Von&nbsp;Luxburg. 2007. A tutorial on spectral clustering.
        <em><em>Statistics and computing</em></em> 17, 4 (2007),
        395–416.</li>
        <li id="BibPLXBIB0032" label="[32]">Ulrike
        Von&nbsp;Luxburg, Mikhail Belkin, and Olivier Bousquet.
        2008. Consistency of spectral clustering. <em><em>The
        Annals of Statistics</em></em> (2008), 555–586.</li>
        <li id="BibPLXBIB0033" label="[33]">Tao Xiang and Shaogang
        Gong. 2008. Spectral clustering with eigenvector selection.
        <em><em>Pattern Recognition</em></em> 41, 3 (2008),
        1012–1029.</li>
        <li id="BibPLXBIB0034" label="[34]">Donghui Yan, Ling
        Huang, and Michael&nbsp;I Jordan. 2009. Fast approximate
        spectral clustering. In <em><em>KDD</em></em> .
        907–916.</li>
        <li id="BibPLXBIB0035" label="[35]">Wei Ye, Sebastian
        Goebl, Claudia Plant, and Christian Böhm. 2016. FUSE: Full
        Spectral Clustering. In <em><em>KDD</em></em> .
        1985–1994.</li>
        <li id="BibPLXBIB0036" label="[36]">Stella&nbsp;X. Yu and
        Jianbo Shi. 2003. Multiclass spectral clustering. In
        <em><em>ICCV</em></em> . 313–319.</li>
        <li id="BibPLXBIB0037" label="[37]">Lihi Zelnik-Manor and
        Pietro Perona. 2004. Self-tuning spectral clustering. In
        <em><em>NIPS</em></em> . 1601–1608.</li>
        <li id="BibPLXBIB0038" label="[38]">Xiatian Zhu, Chen
        Change&nbsp;Loy, and Shaogang Gong. 2014. Constructing
        robust affinity graphs for spectral clustering. In
        <em><em>CVPR</em></em> . 1450–1457.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>There are a
    number of variants of spectral clustering methods. Our
    description here is based on the NJW method&nbsp;[<a class=
    "bib" data-trigger="hover" data-toggle="popover"
    data-placement="top" href="#BibPLXBIB0026">26</a>]. More
    details will be presented in Section&nbsp;<a class="sec" href=
    "#sec-3">2</a>.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>We say that an
    eigenvector e <sub><em>i</em></sub> is smaller than another
    eigenvector e <sub><em>j</em></sub> if e <sub><em>i</em></sub>
    ’s eigenvalue is smaller than that of e <sub><em>j</em></sub>
    ’s.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>Given a matrix
    <em>M</em>, we use a pair of subscripts to specify an entry of
    <em>M</em>.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>We will
    regularize <em>Z</em> to avoid the trivial solution of
    <em>Z</em> being the identity matrix.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23–27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3185993">https://doi.org/10.1145/3178876.3185993</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
