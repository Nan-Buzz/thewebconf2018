<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>VideoKen: Automatic Video Summarization and Course Curation to Support Learning</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3184558.3186988"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3186988'>https://doi.org/10.1145/3184558.3186988</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186988'>https://w3id.org/oa/10.1145/3184558.3186988</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">VideoKen: Automatic Video Summarization and Course Curation to Support Learning</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Debabrata</span> <span class="surName">Mahapatra</span> Videoken, Bangalore, India, <a href="mailto:debabrata.mahapatra@videoken.com">debabrata.mahapatra@videoken.com</a>
        </div>
        <div class="author">
          <span class="givenName">Ragunathan</span> <span class="surName">Mariappan</span> School of Computing National University of Singapore, <a href="mailto:mragunathan@nus.edu.sg">mragunathan@nus.edu.sg</a>
        </div>
        <div class="author">
          <span class="givenName">Vaibhav</span> <span class="surName">Rajan</span> School of Computing National University of Singapore, <a href="mailto:vaibhav.rajan@nus.edu.sg">vaibhav.rajan@nus.edu.sg</a>
        </div>
        <div class="author">
          <span class="givenName">Kuldeep</span> <span class="surName">Yadav</span> Videoken, Bangalore, India, <a href="mailto:kuldeep@videoken.com">kuldeep@videoken.com</a>
        </div>
        <div class="author">
          <span class="givenName">Seby</span> <span class="surName">A</span> Videoken, Bangalore, India, <a href="mailto:seby@videoken.com">seby@videoken.com</a>
        </div>
        <div class="author">
          <span class="givenName">Sudeshna</span> <span class="surName">Roy</span> Videoken, Bangalore, India, <a href="mailto:sudeshna.roy@videoken.com">sudeshna.roy@videoken.com</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186988" target="_blank">https://doi.org/10.1145/3184558.3186988</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>The number of high quality online videos is increasing rapidly. Online courses as well as universities do not fully leverage the content due to several open challenges in video search, indexing, summarization and customization requirements for specific courses, instructors or learners. We present a new web-based social learning platform called Videoken. Using novel video summarization algorithms, Videoken automatically creates Table of Contents for videos. This allows a textbook-like facility for non-linear search and navigation through the video, enables extraction of semantically coherent clips from within a video and improves video search through better semantic indexing. The platform also allows new ways of course creation and sharing of learning modules; and can be both integrated with existing Learning Management Systems and used independently.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Computing methodologies</strong> → <strong>Information extraction;</strong> <strong>Video summarization;</strong> <strong>Video segmentation;</strong> • <strong>Applied computing</strong> → <em>Education;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Videoken; Video Summarization; Course Curation; Table of Contents; Phrase Cloud</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Debabrata Mahapatra, Ragunathan Mariappan, Vaibhav Rajan, Kuldeep Yadav, Seby A, and Sudeshna Roy. 2018. VideoKen: Automatic Video Summarization and Course Curation to Support Learning. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em> <em>April 23–27, 2018 (WWW ’18 Companion),</em> <em>Lyon, France. ACM, New York, NY, USA</em> 4 Pages. <a href="https://doi.org/10.1145/3184558.3186988" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3184558.3186988</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Massive Open Online Courses (MOOC) and other online resources provide many high quality educational videos on the Internet. The overwhelming number of videos presents challenges of finding the most appropriate video and consuming the content effectively.</p>
      <p>Consider the case of a learner trying to find a relevant video to learn any given topic. Online search would present thousands of videos that differ in content, presentation styles, duration, video quality etc. Finding the right video that is most suitable to the learner's background, learning goal and preferred learning style is non-trivial. Further, navigating through the search results usually involves skimming through the video contents by watching snippets of the video just to know whether the video is appropriate. This is time consuming even for a single video and definitely cannot practically scale beyond a few videos.</p>
      <p>Although MOOC platforms provide well organized courses, according to the study conducted in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] that investigates high dropout rates in MOOCs, the structure of a course designed by the course creators may not be helpful to all participants. Moreover, the authors suggest considering different design patterns for a course. A similar study in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>] reveals two important factors, among others, for high dropout rate: a) there exists a gap in the perceived expectation of the course content and what is actually delivered, b) the perceived difficulty level of a course vary among the participants. MOOCs are designed for large scale use. However, the studies in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] suggest that this very design may not have the flexibility that is necessary to reach a large audience. Instead, a customized approach for knowledge delivery can potentially lead to better learning outcomes.</p>
      <p>Recently <em>Byte Sized Learning (BSL)</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>] has gained popularity in corporate training and instructors are (re)creating existing video contents to incorporate BSL in their courses. Thus, instructors cannot directly use the already available high-quality videos that are not short enough for BSL.</p>
      <p>In this paper, we present an innovative web-based platform, called <strong>VideoKen</strong>, that addresses these challenges using novel multimedia technologies. In particular, we develop certain tools that, when used together, can provide efficient solutions to the problems of both the learners as well as instructors. Our platform brings the following novel additions to Learning Management Systems (LMS):</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We provide two different types of automatic video summarization: Multimedia Table of Content (<em>MMToC</em>) and Phrase Cloud, that enables the user to quickly perceive the content of a video without having to watch it. These features are hyperlinked within the timeline of video, which facilitates non-linear navigation of a video. Further, the use of these summarization tools improves video search.<br /></li>
        <li id="list2" label="•">By utilizing these video summarization capabilities, we build a tool that allows a user to quickly create a <em>Learning Object</em> from clips of videos, called <em>Kenlist</em>. This permits a learner to modularize the knowledge of interest in a customized manner and efficiently create BSL-based courses with no extra content creation.<br /></li>
        <li id="list3" label="•">Integrating the standard features provided by a typical LMS system with kenlists, we develop a novel Course Curation functionality. This would empower an instructor to create high quality, highly customizable course material by using either existing publicly available videos or their own content.<br /></li>
      </ul>
      <p>VideoKen is accessible on the web at <a class="link-inline force-break" href="http://www.videoken.com">www.videoken.com</a>. In the following sections we briefly describe the key features of Videoken that we intend to demonstrate.</p>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> MMToC: Multi-modal Table of Contents for Videos</h2>
        </div>
      </header>
      <p>In a textbook, the Table of Contents (<strong>ToC</strong>) helps a reader to find and navigate to a particular topic. However, in case of a video, especially if it is long, it is difficult to locate where a particular topic is covered in its time line. For typical educational videos that contain slides, we have developed an algorithm to automatically generate the ToC. Figure <a class="fig" href="#fig1">1</a> and <a class="fig" href="#fig2">2</a> illustrate the user interface for ToC of a video, to the left of the video. For this, we use information from different modalities, visual, audio and text (obtained from the transcript); hence the name Multi-modal ToC or <strong>MMToC</strong>. From a user experience point of view, MMToC provides several advantages.</p>
      <p>First, it becomes as simple as navigating a book; because the titles of MMToC are linked to their corresponding time of occurrence in the video. So by clicking a particular ToC entry one can directly go to the start time of its corresponding frame in the video, much like the beginning of a chapter beginning in a book. According to the user study done in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>], having watched a video earlier, the user can locate a particular concept in a video faster if MMToC is available.</p>
      <p>Second, for long lecture videos, the user can get a quick summary of topics covered by seeing the MMToC, even without watching the full video. This enables the user to find a better match for their requirements among many options in a video search result.</p>
      <p>In order to generate MMToC automatically, we first segment the video temporally into scenes. Each scene is then represented by some of the key-frames in it. Using Optical Character Recognition (OCR), we extract the text information from the key-frames. In this work, we use a commercially available OCR engine, offered by the <em>Cognitive Services</em> of Microsoft Azure cloud computing. To prioritize among all the extracted texts, saliency scores of these texts are computed using information from multiple modalities. Visual saliency scores of a text include stroke width [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] of the characters, font size, geometrical location of the text etc. Occurrence of the text in the transcript of the video is also taken into account. If the transcript is not available, then we obtain it through automatic speech recognition (ASR) (Kaldi [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]). If the instructor has uttered the text then pitch information of the corresponding audio signal is also used in the saliency score. Using these scores, the titles are chosen among the extracted texts that form the MMToC entries.</p>
      <p>In general, many educational videos are produced with slides to facilitate instruction. However, there are also videos wherein information is conveyed without using slides such as classroom lectures delivered orally with or without using a black board, panel discussions, skill development videos from various domains, such as sports, cooking, code walk-through etc. In principle, for any instructional video, MMToC can improve the effectiveness of video consumption.</p>
      <p>Due to recognition errors in OCR and ASR, there may be errors in the titles of MMToC entry. We build a moderation tool that can be used to modify the existing ToC of a video or to create a ToC in case it is not already generated automatically. To create such a manual ToC for a video, where the topic titles are not apparent, the user has to understand the content and come up with suitable titles. This exercise encourages the user to reflect up on his/her learning, hence leading to better comprehension.</p>
      <figure id="fig1">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186988/images/www18companion-228-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class="figure-title">User interface of MMToC and phrase cloud. Both kinds of summaries can be scrolled through. Upon clicking a phrase the (here: “data domain”), markers appear in the blue line parallel to the video timeline representing the time points where this phrase is present (in the video frame or audio).</span>
        </div>
      </figure>
      <figure id="fig2">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186988/images/www18companion-228-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class="figure-title">Illustration of stripping a video to a clip and adding to a kenlist. The red markers on the video timeline can be adjusted to select the portion of the video that will be added to a kenlist. All kenlists are listed on the leftmost panel.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Phrase Cloud</h2>
        </div>
      </header>
      <p>A textbook also contains a set of index words. While the book's ToC covers the major topics, the index points to many specialized concepts, presented in the book. In our platform, we bring similar capabilities for a video. Instead of limiting to words, we automatically find the relevant phrases used in the video that might be of interest to users. We call this set of extracted phrases the <em>phrase cloud</em>.</p>
      <p>The user interface, illustrated in Figure <a class="fig" href="#fig1">1</a>, has a parallel time-line to the video player. Upon clicking an item in the phrase cloud (which is on the right of the video), the occurrences of the selectedd phrase in the time-line is shown with markers. This allows the user to navigate a video in non-linear manner, and quickly reach to the point of interest that discusses the selected phrase.</p>
      <p>Like MMToC, the user can scroll through the entire phrase cloud and get a quick summary of the major concepts discussed in the video. Thus MMToC and phrase cloud provide two different forms of hyperlinked video summarization.</p>
      <p>The relative significance of phrases (measured by the frequency of occurrence) is depicted by its font size in the cloud: higher the font size more the significance. Furthermore, with the 2D space to show these phrases, we segregate them topic-wise.</p>
      <p>The phrase cloud is generated in multiple steps. First, we obtain the transcript of the video. If not available, then we perform Automatic Speech Recognition on the audio. For this, we use Kaldi [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>] ASR engine. Then the most important concepts are extracted using <em>Text Rank</em> [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] for this. As a result, we get a set of key words/phrases. Then, we represent the extracted words in the form of continuous vectors by using Word2Vec [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>] for further processing. Using a dictionary of concepts from various domains, we compute the significance value of each extracted phrase. The word mover's distance [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>] metric is used to find the distance of a particular phrase with respect to the concept dictionary; lesser the distance higher the significance. Finally, we use DBSCAN [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>] to cluster these phrases and put them in disjoint sets that represent separate topics.</p>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Kenlist: A Learning Object</h2>
        </div>
      </header>
      <p>Often times, one would like to refer back to previously watched videos that are associated with a particular topic or knowledge domain. The <em>playlist</em> feature of YouTube enables users to do that. However, in general, whatever knowledge we are searching for may not be readily available in a single video. The user's requirement might be distributed across different videos; combining the useful portions of these videos would make a complete package of knowledge that is specific to the user's requirements. For example, while learning various sorting algorithms, one student may find that merge sort is best explained in first 10 minutes of an hour long lecture, quick sort is best explained somewhere in the middle of another video for 20 minutes, and so on. In Videoken we provide a functionality, namely <strong>Kenlist</strong>, that allows the user to efficiently aggregate portions of video material from different sources.</p>
      <p>Kenlists can be considered as a learning <em>object</em> for a particular knowledge domain that is constructed from a set of video clips. The embedded video player in our platform provides clipping options within a video. Note that neither the video nor the clip is stored in Videoken, only relevant time pointers are stored and the video is played directly from Youtube or the selected channel.</p>
      <p>Features like MMToC and phrase cloud are complementary to this clipping action. Because, using these hyperlinked summaries one can precisely locate the portion of interest and customize the amount of information that needs to be registered for future references. This is illustrated in Figure <a class="fig" href="#fig2">2</a>. Besides the semantic content exploration, we also provide relevant search results (discussed in section <a class="sec" href="#sec-12">6</a>) for user query that can further help in building useful kenlists.</p>
      <p>In addition, Videoken provides features found in other Learning Management Systems (LMS). Users can add bookmarks to the videos at specific points in time and annotate it by adding notes. To encourage peer-to-peer learning, we enable sharing of kenlists. While accessing a shared kenlist, a user can provide her feedback to the creator of that kenlist as well.</p>
    </section>
    <section id="sec-11">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Course Curation</h2>
        </div>
      </header>
      <p>While a learning object like Kenlist helps a learner put together videos for later references, from an instructor's point of view, it may not be directly used as a course. We provide some tools that allow an instructor to extend a kenlist to a full fledged course. A course is structured by adding sessions to it. The instructor can add kenlists to a session and supporting contents relevant to it; for example, documents for further readings and assignments.</p>
      <p>We also provide LMS features: quizzes can be added at appropriate places in the course; to provide an engaging learning experience the instructor can add questions with multiple choice answers at different points in the kenlist. While watching the kenlist as a student, these questions automatically appear pausing the video until the question is answered. To promote social learning, students can start a discussion for sessions, kenlists, videos, documents and quizzes as well. These courses can also be shared among instructors.</p>
      <p>To track the learning outcomes of students we provide an analytics dashboard for the instructor that shows information such as course performance summaries, leaderboards and how much time is being spent for a video by the students. The course creation features allows an instructor to leverage thousands of publicly accessible educational videos and build a customized course. Therefore, it avoids recreating some of the already available videos.</p>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Search</h2>
        </div>
      </header>
      <p>Given a search query from a user we first obtain the search results (list of videos) from YouTube. Then we re-rank this list to make it more relevant to the user's query and profile. We leverage the semantic metadata extracted from videos for re-ranking. It is achieved by using a recommendation system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>] that takes user profile and video features into account. User profiles are created by using their activity logs on our platform. Apart from the description of a video (provided by the content creator), which is already used in YouTube search, we use the MMToC, phrase cloud and its presence in a <em>KenList</em> as some of the extra video features.</p>
      <p>In real time, the user query and the YouTube list of videos are given as input to the recommendation system. We provide the pre-computed user and video features to the trained recommendation model to score each of the video in the list, for a particular user and re-rank the results. A typical search result is illustrated in Figure <a class="sec" href="#sec-12">6</a>. Upon hovering the cursor on either phrase cloud or MMToC icon, one can see the corresponding form of summary for the video. So, if the content is not relevant, one need not even view the video. In the final search result, we also present already existing courses relevant to the query. However, the course curated on our platform only appears if the creator makes it publicly available. Further, to narrow down the search results, we provide category and channel in filters while searching. These are primarily YouTube channels, such as Khan Academy, MIT OCW, NPTEL, Stanford, TED Ed etc.</p>
      <figure id="fig3">
        <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186988/images/www18companion-228-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 3:</span> <span class="figure-title">Illustration of search results upon user query. Adjacent to each video entry in the results are icons for MMToC (orange) and Phrase Cloud (blue). Hovering on the corresponding icon shows the MMToC and Phrase Cloud thereby providing two kinds of video summarization, that can help the user find the most appropriate video.</span>
        </div>
      </figure>
      <p></p>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Conclusions</h2>
        </div>
      </header>
      <p>Online education through the use of ever-increasing number of videos is still far from being successful, due to difficulties in effective video consumption and knowledge delivery through online courses. We develop VideoKen, a web-based social learning platform that provides tools to help both the learner as well as the instructor, through novel tools for finding relevant videos and curating effective courses. MMToC and phrase cloud are two video summarization tools that also aid in non-linear navigation and kenlist creation. Using kenlists, a user can quickly put together a coherent set of video clips. If utilized to its true potential by curating courses with the existing educational videos, this platform can expedite the process of customized knowledge creation and acquisition.</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span> Acknowledgments</h2>
        </div>
      </header>
      <p>We thank Kundan Shrivastava, Ranjeet Kumar, Abhinay Kumar, Onkar Hoysala, Astha Srivastava, Aakash Tyagi, Saahil Madaan, Harish Reddy Pittu and Shant Purushotam Kumar for their contributions in building the Videoken platform. We also thank Vishnu Raned, Ashish Vikram and Manish Gupta for useful discussions.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Arijit Biswas, Ankit Gandhi, and Om Deshmukh. 2015. MMToC: A Multimodal Method for Table of Content Creation in Educational Videos. <em><em>In Proceedings of the 23rd ACM International Conference on Multimedia</em></em> (<em>MM ’15</em>). ACM, New York, NY, USA, 621–630. <a class="link-inline force-break" href="https://doi.org/10.1145/2733373.2806253" target="_blank">https://doi.org/10.1145/2733373.2806253</a>
        </li>
        <li id="BibPLXBIB0002" label="[2]">B. Epshtein, E. Ofek, and Y. Wexler. 2010. Detecting text in natural scenes with stroke width transform. <em><em>In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em></em> . 2963–2970. <a class="link-inline force-break" href="https://doi.org/10.1109/CVPR.2010.5540041" target="_blank">https://doi.org/10.1109/CVPR.2010.5540041</a>
        </li>
        <li id="BibPLXBIB0003" label="[3]">Thommy Eriksson, Tom Adawi, and Christian Stöhr. 2017. “Time is the bottleneck”: a qualitative study exploring why learners drop out of MOOCs. <em><em>Journal of Computing in Higher Education</em></em> 29, 1 (01 Apr 2017), 133–146. <a class="link-inline force-break" href="https://doi.org/10.1007/s12528-016-9127-8" target="_blank">https://doi.org/10.1007/s12528-016-9127-8</a>
        </li>
        <li id="BibPLXBIB0004" label="[4]">Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, <em>et al.</em> 1996. A density-based algorithm for discovering clusters in large spatial databases with noise.. <em><em>In Kdd</em></em> , Vol.&nbsp;96. 226–231.</li>
        <li id="BibPLXBIB0005" label="[5]">Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to documentdistances. <em><em>In International Conference on Machine Learning</em></em> . 957–966.</li>
        <li id="BibPLXBIB0006" label="[6]">Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. <em><em>In Proceedings of the 2004 conference on empirical methods in natural language processing</em></em> .</li>
        <li id="BibPLXBIB0007" label="[7]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. <em><em>arXiv preprint arXiv:1301.3781</em></em> (2013).</li>
        <li id="BibPLXBIB0008" label="[8]">Nagarajan Natarajan and Inderjit&nbsp;S Dhillon. 2014. Inductive matrix completion for predicting gene–disease associations. <em><em>Bioinformatics</em></em> 30, 12 (2014), i60–i68.</li>
        <li id="BibPLXBIB0009" label="[9]">A.&nbsp;H. Omer. 2015. Is Bite Sized Learning The Future Of eLearning?https://elearningindustry.com/bite-sized-learning-future-of-elearning. (2015). Accessed: 2018-01-10.</li>
        <li id="BibPLXBIB0010" label="[10]">Daniel&nbsp;FO Onah, Jane Sinclair, and Russell Boyatt. 2014. Dropout rates of massive open online courses: behavioural patterns. <em><em>EDULEARN14 Proceedings</em></em> (2014), 5825–5834.</li>
        <li id="BibPLXBIB0011" label="[11]">Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. <em><em>In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</em></em> . IEEE Signal Processing Society. IEEE Catalog No.: CFP11SRW-USB.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3184558.3186988">https://doi.org/10.1145/3184558.3186988</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
