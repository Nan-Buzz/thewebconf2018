<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Question Answering Mediated by Visual Clues and Knowledge Graphs</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Question Answering Mediated by Visual Clues and Knowledge Graphs</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Fabr&#x00ED;cio F.</span>      <span class="surName">Faria</span>,     Federal University of Rio de Janeiro, Rio de Janeiro, Brazil, <a href="mailto:firminodefaria@ufrj.br">firminodefaria@ufrj.br</a>     </div>     <div class="author">     <span class="givenName">Ricardo</span>      <span class="surName">Usbeck</span>,     Paderborn University, Paderborn, Germany, <a href="mailto:ricardo.usbeck@uni-paderborn.de">ricardo.usbeck@uni-paderborn.de</a>     </div>     <div class="author">     <span class="givenName">Alessio</span>      <span class="surName">Sarullo</span>,     University of Manchester, Manchester, UK, <a href="mailto:alessio.sarullo@manchester.ac.uk">alessio.sarullo@manchester.ac.uk</a>     </div>     <div class="author">     <span class="givenName">Tingting</span>      <span class="surName">Mu</span>,     University of Manchester, Manchester, UK, <a href="mailto:tingting.mu@manchester.ac.uk">tingting.mu@manchester.ac.uk</a>     </div>     <div class="author">     <span class="givenName">Andre</span>      <span class="surName">Freitas</span>,     University of Manchester, Manchester, UK, <a href="mailto:andre.freitas@manchester.ac.uk">andre.freitas@manchester.ac.uk</a>     </div>                         </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3192318" target="_blank">https://doi.org/10.1145/3184558.3192318</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>This challenge focuses on the use of semantic representation methods to support Visual Question Answering: given a large image collection, find a set of images matching natural language queries. The task supports advancing the state-of-the-art in Visual Question Answering by focusing on methods which explore the interplay between contemporary machine learning techniques, semantic representation and reasoning mechanisms.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Information systems </strong>&#x2192; <strong>Question answering;</strong> <em>Information extraction;</em> &#x2022;<strong> Computing methodologies </strong>&#x2192; <strong>Visual content-based indexing and retrieval;</strong></small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Visual Question Answering</small>, </span>     <span class="keyword">      <small> Gold-standard</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Fabr&#x00ED;cio F. Faria, Ricardo Usbeck, Alessio Sarullo, Tingting Mu, and Andre Freitas. 2018. Question Answering Mediated by Visual Clues and Knowledge Graphs. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France.</em> ACM, New York, NY, USA, 3 pages. <a href="https://doi.org/10.1145/3184558.3192318" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3192318</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>As of 2017, it is estimated that images and videos account for up 73% of all all consumer Internet traffic according to Cisco Visual Networking Index&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. However, due to its non-symbolic nature, most of the content present in visual form depends on the use of associated textual content or annotations to become accessible and searchable to end-users. These associated textual information provide a limited slice to the full information content (such as entities and relations) expressed in the images. With recent advances in machine learning techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>], in subparticular, in the field of computer vision, the detection and classification of objects embedded in images became a very active research area&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>]. More recently, the detection of relations between objects in an image scene&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>] defined the use of lexico-semantic graphs as a lightweight representation device for images. The emergence of richer symbolic-level representation models opened the doors to more sophisticated semantic interpretation models and applications such as Visual Question Answering (VQA).</p>    <p>However, addressing the problem of Question Answering over visual data requires principled semantic representation models. These models support semantic approximation and reasoning operations necessary to bridge the gap between queries and the immediate description of an image. In this process the integration of sub-symbolic methods derived from computer vision should be integrated to symbolic AI methods.</p>    <p>This challenge aims at advancing the dialogue between Computer Vision and Natural Language Processing, by creating a test collection which explores the semantic aspects of VQA.</p>   </section>   <section id="sec-7">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Dataset Description</h2>     </div>    </header>    <p>Most natural language queries in the dataset require the integration of one or more knowledge graphs or linguistic resources and explore different types of representation and reasoning on the top of the scene description. The test collection is derived from the Visual Genome Dataset (1.7 million object instances and 2.3 million relationships)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>].</p>    <p>The training dataset is composed by 4475 queries while the test dataset is composed of 514 queries. The datasets are represented in a JSON file where each JSON object is composed by a query and a list of visual genome image ids. The datasets were created by randomly selecting images from the Visual Genome dataset and asking 25 human annotators to generate natural language queries which describe the image as an answer to this query, and which explored broader types of relations, categories and aggregations present within the image. An example of an image and associated query is depicted in Figure <a class="fig" href="#fig1">1</a>. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3200000/3192318/images/www18companion-436-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Example of images, natural language queries and supporting knowledge graphs.</span>     </div>     </figure>    </p>    <p>The annotation protocol is available at [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] while the final datasets are available at [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>].</p>    <p>The final query set contains a dataset which expresses different semantic categories and semantic relations. Examples of queries contain queries with references to: <strong>sensorial elements</strong> (<em>Are there images of tennis players wearing red clothes?</em>), <strong>metonymic expressions</strong> (<em>Give me recipes with olives in it</em>), <strong>similarity relations</strong> (<em>Show me kites that look like parachutes</em>), <strong>direct spatial references</strong> (<em>Show me a bridge close to NYC</em>), <strong>indirect spatial references</strong> (<em>Find German people protesting, Show me a hydrant in an Asian city</em>), <strong>picture properties</strong> (<em>Give me black and white images of a skate manoeuvre</em>), <strong>category references</strong> (<em>Give me images of a snow sport</em>), <strong>non-essential properties</strong> (<em>Show me an empty kitchen</em>), <strong>counting references</strong> (<em>Show me a meeting room for 6 people</em>), <strong>events</strong> (<em>Show me a traffic accident between a double decker bus and a motorcycle</em>), <strong>actions</strong> (<em>Find a picture of a person feeding birds</em>), <strong>spatial relations</strong> (<em>Show me a sink with a view to trees outside</em>), <strong>normative deviation queries</strong> (<em>Give me images of a modified traffic sign</em>), <strong>affordance</strong> (<em>Give me images of a kind of object which can be used to extinguish fire</em>), <strong>internal textual references</strong> (<em>Show me images of trains to Chester</em>).</p>   </section>   <section id="sec-8">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Visual Question Answering: State-of-the-Art</h2>     </div>    </header>    <p>In comparison to existing datasets this challenge focuses more on capturing the semantic found inside the image, with the aim to better support reasoning mechanisms in complement to statistical learning. We therefore need to investigate whether current datasets are able to capture the semantic capabilities of systems. In the past, several communities hosted workshops and challenges (e.g., ImageCLEF from 2013 - 2018<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> or VQA 2016 - 2017&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]) creating a multitude of datasets. Also, the <a class="link-inline force-break" href="http://parl.ai">parl.ai</a> initiative&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>] provides a large set of benchmarks for Visual Question Answering.</p>    <p>For the sake of space in this paper, we just want to highlight only a few datasets, such as FVQADescription&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>], VQA version 1 and 2&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>] or CLEVR&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>].</p>    <p>Kafle et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>] wrote an overview article in 2016 which suggests the following features to categorize datasets: number of total images, number of question-answer pairs, distinct answers, human accuracy, number of words in longest question, longest answer, average answer length, image source, annotation method and evaluation type (such as open-ended (OE) or multiple-choice (MC)) and question types. In 2017, Wu et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>] used a different but partly overlapping set of features for their survey paper, namely: number of images, number of questions, image source, number of questions/image, number of question categories, question collection (aka annotation method), average question length, average answer length and evaluation metric. This challenge brings another dimension to discussion, which is the <em>types of inference</em> which can be expressed in information needs. For further information about other available methods or a detailed, structured overview over datasets, we refer the interested reader to two surveys&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>].</p>    <p>     <strong>Acknowledgements</strong> This work has been supported by the BMVI projects LIMBO (project no. 19F2029C) and OPAL (project no. 19F20284) as well as by the German Federal Ministry of Education and Research (BMBF) within &#x2019;KMU-innovativ: Forschung f&#x00FC;r die zivile Sicherheit&#x2019; in particular &#x2019;Forschung f&#x00FC;r die zivile Sicherheit&#x2019; and the project SOLIDE (no. 13N14456). Furthermore, we would like to thank the annotators Ria Hari Gusmati, Rricha Jalota, Paramjot Kaur, Hussain Abid Syed and Jan Reineke.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C.&#x00A0;Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In <em>      <em>2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015</em>     </em>. IEEE Computer Society, 2425&#x2013;2433. <a class="link-inline force-break" href="https://doi.org/10.1109/ICCV.2015.279"      target="_blank">https://doi.org/10.1109/ICCV.2015.279</a></li>     <li id="BibPLXBIB0002" label="[2]">I Cisco. 2012. Cisco visual networking index: Forecast and methodology, 2011&#x2013;2016. <em>      <em>CISCO White paper</em>     </em> (2012), 2011&#x2013;2016.</li>     <li id="BibPLXBIB0003" label="[3]">Fabr&#x00ED;cio Firmino, Andr&#x00E9; Freitas, Ricardos Usbeck, Alessio Sarullo, and Tingting Mu. 2018. Protocol for Question Answering Mediated by Visual Clues and Knowledge Graphs. https://visual-question-answering-challenge.github.io/protocol. (2018). [Online; accessed 04-March-2018].</li>     <li id="BibPLXBIB0004" label="[4]">Fabr&#x00ED;cio Firmino, Andr&#x00E9; Freitas, Ricardos Usbeck, Alessio Sarullo, and Tingting Mu. 2018. Question Answering Mediated by Visual Clues and Knowledge Graphs - WWW 2018 LYON, FRANCE. https://visual-question-answering-challenge.github.io. (2018). [Online; accessed 04-March-2018].</li>     <li id="BibPLXBIB0005" label="[5]">Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. <em>      <em>CoRR</em>     </em>abs/1612.00837(2016). arxiv:1612.00837<a class="link-inline force-break" href="http://arxiv.org/abs/1612.00837"      target="_blank">http://arxiv.org/abs/1612.00837</a></li>     <li id="BibPLXBIB0006" label="[6]">Klaus Greff, Rupesh&#x00A0;K Srivastava, Jan Koutn&#x00ED;k, Bas&#x00A0;R Steunebrink, and J&#x00FC;rgen Schmidhuber. 2017. LSTM: A search space odyssey. <em>      <em>IEEE transactions on neural networks and learning systems</em>     </em>28, 10(2017), 2222&#x2013;2232.</li>     <li id="BibPLXBIB0007" label="[7]">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In <em>      <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>     </em>. 770&#x2013;778.</li>     <li id="BibPLXBIB0008" label="[8]">Justin Johnson, Bharath Hariharan, Laurens van&#x00A0;der Maaten, Li Fei-Fei, C.&#x00A0;Lawrence Zitnick, and Ross&#x00A0;B. Girshick. 2016. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. <em>      <em>CoRR</em>     </em>abs/1612.06890(2016). arxiv:1612.06890<a class="link-inline force-break" href="http://arxiv.org/abs/1612.06890"      target="_blank">http://arxiv.org/abs/1612.06890</a></li>     <li id="BibPLXBIB0009" label="[9]">Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. 2015. Image retrieval using scene graphs. In <em>      <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>     </em>. 3668&#x2013;3678.</li>     <li id="BibPLXBIB0010" label="[10]">Kushal Kafle and Christopher Kanan. 2016. Visual Question Answering: Datasets, Algorithms, and Future Challenges. <em>      <em>CoRR</em>     </em>abs/1610.01465(2016). arxiv:1610.01465<a class="link-inline force-break" href="http://arxiv.org/abs/1610.01465"      target="_blank">http://arxiv.org/abs/1610.01465</a></li>     <li id="BibPLXBIB0011" label="[11]">Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. 2017. A hierarchical approach for generating descriptive image paragraphs. In <em>      <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>     </em>. IEEE, 3337&#x2013;3345.</li>     <li id="BibPLXBIB0012" label="[12]">Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David&#x00A0;A Shamma, <em>et al.</em> 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. <em>      <em>International Journal of Computer Vision</em>     </em>123, 1 (2017), 32&#x2013;73.</li>     <li id="BibPLXBIB0013" label="[13]">Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander&#x00A0;C Berg. 2016. Ssd: Single shot multibox detector. In <em>      <em>European conference on computer vision</em>     </em>. Springer, 21&#x2013;37.</li>     <li id="BibPLXBIB0014" label="[14]">A.&#x00A0;H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bordes, D. Parikh, and J. Weston. 2017. ParlAI: A Dialog Research Software Platform. <em>      <em>arXiv preprint arXiv:1705.06476</em>     </em>(2017).</li>     <li id="BibPLXBIB0015" label="[15]">Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In <em>      <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>     </em>. 779&#x2013;788.</li>     <li id="BibPLXBIB0016" label="[16]">Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei, and Christopher&#x00A0;D Manning. 2015. Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In <em>      <em>Proceedings of the fourth workshop on vision and language</em>     </em>. 70&#x2013;80.</li>     <li id="BibPLXBIB0017" label="[17]">Ji Wan, Dayong Wang, Steven Chu&#x00A0;Hong Hoi, Pengcheng Wu, Jianke Zhu, Yongdong Zhang, and Jintao Li. 2014. Deep learning for content-based image retrieval: A comprehensive study. In <em>      <em>Proceedings of the 22nd ACM international conference on Multimedia</em>     </em>. ACM, 157&#x2013;166.</li>     <li id="BibPLXBIB0018" label="[18]">Peng Wang, Qi Wu, Chunhua Shen, Anton van&#x00A0;den Hengel, and Anthony&#x00A0;R. Dick. 2016. FVQA: Fact-based Visual Question Answering. <em>      <em>CoRR</em>     </em>abs/1606.05433(2016). arxiv:1606.05433<a class="link-inline force-break" href="http://arxiv.org/abs/1606.05433"      target="_blank">http://arxiv.org/abs/1606.05433</a></li>     <li id="BibPLXBIB0019" label="[19]">Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony&#x00A0;R. Dick, and Anton van&#x00A0;den Hengel. 2016. Visual Question Answering: A Survey of Methods and Datasets. <em>      <em>CoRR</em>     </em>abs/1607.05910(2016). arxiv:1607.05910<a class="link-inline force-break" href="http://arxiv.org/abs/1607.05910"      target="_blank">http://arxiv.org/abs/1607.05910</a></li>     <li id="BibPLXBIB0020" label="[20]">Danfei Xu, Yuke Zhu, Christopher&#x00A0;B Choy, and Li Fei-Fei. 2017. Scene graph generation by iterative message passing. In <em>      <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>     </em>, Vol.&#x00A0;2.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://www.imageclef.org/">http://www.imageclef.org/</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18 Companion, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/> ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3192318">https://doi.org/10.1145/3184558.3192318</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
