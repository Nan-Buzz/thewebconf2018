<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>Machine Learning for the Peer Assessment Credibility</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">Machine Learning for the Peer Assessment Credibility</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Yingru</span>      <span class="surName">Lin</span>     University of Tasmania, private bag 87Hobart, TAS 7005, <a href="mailto:yingrulinn@gmail.com">yingrulinn@gmail.com</a>     </div>     <div class="author">     <span class="givenName">Soyeon Caren</span>      <span class="surName">Han</span>     University of Sydney, 1 Cleveland StreetSydney, NSW 2006, <a href="mailto:caren.han@sydney.edu.au">caren.han@sydney.edu.au</a>     </div>     <div class="author">     <span class="givenName">Byeong Ho</span>      <span class="surName">Kang</span>     University of Tasmania, private bag 87Hobart, TAS 7005, <a href="mailto:byeong.kang@utas.edu.au">byeong.kang@utas.edu.au</a>     </div>                 </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3184558.3186957" target="_blank">https://doi.org/10.1145/3184558.3186957</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>The peer assessment approach is considered to be one of the best solutions for scaling both assessment and peer learning to global classrooms, such as MOOCs. However, some academic staff hesitate to use a peer assessment approach for their classes due to concerns about its credibility and reliability. The focus of our research is to detect the credibility level of each assessment performed by students during peer assessment. We found three major scopes in assessing the credibility level of evaluations, 1) Informativity, 2) Accuracy, and 3) Consistency. We collect assessments, including comments and grades provided by students during the peer assessment process and then each feedback-and-grade pair is labeled with its credibility level by Mechanical Turk evaluators. We extract relevant features from each labeled assessment and use them to build a classifier that attempts to automatically assess its level of credibility in C5.0 Decision Tree classifier. The evaluation results show that the model can be used to automatically classify peer assessments as credible or non-credible, with accuracy in the range of 88%.</small>     </p>    </div>    <div class="CCSconcepts">     <p> <small> <span style="font-weight:bold;">CCS Concepts:</span> &#x2022;<strong> Computer systems organization </strong>&#x2192; <strong>Embedded systems;</strong> <em>Redundancy;</em> Robotics; &#x2022;<strong> Networks </strong>&#x2192; Network reliability;</small> </p>    </div>    <div class="classifications">     <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Peer Assessment ; Educational Data Mining ; Credibility Assessment</small>     </span>     </div>     <br/>     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Yingru Lin, Soyeon Caren Han, and Byeong Ho Kang. 2018. Machine Learning for the Peer Assessment Credibility. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 2 Pages. <a href="https://doi.org/10.1145/3184558.3186957" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3184558.3186957</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>MOOCs provide students video lectures with assignments, and use automated assessment, which precludes open-ended work, such as understanding and critiquing others&#x2019; work. However, it is impossible to apply traditional assessment approaches to these large online classes since it requires considerable overheads in time and cost for teaching staff to assess them with detailed feedback&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. The peer assessment approach is now considered as one of the best solutions for scaling both assessment and peer learning to the global classroom. However, some academic staff hesitate from using the peer assessment approach to their classes due to concerns about a lack of credibility and reliability&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. One concern is that students may engage in cronyism where students come to an informal agreement to give each other the highest mark&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>]. Another concern is whether students can evaluate their classmates as accurately as the instructor, who has a greater understanding of the assignment task&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. Several credibility index and manual validation models were proposed for improving the credibility of peer assessment results but it is impossible to change the model based on different education domains&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>].</p>    <p>     <strong>Credibility in Peer Assessment</strong> The focus of our research is detecting the credibility level of each assessment performed by students during peer assessment. We found three major scopes in assessing the credibility level of evaluations, 1) Informativity, 2) Accuracy, and 3) Consistency. First, we check whether the peer assessor conforms to the communicative principles by being informative to establish coherence and continuity in the formative assessment. An assessment without informative and reasonable context would not inspire individuals nor motivate their performance, and they indicated several relevant features&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>]. Secondly, the accuracy of each assessment is defined by whether the assessment feedback is aligned with the assignment marking guidelines provided by the instructor&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>]. The last scope &#x2019;Consistency&#x2019; can be verified with the consistency of peer assessment comment and mark. Patchan et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] mentioned that students who does not take their assessments seriously provide inconsistent ratings with random judgments.</p>   </section>   <section id="sec-5">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Methodology</h2>     </div>    </header>    <p>We focus on assessing the credibility levels of students&#x2019; assessments during the peer assessment process. The student&#x0027;s peer assessment data was collected from the online peer assessment system (PA system), which is implemented by the University of Tasmania (Australia). We collected the assessment data produced during semester 2, 2016 and the total number of assessment data is 13,198. The process for the online PA system can be described as follows: Teaching staff set assessment tasks and assessment criteria, allocate peer assessors for each student (assessee), and educates students in their role as an assessor. After completing the assignment task, the student submits his/her assignment to the peer assessment system. Each assessor reviews the assignment and gives an assessment score with comments supporting his/her assessment. Only the comments are available to the assessee. In return, the assessee evaluates the assessors&#x2019; comments and provides a feedback score.</p>    <p>Next, we focus on the credibility assessment labelling. In order to train a supervised classifier over the peer assessment data, the dataset must be labeled based on the level of peer assessment credibility. We asked Mechanical Turk human evaluators to indicate credibility level for each peer assessment. Each evaluator is provided a set of a peer assessment comments provided by students and assignment marking criteria organised by teaching staff. They are asked to label the credibility level for each assessment comment based on the Likert Scale (1 to 4): 1) strongly disagree, 2) disagree , 3) agree, and 4) strongly agree. We also asked evaluators to provide a short sentence to justify their answers, and we discarded evaluations lacking that justification sentence. Each task provided one assignment marking comment, one grade and the related criteria. We randomly selected 500 cases and asked evaluators to label the level of credibility of peer assessment. After labelling, we reviewed thirteen factors that are useful to estimate the level of credibility. Thirteen factors can be classified into three major scopes in assessing the credibility level of evaluations, 1) Informativity: the amount of informative and comprehensive context delivered by students (incl. word length, character length, existence of question mark, existence of examination mark, portion of nouns, and portion of adjectives), 2) Accuracy: the degrees of the conjunction of assignment criteria and feedback comments (incl. Cosine similarity, Jaccard similarity, and Divide and Conquer distance), and 3) Consistency: the level of consistency conveyed by students (incl. positive sentiment weight, negative sentiment weight, and the percentage of grade provided by students).</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Results for the credibility classification</span>     </div>     <table class="table">     <tbody>      <tr>       <td style="text-align:left;">Class</td>       <td style="text-align:center;">TP Rate</td>       <td style="text-align:center;">FP Rate</td>       <td style="text-align:center;">Prec.</td>       <td style="text-align:center;">Recall</td>       <td>F1</td>      </tr>      <tr>       <td style="text-align:left;">Credible</td>       <td style="text-align:center;">0.935</td>       <td style="text-align:center;">0.2391</td>       <td style="text-align:center;">0.9066</td>       <td style="text-align:center;">0.9351</td>       <td>0.9206</td>      </tr>      <tr>       <td style="text-align:left;">Non-Credible</td>       <td style="text-align:center;">0.7609</td>       <td style="text-align:center;">0.0649</td>       <td style="text-align:center;">0.8253</td>       <td style="text-align:center;">0.7609</td>       <td>0.7918</td>      </tr>      <tr>       <td style="text-align:left;">W.Avg.</td>       <td style="text-align:center;">0.8851</td>       <td style="text-align:center;">0.1891</td>       <td style="text-align:center;">        <strong>0.8832</strong>       </td>       <td style="text-align:center;">        <strong>0.8851</strong>       </td>       <td>0.8836</td>      </tr>     </tbody>     </table>    </div>    <p>Finally, we trained a supervised classifier to estimate the level of credibility of students&#x2019; peer assessment. To do this, we aggregate the &#x201D;strongly credible&#x201D; class and the &#x201D;credible&#x201D; class as a class &#x201D;Credible&#x201D;, and we aggregate the &#x201D;strongly non-credible&#x201D; class and the &#x201D;non-credible&#x201D; class as a class &#x201D;Non-Credible&#x201D;. In total, 574 cases correspond to the class &#x2019;Credible&#x2019; and 526 cases correspond to the class &#x201D;non-credible&#x201D;. We then evaluate the predictability of the credibility data. We tried a number of machine learning algorithms and the best results were achieved by a C5.0 decision tree. In the training/validation process we perform a 10-fold cross validation. As shown in Table&#x00A0;<a class="tbl" href="#tab1">1</a>. The performance for both classes is similar. The F1 in both classes is acceptable, indicating a good balance between precision and recall. The third row shows the weighted averaged performance results (88%) calculated across both classes. We also applied ROC curve for comparing performance of predicting credibility levels based on different feature groups . Figure&#x00A0;<a class="fig" href="#fig1">1</a> shows ROC curves comparing the performance with C5.0 Decision Tree and K-Nearest Neighbor at predicting credibility levels given three different feature groups: consistency-based feature group, informativity-based feature group, accuracy-based feature group and all three groups. we can see that AUCs in case of all features are the highest indicating that using all features have the highest accuracy rate. If all features are used, it takes multiple aspects into consideration at the same time, which gives a more uniformed result. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186957/images/www18companion-197-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">ROC curve (with C5.0 Decision Tree and K-Nearest Neighbour) at predicting future class participation</span>     </div>     </figure>    </p>   </section>   <section id="sec-6">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Conclusions</h2>     </div>    </header>    <p>The main objective of this research is to determine if we can automatically assess the level of credibility of assessment performed by students during the peer assessment process. The evaluation results show that the model can be used to automatically classify peer assessments in different domains as credible or non-credible, with accuracy in the range of 88%. Compared to other peer assessment credibility validation models, the proposed model can be used and updated in different education domains. We believe this paper is a first study of how a machine learning technique can be used for assessing the level of credibility in peer assessment. For future work, we plan to extend the experiments to larger datasets and explore more deeply the other factors that may lead students to declare an assessment as credible.</p>   </section>  </section>  <section class="back-matter">   <section id="sec-7">    <header>     <div class="title-info">     <h2>ACKNOWLEDGMENTS</h2>     </div>    </header>    <p>This work was supported by the Industrial Core Technology Development Program (10049079, Develop of mining core technology exploiting personal big data) funded by the Ministry of Trade, Industry and Energy (MOTIE, Korea). This work was also funded by the Asian Office of Aerospace Research and Development(AOARD) grant, #FA2386-16-1-404</p>   </section>   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">Burford Furman and William Robinson. 2003. Improving engineering report writing with calibrated peer review/sup TM. In <em>      <em>Frontiers in Education, 2003. FIE 2003 33rd Annual</em>     </em>, Vol.&#x00A0;2. IEEE, F3E_14&#x2013;F3E_16.</li>     <li id="BibPLXBIB0002" label="[2]">Douglas Magin. 2001. Reciprocity as a source of bias in multiple peer assessment of group work. <em>      <em>Studies in Higher Education</em>     </em>26, 1 (2001), 53&#x2013;63.</li>     <li id="BibPLXBIB0003" label="[3]">Melissa&#x00A0;M Patchan, Christian&#x00A0;D Schunn, and Russell&#x00A0;J Clark. 2017. Accountability in peer assessment: examining the effects of reviewing grades on peer ratings and peer feedback. <em>      <em>Studies in Higher Education</em>     </em>(2017), 1&#x2013;16.</li>     <li id="BibPLXBIB0004" label="[4]">Daniel Reinholz. 2016. The assessment cycle: a model for learning through peer assessment. <em>      <em>Assessment &#x0026; Evaluation in Higher Education</em>     </em>41, 2 (2016), 301&#x2013;315.</li>     <li id="BibPLXBIB0005" label="[5]">Sally S. Richmond, Kailasam Satyamurthy, and Joanna F. DeFranco. 2016. Exploring the Value of Peer Assessment. <em>      <em>American Society for Engineering Education</em>     </em>(2016).</li>     <li id="BibPLXBIB0006" label="[6]">Yang Song, Zhewei Hu, Edward&#x00A0;F Gehringer, Julia Morris, Jennifer Kidd, and Stacie Ringleb. 2016. Toward Better Training in Peer Assessment: Does Calibration Help?. In <em>      <em>EDM (Workshops)</em>     </em>.</li>     <li id="BibPLXBIB0007" label="[7]">Hoi&#x00A0;K Suen. 2014. Peer assessment for massive open online courses (MOOCs). <em>      <em>The International Review of Research in Open and Distributed Learning</em>     </em>15, 3(2014).</li>     <li id="BibPLXBIB0008" label="[8]">Seng&#x00A0;Yue Wong, Wee&#x00A0;Jing Tee, and Wei&#x00A0;Wei Goh. 2016. A Comparative Analysis Between Teacher Assessment and Peer Assessment in Online Assessment Environment for Foundation Students. In <em>      <em>Assessment for Learning Within and Beyond the Classroom</em>     </em>. Springer, 381&#x2013;389.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186957">https://doi.org/10.1145/3184558.3186957</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
