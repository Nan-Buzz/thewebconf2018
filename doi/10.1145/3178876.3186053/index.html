<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Multi-Task Pharmacovigilance Mining from Social Media
  Posts</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Multi-Task Pharmacovigilance
          Mining from Social Media Posts</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Shaika</span> <span class=
          "surName">Chowdhury</span>, Department of Computer
          ScienceUniversity of Illinois at Chicago Chicago, IL
          60607, <a href=
          "mailto:schowd21@uic.edu">schowd21@uic.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Chenwei</span> <span class=
          "surName">Zhang</span>, Department of Computer
          ScienceUniversity of Illinois at Chicago Chicago, IL
          60607, <a href=
          "mailto:czhang99@uic.edu">czhang99@uic.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Philip S.</span> <span class=
          "surName">Yu</span>, Department of Computer
          ScienceUniversity of Illinois at Chicago Chicago, IL
          60607, <a href="mailto:psyu@uic.edu">psyu@uic.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186053"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186053</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Social media has grown to be a crucial
        information source for pharmacovigilance studies where an
        increasing number of people post adverse reactions to
        medical drugs that are previously unreported. Aiming to
        effectively monitor various aspects of Adverse Drug
        Reactions (ADRs) from diversely expressed social medical
        posts, we propose a multi-task neural network framework
        that learns several tasks associated with ADR monitoring
        with different levels of supervisions collectively. Besides
        being able to correctly classify ADR posts and accurately
        extract ADR mentions from online posts, the proposed
        framework is also able to further understand reasons for
        which the drug is being taken, known as ‘indications’, from
        the given social media post. A <em>coverage-based</em>
        attention mechanism is adopted in our framework to help the
        model properly identify ‘phrasal’ ADRs and Indications that
        are attentive to multiple words in a post. Our framework is
        applicable in situations where limited parallel data for
        different pharmacovigilance tasks are available. We
        evaluate the proposed framework on real-world Twitter
        datasets, where the proposed model outperforms the
        state-of-the-art alternatives of each individual task
        consistently.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Content analysis and feature selection;</strong>
        <strong>Information extraction;</strong> •
        <strong>Computing methodologies</strong> →
        <strong>Information extraction;</strong> <em>Multi-task
        learning;</em> <em>Neural networks;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Multi-Task
          Learning</small>,</span> <span class=
          "keyword"><small>Pharmacovigilance</small>,</span>
          <span class="keyword"><small>Adverse Drug
          Reaction</small>,</span> <span class=
          "keyword"><small>Attention Mechanism</small>,</span>
          <span class="keyword"><small>Coverage</small>,</span>
          <span class="keyword"><small>Recurrent Neural
          Network</small>,</span> <span class=
          "keyword"><small>Social Media</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Shaika Chowdhury, Chenwei Zhang, and Philip S. Yu. 2018.
          Multi-Task Pharmacovigilance Mining from Social Media
          Posts. In <em>WWW 2018: The 2018 Web Conference,</em>
          <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New
          York, NY, USA</em> 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186053" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186053</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Various prescription drugs intended to be taken for
      medical treatment are released into the market. However,
      studies have found that many of such drugs can be
      counterproductive [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>]. The harmful reactions or injuries
      that are caused by the intake of drugs are known as Adverse
      Drug Reactions (ADR) [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>] and happens to be the fourth leading
      cause of death in the United States [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>]. Preventive steps such as
      ADR monitoring and detection, which are collectively called
      Pharmacovigilance, are critical to ensure safety to patients’
      health. Initial activities toward pharmacovigilance during
      clinical trials fail to reliably signal all the negative
      effects that could potentially be caused by a drug due to
      certain restrictions, calling the need for post-market ADR
      surveillance. Nevertheless, potentially harmful drugs remain
      unflagged as traditional post-market ADR monitoring methods
      suffer from under-reporting, incomplete data, and delays in
      reporting [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>].</p>
      <p>Owing to the presence of large user base on social media,
      a vast amount of data gets generated. Compared to clinical
      information retrieval from electronic health records (EHR)
      that pose the problem of limited access [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>], the free accessibility
      of web data presents a lucrative source of medical data. This
      data has recently caught the attention of researchers for
      public health monitoring[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0026">26</a>]. Task-oriented crowdsourcing
      platforms are designed to collect patient feedbacks as
      introduced in [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>]. A popular platform where
      health-related data in the form of posts/ tweets are
      exchanged between users is Twitter<a class="fn" href="#fn1"
      id="foot-fn1"><sup>1</sup></a>. The posts cover a wide range
      of topics concerning health such as experience getting a
      disease/illness, symptoms of it, the drugs that were taken
      and harmful reactions from them.</p>
      <p>Social media platforms offer a robust source of
      health-related data that patients are found to consult to
      learn about other's shared health-related experiences
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0041">41</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0042">42</a>]. The fact
      that this data is up-to-date and is generated by patients
      overcomes the weaknesses of traditional ADR surveillance
      techniques. Thus social media is indispensably important and
      could complement traditional information sources for more
      effective pharmacovigilance studies, as well as potentially
      serve as an early warning system for unknown ADRs [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>]. However,
      users with different background knowledge or various
      linguistic preferences tend to generate social media posts
      with diverse expressions and ambiguous mentions, which pose a
      series of challenges for pharmacovigilance studies:</p>
      <ul class="list-no-style">
        <li id="list1" label="•"><strong>Diversely expressed
        ADRs</strong>: In Twitter, the use of non-medical terms to
        craft one's tweet is very prevalent. For example, <em>never
        sleeping</em> or <em>sleep deprived</em> being used to
        describe the ADR <em>insomnia</em>. These ‘phrasal’ ADRs
        framed with casual words could easily merge with other
        irrelevant words in the post and make the detection task
        more difficult.<br /></li>
        <li id="list2" label="•"><strong>Indications misidentified
        as ADRs</strong>: An ‘Indication’ in a health-related post
        can be defined as a medical condition, disease or illness
        for which the drug has been prescribed to treat it. As both
        indications and ADRs can be referred to by the same drug,
        indications could be easily misidentified for ADRs. Without
        a deep understanding of the role that each symptom plays in
        a post, such misidentification could be
        problematic.<br /></li>
      </ul>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186053/images/www2018-62-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">Pharmacovigilance reported in social media
          posts.</span>
        </div>
      </figure>
      <p>Figure <a class="fig" href="#fig1">1</a> shows common
      tweets for Pharmacovigilance taken from PSB 2016 Social Media
      Shared Task Twitter dataset [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>], where the token in red denotes the
      ‘Drug’ being taken, blue denotes the ‘Indication’, and green
      the ‘ADR’. In Tweet 1, the drug <em>Paroxetine</em> is being
      taken to treat <em>panic attacks</em>, with an adverse
      reaction of weight gain mentioned as <em>fat</em>. So
      <em>panic attacks</em> is an instance of Indication while
      <em>fat</em> is the ADR. In Tweet 2 while the drug
      <em>Paxil</em> is prescribed to treat the indication
      <em>depression</em>, it causes ADRs of <em>gain unwanted
      pounds</em> and <em>water weight</em>. In both cases it is
      possible for the Indications ’panic attacks’ and ’depression’
      to get mislabeled as both commonly occur as ADRs as well.</p>
      <p>All previous works [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>] in pharmacovigilance studies have
      focused on solving the ADR tasks separately. As a tweet
      containing ADR mention can also have other medical mentions
      such as Indications or beneficial effects, it is important to
      incorporate these representations to ensure disambiguity when
      trying to learn each task. Lexicon-based approaches
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0017">17</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0039">39</a>] have been
      commonly adopted in earlier approaches, where ADR tokens in a
      text fragment are looked up in a corpus containing ADR
      mentions. However, the occurrence of non-medical
      terminologies to describe ADRs in social media makes them
      unsuitable. While machine learning approaches such as Naive
      Bayes, Support Vector Machine and Maximum Entropy [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0014">14</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0040">40</a>] require
      the use of hand-engineered features.</p>
      <p>Trying to conquer these problems, we propose a multi-task
      framework based on the sequence learning model that jointly
      learns several related pharmacovigilance tasks. Mining from
      pharmacovigilance forms an interesting multi-task problem as
      complimentary pharmacovigilance tasks with supervision at
      different levels can be jointly learned. The data for these
      related tasks share semantic and syntactic similarities, and
      leveraging these shared representations can improve learning
      efficiency and prediction accuracy. The joint learning of the
      objective functions of the tasks can transfer what is learned
      from one task to other tasks and improve them. This is
      suitable for pharmacovigilance tasks as they contain data
      with occurrences of multiple medical terms, which otherwise
      makes identification of a certain category (i.e ADR or
      Indication) of medical term difficult. We incorporate these
      ideas by extending the basic recurrent neural network
      encoder-decoder such that our multi-task model shares an
      encoder among all the tasks and uses a different decoder for
      each task. Our assumption is that the shared encoder would
      learn predictive representations capturing the nuances of
      each task and, hence, help disambiguate an ’ADR’ from an
      ’Indication’. On the other hand, a task-specific decoder
      decodes the shared encoder representation to produce
      task-specific output. Furthermore, by incorporating
      multi-granular supervision through different tasks, the
      decoder can successfully produce output at the sentence and
      word level.</p>
      <figure id="fig2">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186053/images/www2018-62-fig2.jpg"
        class="img-responsive" alt="Figure 2" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 2:</span> <span class=
          "figure-title">Model Architecture. We use different
          colors for each task-specific decoder. The shade of the
          colored block indicates its value. For example, the
          second word <em>panic</em> is an Indication, so gets
          higher attention weight (the dark blue block), which
          helps in successfully labeling it as an
          Indication.</span>
        </div>
      </figure>
      <p></p>
      <p>Recent years have seen a spike in the use of
      sequence-to-sequence models for difficult learning tasks like
      machine translation [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0034">34</a>] and summarization [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>]. A sequence-to-sequence
      model takes a sequence as input into an encoder, projects it
      to an intermediate encoded representation, which is then
      passed through a decoder to generate a sequence as the
      output. The proposed architecture shown in Figure <a class=
      "fig" href="#fig2">2</a> is a recurrent neural network-based
      encoder-decoder model augmented with attention and coverage
      mechanism to handle multiple tasks for pharmacovigilance. The
      reason for using a sequence to sequence model to model the
      multiple tasks is to be able to capture their shared
      representations with the encoder and generate outputs for
      each task with multiple decoders. We propose a multi-task
      learning framework for three pharmacovigilance tasks — ADR
      Classification, ADR Labeling and Indication Labeling—
      hypothesizing that the interactions between the tasks through
      joint learning would improve learning and generalization
      ability of each individual task. While most works on ADR
      classification and detection have tried to learn a single
      objective function, by jointly learning multiple objective
      functions for the tasks we introduce a novel approach for
      pharmacovigilance that is seen to boost the performance.
      Moreover, the learned features from multiple tasks help to
      reduce the false positives in mislabeling Indications as ADRs
      and vice versa. When an ADR\Indication occurs as a phrase
      rather than a single word, it can make the detection task
      more difficult. Adding <em>coverage</em> to the attention
      mechanism helps overcome this as it accumulates the attention
      from all the previous decoder time steps and facilitates in
      learning all previous ADR\Indication words in a phrase.</p>
      <p>The main contributions of this work are summarized as
      follows:</p>
      <ol class="list-no-style">
        <li id="list3" label="(1)">Designed a unified machine
        learning framework to learn several pharmacovigilance tasks
        from social media posts simultaneously. To the best of our
        knowledge, this problem has not been studied carefully and
        has scope for novel research.<br /></li>
        <li id="list4" label="(2)">Adding <em>coverage</em> to
        attention mechanism has shown to improve detection of not
        only ’phrasal’, but also single worded ADRs and
        Indications.<br /></li>
        <li id="list5" label="(3)">State-of-the-art in terms of
        results obtained on real-world Twitter datasets compared to
        contemporary work in pharmacovigilance.<br /></li>
      </ol>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Preliminary:
          Tasks</h2>
        </div>
      </header>
      <p>With our multi-task framework, we jointly learn three
      pharmacovigilance tasks, where the input tweet representation
      is encoded through a shared encoder. Each task is modeled as
      a sequence classification or sequence labeling problem.
      Description of each task is given below.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> ADR
            Classification</h3>
          </div>
        </header>
        <p>This is a binary classification task to separate out the
        ADR assertive posts. The two classes are ‘ADR’ and ‘NotADR’
        where ‘ADR’ label indicates a post with ADR mention in it.
        While ‘NotADR’ means it does not have any ADR, although can
        have other medical terms such as drugs and Indications.</p>
        <p>It tries to learn a function that maps a sequence
        <strong>x</strong> to a class label <em>l</em> ∈
        <em>R<sup>L</sup></em> where <em>L</em> is the total number
        of classes [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0038">38</a>] and <em>L</em> = 2.</p>
        <div class="table-responsive" id="Xeq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} {\bf x}
            \rightarrow l, l\in R^L \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> ADR
            Labeling</h3>
          </div>
        </header>
        <p>A sequence labeling task aiming to identify ADR in a
        post. The detected ADRs are tagged with the ‘ADR’ label. It
        tries to find the most likely sequence of tags given the
        input sequence <strong>x</strong>, that is the sequence
        with the highest probability.</p>
        <div class="table-responsive" id="Xeq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} {\mathbf
            {y}}^{\prime } = \mathop {\operatorname{argmax}
            }\limits _y P(y|x) \end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <p></p>
        <p>We use two tags to annotate the tokens of the input for
        ADR labeling. ’ADR’ tag corresponds to a token with ADR
        mention and ’O’ tag denotes a non-ADR word.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Indication
            Labeling</h3>
          </div>
        </header>
        <p>A sequence labeling task aiming to identify Indication
        in a post. The detected Indications are tagged with the
        ‘Indication’ label. Like ADR labeling, it tries to find the
        tagged sequence of highest probability.</p>
        <p>We used two tags to annotate the tokens of the input for
        Indication labeling. ‘IND’ tag corresponds to a token with
        Indication mention and ‘O’ tag denotes a non-Indication
        word.</p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Our Model</h2>
        </div>
      </header>
      <p>Our proposed multi-task framework is depicted in Figure
      <a class="fig" href="#fig2">2</a>. It is composed mainly of
      three components — embedding module, encoder and decoder. The
      embedding module is intended to capture the meaning and
      semantic associations between pharmacovigilance words. All
      the tasks have a common encoder so that shared
      representations can be generated, capturing both the ADR and
      Indication contextual information. Lastly, the decoders
      employ a combined attention and coverage mechanism to
      facilitate the detection of ADR and Indications of various
      lengths. Each component of our RNN Encoder-Decoder model is
      described in detail in the following subsections.</p>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Input</h3>
          </div>
        </header>
        <p>Our input <strong>x</strong> = (<em>x</em> <sub>1</sub>,
        <em>x</em> <sub>2</sub>, ..., <em>x<sub>T</sub></em> ) is a
        piece of text corresponding to a social media post
        comprising a sequence of <em>T</em> words, where each
        <em>x<sub>i</sub></em> represents a word in the vocabulary
        of size <em>V</em>.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Word
            Representations</h3>
          </div>
        </header>
        <p>Medical terms in a tweet post can take different roles
        and the same medical word or phrase can mean different
        things in different context. For example, ‘panic attacks’
        can occur both as an ADR as well as an Indication depending
        on the mention of the drug and other patterns. In order to
        capture their meanings as well as the semantic
        relationships and context they are being used in, we
        generate their word embeddings.</p>
        <p>The character representation of each word is also
        generated to capture its morphological features. These
        character representations can help in capturing the
        representations for words such as ‘sleeping’ where the word
        embedding matrix might have an entry for just ‘sleep’. This
        character representation is similar to that implemented for
        doing Named Entity Recognition [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0029">29</a>].</p>
        <p>The final representation of a word is the concatenation
        of its word embedding and character representations to
        incorporate users’ diverse expressions.</p>
        <p><strong>Word Embedding</strong>: For each word
        <em>x<sub>t</sub></em> we get its corresponding
        low-dimensional dense vector, <span class=
        "inline-equation"><span class=
        "tex">$e\_{word_t}$</span></span> , by looking up in a V x
        m size word-embedding matrix, where m is the dimensions of
        the word-embedding.</p>
        <p><strong>Character Representation</strong>: Each word in
        a sentence can be denoted as <em>x<sub>t</sub></em> =
        (<em>c</em>1, <em>c</em>2, ...) such that
        <em>c<sub>t</sub></em> ∈ <em>R<sup>G</sup></em> , where G
        is the vocabulary size of all the characters. Similar to
        word embedding, we first get the character embedding vector
        for each character by looking up in a G x p
        character-embedding matrix, where p is the dimensions of
        the character-embedding. The sequence of
        character-embedding vectors, <span class=
        "inline-equation"><span class=
        "tex">$e\_{char_t}$</span></span> , of the word is then fed
        to a bidirectional LSTM [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>]. The final character
        representation, <span class="inline-equation"><span class=
        "tex">$e^{\prime }\_{char_t}$</span></span> , is obtained
        by concatenating the forward and backward final states.</p>
        <p>The final word embedding vector <em>e<sub>t</sub></em>
        for each word is thus,</p>
        <div class="table-responsive" id="Xeq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} e_t =
            \left[e\_{word_t}, e^{\prime }\_{char_t}\right]
            \end{equation}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Encoder</h3>
          </div>
        </header>
        <p>We use a single layer Bi-Directional RNN as the encoder,
        with LSTM as the basic recurrent unit due to its ability to
        incorporate long-term dependencies [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0012">12</a>]. As the purpose of the
        encoder is to capture the shared representations of the
        multiple tasks, these representations should include
        contextual information in both the forward and backward
        directions, where the outputs of each task can depend on
        these previous and future elements in the sequence. So we
        pass the input sequence through a Bi-LSTM to serve this
        purpose. The Bi-LSTM achieves this by passing the input
        sequence in its original order through a forward LSTM,
        which encodes a hidden state <span class=
        "inline-equation"><span class=
        "tex">$\overrightarrow{h}_t$</span></span> for each time
        step t. Also, a reversed copy of the input sequence is
        passed through a backward LSTM, that encodes a hidden state
        <span class="inline-equation"><span class=
        "tex">$\overleftarrow{h}_t$</span></span> . The forward and
        backward hidden states are then concatenated to represent
        the final hidden state of the encoder at each time
        step.</p>
        <div class="table-responsive" id="Xeq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} h_t =
            \left[\overrightarrow{h}_t, \overleftarrow{h}_t\right]
            \end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>
        <div class="table-responsive" id="Xeq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \overrightarrow{h}_t = LSTM\left[x_t,
            \overrightarrow{h}_{t-1}\right]
            \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <div class="table-responsive" id="Xeq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            \overleftarrow{h}_t = LSTM\left[x_t,
            \overleftarrow{h}_{t-1}\right]
            \end{equation}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span>
            Decoder</h3>
          </div>
        </header>
        <p>On the decoder side, we allocate a single layer
        uni-directional LSTM with attention mechanism for each task
        in order to produce output specifically for that task. The
        <em>Coverage</em> mechanism is integrated in the attention
        mechanism to give the model a sense of how much attention
        it has already assigned so far.</p>
        <section id="sec-14">
          <p><em>3.4.1 Attention Mechanism.</em> Conventional
          Encoder-Decoder for sequence-to-sequence tasks has shown
          to have limitations as it tries to decode a fixed-length
          encoded vector when generating output [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0002">2</a>]. The
          fact that the last encoder hidden state is required to
          hold the summary of all the timesteps, in practice,
          doesn't hold, especially, for longer sentences [<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0006">6</a>].</p>
          <p>The use of attention has come a long way in tasks
          ranging from image captioning [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0036">36</a>] to
          machine translation [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0006">6</a>]. By peeking over all the encoder
          states and weighing them according to their relevance to
          the current output to generate, it produces the
          <em>attention distribution</em>. This attention
          distribution gives a signal where to attend to more in
          the input sequence.</p>
          <p>We use additive attention from [<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0002">2</a>], where
          <em>attention distribution a<sub>t</sub></em> is
          calculated as:</p>
          <div class="table-responsive" id="Xeq7">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} {{\it
              g}_t}_i = {{\it v}_a^T}tanh\left(W_ah_i + W_bs_{t-1}
              + b_{attn}\right) \end{equation}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>
          <div class="table-responsive" id="Xeq8">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} a_t =
              softmax\left({{\it g}}_t\right)
              \end{equation}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>
          <p></p>
          <p><em>h<sub>i</sub></em> is an encoder hidden state and
          <em>s</em> <sub><em>t</em> − 1</sub> is the previous
          decoder hidden state at decoding timestep t.
          <em>v<sub>a</sub></em> , <em>W<sub>a</sub></em> ,
          <em>W<sub>b</sub></em> , and <em>b<sub>attn</sub></em>
          are learnable attention parameters. Finding an attention
          score using the attention function <em>g<sub>t</sub></em>
          <sub><em>i</em></sub> (<em>h<sub>i</sub></em> ,
          <em>s</em> <sub><em>t</em> − 1</sub>) between each
          encoder state <em>h<sub>i</sub></em> and the decoder
          state preceding the current output state, tells exactly
          where to pay attention to in the input sequence when
          decoding.</p>
          <p>The encoder hidden states are then combined with the
          attention distribution to give the <em>context
          vector</em> defined as the attention weighted sum of the
          encoder hidden states,</p>
          <div class="table-responsive" id="Xeq9">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} con_t = \sum
              \nolimits _{i} h_i{a_t}_i \end{equation}</span><br />
              <span class="equation-number">(9)</span>
            </div>
          </div>
          <p></p>
          <p>This context vector, along with the previous decoder
          hidden state <em>s</em> <sub><em>t</em> − 1</sub>,
          outputs the label generated at previous timestep,
          <em>y</em> <sub><em>t</em> − 1</sub>. The aligned encoder
          hidden state <em>h<sub>t</sub></em> is used to compute
          the decoder state <em>s<sub>t</sub></em> at time step t
          for ADR and Indication labeling tasks.</p>
          <div class="table-responsive" id="Xeq10">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} s_t = {\it
              f}\left(s_{t-1},h_t, y_{t-1}, {con}_t\right)
              \end{equation}</span><br />
              <span class="equation-number">(10)</span>
            </div>
          </div>
          <p></p>
          <p>Whereas for the ADR Classification task, the decoder
          state at time step t is a function of <em>s</em>
          <sub><em>t</em> − 1</sub> and <em>con<sub>t</sub></em>
          .</p>
          <div class="table-responsive" id="Xeq11">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} s_t = {\it
              f}\left(s_{t-1}, {con}_t\right)
              \end{equation}</span><br />
              <span class="equation-number">(11)</span>
            </div>
          </div>
          <p></p>
          <p>For all the tasks, initial decoder hidden state
          <em>s</em> <sub>0</sub> is set to the final encoder
          hidden state.</p>
        </section>
        <section id="sec-15">
          <p><em>3.4.2 Coverage Mechanism.</em> Due to the
          colloquial nature of conversations on social network,
          ADRs are expressed in everyday language and also take
          phrasal form. Consider the following example tweet,
          <em>Just took Seroquel. Now I'm freaking out that I will
          end up sleeping 15 hrs and miss my 12pm appt
          tomorrow</em>, where <em>sleeping 15 hrs</em> is an ADR
          taking a phrase form. The phrasal ADRs can also be
          expressed as a list of ADRs, illustrated by the tweet
          <em>@user I hated Effexor. It makes you hungry, dizzy,
          and lethargic. That culminated in a large weight gain for
          me</em>, where <em>hungry</em>, <em>dizzy</em>, and
          <em>lethargic</em> are a list of ADRs occurring as a
          phrase for the drug Effexor. As we found from preliminary
          experiments that with just attention it cannot detect all
          the ADR words in these phrases, we introduce
          <em>coverage</em> into our decoder to solve this problem.
          For every decoder timestep we keep track of a coverage
          vector <em>c<sub>t</sub></em> , implemented as in
          [<a class="bib" data-trigger="hover" data-toggle=
          "popover" data-placement="top" href=
          "#BibPLXBIB0033">33</a>] for summarization. The coverage
          vector sums up the attention distribution from all the
          previous decoder timesteps. However, unlike in [<a class=
          "bib" data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0033">33</a>] we
          impose a window for the previous decoder timesteps and
          set it to 3. We do this as most ADRs are comprised of few
          words and considering attention from all the previous
          timesteps can include attention from non-ADR words, which
          can jeopardize the attention distribution of ADR phrase
          located at the end of a post. With the coverage vector,
          in case if a word in a phrasal ADR does not get
          identified, the attentive words based on the attention
          distribution for the neighboring ADR words can help
          locate it. In other words, it helps secure the ADR label
          for the other words in a phrasal ADR. Coverage also
          contributes in avoiding mislabeling an Indication as an
          ADR and vice versa as it implicitly keeps track of the
          location of a word. That is, unlike conventional additive
          attention - which only uses encoder states
          <em>h<sub>t</sub></em> to find the attentive words - by
          summing up attention values assigned so far, it provides
          information about how far the model has attended from the
          beginning of the sentence. This way it learns the
          boundaries of attention for the ADR and Indication words.
          It is defined as,</p>
          <div class="table-responsive" id="Xeq12">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} c_t = \sum
              \limits _{s=t-1}^{s=t-3} a_s
              \end{equation}</span><br />
              <span class="equation-number">(12)</span>
            </div>
          </div>
          <p></p>
          <p>With only attention mechanism, if for the word
          <em>sleeping</em> in the first example attention was
          focused on the words <em>Seroquel</em>, <em>freaking</em>
          and <em>out</em> to identify it as an ADR, it does not
          guarantee that it will attend to similar words for
          <em>15</em> and recognize it as a part of the ADR phrase
          since <em>15</em> can frequently appear with words
          irrelevant to ADR mention. By augmenting attention
          mechanism with <em>coverage</em>, we hypothesize that by
          also focusing on the words that were attended to in the
          previous time steps, <em>15</em> can be properly tagged
          as an ADR. That is, when decoding <em>15</em> it will pay
          attention to the words <em>Seroquel</em>,
          <em>freaking</em> and <em>out</em>, that were helpful in
          tagging <em>sleeping</em> correctly. Hence this hybrid
          attention with coverage module can enable capturing the
          words in an ADR phrase, which otherwise just with
          attention can get missed.</p>
          <p>We update the attention distribution to the following
          as coverage is now part of the attention mechanism,</p>
          <div class="table-responsive" id="Xeq13">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} {{\it
              g}_t}_i = {{\it v}_a^T}tanh\left(W_ah_i + W_bs_{t-1}
              + W_c{c_t}_i + b_{attn}\right)
              \end{equation}</span><br />
              <span class="equation-number">(13)</span>
            </div>
          </div>
          <p></p>
        </section>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span> Output</h3>
          </div>
        </header>
        <p>Our task-specific output is calculated as the
        following,</p>
        <p><strong>ADR Classification</strong>: The context vector
        can be viewed as a high-level representation of the post
        and is used as features for the classification task.</p>
        <div class="table-responsive" id="Xeq14">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} y =
            softmax\left(W_{class}{con} + b_{class}\right),
            \end{equation}</span><br />
            <span class="equation-number">(14)</span>
          </div>
        </div>where <em>W<sub>class</sub></em> and
        <em>b<sub>class</sub></em> are learnable parameters.
        <p></p>
        <p><strong>ADR Labeling and Indication Labeling</strong>:
        The context vector at each decoder timestep,
        <em>con<sub>i</sub></em> , can be viewed as the final
        representation for the word, which is used with the decoder
        state, <em>s<sub>i</sub></em> , to predict the output at
        that timestep.</p>
        <div class="table-responsive" id="Xeq15">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} y_i =
            softmax\left(W^{\prime }\left[s_i, {con}_i\right] +
            b^{\prime }\right), \end{equation}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>where W’ and b’ are learned during training
        independently for the ADR labeling and Indication labeling
        tasks.
        <p></p>
      </section>
    </section>
    <section id="sec-17">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Training the
          Model</h2>
        </div>
      </header>
      <p>We train our model jointly over all the datasets.
      Cross-entropy is used as the loss function during training,
      which is defined for each task as:</p>
      <p><strong>ADR Classification</strong>:</p>
      <div class="table-responsive" id="Xeq16">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} L\left(\theta
          _{class}\right) = - \frac{1}{n}\sum \nolimits _x
          {\left({{Y_x}\log {{Y^{\prime }}_x} + \left({1 - {Y_x}}
          \right)\log (1 - {{Y^{\prime }}_x})} \right)},
          \end{equation}</span><br />
          <span class="equation-number">(16)</span>
        </div>
      </div>where n is the training data size, x means over all
      inputs, Y is the true label and Y’ the predicted label
      probability. <em>θ<sub>clas</sub></em> =
      (<em>θ<sub>src</sub></em> , <em>θ</em> <sub>1</sub>), where
      <em>θ<sub>src</sub></em> is a collection of shared parameters
      among all the tasks for the encoder and <em>θ</em>
      <sub>1</sub> are the parameters for ADR Classification
      decoder.
      <p></p>
      <p><strong>ADR Labeling</strong>:</p>
      <div class="table-responsive" id="Xeq17">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} L\left(\theta
          _{ADRlab}\right) = \frac{1}{n}\sum \nolimits _{x}{\log
          {P}\left(Y_x|X_x;\theta _{ADRlab}\right)},
          \end{equation}</span><br />
          <span class="equation-number">(17)</span>
        </div>
      </div>where <em>θ<sub>ADRlab</sub></em> =
      (<em>θ<sub>src</sub></em> , <em>θ</em> <sub>2</sub>) and
      <em>θ</em> <sub>2</sub> is the parameter for ADR Labeling
      decoder.
      <p></p>
      <p><strong>Indication Labeling</strong>:</p>
      <div class="table-responsive" id="Xeq18">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation} L\left(\theta
          _{INDlab}\right) = \frac{1}{n}\sum \nolimits _{x}{\log
          {P}\left(Y_x|X_x;\theta _{INDlab}\right)},
          \end{equation}</span><br />
          <span class="equation-number">(18)</span>
        </div>
      </div>where <em>θ<sub>INDlab</sub></em> =
      (<em>θ<sub>src</sub></em> , <em>θ</em> <sub>3</sub>) and
      <em>θ</em> <sub>3</sub> is the parameter for Indication
      Labeling decoder.
      <p></p>
      <p>During training, we use the weighted loss which consists
      of a total number of T tasks:</p>
      <div class="table-responsive" id="Xeq19">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation}
          total\hspace{2.84544pt}loss = \sum \limits _{t=1}^{T}
          \omega _tL_t. \end{equation}</span><br />
          <span class="equation-number">(19)</span>
        </div>
      </div>
      <p></p>
      <p>Total loss is a linear combination of loss for all tasks,
      where <em>ω<sub>t</sub></em> is the weight for each task t
      respectively.</p>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experimental
          Settings</h2>
        </div>
      </header>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span>
            Datasets</h3>
          </div>
        </header>
        <p><strong>ADR Classification</strong>: We use the Twitter
        dataset from PSB 2016 Social Media Shared Task for ADR
        Classification [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0032">32</a>]. This dataset was created from
        tweets collected using generic and brand names of the
        drugs, along with their phonetic misspellings. It contains
        binary annotations of ‘0’ and ‘1’ referring to ‘ADR’ and
        ‘notADR’ respectively. Although the original dataset
        contains a total of 10,822 tweets, we could download only
        7044 tweets from the given IDs. We consider only the 1081
        tweets out of these that overlap with the tweets in ADR
        Labeling dataset as belonging to ‘ADR’ class. Another 1081
        tweets are randomly sampled from the ‘notADR’ class. This
        is done as all the tasks share the same encoder and we
        exploit the shared input representations. We manually
        labeled the supplemental dataset discussed below and add it
        to the existing dataset. We divided the tweets randomly
        into training, test and validation datasets with splits of
        70%-15%-15%.</p>
        <p><strong>ADR Labeling</strong>: For ADR Labeling, we use
        the Twitter dataset from PSB 2016 Social Media Shared Task
        for ADR Extraction [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0032">32</a>]. It contains around 2000 tweets
        annotated with tweet ID, start offset, end offset, semantic
        type (ADR/Indication), UMLS ID, annotated text span and the
        related drug. However, at the time of this study 1081 of
        the annotated tweets were available for download. We
        supplemented this dataset with a small dataset [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0007">7</a>] of 203
        tweets collected between May 2015 to December 2015 from
        Twitter. We use the split 70%-15%-15% for the training,
        test and validation datasets. We customized this dataset to
        only include the ADR annotations.</p>
        <p><strong>Indication Labeling</strong>: The corpus and
        splits used for indication labeling are the same as that
        for ADR Labeling. We customized this dataset to only
        include the Indication annotations.</p>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Training
            Details</h3>
          </div>
        </header>
        <p>Our implementation is based on the open source deep
        learning package Tensorflow [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>]. Glove [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0028">28</a>] toolkit was used to
        pre-train the word embeddings, which were then used to
        initialize the embeddings in the model. We specifically
        used the model trained on Twitter in Glove to be able to
        generate the word-embeddings of the words unique to our
        Twitter dataset. We set the number of units in the LSTM
        cell to 128 and the dimensionality of word and char
        embeddings are set to 200 and 128 respectively. All the
        forget biases in the LSTMs are set to 1. In every epoch, we
        perform mini-batch training of each parallel task corpus
        with a batch size of 16. Regularization is done on the
        non-recurrent connections with a dropout rate of 0.5. We
        used Adam Optimization [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>] method with a learning rate of 0.1
        during training. All weights and biases in the attention
        and coverage component were initialized with Xavier
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0011">11</a>]
        initialization. The development set was used to tune the
        value of the hyper-parameter weight, <em>ω</em>, for the
        loss of each task. The setting 1, 1 and 0.1 for the losses
        of ADR labeling, Indication labeling, and ADR
        classification are used for the final experiments as it
        gave best results on the development set.</p>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Results and
          Discussion</h2>
        </div>
      </header>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span>
            Evaluation</h3>
          </div>
        </header>
        <p>We use precision, recall and F-1 score as the evaluation
        measures. For a particular class <em>i</em>, precision and
        recall can be defined with the respective equations
        <span class="inline-equation"><span class="tex">$p_i =
        \frac{{TP}_i}{{TP}_i + {FN}_i}$</span></span> and
        <span class="inline-equation"><span class="tex">$r_i =
        \frac{{TP}_i}{{TP}_i + {FP}_i}$</span></span> , where
        <em>TP<sub>i</sub></em> is the number of true positives,
        <em>FN<sub>i</sub></em> is the number of false negatives
        and <em>FP<sub>i</sub></em> is the number of false
        positives. F-1 score is calculated from the precision and
        recall as <span class="inline-equation"><span class=
        "tex">$F\text{-}1 = \frac{2{p_i}{r_i}}{p_i +
        r_i}$</span></span> .</p>
        <p>For two tagging tasks, namely, ADR and Indication
        Labeling, we did approximate matching [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0007">7</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>] on the predicted labels
        for the ADR phrasal words against their actual labels.
        Approximate matching works by checking if one or more of
        the ADR spans in an ADR phrase could be identified
        correctly with the ‘ADR’ tags. For example, for the
        following tweet <em>I was on Cymbalta for 5 days. Cold
        turkey had sweats, migraine, tremors while on &amp; 3 days
        after.</em> with the actual ADR span <em>sweats, migraine,
        tremors</em>, predicting the tag as ‘ADR’ for any of the
        three spans or their combinations would be considered
        correct. The approximate match precision and recall are
        calculated as:</p>
        <div class="table-responsive" id="Xeq20">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} p_i = \frac{\#
            \text{ of ADR spans correctly tagged}}{\# \text{ of
            predicted ADR spans}} \end{equation}</span><br />
            <span class="equation-number">(20)</span>
          </div>
        </div>
        <div class="table-responsive" id="Xeq21">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} r_i = \frac{\#
            \text{ of ADR spans correctly tagged}}{\# \text{ of
            actual ADR spans}} \end{equation}</span><br />
            <span class="equation-number">(21)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span>
            Baselines</h3>
          </div>
        </header>
        <p>As we could not find any previous work that performs
        multi-task learning on pharmacovigilance tasks, we compare
        it against two baseline methods and state-of-the-art
        approaches for the independent tasks to demonstrate the
        effectiveness of our proposed model.</p>
        <ul class="list-no-style">
          <li id="list6" label="•">
            <strong>BLSTM-Random, BLSTM-Pretrained-learnable,
            BLSTM-Pretrained-fixed</strong>: The architecture of
            this model is known as Bidirectional Long Short-Term
            Memory (BLSTM) RNN [<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0007">7</a>]. It combines a forward RNN and
            a backward RNN and uses only word embeddings as the
            features. In BLSTM-Random the word-embeddings are
            randomly initialized and treated as learnable
            parameters. BLSTM-Pretrained-learnable and
            BLSTM-Pretrained-fixed use pre-trained word-embeddings
            trained on a large non-domain specific Twitter dataset.
            The only difference between them is that
            BLSTM-Pretrained-learnable treats the words-embedding
            values as learnable parameters, while
            BLSTM-Pretrained-fixed as fixed constants.<br />
          </li>
          <li id="list7" label="•">
            <strong>CRNN</strong>: State-of-the-art model for doing
            ADR Classification task. CRNN [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0013">13</a>] is a
            convolutional neural network concatenated with a
            recurrent neural network. They used GRU as the basic
            RNN unit and RLU for the convolutional layer.<br />
          </li>
          <li id="list8" label="•">
            <strong>CNNA</strong>: State-of-the-art model for doing
            ADR Classification task. CNNA [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0013">13</a>] is a
            convolutional neural network with attention mechanism
            incorporated.<br />
          </li>
          <li id="list9" label="•"><strong>MT-NoAtten</strong>: Our
          multi-task framework for pharmacovigilance tasks without
          any attention. We use a non-attention RNN as the decoder
          in this case.<br /></li>
          <li id="list10" label="•"><strong>MT-Atten</strong>: Our
          multi-task framework for pharmacovigilance tasks with
          just attention mechanism. Coverage is turned off during
          training.<br /></li>
          <li id="list11" label="•"><strong>MT-Atten-Cov</strong>:
          This is our proposed multi-task framework for
          pharmacovigilance tasks with combined attention and
          coverage mechanism.<br /></li>
        </ul>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.3</span> Overall
            Performance</h3>
          </div>
        </header>
        <p>To show the validity of our model, we report results on
        three experiments. The results obtained from applying our
        model to the test sets are presented in Tables <a class=
        "tbl" href="#tab1">1</a>, <a class="tbl" href="#tab2">2</a>
        and <a class="tbl" href="#tab3">3</a>.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">Test set results for the three tasks with
            the proposed model and the two baselines.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;" colspan="3">
                  MT-Atten-Cov
                  <hr />
                </th>
                <th style="text-align:left;" colspan="3">
                  MT-Atten Baseline
                  <hr />
                </th>
                <th style="text-align:left;" colspan="3">
                  MT-NoAtten Baseline
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:left;">Metrics</th>
                <th style="text-align:left;">P (%)</th>
                <th style="text-align:left;">R (%)</th>
                <th style="text-align:left;">F-1 (%)</th>
                <th style="text-align:left;">P (%)</th>
                <th style="text-align:left;">R (%)</th>
                <th style="text-align:left;">F-1 (%)</th>
                <th style="text-align:left;">P (%)</th>
                <th style="text-align:left;">R (%)</th>
                <th>F-1 (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">ADR
                Classification</td>
                <td style="text-align:left;">&nbsp;N/A</td>
                <td style="text-align:left;">&nbsp;N/A</td>
                <td style="text-align:left;">&nbsp;N/A</td>
                <td style="text-align:left;">72.88</td>
                <td style="text-align:left;">70.54</td>
                <td style="text-align:left;">70.69</td>
                <td style="text-align:left;">69.63</td>
                <td style="text-align:left;">60.60</td>
                <td>55.62</td>
              </tr>
              <tr>
                <td style="text-align:left;">ADR Labeling</td>
                <td style="text-align:left;">72.31</td>
                <td style="text-align:left;">87.5</td>
                <td style="text-align:left;">79.24</td>
                <td style="text-align:left;">70.88</td>
                <td style="text-align:left;">86.81</td>
                <td style="text-align:left;">78.04</td>
                <td style="text-align:left;">60.50</td>
                <td style="text-align:left;">78.88</td>
                <td>68.47</td>
              </tr>
              <tr>
                <td style="text-align:left;">Indication
                Labeling</td>
                <td style="text-align:left;">47.50</td>
                <td style="text-align:left;">50.2</td>
                <td style="text-align:left;">48.82</td>
                <td style="text-align:left;">46.87</td>
                <td style="text-align:left;">50.00</td>
                <td style="text-align:left;">48.38</td>
                <td style="text-align:left;">34.22</td>
                <td style="text-align:left;">41.20</td>
                <td>37.38</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Comparison of ADR Classification Task
            test results to previous approaches. Single-Atten-Cov
            Task refers to the independent task model trained on
            only the ADR Classification dataset.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">P (%)</th>
                <th style="text-align:left;">R (%)</th>
                <th style="text-align:left;">F-1 (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">CRNN</td>
                <td style="text-align:left;">49.00</td>
                <td style="text-align:left;">55.00</td>
                <td style="text-align:left;">51.00</td>
              </tr>
              <tr>
                <td style="text-align:left;">CNNA</td>
                <td style="text-align:left;">40.00</td>
                <td style="text-align:left;">66.00</td>
                <td style="text-align:left;">49.00</td>
              </tr>
              <tr>
                <td style="text-align:left;">Single-Atten-Cov</td>
                <td style="text-align:left;">70.21</td>
                <td style="text-align:left;">70.05</td>
                <td style="text-align:left;">70.13</td>
              </tr>
              <tr>
                <td style="text-align:left;">MT-Atten</td>
                <td style="text-align:left;">
                <strong>72.88</strong></td>
                <td style="text-align:left;">
                <strong>70.54</strong></td>
                <td style="text-align:left;">
                <strong>70.69</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Comparison of ADR Labeling Task test
            results to previous approaches. Single-Atten-Cov Task
            refers to the independent task model trained on only
            the ADR Labeling dataset.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;"></th>
                <th style="text-align:left;">P (%)</th>
                <th style="text-align:left;">R (%)</th>
                <th style="text-align:left;">F-1 (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">BLSTM-Random</td>
                <td style="text-align:left;">64.57</td>
                <td style="text-align:left;">63.32</td>
                <td style="text-align:left;">62.72</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                BLSTM-Pretrained-learnable</td>
                <td style="text-align:left;">60.47</td>
                <td style="text-align:left;">80.70</td>
                <td style="text-align:left;">68.58</td>
              </tr>
              <tr>
                <td style="text-align:left;">
                BLSTM-Pretrained-fixed</td>
                <td style="text-align:left;">70.43</td>
                <td style="text-align:left;">82.86</td>
                <td style="text-align:left;">75.49</td>
              </tr>
              <tr>
                <td style="text-align:left;">Single-Atten-Cov</td>
                <td style="text-align:left;">71.50</td>
                <td style="text-align:left;">86.22</td>
                <td style="text-align:left;">78.17</td>
              </tr>
              <tr>
                <td style="text-align:left;">MT-Atten-Cov</td>
                <td style="text-align:left;">
                <strong>72.31</strong></td>
                <td style="text-align:left;">
                <strong>87.50</strong></td>
                <td style="text-align:left;">
                <strong>79.24</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In the first experiment, we train our multi-task model
        jointly on the three parallel datasets corresponding to
        each of the three tasks. Results for each task from this
        experiment are compared against two baselines, which are
        depicted in Table 1. As the purpose of coverage is to be
        able to have a greater coverage for the ADR/ Indication
        words in an ADR/ Indication phrase, we turn it off in the
        classification decoder and use it in the two tagging tasks.
        So, results for classification task are not reported with
        MT-Atten-Cov model. For the remaining two tasks, we can
        observe that our method outperforms both the baselines in
        terms of the precision, recall, and F-score. Although
        approximate matching would consider the identification of
        any ADR/ Indication span as a true positive and we would
        expect comparable results for MT-Atten-Cov and MT-Atten
        models, the fact that MT-Atten-Cov has superior results
        empirically confirms that incorporating coverage with
        attention helps in capturing ‘phrasal’ and single ADR words
        which are not attended to with just the attention
        mechanism. We achieve a 1.50% and 0.90% F-1 score
        improvement for ADR labeling and Indication labeling
        respectively over the MT-Atten model. The experimental
        values for Indication labeling task happen to be low across
        all the models due to the sparsity of tweets containing
        indication words. Nevertheless, we gain improvement with
        our model against both the baselines. We gain an F-1 score
        improvement of 21. 32%, 13.59% and 23.43% over the
        MT-NoAtten model for classification, ADR detection, and
        Indication labeling respectively. By examining the
        precision and recall results, we can say that the better
        performance of our model can be attributed to the
        improvement of both.</p>
        <p>In the second and third experiments, we train our model
        separately on ADR classification and ADR detection tasks
        respectively and provide the comparison against results
        from the model trained in experiment one. As Indication
        detection was not performed as an independent task in any
        previous works, we do not provide a separate table with
        comparisons to previous approaches for it. We can see from
        Table 2 that our multi-task model improves the performance
        in terms of the F-1 score with 0.79 % for classification
        compared to a single task model. While for the ADR
        detection task results in Table 3, it makes 1.35%
        improvement. These empirical findings cast light on that
        shared input representations and interactions between tasks
        result in mutual benefit for all tasks. Comparing all the
        independent classification models (Single-Atten-Cov Task,
        CRNN, and CNNA) against each other, we can further see the
        advantage of using attention for the classification task,
        where Single-Atten-Cov has an average improvement of 28.71%
        over CRNN and CNNA. Although CNNA incorporates attention in
        its model, we assume using an RNN encoder-decoder with
        attention is more helpful. Similarly for ADR detection,
        from Table 3 we can see that our single task ADR detection
        model (Single-Atten-Cov Task) outperforms the best model
        among BLSTM (BLSTM-Pretrained-fixed) by 3.43%.
        Classification performance for our multi-task model
        attained higher F-1 scores over the CRNN and CNNA models by
        27.85% and 30.66% respectively, while for ADR detection,
        multi-task model improves by 4.73% over the best performing
        BLSTM models.</p>
      </section>
      <section id="sec-25">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.4</span> Case
            Study</h3>
          </div>
        </header>
        <p>To get a deeper insight into how augmenting coverage to
        attention mechanism benefits our model, we sample several
        tweets from the test dataset. The tags predicted by our
        model for these tweets are compared against those from the
        baseline MT-Atten model. All usernames are anonymized for
        privacy concerns. In order to validate the results that our
        model produced, we further visualize the attention heatmaps
        for some of the tweets depicted in Figure 3.</p>
        <p>The following two tweets illustrate the ability of our
        model to correctly label the single ADR word as ‘ADR’,
        while the MT-Atten model makes wrong prediction. This
        justifies the higher precision and recall gained by our
        model over MT-Atten.</p>
        <ul class="list-no-style">
          <li id="list12" label="•"><strong>Tweet 1</strong>:
          <em>@user1 bloody zombie I also take Venlafaxine thats
          for cronic depression bipolar is a difficult illness to
          deal with</em><br />
          <strong>True Tags</strong>: <em>[‘O’, ‘O’, ‘ADR’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’]</em><br />
          <strong>MT-Atten</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘ADR’,
          ‘O’, ‘O’, ‘O’, ‘O’]</em><br />
          <strong>MT-Atten-Cov</strong>: <em>[‘O’, ‘O’, ‘ADR’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ’ O ’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’]</em><br />
          <strong>ADR</strong>: zombie<br /></li>
        </ul>
        <p>The degree of <em>attention</em> received by each source
        word when decoding a target word for Tweet 1 is visualized
        in Figure 3, where darker shades denote higher score. When
        it is predicting the tag for target token ‘zombie’, more
        attention is given to the words ‘zombie’, ‘Venlafaxine’,
        ‘difficult’ and ‘illness’. Due to coverage, attentive words
        ‘difficult’ and ‘illness’ from previous timesteps were also
        attended. These words facilitated in predicting the ‘ADR’
        tag for this word. Furthermore, <em>indication</em> words
        ‘bipolar’ and ‘depression’ receiving low attention weights
        are prevented from mislabeling.</p>
        <ul class="list-no-style">
          <li id="list13" label="•"><strong>Tweet 2</strong>:
          <em>Cymbalta, my mood has worsened</em><br />
          <strong>True Tags</strong>: <em>[‘O’, ‘O’, ‘ADR’, ‘O’,
          ‘O’]</em><br />
          <strong>MT-Atten</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’,
          ‘ADR’]</em><br />
          <strong>MT-Atten-Cov</strong>: <em>[‘O’, ‘O’, ‘ADR’, ‘O’,
          ‘ADR’]</em><br />
          <strong>ADR</strong>: mood<br /></li>
        </ul>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186053/images/www2018-62-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Attention Heatmaps for
            Tweets 1, 4 and 5.</span>
          </div>
        </figure>
        <p>The following tweets contain ADRs captured by our model
        but missed by MT-Atten.</p>
        <ul class="list-no-style">
          <li id="list14" label="•"><strong>Tweet 3</strong>:
          <em>I'm sorry you have ostrich halitosis @user2 but that
          IS a side effect of once monthly boniva.</em><br />
          <strong>True Tags</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’]</em><br />
          <strong>MT-Atten</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’]</em><br />
          <strong>MT-Atten-Cov</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’]</em><br />
          <strong>ADR</strong>: halitosis<br /></li>
        </ul>
        <ul class="list-no-style">
          <li id="list15" label="•"><strong>Tweet 4</strong>:
          <em>Crying randomly at nothing and everything. Sigh.
          Thank you #effexor #withdrawls Can you just exit my
          system already.</em><br />
          <strong>True Tags</strong>: <em>[ADR’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’]</em><br />
          <strong>MT-Atten</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’]</em><br />
          <strong>MT-Atten-Cov</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’]</em><br />
          <strong>ADR</strong>: Crying <strong>ADR</strong>:
          withdrawls<br /></li>
        </ul>
        <p>From the attention heatmap of this tweet shown in Figure
        <a class="fig" href="#fig3">3</a>, we can see that the
        first ADR word ’crying’ has overall low attention for all
        the words. Also, as it happens to be the first word of the
        sentence, no coverage was passed. This is the reason why it
        is not detected as an ADR by both the baseline and our
        model. Whereas, for the second ADR ‘withdrawals’ source
        words ‘crying’, ‘randomly’, ‘at’ and ‘nothing’ were
        attended to more than others. Moreover, ‘Effexor’ and
        ‘withdrawals’ from the attention of previous word ‘Effexor’
        were also instrumental in the right tag assignment of
        ‘withdrawals’.</p>
        <p>With the following tweet we can see how
        <em>coverage</em> causes attention from previous ADR word
        ’heart’ to identify ’was’ as part of the ADR phrase. We can
        see the baseline model MT-Atten mislabels it.</p>
        <ul class="list-no-style">
          <li id="list16" label="•"><strong>Tweet 5</strong>:
          <em>@user3 I took a vyvanse and drank a 12oz redbull this
          morning. My heart was hurting a lil bit haha</em><br />
          <strong>True Tags</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘ADR’,
          ‘ADR’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’]</em><br />
          <strong>MT-Atten</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘ADR’, ‘O’,
          ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’]</em><br />
          <strong>MT-Atten-Cov</strong>: <em>[‘O’, ‘O’, ‘O’, ‘O’,
          ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘ADR’,
          ‘ADR’, ‘ADR’, ‘O’, ‘O’, ‘O’, ‘O’]</em><br />
          <strong>ADR</strong>: heart was hurting<br /></li>
        </ul>
      </section>
    </section>
    <section id="sec-26">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Related
          Work</h2>
        </div>
      </header>
      <p><strong>Pharmacovigilance in Social Media</strong>:
      Pharmacovigilance has become a very active area of research
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0024">24</a>]. Initial
      work on pharmacovigilance in social media performed detection
      and extraction tasks on health forum (e.g., DailyStrength,
      MedHelp) data [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>]. These works also focused on
      investigating posts mentioning adverse reactions associated
      with a fewer number of drugs. Over the years, a wide range of
      drugs have been used and other platforms such as Twitter have
      emerged as a valuable source for ADR monitoring [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0009">9</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0010">10</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>].</p>
      <p>Two of the most prevalent tasks for pharmacovigilance
      studies are the detection of posts containing ADR mention and
      extraction of the ADR mentions from posts [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0031">31</a>]. Due to the scarcity of
      annotated resources for this study, most works have been
      performed using small annotated datasets [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0017">17</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>]. While some work used
      large unannotated dataset employing an unsupervised approach
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>]. For ADR
      extraction, ADR lexicons and knowledge bases have been the
      most widely used resource [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>]. However, medical terms are rarely
      used in social media posts and raise mapping issues. Applying
      deep neural networks for pharmacovigilance has found its way
      in some recent works [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>]. These approaches benefit from not
      having to explicitly specify the features, but rather learn
      them in the training process.</p>
      <p>As mention of ‘Indication’ co-occurs with ADRs in a post,
      no previous work has performed ‘Indication’
      detection/extraction as a separate task. Although some works
      have used it as features in a machine learning approach
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>], the
      sparsity of ‘indications’ has been an issue. With our
      multi-task approach that includes ‘Indication’ extraction as
      a separate task, we show that interaction between related
      tasks can improve learning for all.</p>
      <p><strong>Multi-task Learning</strong>: Use of multi-task
      learning models has become ubiquitous for many machine
      learning applications in areas ranging from natural language
      processing, speech recognition to computer vision [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0030">30</a>].
      Multi-task learning with encoder-decoder come in one of the
      three flavors: one-to-many, many-to-one and many-to-many
      approaches [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0019">19</a>].
      In One-to-many models, tasks share a common encoder and have
      a task-specific decoder. A one-to-many approach was used in
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0008">8</a>] for
      translation task from a source language to multiple target
      languages. The shared encoder captured the syntactic and
      semantic similarity existing across different languages. On
      the other hand, a many-to-one approach is suitable for tasks
      where a decoder can be easily shared, such as multi-source
      translation. Lastly, a many-to-many approach allows multiple
      encoders and decoders.</p>
      <p>Our one-to-many approach is similar in spirit to
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>] used for
      joint slot filling and intent detection tasks. However, our
      work has the following differences: 1) learns more tasks
      jointly 2) use of coverage with attention mechanism. 3) use
      of pre-trained word embeddings.</p>
    </section>
    <section id="sec-27">
      <header>
        <div class="title-info">
          <h2><span class="section-number">8</span> Conclusion</h2>
        </div>
      </header>
      <p>Performing pharmacovigilance on Twitter can augment the
      existing ADR surveillance system which suffers from various
      clinical limitations. In this work, we provide an end-to-end
      solution for three ADR detection and extraction tasks. These
      problems have conventionally been approached separately and
      didn't leverage the interactions between the tasks.
      Exploiting the similarity that these tasks have, we proposed
      a multi-task encoder-decoder framework. All the tasks share
      one encoder to model the interactions and semantic/syntactic
      similarity between them. While each has its own decoder to
      produce output specific to that task. Our empirical findings
      validate how learning all tasks jointly improves precision
      and recall over state-of-the-art approaches. Additionally,
      the proposed solution is a hybrid attention model with
      coverage to deal with ADRs occurring as phrases. Through
      results and case studies we show that not only is this hybrid
      model able to achieve higher phrasal ADR word coverage
      compared to the baselines but is also able to identify single
      ADRs correctly.</p>
    </section>
    <section id="sec-28">
      <header>
        <div class="title-info">
          <h2><span class="section-number">9</span>
          Acknowledgments</h2>
        </div>
      </header>
      <p>This work is supported in part by NSF through grants
      IIS-1526499, CNS-1626432 and NSFC 61672313.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Martín Abadi, Paul
        Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
        Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
        Michael Isard, <em>et al.</em> 2016. TensorFlow: A System
        for Large-Scale Machine Learning.. In
        <em><em>OSDI</em></em> , Vol.&nbsp;16. 265–283.</li>
        <li id="BibPLXBIB0002" label="[2]">Dzmitry Bahdanau,
        Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine
        translation by jointly learning to align and translate.
        <em><em>arXiv preprint arXiv:1409.0473</em></em>
        (2014).</li>
        <li id="BibPLXBIB0003" label="[3]">Adrian Benton, Lyle
        Ungar, Shawndra Hill, Sean Hennessy, Jun Mao, Annie Chung,
        Charles&nbsp;E Leonard, and John&nbsp;H Holmes. 2011.
        Identifying potential adverse effects using the web: A new
        approach to medical hypothesis generation. <em><em>Journal
        of biomedical informatics</em></em> 44, 6 (2011),
        989–996.</li>
        <li id="BibPLXBIB0004" label="[4]">Jiang Bian, Umit
        Topaloglu, and Fan Yu. 2012. Towards large-scale twitter
        mining for drug-related adverse events. In
        <em><em>Proceedings of the 2012 international workshop on
        Smart health and wellbeing</em></em> . ACM, 25–32.</li>
        <li id="BibPLXBIB0005" label="[5]">Brant&nbsp;W Chee,
        Richard Berlin, and Bruce Schatz. 2011. Predicting adverse
        drug events from personal health messages. In <em><em>AMIA
        Annual Symposium Proceedings</em></em> , Vol.&nbsp;2011.
        American Medical Informatics Association, 217.</li>
        <li id="BibPLXBIB0006" label="[6]">Kyunghyun Cho, Bart
        Van&nbsp;Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
        2014. On the properties of neural machine translation:
        Encoder-decoder approaches. <em><em>arXiv preprint
        arXiv:1409.1259</em></em> (2014).</li>
        <li id="BibPLXBIB0007" label="[7]">Anne Cocos,
        Alexander&nbsp;G Fiks, and Aaron&nbsp;J Masino. 2017. Deep
        learning for pharmacovigilance: recurrent neural network
        architectures for labeling adverse drug reactions in
        Twitter posts. <em><em>Journal of the American Medical
        Informatics Association</em></em> (2017), ocw180.</li>
        <li id="BibPLXBIB0008" label="[8]">Daxiang Dong, Hua Wu,
        Wei He, Dianhai Yu, and Haifeng Wang. 2015. Multi-Task
        Learning for Multiple Language Translation.. In <em><em>ACL
        (1)</em></em> . 1723–1732.</li>
        <li id="BibPLXBIB0009" label="[9]">Clark&nbsp;C Freifeld,
        John&nbsp;S Brownstein, Christopher&nbsp;M Menone, Wenjie
        Bao, Ross Filice, Taha Kass-Hout, and Nabarun Dasgupta.
        2014. Digital drug safety surveillance: monitoring
        pharmaceutical products in twitter. <em><em>Drug
        safety</em></em> 37, 5 (2014), 343–350.</li>
        <li id="BibPLXBIB0010" label="[10]">Rachel Ginn, Pranoti
        Pimpalkhute, Azadeh Nikfarjam, Apurv Patki, Karen O'Connor,
        Abeed Sarker, Karen Smith, and Graciela Gonzalez. 2014.
        Mining Twitter for adverse drug reaction mentions: a corpus
        and classification benchmark. In <em><em>Proceedings of the
        fourth workshop on building and evaluating resources for
        health and biomedical text processing</em></em> .</li>
        <li id="BibPLXBIB0011" label="[11]">Xavier Glorot and
        Yoshua Bengio. 2010. Understanding the difficulty of
        training deep feedforward neural networks. In
        <em><em>Proceedings of the Thirteenth International
        Conference on Artificial Intelligence and
        Statistics</em></em> . 249–256.</li>
        <li id="BibPLXBIB0012" label="[12]">Sepp Hochreiter and
        Jürgen Schmidhuber. 1997. Long short-term memory.
        <em><em>Neural computation</em></em> 9, 8 (1997),
        1735–1780.</li>
        <li id="BibPLXBIB0013" label="[13]">Trung Huynh, Yulan He,
        Alistair Willis, and Stefan Rüger. 2016. Adverse drug
        reaction classification with deep neural networks. In
        <em><em>Proceedings of COLING 2016, the 26th International
        Conference on Computational Linguistics: Technical
        Papers</em></em> . 877–887.</li>
        <li id="BibPLXBIB0014" label="[14]">Keyuan Jiang and Yujing
        Zheng. 2013. Mining Twitter data for potential drug
        effects. In <em><em>International Conference on Advanced
        Data Mining and Applications</em></em> . Springer,
        434–443.</li>
        <li id="BibPLXBIB0015" label="[15]">Diederik Kingma and
        Jimmy Ba. 2014. Adam: A method for stochastic optimization.
        <em><em>arXiv preprint arXiv:1412.6980</em></em>
        (2014).</li>
        <li id="BibPLXBIB0016" label="[16]">Vasileios Lampos, Elad
        Yom-Tov, Richard Pebody, and Ingemar&nbsp;J Cox. 2015.
        Assessing the impact of a health intervention via
        user-generated Internet content. <em><em>Data Mining and
        Knowledge Discovery</em></em> 29, 5 (2015), 1434–1457.</li>
        <li id="BibPLXBIB0017" label="[17]">Robert Leaman, Laura
        Wojtulewicz, Ryan Sullivan, Annie Skariah, Jian Yang, and
        Graciela Gonzalez. 2010. Towards internet-age
        pharmacovigilance: extracting adverse drug reactions from
        user posts to health-related social networks. In
        <em><em>Proceedings of the 2010 workshop on biomedical
        natural language processing</em></em> . Association for
        Computational Linguistics, 117–125.</li>
        <li id="BibPLXBIB0018" label="[18]">Bing Liu and Ian Lane.
        2016. Attention-based recurrent neural network models for
        joint intent detection and slot filling. <em><em>arXiv
        preprint arXiv:1609.01454</em></em> (2016).</li>
        <li id="BibPLXBIB0019" label="[19]">Minh-Thang Luong,
        Quoc&nbsp;V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz
        Kaiser. 2015. Multi-task sequence to sequence learning.
        <em><em>arXiv preprint arXiv:1511.06114</em></em>
        (2015).</li>
        <li id="BibPLXBIB0020" label="[20]">Ramesh Nallapati,
        Feifei Zhai, and Bowen Zhou. 2017. SummaRuNNer: A recurrent
        neural network based sequence model for extractive
        summarization of documents. <em><em>hiP (yi= 1| hi, si,
        d)</em></em> 1 (2017), 1.</li>
        <li id="BibPLXBIB0021" label="[21]">Azadeh Nikfarjam and
        Graciela&nbsp;H Gonzalez. 2011. Pattern mining for
        extraction of mentions of adverse drug reactions from user
        comments. In <em><em>AMIA Annual Symposium
        Proceedings</em></em> , Vol.&nbsp;2011. American Medical
        Informatics Association, 1019.</li>
        <li id="BibPLXBIB0022" label="[22]">Azadeh Nikfarjam, Abeed
        Sarker, Karen O'Connor, Rachel Ginn, and Graciela Gonzalez.
        2015. Pharmacovigilance from social media: mining adverse
        drug reaction mentions using sequence labeling with word
        embedding cluster features. <em><em>Journal of the American
        Medical Informatics Association</em></em> 22, 3(2015),
        671–681.</li>
        <li id="BibPLXBIB0023" label="[23]">Karen O'Connor, Pranoti
        Pimpalkhute, Azadeh Nikfarjam, Rachel Ginn, Karen&nbsp;L
        Smith, and Graciela Gonzalez. 2014. Pharmacovigilance on
        twitter? mining tweets for adverse drug reactions. In
        <em><em>AMIA annual symposium proceedings</em></em> ,
        Vol.&nbsp;2014. American Medical Informatics Association,
        924.</li>
        <li id="BibPLXBIB0024" label="[24]">World&nbsp;Health
        Organization <em>et al.</em> 2002. The importance of
        pharmacovigilance. (2002).</li>
        <li id="BibPLXBIB0025" label="[25]">Apurv Patki, Abeed
        Sarker, Pranoti Pimpalkhute, Azadeh Nikfarjam, Rachel Ginn,
        Karen O'Connor, Karen Smith, and Graciela Gonzalez. 2014.
        Mining adverse drug reaction signals from social media:
        going beyond extraction. <em><em>Proceedings of
        BioLinkSig</em></em> 2014 (2014), 1–8.</li>
        <li id="BibPLXBIB0026" label="[26]">Michael&nbsp;J Paul and
        Mark Dredze. 2011. You are what you Tweet: Analyzing
        Twitter for public health. <em><em>ICWSM</em></em>
        20(2011), 265–272.</li>
        <li id="BibPLXBIB0027" label="[27]">Mor Peleg,
        Tiffany&nbsp;I Leung, Manisha Desai, and Michel Dumontier.
        2017. Is Crowdsourcing Patient-Reported Outcomes the Future
        of Evidence-Based Medicine? A Case Study of Back Pain. In
        <em><em>Conference on Artificial Intelligence in Medicine
        in Europe</em></em> . Springer, 245–255.</li>
        <li id="BibPLXBIB0028" label="[28]">Jeffrey Pennington,
        Richard Socher, and Christopher Manning. 2014. Glove:
        Global vectors for word representation. In
        <em><em>Proceedings of the 2014 conference on empirical
        methods in natural language processing (EMNLP)</em></em> .
        1532–1543.</li>
        <li id="BibPLXBIB0029" label="[29]">Marek Rei,
        Gamal&nbsp;KO Crichton, and Sampo Pyysalo. 2016. Attending
        to Characters in Neural Sequence Labeling Models.
        <em><em>arXiv preprint arXiv:1611.04361</em></em>
        (2016).</li>
        <li id="BibPLXBIB0030" label="[30]">Sebastian Ruder. 2017.
        An overview of multi-task learning in deep neural networks.
        <em><em>arXiv preprint arXiv:1706.05098</em></em>
        (2017).</li>
        <li id="BibPLXBIB0031" label="[31]">Abeed Sarker, Rachel
        Ginn, Azadeh Nikfarjam, Karen O'Connor, Karen Smith, Swetha
        Jayaraman, Tejaswi Upadhaya, and Graciela Gonzalez. 2015.
        Utilizing social media data for pharmacovigilance: A
        review. <em><em>Journal of biomedical informatics</em></em>
        54 (2015), 202–212.</li>
        <li id="BibPLXBIB0032" label="[32]">Abeed Sarker, Azadeh
        Nikfarjam, and Graciela Gonzalez. 2016. Social media mining
        shared task workshop. In <em><em>Biocomputing 2016:
        Proceedings of the Pacific Symposium</em></em> .
        581–592.</li>
        <li id="BibPLXBIB0033" label="[33]">Abigail See,
        Peter&nbsp;J Liu, and Christopher&nbsp;D Manning. 2017. Get
        To The Point: Summarization with Pointer-Generator
        Networks. <em><em>arXiv preprint arXiv:1704.04368</em></em>
        (2017).</li>
        <li id="BibPLXBIB0034" label="[34]">Ilya Sutskever, Oriol
        Vinyals, and Quoc&nbsp;V Le. 2014. Sequence to sequence
        learning with neural networks. In <em><em>Advances in
        neural information processing systems</em></em> .
        3104–3112.</li>
        <li id="BibPLXBIB0035" label="[35]">Richard Tzong-Han Tsai,
        Shih-Hung Wu, Wen-Chi Chou, Yu-Chun Lin, Ding He, Jieh
        Hsiang, Ting-Yi Sung, and Wen-Lian Hsu. 2006. Various
        criteria in the evaluation of biomedical named entity
        recognition. <em><em>BMC bioinformatics</em></em> 7, 1
        (2006), 92.</li>
        <li id="BibPLXBIB0036" label="[36]">Oriol Vinyals,
        Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015.
        Show and tell: A neural image caption generator. In
        <em><em>Proceedings of the IEEE conference on computer
        vision and pattern recognition</em></em> . 3156–3164.</li>
        <li id="BibPLXBIB0037" label="[37]">Stephen Wu, Sijia Liu,
        Yanshan Wang, Tamara Timmons, Harsha Uppili, Steven
        Bedrick, William Hersh, and Hongfang Liu. 2017.
        Intrainstitutional EHR collections for patient-level
        information retrieval. <em><em>Journal of the Association
        for Information Science and Technology</em></em>
        (2017).</li>
        <li id="BibPLXBIB0038" label="[38]">Zhengzheng Xing, Jian
        Pei, and Eamonn Keogh. 2010. A brief survey on sequence
        classification. <em><em>ACM Sigkdd Explorations
        Newsletter</em></em> 12, 1 (2010), 40–48.</li>
        <li id="BibPLXBIB0039" label="[39]">Christopher&nbsp;C
        Yang, Haodong Yang, Ling Jiang, and Mi Zhang. 2012. Social
        media mining for drug safety signal detection. In
        <em><em>Proceedings of the 2012 international workshop on
        Smart health and wellbeing</em></em> . ACM, 33–40.</li>
        <li id="BibPLXBIB0040" label="[40]">Ming Yang, Xiaodi Wang,
        and Melody&nbsp;Y Kiang. 2013. Identification of Consumer
        Adverse Drug Reaction Messages on Social Media.. In
        <em><em>PACIS</em></em> . 193.</li>
        <li id="BibPLXBIB0041" label="[41]">Elad Yom-Tov. 2017.
        Predicting drug recalls from Internet search engine
        queries. <em><em>IEEE Journal of Translational Engineering
        in Health and Medicine</em></em> 5 (2017), 1–6.</li>
        <li id="BibPLXBIB0042" label="[42]">Bin Zou, Vasileios
        Lampos, Russell Gorton, and Ingemar&nbsp;J Cox. 2016. On
        infectious intestinal disease surveillance using social
        media content. In <em><em>Proceedings of the 6th
        International Conference on Digital Health
        Conference</em></em> . ACM, 157–161.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a href=
    "https://twitter.com" target=
    "_blank">https://twitter.com</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186053">https://doi.org/10.1145/3178876.3186053</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
