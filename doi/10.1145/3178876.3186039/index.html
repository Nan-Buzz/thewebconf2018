<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Reinforcement Mechanism Design for e-commerce</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Reinforcement Mechanism Design
          for e-commerce</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Qingpeng</span> <span class=
          "surName">Cai</span>, Tsinghua University, Beijing,
          China, <a href=
          "mailto:cqp14@mails.tsinghua.edu.cn">cqp14@mails.tsinghua.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Aris</span> <span class=
          "surName">Filos-Ratsikas</span>, University of Oxford,
          Oxford, United Kingdom, <a href=
          "mailto:Aris.Filos-Ratsikas@cs.ox.ac.uk">Aris.Filos-Ratsikas@cs.ox.ac.uk</a>
        </div>
        <div class="author">
          <span class="givenName">Pingzhong</span> <span class=
          "surName">Tang</span>, Tsinghua University, Beijing,
          China, <a href=
          "mailto:kenshinping@gmail.com">kenshinping@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Yiwei</span> <span class=
          "surName">Zhang</span>, University of California,
          Berkeley, Berkeley, United States of America, <a href=
          "mailto:zhangyiwei1234567@126.com">zhangyiwei1234567@126.com</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186039"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186039</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We study the problem of allocating impressions to
        sellers in e-commerce websites, such as Amazon, eBay or
        Taobao, aiming to maximize the total revenue generated by
        the platform. We employ a general framework of
        <em>reinforcement mechanism design</em>, which uses deep
        reinforcement learning to design efficient algorithms,
        taking the strategic behaviour of the sellers into account.
        Specifically, we model the impression allocation problem as
        a Markov decision process, where the states encode the
        history of impressions, prices, transactions and generated
        revenue and the actions are the possible impression
        allocations in each round. To tackle the problem of
        continuity and high-dimensionality of states and actions,
        we adopt the ideas of the DDPG algorithm to design an
        actor-critic policy gradient algorithm which takes
        advantage of the problem domain in order to achieve
        convergence and stability.</small></p>
        <p><small>We evaluate our proposed algorithm, coined
        IA(GRU), by comparing it against DDPG, as well as several
        natural heuristics, under different rationality models for
        the sellers - we assume that sellers follow well-known
        no-regret type strategies which may vary in their degree of
        sophistication. We find that IA(GRU) outperforms all
        algorithms in terms of the total revenue.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Theory of computation</strong> →
        <strong>Algorithmic mechanism design;</strong> •
        <strong>Computing methodologies</strong> →
        <strong>Reinforcement learning;</strong> • <strong>Applied
        computing</strong> → <strong>Electronic
        commerce;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>e-commerce; impression
          allocation; mechanism design; reinforcement
          learning</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and
          Yiwei Zhang. 2018. Reinforcement Mechanism Design for
          e-commerce. In <em>WWW 2018: The 2018 Web
          Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 10 Pages. <a href=
          "https://doi.org/10.1145/3178876.3186039" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186039</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>A fundamental problem that all e-commerce websites are
      faced with is to decide how to allocate the buyer impressions
      to the potential sellers. When a buyer searches a keyword
      such as “iPhone 7 rose gold”, the platform will return a
      ranking of different sellers providing an item that fits the
      keyword, with different prices and different historical sale
      records. The goal of the platform is to come up with
      algorithms that will allocate the impressions to the most
      appropriate sellers, eventually generating more revenue from
      the transactions.</p>
      <p>This setting can be modeled as a resource allocation
      problem over a sequence of rounds, where in each round,
      buyers arrive, the algorithm inputs the historical records of
      the sellers and their prices and outputs such an allocation
      of impressions. The sellers and the buyers carry out their
      transactions and the historical records are updated. In
      reality, most e-commerce websites employ a class of heuristic
      algorithms, such as collaborative filtering or content based
      filtering [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0034">34</a>],
      many of which rank sellers in terms of “historical scores”
      calculated based on the transaction history of the sellers
      with buyers of similar characteristics.</p>
      <p>However, this approach does not typically take into
      account the fact that sellers strategize with the choice of
      prices, as certain sub-optimal prices in one round might
      affect the record histories of sellers in subsequent rounds,
      yielding more revenue for them in the long run. Even worse,
      since the sellers are usually not aware of the algorithm in
      use, they might “explore” with their pricing schemes,
      rendering the system uncontrollable at times. It seems
      natural that a more sophisticated approach that takes all
      these factors into account should be in place.</p>
      <p>In the presence of strategic or <em>rational</em>
      individuals, the field of <em>mechanism design</em>
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0029">29</a>] has
      provided a concrete toolbox for managing or preventing the
      ill effects of selfish behaviour and achieving desirable
      objectives. Its main principle is the design of systems in
      such a way that the strategic behaviour of the participants
      will lead to outcomes that are aligned with the goals of the
      society, or the objectives of the designer. Cai et al.
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0010">10</a>] tackle the
      problem of faking transactions and fraudulent seller
      behaviour in e-commerce using the tools from the field of
      <em>mechanism design</em>. A common denominator in most of
      the classical work in economics is that the participants have
      access to either full information or some distributional
      estimate of the preferences of others. However, in large and
      constantly evolving systems like e-commerce websites, the
      participants interact with the environment in various ways,
      and adjust their own strategies accordingly and dynamically
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>]. In
      addition to that, their rationality levels are often bounded
      by either computational or financial constraints, or even
      cognitive limitations [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0035">35</a>].</p>
      <p>For the reasons mentioned above, a large recent body of
      work has advocated that other types of agent behaviour, based
      on <em>learning</em> and <em>exploration</em>, are perhaps
      more appropriate for such large-scale online problems
      encountered in reality [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0033">33</a>]. In turn, this generates a
      requirement for new algorithmic techniques for solving those
      problems. Our approach is to use techniques from <em>deep
      reinforcement learning</em> for solving the problem of the
      impression allocation to sellers, given their selfish nature.
      In other words, given a rationality model for the sellers, we
      design reinforcement learning algorithms that take this model
      into account and solve the impression allocation problem
      efficiently. This general approach is called
      <em>reinforcement mechanism design</em> [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0036">36</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0040">40</a>], and we can view our
      contribution in this paper as an instance of this
      framework.</p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3>No-regret learning as agent rationality</h3>
          </div>
        </header>
        <p>As mentioned earlier, the strong informational
        assumptions of classical mechanism design are arguably
        unrealistic in complex and dynamic systems, like diverse
        online marketplaces. Such repeated game formulations
        typically require that the participants know the
        <em>values</em> of their competitors (or that they can
        estimate them pretty accurately based on known prior
        distributions) and that they can compute their
        payoff-maximizing strategies over a long sequence of
        rounds. Such tasks are usually computationally burdensome
        and require strong cognitive assumptions, as the
        participants would have to reason about the future, against
        all possible choices of their opponents, and in a
        constantly evolving environment.</p>
        <p>Given this motivation, an alternative approach in the
        forefront of much of the recent literature in algorithmic
        mechanism design is to assume that the agents follow some
        type of <em>no-regret strategies</em>; the agent picks a
        probability mixture over actions at each round and based on
        the generated payoffs, it updates the probabilities
        accordingly, minimizing the long-term regret. This is more
        easily conceivable, since the agents only have to reason
        about their own strategies and their interaction with the
        environment, and there is a plethora of no-regret
        algorithms at their disposal. Precisely the same argument
        has been made in several recent works [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0013">13</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0021">21</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0028">28</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0032">32</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0033">33</a>] that study popular
        auction settings under the same rationality assumptions of
        no-regret, or similar types. In fact, there exist data from
        Microsoft's Ad Actions which suggest that advertisers do
        use no-regret algorithms for their actions [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0041">41</a>]. For a more detailed
        discussion on related rationality models, the reader is
        referred to [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>].</p>
        <p><strong>The seller rationality model:</strong> To model
        the different sophistication levels of sellers, we consider
        four different models of rationality, based on
        well-established no-regret learning approaches. The first
        two, <em>ϵ-Greedy</em> [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0043">43</a>] and <em>ϵ-First</em> are known as
        <em>semi-uniform methods</em>, because they maintain a
        distinction between exploration and exploitation. The later
        is often referred to as “A/B testing” and is widely used in
        practice [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0009">9</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0012">12</a>]. The
        other two approaches, <em>UCB1</em> [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0002">2</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0005">5</a>] and <em>Exp3</em>
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0005">5</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0006">6</a>] are more
        sophisticated algorithms that differ in their assumptions
        about the nature of the rewards, i.e. whether they follow
        unknown distributions or whether they are completely
        adversarial. Note that all of our rationality models employ
        algorithms for the <em>multi-arm bandit</em> setting, as in
        platforms like Taobao or eBay, the impression allocation
        algorithms are unknown to the sellers and therefore they
        can not calculate the payoffs of unused auctions. The
        update of the weights to the strategies is based solely on
        the observed payoffs, which is often referred to as the
        <em>bandit feedback</em> setting [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0016">16</a>].</p>
        <p>We note here that while other related rationality models
        can be used, the goal is to choose a model that <em>real
        sellers would conceivably use in practice</em>. The
        semi-uniform algorithms are quite simpler and model a lower
        degree of seller sophistication, whereas the other two
        choices correspond to sellers that perhaps put more effort
        and resources into optimizing their strategies - some
        examples of sophisticated optimization services that are
        being used by online agents are provided in [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0028">28</a>]. Note that both UCB1
        and Exp3 are very well-known [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>] and the latter is perhaps the most
        popular bandit feedback implementation of the famous
        <em>Hedge</em> (or <em>Multiplicative Weights Update</em>)
        algorithm for no-regret learning in the fully informed
        feedback setting.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3>The impression allocation problem</h3>
          </div>
        </header>
        <p>We model the impression allocation problem as a Markov
        decision process (MDP) in which the information about the
        prices, past transactions, past allocations of impressions
        and generated revenue is stored in the states, and the
        actions correspond to all the different ways of allocating
        the impressions, with the rewards being the immediate
        revenue generated by each allocation. Given that the costs
        of the sellers (which depend on their production costs) are
        private information, it seems natural to employ
        reinforcement learning techniques for solving the MDP and
        obtain more sophisticated impression allocation algorithms
        than the heuristics that platforms currently employ.</p>
        <p>In our setting however, since we are allocating a very
        large number of impressions, both the state space and the
        action space are extremely large and high-dimensional,
        which renders traditional reinforcement learning techniques
        such as temporal difference learning [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0038">38</a>] or more specifically
        Q-learning [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>] not suitable for solving the MDP.
        In a highly influential paper, Mnih et al. [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0031">31</a>] employed the use of
        deep neural networks as function approximators to estimate
        the action-value function. The resulting algorithm, coined
        “Deep Q Network” (DQN), can handle large (or even
        continuous) state spaces but crucially, it can not be used
        for large or continuous action domains, as it relies on
        finding the action that maximizes the Q-function at each
        step.</p>
        <p>To handle the large action space, policy gradient
        methods have been proposed in the literature of
        reinforcement learning with actor-critic algorithms rising
        as prominent examples [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0039">39</a>], where the critic estimates the
        Q-function by exploring, while the actor adjusts the
        parameters of the policy by stochastic gradient ascent. To
        handle the high-dimensionality of the action space, Silver
        et al. [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0037">37</a>] designed a deterministic
        actor-critic algorithm, coined “Deterministic Policy
        Gradient” (DPG) which performs well in standard
        reinforcement-learning benchmarks such as mountain car,
        pendulum and 2D puddle world. As Lillicrap et al.
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0027">27</a>] point
        out however, the algorithm falls short in large-scale
        problems and for that reason, they developed the “Deep-DPG”
        (DDPG) algorithm which uses the main idea from [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0031">31</a>] and
        combines the deterministic policy gradient approach of DPG
        with deep neural networks as function approximators. To
        improve convergence and stability, they employ previously
        known techniques such as batch normalization [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0023">23</a>], target
        Q-networks [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0030">30</a>], and experience replay [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0001">1</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0031">31</a>].</p>
        <p><strong>The IA(GRU) algorithm:</strong> We draw
        inspiration from the DDPG algorithm to design a new
        actor-critic policy gradient algorithm for the impression
        allocation problem, which we refer to as the
        <em>IA(GRU)</em> algorithm. IA(GRU) takes advantage of the
        domain properties of the impression allocation problem to
        counteract the shortcomings of DDPG, which basically lie in
        its convergence when the number of sellers increases. The
        modifications of IA(GRU) to the actor and critic networks
        reduce the policy space to improve convergence and render
        the algorithm robust to settings with variable sellers,
        which may arrive and depart in each round, for which DDPG
        performs poorly. We evaluate IA(GRU) against DDPG as well
        as several natural heuristics similar to those usually
        employed by the online platforms and perform comparisons in
        terms of the total revenue generated. We show that IA(GRU)
        outperforms all the other algorithms for all four
        rationality models, as well as a combined pool of sellers
        of different degrees of sophistication.</p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> The
          setting</h2>
        </div>
      </header>
      <p>In the impression allocation problem of e-commerce
      websites, there are <em>m</em> sellers who compete for a unit
      of buyer impression.<a class="fn" href="#fn1" id=
      "foot-fn1"><sup>1</sup></a> In each round, a buyer<a class=
      "fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> searches for
      a keyword and the platform returns a ranking of sellers who
      provide an item that matches the keyword; for simplicity, we
      will assume that all sellers provide identical items that
      match the keyword exactly. Each seller <em>i</em> has a
      private cost <em>c<sub>i</sub></em> for the item, which can
      be interpreted as a production or a purchasing cost drawn
      from an i.i.d. distribution <em>F<sub>s</sub></em> .</p>
      <p>Typically, there are <em>n</em> slots (e.g. positions on a
      webpage) to be allocated and we let <em>x<sub>ij</sub></em>
      denote the probability (or the fraction of time) that seller
      <em>i</em> is allocated the impression at slot <em>j</em>.
      With each slot, there is an associated click-through-rate
      <em>α<sub>j</sub></em> which captures the “clicking
      potential” of each slot, and is independent of the seller, as
      all items offered are identical. We let <span class=
      "inline-equation"><span class="tex">$q_i=\sum
      _{j=1}^{n}x_{ij}\alpha _{j}$</span></span> denote the
      probability that the buyer will click the item of seller
      <em>i</em>. Given this definition (and assuming that sellers
      can appear in multiple slots in each page), the usual
      feasibility constraints for allocations, i.e. for all
      <em>i</em>, for all <em>j</em>, t holds that 0 ≤
      <em>x<sub>ij</sub></em> ≤ 1 and or all <em>j</em>, it holds
      that <span class="inline-equation"><span class="tex">$\sum
      _{i=1}^{m}x_{ij}\le 1$</span></span> can be alternatively
      written as</p>
      <div class="table-responsive">
        <div class="display-equation">
          <span class="tex mytex">\[ \text{for all } i, q_i\ge 0,
          \text{ it holds that } \sum _{i=1}^{m}q_i\le \sum
          _{j=1}^{n}\alpha _j \text{ and } \sum _{j=1}^{n}\alpha
          _j=1. \]</span><br />
        </div>
      </div>That is, for any such allocation <em>q</em>, there is a
      feasible ranking <em>x</em> that realizes <em>q</em> (for
      ease of notation, we assume that the sum of click-through
      rates of all slots is 1) and therefore we can allocate the
      buyer impression to sellers directly instead of outputting a
      ranking over these items when a buyer searches a
      keyword.<a class="fn" href="#fn3" id=
      "foot-fn3"><sup>3</sup></a>
      <p></p>
      <p>Let <em>h<sub>it</sub></em> = (<em>v<sub>it</sub></em> ,
      <em>p<sub>it</sub></em> , <em>n<sub>it</sub></em> , ℓ
      <sub><em>it</em></sub> ) denote the <em>records</em> of
      seller <em>i</em> at round <em>t</em>, which is a tuple
      consisting of the following quantities:</p>
      <ol class="list-no-style">
        <li id="list1" label="(1)"><em>v<sub>it</sub></em> is the
        expected fraction of impressions that seller <em>i</em>
        gets,<br /></li>
        <li id="list2" label="(2)"><em>p<sub>it</sub></em> is the
        price that seller <em>i</em> sets,<br /></li>
        <li id="list3" label="(3)"><em>n<sub>it</sub></em> is the
        expected amount of transactions that seller <em>i</em>
        makes,<br /></li>
        <li id="list4" label="(4)">ℓ <sub><em>it</em></sub> is the
        expected revenue that seller <em>i</em> makes at round
        <em>t</em>.<br /></li>
      </ol>
      <p>Let <em>H<sub>t</sub></em> = (<em>h</em>
      <sub>1<em>t</em></sub> , <em>h</em> <sub>2<em>t</em></sub> ,
      ..., <em>h<sub>it</sub></em> ) denote the records of all
      sellers at round <em>t</em>, and let <em>H<sub>it</sub></em>
      = (<em>h</em> <sub><em>i</em>1</sub>, <em>h</em>
      <sub><em>i</em>2</sub>, ..., <em>h<sub>it</sub></em> ) denote
      the vectors of records of seller <em>i</em> from round 1 to
      round <em>t</em>, which we will refer to as the
      <em>records</em> of the seller. At each round <em>t</em> + 1,
      seller <em>i</em> chooses a price <em>p</em>
      <sub><em>i</em>(<em>t</em> + 1)</sub> for its item and the
      algorithm allocates the buyer impression to sellers.</p>
      <p><strong>MDP formulation:</strong> The setting can be
      defined as a <em>Markov decision process</em> (MDP) defined
      by the following components: a continuous state space
      <span class="inline-equation"><span class="tex">$\mathcal
      {S}$</span></span> , a continuous action space <span class=
      "inline-equation"><span class="tex">$\mathcal
      {A}$</span></span> , with an initial state distribution with
      density <em>p</em> <sub>0</sub>(<em>s</em> <sub>0</sub>), and
      a transition distribution of states with conditional density
      <em>p</em>(<em>s</em> <sub><em>t</em> +
      1</sub>|<em>s<sub>t</sub></em> , <em>a<sub>t</sub></em> )
      satisfying the Markov property, i.e. <em>p</em>(<em>s</em>
      <sub><em>t</em> + 1</sub>|<em>s</em> <sub>0</sub>, <em>a</em>
      <sub>0</sub>, ..., <em>s<sub>t</sub></em> ,
      <em>a<sub>t</sub></em> ) = <em>p</em>(<em>s</em>
      <sub><em>t</em> + 1</sub>|<em>s<sub>t</sub></em> ,
      <em>a<sub>t</sub></em> ). Furthermore, there is an associated
      reward function <span class="inline-equation"><span class=
      "tex">$r:\mathcal {S} \times \mathcal {A}\rightarrow \mathcal
      {R}$</span></span> assigning payoffs to pairs of states and
      actions. Generally, a policy is a function <em>π</em> that
      selects stochastic actions given a state, i.e, <span class=
      "inline-equation"><span class="tex">$\pi :\mathcal
      {S}\rightarrow \mathcal {P}(\mathcal {A})$</span></span> ,
      where <span class="inline-equation"><span class=
      "tex">$\mathcal {P}(\mathcal {A})$</span></span> is the set
      of probability distributions on <span class=
      "inline-equation"><span class="tex">$\mathcal
      {A}$</span></span> . Let <em>R<sub>t</sub></em> denote the
      discounted sum of rewards from the state
      <em>s<sub>t</sub></em> , i.e, <span class=
      "inline-equation"><span class="tex">$R_t(s_t)=\sum
      _{k=t}^{\infty }{\gamma }^{k-t}r(s_k,a_k)$</span></span> ,
      where 0 &lt; <em>γ</em> &lt; 1. Given a policy and a state,
      the <em>value function</em> is defined to be the expected
      total discounted reward, i.e. <em>V<sup>π</sup></em>
      (<em>s</em>) = <em>E</em>[<em>R<sub>t</sub></em>
      (<em>s<sub>t</sub></em> )|<em>s<sub>t</sub></em> =
      <em>s</em>; <em>π</em>] and the <em>action-value
      function</em> is defined as <em>Q<sup>π</sup></em>
      (<em>s</em>, <em>a</em>) = <em>E</em>[<em>R<sub>t</sub></em>
      (<em>s<sub>t</sub></em> )|<em>s<sub>t</sub></em> =
      <em>s</em>, <em>a<sub>t</sub></em> = <em>a</em>;
      <em>π</em>].</p>
      <p>For our problem, a state <em>s<sub>t</sub></em> of the MDP
      consists of the records of all sellers in the last <em>T</em>
      rounds, i.e. <em>s<sub>t</sub></em> = (<em>H</em>
      <sub><em>t</em> − <em>T</em></sub> , ..., <em>H</em>
      <sub><em>t</em> − 1</sub>), that is, the state is a
      (<em>T</em>, <em>m</em>, 4) tensor, the allocation outcome of
      the round is the action, and the immediate reward is the
      expected total revenue generated in this round. The
      performance of an algorithm is defined as the average
      expected total revenue over a sequence of <em>T</em>
      <sub>0</sub> rounds.</p>
      <p><strong>Buyer Behaviour:</strong> We model the behaviour
      of the buyer as being dependent on a valuation that comes
      from a distribution with cumulative distribution function
      <em>F<sub>b</sub></em> . Intuitively, this captures the fact
      that buyers may have different spending capabilities
      (captured by the distribution). Specifically, the probability
      that the buyer purchases item <em>i</em> is
      <em>n<sub>it</sub></em> = (1 − <em>F<sub>b</sub></em>
      (<em>p<sub>it</sub></em> )) · <em>v<sub>it</sub></em> , that
      is, the probability of purchasing is decided by the
      impression allocation and the price seller <em>i</em> sets.
      For simplicity and without loss of generality with respect to
      our framework, we assume that the buyer's valuation is drawn
      from <em>U</em>(0, 1), i.e. the uniform distribution over [0,
      1].</p>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3>Seller Rationality</h3>
          </div>
        </header>
        <p>As we mentioned in the introduction, following a large
        body of recent literature, we will assume that the sellers
        employ no-regret type strategies for choosing their prices
        in the next round. Generally, a seller starts with a
        probability distribution over all the possible prices, and
        after each round, it observes the payoffs that these
        strategies generate and adjusts the probabilities
        accordingly. As we already explained earlier, it is most
        natural to assume strategies in the <em>bandit
        feedback</em> setting, where the seller does not observe
        the payoffs of strategies in the support of its strategy
        which were not actually used. The reason is that even if we
        assume that a seller can see the prices chosen in a round
        by its competitors, it typically does not have sufficient
        information about the allocation algorithm used by the
        platform to calculate the payoffs that other prices would
        have yielded. Therefore it is much more natural to assume
        that the seller updates its strategy based on the
        <em>observed rewards</em>, using a multi-arm bandit
        algorithm.</p>
        <p>More concretely, the payoff of a seller <em>i</em> that
        receives <em>v<sub>it</sub></em> impressions in round
        <em>t</em> when using price <em>p<sub>ij</sub></em>
        (<em>t</em>), is given by <em>u<sub>ij</sub></em>
        (<em>t</em>) = <em>n<sub>it</sub></em>
        (<em>p<sub>ij</sub></em> (<em>t</em>) −
        <em>c<sub>i</sub></em> ) = <em>v<sub>it</sub></em> (1 −
        <em>F<sub>b</sub></em> (<em>p<sub>it</sub></em>
        ))(<em>p<sub>ij</sub></em> (<em>t</em>) −
        <em>c<sub>i</sub></em> ). For consistency, we normalize the
        costs and the prices to lie in the unit interval [0, 1] and
        we discretize the price space to a “dense enough” grid (of
        size 1/<em>K</em>, for some large enough <em>K</em>). This
        discretization can either be enforced by the platform (e.g.
        the sellers are required to submit bids which are multiples
        of 0.05) or can be carried out by the sellers themselves in
        order to be able to employ the multi-arm bandit algorithms
        which require the set of actions to be finite, and since
        small differences in prices are unlikely to make much
        difference in their payoffs.</p>
        <p>We consider the following possible strategies for the
        sellers, based on well-known bandit algorithms.</p>
        <p><strong>ɛ-Greedy [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0043">43</a>]:</strong> With probability ɛ, each
        seller selects a strategy uniformly at random and with
        probability 1 − ɛ, the strategy with the best observed
        (empirical) mean payoff so far. The parameter ɛ denotes the
        degree of <em>exploration</em> of the seller, whereas 1 − ɛ
        is the degree of <em>exploitation</em>; here ɛ is drawn
        i.i.d. from the normal distribution <em>N</em>(0.1,
        0.1/3).</p>
        <p><strong>ɛ-First:</strong> For a horizon of <span class=
        "inline-equation"><span class="tex">$\mathcal
        {T}$</span></span> rounds, this strategy consists of an
        <em>exploration</em> phase first, over <span class=
        "inline-equation"><span class="tex">$\varepsilon \cdot
        \mathcal {T}$</span></span> rounds, followed by an
        <em>exploitation</em> phase, for the remaining period. In
        the exploration phase, the seller picks a strategy
        uniformly at random. In the remaining rounds, the sellers
        picks the strategy that maximizes the empirical mean of the
        observed rewards. For each seller, we set <span class=
        "inline-equation"><span class="tex">$\mathcal
        {T}=200$</span></span> and ɛ = 0.1.</p>
        <p><strong>Exponential-weight Algorithm for Exploration and
        Exploitation (Exp3) [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>]:</strong> We use the definition of
        the algorithm from [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0006">6</a>]. Let <em>γ</em> ∈ (0, 1] be a real
        number and initialize <em>w<sub>i</sub></em> (1) = 1 for
        <em>i</em> = 1, …, <em>K</em> + 1 to be the initial weights
        of the possible prices<a class="fn" href="#fn4" id=
        "foot-fn4"><sup>4</sup></a>. In each round <em>t</em>,</p>
        <ul class="list-no-style">
          <li id="list5" label="•">For <em>i</em> = 1, …,
          <em>K</em> + 1, let <span class=
          "inline-equation"><span class="tex">$\pi _i(t)=
          (1-\gamma) \frac{w_i(t)}{\sum _{j=1}^{K+1} w_j(t)} +
          \frac{\gamma }{K+1}$</span></span> , where
          <em>w<sub>i</sub></em> (<em>t</em>) is the weight of
          price <em>p<sub>i</sub></em> in round
          <em>t</em>.<br /></li>
          <li id="list6" label="•">Select a price
          <em>p<sub>j</sub></em> (<em>t</em>) according to the
          probability distribution defined by <em>π</em>
          <sub>1</sub>(<em>t</em>), …, <em>π</em> <sub><em>K</em> +
          1</sub>(<em>t</em>).<br /></li>
          <li id="list7" label="•">Receive payoff
          <em>u<sub>j</sub></em> (<em>t</em>) ∈ [0, 1].<br /></li>
          <li id="list8" label="•">For ℓ = 1, …, <em>K</em> + 1,
          let
            <div class="table-responsive">
              <div class="display-equation">
                <span class="tex mytex">\[ \hat{u}_{\ell }(t)=
                {\left\lbrace \begin{array}{@{}l@{\quad
                }l@{}}u_{\ell }(t)/\pi _{\ell }(t), &amp; \text{if
                } \ell =j \\ 0, &amp; \text{otherwise} \\
                \end{array}\right.} \]</span><br />
              </div>
            </div>and <span class="inline-equation"><span class=
            "tex">$w_{\ell }(t+1) = w_{\ell }(t)e^{\gamma \cdot
            \hat{u}_{\ell }(t)/(K+1)}$</span></span> .<br />
          </li>
        </ul>
        <p>We remark here that since the payoff of each seller in
        some round <em>t</em> actually takes values in [ − 1, 1],
        we scale the payoff to [0, 1] by applying the
        transformation <em>f</em>(<em>u</em>) = (<em>u</em> + 1)/2
        to any payoff <em>u</em>.</p>
        <p><strong>Upper Confidence Bound Algorithm (UCB1)
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0002">2</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0004">4</a>]:</strong>
        For each price <em>p<sub>j</sub></em> ∈ [0, 1/<em>K</em>,
        2/<em>K</em>, …, 1], initialize <em>x<sub>j</sub></em> (1)
        = 0. At the end of each round <em>t</em>, update
        <em>x<sub>j</sub></em> (<em>t</em>) as:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ x_j(t)= {\left\lbrace
            \begin{array}{@{}l@{\quad }l@{}}x_j(t-1)/t + u_j(t)/t,
            &amp; \text{if } j \text{ was chosen in this round } t
            \\ x_j(t-1), &amp; \text{otherwise} \\
            \end{array}\right.} \]</span><br />
          </div>
        </div>For any round <em>t</em> ∈ {0, …, <em>K</em>}, the
        seller chooses a price <em>p<sub>j</sub></em> that has not
        been used before in any of the previous rounds (breaking
        ties arbitrarily). For any round <em>t</em> ≥ <em>K</em> +
        1, the seller chooses the price <em>p<sub>j</sub></em> with
        the maximum weighted value <em>x<sub>j</sub></em> , i.e,
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ p_j(t) \in \arg \max _{j \in
            \lbrace 0,1/K,\ldots ,1\rbrace } x_j(t)+\frac{\log _2
            t}{\sum _{\tau =1}^{t}I_{j\tau }} \]</span><br />
          </div>
        </div>, where <em>I<sub>jτ</sub></em> is the indicator
        function, i.e.
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ I_{j\tau }= {\left\lbrace
            \begin{array}{@{}l@{\quad }l@{}}1, &amp; \text{if } p_j
            \text{ was chosen in round } \tau \\ 0, &amp;
            \text{otherwise.} \\ \end{array}\right.}
            \]</span><br />
          </div>
        </div>ɛ-Greedy and ɛ-First are simple strategies that
        maintain a clear distinction between exploration and
        exploitation and belong to the class of semi-uniform
        strategies. Exp3 is the most widely used bandit version of
        perhaps the most popular no-regret algorithm for the full
        information setting, the Hedge (or Multiplicative Weight
        updates) algorithm [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0017">17</a>] and works in the
        <em>adversarial</em> bandit feedback model [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0006">6</a>], where no distributional
        assumptions are being made about the nature of the rewards.
        UCB1, as the name suggests, maintains a certain level of
        optimism towards less frequently played actions (given by
        the second part of the sum) and together with this, it uses
        the empirical mean of observed actions so far to choose the
        action in the next round. The algorithm is best suited in
        scenarios where the rewards do follow some distribution
        which is however unknown to the seller.
        <p></p>
        <p>For a more detailed exposition of all these different
        algorithms, [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>] provides a concise survey. The
        point made here is that these choices are quite sensible as
        they (i) constitute choices that a relatively sophisticated
        seller, perhaps with a research team at its disposal could
        make, (ii) can model sellers with different degrees of
        sophistication or pricing philosophies and (iii) are
        consistent with the recent literature on algorithmic
        mechanism design, in terms of modeling agent rationality in
        complex dynamic environments.</p>
      </section>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Allocation
          algorithms</h2>
        </div>
      </header>
      <p>In this section, we will briefly describe the algorithms
      that we will be comparing IA(GRU) against - two natural
      heuristics similar to those employed by platforms for the
      impression allocation problem, as well as the DDPG algorithm
      of Lillicrap et al. [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>].</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3>Heuristic Allocation Algorithms</h3>
          </div>
        </header>
        <p>As the strategies of the sellers are unknown to the
        platform, and the only information available is the
        sellers’ historical records, the platform can only use that
        information for the allocation. Note that these heuristics
        do not take the rationality of the sellers into account,
        when deciding on the allocation of impressions.</p>
        <p>The first algorithm is a simple greedy algorithm, which
        allocates the impressions proportionally to the revenue
        contribution.</p>
        <p><em><strong>Greedy Myopic Algorithm</strong></em>
        <strong>:</strong> At round 0, the algorithm allocates a
        1/<em>m</em>-fraction of the buyer impression to each
        seller. At any other round <em>τ</em> + 1 (for <em>τ</em> ≥
        0), the algorithm allocates a fraction of <span class=
        "inline-equation"><span class="tex">$\ell _{i\tau }/\sum
        _{j=1}^{m}{\ell _{j\tau }}$</span></span> of the buyer
        impression to each seller, i.e. proportionally to the
        contribution of each seller to the total revenue of the
        last round.</p>
        <p>The second algorithm is an algorithm for the
        <em>contextual multi-arm bandit</em> problem, proposed by
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0026">26</a>], based
        on the principles of the family of upper confidence bound
        algorithms (UCB1 is an algorithm in this family). The
        algorithm is among the state of the art solutions for
        recommender systems [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>] and is an example of contextual
        bandit approaches, which are widely applied to such
        settings [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href="#BibPLXBIB0003">3</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0008">8</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0025">25</a>,
        <a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0026">26</a>]. To
        prevent any confusion, we clarify here that while we also
        used bandit algorithms for the seller rationality models,
        the approach here is fundamentally different as the Linear
        UCB Algorithm is used for the allocation of impressions -
        not the choice of prices - and the arms in this case are
        the different sellers.</p>
        <p><em><strong>Linear UCB Algorithm [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0026">26</a>]</strong></em>
        <strong>:</strong> We implement the algorithm as described
        in [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0026">26</a>] - in the interest of space, we do
        not provide the definition of the algorithm, but refer the
        reader to <em>Algorithm 1</em> in [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0026">26</a>]. We model each seller
        as an <em>arm</em> and set <em>h<sub>it</sub></em> as the
        feature of each arm <em>i</em> in each round <em>t</em>.
        The parameter <em>α</em> is set to 1.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3>Deep Deterministic Policy Gradient</h3>
          </div>
        </header>
        <p>Here, we briefly describe the DDPG algorithm of
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0027">27</a>], which
        we we draw inspiration from in order to design our
        impression allocation algorithm. Before describing the
        algorithm, we briefly mention the main ingredients of its
        predecessor, the DPG algorithm of Silver et al. [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0037">37</a>].</p>
        <p><em><strong>Deterministic Policy Gradient</strong></em>
        <strong>:</strong> The shortcoming of DQN [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0031">31</a>] is that while it can
        handle continuous states, it can not handle continuous
        actions or high-dimensional action spaces. Although
        stochastic actor-critic algorithms could handle continuous
        actions, they are hard to converge in high dimensional
        action spaces. The DPG algorithm [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0037">37</a>] aims to train a
        deterministic policy <span class=
        "inline-equation"><span class="tex">$\mu _{\theta
        }:\mathcal {S}\rightarrow \mathcal {A}$</span></span> with
        parameter vector <em>θ</em> ∈ <em>R<sup>n</sup></em> . This
        algorithm consists of two components: an actor, which
        adjusts the parameters <em>θ</em> of the deterministic
        policy <em>μ<sub>θ</sub></em> (<em>s</em>) by stochastic
        gradient ascent of the gradient of the discounted sum of
        rewards, and the critic, which approximates the
        action-value function.</p>
        <p><em><strong>Deep Deterministic Policy
        Gradient</strong></em> <strong>:</strong> Directly training
        neural networks for the actor and the critic of the DPG
        algorithm fails to achieve convergence; the main reason is
        the high degree of temporal correlation which introduces
        high variance in the approximation of the Q-function by the
        critic. For this reason, the DDPG algorithm uses a
        technique known as <em>experience replay</em>, according to
        which the experiences of the agent at each time step are
        stored in a replay buffer and then a mini-batch is sampled
        uniformly at random from this set for learning, to
        eliminate the temporal correlation. The other modification
        is the employment of <em>target networks</em> for the
        regularization of the learning algorithm. The target
        network is used to update the values of <em>μ</em> and
        <em>Q</em> at a slower rate instead of updating by the
        gradient network; the prediction <em>y<sub>t</sub></em>
        will be relatively fixed and violent jitter at the
        beginning of training is absorbed by the target network. A
        similar idea appears in [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0042">42</a>] with the form of double Q-value
        learning.</p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> The Impression
          Allocation (GRU) algorithm</h2>
        </div>
      </header>
      <p>In this section, we present our main deep reinforcement
      learning algorithm, termed IA(GRU) (“IA” stands for
      “impression allocation” and “GRU” stands for “gated recurrent
      unit”) which is in the center of our framework for impression
      allocations in e-commerce platforms and is based on the ideas
      of the DDPG algorithm. Before we present the algorithm, we
      highlight why simply applying DDPG to our problem can not
      work.</p>
      <p><strong>Shortcomings of DDPG</strong>: First of all, while
      DDPG is designed for settings with continuous and often
      high-dimensional action spaces, the blow-up in the number of
      actions in our problem is very sharp as the number of sellers
      increases; this is because the action space is the set of all
      feasible allocations, which increases very rapidly with the
      number of sellers. As we will show in Section <a class="sec"
      href="#sec-14">5</a>, the direct application of the algorithm
      fails to converge even for a moderately small number of
      sellers. The second problem comes from the inability of DDPG
      to handle variability on the set of sellers. Since the
      algorithm uses a two-layer fully connected network, the
      position of each seller plays a fundamental role; each seller
      is treated as a different entity according to that position.
      As we show in Section <a class="sec" href="#sec-14">5</a>, if
      the costs of sellers at each round are randomly selected, the
      performance of the DDPG algorithm deteriorates rapidly. The
      settings in real-life e-commerce platforms however are quite
      dynamic, with sellers arriving and leaving or their costs
      varying over time, and for an allocation algorithm to be
      applicable, it should be able to handle such variability. We
      expect that each seller's features are only affected by its
      historical records, not some “identity” designated by the
      allocation algorithm; we refer to this highly desirable
      property as ”permutation invariance”. Based on time-serial
      techniques, our algorithm uses Recurrent Neural Networks at
      the dimension of the sellers and achieves the property.</p>
      <p><em><strong>The IA(GRU) algorithm</strong></em>
      <strong>:</strong>Next, we explain the design of our
      algorithm, but we postpone some implementation details for
      Section <a class="sec" href="#sec-14">5</a>. At a high level,
      the algorithm uses the framework of DDPG with different
      network structures and different inputs of networks. It
      maintains a sub-actor network and a sub-critic network for
      each seller and employs <em>input preprocessing</em> at each
      training step, to ensure permutation invariance.</p>
      <p><strong>Input Preprocessing:</strong> In each step of
      training, with a state tensor of shape (<em>T</em>,
      <em>m</em>, 4), we firstly utilize a <em>background
      network</em> to calculate a public vector containing
      information of all sellers: it transforms the state tensor to
      a (<em>T</em>, <em>m</em> × 4) tensor and performs RNN
      operations on the axis of rounds. At this step, it applies a
      <em>permutation transformation</em>, i.e. a technique for
      maintaining permutation invariance. Specifically, it first
      orders the sellers according to a certain metric, such as the
      weighted average of their past generated revenue and then
      inputs the (state, action) pair following this order to
      obtain the public vector (<em>pv</em>). On the other hand,
      for each seller <em>i</em>, it applies a similar RNN
      operation on its history, resulting in an individual temporal
      feature called (<em>f<sub>i</sub></em> ). Combining those two
      features, we obtain a feature vector (<em>pv</em>,
      <em>f<sub>i</sub></em> ) that we will use as input for the
      sellers’ sub-actor and sub-critic networks.</p>
      <p><strong>Actor network:</strong> For each seller, the input
      to the sub-actor network is (<em>pv</em>,
      <em>f<sub>i</sub></em> ) and the output is a score. This
      algorithm uses a softmax function over the outputs of all
      sub-actor networks in order to choose an action. The
      structure of the policy which is shown in Figure <a class=
      "fig" href="#fig1">1</a> ensures that the policy space is
      much smaller than that of DDPG as the space of inputs of all
      sub-actor networks is restricted, and allows for easier
      convergence, as we will show in Section <a class="sec" href=
      "#sec-14">5</a>.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The framework of the actor network of the
          IA(GRU) algorithm.</span>
        </div>
      </figure><strong>Critic network:</strong> For the critic, we
      make use of a domain-specific property, namely that the
      immediate reward of each round is the sum of revenues of all
      sellers and the record of each seller has the same space.
      Each sub-critic network inputs the expected fraction of buyer
      impression the seller gets (the sub-action) and (<em>pv</em>,
      <em>f<sub>i</sub></em> ) (the sub-state) as input and outputs
      the Q-value of the corresponding seller, i.e, the expected
      discounted sum of revenues from the sub-state following the
      policy. Then, it sums up the estimated Q-value of all
      sub-critic networks to output the final estimated Q-value,
      with the assumption that the strategy of each seller is
      independent of the records of other sellers, which is the
      case in all of our rationality models. The framework of the
      critic network is similar to Figure <a class="fig" href=
      "#fig1">1</a>.
      <p></p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experimental
          Evaluation</h2>
        </div>
      </header>
      <p>In this section, we present the evaluation of our
      algorithms in terms of convergence time and revenue
      performance against several benchmarks, namely the direct
      application of the DDPG algorithm (with a fully connected
      network) and the heuristic allocation algorithms that we
      defined in Section <a class="sec" href="#sec-10">3</a>. We
      use Tensorflow and Keras as the engine for the deep learning,
      combining the idea of DDPG and the techniques mentioned in
      Section <a class="sec" href="#sec-13">4</a>, to train the
      neural network.</p>
      <p><strong>Designed experiments:</strong> First, we will
      compare IA(GRU) and DDPG in terms of their convergence
      properties in the training phase and show that the former
      converges while the latter does not. Next, we will compare
      the four different algorithms (Greedy Myopic, Linear UCB,
      DDPG and IA(GRU)) in terms of the generated revenue for two
      different settings, a setting with <em>fixed sellers</em> and
      a setting with <em>variable sellers</em>. The difference is
      that in the former case, we sample the costs
      <em>c<sub>i</sub></em> once in the beginning whereas in the
      latter case, the cost <em>c<sub>i</sub></em> of each seller
      is sampled again in each round. This can either model the
      fact that the production costs of sellers may vary based on
      unforeseeable factors or simply that sellers of different
      capabilities may enter the market in each round.</p>
      <p>For each one of these two settings, we will compare the
      four algorithms for each one of the four different
      rationality models (ϵ-Greedy, ϵ-First, UCB1 and Exp3)
      <em>separately</em> as well as in a <em>combined</em> manner,
      by assuming a mixed pool of sellers, each of which may adopt
      a different rationality model from the ones above. The latter
      comparison is meant to capture cases where the population of
      sellers is heterogeneous and may consist of more capable
      sellers that employ their R&amp;D resources to come up with
      more sophisticated approaches (such as UCB1 or Exp3) but also
      on more basic sellers that employ simpler strategies (such as
      ϵ-Greedy). Another interpretation is that the distinction is
      not necessarily in terms of sophistication, but could also be
      due to different market research, goals, or general business
      strategies, which may lead to different decisions in terms of
      which strategy to adopt.</p>
      <p>Our experiments are run for 200 sellers, a case which
      already captures a lot of scenarios of interest in real
      e-commerce platforms. A straightforward application of the
      reinforcement learning algorithms for much larger numbers of
      sellers is problematic however, as the action space of the
      MDP increases significantly, which has drastic effects on
      their running time. To ensure scalability, we employ a very
      natural heuristic, where we divide the impression allocation
      problem into sub-problems and then solve each one of those in
      parallel. We show at the end of the section that this
      “scale-and-solve” version of IA(GRU) clearly outperforms the
      other algorithms for large instances consisting of as many as
      10.000 sellers.</p>
      <p><strong>Experimental Setup:</strong> In the implementation
      of DDPG, the actor network uses two full connected layers, a
      rectified linear unit (ReLu) as the activation function, and
      outputs the action by a softmax function. The critic network
      inputs a (state,action) pair and outputs the estimation of
      the Q-value using similar structure. The algorithm IA(GRU)
      uses the same structure, i.e. the fully connected network in
      the sub-actor and sub-critic networks, and uses a Recurrent
      Neural Network with gate recurrent units (GRU) in cyclic
      layers to obtain the inputs of these networks. For the
      experiments we set <em>T</em> = 1, i.e, the record of all
      items of the last round is viewed as the state.<a class="fn"
      href="#fn5" id="foot-fn5"><sup>5</sup></a> We employ
      heuristic algorithms such as the Greedy Myopic Algorithm for
      exploration, i.e. we add these samples to the replay buffer
      before training.</p>
      <p><strong>Experimental Parameters:</strong> We use 1000
      episodes for both training and testing, and there are 1000
      steps in each episode. The valuation of the buyer in each
      round is drawn from the standard uniform distribution
      <em>U</em>(0, 1) and the costs of sellers follow a Gaussian
      distribution with mean 1/2 and variance 1/2. The size of the
      replay buffer is 10<sup>5</sup>, the discount factor
      <em>γ</em> is 0.99, and the rate of update of the target
      network is 10<sup>− 3</sup>. The actor network and the critic
      network are trained via the <em>Adam</em> algorithm, a
      gradient descent algorithm presented in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>], and the learning rates
      of these two networks are 10<sup>− 4</sup>. Following the
      same idea as in [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>], we add Gaussian noise to the action
      outputted by the actor network, with the mean of the noise
      decaying with the number of episodes in the exploration.</p>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3>Convergence of DDPG and IA(GRU)</h3>
          </div>
        </header>
        <p>First, to show the difference in the convergence
        properties of DDPG and IA(GRU), we train the algorithms for
        200 sellers using the ϵ-greedy strategy as the rationality
        model with variable costs for the sellers. Figure <a class=
        "fig" href="#fig2">2</a> shows the comparison between the
        rewards of the algorithms and Figure <a class="fig" href=
        "#fig3">3</a> shows the comparison in terms of the training
        loss with the number of steps.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Rewards of DDPG and IA(GRU)
            in training for rational sellers.</span>
          </div>
        </figure>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Loss of DDPG and IA(GRU) in
            training for rational sellers.</span>
          </div>
        </figure>
        <p></p>
        <p>The gray band shows the variance of the vector of
        rewards near each step. From the figures, we see that DDPG
        does not converge, while IA(GRU) converges, as the training
        loss of the algorithm decreases with the number of steps.
        The convergence properties for the other rationality models
        are very similar.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3>Performance Comparison</h3>
          </div>
        </header>
        <p>In this subsection, we present the revenue guarantees of
        IA(GRU) in the setting with 200 sellers and how it fairs
        against the heuristics and DDPG, for either each
        rationality model separately, or for a heterogeneous pool
        of sellers, with a 1/4-fraction of the sellers following
        each strategy. As explained in the previous page, we
        consider both the case of <em>fixed</em> sellers and
        <em>variable</em> sellers.</p>
        <p><strong>Performance Comparison for Fixed
        Sellers:</strong> We show the performance of DDPG, IA(GRU),
        Greedy Myopic and Linear UCB on sellers using</p>
        <ul class="list-no-style">
          <li id="list9" label="•">the ϵ-Greedy strategy (Figure
          <a class="fig" href="#fig4">4</a>),<br />
          </li>
          <li id="list10" label="•">the ϵ-First strategy (Figure
          <a class="fig" href="#fig5">5</a>),<br />
          </li>
          <li id="list11" label="•">the UCB1 strategy (Figure
          <a class="fig" href="#fig6">6</a>),<br />
          </li>
          <li id="list12" label="•">the Exp3 strategy (Figure
          <a class="fig" href="#fig7">7</a>).<br />
          </li>
        </ul>
        <p>We also show the performance of the four different
        algorithms in the case of a heterogeneous population of
        sellers in Figure <a class="fig" href="#fig8">8</a>.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Rewards for fixed sellers
            and ϵ-Greedy strategies.</span>
          </div>
        </figure>
        <figure id="fig5">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig5.jpg"
          class="img-responsive" alt="Figure 5" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 5:</span>
            <span class="figure-title">Rewards for fixed sellers
            and ϵ-First strategies.</span>
          </div>
        </figure>
        <figure id="fig6">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig6.jpg"
          class="img-responsive" alt="Figure 6" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 6:</span>
            <span class="figure-title">Rewards for fixed sellers
            and UCB1 strategies.</span>
          </div>
        </figure>
        <figure id="fig7">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig7.jpg"
          class="img-responsive" alt="Figure 7" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 7:</span>
            <span class="figure-title">Rewards for fixed sellers
            and Exp3 strategies.</span>
          </div>
        </figure>
        <figure id="fig8">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig8.jpg"
          class="img-responsive" alt="Figure 8" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 8:</span>
            <span class="figure-title">Rewards for fixed sellers
            and heterogeneous strategies.</span>
          </div>
        </figure>
        <p>Every point of the figures shows the reward at the
        corresponding step. We can conclude that the IA(GRU)
        algorithm is clearly better than the other algorithms in
        terms of the average reward on all rationality models. We
        also note that DPPG does not converge with 200 sellers and
        this is the reason for its poor performance.</p>
        <p><strong>Performance Comparison for Variable
        Sellers:</strong> We show the performance of DDPG, IA(GRU),
        Greedy Myopic and Linear UCB on sellers using</p>
        <ul class="list-no-style">
          <li id="list13" label="•">the ϵ-Greedy strategy (Figure
          <a class="fig" href="#fig9">9</a>),<br />
          </li>
          <li id="list14" label="•">the ϵ-First strategy (Figure
          <a class="fig" href="#fig10">10</a>),<br />
          </li>
          <li id="list15" label="•">the UCB1 strategy (Figure
          <a class="fig" href="#fig11">11</a>),<br />
          </li>
          <li id="list16" label="•">the Exp3 strategy (Figure
          <a class="fig" href="#fig12">12</a>).<br />
          </li>
        </ul>
        <p>We also show the performance of the four different
        algorithms in the case of a heterogeneous population of
        sellers in Figure <a class="fig" href="#fig13">13</a>.
        Again here, we can conclude that the IA(GRU) algorithm
        clearly outperforms all the other algorithms in terms of
        the average reward on all rationality models. Also, IA(GRU)
        fairs better in terms of stability, as the other algorithms
        perform worse in the setting with variable sellers,
        compared to the setting with fixed sellers.</p>
        <figure id="fig9">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig9.jpg"
          class="img-responsive" alt="Figure 9" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 9:</span>
            <span class="figure-title">Rewards for variable sellers
            and ϵ-Greedy strategies.</span>
          </div>
        </figure>
        <figure id="fig10">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig10.jpg"
          class="img-responsive" alt="Figure 10" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 10:</span>
            <span class="figure-title">Rewards for variable sellers
            and ϵ-First strategies.</span>
          </div>
        </figure>
        <figure id="fig11">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig11.jpg"
          class="img-responsive" alt="Figure 11" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 11:</span>
            <span class="figure-title">Rewards for variable sellers
            and UCB1 strategies.</span>
          </div>
        </figure>
        <figure id="fig12">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig12.jpg"
          class="img-responsive" alt="Figure 12" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 12:</span>
            <span class="figure-title">Rewards for variable sellers
            and Exp3 strategies.</span>
          </div>
        </figure>
        <figure id="fig13">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig13.jpg"
          class="img-responsive" alt="Figure 13" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 13:</span>
            <span class="figure-title">Rewards for variable sellers
            and heterogeneous strategies.</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3>Scalability</h3>
          </div>
        </header>
        <p>In this subsection, we present the revenue guarantees of
        IA(GRU) in the setting with 10000 <em>fixed</em> sellers
        and how it fairs against the heuristics and DDPG to show
        the scalability properties of IA(GRU) with the number of
        sellers. For IA(GRU) and DDPG, we will employ a simple
        “scale-and-solve” variant, since applying either of them
        directly to the pool of 10.000 sellers is prohibitive in
        terms of their running time. We design 50 allocation
        sub-problems, consisting of 200 sellers each, and divide
        the total number of impressions in 50 sets of equal size,
        reserved for each sub-problem. We run IA(GRU) and DDPG
        algorithms in parallel for each sub-problem, which is
        feasible in reasonable time. For the heuristics, we run the
        algorithms directly on the large population of 10.000
        sellers. The results for the case of ϵ-Greedy seller
        strategies are show in Figure <a class="fig" href=
        "#fig14">14</a> (the results for other strategies are
        similar). We can see that even though we are applying a
        heuristic version, the performance of IA(GRU) is still
        clearly superior to all the other algorithms, which attests
        to the algorithm being employable in larger-case problems
        as well.</p>
        <figure id="fig14">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186039/images/www2018-48-fig14.jpg"
          class="img-responsive" alt="Figure 14" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 14:</span>
            <span class="figure-title">Rewards for 10.000 fixed
            sellers and ϵ-Greedy strategies.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we employed a reinforcement mechanism
      design framework for solving the impression allocation
      problem of large e-commerce websites, while taking the
      rationality of sellers into account. Inspired by recent
      advances in reinforcement learning, we designed a deep
      reinforcement learning algorithm which outperforms several
      natural heuristics under different realistic rationality
      assumptions for the sellers in terms of the generated
      revenue, as well as state-of-the-art reinforcement learning
      algorithms in terms of performance and convergence
      guarantees.</p>
      <p>Our algorithm can be applied to other dynamical settings
      for which the objectives are similar, i.e. there are multiple
      agents with evolving strategies, with the objective of
      maximizing a sum of payments or the generated revenue of each
      agent. It is an interesting future direction to identify
      several such concrete settings and apply our algorithm (or
      more generally our framework), to see if it provides
      improvements over the standard approaches, as it does
      here.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-19">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>Qingpeng Cai and Pingzhong Tang were supported in part by
      the National Natural Science Foundation of China Grant
      61561146398, a Tsinghua University Initiative Scientific
      Research Grant, a China Youth 1000-talent program and Alibaba
      Innovative Research program. Aris Filos-Ratsikas was
      supported by the ERC Advanced Grant 321171 (ALGAME).</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Sander Adam, Lucian
        Busoniu, and Robert Babuska. 2012. Experience replay for
        real-time reinforcement learning control. <em><em>IEEE
        Transactions on Systems, Man, and Cybernetics, Part C
        (Applications and Reviews)</em></em> 42, 2 (2012),
        201–212.</li>
        <li id="BibPLXBIB0002" label="[2]">Rajeev Agrawal. 1995.
        Sample mean based index policies by O (log n) regret for
        the multi-armed bandit problem. <em><em>Advances in Applied
        Probability</em></em> 27, 4 (1995), 1054–1078.</li>
        <li id="BibPLXBIB0003" label="[3]">Shipra Agrawal and Navin
        Goyal. 2013. Thompson sampling for contextual bandits with
        linear payoffs. In <em><em>International Conference on
        Machine Learning</em></em> . 127–135.</li>
        <li id="BibPLXBIB0004" label="[4]">Peter Auer, Nicolo
        Cesa-Bianchi, and Paul Fischer. 2002. Finite-time analysis
        of the multiarmed bandit problem. <em><em>Machine
        learning</em></em> 47, 2-3 (2002), 235–256.</li>
        <li id="BibPLXBIB0005" label="[5]">Peter Auer, Nicolo
        Cesa-Bianchi, Yoav Freund, and Robert&nbsp;E Schapire.
        1995. Gambling in a rigged casino: The adversarial
        multi-armed bandit problem. In <em><em>Foundations of
        Computer Science, 1995. Proceedings., 36th Annual Symposium
        on</em></em> . IEEE, 322–331.</li>
        <li id="BibPLXBIB0006" label="[6]">Peter Auer, Nicolo
        Cesa-Bianchi, Yoav Freund, and Robert&nbsp;E Schapire.
        2002. The nonstochastic multiarmed bandit problem.
        <em><em>SIAM journal on computing</em></em> 32, 1 (2002),
        48–77.</li>
        <li id="BibPLXBIB0007" label="[7]">Shalabh Bhatnagar,
        Richard&nbsp;S Sutton, Mohammad Ghavamzadeh, and Mark Lee.
        2007. Incremental Natural Actor-Critic Algorithms.. In
        <em><em>NIPS</em></em> . 105–112.</li>
        <li id="BibPLXBIB0008" label="[8]">Djallel Bouneffouf, Amel
        Bouzeghoub, and Alda&nbsp;Lopes Gançarski. 2012. A
        contextual-bandit algorithm for mobile context-aware
        recommender system. In <em><em>International Conference on
        Neural Information Processing</em></em> . Springer,
        324–331.</li>
        <li id="BibPLXBIB0009" label="[9]">Giuseppe Burtini, Jason
        Loeppky, and Ramon Lawrence. 2015. A survey of online
        experiment design with the stochastic multi-armed bandit.
        <em><em>arXiv preprint arXiv:1510.00757</em></em>
        (2015).</li>
        <li id="BibPLXBIB0010" label="[10]">Qingpeng Cai, Aris
        Filos-Ratsikas, Chang Liu, and Pingzhong Tang. 2016.
        Mechanism Design for Personalized Recommender Systems. In
        <em><em>Proceedings of the 10th ACM Conference on
        Recommender Systems</em></em> . ACM, 159–166.</li>
        <li id="BibPLXBIB0011" label="[11]">Qingpeng Cai, Aris
        Filos-Ratsikas, Pingzhong Tang, and Yiwei Zhang. 2018.
        Reinforcement Mechanism Design for Fraudulent Behaviour in
        e-Commerce. (2018).</li>
        <li id="BibPLXBIB0012" label="[12]">Shuchi Chawla, Jason
        Hartline, and Denis Nekipelov. 2016. A/B testing of
        auctions. In <em><em>Proceedings of the 2016 ACM Conference
        on Economics and Computation</em></em> . ACM, 19–20.</li>
        <li id="BibPLXBIB0013" label="[13]">Constantinos Daskalakis
        and Vasilis Syrgkanis. 2016. Learning in auctions: Regret
        is hard, envy is easy. In <em><em>Foundations of Computer
        Science (FOCS), 2016 IEEE 57th Annual Symposium
        on</em></em> . IEEE, 219–228.</li>
        <li id="BibPLXBIB0014" label="[14]">Peter Dayan and CJCH
        Watkins. 1992. Q-learning. <em><em>Machine
        learning</em></em> 8, 3 (1992), 279–292.</li>
        <li id="BibPLXBIB0015" label="[15]">Thomas Degris,
        Patrick&nbsp;M Pilarski, and Richard&nbsp;S Sutton. 2012.
        Model-free reinforcement learning with continuous action in
        practice. In <em><em>American Control Conference (ACC),
        2012</em></em> . IEEE, 2177–2182.</li>
        <li id="BibPLXBIB0016" label="[16]">Dylan&nbsp;J Foster,
        Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva
        Tardos. 2016. Learning in games: Robustness of fast
        convergence. In <em><em>Advances in Neural Information
        Processing Systems</em></em> . 4734–4742.</li>
        <li id="BibPLXBIB0017" label="[17]">Yoav Freund and
        Robert&nbsp;E Schapire. 1995. A desicion-theoretic
        generalization of on-line learning and an application to
        boosting. In <em><em>European conference on computational
        learning theory</em></em> . Springer, 23–37.</li>
        <li id="BibPLXBIB0018" label="[18]">Sergiu Hart. 2005.
        Adaptive heuristics. <em><em>Econometrica</em></em> 73, 5
        (2005), 1401–1430.</li>
        <li id="BibPLXBIB0019" label="[19]">Sergiu Hart and Andreu
        Mas-Colell. 2000. A simple adaptive procedure leading to
        correlated equilibrium. <em><em>Econometrica</em></em> 68,
        5 (2000), 1127–1150.</li>
        <li id="BibPLXBIB0020" label="[20]">Sergiu Hart and Andreu
        Mas-Colell. 2001. A general class of adaptive strategies.
        <em><em>Journal of Economic Theory</em></em> 98, 1 (2001),
        26–54.</li>
        <li id="BibPLXBIB0021" label="[21]">Jason Hartline, Vasilis
        Syrgkanis, and Eva Tardos. 2015. No-regret learning in
        Bayesian games. In <em><em>Advances in Neural Information
        Processing Systems</em></em> . 3061–3069.</li>
        <li id="BibPLXBIB0022" label="[22]">Nicolas Heess,
        Jonathan&nbsp;J Hunt, Timothy&nbsp;P Lillicrap, and David
        Silver. 2015. Memory-based control with recurrent neural
        networks. <em><em>arXiv preprint arXiv:1512.04455</em></em>
        (2015).</li>
        <li id="BibPLXBIB0023" label="[23]">Sergey Ioffe and
        Christian Szegedy. 2015. Batch normalization: Accelerating
        deep network training by reducing internal covariate shift.
        <em><em>arXiv preprint arXiv:1502.03167</em></em>
        (2015).</li>
        <li id="BibPLXBIB0024" label="[24]">Diederik Kingma and
        Jimmy Ba. 2014. Adam: A method for stochastic optimization.
        <em><em>arXiv preprint arXiv:1412.6980</em></em>
        (2014).</li>
        <li id="BibPLXBIB0025" label="[25]">Andreas Krause and
        Cheng&nbsp;S Ong. 2011. Contextual gaussian process bandit
        optimization. In <em><em>Advances in Neural Information
        Processing Systems</em></em> . 2447–2455.</li>
        <li id="BibPLXBIB0026" label="[26]">Lihong Li, Wei Chu,
        John Langford, and Robert&nbsp;E Schapire. 2010. A
        contextual-bandit approach to personalized news article
        recommendation. In <em><em>Proceedings of the 19th
        international conference on World wide web</em></em> . ACM,
        661–670.</li>
        <li id="BibPLXBIB0027" label="[27]">Timothy&nbsp;P
        Lillicrap, Jonathan&nbsp;J Hunt, Alexander Pritzel, Nicolas
        Heess, Tom Erez, Yuval Tassa, David Silver, and Daan
        Wierstra. 2015. Continuous control with deep reinforcement
        learning. <em><em>arXiv preprint arXiv:1509.02971</em></em>
        (2015).</li>
        <li id="BibPLXBIB0028" label="[28]">Thodoris Lykouris,
        Vasilis Syrgkanis, and Éva Tardos. 2016. Learning and
        efficiency in games with dynamic population. In
        <em><em>Proceedings of the Twenty-Seventh Annual ACM-SIAM
        Symposium on Discrete Algorithms</em></em> . Society for
        Industrial and Applied Mathematics, 120–129.</li>
        <li id="BibPLXBIB0029" label="[29]">Eric&nbsp;S Maskin.
        2008. Mechanism design: How to implement social goals.
        <em><em>The American Economic Review</em></em> 98, 3
        (2008), 567–576.</li>
        <li id="BibPLXBIB0030" label="[30]">Volodymyr Mnih, Koray
        Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,
        Daan Wierstra, and Martin Riedmiller. 2013. Playing atari
        with deep reinforcement learning. <em><em>arXiv preprint
        arXiv:1312.5602</em></em> (2013).</li>
        <li id="BibPLXBIB0031" label="[31]">Volodymyr Mnih, Koray
        Kavukcuoglu, David Silver, Andrei&nbsp;A Rusu, Joel Veness,
        Marc&nbsp;G Bellemare, Alex Graves, Martin Riedmiller,
        Andreas&nbsp;K Fidjeland, Georg Ostrovski, <em>et al.</em>
        2015. Human-level control through deep reinforcement
        learning. <em><em>Nature</em></em> 518, 7540 (2015),
        529–533.</li>
        <li id="BibPLXBIB0032" label="[32]">Denis Nekipelov,
        Vasilis Syrgkanis, and Eva Tardos. 2015. Econometrics for
        learning agents. In <em><em>Proceedings of the Sixteenth
        ACM Conference on Economics and Computation</em></em> .
        ACM, 1–18.</li>
        <li id="BibPLXBIB0033" label="[33]">A&nbsp;Blum PI, M Blum,
        M Kearns, T Sandholm, and MT Hajiaghayi. [n. d.]. Machine
        Learning, Game Theory, and Mechanism Design for a Networked
        World. ([n. d.]).</li>
        <li id="BibPLXBIB0034" label="[34]">Francesco Ricci, Lior
        Rokach, and Bracha Shapira. 2011. <em><em>Introduction to
        recommender systems handbook</em></em> . Springer.</li>
        <li id="BibPLXBIB0035" label="[35]">Ariel Rubinstein. 1998.
        <em><em>Modeling bounded rationality</em></em> . MIT
        press.</li>
        <li id="BibPLXBIB0036" label="[36]">Weiran Shen, Binghui
        Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan Hong,
        Zhi Guo, Zongyao Ding, Pengjun Lu, and Pingzhong Tang.
        2017. Reinforcement mechanism design, with applications to
        dynamic pricing in sponsored search auctions. <em><em>arXiv
        preprint arXiv:1711.10279</em></em> (2017).</li>
        <li id="BibPLXBIB0037" label="[37]">David Silver, Guy
        Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
        Martin Riedmiller. 2014. Deterministic policy gradient
        algorithms. In <em><em>International Conference on Machine
        Learning (ICML</em></em> .</li>
        <li id="BibPLXBIB0038" label="[38]">Richard&nbsp;S Sutton.
        1988. Learning to predict by the methods of temporal
        differences. <em><em>Machine learning</em></em> 3, 1
        (1988), 9–44.</li>
        <li id="BibPLXBIB0039" label="[39]">Richard&nbsp;S Sutton,
        David&nbsp;A McAllester, Satinder&nbsp;P Singh, Yishay
        Mansour, <em>et al.</em> 1999. Policy gradient methods for
        reinforcement learning with function approximation.. In
        <em><em>NIPS</em></em> , Vol.&nbsp;99. 1057–1063.</li>
        <li id="BibPLXBIB0040" label="[40]">Pingzhong Tang. 2017.
        Reinforcement Mechanism Design. In <em><em>Early Carrer
        Highlights at Proceedings of the 26th International Joint
        Conference on Artificial Intelligence (IJCAI</em></em> .
        5146–5150.</li>
        <li id="BibPLXBIB0041" label="[41]">Eva Tardos. 2017.
        Learning and Efficiency of Outcomes in Games. (2017).
        Seminar Slides.</li>
        <li id="BibPLXBIB0042" label="[42]">Hado Van&nbsp;Hasselt,
        Arthur Guez, and David Silver. 2016. Deep Reinforcement
        Learning with Double Q-Learning.. In <em><em>AAAI</em></em>
        . 2094–2100.</li>
        <li id="BibPLXBIB0043" label="[43]">Christopher John
        Cornish&nbsp;Hellaby Watkins. 1989. <em>Learning from
        delayed rewards</em>. Ph.D. Dissertation. King's College,
        Cambridge.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>Since the buyer
    impressions to be allocated is a huge number, we model it as a
    continuous unit to be fractionally allocated. Even if we used a
    large integer number instead, the traditional approaches like
    DDPG fall short for the same reasons and furthermore all of the
    performance guarantees of IA(GRU) extend to that case.</p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>As the
    purchasing behavior is determined by the valuation of buyers
    over the item, without loss of generality we could consider
    only one buyer at each round.</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>The framework
    extends to cases where we need return similar but different
    items to a buyer, i.e, the algorithm outputs a ranking over
    these items. Furthermore, our approach extends trivially to the
    case when sellers have multiple items.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>For ease of
    notation, we drop the subscript referring to a specific seller,
    as there is no ambiguity.</p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>We found out
    that training our algorithms for larger values of <em>T</em>
    does not help to improve the performance.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186039">https://doi.org/10.1145/3178876.3186039</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
